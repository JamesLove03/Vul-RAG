{
    "vul_data": [
        {
            "id": 65,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "int dccp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tconst struct sockaddr_in *usin = (struct sockaddr_in *)uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\t__be16 orig_sport, orig_dport;\n\t__be32 daddr, nexthop;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < sizeof(struct sockaddr_in))\n\t\treturn -EINVAL;\n\n\tif (usin->sin_family != AF_INET)\n\t\treturn -EAFNOSUPPORT;\n\n\tnexthop = daddr = usin->sin_addr.s_addr;\n\tif (inet->opt != NULL && inet->opt->srr) {\n\t\tif (daddr == 0)\n\t\t\treturn -EINVAL;\n\t\tnexthop = inet->opt->faddr;\n\t}\n\n\torig_sport = inet->inet_sport;\n\torig_dport = usin->sin_port;\n\trt = ip_route_connect(&fl4, nexthop, inet->inet_saddr,\n\t\t\t      RT_CONN_FLAGS(sk), sk->sk_bound_dev_if,\n\t\t\t      IPPROTO_DCCP,\n\t\t\t      orig_sport, orig_dport, sk, true);\n\tif (IS_ERR(rt))\n\t\treturn PTR_ERR(rt);\n\n\tif (rt->rt_flags & (RTCF_MULTICAST | RTCF_BROADCAST)) {\n\t\tip_rt_put(rt);\n\t\treturn -ENETUNREACH;\n\t}\n\n\tif (inet->opt == NULL || !inet->opt->srr)\n\t\tdaddr = rt->rt_dst;\n\n\tif (inet->inet_saddr == 0)\n\t\tinet->inet_saddr = rt->rt_src;\n\tinet->inet_rcv_saddr = inet->inet_saddr;\n\n\tinet->inet_dport = usin->sin_port;\n\tinet->inet_daddr = daddr;\n\n\tinet_csk(sk)->icsk_ext_hdr_len = 0;\n\tif (inet->opt != NULL)\n\t\tinet_csk(sk)->icsk_ext_hdr_len = inet->opt->optlen;\n\t/*\n\t * Socket identity is still unknown (sport may be zero).\n\t * However we set state to DCCP_REQUESTING and not releasing socket\n\t * lock select source port, enter ourselves into the hash tables and\n\t * complete initialization after this.\n\t */\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet_hash_connect(&dccp_death_row, sk);\n\tif (err != 0)\n\t\tgoto failure;\n\n\trt = ip_route_newports(&fl4, rt, orig_sport, orig_dport,\n\t\t\t       inet->inet_sport, inet->inet_dport, sk);\n\tif (IS_ERR(rt)) {\n\t\trt = NULL;\n\t\tgoto failure;\n\t}\n\t/* OK, now commit destination to socket.  */\n\tsk_setup_caps(sk, &rt->dst);\n\n\tdp->dccps_iss = secure_dccp_sequence_number(inet->inet_saddr,\n\t\t\t\t\t\t    inet->inet_daddr,\n\t\t\t\t\t\t    inet->inet_sport,\n\t\t\t\t\t\t    inet->inet_dport);\n\tinet->inet_id = dp->dccps_iss ^ jiffies;\n\n\terr = dccp_connect(sk);\n\trt = NULL;\n\tif (err != 0)\n\t\tgoto failure;\nout:\n\treturn err;\nfailure:\n\t/*\n\t * This unhashes the socket and releases the local port, if necessary.\n\t */\n\tdccp_set_state(sk, DCCP_CLOSED);\n\tip_rt_put(rt);\n\tsk->sk_route_caps = 0;\n\tinet->inet_dport = 0;\n\tgoto out;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 66,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "static struct sock *dccp_v6_request_recv_sock(struct sock *sk,\n\t\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t\t      struct request_sock *req,\n\t\t\t\t\t      struct dst_entry *dst)\n{\n\tstruct inet6_request_sock *ireq6 = inet6_rsk(req);\n\tstruct ipv6_pinfo *newnp, *np = inet6_sk(sk);\n\tstruct inet_sock *newinet;\n\tstruct dccp6_sock *newdp6;\n\tstruct sock *newsk;\n\tstruct ipv6_txoptions *opt;\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t/*\n\t\t *\tv6 mapped\n\t\t */\n\t\tnewsk = dccp_v4_request_recv_sock(sk, skb, req, dst);\n\t\tif (newsk == NULL)\n\t\t\treturn NULL;\n\n\t\tnewdp6 = (struct dccp6_sock *)newsk;\n\t\tnewinet = inet_sk(newsk);\n\t\tnewinet->pinet6 = &newdp6->inet6;\n\t\tnewnp = inet6_sk(newsk);\n\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\t\tipv6_addr_set_v4mapped(newinet->inet_daddr, &newnp->daddr);\n\n\t\tipv6_addr_set_v4mapped(newinet->inet_saddr, &newnp->saddr);\n\n\t\tipv6_addr_copy(&newnp->rcv_saddr, &newnp->saddr);\n\n\t\tinet_csk(newsk)->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tnewsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\t\tnewnp->pktoptions  = NULL;\n\t\tnewnp->opt\t   = NULL;\n\t\tnewnp->mcast_oif   = inet6_iif(skb);\n\t\tnewnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;\n\n\t\t/*\n\t\t * No need to charge this sock to the relevant IPv6 refcnt debug socks count\n\t\t * here, dccp_create_openreq_child now does this for us, see the comment in\n\t\t * that function for the gory details. -acme\n\t\t */\n\n\t\t/* It is tricky place. Until this moment IPv4 tcp\n\t\t   worked with IPv6 icsk.icsk_af_ops.\n\t\t   Sync it now.\n\t\t */\n\t\tdccp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);\n\n\t\treturn newsk;\n\t}\n\n\topt = np->opt;\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto out_overflow;\n\n\tif (dst == NULL) {\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = IPPROTO_DCCP;\n\t\tipv6_addr_copy(&fl6.daddr, &ireq6->rmt_addr);\n\t\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\t\tipv6_addr_copy(&fl6.saddr, &ireq6->loc_addr);\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.fl6_dport = inet_rsk(req)->rmt_port;\n\t\tfl6.fl6_sport = inet_rsk(req)->loc_port;\n\t\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p, false);\n\t\tif (IS_ERR(dst))\n\t\t\tgoto out;\n\t}\n\n\tnewsk = dccp_create_openreq_child(sk, req, skb);\n\tif (newsk == NULL)\n\t\tgoto out_nonewsk;\n\n\t/*\n\t * No need to charge this sock to the relevant IPv6 refcnt debug socks\n\t * count here, dccp_create_openreq_child now does this for us, see the\n\t * comment in that function for the gory details. -acme\n\t */\n\n\t__ip6_dst_store(newsk, dst, NULL, NULL);\n\tnewsk->sk_route_caps = dst->dev->features & ~(NETIF_F_IP_CSUM |\n\t\t\t\t\t\t      NETIF_F_TSO);\n\tnewdp6 = (struct dccp6_sock *)newsk;\n\tnewinet = inet_sk(newsk);\n\tnewinet->pinet6 = &newdp6->inet6;\n\tnewnp = inet6_sk(newsk);\n\n\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\tipv6_addr_copy(&newnp->daddr, &ireq6->rmt_addr);\n\tipv6_addr_copy(&newnp->saddr, &ireq6->loc_addr);\n\tipv6_addr_copy(&newnp->rcv_saddr, &ireq6->loc_addr);\n\tnewsk->sk_bound_dev_if = ireq6->iif;\n\n\t/* Now IPv6 options...\n\n\t   First: no IPv4 options.\n\t */\n\tnewinet->opt = NULL;\n\n\t/* Clone RX bits */\n\tnewnp->rxopt.all = np->rxopt.all;\n\n\t/* Clone pktoptions received with SYN */\n\tnewnp->pktoptions = NULL;\n\tif (ireq6->pktopts != NULL) {\n\t\tnewnp->pktoptions = skb_clone(ireq6->pktopts, GFP_ATOMIC);\n\t\tkfree_skb(ireq6->pktopts);\n\t\tireq6->pktopts = NULL;\n\t\tif (newnp->pktoptions)\n\t\t\tskb_set_owner_r(newnp->pktoptions, newsk);\n\t}\n\tnewnp->opt\t  = NULL;\n\tnewnp->mcast_oif  = inet6_iif(skb);\n\tnewnp->mcast_hops = ipv6_hdr(skb)->hop_limit;\n\n\t/*\n\t * Clone native IPv6 options from listening socket (if any)\n\t *\n\t * Yes, keeping reference count would be much more clever, but we make\n\t * one more one thing there: reattach optmem to newsk.\n\t */\n\tif (opt != NULL) {\n\t\tnewnp->opt = ipv6_dup_options(newsk, opt);\n\t\tif (opt != np->opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t}\n\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (newnp->opt != NULL)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +\n\t\t\t\t\t\t     newnp->opt->opt_flen);\n\n\tdccp_sync_mss(newsk, dst_mtu(dst));\n\n\tnewinet->inet_daddr = newinet->inet_saddr = LOOPBACK4_IPV6;\n\tnewinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tsock_put(newsk);\n\t\tgoto out;\n\t}\n\t__inet6_hash(newsk, NULL);\n\n\treturn newsk;\n\nout_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nout_nonewsk:\n\tdst_release(dst);\nout:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\tif (opt != NULL && opt != np->opt)\n\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\treturn NULL;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 67,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "int inet_sk_rebuild_header(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct rtable *rt = (struct rtable *)__sk_dst_check(sk, 0);\n\t__be32 daddr;\n\tint err;\n\n\t/* Route is OK, nothing to do. */\n\tif (rt)\n\t\treturn 0;\n\n\t/* Reroute. */\n\tdaddr = inet->inet_daddr;\n\tif (inet->opt && inet->opt->srr)\n\t\tdaddr = inet->opt->faddr;\n\trt = ip_route_output_ports(sock_net(sk), sk, daddr, inet->inet_saddr,\n\t\t\t\t   inet->inet_dport, inet->inet_sport,\n\t\t\t\t   sk->sk_protocol, RT_CONN_FLAGS(sk),\n\t\t\t\t   sk->sk_bound_dev_if);\n\tif (!IS_ERR(rt)) {\n\t\terr = 0;\n\t\tsk_setup_caps(sk, &rt->dst);\n\t} else {\n\t\terr = PTR_ERR(rt);\n\n\t\t/* Routing failed... */\n\t\tsk->sk_route_caps = 0;\n\t\t/*\n\t\t * Other protocols have to map its equivalent state to TCP_SYN_SENT.\n\t\t * DCCP maps its DCCP_REQUESTING state to TCP_SYN_SENT. -acme\n\t\t */\n\t\tif (!sysctl_ip_dynaddr ||\n\t\t    sk->sk_state != TCP_SYN_SENT ||\n\t\t    (sk->sk_userlocks & SOCK_BINDADDR_LOCK) ||\n\t\t    (err = inet_sk_reselect_saddr(sk)) != 0)\n\t\t\tsk->sk_err_soft = -err;\n\t}\n\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 68,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "void inet_sock_destruct(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\t__skb_queue_purge(&sk->sk_receive_queue);\n\t__skb_queue_purge(&sk->sk_error_queue);\n\n\tsk_mem_reclaim(sk);\n\n\tif (sk->sk_type == SOCK_STREAM && sk->sk_state != TCP_CLOSE) {\n\t\tpr_err(\"Attempt to release TCP socket in state %d %p\\n\",\n\t\t       sk->sk_state, sk);\n\t\treturn;\n\t}\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tpr_err(\"Attempt to release alive inet socket %p\\n\", sk);\n\t\treturn;\n\t}\n\n\tWARN_ON(atomic_read(&sk->sk_rmem_alloc));\n\tWARN_ON(atomic_read(&sk->sk_wmem_alloc));\n\tWARN_ON(sk->sk_wmem_queued);\n\tWARN_ON(sk->sk_forward_alloc);\n\n\tkfree(inet->opt);\n\tdst_release(rcu_dereference_check(sk->sk_dst_cache, 1));\n\tsk_refcnt_debug_dec(sk);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 69,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "static int inet_sk_reselect_saddr(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\t__be32 old_saddr = inet->inet_saddr;\n\t__be32 daddr = inet->inet_daddr;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\t__be32 new_saddr;\n\n\tif (inet->opt && inet->opt->srr)\n\t\tdaddr = inet->opt->faddr;\n\n\t/* Query new route. */\n\trt = ip_route_connect(&fl4, daddr, 0, RT_CONN_FLAGS(sk),\n\t\t\t      sk->sk_bound_dev_if, sk->sk_protocol,\n\t\t\t      inet->inet_sport, inet->inet_dport, sk, false);\n\tif (IS_ERR(rt))\n\t\treturn PTR_ERR(rt);\n\n\tsk_setup_caps(sk, &rt->dst);\n\n\tnew_saddr = rt->rt_src;\n\n\tif (new_saddr == old_saddr)\n\t\treturn 0;\n\n\tif (sysctl_ip_dynaddr > 1) {\n\t\tprintk(KERN_INFO \"%s(): shifting inet->saddr from %pI4 to %pI4\\n\",\n\t\t       __func__, &old_saddr, &new_saddr);\n\t}\n\n\tinet->inet_saddr = inet->inet_rcv_saddr = new_saddr;\n\n\t/*\n\t * XXX The only one ugly spot where we need to\n\t * XXX really change the sockets identity after\n\t * XXX it has entered the hashes. -DaveM\n\t *\n\t * Besides that, it does not check for connection\n\t * uniqueness. Wait for troubles.\n\t */\n\t__sk_prot_rehash(sk);\n\treturn 0;\n}",
            "detect_result": "NO\n\nThe provided code snippet appears to be part of the Linux kernel's networking stack, specifically dealing with reselecting the source address for an IPv4 socket. Here's a breakdown of its behavior:\n\n1. It retrieves the current source and destination addresses for the socket.\n2. If the socket options indicate source routing, it updates the destination address accordingly.\n3. It queries a new route based on the updated destination address and other socket parameters.\n4. If the route query fails, it returns an error.\n5. If the route query succeeds, it sets up the socket capabilities based on the new route.\n6. It checks if the new source address differs from the old one.\n7. If the system configuration allows dynamic IP addresses (`sysctl_ip_dynaddr > 1`), it logs the change.\n8. It updates the socket's source and receive source addresses to the new source address.\n9. It rehashes the socket in the protocol hash table to reflect the change in identity.\n\nWhile the code does have a comment indicating potential issues with changing a socket's identity after it has been hashed, this is more of a design consideration rather than a direct security vulnerability. There is no obvious buffer overflow, improper memory access, or other common vulnerabilities in the provided code snippet. However, the comment suggests that there might be implications for connection uniqueness and stability, which could be considered a broader architectural concern.",
            "final_result": 0
        },
        {
            "id": 70,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "void cipso_v4_req_delattr(struct request_sock *req)\n{\n\tstruct ip_options *opt;\n\tstruct inet_request_sock *req_inet;\n\n\treq_inet = inet_rsk(req);\n\topt = req_inet->opt;\n\tif (opt == NULL || opt->cipso == 0)\n\t\treturn;\n\n\tcipso_v4_delopt(&req_inet->opt);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 71,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "void cipso_v4_sock_delattr(struct sock *sk)\n{\n\tint hdr_delta;\n\tstruct ip_options *opt;\n\tstruct inet_sock *sk_inet;\n\n\tsk_inet = inet_sk(sk);\n\topt = sk_inet->opt;\n\tif (opt == NULL || opt->cipso == 0)\n\t\treturn;\n\n\thdr_delta = cipso_v4_delopt(&sk_inet->opt);\n\tif (sk_inet->is_icsk && hdr_delta > 0) {\n\t\tstruct inet_connection_sock *sk_conn = inet_csk(sk);\n\t\tsk_conn->icsk_ext_hdr_len -= hdr_delta;\n\t\tsk_conn->icsk_sync_mss(sk, sk_conn->icsk_pmtu_cookie);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 72,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "int cipso_v4_sock_getattr(struct sock *sk, struct netlbl_lsm_secattr *secattr)\n{\n\tstruct ip_options *opt;\n\n\topt = inet_sk(sk)->opt;\n\tif (opt == NULL || opt->cipso == 0)\n\t\treturn -ENOMSG;\n\n\treturn cipso_v4_getattr(opt->__data + opt->cipso - sizeof(struct iphdr),\n\t\t\t\tsecattr);\n}",
            "detect_result": "NO\n\nExplanation of the code behavior:\nThe function `cipso_v4_sock_getattr` is designed to retrieve CIPSO (Commercial IP Security Option) attributes from a socket's IP options. Here's a step-by-step breakdown of what the function does:\n\n1. It first retrieves the `ip_options` structure associated with the socket `sk` using `inet_sk(sk)->opt`.\n2. It checks if the `ip_options` structure (`opt`) is `NULL` or if the CIPSO option is not present (`opt->cipso == 0`). If either condition is true, it returns `-ENOMSG`, indicating that there is no CIPSO message available.\n3. If the CIPSO option is present, it calls `cipso_v4_getattr` with a pointer to the start of the CIPSO data within the IP options and the `secattr` structure where the security attributes will be stored.\n\nDetermination of vulnerability:\nBased on the provided code snippet alone, there are no obvious vulnerabilities such as buffer overflows, null pointer dereferences (other than the checked one), or other common issues. The function properly checks for the presence of the CIPSO option before attempting to access it. However, without seeing the implementation of `cipso_v4_getattr` and understanding how the `secattr` structure is used, it's impossible to rule out vulnerabilities in those areas. But strictly based on this function, there are no apparent issues.",
            "final_result": 0
        },
        {
            "id": 73,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "int cipso_v4_sock_setattr(struct sock *sk,\n\t\t\t  const struct cipso_v4_doi *doi_def,\n\t\t\t  const struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val = -EPERM;\n\tunsigned char *buf = NULL;\n\tu32 buf_len;\n\tu32 opt_len;\n\tstruct ip_options *opt = NULL;\n\tstruct inet_sock *sk_inet;\n\tstruct inet_connection_sock *sk_conn;\n\n\t/* In the case of sock_create_lite(), the sock->sk field is not\n\t * defined yet but it is not a problem as the only users of these\n\t * \"lite\" PF_INET sockets are functions which do an accept() call\n\t * afterwards so we will label the socket as part of the accept(). */\n\tif (sk == NULL)\n\t\treturn 0;\n\n\t/* We allocate the maximum CIPSO option size here so we are probably\n\t * being a little wasteful, but it makes our life _much_ easier later\n\t * on and after all we are only talking about 40 bytes. */\n\tbuf_len = CIPSO_V4_OPT_LEN_MAX;\n\tbuf = kmalloc(buf_len, GFP_ATOMIC);\n\tif (buf == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto socket_setattr_failure;\n\t}\n\n\tret_val = cipso_v4_genopt(buf, buf_len, doi_def, secattr);\n\tif (ret_val < 0)\n\t\tgoto socket_setattr_failure;\n\tbuf_len = ret_val;\n\n\t/* We can't use ip_options_get() directly because it makes a call to\n\t * ip_options_get_alloc() which allocates memory with GFP_KERNEL and\n\t * we won't always have CAP_NET_RAW even though we _always_ want to\n\t * set the IPOPT_CIPSO option. */\n\topt_len = (buf_len + 3) & ~3;\n\topt = kzalloc(sizeof(*opt) + opt_len, GFP_ATOMIC);\n\tif (opt == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto socket_setattr_failure;\n\t}\n\tmemcpy(opt->__data, buf, buf_len);\n\topt->optlen = opt_len;\n\topt->cipso = sizeof(struct iphdr);\n\tkfree(buf);\n\tbuf = NULL;\n\n\tsk_inet = inet_sk(sk);\n\tif (sk_inet->is_icsk) {\n\t\tsk_conn = inet_csk(sk);\n\t\tif (sk_inet->opt)\n\t\t\tsk_conn->icsk_ext_hdr_len -= sk_inet->opt->optlen;\n\t\tsk_conn->icsk_ext_hdr_len += opt->optlen;\n\t\tsk_conn->icsk_sync_mss(sk, sk_conn->icsk_pmtu_cookie);\n\t}\n\topt = xchg(&sk_inet->opt, opt);\n\tkfree(opt);\n\n\treturn 0;\n\nsocket_setattr_failure:\n\tkfree(buf);\n\tkfree(opt);\n\treturn ret_val;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 74,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "int cipso_v4_req_setattr(struct request_sock *req,\n\t\t\t const struct cipso_v4_doi *doi_def,\n\t\t\t const struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val = -EPERM;\n\tunsigned char *buf = NULL;\n\tu32 buf_len;\n\tu32 opt_len;\n\tstruct ip_options *opt = NULL;\n\tstruct inet_request_sock *req_inet;\n\n\t/* We allocate the maximum CIPSO option size here so we are probably\n\t * being a little wasteful, but it makes our life _much_ easier later\n\t * on and after all we are only talking about 40 bytes. */\n\tbuf_len = CIPSO_V4_OPT_LEN_MAX;\n\tbuf = kmalloc(buf_len, GFP_ATOMIC);\n\tif (buf == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto req_setattr_failure;\n\t}\n\n\tret_val = cipso_v4_genopt(buf, buf_len, doi_def, secattr);\n\tif (ret_val < 0)\n\t\tgoto req_setattr_failure;\n\tbuf_len = ret_val;\n\n\t/* We can't use ip_options_get() directly because it makes a call to\n\t * ip_options_get_alloc() which allocates memory with GFP_KERNEL and\n\t * we won't always have CAP_NET_RAW even though we _always_ want to\n\t * set the IPOPT_CIPSO option. */\n\topt_len = (buf_len + 3) & ~3;\n\topt = kzalloc(sizeof(*opt) + opt_len, GFP_ATOMIC);\n\tif (opt == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto req_setattr_failure;\n\t}\n\tmemcpy(opt->__data, buf, buf_len);\n\topt->optlen = opt_len;\n\topt->cipso = sizeof(struct iphdr);\n\tkfree(buf);\n\tbuf = NULL;\n\n\treq_inet = inet_rsk(req);\n\topt = xchg(&req_inet->opt, opt);\n\tkfree(opt);\n\n\treturn 0;\n\nreq_setattr_failure:\n\tkfree(buf);\n\tkfree(opt);\n\treturn ret_val;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 75,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "static void icmp_reply(struct icmp_bxm *icmp_param, struct sk_buff *skb)\n{\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct net *net = dev_net(rt->dst.dev);\n\tstruct sock *sk;\n\tstruct inet_sock *inet;\n\t__be32 daddr;\n\n\tif (ip_options_echo(&icmp_param->replyopts, skb))\n\t\treturn;\n\n\tsk = icmp_xmit_lock(net);\n\tif (sk == NULL)\n\t\treturn;\n\tinet = inet_sk(sk);\n\n\ticmp_param->data.icmph.checksum = 0;\n\n\tinet->tos = ip_hdr(skb)->tos;\n\tdaddr = ipc.addr = rt->rt_src;\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\tif (icmp_param->replyopts.optlen) {\n\t\tipc.opt = &icmp_param->replyopts;\n\t\tif (ipc.opt->srr)\n\t\t\tdaddr = icmp_param->replyopts.faddr;\n\t}\n\t{\n\t\tstruct flowi4 fl4 = {\n\t\t\t.daddr = daddr,\n\t\t\t.saddr = rt->rt_spec_dst,\n\t\t\t.flowi4_tos = RT_TOS(ip_hdr(skb)->tos),\n\t\t\t.flowi4_proto = IPPROTO_ICMP,\n\t\t};\n\t\tsecurity_skb_classify_flow(skb, flowi4_to_flowi(&fl4));\n\t\trt = ip_route_output_key(net, &fl4);\n\t\tif (IS_ERR(rt))\n\t\t\tgoto out_unlock;\n\t}\n\tif (icmpv4_xrlim_allow(net, rt, icmp_param->data.icmph.type,\n\t\t\t       icmp_param->data.icmph.code))\n\t\ticmp_push_reply(icmp_param, &ipc, &rt);\n\tip_rt_put(rt);\nout_unlock:\n\ticmp_xmit_unlock(sk);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 76,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "void icmp_send(struct sk_buff *skb_in, int type, int code, __be32 info)\n{\n\tstruct iphdr *iph;\n\tint room;\n\tstruct icmp_bxm icmp_param;\n\tstruct rtable *rt = skb_rtable(skb_in);\n\tstruct ipcm_cookie ipc;\n\t__be32 saddr;\n\tu8  tos;\n\tstruct net *net;\n\tstruct sock *sk;\n\n\tif (!rt)\n\t\tgoto out;\n\tnet = dev_net(rt->dst.dev);\n\n\t/*\n\t *\tFind the original header. It is expected to be valid, of course.\n\t *\tCheck this, icmp_send is called from the most obscure devices\n\t *\tsometimes.\n\t */\n\tiph = ip_hdr(skb_in);\n\n\tif ((u8 *)iph < skb_in->head ||\n\t    (skb_in->network_header + sizeof(*iph)) > skb_in->tail)\n\t\tgoto out;\n\n\t/*\n\t *\tNo replies to physical multicast/broadcast\n\t */\n\tif (skb_in->pkt_type != PACKET_HOST)\n\t\tgoto out;\n\n\t/*\n\t *\tNow check at the protocol level\n\t */\n\tif (rt->rt_flags & (RTCF_BROADCAST | RTCF_MULTICAST))\n\t\tgoto out;\n\n\t/*\n\t *\tOnly reply to fragment 0. We byte re-order the constant\n\t *\tmask for efficiency.\n\t */\n\tif (iph->frag_off & htons(IP_OFFSET))\n\t\tgoto out;\n\n\t/*\n\t *\tIf we send an ICMP error to an ICMP error a mess would result..\n\t */\n\tif (icmp_pointers[type].error) {\n\t\t/*\n\t\t *\tWe are an error, check if we are replying to an\n\t\t *\tICMP error\n\t\t */\n\t\tif (iph->protocol == IPPROTO_ICMP) {\n\t\t\tu8 _inner_type, *itp;\n\n\t\t\titp = skb_header_pointer(skb_in,\n\t\t\t\t\t\t skb_network_header(skb_in) +\n\t\t\t\t\t\t (iph->ihl << 2) +\n\t\t\t\t\t\t offsetof(struct icmphdr,\n\t\t\t\t\t\t\t  type) -\n\t\t\t\t\t\t skb_in->data,\n\t\t\t\t\t\t sizeof(_inner_type),\n\t\t\t\t\t\t &_inner_type);\n\t\t\tif (itp == NULL)\n\t\t\t\tgoto out;\n\n\t\t\t/*\n\t\t\t *\tAssume any unknown ICMP type is an error. This\n\t\t\t *\tisn't specified by the RFC, but think about it..\n\t\t\t */\n\t\t\tif (*itp > NR_ICMP_TYPES ||\n\t\t\t    icmp_pointers[*itp].error)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk = icmp_xmit_lock(net);\n\tif (sk == NULL)\n\t\treturn;\n\n\t/*\n\t *\tConstruct source address and options.\n\t */\n\n\tsaddr = iph->daddr;\n\tif (!(rt->rt_flags & RTCF_LOCAL)) {\n\t\tstruct net_device *dev = NULL;\n\n\t\trcu_read_lock();\n\t\tif (rt_is_input_route(rt) &&\n\t\t    net->ipv4.sysctl_icmp_errors_use_inbound_ifaddr)\n\t\t\tdev = dev_get_by_index_rcu(net, rt->rt_iif);\n\n\t\tif (dev)\n\t\t\tsaddr = inet_select_addr(dev, 0, RT_SCOPE_LINK);\n\t\telse\n\t\t\tsaddr = 0;\n\t\trcu_read_unlock();\n\t}\n\n\ttos = icmp_pointers[type].error ? ((iph->tos & IPTOS_TOS_MASK) |\n\t\t\t\t\t   IPTOS_PREC_INTERNETCONTROL) :\n\t\t\t\t\t  iph->tos;\n\n\tif (ip_options_echo(&icmp_param.replyopts, skb_in))\n\t\tgoto out_unlock;\n\n\n\t/*\n\t *\tPrepare data for ICMP header.\n\t */\n\n\ticmp_param.data.icmph.type\t = type;\n\ticmp_param.data.icmph.code\t = code;\n\ticmp_param.data.icmph.un.gateway = info;\n\ticmp_param.data.icmph.checksum\t = 0;\n\ticmp_param.skb\t  = skb_in;\n\ticmp_param.offset = skb_network_offset(skb_in);\n\tinet_sk(sk)->tos = tos;\n\tipc.addr = iph->saddr;\n\tipc.opt = &icmp_param.replyopts;\n\tipc.tx_flags = 0;\n\n\trt = icmp_route_lookup(net, skb_in, iph, saddr, tos,\n\t\t\t       type, code, &icmp_param);\n\tif (IS_ERR(rt))\n\t\tgoto out_unlock;\n\n\tif (!icmpv4_xrlim_allow(net, rt, type, code))\n\t\tgoto ende;\n\n\t/* RFC says return as much as we can without exceeding 576 bytes. */\n\n\troom = dst_mtu(&rt->dst);\n\tif (room > 576)\n\t\troom = 576;\n\troom -= sizeof(struct iphdr) + icmp_param.replyopts.optlen;\n\troom -= sizeof(struct icmphdr);\n\n\ticmp_param.data_len = skb_in->len - icmp_param.offset;\n\tif (icmp_param.data_len > room)\n\t\ticmp_param.data_len = room;\n\ticmp_param.head_len = sizeof(struct icmphdr);\n\n\ticmp_push_reply(&icmp_param, &ipc, &rt);\nende:\n\tip_rt_put(rt);\nout_unlock:\n\ticmp_xmit_unlock(sk);\nout:;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 77,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "static struct rtable *icmp_route_lookup(struct net *net, struct sk_buff *skb_in,\n\t\t\t\t\tconst struct iphdr *iph,\n\t\t\t\t\t__be32 saddr, u8 tos,\n\t\t\t\t\tint type, int code,\n\t\t\t\t\tstruct icmp_bxm *param)\n{\n\tstruct flowi4 fl4 = {\n\t\t.daddr = (param->replyopts.srr ?\n\t\t\t  param->replyopts.faddr : iph->saddr),\n\t\t.saddr = saddr,\n\t\t.flowi4_tos = RT_TOS(tos),\n\t\t.flowi4_proto = IPPROTO_ICMP,\n\t\t.fl4_icmp_type = type,\n\t\t.fl4_icmp_code = code,\n\t};\n\tstruct rtable *rt, *rt2;\n\tint err;\n\n\tsecurity_skb_classify_flow(skb_in, flowi4_to_flowi(&fl4));\n\trt = __ip_route_output_key(net, &fl4);\n\tif (IS_ERR(rt))\n\t\treturn rt;\n\n\t/* No need to clone since we're just using its address. */\n\trt2 = rt;\n\n\tif (!fl4.saddr)\n\t\tfl4.saddr = rt->rt_src;\n\n\trt = (struct rtable *) xfrm_lookup(net, &rt->dst,\n\t\t\t\t\t   flowi4_to_flowi(&fl4), NULL, 0);\n\tif (!IS_ERR(rt)) {\n\t\tif (rt != rt2)\n\t\t\treturn rt;\n\t} else if (PTR_ERR(rt) == -EPERM) {\n\t\trt = NULL;\n\t} else\n\t\treturn rt;\n\n\terr = xfrm_decode_session_reverse(skb_in, flowi4_to_flowi(&fl4), AF_INET);\n\tif (err)\n\t\tgoto relookup_failed;\n\n\tif (inet_addr_type(net, fl4.saddr) == RTN_LOCAL) {\n\t\trt2 = __ip_route_output_key(net, &fl4);\n\t\tif (IS_ERR(rt2))\n\t\t\terr = PTR_ERR(rt2);\n\t} else {\n\t\tstruct flowi4 fl4_2 = {};\n\t\tunsigned long orefdst;\n\n\t\tfl4_2.daddr = fl4.saddr;\n\t\trt2 = ip_route_output_key(net, &fl4_2);\n\t\tif (IS_ERR(rt2)) {\n\t\t\terr = PTR_ERR(rt2);\n\t\t\tgoto relookup_failed;\n\t\t}\n\t\t/* Ugh! */\n\t\torefdst = skb_in->_skb_refdst; /* save old refdst */\n\t\terr = ip_route_input(skb_in, fl4.daddr, fl4.saddr,\n\t\t\t\t     RT_TOS(tos), rt2->dst.dev);\n\n\t\tdst_release(&rt2->dst);\n\t\trt2 = skb_rtable(skb_in);\n\t\tskb_in->_skb_refdst = orefdst; /* restore old refdst */\n\t}\n\n\tif (err)\n\t\tgoto relookup_failed;\n\n\trt2 = (struct rtable *) xfrm_lookup(net, &rt2->dst,\n\t\t\t\t\t    flowi4_to_flowi(&fl4), NULL,\n\t\t\t\t\t    XFRM_LOOKUP_ICMP);\n\tif (!IS_ERR(rt2)) {\n\t\tdst_release(&rt->dst);\n\t\trt = rt2;\n\t} else if (PTR_ERR(rt2) == -EPERM) {\n\t\tif (rt)\n\t\t\tdst_release(&rt->dst);\n\t\treturn rt2;\n\t} else {\n\t\terr = PTR_ERR(rt2);\n\t\tgoto relookup_failed;\n\t}\n\treturn rt;\n\nrelookup_failed:\n\tif (rt)\n\t\treturn rt;\n\treturn ERR_PTR(err);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 78,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "struct dst_entry *inet_csk_route_req(struct sock *sk,\n\t\t\t\t     const struct request_sock *req)\n{\n\tstruct rtable *rt;\n\tconst struct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ip_options *opt = inet_rsk(req)->opt;\n\tstruct net *net = sock_net(sk);\n\tstruct flowi4 fl4;\n\n\tflowi4_init_output(&fl4, sk->sk_bound_dev_if, sk->sk_mark,\n\t\t\t   RT_CONN_FLAGS(sk), RT_SCOPE_UNIVERSE,\n\t\t\t   sk->sk_protocol, inet_sk_flowi_flags(sk),\n\t\t\t   (opt && opt->srr) ? opt->faddr : ireq->rmt_addr,\n\t\t\t   ireq->loc_addr, ireq->rmt_port, inet_sk(sk)->inet_sport);\n\tsecurity_req_classify_flow(req, flowi4_to_flowi(&fl4));\n\trt = ip_route_output_flow(net, &fl4, sk);\n\tif (IS_ERR(rt))\n\t\tgoto no_route;\n\tif (opt && opt->is_strictroute && rt->rt_dst != rt->rt_gateway)\n\t\tgoto route_err;\n\treturn &rt->dst;\n\nroute_err:\n\tip_rt_put(rt);\nno_route:\n\tIP_INC_STATS_BH(net, IPSTATS_MIB_OUTNOROUTES);\n\treturn NULL;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 79,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "void ip_options_build(struct sk_buff * skb, struct ip_options * opt,\n\t\t\t    __be32 daddr, struct rtable *rt, int is_frag)\n{\n\tunsigned char *iph = skb_network_header(skb);\n\n\tmemcpy(&(IPCB(skb)->opt), opt, sizeof(struct ip_options));\n\tmemcpy(iph+sizeof(struct iphdr), opt->__data, opt->optlen);\n\topt = &(IPCB(skb)->opt);\n\n\tif (opt->srr)\n\t\tmemcpy(iph+opt->srr+iph[opt->srr+1]-4, &daddr, 4);\n\n\tif (!is_frag) {\n\t\tif (opt->rr_needaddr)\n\t\t\tip_rt_get_source(iph+opt->rr+iph[opt->rr+2]-5, rt);\n\t\tif (opt->ts_needaddr)\n\t\t\tip_rt_get_source(iph+opt->ts+iph[opt->ts+2]-9, rt);\n\t\tif (opt->ts_needtime) {\n\t\t\tstruct timespec tv;\n\t\t\t__be32 midtime;\n\t\t\tgetnstimeofday(&tv);\n\t\t\tmidtime = htonl((tv.tv_sec % 86400) * MSEC_PER_SEC + tv.tv_nsec / NSEC_PER_MSEC);\n\t\t\tmemcpy(iph+opt->ts+iph[opt->ts+2]-5, &midtime, 4);\n\t\t}\n\t\treturn;\n\t}\n\tif (opt->rr) {\n\t\tmemset(iph+opt->rr, IPOPT_NOP, iph[opt->rr+1]);\n\t\topt->rr = 0;\n\t\topt->rr_needaddr = 0;\n\t}\n\tif (opt->ts) {\n\t\tmemset(iph+opt->ts, IPOPT_NOP, iph[opt->ts+1]);\n\t\topt->ts = 0;\n\t\topt->ts_needaddr = opt->ts_needtime = 0;\n\t}\n}",
            "detect_result": "YES",
            "final_result": 1
        },
        {
            "id": 107,
            "cve_id": "CVE-2012-4508",
            "code_snippet": "static int ext4_split_extent_at(handle_t *handle,\n\t\t\t     struct inode *inode,\n\t\t\t     struct ext4_ext_path *path,\n\t\t\t     ext4_lblk_t split,\n\t\t\t     int split_flag,\n\t\t\t     int flags)\n{\n\text4_fsblk_t newblock;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex, newex, orig_ex;\n\tstruct ext4_extent *ex2 = NULL;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\n\text_debug(\"ext4_split_extents_at: inode %lu, logical\"\n\t\t\"block %llu\\n\", inode->i_ino, (unsigned long long)split);\n\n\text4_ext_show_leaf(inode, path);\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tnewblock = split - ee_block + ext4_ext_pblock(ex);\n\n\tBUG_ON(split < ee_block || split >= (ee_block + ee_len));\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\n\tif (split == ee_block) {\n\t\t/*\n\t\t * case b: block @split is the block that the extent begins with\n\t\t * then we just change the state of the extent, and splitting\n\t\t * is not needed.\n\t\t */\n\t\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\t\text4_ext_mark_uninitialized(ex);\n\t\telse\n\t\t\text4_ext_mark_initialized(ex);\n\n\t\tif (!(flags & EXT4_GET_BLOCKS_PRE_IO))\n\t\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tgoto out;\n\t}\n\n\t/* case a */\n\tmemcpy(&orig_ex, ex, sizeof(orig_ex));\n\tex->ee_len = cpu_to_le16(split - ee_block);\n\tif (split_flag & EXT4_EXT_MARK_UNINIT1)\n\t\text4_ext_mark_uninitialized(ex);\n\n\t/*\n\t * path may lead to new leaf, not to original leaf any more\n\t * after ext4_ext_insert_extent() returns,\n\t */\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\tif (err)\n\t\tgoto fix_extent_len;\n\n\tex2 = &newex;\n\tex2->ee_block = cpu_to_le32(split);\n\tex2->ee_len   = cpu_to_le16(ee_len - (split - ee_block));\n\text4_ext_store_pblock(ex2, newblock);\n\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\text4_ext_mark_uninitialized(ex2);\n\n\terr = ext4_ext_insert_extent(handle, inode, path, &newex, flags);\n\tif (err == -ENOSPC && (EXT4_EXT_MAY_ZEROOUT & split_flag)) {\n\t\terr = ext4_ext_zeroout(inode, &orig_ex);\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\t\t/* update the extent length and mark as initialized */\n\t\tex->ee_len = cpu_to_le16(ee_len);\n\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tgoto out;\n\t} else if (err)\n\t\tgoto fix_extent_len;\n\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n\nfix_extent_len:\n\tex->ee_len = orig_ex.ee_len;\n\text4_ext_dirty(handle, inode, path + depth);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 167,
            "cve_id": "CVE-2013-0871",
            "code_snippet": "void __ptrace_unlink(struct task_struct *child)\n{\n\tBUG_ON(!child->ptrace);\n\n\tchild->ptrace = 0;\n\tchild->parent = child->real_parent;\n\tlist_del_init(&child->ptrace_entry);\n\n\tspin_lock(&child->sighand->siglock);\n\n\t/*\n\t * Clear all pending traps and TRAPPING.  TRAPPING should be\n\t * cleared regardless of JOBCTL_STOP_PENDING.  Do it explicitly.\n\t */\n\ttask_clear_jobctl_pending(child, JOBCTL_TRAP_MASK);\n\ttask_clear_jobctl_trapping(child);\n\n\t/*\n\t * Reinstate JOBCTL_STOP_PENDING if group stop is in effect and\n\t * @child isn't dead.\n\t */\n\tif (!(child->flags & PF_EXITING) &&\n\t    (child->signal->flags & SIGNAL_STOP_STOPPED ||\n\t     child->signal->group_stop_count)) {\n\t\tchild->jobctl |= JOBCTL_STOP_PENDING;\n\n\t\t/*\n\t\t * This is only possible if this thread was cloned by the\n\t\t * traced task running in the stopped group, set the signal\n\t\t * for the future reports.\n\t\t * FIXME: we should change ptrace_init_task() to handle this\n\t\t * case.\n\t\t */\n\t\tif (!(child->jobctl & JOBCTL_STOP_SIGMASK))\n\t\t\tchild->jobctl |= SIGSTOP;\n\t}\n\n\t/*\n\t * If transition to TASK_STOPPED is pending or in TASK_TRACED, kick\n\t * @child in the butt.  Note that @resume should be used iff @child\n\t * is in TASK_TRACED; otherwise, we might unduly disrupt\n\t * TASK_KILLABLE sleeps.\n\t */\n\tif (child->jobctl & JOBCTL_STOP_PENDING || task_is_traced(child))\n\t\tsignal_wake_up(child, task_is_traced(child));\n\n\tspin_unlock(&child->sighand->siglock);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 288,
            "cve_id": "CVE-2013-3302",
            "code_snippet": "static int\nsmb_send_kvec(struct TCP_Server_Info *server, struct kvec *iov, size_t n_vec,\n\t\tsize_t *sent)\n{\n\tint rc = 0;\n\tint i = 0;\n\tstruct msghdr smb_msg;\n\tunsigned int remaining;\n\tsize_t first_vec = 0;\n\tstruct socket *ssocket = server->ssocket;\n\n\t*sent = 0;\n\n\tif (ssocket == NULL)\n\t\treturn -ENOTSOCK; /* BB eventually add reconnect code here */\n\n\tsmb_msg.msg_name = (struct sockaddr *) &server->dstaddr;\n\tsmb_msg.msg_namelen = sizeof(struct sockaddr);\n\tsmb_msg.msg_control = NULL;\n\tsmb_msg.msg_controllen = 0;\n\tif (server->noblocksnd)\n\t\tsmb_msg.msg_flags = MSG_DONTWAIT + MSG_NOSIGNAL;\n\telse\n\t\tsmb_msg.msg_flags = MSG_NOSIGNAL;\n\n\tremaining = 0;\n\tfor (i = 0; i < n_vec; i++)\n\t\tremaining += iov[i].iov_len;\n\n\ti = 0;\n\twhile (remaining) {\n\t\t/*\n\t\t * If blocking send, we try 3 times, since each can block\n\t\t * for 5 seconds. For nonblocking  we have to try more\n\t\t * but wait increasing amounts of time allowing time for\n\t\t * socket to clear.  The overall time we wait in either\n\t\t * case to send on the socket is about 15 seconds.\n\t\t * Similarly we wait for 15 seconds for a response from\n\t\t * the server in SendReceive[2] for the server to send\n\t\t * a response back for most types of requests (except\n\t\t * SMB Write past end of file which can be slow, and\n\t\t * blocking lock operations). NFS waits slightly longer\n\t\t * than CIFS, but this can make it take longer for\n\t\t * nonresponsive servers to be detected and 15 seconds\n\t\t * is more than enough time for modern networks to\n\t\t * send a packet.  In most cases if we fail to send\n\t\t * after the retries we will kill the socket and\n\t\t * reconnect which may clear the network problem.\n\t\t */\n\t\trc = kernel_sendmsg(ssocket, &smb_msg, &iov[first_vec],\n\t\t\t\t    n_vec - first_vec, remaining);\n\t\tif (rc == -ENOSPC || rc == -EAGAIN) {\n\t\t\t/*\n\t\t\t * Catch if a low level driver returns -ENOSPC. This\n\t\t\t * WARN_ON will be removed by 3.10 if no one reports\n\t\t\t * seeing this.\n\t\t\t */\n\t\t\tWARN_ON_ONCE(rc == -ENOSPC);\n\t\t\ti++;\n\t\t\tif (i >= 14 || (!server->noblocksnd && (i > 2))) {\n\t\t\t\tcERROR(1, \"sends on sock %p stuck for 15 \"\n\t\t\t\t\t  \"seconds\", ssocket);\n\t\t\t\trc = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmsleep(1 << i);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (rc < 0)\n\t\t\tbreak;\n\n\t\t/* send was at least partially successful */\n\t\t*sent += rc;\n\n\t\tif (rc == remaining) {\n\t\t\tremaining = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (rc > remaining) {\n\t\t\tcERROR(1, \"sent %d requested %d\", rc, remaining);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (rc == 0) {\n\t\t\t/* should never happen, letting socket clear before\n\t\t\t   retrying is our only obvious option here */\n\t\t\tcERROR(1, \"tcp sent no data\");\n\t\t\tmsleep(500);\n\t\t\tcontinue;\n\t\t}\n\n\t\tremaining -= rc;\n\n\t\t/* the line below resets i */\n\t\tfor (i = first_vec; i < n_vec; i++) {\n\t\t\tif (iov[i].iov_len) {\n\t\t\t\tif (rc > iov[i].iov_len) {\n\t\t\t\t\trc -= iov[i].iov_len;\n\t\t\t\t\tiov[i].iov_len = 0;\n\t\t\t\t} else {\n\t\t\t\t\tiov[i].iov_base += rc;\n\t\t\t\t\tiov[i].iov_len -= rc;\n\t\t\t\t\tfirst_vec = i;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\ti = 0; /* in case we get ENOSPC on the next send */\n\t\trc = 0;\n\t}\n\treturn rc;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 366,
            "cve_id": "CVE-2013-7026",
            "code_snippet": "\nSYSCALL_DEFINE3(shmctl, int, shmid, int, cmd, struct shmid_ds __user *, buf)\n{\n\tstruct shmid_kernel *shp;\n\tint err, version;\n\tstruct ipc_namespace *ns;\n\n\tif (cmd < 0 || shmid < 0)\n\t\treturn -EINVAL;\n\n\tversion = ipc_parse_version(&cmd);\n\tns = current->nsproxy->ipc_ns;\n\n\tswitch (cmd) {\n\tcase IPC_INFO:\n\tcase SHM_INFO:\n\tcase SHM_STAT:\n\tcase IPC_STAT:\n\t\treturn shmctl_nolock(ns, shmid, cmd, version, buf);\n\tcase IPC_RMID:\n\tcase IPC_SET:\n\t\treturn shmctl_down(ns, shmid, cmd, buf, version);\n\tcase SHM_LOCK:\n\tcase SHM_UNLOCK:\n\t{\n\t\tstruct file *shm_file;\n\n\t\trcu_read_lock();\n\t\tshp = shm_obtain_object_check(ns, shmid);\n\t\tif (IS_ERR(shp)) {\n\t\t\terr = PTR_ERR(shp);\n\t\t\tgoto out_unlock1;\n\t\t}\n\n\t\taudit_ipc_obj(&(shp->shm_perm));\n\t\terr = security_shm_shmctl(shp, cmd);\n\t\tif (err)\n\t\t\tgoto out_unlock1;\n\n\t\tipc_lock_object(&shp->shm_perm);\n\t\tif (!ns_capable(ns->user_ns, CAP_IPC_LOCK)) {\n\t\t\tkuid_t euid = current_euid();\n\t\t\terr = -EPERM;\n\t\t\tif (!uid_eq(euid, shp->shm_perm.uid) &&\n\t\t\t    !uid_eq(euid, shp->shm_perm.cuid))\n\t\t\t\tgoto out_unlock0;\n\t\t\tif (cmd == SHM_LOCK && !rlimit(RLIMIT_MEMLOCK))\n\t\t\t\tgoto out_unlock0;\n\t\t}\n\n\t\tshm_file = shp->shm_file;\n\t\tif (is_file_hugepages(shm_file))\n\t\t\tgoto out_unlock0;\n\n\t\tif (cmd == SHM_LOCK) {\n\t\t\tstruct user_struct *user = current_user();\n\t\t\terr = shmem_lock(shm_file, 1, user);\n\t\t\tif (!err && !(shp->shm_perm.mode & SHM_LOCKED)) {\n\t\t\t\tshp->shm_perm.mode |= SHM_LOCKED;\n\t\t\t\tshp->mlock_user = user;\n\t\t\t}\n\t\t\tgoto out_unlock0;\n\t\t}\n\n\t\t/* SHM_UNLOCK */\n\t\tif (!(shp->shm_perm.mode & SHM_LOCKED))\n\t\t\tgoto out_unlock0;\n\t\tshmem_lock(shm_file, 0, shp->mlock_user);\n\t\tshp->shm_perm.mode &= ~SHM_LOCKED;\n\t\tshp->mlock_user = NULL;\n\t\tget_file(shm_file);\n\t\tipc_unlock_object(&shp->shm_perm);\n\t\trcu_read_unlock();\n\t\tshmem_unlock_mapping(shm_file->f_mapping);\n\n\t\tfput(shm_file);\n\t\treturn err;\n\t}\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\nout_unlock0:\n\tipc_unlock_object(&shp->shm_perm);\nout_unlock1:\n\trcu_read_unlock();\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 560,
            "cve_id": "CVE-2014-2706",
            "code_snippet": "void ieee80211_sta_ps_deliver_wakeup(struct sta_info *sta)\n{\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct sk_buff_head pending;\n\tint filtered = 0, buffered = 0, ac;\n\tunsigned long flags;\n\n\tclear_sta_flag(sta, WLAN_STA_SP);\n\n\tBUILD_BUG_ON(BITS_TO_LONGS(IEEE80211_NUM_TIDS) > 1);\n\tsta->driver_buffered_tids = 0;\n\n\tif (!(local->hw.flags & IEEE80211_HW_AP_LINK_PS))\n\t\tdrv_sta_notify(local, sdata, STA_NOTIFY_AWAKE, &sta->sta);\n\n\tskb_queue_head_init(&pending);\n\n\t/* Send all buffered frames to the station */\n\tfor (ac = 0; ac < IEEE80211_NUM_ACS; ac++) {\n\t\tint count = skb_queue_len(&pending), tmp;\n\n\t\tspin_lock_irqsave(&sta->tx_filtered[ac].lock, flags);\n\t\tskb_queue_splice_tail_init(&sta->tx_filtered[ac], &pending);\n\t\tspin_unlock_irqrestore(&sta->tx_filtered[ac].lock, flags);\n\t\ttmp = skb_queue_len(&pending);\n\t\tfiltered += tmp - count;\n\t\tcount = tmp;\n\n\t\tspin_lock_irqsave(&sta->ps_tx_buf[ac].lock, flags);\n\t\tskb_queue_splice_tail_init(&sta->ps_tx_buf[ac], &pending);\n\t\tspin_unlock_irqrestore(&sta->ps_tx_buf[ac].lock, flags);\n\t\ttmp = skb_queue_len(&pending);\n\t\tbuffered += tmp - count;\n\t}\n\n\tieee80211_add_pending_skbs_fn(local, &pending, clear_sta_ps_flags, sta);\n\n\t/* This station just woke up and isn't aware of our SMPS state */\n\tif (!ieee80211_smps_is_restrictive(sta->known_smps_mode,\n\t\t\t\t\t   sdata->smps_mode) &&\n\t    sta->known_smps_mode != sdata->bss->req_smps &&\n\t    sta_info_tx_streams(sta) != 1) {\n\t\tht_dbg(sdata,\n\t\t       \"%pM just woke up and MIMO capable - update SMPS\\n\",\n\t\t       sta->sta.addr);\n\t\tieee80211_send_smps_action(sdata, sdata->bss->req_smps,\n\t\t\t\t\t   sta->sta.addr,\n\t\t\t\t\t   sdata->vif.bss_conf.bssid);\n\t}\n\n\tlocal->total_ps_buffered -= buffered;\n\n\tsta_info_recalc_tim(sta);\n\n\tps_dbg(sdata,\n\t       \"STA %pM aid %d sending %d filtered/%d PS frames since STA not sleeping anymore\\n\",\n\t       sta->sta.addr, sta->sta.aid, filtered, buffered);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 629,
            "cve_id": "CVE-2014-4652",
            "code_snippet": "static int snd_ctl_elem_user_put(struct snd_kcontrol *kcontrol,\n\t\t\t\t struct snd_ctl_elem_value *ucontrol)\n{\n\tint change;\n\tstruct user_element *ue = kcontrol->private_data;\n\n\tchange = memcmp(&ucontrol->value, ue->elem_data, ue->elem_data_size) != 0;\n\tif (change)\n\t\tmemcpy(ue->elem_data, &ucontrol->value, ue->elem_data_size);\n\treturn change;\n}",
            "detect_result": "NO\n\nExplanation:\nThe function `snd_ctl_elem_user_put` appears to be part of the ALSA (Advanced Linux Sound Architecture) kernel module, specifically handling the update of user control elements. Here's a breakdown of its behavior:\n\n1. It takes two parameters: `kcontrol`, which is a pointer to a sound control structure, and `ucontrol`, which contains the new value for the control element.\n\n2. It retrieves a pointer to a `user_element` structure from the private data of `kcontrol`.\n\n3. It compares the new value (`ucontrol->value`) with the current stored value (`ue->elem_data`) using `memcmp`. If they are different, `change` is set to 1 (true); otherwise, it is set to 0 (false).\n\n4. If there is a change, it copies the new value into the stored location using `memcpy`.\n\n5. Finally, it returns the `change` flag, indicating whether an actual update occurred.\n\nFrom this analysis, there are no obvious vulnerabilities in the provided code snippet. The use of `memcmp` and `memcpy` with a size specified by `ue->elem_data_size` suggests that the code is attempting to handle memory safely. However, without more context about how `ue->elem_data_size` is determined and validated, it's impossible to rule out all potential issues. But based solely on the given code, there are no immediate signs of buffer overflows or other common vulnerabilities.",
            "final_result": 0
        },
        {
            "id": 756,
            "cve_id": "CVE-2014-9710",
            "code_snippet": "static struct btrfs_dir_item *btrfs_match_dir_item_name(struct btrfs_root *root,\n\t\t\t      struct btrfs_path *path,\n\t\t\t      const char *name, int name_len)\n{\n\tstruct btrfs_dir_item *dir_item;\n\tunsigned long name_ptr;\n\tu32 total_len;\n\tu32 cur = 0;\n\tu32 this_len;\n\tstruct extent_buffer *leaf;\n\n\tleaf = path->nodes[0];\n\tdir_item = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_dir_item);\n\tif (verify_dir_item(root, leaf, dir_item))\n\t\treturn NULL;\n\n\ttotal_len = btrfs_item_size_nr(leaf, path->slots[0]);\n\twhile (cur < total_len) {\n\t\tthis_len = sizeof(*dir_item) +\n\t\t\tbtrfs_dir_name_len(leaf, dir_item) +\n\t\t\tbtrfs_dir_data_len(leaf, dir_item);\n\t\tname_ptr = (unsigned long)(dir_item + 1);\n\n\t\tif (btrfs_dir_name_len(leaf, dir_item) == name_len &&\n\t\t    memcmp_extent_buffer(leaf, name, name_ptr, name_len) == 0)\n\t\t\treturn dir_item;\n\n\t\tcur += this_len;\n\t\tdir_item = (struct btrfs_dir_item *)((char *)dir_item +\n\t\t\t\t\t\t     this_len);\n\t}\n\treturn NULL;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 817,
            "cve_id": "CVE-2015-3212",
            "code_snippet": "static int sctp_init_sock(struct sock *sk)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct sctp_sock *sp;\n\n\tpr_debug(\"%s: sk:%p\\n\", __func__, sk);\n\n\tsp = sctp_sk(sk);\n\n\t/* Initialize the SCTP per socket area.  */\n\tswitch (sk->sk_type) {\n\tcase SOCK_SEQPACKET:\n\t\tsp->type = SCTP_SOCKET_UDP;\n\t\tbreak;\n\tcase SOCK_STREAM:\n\t\tsp->type = SCTP_SOCKET_TCP;\n\t\tbreak;\n\tdefault:\n\t\treturn -ESOCKTNOSUPPORT;\n\t}\n\n\t/* Initialize default send parameters. These parameters can be\n\t * modified with the SCTP_DEFAULT_SEND_PARAM socket option.\n\t */\n\tsp->default_stream = 0;\n\tsp->default_ppid = 0;\n\tsp->default_flags = 0;\n\tsp->default_context = 0;\n\tsp->default_timetolive = 0;\n\n\tsp->default_rcv_context = 0;\n\tsp->max_burst = net->sctp.max_burst;\n\n\tsp->sctp_hmac_alg = net->sctp.sctp_hmac_alg;\n\n\t/* Initialize default setup parameters. These parameters\n\t * can be modified with the SCTP_INITMSG socket option or\n\t * overridden by the SCTP_INIT CMSG.\n\t */\n\tsp->initmsg.sinit_num_ostreams   = sctp_max_outstreams;\n\tsp->initmsg.sinit_max_instreams  = sctp_max_instreams;\n\tsp->initmsg.sinit_max_attempts   = net->sctp.max_retrans_init;\n\tsp->initmsg.sinit_max_init_timeo = net->sctp.rto_max;\n\n\t/* Initialize default RTO related parameters.  These parameters can\n\t * be modified for with the SCTP_RTOINFO socket option.\n\t */\n\tsp->rtoinfo.srto_initial = net->sctp.rto_initial;\n\tsp->rtoinfo.srto_max     = net->sctp.rto_max;\n\tsp->rtoinfo.srto_min     = net->sctp.rto_min;\n\n\t/* Initialize default association related parameters. These parameters\n\t * can be modified with the SCTP_ASSOCINFO socket option.\n\t */\n\tsp->assocparams.sasoc_asocmaxrxt = net->sctp.max_retrans_association;\n\tsp->assocparams.sasoc_number_peer_destinations = 0;\n\tsp->assocparams.sasoc_peer_rwnd = 0;\n\tsp->assocparams.sasoc_local_rwnd = 0;\n\tsp->assocparams.sasoc_cookie_life = net->sctp.valid_cookie_life;\n\n\t/* Initialize default event subscriptions. By default, all the\n\t * options are off.\n\t */\n\tmemset(&sp->subscribe, 0, sizeof(struct sctp_event_subscribe));\n\n\t/* Default Peer Address Parameters.  These defaults can\n\t * be modified via SCTP_PEER_ADDR_PARAMS\n\t */\n\tsp->hbinterval  = net->sctp.hb_interval;\n\tsp->pathmaxrxt  = net->sctp.max_retrans_path;\n\tsp->pathmtu     = 0; /* allow default discovery */\n\tsp->sackdelay   = net->sctp.sack_timeout;\n\tsp->sackfreq\t= 2;\n\tsp->param_flags = SPP_HB_ENABLE |\n\t\t\t  SPP_PMTUD_ENABLE |\n\t\t\t  SPP_SACKDELAY_ENABLE;\n\n\t/* If enabled no SCTP message fragmentation will be performed.\n\t * Configure through SCTP_DISABLE_FRAGMENTS socket option.\n\t */\n\tsp->disable_fragments = 0;\n\n\t/* Enable Nagle algorithm by default.  */\n\tsp->nodelay           = 0;\n\n\tsp->recvrcvinfo = 0;\n\tsp->recvnxtinfo = 0;\n\n\t/* Enable by default. */\n\tsp->v4mapped          = 1;\n\n\t/* Auto-close idle associations after the configured\n\t * number of seconds.  A value of 0 disables this\n\t * feature.  Configure through the SCTP_AUTOCLOSE socket option,\n\t * for UDP-style sockets only.\n\t */\n\tsp->autoclose         = 0;\n\n\t/* User specified fragmentation limit. */\n\tsp->user_frag         = 0;\n\n\tsp->adaptation_ind = 0;\n\n\tsp->pf = sctp_get_pf_specific(sk->sk_family);\n\n\t/* Control variables for partial data delivery. */\n\tatomic_set(&sp->pd_mode, 0);\n\tskb_queue_head_init(&sp->pd_lobby);\n\tsp->frag_interleave = 0;\n\n\t/* Create a per socket endpoint structure.  Even if we\n\t * change the data structure relationships, this may still\n\t * be useful for storing pre-connect address information.\n\t */\n\tsp->ep = sctp_endpoint_new(sk, GFP_KERNEL);\n\tif (!sp->ep)\n\t\treturn -ENOMEM;\n\n\tsp->hmac = NULL;\n\n\tsk->sk_destruct = sctp_destruct_sock;\n\n\tSCTP_DBG_OBJCNT_INC(sock);\n\n\tlocal_bh_disable();\n\tpercpu_counter_inc(&sctp_sockets_allocated);\n\tsock_prot_inuse_add(net, sk->sk_prot, 1);\n\tif (net->sctp.default_auto_asconf) {\n\t\tlist_add_tail(&sp->auto_asconf_list,\n\t\t    &net->sctp.auto_asconf_splist);\n\t\tsp->do_auto_asconf = 1;\n\t} else\n\t\tsp->do_auto_asconf = 0;\n\tlocal_bh_enable();\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 856,
            "cve_id": "CVE-2015-7613",
            "code_snippet": "static int newseg(struct ipc_namespace *ns, struct ipc_params *params)\n{\n\tkey_t key = params->key;\n\tint shmflg = params->flg;\n\tsize_t size = params->u.size;\n\tint error;\n\tstruct shmid_kernel *shp;\n\tsize_t numpages = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tstruct file *file;\n\tchar name[13];\n\tint id;\n\tvm_flags_t acctflag = 0;\n\n\tif (size < SHMMIN || size > ns->shm_ctlmax)\n\t\treturn -EINVAL;\n\n\tif (numpages << PAGE_SHIFT < size)\n\t\treturn -ENOSPC;\n\n\tif (ns->shm_tot + numpages < ns->shm_tot ||\n\t\t\tns->shm_tot + numpages > ns->shm_ctlall)\n\t\treturn -ENOSPC;\n\n\tshp = ipc_rcu_alloc(sizeof(*shp));\n\tif (!shp)\n\t\treturn -ENOMEM;\n\n\tshp->shm_perm.key = key;\n\tshp->shm_perm.mode = (shmflg & S_IRWXUGO);\n\tshp->mlock_user = NULL;\n\n\tshp->shm_perm.security = NULL;\n\terror = security_shm_alloc(shp);\n\tif (error) {\n\t\tipc_rcu_putref(shp, ipc_rcu_free);\n\t\treturn error;\n\t}\n\n\tsprintf(name, \"SYSV%08x\", key);\n\tif (shmflg & SHM_HUGETLB) {\n\t\tstruct hstate *hs;\n\t\tsize_t hugesize;\n\n\t\ths = hstate_sizelog((shmflg >> SHM_HUGE_SHIFT) & SHM_HUGE_MASK);\n\t\tif (!hs) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto no_file;\n\t\t}\n\t\thugesize = ALIGN(size, huge_page_size(hs));\n\n\t\t/* hugetlb_file_setup applies strict accounting */\n\t\tif (shmflg & SHM_NORESERVE)\n\t\t\tacctflag = VM_NORESERVE;\n\t\tfile = hugetlb_file_setup(name, hugesize, acctflag,\n\t\t\t\t  &shp->mlock_user, HUGETLB_SHMFS_INODE,\n\t\t\t\t(shmflg >> SHM_HUGE_SHIFT) & SHM_HUGE_MASK);\n\t} else {\n\t\t/*\n\t\t * Do not allow no accounting for OVERCOMMIT_NEVER, even\n\t\t * if it's asked for.\n\t\t */\n\t\tif  ((shmflg & SHM_NORESERVE) &&\n\t\t\t\tsysctl_overcommit_memory != OVERCOMMIT_NEVER)\n\t\t\tacctflag = VM_NORESERVE;\n\t\tfile = shmem_kernel_file_setup(name, size, acctflag);\n\t}\n\terror = PTR_ERR(file);\n\tif (IS_ERR(file))\n\t\tgoto no_file;\n\n\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n\tif (id < 0) {\n\t\terror = id;\n\t\tgoto no_id;\n\t}\n\n\tshp->shm_cprid = task_tgid_vnr(current);\n\tshp->shm_lprid = 0;\n\tshp->shm_atim = shp->shm_dtim = 0;\n\tshp->shm_ctim = get_seconds();\n\tshp->shm_segsz = size;\n\tshp->shm_nattch = 0;\n\tshp->shm_file = file;\n\tshp->shm_creator = current;\n\tlist_add(&shp->shm_clist, &current->sysvshm.shm_clist);\n\n\t/*\n\t * shmid gets reported as \"inode#\" in /proc/pid/maps.\n\t * proc-ps tools use this. Changing this will break them.\n\t */\n\tfile_inode(file)->i_ino = shp->shm_perm.id;\n\n\tns->shm_tot += numpages;\n\terror = shp->shm_perm.id;\n\n\tipc_unlock_object(&shp->shm_perm);\n\trcu_read_unlock();\n\treturn error;\n\nno_id:\n\tif (is_file_hugepages(file) && shp->mlock_user)\n\t\tuser_shm_unlock(size, shp->mlock_user);\n\tfput(file);\nno_file:\n\tipc_rcu_putref(shp, shm_rcu_free);\n\treturn error;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 864,
            "cve_id": "CVE-2015-7990",
            "code_snippet": "int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct rds_sock *rs = rds_sk_to_rs(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);\n\t__be32 daddr;\n\t__be16 dport;\n\tstruct rds_message *rm = NULL;\n\tstruct rds_connection *conn;\n\tint ret = 0;\n\tint queued = 0, allocated_mr = 0;\n\tint nonblock = msg->msg_flags & MSG_DONTWAIT;\n\tlong timeo = sock_sndtimeo(sk, nonblock);\n\n\t/* Mirror Linux UDP mirror of BSD error message compatibility */\n\t/* XXX: Perhaps MSG_MORE someday */\n\tif (msg->msg_flags & ~(MSG_DONTWAIT | MSG_CMSG_COMPAT)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tif (msg->msg_namelen) {\n\t\t/* XXX fail non-unicast destination IPs? */\n\t\tif (msg->msg_namelen < sizeof(*usin) || usin->sin_family != AF_INET) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\tdport = usin->sin_port;\n\t} else {\n\t\t/* We only care about consistency with ->connect() */\n\t\tlock_sock(sk);\n\t\tdaddr = rs->rs_conn_addr;\n\t\tdport = rs->rs_conn_port;\n\t\trelease_sock(sk);\n\t}\n\n\t/* racing with another thread binding seems ok here */\n\tif (daddr == 0 || rs->rs_bound_addr == 0) {\n\t\tret = -ENOTCONN; /* XXX not a great errno */\n\t\tgoto out;\n\t}\n\n\tif (payload_len > rds_sk_sndbuf(rs)) {\n\t\tret = -EMSGSIZE;\n\t\tgoto out;\n\t}\n\n\t/* size of rm including all sgs */\n\tret = rds_rm_size(msg, payload_len);\n\tif (ret < 0)\n\t\tgoto out;\n\n\trm = rds_message_alloc(ret, GFP_KERNEL);\n\tif (!rm) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t/* Attach data to the rm */\n\tif (payload_len) {\n\t\trm->data.op_sg = rds_message_alloc_sgs(rm, ceil(payload_len, PAGE_SIZE));\n\t\tif (!rm->data.op_sg) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tret = rds_message_copy_from_user(rm, &msg->msg_iter);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\trm->data.op_active = 1;\n\n\trm->m_daddr = daddr;\n\n\t/* rds_conn_create has a spinlock that runs with IRQ off.\n\t * Caching the conn in the socket helps a lot. */\n\tif (rs->rs_conn && rs->rs_conn->c_faddr == daddr)\n\t\tconn = rs->rs_conn;\n\telse {\n\t\tconn = rds_conn_create_outgoing(sock_net(sock->sk),\n\t\t\t\t\t\trs->rs_bound_addr, daddr,\n\t\t\t\t\trs->rs_transport,\n\t\t\t\t\tsock->sk->sk_allocation);\n\t\tif (IS_ERR(conn)) {\n\t\t\tret = PTR_ERR(conn);\n\t\t\tgoto out;\n\t\t}\n\t\trs->rs_conn = conn;\n\t}\n\n\t/* Parse any control messages the user may have included. */\n\tret = rds_cmsg_send(rs, rm, msg, &allocated_mr);\n\tif (ret)\n\t\tgoto out;\n\n\tif (rm->rdma.op_active && !conn->c_trans->xmit_rdma) {\n\t\tprintk_ratelimited(KERN_NOTICE \"rdma_op %p conn xmit_rdma %p\\n\",\n\t\t\t       &rm->rdma, conn->c_trans->xmit_rdma);\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tif (rm->atomic.op_active && !conn->c_trans->xmit_atomic) {\n\t\tprintk_ratelimited(KERN_NOTICE \"atomic_op %p conn xmit_atomic %p\\n\",\n\t\t\t       &rm->atomic, conn->c_trans->xmit_atomic);\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\trds_conn_connect_if_down(conn);\n\n\tret = rds_cong_wait(conn->c_fcong, dport, nonblock, rs);\n\tif (ret) {\n\t\trs->rs_seen_congestion = 1;\n\t\tgoto out;\n\t}\n\n\twhile (!rds_send_queue_rm(rs, conn, rm, rs->rs_bound_port,\n\t\t\t\t  dport, &queued)) {\n\t\trds_stats_inc(s_send_queue_full);\n\n\t\tif (nonblock) {\n\t\t\tret = -EAGAIN;\n\t\t\tgoto out;\n\t\t}\n\n\t\ttimeo = wait_event_interruptible_timeout(*sk_sleep(sk),\n\t\t\t\t\trds_send_queue_rm(rs, conn, rm,\n\t\t\t\t\t\t\t  rs->rs_bound_port,\n\t\t\t\t\t\t\t  dport,\n\t\t\t\t\t\t\t  &queued),\n\t\t\t\t\ttimeo);\n\t\trdsdebug(\"sendmsg woke queued %d timeo %ld\\n\", queued, timeo);\n\t\tif (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)\n\t\t\tcontinue;\n\n\t\tret = timeo;\n\t\tif (ret == 0)\n\t\t\tret = -ETIMEDOUT;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * By now we've committed to the send.  We reuse rds_send_worker()\n\t * to retry sends in the rds thread if the transport asks us to.\n\t */\n\trds_stats_inc(s_send_queued);\n\n\tret = rds_send_xmit(conn);\n\tif (ret == -ENOMEM || ret == -EAGAIN)\n\t\tqueue_delayed_work(rds_wq, &conn->c_send_w, 1);\n\n\trds_message_put(rm);\n\treturn payload_len;\n\nout:\n\t/* If the user included a RDMA_MAP cmsg, we allocated a MR on the fly.\n\t * If the sendmsg goes through, we keep the MR. If it fails with EAGAIN\n\t * or in any other way, we need to destroy the MR again */\n\tif (allocated_mr)\n\t\trds_rdma_unuse(rs, rds_rdma_cookie_key(rm->m_rdma_cookie), 1);\n\n\tif (rm)\n\t\trds_message_put(rm);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 908,
            "cve_id": "CVE-2015-8767",
            "code_snippet": "void sctp_generate_proto_unreach_event(unsigned long data)\n{\n\tstruct sctp_transport *transport = (struct sctp_transport *) data;\n\tstruct sctp_association *asoc = transport->asoc;\n\tstruct net *net = sock_net(asoc->base.sk);\n\n\tbh_lock_sock(asoc->base.sk);\n\tif (sock_owned_by_user(asoc->base.sk)) {\n\t\tpr_debug(\"%s: sock is busy\\n\", __func__);\n\n\t\t/* Try again later.  */\n\t\tif (!mod_timer(&transport->proto_unreach_timer,\n\t\t\t\tjiffies + (HZ/20)))\n\t\t\tsctp_association_hold(asoc);\n\t\tgoto out_unlock;\n\t}\n\n\t/* Is this structure just waiting around for us to actually\n\t * get destroyed?\n\t */\n\tif (asoc->base.dead)\n\t\tgoto out_unlock;\n\n\tsctp_do_sm(net, SCTP_EVENT_T_OTHER,\n\t\t   SCTP_ST_OTHER(SCTP_EVENT_ICMP_PROTO_UNREACH),\n\t\t   asoc->state, asoc->ep, asoc, transport, GFP_ATOMIC);\n\nout_unlock:\n\tbh_unlock_sock(asoc->base.sk);\n\tsctp_association_put(asoc);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 918,
            "cve_id": "CVE-2015-8839",
            "code_snippet": "int ext4_collapse_range(struct inode *inode, loff_t offset, loff_t len)\n{\n\tstruct super_block *sb = inode->i_sb;\n\text4_lblk_t punch_start, punch_stop;\n\thandle_t *handle;\n\tunsigned int credits;\n\tloff_t new_size, ioffset;\n\tint ret;\n\n\t/*\n\t * We need to test this early because xfstests assumes that a\n\t * collapse range of (0, 1) will return EOPNOTSUPP if the file\n\t * system does not support collapse range.\n\t */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\treturn -EOPNOTSUPP;\n\n\t/* Collapse range works only on fs block size aligned offsets. */\n\tif (offset & (EXT4_CLUSTER_SIZE(sb) - 1) ||\n\t    len & (EXT4_CLUSTER_SIZE(sb) - 1))\n\t\treturn -EINVAL;\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EINVAL;\n\n\ttrace_ext4_collapse_range(inode, offset, len);\n\n\tpunch_start = offset >> EXT4_BLOCK_SIZE_BITS(sb);\n\tpunch_stop = (offset + len) >> EXT4_BLOCK_SIZE_BITS(sb);\n\n\t/* Call ext4_force_commit to flush all data in case of data=journal. */\n\tif (ext4_should_journal_data(inode)) {\n\t\tret = ext4_force_commit(inode->i_sb);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Need to round down offset to be aligned with page size boundary\n\t * for page size > block size.\n\t */\n\tioffset = round_down(offset, PAGE_SIZE);\n\n\t/* Write out all dirty pages */\n\tret = filemap_write_and_wait_range(inode->i_mapping, ioffset,\n\t\t\t\t\t   LLONG_MAX);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Take mutex lock */\n\tmutex_lock(&inode->i_mutex);\n\n\t/*\n\t * There is no need to overlap collapse range with EOF, in which case\n\t * it is effectively a truncate operation\n\t */\n\tif (offset + len >= i_size_read(inode)) {\n\t\tret = -EINVAL;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Currently just for extent based files */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_mutex;\n\t}\n\n\ttruncate_pagecache(inode, ioffset);\n\n\t/* Wait for existing dio to complete */\n\text4_inode_block_unlocked_dio(inode);\n\tinode_dio_wait(inode);\n\n\tcredits = ext4_writepage_trans_blocks(inode);\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\tgoto out_dio;\n\t}\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_discard_preallocations(inode);\n\n\tret = ext4_es_remove_extent(inode, punch_start,\n\t\t\t\t    EXT_MAX_BLOCKS - punch_start);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\tret = ext4_ext_remove_space(inode, punch_start, punch_stop - 1);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\text4_discard_preallocations(inode);\n\n\tret = ext4_ext_shift_extents(inode, handle, punch_stop,\n\t\t\t\t     punch_stop - punch_start, SHIFT_LEFT);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\tnew_size = i_size_read(inode) - len;\n\ti_size_write(inode, new_size);\n\tEXT4_I(inode)->i_disksize = new_size;\n\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\nout_stop:\n\text4_journal_stop(handle);\nout_dio:\n\text4_inode_resume_unlocked_dio(inode);\nout_mutex:\n\tmutex_unlock(&inode->i_mutex);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 919,
            "cve_id": "CVE-2015-8839",
            "code_snippet": "int ext4_insert_range(struct inode *inode, loff_t offset, loff_t len)\n{\n\tstruct super_block *sb = inode->i_sb;\n\thandle_t *handle;\n\tstruct ext4_ext_path *path;\n\tstruct ext4_extent *extent;\n\text4_lblk_t offset_lblk, len_lblk, ee_start_lblk = 0;\n\tunsigned int credits, ee_len;\n\tint ret = 0, depth, split_flag = 0;\n\tloff_t ioffset;\n\n\t/*\n\t * We need to test this early because xfstests assumes that an\n\t * insert range of (0, 1) will return EOPNOTSUPP if the file\n\t * system does not support insert range.\n\t */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\treturn -EOPNOTSUPP;\n\n\t/* Insert range works only on fs block size aligned offsets. */\n\tif (offset & (EXT4_CLUSTER_SIZE(sb) - 1) ||\n\t\t\tlen & (EXT4_CLUSTER_SIZE(sb) - 1))\n\t\treturn -EINVAL;\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EOPNOTSUPP;\n\n\ttrace_ext4_insert_range(inode, offset, len);\n\n\toffset_lblk = offset >> EXT4_BLOCK_SIZE_BITS(sb);\n\tlen_lblk = len >> EXT4_BLOCK_SIZE_BITS(sb);\n\n\t/* Call ext4_force_commit to flush all data in case of data=journal */\n\tif (ext4_should_journal_data(inode)) {\n\t\tret = ext4_force_commit(inode->i_sb);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Need to round down to align start offset to page size boundary\n\t * for page size > block size.\n\t */\n\tioffset = round_down(offset, PAGE_SIZE);\n\n\t/* Write out all dirty pages */\n\tret = filemap_write_and_wait_range(inode->i_mapping, ioffset,\n\t\t\tLLONG_MAX);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Take mutex lock */\n\tmutex_lock(&inode->i_mutex);\n\n\t/* Currently just for extent based files */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Check for wrap through zero */\n\tif (inode->i_size + len > inode->i_sb->s_maxbytes) {\n\t\tret = -EFBIG;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Offset should be less than i_size */\n\tif (offset >= i_size_read(inode)) {\n\t\tret = -EINVAL;\n\t\tgoto out_mutex;\n\t}\n\n\ttruncate_pagecache(inode, ioffset);\n\n\t/* Wait for existing dio to complete */\n\text4_inode_block_unlocked_dio(inode);\n\tinode_dio_wait(inode);\n\n\tcredits = ext4_writepage_trans_blocks(inode);\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\tgoto out_dio;\n\t}\n\n\t/* Expand file to avoid data loss if there is error while shifting */\n\tinode->i_size += len;\n\tEXT4_I(inode)->i_disksize += len;\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\tret = ext4_mark_inode_dirty(handle, inode);\n\tif (ret)\n\t\tgoto out_stop;\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_discard_preallocations(inode);\n\n\tpath = ext4_find_extent(inode, offset_lblk, NULL, 0);\n\tif (IS_ERR(path)) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\tdepth = ext_depth(inode);\n\textent = path[depth].p_ext;\n\tif (extent) {\n\t\tee_start_lblk = le32_to_cpu(extent->ee_block);\n\t\tee_len = ext4_ext_get_actual_len(extent);\n\n\t\t/*\n\t\t * If offset_lblk is not the starting block of extent, split\n\t\t * the extent @offset_lblk\n\t\t */\n\t\tif ((offset_lblk > ee_start_lblk) &&\n\t\t\t\t(offset_lblk < (ee_start_lblk + ee_len))) {\n\t\t\tif (ext4_ext_is_unwritten(extent))\n\t\t\t\tsplit_flag = EXT4_EXT_MARK_UNWRIT1 |\n\t\t\t\t\tEXT4_EXT_MARK_UNWRIT2;\n\t\t\tret = ext4_split_extent_at(handle, inode, &path,\n\t\t\t\t\toffset_lblk, split_flag,\n\t\t\t\t\tEXT4_EX_NOCACHE |\n\t\t\t\t\tEXT4_GET_BLOCKS_PRE_IO |\n\t\t\t\t\tEXT4_GET_BLOCKS_METADATA_NOFAIL);\n\t\t}\n\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t\tif (ret < 0) {\n\t\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\t\tgoto out_stop;\n\t\t}\n\t}\n\n\tret = ext4_es_remove_extent(inode, offset_lblk,\n\t\t\tEXT_MAX_BLOCKS - offset_lblk);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\t/*\n\t * if offset_lblk lies in a hole which is at start of file, use\n\t * ee_start_lblk to shift extents\n\t */\n\tret = ext4_ext_shift_extents(inode, handle,\n\t\tee_start_lblk > offset_lblk ? ee_start_lblk : offset_lblk,\n\t\tlen_lblk, SHIFT_RIGHT);\n\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\nout_stop:\n\text4_journal_stop(handle);\nout_dio:\n\text4_inode_resume_unlocked_dio(inode);\nout_mutex:\n\tmutex_unlock(&inode->i_mutex);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 920,
            "cve_id": "CVE-2015-8839",
            "code_snippet": "static int ext4_dax_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tint result;\n\thandle_t *handle = NULL;\n\tstruct super_block *sb = file_inode(vma->vm_file)->i_sb;\n\tbool write = vmf->flags & FAULT_FLAG_WRITE;\n\n\tif (write) {\n\t\tsb_start_pagefault(sb);\n\t\tfile_update_time(vma->vm_file);\n\t\thandle = ext4_journal_start_sb(sb, EXT4_HT_WRITE_PAGE,\n\t\t\t\t\t\tEXT4_DATA_TRANS_BLOCKS(sb));\n\t}\n\n\tif (IS_ERR(handle))\n\t\tresult = VM_FAULT_SIGBUS;\n\telse\n\t\tresult = __dax_fault(vma, vmf, ext4_get_block_dax,\n\t\t\t\t\t\text4_end_io_unwritten);\n\n\tif (write) {\n\t\tif (!IS_ERR(handle))\n\t\t\text4_journal_stop(handle);\n\t\tsb_end_pagefault(sb);\n\t}\n\n\treturn result;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 921,
            "cve_id": "CVE-2015-8839",
            "code_snippet": "static int ext4_dax_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\treturn dax_mkwrite(vma, vmf, ext4_get_block_dax,\n\t\t\t\text4_end_io_unwritten);\n}",
            "detect_result": "NO\n\nThe provided code snippet is a function `ext4_dax_mkwrite` that appears to be part of the Linux kernel's ext4 filesystem implementation, specifically dealing with Direct Access (DAX) writes. The function calls another function `dax_mkwrite` with parameters including the virtual memory area structure (`vma`), the virtual memory fault structure (`vmf`), and two callback functions: `ext4_get_block_dax` and `ext4_end_io_unwritten`.\n\nBased on the given code alone, there are no obvious vulnerabilities such as buffer overflows, use-after-free, or improper input validation. However, it's important to note that the security of this function would also depend on the implementations of the callback functions `ext4_get_block_dax` and `ext4_end_io_unwritten`, as well as the context in which `ext4_dax_mkwrite` is called. Without further details about these components, we cannot definitively state that there are no vulnerabilities, but based solely on the provided code, the answer is NO.",
            "final_result": 0
        },
        {
            "id": 922,
            "cve_id": "CVE-2015-8839",
            "code_snippet": "static int ext4_dax_pmd_fault(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t\t\t\tpmd_t *pmd, unsigned int flags)\n{\n\tint result;\n\thandle_t *handle = NULL;\n\tstruct inode *inode = file_inode(vma->vm_file);\n\tstruct super_block *sb = inode->i_sb;\n\tbool write = flags & FAULT_FLAG_WRITE;\n\n\tif (write) {\n\t\tsb_start_pagefault(sb);\n\t\tfile_update_time(vma->vm_file);\n\t\thandle = ext4_journal_start_sb(sb, EXT4_HT_WRITE_PAGE,\n\t\t\t\text4_chunk_trans_blocks(inode,\n\t\t\t\t\t\t\tPMD_SIZE / PAGE_SIZE));\n\t}\n\n\tif (IS_ERR(handle))\n\t\tresult = VM_FAULT_SIGBUS;\n\telse\n\t\tresult = __dax_pmd_fault(vma, addr, pmd, flags,\n\t\t\t\text4_get_block_dax, ext4_end_io_unwritten);\n\n\tif (write) {\n\t\tif (!IS_ERR(handle))\n\t\t\text4_journal_stop(handle);\n\t\tsb_end_pagefault(sb);\n\t}\n\n\treturn result;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 939,
            "cve_id": "CVE-2015-8963",
            "code_snippet": "static void perf_event_init_cpu(int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\n\tmutex_lock(&swhash->hlist_mutex);\n\tswhash->online = true;\n\tif (swhash->hlist_refcount > 0) {\n\t\tstruct swevent_hlist *hlist;\n\n\t\thlist = kzalloc_node(sizeof(*hlist), GFP_KERNEL, cpu_to_node(cpu));\n\t\tWARN_ON(!hlist);\n\t\trcu_assign_pointer(swhash->swevent_hlist, hlist);\n\t}\n\tmutex_unlock(&swhash->hlist_mutex);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 949,
            "cve_id": "CVE-2015-9016",
            "code_snippet": "static bool blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq)\n{\n\tstruct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];\n\tstruct request *first_rq =\n\t\tlist_first_entry(pending, struct request, flush.list);\n\tstruct request *flush_rq = fq->flush_rq;\n\n\t/* C1 described at the top of this file */\n\tif (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))\n\t\treturn false;\n\n\t/* C2 and C3 */\n\tif (!list_empty(&fq->flush_data_in_flight) &&\n\t    time_before(jiffies,\n\t\t\tfq->flush_pending_since + FLUSH_PENDING_TIMEOUT))\n\t\treturn false;\n\n\t/*\n\t * Issue flush and toggle pending_idx.  This makes pending_idx\n\t * different from running_idx, which means flush is in flight.\n\t */\n\tfq->flush_pending_idx ^= 1;\n\n\tblk_rq_init(q, flush_rq);\n\n\t/*\n\t * Borrow tag from the first request since they can't\n\t * be in flight at the same time.\n\t */\n\tif (q->mq_ops) {\n\t\tflush_rq->mq_ctx = first_rq->mq_ctx;\n\t\tflush_rq->tag = first_rq->tag;\n\t}\n\n\tflush_rq->cmd_type = REQ_TYPE_FS;\n\tflush_rq->cmd_flags = WRITE_FLUSH | REQ_FLUSH_SEQ;\n\tflush_rq->rq_disk = first_rq->rq_disk;\n\tflush_rq->end_io = flush_end_io;\n\n\treturn blk_flush_queue_rq(flush_rq, false);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 950,
            "cve_id": "CVE-2015-9016",
            "code_snippet": "static void bt_for_each(struct blk_mq_hw_ctx *hctx,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t     \trq = blk_mq_tag_to_rq(hctx->tags, off + bit);\n\t\t\tif (rq->q == hctx->queue)\n\t\t\t\tfn(hctx, rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}",
            "detect_result": "NO\n\nExplanation of the code behavior:\nThe function `bt_for_each` iterates over a set of request queues managed by a block device queueing mechanism (`blk_mq`). It uses a bitmap to track which requests are currently busy. The function goes through each word in the bitmap (`bt->map`), finds the first and subsequent bits that are set (indicating a busy request), converts these bits to request pointers using `blk_mq_tag_to_rq`, and then calls a user-provided function `fn` for each busy request.\n\nVulnerability assessment:\nThe provided code snippet does not show any obvious vulnerabilities such as buffer overflows, use-after-free, or improper input validation. However, the safety of this function heavily depends on the correctness and safety of the functions it calls, particularly `blk_mq_tag_to_rq`. If `blk_mq_tag_to_rq` can return an invalid pointer or if `rq->q` can be manipulated to point to an incorrect queue, there could be issues. But based solely on the given code, there are no direct vulnerabilities present.",
            "final_result": 0
        },
        {
            "id": 966,
            "cve_id": "CVE-2016-10200",
            "code_snippet": "static int l2tp_ip6_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_l2tpip6 *addr = (struct sockaddr_l2tpip6 *) uaddr;\n\tstruct net *net = sock_net(sk);\n\t__be32 v4addr = 0;\n\tint addr_type;\n\tint err;\n\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\treturn -EINVAL;\n\tif (addr->l2tp_family != AF_INET6)\n\t\treturn -EINVAL;\n\tif (addr_len < sizeof(*addr))\n\t\treturn -EINVAL;\n\n\taddr_type = ipv6_addr_type(&addr->l2tp_addr);\n\n\t/* l2tp_ip6 sockets are IPv6 only */\n\tif (addr_type == IPV6_ADDR_MAPPED)\n\t\treturn -EADDRNOTAVAIL;\n\n\t/* L2TP is point-point, not multicast */\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -EADDRNOTAVAIL;\n\n\terr = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip6_lock);\n\tif (__l2tp_ip6_bind_lookup(net, &addr->l2tp_addr,\n\t\t\t\t   sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\tread_unlock_bh(&l2tp_ip6_lock);\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_CLOSE)\n\t\tgoto out_unlock;\n\n\t/* Check if the address belongs to the host. */\n\trcu_read_lock();\n\tif (addr_type != IPV6_ADDR_ANY) {\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t\t    addr->l2tp_scope_id) {\n\t\t\t\t/* Override any existing binding, if another\n\t\t\t\t * one is supplied by user.\n\t\t\t\t */\n\t\t\t\tsk->sk_bound_dev_if = addr->l2tp_scope_id;\n\t\t\t}\n\n\t\t\t/* Binding to link-local address requires an\n\t\t\t   interface */\n\t\t\tif (!sk->sk_bound_dev_if)\n\t\t\t\tgoto out_unlock_rcu;\n\n\t\t\terr = -ENODEV;\n\t\t\tdev = dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\t\tif (!dev)\n\t\t\t\tgoto out_unlock_rcu;\n\t\t}\n\n\t\t/* ipv4 addr of the socket is invalid.  Only the\n\t\t * unspecified and mapped address have a v4 equivalent.\n\t\t */\n\t\tv4addr = LOOPBACK4_IPV6;\n\t\terr = -EADDRNOTAVAIL;\n\t\tif (!ipv6_chk_addr(sock_net(sk), &addr->l2tp_addr, dev, 0))\n\t\t\tgoto out_unlock_rcu;\n\t}\n\trcu_read_unlock();\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = v4addr;\n\tsk->sk_v6_rcv_saddr = addr->l2tp_addr;\n\tnp->saddr = addr->l2tp_addr;\n\n\tl2tp_ip6_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip6_lock);\n\tsk_add_bind_node(sk, &l2tp_ip6_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip6_lock);\n\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\trelease_sock(sk);\n\treturn 0;\n\nout_unlock_rcu:\n\trcu_read_unlock();\nout_unlock:\n\trelease_sock(sk);\n\treturn err;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip6_lock);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 977,
            "cve_id": "CVE-2016-10906",
            "code_snippet": "static void arc_emac_tx_clean(struct net_device *ndev)\n{\n\tstruct arc_emac_priv *priv = netdev_priv(ndev);\n\tstruct net_device_stats *stats = &ndev->stats;\n\tunsigned int i;\n\n\tfor (i = 0; i < TX_BD_NUM; i++) {\n\t\tunsigned int *txbd_dirty = &priv->txbd_dirty;\n\t\tstruct arc_emac_bd *txbd = &priv->txbd[*txbd_dirty];\n\t\tstruct buffer_state *tx_buff = &priv->tx_buff[*txbd_dirty];\n\t\tstruct sk_buff *skb = tx_buff->skb;\n\t\tunsigned int info = le32_to_cpu(txbd->info);\n\n\t\tif ((info & FOR_EMAC) || !txbd->data)\n\t\t\tbreak;\n\n\t\tif (unlikely(info & (DROP | DEFR | LTCL | UFLO))) {\n\t\t\tstats->tx_errors++;\n\t\t\tstats->tx_dropped++;\n\n\t\t\tif (info & DEFR)\n\t\t\t\tstats->tx_carrier_errors++;\n\n\t\t\tif (info & LTCL)\n\t\t\t\tstats->collisions++;\n\n\t\t\tif (info & UFLO)\n\t\t\t\tstats->tx_fifo_errors++;\n\t\t} else if (likely(info & FIRST_OR_LAST_MASK)) {\n\t\t\tstats->tx_packets++;\n\t\t\tstats->tx_bytes += skb->len;\n\t\t}\n\n\t\tdma_unmap_single(&ndev->dev, dma_unmap_addr(tx_buff, addr),\n\t\t\t\t dma_unmap_len(tx_buff, len), DMA_TO_DEVICE);\n\n\t\t/* return the sk_buff to system */\n\t\tdev_kfree_skb_irq(skb);\n\n\t\ttxbd->data = 0;\n\t\ttxbd->info = 0;\n\n\t\t*txbd_dirty = (*txbd_dirty + 1) % TX_BD_NUM;\n\t}\n\n\t/* Ensure that txbd_dirty is visible to tx() before checking\n\t * for queue stopped.\n\t */\n\tsmp_mb();\n\n\tif (netif_queue_stopped(ndev) && arc_emac_tx_avail(priv))\n\t\tnetif_wake_queue(ndev);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 984,
            "cve_id": "CVE-2016-2069",
            "code_snippet": "void flush_tlb_current_task(void)\n{\n\tstruct mm_struct *mm = current->mm;\n\n\tpreempt_disable();\n\n\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);\n\tlocal_flush_tlb();\n\ttrace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);\n\tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)\n\t\tflush_tlb_others(mm_cpumask(mm), mm, 0UL, TLB_FLUSH_ALL);\n\tpreempt_enable();\n}",
            "detect_result": "NO\n\nExplanation: The provided code snippet appears to be part of a Linux kernel function responsible for flushing the Translation Lookaside Buffer (TLB) for the current task. The function `flush_tlb_current_task` performs several operations:\n\n1. It retrieves the memory descriptor (`mm_struct`) for the current task.\n2. It disables preemption to ensure that the operation is atomic.\n3. It counts a TLB local flush event and then flushes the TLB locally.\n4. It traces the TLB flush event.\n5. It checks if there are any other CPUs that need to have their TLBs flushed for the same memory descriptor. If so, it calls `flush_tlb_others` to perform the flush on those CPUs.\n6. Finally, it re-enables preemption.\n\nThe code does not appear to contain any obvious vulnerabilities such as buffer overflows, use-after-free, or improper input validation. However, it's important to note that this is a low-level kernel function, and vulnerabilities can sometimes arise from interactions with other parts of the system or incorrect assumptions about the state of the system. But based solely on the provided code snippet, there are no apparent vulnerabilities.",
            "final_result": 0
        },
        {
            "id": 1004,
            "cve_id": "CVE-2016-2546",
            "code_snippet": "static long snd_timer_user_ioctl(struct file *file, unsigned int cmd,\n\t\t\t\t unsigned long arg)\n{\n\tstruct snd_timer_user *tu;\n\tvoid __user *argp = (void __user *)arg;\n\tint __user *p = argp;\n\n\ttu = file->private_data;\n\tswitch (cmd) {\n\tcase SNDRV_TIMER_IOCTL_PVERSION:\n\t\treturn put_user(SNDRV_TIMER_VERSION, p) ? -EFAULT : 0;\n\tcase SNDRV_TIMER_IOCTL_NEXT_DEVICE:\n\t\treturn snd_timer_user_next_device(argp);\n\tcase SNDRV_TIMER_IOCTL_TREAD:\n\t{\n\t\tint xarg;\n\n\t\tmutex_lock(&tu->tread_sem);\n\t\tif (tu->timeri)\t{\t/* too late */\n\t\t\tmutex_unlock(&tu->tread_sem);\n\t\t\treturn -EBUSY;\n\t\t}\n\t\tif (get_user(xarg, p)) {\n\t\t\tmutex_unlock(&tu->tread_sem);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\ttu->tread = xarg ? 1 : 0;\n\t\tmutex_unlock(&tu->tread_sem);\n\t\treturn 0;\n\t}\n\tcase SNDRV_TIMER_IOCTL_GINFO:\n\t\treturn snd_timer_user_ginfo(file, argp);\n\tcase SNDRV_TIMER_IOCTL_GPARAMS:\n\t\treturn snd_timer_user_gparams(file, argp);\n\tcase SNDRV_TIMER_IOCTL_GSTATUS:\n\t\treturn snd_timer_user_gstatus(file, argp);\n\tcase SNDRV_TIMER_IOCTL_SELECT:\n\t\treturn snd_timer_user_tselect(file, argp);\n\tcase SNDRV_TIMER_IOCTL_INFO:\n\t\treturn snd_timer_user_info(file, argp);\n\tcase SNDRV_TIMER_IOCTL_PARAMS:\n\t\treturn snd_timer_user_params(file, argp);\n\tcase SNDRV_TIMER_IOCTL_STATUS:\n\t\treturn snd_timer_user_status(file, argp);\n\tcase SNDRV_TIMER_IOCTL_START:\n\tcase SNDRV_TIMER_IOCTL_START_OLD:\n\t\treturn snd_timer_user_start(file);\n\tcase SNDRV_TIMER_IOCTL_STOP:\n\tcase SNDRV_TIMER_IOCTL_STOP_OLD:\n\t\treturn snd_timer_user_stop(file);\n\tcase SNDRV_TIMER_IOCTL_CONTINUE:\n\tcase SNDRV_TIMER_IOCTL_CONTINUE_OLD:\n\t\treturn snd_timer_user_continue(file);\n\tcase SNDRV_TIMER_IOCTL_PAUSE:\n\tcase SNDRV_TIMER_IOCTL_PAUSE_OLD:\n\t\treturn snd_timer_user_pause(file);\n\t}\n\treturn -ENOTTY;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1008,
            "cve_id": "CVE-2016-2547",
            "code_snippet": "static int snd_timer_start_slave(struct snd_timer_instance *timeri)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&slave_active_lock, flags);\n\ttimeri->flags |= SNDRV_TIMER_IFLG_RUNNING;\n\tif (timeri->master)\n\t\tlist_add_tail(&timeri->active_list,\n\t\t\t      &timeri->master->slave_active_head);\n\tspin_unlock_irqrestore(&slave_active_lock, flags);\n\treturn 1; /* delayed start */\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1119,
            "cve_id": "CVE-2016-5195",
            "code_snippet": "static struct page *follow_page_pte(struct vm_area_struct *vma,\n\t\tunsigned long address, pmd_t *pmd, unsigned int flags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct dev_pagemap *pgmap = NULL;\n\tstruct page *page;\n\tspinlock_t *ptl;\n\tpte_t *ptep, pte;\n\nretry:\n\tif (unlikely(pmd_bad(*pmd)))\n\t\treturn no_page_table(vma, flags);\n\n\tptep = pte_offset_map_lock(mm, pmd, address, &ptl);\n\tpte = *ptep;\n\tif (!pte_present(pte)) {\n\t\tswp_entry_t entry;\n\t\t/*\n\t\t * KSM's break_ksm() relies upon recognizing a ksm page\n\t\t * even while it is being migrated, so for that case we\n\t\t * need migration_entry_wait().\n\t\t */\n\t\tif (likely(!(flags & FOLL_MIGRATION)))\n\t\t\tgoto no_page;\n\t\tif (pte_none(pte))\n\t\t\tgoto no_page;\n\t\tentry = pte_to_swp_entry(pte);\n\t\tif (!is_migration_entry(entry))\n\t\t\tgoto no_page;\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tmigration_entry_wait(mm, pmd, address);\n\t\tgoto retry;\n\t}\n\tif ((flags & FOLL_NUMA) && pte_protnone(pte))\n\t\tgoto no_page;\n\tif ((flags & FOLL_WRITE) && !pte_write(pte)) {\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\treturn NULL;\n\t}\n\n\tpage = vm_normal_page(vma, address, pte);\n\tif (!page && pte_devmap(pte) && (flags & FOLL_GET)) {\n\t\t/*\n\t\t * Only return device mapping pages in the FOLL_GET case since\n\t\t * they are only valid while holding the pgmap reference.\n\t\t */\n\t\tpgmap = get_dev_pagemap(pte_pfn(pte), NULL);\n\t\tif (pgmap)\n\t\t\tpage = pte_page(pte);\n\t\telse\n\t\t\tgoto no_page;\n\t} else if (unlikely(!page)) {\n\t\tif (flags & FOLL_DUMP) {\n\t\t\t/* Avoid special (like zero) pages in core dumps */\n\t\t\tpage = ERR_PTR(-EFAULT);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (is_zero_pfn(pte_pfn(pte))) {\n\t\t\tpage = pte_page(pte);\n\t\t} else {\n\t\t\tint ret;\n\n\t\t\tret = follow_pfn_pte(vma, address, ptep, flags);\n\t\t\tpage = ERR_PTR(ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (flags & FOLL_SPLIT && PageTransCompound(page)) {\n\t\tint ret;\n\t\tget_page(page);\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tlock_page(page);\n\t\tret = split_huge_page(page);\n\t\tunlock_page(page);\n\t\tput_page(page);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t\tgoto retry;\n\t}\n\n\tif (flags & FOLL_GET) {\n\t\tget_page(page);\n\n\t\t/* drop the pgmap reference now that we hold the page */\n\t\tif (pgmap) {\n\t\t\tput_dev_pagemap(pgmap);\n\t\t\tpgmap = NULL;\n\t\t}\n\t}\n\tif (flags & FOLL_TOUCH) {\n\t\tif ((flags & FOLL_WRITE) &&\n\t\t    !pte_dirty(pte) && !PageDirty(page))\n\t\t\tset_page_dirty(page);\n\t\t/*\n\t\t * pte_mkyoung() would be more correct here, but atomic care\n\t\t * is needed to avoid losing the dirty bit: it is easier to use\n\t\t * mark_page_accessed().\n\t\t */\n\t\tmark_page_accessed(page);\n\t}\n\tif ((flags & FOLL_MLOCK) && (vma->vm_flags & VM_LOCKED)) {\n\t\t/* Do not mlock pte-mapped THP */\n\t\tif (PageTransCompound(page))\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * The preliminary mapping check is mainly to avoid the\n\t\t * pointless overhead of lock_page on the ZERO_PAGE\n\t\t * which might bounce very badly if there is contention.\n\t\t *\n\t\t * If the page is already locked, we don't need to\n\t\t * handle it now - vmscan will handle it later if and\n\t\t * when it attempts to reclaim the page.\n\t\t */\n\t\tif (page->mapping && trylock_page(page)) {\n\t\t\tlru_add_drain();  /* push cached pages to LRU */\n\t\t\t/*\n\t\t\t * Because we lock page here, and migration is\n\t\t\t * blocked by the pte's page reference, and we\n\t\t\t * know the page is still mapped, we don't even\n\t\t\t * need to check for file-cache page truncation.\n\t\t\t */\n\t\t\tmlock_vma_page(page);\n\t\t\tunlock_page(page);\n\t\t}\n\t}\nout:\n\tpte_unmap_unlock(ptep, ptl);\n\treturn page;\nno_page:\n\tpte_unmap_unlock(ptep, ptl);\n\tif (!pte_none(pte))\n\t\treturn NULL;\n\treturn no_page_table(vma, flags);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1212,
            "cve_id": "CVE-2016-8655",
            "code_snippet": "static int\npacket_setsockopt(struct socket *sock, int level, int optname, char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint ret;\n\n\tif (level != SOL_PACKET)\n\t\treturn -ENOPROTOOPT;\n\n\tswitch (optname) {\n\tcase PACKET_ADD_MEMBERSHIP:\n\tcase PACKET_DROP_MEMBERSHIP:\n\t{\n\t\tstruct packet_mreq_max mreq;\n\t\tint len = optlen;\n\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\tif (len < sizeof(struct packet_mreq))\n\t\t\treturn -EINVAL;\n\t\tif (len > sizeof(mreq))\n\t\t\tlen = sizeof(mreq);\n\t\tif (copy_from_user(&mreq, optval, len))\n\t\t\treturn -EFAULT;\n\t\tif (len < (mreq.mr_alen + offsetof(struct packet_mreq, mr_address)))\n\t\t\treturn -EINVAL;\n\t\tif (optname == PACKET_ADD_MEMBERSHIP)\n\t\t\tret = packet_mc_add(sk, &mreq);\n\t\telse\n\t\t\tret = packet_mc_drop(sk, &mreq);\n\t\treturn ret;\n\t}\n\n\tcase PACKET_RX_RING:\n\tcase PACKET_TX_RING:\n\t{\n\t\tunion tpacket_req_u req_u;\n\t\tint len;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\t\tlen = sizeof(req_u.req);\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\tdefault:\n\t\t\tlen = sizeof(req_u.req3);\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&req_u.req, optval, len))\n\t\t\treturn -EFAULT;\n\t\treturn packet_set_ring(sk, &req_u, 0,\n\t\t\toptname == PACKET_TX_RING);\n\t}\n\tcase PACKET_COPY_THRESH:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpkt_sk(sk)->copy_thresh = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VERSION:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tswitch (val) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\tcase TPACKET_V3:\n\t\t\tpo->tp_version = val;\n\t\t\treturn 0;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\tcase PACKET_RESERVE:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_reserve = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_LOSS:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_loss = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_AUXDATA:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->auxdata = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_ORIGDEV:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->origdev = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VNET_HDR:\n\t{\n\t\tint val;\n\n\t\tif (sock->type != SOCK_RAW)\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->has_vnet_hdr = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_TIMESTAMP:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->tp_tstamp = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_FANOUT:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\treturn fanout_add(sk, val & 0xffff, val >> 16);\n\t}\n\tcase PACKET_FANOUT_DATA:\n\t{\n\t\tif (!po->fanout)\n\t\t\treturn -EINVAL;\n\n\t\treturn fanout_set_data(po, optval, optlen);\n\t}\n\tcase PACKET_TX_HAS_OFF:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_tx_has_off = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_QDISC_BYPASS:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->xmit = val ? packet_direct_xmit : dev_queue_xmit;\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1262,
            "cve_id": "CVE-2017-1000112",
            "code_snippet": "ssize_t\tip_append_page(struct sock *sk, struct flowi4 *fl4, struct page *page,\n\t\t       int offset, size_t size, int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct rtable *rt;\n\tstruct ip_options *opt = NULL;\n\tstruct inet_cork *cork;\n\tint hh_len;\n\tint mtu;\n\tint len;\n\tint err;\n\tunsigned int maxfraglen, fragheaderlen, fraggap, maxnonfragsize;\n\n\tif (inet->hdrincl)\n\t\treturn -EPERM;\n\n\tif (flags&MSG_PROBE)\n\t\treturn 0;\n\n\tif (skb_queue_empty(&sk->sk_write_queue))\n\t\treturn -EINVAL;\n\n\tcork = &inet->cork.base;\n\trt = (struct rtable *)cork->dst;\n\tif (cork->flags & IPCORK_OPT)\n\t\topt = cork->opt;\n\n\tif (!(rt->dst.dev->features&NETIF_F_SG))\n\t\treturn -EOPNOTSUPP;\n\n\thh_len = LL_RESERVED_SPACE(rt->dst.dev);\n\tmtu = cork->fragsize;\n\n\tfragheaderlen = sizeof(struct iphdr) + (opt ? opt->optlen : 0);\n\tmaxfraglen = ((mtu - fragheaderlen) & ~7) + fragheaderlen;\n\tmaxnonfragsize = ip_sk_ignore_df(sk) ? 0xFFFF : mtu;\n\n\tif (cork->length + size > maxnonfragsize - fragheaderlen) {\n\t\tip_local_error(sk, EMSGSIZE, fl4->daddr, inet->inet_dport,\n\t\t\t       mtu - (opt ? opt->optlen : 0));\n\t\treturn -EMSGSIZE;\n\t}\n\n\tskb = skb_peek_tail(&sk->sk_write_queue);\n\tif (!skb)\n\t\treturn -EINVAL;\n\n\tif ((size + skb->len > mtu) &&\n\t    (sk->sk_protocol == IPPROTO_UDP) &&\n\t    (rt->dst.dev->features & NETIF_F_UFO)) {\n\t\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tskb_shinfo(skb)->gso_size = mtu - fragheaderlen;\n\t\tskb_shinfo(skb)->gso_type = SKB_GSO_UDP;\n\t}\n\tcork->length += size;\n\n\twhile (size > 0) {\n\t\tif (skb_is_gso(skb)) {\n\t\t\tlen = size;\n\t\t} else {\n\n\t\t\t/* Check if the remaining data fits into current packet. */\n\t\t\tlen = mtu - skb->len;\n\t\t\tif (len < size)\n\t\t\t\tlen = maxfraglen - skb->len;\n\t\t}\n\t\tif (len <= 0) {\n\t\t\tstruct sk_buff *skb_prev;\n\t\t\tint alloclen;\n\n\t\t\tskb_prev = skb;\n\t\t\tfraggap = skb_prev->len - maxfraglen;\n\n\t\t\talloclen = fragheaderlen + hh_len + fraggap + 15;\n\t\t\tskb = sock_wmalloc(sk, alloclen, 1, sk->sk_allocation);\n\t\t\tif (unlikely(!skb)) {\n\t\t\t\terr = -ENOBUFS;\n\t\t\t\tgoto error;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t *\tFill in the control structures\n\t\t\t */\n\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\t\tskb->csum = 0;\n\t\t\tskb_reserve(skb, hh_len);\n\n\t\t\t/*\n\t\t\t *\tFind where to start putting bytes.\n\t\t\t */\n\t\t\tskb_put(skb, fragheaderlen + fraggap);\n\t\t\tskb_reset_network_header(skb);\n\t\t\tskb->transport_header = (skb->network_header +\n\t\t\t\t\t\t fragheaderlen);\n\t\t\tif (fraggap) {\n\t\t\t\tskb->csum = skb_copy_and_csum_bits(skb_prev,\n\t\t\t\t\t\t\t\t   maxfraglen,\n\t\t\t\t\t\t    skb_transport_header(skb),\n\t\t\t\t\t\t\t\t   fraggap, 0);\n\t\t\t\tskb_prev->csum = csum_sub(skb_prev->csum,\n\t\t\t\t\t\t\t  skb->csum);\n\t\t\t\tpskb_trim_unique(skb_prev, maxfraglen);\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Put the packet on the pending queue.\n\t\t\t */\n\t\t\t__skb_queue_tail(&sk->sk_write_queue, skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (len > size)\n\t\t\tlen = size;\n\n\t\tif (skb_append_pagefrags(skb, page, offset, len)) {\n\t\t\terr = -EMSGSIZE;\n\t\t\tgoto error;\n\t\t}\n\n\t\tif (skb->ip_summed == CHECKSUM_NONE) {\n\t\t\t__wsum csum;\n\t\t\tcsum = csum_page(page, offset, len);\n\t\t\tskb->csum = csum_block_add(skb->csum, csum, skb->len);\n\t\t}\n\n\t\tskb->len += len;\n\t\tskb->data_len += len;\n\t\tskb->truesize += len;\n\t\trefcount_add(len, &sk->sk_wmem_alloc);\n\t\toffset += len;\n\t\tsize -= len;\n\t}\n\treturn 0;\n\nerror:\n\tcork->length -= size;\n\tIP_INC_STATS(sock_net(sk), IPSTATS_MIB_OUTDISCARDS);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1320,
            "cve_id": "CVE-2017-1000405",
            "code_snippet": "struct page *follow_devmap_pmd(struct vm_area_struct *vma, unsigned long addr,\n\t\tpmd_t *pmd, int flags)\n{\n\tunsigned long pfn = pmd_pfn(*pmd);\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct dev_pagemap *pgmap;\n\tstruct page *page;\n\n\tassert_spin_locked(pmd_lockptr(mm, pmd));\n\n\t/*\n\t * When we COW a devmap PMD entry, we split it into PTEs, so we should\n\t * not be in this function with `flags & FOLL_COW` set.\n\t */\n\tWARN_ONCE(flags & FOLL_COW, \"mm: In follow_devmap_pmd with FOLL_COW set\");\n\n\tif (flags & FOLL_WRITE && !pmd_write(*pmd))\n\t\treturn NULL;\n\n\tif (pmd_present(*pmd) && pmd_devmap(*pmd))\n\t\t/* pass */;\n\telse\n\t\treturn NULL;\n\n\tif (flags & FOLL_TOUCH)\n\t\ttouch_pmd(vma, addr, pmd);\n\n\t/*\n\t * device mapped pages can only be returned if the\n\t * caller will manage the page reference count.\n\t */\n\tif (!(flags & FOLL_GET))\n\t\treturn ERR_PTR(-EEXIST);\n\n\tpfn += (addr & ~PMD_MASK) >> PAGE_SHIFT;\n\tpgmap = get_dev_pagemap(pfn, NULL);\n\tif (!pgmap)\n\t\treturn ERR_PTR(-EFAULT);\n\tpage = pfn_to_page(pfn);\n\tget_page(page);\n\tput_dev_pagemap(pgmap);\n\n\treturn page;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1336,
            "cve_id": "CVE-2017-12146",
            "code_snippet": "static ssize_t driver_override_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct platform_device *pdev = to_platform_device(dev);\n\n\treturn sprintf(buf, \"%s\\n\", pdev->driver_override);\n}",
            "detect_result": "NO\n\nExplanation: The provided code snippet defines a function `driver_override_show` which is used to display the value of `driver_override` for a given platform device. It uses `sprintf` to format the string and write it into the buffer `buf`. \n\nFrom the snippet alone, there are no obvious vulnerabilities such as buffer overflows, format string vulnerabilities, or improper input validation. However, it's important to note that the safety of this function also depends on how `buf` is allocated and managed elsewhere in the code. If `buf` is not large enough to hold the formatted string, it could lead to a buffer overflow. But based solely on the provided code, there is no direct evidence of a vulnerability.",
            "final_result": 0
        },
        {
            "id": 1382,
            "cve_id": "CVE-2017-15265",
            "code_snippet": "struct snd_seq_client_port *snd_seq_create_port(struct snd_seq_client *client,\n\t\t\t\t\t\tint port)\n{\n\tunsigned long flags;\n\tstruct snd_seq_client_port *new_port, *p;\n\tint num = -1;\n\t\n\t/* sanity check */\n\tif (snd_BUG_ON(!client))\n\t\treturn NULL;\n\n\tif (client->num_ports >= SNDRV_SEQ_MAX_PORTS) {\n\t\tpr_warn(\"ALSA: seq: too many ports for client %d\\n\", client->number);\n\t\treturn NULL;\n\t}\n\n\t/* create a new port */\n\tnew_port = kzalloc(sizeof(*new_port), GFP_KERNEL);\n\tif (!new_port)\n\t\treturn NULL;\t/* failure, out of memory */\n\t/* init port data */\n\tnew_port->addr.client = client->number;\n\tnew_port->addr.port = -1;\n\tnew_port->owner = THIS_MODULE;\n\tsprintf(new_port->name, \"port-%d\", num);\n\tsnd_use_lock_init(&new_port->use_lock);\n\tport_subs_info_init(&new_port->c_src);\n\tport_subs_info_init(&new_port->c_dest);\n\n\tnum = port >= 0 ? port : 0;\n\tmutex_lock(&client->ports_mutex);\n\twrite_lock_irqsave(&client->ports_lock, flags);\n\tlist_for_each_entry(p, &client->ports_list_head, list) {\n\t\tif (p->addr.port > num)\n\t\t\tbreak;\n\t\tif (port < 0) /* auto-probe mode */\n\t\t\tnum = p->addr.port + 1;\n\t}\n\t/* insert the new port */\n\tlist_add_tail(&new_port->list, &p->list);\n\tclient->num_ports++;\n\tnew_port->addr.port = num;\t/* store the port number in the port */\n\twrite_unlock_irqrestore(&client->ports_lock, flags);\n\tmutex_unlock(&client->ports_mutex);\n\tsprintf(new_port->name, \"port-%d\", num);\n\n\treturn new_port;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1569,
            "cve_id": "CVE-2017-6346",
            "code_snippet": "static void fanout_release(struct sock *sk)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f;\n\n\tf = po->fanout;\n\tif (!f)\n\t\treturn;\n\n\tmutex_lock(&fanout_mutex);\n\tpo->fanout = NULL;\n\n\tif (atomic_dec_and_test(&f->sk_ref)) {\n\t\tlist_del(&f->list);\n\t\tdev_remove_pack(&f->prot_hook);\n\t\tfanout_release_data(f);\n\t\tkfree(f);\n\t}\n\tmutex_unlock(&fanout_mutex);\n\n\tif (po->rollover)\n\t\tkfree_rcu(po->rollover, rcu);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1574,
            "cve_id": "CVE-2017-6874",
            "code_snippet": "static struct ucounts *get_ucounts(struct user_namespace *ns, kuid_t uid)\n{\n\tstruct hlist_head *hashent = ucounts_hashentry(ns, uid);\n\tstruct ucounts *ucounts, *new;\n\n\tspin_lock_irq(&ucounts_lock);\n\tucounts = find_ucounts(ns, uid, hashent);\n\tif (!ucounts) {\n\t\tspin_unlock_irq(&ucounts_lock);\n\n\t\tnew = kzalloc(sizeof(*new), GFP_KERNEL);\n\t\tif (!new)\n\t\t\treturn NULL;\n\n\t\tnew->ns = ns;\n\t\tnew->uid = uid;\n\t\tatomic_set(&new->count, 0);\n\n\t\tspin_lock_irq(&ucounts_lock);\n\t\tucounts = find_ucounts(ns, uid, hashent);\n\t\tif (ucounts) {\n\t\t\tkfree(new);\n\t\t} else {\n\t\t\thlist_add_head(&new->node, hashent);\n\t\t\tucounts = new;\n\t\t}\n\t}\n\tif (!atomic_add_unless(&ucounts->count, 1, INT_MAX))\n\t\tucounts = NULL;\n\tspin_unlock_irq(&ucounts_lock);\n\treturn ucounts;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1597,
            "cve_id": "CVE-2017-7533",
            "code_snippet": "int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,\n\t       struct inode *new_dir, struct dentry *new_dentry,\n\t       struct inode **delegated_inode, unsigned int flags)\n{\n\tint error;\n\tbool is_dir = d_is_dir(old_dentry);\n\tconst unsigned char *old_name;\n\tstruct inode *source = old_dentry->d_inode;\n\tstruct inode *target = new_dentry->d_inode;\n\tbool new_is_dir = false;\n\tunsigned max_links = new_dir->i_sb->s_max_links;\n\n\tif (source == target)\n\t\treturn 0;\n\n\terror = may_delete(old_dir, old_dentry, is_dir);\n\tif (error)\n\t\treturn error;\n\n\tif (!target) {\n\t\terror = may_create(new_dir, new_dentry);\n\t} else {\n\t\tnew_is_dir = d_is_dir(new_dentry);\n\n\t\tif (!(flags & RENAME_EXCHANGE))\n\t\t\terror = may_delete(new_dir, new_dentry, is_dir);\n\t\telse\n\t\t\terror = may_delete(new_dir, new_dentry, new_is_dir);\n\t}\n\tif (error)\n\t\treturn error;\n\n\tif (!old_dir->i_op->rename)\n\t\treturn -EPERM;\n\n\t/*\n\t * If we are going to change the parent - check write permissions,\n\t * we'll need to flip '..'.\n\t */\n\tif (new_dir != old_dir) {\n\t\tif (is_dir) {\n\t\t\terror = inode_permission(source, MAY_WRITE);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\t\tif ((flags & RENAME_EXCHANGE) && new_is_dir) {\n\t\t\terror = inode_permission(target, MAY_WRITE);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\t}\n\n\terror = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,\n\t\t\t\t      flags);\n\tif (error)\n\t\treturn error;\n\n\told_name = fsnotify_oldname_init(old_dentry->d_name.name);\n\tdget(new_dentry);\n\tif (!is_dir || (flags & RENAME_EXCHANGE))\n\t\tlock_two_nondirectories(source, target);\n\telse if (target)\n\t\tinode_lock(target);\n\n\terror = -EBUSY;\n\tif (is_local_mountpoint(old_dentry) || is_local_mountpoint(new_dentry))\n\t\tgoto out;\n\n\tif (max_links && new_dir != old_dir) {\n\t\terror = -EMLINK;\n\t\tif (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)\n\t\t\tgoto out;\n\t\tif ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&\n\t\t    old_dir->i_nlink >= max_links)\n\t\t\tgoto out;\n\t}\n\tif (is_dir && !(flags & RENAME_EXCHANGE) && target)\n\t\tshrink_dcache_parent(new_dentry);\n\tif (!is_dir) {\n\t\terror = try_break_deleg(source, delegated_inode);\n\t\tif (error)\n\t\t\tgoto out;\n\t}\n\tif (target && !new_is_dir) {\n\t\terror = try_break_deleg(target, delegated_inode);\n\t\tif (error)\n\t\t\tgoto out;\n\t}\n\terror = old_dir->i_op->rename(old_dir, old_dentry,\n\t\t\t\t       new_dir, new_dentry, flags);\n\tif (error)\n\t\tgoto out;\n\n\tif (!(flags & RENAME_EXCHANGE) && target) {\n\t\tif (is_dir)\n\t\t\ttarget->i_flags |= S_DEAD;\n\t\tdont_mount(new_dentry);\n\t\tdetach_mounts(new_dentry);\n\t}\n\tif (!(old_dir->i_sb->s_type->fs_flags & FS_RENAME_DOES_D_MOVE)) {\n\t\tif (!(flags & RENAME_EXCHANGE))\n\t\t\td_move(old_dentry, new_dentry);\n\t\telse\n\t\t\td_exchange(old_dentry, new_dentry);\n\t}\nout:\n\tif (!is_dir || (flags & RENAME_EXCHANGE))\n\t\tunlock_two_nondirectories(source, target);\n\telse if (target)\n\t\tinode_unlock(target);\n\tdput(new_dentry);\n\tif (!error) {\n\t\tfsnotify_move(old_dir, new_dir, old_name, is_dir,\n\t\t\t      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);\n\t\tif (flags & RENAME_EXCHANGE) {\n\t\t\tfsnotify_move(new_dir, old_dir, old_dentry->d_name.name,\n\t\t\t\t      new_is_dir, NULL, new_dentry);\n\t\t}\n\t}\n\tfsnotify_oldname_free(old_name);\n\n\treturn error;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1664,
            "cve_id": "CVE-2018-1000004",
            "code_snippet": "static struct snd_seq_client *seq_create_client1(int client_index, int poolsize)\n{\n\tunsigned long flags;\n\tint c;\n\tstruct snd_seq_client *client;\n\n\t/* init client data */\n\tclient = kzalloc(sizeof(*client), GFP_KERNEL);\n\tif (client == NULL)\n\t\treturn NULL;\n\tclient->pool = snd_seq_pool_new(poolsize);\n\tif (client->pool == NULL) {\n\t\tkfree(client);\n\t\treturn NULL;\n\t}\n\tclient->type = NO_CLIENT;\n\tsnd_use_lock_init(&client->use_lock);\n\trwlock_init(&client->ports_lock);\n\tmutex_init(&client->ports_mutex);\n\tINIT_LIST_HEAD(&client->ports_list_head);\n\n\t/* find free slot in the client table */\n\tspin_lock_irqsave(&clients_lock, flags);\n\tif (client_index < 0) {\n\t\tfor (c = SNDRV_SEQ_DYNAMIC_CLIENTS_BEGIN;\n\t\t     c < SNDRV_SEQ_MAX_CLIENTS;\n\t\t     c++) {\n\t\t\tif (clienttab[c] || clienttablock[c])\n\t\t\t\tcontinue;\n\t\t\tclienttab[client->number = c] = client;\n\t\t\tspin_unlock_irqrestore(&clients_lock, flags);\n\t\t\treturn client;\n\t\t}\n\t} else {\n\t\tif (clienttab[client_index] == NULL && !clienttablock[client_index]) {\n\t\t\tclienttab[client->number = client_index] = client;\n\t\t\tspin_unlock_irqrestore(&clients_lock, flags);\n\t\t\treturn client;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&clients_lock, flags);\n\tsnd_seq_pool_delete(&client->pool);\n\tkfree(client);\n\treturn NULL;\t/* no free slot found or busy, return failure code */\n}",
            "detect_result": "NO\n\nThe provided code appears to be a function for creating a new client in a sound sequencer system. It initializes a `snd_seq_client` structure, allocates memory for it and its associated pool, and then attempts to find a free slot in the client table to insert the new client.\n\nThe code includes several checks and initializations that help prevent common vulnerabilities:\n\n1. **Memory Allocation Check**: The function checks if the memory allocation for `client` and `client->pool` was successful before proceeding. If not, it cleans up and returns `NULL`.\n\n2. **Locking Mechanisms**: The function uses spinlocks (`spin_lock_irqsave` and `spin_unlock_irqrestore`) to ensure that the critical section where the client table is modified is thread-safe.\n\n3. **Boundary Checks**: When searching for a free slot in the client table, the function ensures that the index is within the valid range (`SNDRV_SEQ_DYNAMIC_CLIENTS_BEGIN` to `SNDRV_SEQ_MAX_CLIENTS`).\n\n4. **Busy Slot Check**: Before assigning a client to a slot, the function checks if the slot is already occupied or locked (`clienttab[c] || clienttablock[c]`).\n\n5. **Resource Cleanup**: If the function fails to find a free slot or encounters an error, it properly cleans up by deleting the sequence pool and freeing the allocated memory for the client.\n\nGiven these precautions, the code does not appear to have obvious vulnerabilities such as buffer overflows, use-after-free, or race conditions. However, without more context about the surrounding code and how this function is used, it's impossible to rule out all potential issues. Nonetheless, based on the provided code snippet alone, the answer is NO, it does not exhibit any immediate vulnerabilities.",
            "final_result": 0
        },
        {
            "id": 1742,
            "cve_id": "CVE-2018-12232",
            "code_snippet": "static int sock_close(struct inode *inode, struct file *filp)\n{\n\tsock_release(SOCKET_I(inode));\n\treturn 0;\n}",
            "detect_result": "NO\n\nExplanation: The provided code snippet defines a function `sock_close` that is intended to close a socket. It calls `sock_release` with the socket associated with the given `inode`. The function then returns 0, indicating successful closure.\n\nFrom the snippet alone, there are no obvious vulnerabilities such as buffer overflows, improper memory management, or insecure data handling. However, it's important to note that the security of this function would also depend on the implementation details of `sock_release` and how `inode` and `filp` are managed elsewhere in the codebase. Without additional context, we cannot identify any direct vulnerabilities in this specific function.",
            "final_result": 0
        },
        {
            "id": 1875,
            "cve_id": "CVE-2018-20836",
            "code_snippet": "static void smp_task_done(struct sas_task *task)\n{\n\tif (!del_timer(&task->slow_task->timer))\n\t\treturn;\n\tcomplete(&task->slow_task->completion);\n}",
            "detect_result": "NO\n\nExplanation: The provided code snippet appears to be handling the completion of an SAS (Serial Attached SCSI) task in a kernel module. It checks if a timer associated with the task (`task->slow_task->timer`) can be deleted using `del_timer()`. If the timer is successfully deleted, it signals the completion of the task using `complete()`.\n\nFrom the given code alone, there are no obvious vulnerabilities such as buffer overflows, use-after-free, or improper input validation. However, without more context about how `sas_task` and its members are managed and initialized, it's challenging to identify potential issues. But based on the snippet provided, there is no evident vulnerability.",
            "final_result": 0
        },
        {
            "id": 1922,
            "cve_id": "CVE-2018-5814",
            "code_snippet": "static void stub_disconnect(struct usb_device *udev)\n{\n\tstruct stub_device *sdev;\n\tconst char *udev_busid = dev_name(&udev->dev);\n\tstruct bus_id_priv *busid_priv;\n\tint rc;\n\n\tdev_dbg(&udev->dev, \"Enter disconnect\\n\");\n\n\tbusid_priv = get_busid_priv(udev_busid);\n\tif (!busid_priv) {\n\t\tBUG();\n\t\treturn;\n\t}\n\n\tsdev = dev_get_drvdata(&udev->dev);\n\n\t/* get stub_device */\n\tif (!sdev) {\n\t\tdev_err(&udev->dev, \"could not get device\");\n\t\treturn;\n\t}\n\n\tdev_set_drvdata(&udev->dev, NULL);\n\n\t/*\n\t * NOTE: rx/tx threads are invoked for each usb_device.\n\t */\n\tstub_remove_files(&udev->dev);\n\n\t/* release port */\n\trc = usb_hub_release_port(udev->parent, udev->portnum,\n\t\t\t\t  (struct usb_dev_state *) udev);\n\tif (rc) {\n\t\tdev_dbg(&udev->dev, \"unable to release port\\n\");\n\t\treturn;\n\t}\n\n\t/* If usb reset is called from event handler */\n\tif (usbip_in_eh(current))\n\t\treturn;\n\n\t/* shutdown the current connection */\n\tshutdown_busid(busid_priv);\n\n\tusb_put_dev(sdev->udev);\n\n\t/* free sdev */\n\tbusid_priv->sdev = NULL;\n\tstub_device_free(sdev);\n\n\tif (busid_priv->status == STUB_BUSID_ALLOC)\n\t\tbusid_priv->status = STUB_BUSID_ADDED;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1923,
            "cve_id": "CVE-2018-5814",
            "code_snippet": "int del_match_busid(char *busid)\n{\n\tint idx;\n\tint ret = -1;\n\n\tspin_lock(&busid_table_lock);\n\tidx = get_busid_idx(busid);\n\tif (idx < 0)\n\t\tgoto out;\n\n\t/* found */\n\tret = 0;\n\n\tif (busid_table[idx].status == STUB_BUSID_OTHER)\n\t\tmemset(busid_table[idx].name, 0, BUSID_SIZE);\n\n\tif ((busid_table[idx].status != STUB_BUSID_OTHER) &&\n\t    (busid_table[idx].status != STUB_BUSID_ADDED))\n\t\tbusid_table[idx].status = STUB_BUSID_REMOV;\n\nout:\n\tspin_unlock(&busid_table_lock);\n\n\treturn ret;\n}",
            "detect_result": "NO\n\nExplanation of the code behavior:\n- The function `del_match_busid` takes a pointer to a character array `busid` as an argument.\n- It initializes two integer variables: `idx` and `ret`. `ret` is initialized to `-1`, indicating a failure by default.\n- A spin lock (`busid_table_lock`) is acquired to ensure that the following operations on `busid_table` are thread-safe.\n- The function `get_busid_idx` is called with `busid` to find the index of the bus ID in the `busid_table`. If the index is not found (`idx < 0`), the function jumps to the `out` label, releases the lock, and returns `-1`.\n- If the index is found, `ret` is set to `0`, indicating success.\n- The function then checks the status of the entry at `busid_table[idx]`:\n  - If the status is `STUB_BUSID_OTHER`, it clears the name field of the entry by setting all bytes to `0` using `memset`.\n  - If the status is neither `STUB_BUSID_OTHER` nor `STUB_BUSID_ADDED`, it sets the status to `STUB_BUSID_REMOV`.\n- Finally, the spin lock is released, and the function returns the value of `ret`.\n\nVulnerability assessment:\n- The code uses a spin lock to protect access to `busid_table`, which helps prevent race conditions.\n- There are no obvious buffer overflows or other memory-related vulnerabilities in the provided code snippet.\n- The use of `memset` to clear the name field when the status is `STUB_BUSID_OTHER` is a good practice for security, as it prevents information leakage.\n- The function handles the case where the bus ID is not found by returning `-1` immediately.\n\nGiven these points, the code does not appear to have any critical vulnerabilities based on the provided snippet. However, without more context about the rest of the system and how `busid_table` and related functions are implemented, there could be potential issues elsewhere in the codebase.",
            "final_result": 0
        },
        {
            "id": 1924,
            "cve_id": "CVE-2018-5814",
            "code_snippet": "static ssize_t match_busid_show(struct device_driver *drv, char *buf)\n{\n\tint i;\n\tchar *out = buf;\n\n\tspin_lock(&busid_table_lock);\n\tfor (i = 0; i < MAX_BUSID; i++)\n\t\tif (busid_table[i].name[0])\n\t\t\tout += sprintf(out, \"%s \", busid_table[i].name);\n\tspin_unlock(&busid_table_lock);\n\tout += sprintf(out, \"\\n\");\n\n\treturn out - buf;\n}",
            "detect_result": "NO\n\nExplanation of the code behavior:\nThe function `match_busid_show` is designed to display a list of bus IDs associated with a device driver. It iterates over an array `busid_table` up to a maximum index defined by `MAX_BUSID`. For each entry in the table that has a non-empty name (checked by `busid_table[i].name[0]`), it appends the name to the output buffer `buf`, followed by a space. After processing all entries, it appends a newline character to the buffer. The function then returns the number of characters written to the buffer.\n\nVulnerability assessment:\nThe provided code does not appear to have any obvious vulnerabilities such as buffer overflows, format string vulnerabilities, or improper memory handling. However, there are some considerations:\n1. The function assumes that `buf` is large enough to hold all the bus IDs and the newline character. If `buf` is not sufficiently large, this could lead to a buffer overflow, but this is not a flaw in the function itself but rather in how it is used.\n2. The function uses `spin_lock` and `spin_unlock` to protect access to `busid_table`, which suggests that the table can be modified concurrently by other parts of the system. This is generally good practice to prevent race conditions, but it also means that the caller must ensure that the lock is not held for too long to avoid performance issues or deadlocks.\n3. The function does not perform any validation on the contents of `busid_table[i].name`, assuming that these strings are safe to print. If `busid_table` can be populated with malicious data, this could potentially lead to issues, but again, this is more about the context in which the function is used rather than a flaw in the function itself.\n\nGiven the information provided, the function does not contain any inherent vulnerabilities, but its safe use depends on the context in which it is called.",
            "final_result": 0
        },
        {
            "id": 1925,
            "cve_id": "CVE-2018-5814",
            "code_snippet": "struct bus_id_priv *get_busid_priv(const char *busid)\n{\n\tint idx;\n\tstruct bus_id_priv *bid = NULL;\n\n\tspin_lock(&busid_table_lock);\n\tidx = get_busid_idx(busid);\n\tif (idx >= 0)\n\t\tbid = &(busid_table[idx]);\n\tspin_unlock(&busid_table_lock);\n\n\treturn bid;\n}",
            "detect_result": "NO\n\nExplanation:\nThe provided code snippet defines a function `get_busid_priv` that retrieves a pointer to a `bus_id_priv` structure based on a given `busid`. The function uses a spinlock (`busid_table_lock`) to ensure thread safety when accessing the `busid_table`.\n\nHere's a breakdown of the code behavior:\n1. It locks the `busid_table_lock` to prevent concurrent access issues.\n2. It calls `get_busid_idx(busid)` to find the index of the `busid` in the `busid_table`.\n3. If the index is valid (i.e., `idx >= 0`), it sets `bid` to point to the corresponding entry in the `busid_table`.\n4. It unlocks the `busid_table_lock`.\n5. It returns the pointer `bid`.\n\nBased on the provided code, there are no obvious vulnerabilities such as buffer overflows, use-after-free, or improper memory management. However, without more context about the implementation of `get_busid_idx` and the initialization/management of `busid_table`, it's difficult to rule out all potential issues. But given the code snippet alone, there are no immediate security concerns.",
            "final_result": 0
        },
        {
            "id": 1957,
            "cve_id": "CVE-2018-7995",
            "code_snippet": "static ssize_t set_cmci_disabled(struct device *s,\n\t\t\t\t struct device_attribute *attr,\n\t\t\t\t const char *buf, size_t size)\n{\n\tu64 new;\n\n\tif (kstrtou64(buf, 0, &new) < 0)\n\t\treturn -EINVAL;\n\n\tif (mca_cfg.cmci_disabled ^ !!new) {\n\t\tif (new) {\n\t\t\t/* disable cmci */\n\t\t\ton_each_cpu(mce_disable_cmci, NULL, 1);\n\t\t\tmca_cfg.cmci_disabled = true;\n\t\t} else {\n\t\t\t/* enable cmci */\n\t\t\tmca_cfg.cmci_disabled = false;\n\t\t\ton_each_cpu(mce_enable_ce, NULL, 1);\n\t\t}\n\t}\n\treturn size;\n}",
            "detect_result": "NO\n\nExplanation:\nThe provided code snippet appears to be part of a Linux kernel module that manages the configuration of Machine Check Exception (MCE) handling, specifically for Control Machine Check Interrupts (CMCI). The function `set_cmci_disabled` takes input from a user via a sysfs attribute and sets the `cmci_disabled` flag accordingly.\n\nHere's a breakdown of the code:\n\n1. It reads a value from the user-provided buffer (`buf`) into a 64-bit unsigned integer (`new`) using `kstrtou64`. If this conversion fails, it returns `-EINVAL`.\n\n2. It checks if the current state of `mca_cfg.cmci_disabled` is different from the new value (`!!new` converts `new` to a boolean). If they differ, it proceeds to change the state.\n\n3. If `new` is non-zero (indicating the user wants to disable CMCI), it calls `on_each_cpu` with `mce_disable_cmci` to disable CMCI on all CPUs and sets `mca_cfg.cmci_disabled` to `true`.\n\n4. If `new` is zero (indicating the user wants to enable CMCI), it sets `mca_cfg.cmci_disabled` to `false` and then calls `on_each_cpu` with `mce_enable_ce` to enable Correctable Errors (CE) on all CPUs.\n\nThis code does not appear to have any obvious vulnerabilities such as buffer overflows, format string vulnerabilities, or improper input validation. The use of `kstrtou64` ensures that the input is properly converted to an integer, and the subsequent checks prevent unnecessary operations if the state is already as requested. However, the actual functions `mce_disable_cmci` and `mce_enable_ce` would need to be reviewed separately for any potential issues.",
            "final_result": 0
        },
        {
            "id": 2059,
            "cve_id": "CVE-2019-13233",
            "code_snippet": "int insn_get_code_seg_params(struct pt_regs *regs)\n{\n\tstruct desc_struct *desc;\n\tshort sel;\n\n\tif (v8086_mode(regs))\n\t\t/* Address and operand size are both 16-bit. */\n\t\treturn INSN_CODE_SEG_PARAMS(2, 2);\n\n\tsel = get_segment_selector(regs, INAT_SEG_REG_CS);\n\tif (sel < 0)\n\t\treturn sel;\n\n\tdesc = get_desc(sel);\n\tif (!desc)\n\t\treturn -EINVAL;\n\n\t/*\n\t * The most significant byte of the Type field of the segment descriptor\n\t * determines whether a segment contains data or code. If this is a data\n\t * segment, return error.\n\t */\n\tif (!(desc->type & BIT(3)))\n\t\treturn -EINVAL;\n\n\tswitch ((desc->l << 1) | desc->d) {\n\tcase 0: /*\n\t\t * Legacy mode. CS.L=0, CS.D=0. Address and operand size are\n\t\t * both 16-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(2, 2);\n\tcase 1: /*\n\t\t * Legacy mode. CS.L=0, CS.D=1. Address and operand size are\n\t\t * both 32-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(4, 4);\n\tcase 2: /*\n\t\t * IA-32e 64-bit mode. CS.L=1, CS.D=0. Address size is 64-bit;\n\t\t * operand size is 32-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(4, 8);\n\tcase 3: /* Invalid setting. CS.L=1, CS.D=1 */\n\t\t/* fall through */\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2200,
            "cve_id": "CVE-2019-18683",
            "code_snippet": "static int vivid_thread_vid_cap(void *data)\n{\n\tstruct vivid_dev *dev = data;\n\tu64 numerators_since_start;\n\tu64 buffers_since_start;\n\tu64 next_jiffies_since_start;\n\tunsigned long jiffies_since_start;\n\tunsigned long cur_jiffies;\n\tunsigned wait_jiffies;\n\tunsigned numerator;\n\tunsigned denominator;\n\tint dropped_bufs;\n\n\tdprintk(dev, 1, \"Video Capture Thread Start\\n\");\n\n\tset_freezable();\n\n\t/* Resets frame counters */\n\tdev->cap_seq_offset = 0;\n\tdev->cap_seq_count = 0;\n\tdev->cap_seq_resync = false;\n\tdev->jiffies_vid_cap = jiffies;\n\tdev->cap_stream_start = ktime_get_ns();\n\tvivid_cap_update_frame_period(dev);\n\n\tfor (;;) {\n\t\ttry_to_freeze();\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tmutex_lock(&dev->mutex);\n\t\tcur_jiffies = jiffies;\n\t\tif (dev->cap_seq_resync) {\n\t\t\tdev->jiffies_vid_cap = cur_jiffies;\n\t\t\tdev->cap_seq_offset = dev->cap_seq_count + 1;\n\t\t\tdev->cap_seq_count = 0;\n\t\t\tdev->cap_stream_start += dev->cap_frame_period *\n\t\t\t\t\t\t dev->cap_seq_offset;\n\t\t\tvivid_cap_update_frame_period(dev);\n\t\t\tdev->cap_seq_resync = false;\n\t\t}\n\t\tnumerator = dev->timeperframe_vid_cap.numerator;\n\t\tdenominator = dev->timeperframe_vid_cap.denominator;\n\n\t\tif (dev->field_cap == V4L2_FIELD_ALTERNATE)\n\t\t\tdenominator *= 2;\n\n\t\t/* Calculate the number of jiffies since we started streaming */\n\t\tjiffies_since_start = cur_jiffies - dev->jiffies_vid_cap;\n\t\t/* Get the number of buffers streamed since the start */\n\t\tbuffers_since_start = (u64)jiffies_since_start * denominator +\n\t\t\t\t      (HZ * numerator) / 2;\n\t\tdo_div(buffers_since_start, HZ * numerator);\n\n\t\t/*\n\t\t * After more than 0xf0000000 (rounded down to a multiple of\n\t\t * 'jiffies-per-day' to ease jiffies_to_msecs calculation)\n\t\t * jiffies have passed since we started streaming reset the\n\t\t * counters and keep track of the sequence offset.\n\t\t */\n\t\tif (jiffies_since_start > JIFFIES_RESYNC) {\n\t\t\tdev->jiffies_vid_cap = cur_jiffies;\n\t\t\tdev->cap_seq_offset = buffers_since_start;\n\t\t\tbuffers_since_start = 0;\n\t\t}\n\t\tdropped_bufs = buffers_since_start + dev->cap_seq_offset - dev->cap_seq_count;\n\t\tdev->cap_seq_count = buffers_since_start + dev->cap_seq_offset;\n\t\tdev->vid_cap_seq_count = dev->cap_seq_count - dev->vid_cap_seq_start;\n\t\tdev->vbi_cap_seq_count = dev->cap_seq_count - dev->vbi_cap_seq_start;\n\t\tdev->meta_cap_seq_count = dev->cap_seq_count - dev->meta_cap_seq_start;\n\n\t\tvivid_thread_vid_cap_tick(dev, dropped_bufs);\n\n\t\t/*\n\t\t * Calculate the number of 'numerators' streamed since we started,\n\t\t * including the current buffer.\n\t\t */\n\t\tnumerators_since_start = ++buffers_since_start * numerator;\n\n\t\t/* And the number of jiffies since we started */\n\t\tjiffies_since_start = jiffies - dev->jiffies_vid_cap;\n\n\t\tmutex_unlock(&dev->mutex);\n\n\t\t/*\n\t\t * Calculate when that next buffer is supposed to start\n\t\t * in jiffies since we started streaming.\n\t\t */\n\t\tnext_jiffies_since_start = numerators_since_start * HZ +\n\t\t\t\t\t   denominator / 2;\n\t\tdo_div(next_jiffies_since_start, denominator);\n\t\t/* If it is in the past, then just schedule asap */\n\t\tif (next_jiffies_since_start < jiffies_since_start)\n\t\t\tnext_jiffies_since_start = jiffies_since_start;\n\n\t\twait_jiffies = next_jiffies_since_start - jiffies_since_start;\n\t\tschedule_timeout_interruptible(wait_jiffies ? wait_jiffies : 1);\n\t}\n\tdprintk(dev, 1, \"Video Capture Thread End\\n\");\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2201,
            "cve_id": "CVE-2019-18683",
            "code_snippet": "static int vivid_thread_vid_out(void *data)\n{\n\tstruct vivid_dev *dev = data;\n\tu64 numerators_since_start;\n\tu64 buffers_since_start;\n\tu64 next_jiffies_since_start;\n\tunsigned long jiffies_since_start;\n\tunsigned long cur_jiffies;\n\tunsigned wait_jiffies;\n\tunsigned numerator;\n\tunsigned denominator;\n\n\tdprintk(dev, 1, \"Video Output Thread Start\\n\");\n\n\tset_freezable();\n\n\t/* Resets frame counters */\n\tdev->out_seq_offset = 0;\n\tif (dev->seq_wrap)\n\t\tdev->out_seq_count = 0xffffff80U;\n\tdev->jiffies_vid_out = jiffies;\n\tdev->vid_out_seq_start = dev->vbi_out_seq_start = 0;\n\tdev->meta_out_seq_start = 0;\n\tdev->out_seq_resync = false;\n\n\tfor (;;) {\n\t\ttry_to_freeze();\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tmutex_lock(&dev->mutex);\n\t\tcur_jiffies = jiffies;\n\t\tif (dev->out_seq_resync) {\n\t\t\tdev->jiffies_vid_out = cur_jiffies;\n\t\t\tdev->out_seq_offset = dev->out_seq_count + 1;\n\t\t\tdev->out_seq_count = 0;\n\t\t\tdev->out_seq_resync = false;\n\t\t}\n\t\tnumerator = dev->timeperframe_vid_out.numerator;\n\t\tdenominator = dev->timeperframe_vid_out.denominator;\n\n\t\tif (dev->field_out == V4L2_FIELD_ALTERNATE)\n\t\t\tdenominator *= 2;\n\n\t\t/* Calculate the number of jiffies since we started streaming */\n\t\tjiffies_since_start = cur_jiffies - dev->jiffies_vid_out;\n\t\t/* Get the number of buffers streamed since the start */\n\t\tbuffers_since_start = (u64)jiffies_since_start * denominator +\n\t\t\t\t      (HZ * numerator) / 2;\n\t\tdo_div(buffers_since_start, HZ * numerator);\n\n\t\t/*\n\t\t * After more than 0xf0000000 (rounded down to a multiple of\n\t\t * 'jiffies-per-day' to ease jiffies_to_msecs calculation)\n\t\t * jiffies have passed since we started streaming reset the\n\t\t * counters and keep track of the sequence offset.\n\t\t */\n\t\tif (jiffies_since_start > JIFFIES_RESYNC) {\n\t\t\tdev->jiffies_vid_out = cur_jiffies;\n\t\t\tdev->out_seq_offset = buffers_since_start;\n\t\t\tbuffers_since_start = 0;\n\t\t}\n\t\tdev->out_seq_count = buffers_since_start + dev->out_seq_offset;\n\t\tdev->vid_out_seq_count = dev->out_seq_count - dev->vid_out_seq_start;\n\t\tdev->vbi_out_seq_count = dev->out_seq_count - dev->vbi_out_seq_start;\n\t\tdev->meta_out_seq_count = dev->out_seq_count - dev->meta_out_seq_start;\n\n\t\tvivid_thread_vid_out_tick(dev);\n\t\tmutex_unlock(&dev->mutex);\n\n\t\t/*\n\t\t * Calculate the number of 'numerators' streamed since we started,\n\t\t * not including the current buffer.\n\t\t */\n\t\tnumerators_since_start = buffers_since_start * numerator;\n\n\t\t/* And the number of jiffies since we started */\n\t\tjiffies_since_start = jiffies - dev->jiffies_vid_out;\n\n\t\t/* Increase by the 'numerator' of one buffer */\n\t\tnumerators_since_start += numerator;\n\t\t/*\n\t\t * Calculate when that next buffer is supposed to start\n\t\t * in jiffies since we started streaming.\n\t\t */\n\t\tnext_jiffies_since_start = numerators_since_start * HZ +\n\t\t\t\t\t   denominator / 2;\n\t\tdo_div(next_jiffies_since_start, denominator);\n\t\t/* If it is in the past, then just schedule asap */\n\t\tif (next_jiffies_since_start < jiffies_since_start)\n\t\t\tnext_jiffies_since_start = jiffies_since_start;\n\n\t\twait_jiffies = next_jiffies_since_start - jiffies_since_start;\n\t\tschedule_timeout_interruptible(wait_jiffies ? wait_jiffies : 1);\n\t}\n\tdprintk(dev, 1, \"Video Output Thread End\\n\");\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2328,
            "cve_id": "CVE-2019-19537",
            "code_snippet": "int usb_register_dev(struct usb_interface *intf,\n\t\t     struct usb_class_driver *class_driver)\n{\n\tint retval;\n\tint minor_base = class_driver->minor_base;\n\tint minor;\n\tchar name[20];\n\n#ifdef CONFIG_USB_DYNAMIC_MINORS\n\t/*\n\t * We don't care what the device tries to start at, we want to start\n\t * at zero to pack the devices into the smallest available space with\n\t * no holes in the minor range.\n\t */\n\tminor_base = 0;\n#endif\n\n\tif (class_driver->fops == NULL)\n\t\treturn -EINVAL;\n\tif (intf->minor >= 0)\n\t\treturn -EADDRINUSE;\n\n\tmutex_lock(&init_usb_class_mutex);\n\tretval = init_usb_class();\n\tmutex_unlock(&init_usb_class_mutex);\n\n\tif (retval)\n\t\treturn retval;\n\n\tdev_dbg(&intf->dev, \"looking for a minor, starting at %d\\n\", minor_base);\n\n\tdown_write(&minor_rwsem);\n\tfor (minor = minor_base; minor < MAX_USB_MINORS; ++minor) {\n\t\tif (usb_minors[minor])\n\t\t\tcontinue;\n\n\t\tusb_minors[minor] = class_driver->fops;\n\t\tintf->minor = minor;\n\t\tbreak;\n\t}\n\tup_write(&minor_rwsem);\n\tif (intf->minor < 0)\n\t\treturn -EXFULL;\n\n\t/* create a usb class device for this usb interface */\n\tsnprintf(name, sizeof(name), class_driver->name, minor - minor_base);\n\tintf->usb_dev = device_create(usb_class->class, &intf->dev,\n\t\t\t\t      MKDEV(USB_MAJOR, minor), class_driver,\n\t\t\t\t      \"%s\", kbasename(name));\n\tif (IS_ERR(intf->usb_dev)) {\n\t\tdown_write(&minor_rwsem);\n\t\tusb_minors[minor] = NULL;\n\t\tintf->minor = -1;\n\t\tup_write(&minor_rwsem);\n\t\tretval = PTR_ERR(intf->usb_dev);\n\t}\n\treturn retval;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2403,
            "cve_id": "CVE-2019-2213",
            "code_snippet": "static void\nbinder_free_buf(struct binder_proc *proc, struct binder_buffer *buffer)\n{\n\tif (buffer->transaction) {\n\t\tbuffer->transaction->buffer = NULL;\n\t\tbuffer->transaction = NULL;\n\t}\n\tif (buffer->async_transaction && buffer->target_node) {\n\t\tstruct binder_node *buf_node;\n\t\tstruct binder_work *w;\n\n\t\tbuf_node = buffer->target_node;\n\t\tbinder_node_inner_lock(buf_node);\n\t\tBUG_ON(!buf_node->has_async_transaction);\n\t\tBUG_ON(buf_node->proc != proc);\n\t\tw = binder_dequeue_work_head_ilocked(\n\t\t\t\t&buf_node->async_todo);\n\t\tif (!w) {\n\t\t\tbuf_node->has_async_transaction = false;\n\t\t} else {\n\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\tw, &proc->todo);\n\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t}\n\t\tbinder_node_inner_unlock(buf_node);\n\t}\n\ttrace_binder_transaction_buffer_release(buffer);\n\tbinder_transaction_buffer_release(proc, buffer, 0, false);\n\tbinder_alloc_free_buf(&proc->alloc, buffer);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2549,
            "cve_id": "CVE-2020-11884",
            "code_snippet": "static void __crst_table_upgrade(void *arg)\n{\n\tstruct mm_struct *mm = arg;\n\n\tif (current->active_mm == mm)\n\t\tset_user_asce(mm);\n\t__tlb_flush_local();\n}",
            "detect_result": "NO\n\nExplanation of the code behavior:\n- The function `__crst_table_upgrade` takes a single argument `arg`, which is cast to a pointer to `struct mm_struct`.\n- It checks if the current process's active memory descriptor (`current->active_mm`) matches the provided `mm` structure.\n- If they match, it calls `set_user_asce(mm)`, which likely sets up or updates the address space control entry for user space.\n- Finally, it calls `__tlb_flush_local()`, which flushes the Translation Lookaside Buffer (TLB) for the local CPU, ensuring that any changes to the page tables are reflected immediately.\n\nVulnerability assessment:\n- The code does not directly handle user input or perform operations that could lead to common vulnerabilities such as buffer overflows, format string attacks, or use-after-free errors.\n- The function assumes that the `mm` structure passed is valid and correctly points to a memory descriptor for the current process, which is a reasonable assumption within the context of kernel code.\n- There are no obvious race conditions or synchronization issues visible in this snippet alone.\n- However, without more context about how this function is called and what guarantees exist about the state of `mm` when it is passed, it is difficult to rule out all possible vulnerabilities. But based on the given code, there are no immediate red flags indicating a vulnerability.",
            "final_result": 0
        },
        {
            "id": 2551,
            "cve_id": "CVE-2020-12114",
            "code_snippet": "static void drop_mountpoint(struct fs_pin *p)\n{\n\tstruct mount *m = container_of(p, struct mount, mnt_umount);\n\tdput(m->mnt_ex_mountpoint);\n\tpin_remove(p);\n\tmntput(&m->mnt);\n}",
            "detect_result": "NO\n\nExplanation of the code behavior:\n- The function `drop_mountpoint` takes a pointer `p` to a `struct fs_pin`.\n- It uses `container_of` to get a pointer `m` to the `struct mount` that contains the `fs_pin` structure `mnt_umount`.\n- It then calls `dput` on `m->mnt_ex_mountpoint`, which likely decrements the reference count of an inode or dentry, potentially freeing it if the count reaches zero.\n- `pin_remove` is called to remove the pin associated with `p`.\n- Finally, `mntput` is called on `&m->mnt`, which decrements the reference count of the mount structure, potentially freeing it if the count reaches zero.\n\nBased on the provided code snippet, there are no obvious vulnerabilities such as buffer overflows, use-after-free, or improper memory management that can be identified. However, without more context about how this function is used and the definitions of the structures and functions involved, a comprehensive security analysis cannot be performed.",
            "final_result": 0
        },
        {
            "id": 2552,
            "cve_id": "CVE-2020-12114",
            "code_snippet": "static struct mountpoint *get_mountpoint(struct dentry *dentry)\n{\n\tstruct mountpoint *mp, *new = NULL;\n\tint ret;\n\n\tif (d_mountpoint(dentry)) {\n\t\t/* might be worth a WARN_ON() */\n\t\tif (d_unlinked(dentry))\n\t\t\treturn ERR_PTR(-ENOENT);\nmountpoint:\n\t\tread_seqlock_excl(&mount_lock);\n\t\tmp = lookup_mountpoint(dentry);\n\t\tread_sequnlock_excl(&mount_lock);\n\t\tif (mp)\n\t\t\tgoto done;\n\t}\n\n\tif (!new)\n\t\tnew = kmalloc(sizeof(struct mountpoint), GFP_KERNEL);\n\tif (!new)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\n\t/* Exactly one processes may set d_mounted */\n\tret = d_set_mounted(dentry);\n\n\t/* Someone else set d_mounted? */\n\tif (ret == -EBUSY)\n\t\tgoto mountpoint;\n\n\t/* The dentry is not available as a mountpoint? */\n\tmp = ERR_PTR(ret);\n\tif (ret)\n\t\tgoto done;\n\n\t/* Add the new mountpoint to the hash table */\n\tread_seqlock_excl(&mount_lock);\n\tnew->m_dentry = dentry;\n\tnew->m_count = 1;\n\thlist_add_head(&new->m_hash, mp_hash(dentry));\n\tINIT_HLIST_HEAD(&new->m_list);\n\tread_sequnlock_excl(&mount_lock);\n\n\tmp = new;\n\tnew = NULL;\ndone:\n\tkfree(new);\n\treturn mp;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2553,
            "cve_id": "CVE-2020-12114",
            "code_snippet": "void mnt_set_mountpoint(struct mount *mnt,\n\t\t\tstruct mountpoint *mp,\n\t\t\tstruct mount *child_mnt)\n{\n\tmp->m_count++;\n\tmnt_add_count(mnt, 1);\t/* essentially, that's mntget */\n\tchild_mnt->mnt_mountpoint = dget(mp->m_dentry);\n\tchild_mnt->mnt_parent = mnt;\n\tchild_mnt->mnt_mp = mp;\n\thlist_add_head(&child_mnt->mnt_mp_list, &mp->m_list);\n}",
            "detect_result": "NO\n\nExplanation of the code behavior:\n- The function `mnt_set_mountpoint` sets up a new mount point relationship between a parent mount (`mnt`) and a child mount (`child_mnt`) using a mountpoint structure (`mp`).\n- It increments the reference count of the mountpoint (`mp->m_count++`) and the parent mount (`mnt_add_count(mnt, 1)`).\n- It assigns the dentry (directory entry) of the mountpoint to the child mount (`child_mnt->mnt_mountpoint = dget(mp->m_dentry)`), incrementing the reference count of the dentry.\n- It sets the parent of the child mount to be the parent mount (`child_mnt->mnt_parent = mnt`).\n- It links the mountpoint structure to the child mount (`child_mnt->mnt_mp = mp`).\n- Finally, it adds the child mount to the list of mounts associated with the mountpoint (`hlist_add_head(&child_mnt->mnt_mp_list, &mp->m_list)`).\n\nBased on the provided code snippet, there are no obvious vulnerabilities such as buffer overflows, use-after-free, or improper input validation. However, without more context about how this function is used and the surrounding code, it's difficult to rule out all potential issues. But based solely on the given code, it does not exhibit any immediate security vulnerabilities.",
            "final_result": 0
        },
        {
            "id": 2554,
            "cve_id": "CVE-2020-12114",
            "code_snippet": "static void mntput_no_expire(struct mount *mnt)\n{\n\trcu_read_lock();\n\tif (likely(READ_ONCE(mnt->mnt_ns))) {\n\t\t/*\n\t\t * Since we don't do lock_mount_hash() here,\n\t\t * ->mnt_ns can change under us.  However, if it's\n\t\t * non-NULL, then there's a reference that won't\n\t\t * be dropped until after an RCU delay done after\n\t\t * turning ->mnt_ns NULL.  So if we observe it\n\t\t * non-NULL under rcu_read_lock(), the reference\n\t\t * we are dropping is not the final one.\n\t\t */\n\t\tmnt_add_count(mnt, -1);\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\tlock_mount_hash();\n\t/*\n\t * make sure that if __legitimize_mnt() has not seen us grab\n\t * mount_lock, we'll see their refcount increment here.\n\t */\n\tsmp_mb();\n\tmnt_add_count(mnt, -1);\n\tif (mnt_get_count(mnt)) {\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\treturn;\n\t}\n\tif (unlikely(mnt->mnt.mnt_flags & MNT_DOOMED)) {\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\treturn;\n\t}\n\tmnt->mnt.mnt_flags |= MNT_DOOMED;\n\trcu_read_unlock();\n\n\tlist_del(&mnt->mnt_instance);\n\n\tif (unlikely(!list_empty(&mnt->mnt_mounts))) {\n\t\tstruct mount *p, *tmp;\n\t\tlist_for_each_entry_safe(p, tmp, &mnt->mnt_mounts,  mnt_child) {\n\t\t\tumount_mnt(p);\n\t\t}\n\t}\n\tunlock_mount_hash();\n\n\tif (likely(!(mnt->mnt.mnt_flags & MNT_INTERNAL))) {\n\t\tstruct task_struct *task = current;\n\t\tif (likely(!(task->flags & PF_KTHREAD))) {\n\t\t\tinit_task_work(&mnt->mnt_rcu, __cleanup_mnt);\n\t\t\tif (!task_work_add(task, &mnt->mnt_rcu, true))\n\t\t\t\treturn;\n\t\t}\n\t\tif (llist_add(&mnt->mnt_llist, &delayed_mntput_list))\n\t\t\tschedule_delayed_work(&delayed_mntput_work, 1);\n\t\treturn;\n\t}\n\tcleanup_mnt(mnt);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2585,
            "cve_id": "CVE-2020-12652",
            "code_snippet": "static long\n__mptctl_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tmpt_ioctl_header __user *uhdr = (void __user *) arg;\n\tmpt_ioctl_header\t khdr;\n\tint iocnum;\n\tunsigned iocnumX;\n\tint nonblock = (file->f_flags & O_NONBLOCK);\n\tint ret;\n\tMPT_ADAPTER *iocp = NULL;\n\n\tif (copy_from_user(&khdr, uhdr, sizeof(khdr))) {\n\t\tprintk(KERN_ERR MYNAM \"%s::mptctl_ioctl() @%d - \"\n\t\t\t\t\"Unable to copy mpt_ioctl_header data @ %p\\n\",\n\t\t\t\t__FILE__, __LINE__, uhdr);\n\t\treturn -EFAULT;\n\t}\n\tret = -ENXIO;\t\t\t\t/* (-6) No such device or address */\n\n\t/* Verify intended MPT adapter - set iocnum and the adapter\n\t * pointer (iocp)\n\t */\n\tiocnumX = khdr.iocnum & 0xFF;\n\tif (((iocnum = mpt_verify_adapter(iocnumX, &iocp)) < 0) ||\n\t    (iocp == NULL))\n\t\treturn -ENODEV;\n\n\tif (!iocp->active) {\n\t\tprintk(KERN_DEBUG MYNAM \"%s::mptctl_ioctl() @%d - Controller disabled.\\n\",\n\t\t\t\t__FILE__, __LINE__);\n\t\treturn -EFAULT;\n\t}\n\n\t/* Handle those commands that are just returning\n\t * information stored in the driver.\n\t * These commands should never time out and are unaffected\n\t * by TM and FW reloads.\n\t */\n\tif ((cmd & ~IOCSIZE_MASK) == (MPTIOCINFO & ~IOCSIZE_MASK)) {\n\t\treturn mptctl_getiocinfo(arg, _IOC_SIZE(cmd));\n\t} else if (cmd == MPTTARGETINFO) {\n\t\treturn mptctl_gettargetinfo(arg);\n\t} else if (cmd == MPTTEST) {\n\t\treturn mptctl_readtest(arg);\n\t} else if (cmd == MPTEVENTQUERY) {\n\t\treturn mptctl_eventquery(arg);\n\t} else if (cmd == MPTEVENTENABLE) {\n\t\treturn mptctl_eventenable(arg);\n\t} else if (cmd == MPTEVENTREPORT) {\n\t\treturn mptctl_eventreport(arg);\n\t} else if (cmd == MPTFWREPLACE) {\n\t\treturn mptctl_replace_fw(arg);\n\t}\n\n\t/* All of these commands require an interrupt or\n\t * are unknown/illegal.\n\t */\n\tif ((ret = mptctl_syscall_down(iocp, nonblock)) != 0)\n\t\treturn ret;\n\n\tif (cmd == MPTFWDOWNLOAD)\n\t\tret = mptctl_fw_download(arg);\n\telse if (cmd == MPTCOMMAND)\n\t\tret = mptctl_mpt_command(arg);\n\telse if (cmd == MPTHARDRESET)\n\t\tret = mptctl_do_reset(arg);\n\telse if ((cmd & ~IOCSIZE_MASK) == (HP_GETHOSTINFO & ~IOCSIZE_MASK))\n\t\tret = mptctl_hp_hostinfo(arg, _IOC_SIZE(cmd));\n\telse if (cmd == HP_GETTARGETINFO)\n\t\tret = mptctl_hp_targetinfo(arg);\n\telse\n\t\tret = -EINVAL;\n\n\tmutex_unlock(&iocp->ioctl_cmds.mutex);\n\n\treturn ret;\n}",
            "detect_result": "YES",
            "final_result": 1
        },
        {
            "id": 2653,
            "cve_id": "CVE-2020-14416",
            "code_snippet": "static void slcan_write_wakeup(struct tty_struct *tty)\n{\n\tstruct slcan *sl = tty->disc_data;\n\n\tschedule_work(&sl->tx_work);\n}",
            "detect_result": "NO\n\nExplanation: The provided code snippet defines a function `slcan_write_wakeup` that schedules a work item (`tx_work`) associated with an `slcan` structure, which is retrieved from the `disc_data` field of a `tty_struct` object. This function itself does not directly handle any user input or perform operations that could lead to common vulnerabilities such as buffer overflows, format string vulnerabilities, or improper memory access.\n\nHowever, the safety of this function depends on the context and implementation details of the `slcan` structure and how `schedule_work` is used. If `sl` or its members are not properly initialized or managed, there could be underlying issues. But based solely on the given code snippet, there are no obvious vulnerabilities present.",
            "final_result": 0
        },
        {
            "id": 2699,
            "cve_id": "CVE-2020-25285",
            "code_snippet": "static int hugetlb_sysctl_handler_common(bool obey_mempolicy,\n\t\t\t struct ctl_table *table, int write,\n\t\t\t void *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp = h->max_huge_pages;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\ttable->data = &tmp;\n\ttable->maxlen = sizeof(unsigned long);\n\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write)\n\t\tret = __nr_hugepages_store_common(obey_mempolicy, h,\n\t\t\t\t\t\t  NUMA_NO_NODE, tmp, *length);\nout:\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2724,
            "cve_id": "CVE-2020-27067",
            "code_snippet": "static int l2tp_eth_create(struct net *net, struct l2tp_tunnel *tunnel,\n\t\t\t   u32 session_id, u32 peer_session_id,\n\t\t\t   struct l2tp_session_cfg *cfg)\n{\n\tunsigned char name_assign_type;\n\tstruct net_device *dev;\n\tchar name[IFNAMSIZ];\n\tstruct l2tp_session *session;\n\tstruct l2tp_eth *priv;\n\tstruct l2tp_eth_sess *spriv;\n\tint rc;\n\tstruct l2tp_eth_net *pn;\n\n\tif (cfg->ifname) {\n\t\tstrlcpy(name, cfg->ifname, IFNAMSIZ);\n\t\tname_assign_type = NET_NAME_USER;\n\t} else {\n\t\tstrcpy(name, L2TP_ETH_DEV_NAME);\n\t\tname_assign_type = NET_NAME_ENUM;\n\t}\n\n\tsession = l2tp_session_create(sizeof(*spriv), tunnel, session_id,\n\t\t\t\t      peer_session_id, cfg);\n\tif (IS_ERR(session)) {\n\t\trc = PTR_ERR(session);\n\t\tgoto out;\n\t}\n\n\tdev = alloc_netdev(sizeof(*priv), name, name_assign_type,\n\t\t\t   l2tp_eth_dev_setup);\n\tif (!dev) {\n\t\trc = -ENOMEM;\n\t\tgoto out_del_session;\n\t}\n\n\tdev_net_set(dev, net);\n\tdev->min_mtu = 0;\n\tdev->max_mtu = ETH_MAX_MTU;\n\tl2tp_eth_adjust_mtu(tunnel, session, dev);\n\n\tpriv = netdev_priv(dev);\n\tpriv->dev = dev;\n\tpriv->session = session;\n\tINIT_LIST_HEAD(&priv->list);\n\n\tpriv->tunnel_sock = tunnel->sock;\n\tsession->recv_skb = l2tp_eth_dev_recv;\n\tsession->session_close = l2tp_eth_delete;\n#if IS_ENABLED(CONFIG_L2TP_DEBUGFS)\n\tsession->show = l2tp_eth_show;\n#endif\n\n\tspriv = l2tp_session_priv(session);\n\tspriv->dev = dev;\n\n\trc = register_netdev(dev);\n\tif (rc < 0)\n\t\tgoto out_del_dev;\n\n\t__module_get(THIS_MODULE);\n\t/* Must be done after register_netdev() */\n\tstrlcpy(session->ifname, dev->name, IFNAMSIZ);\n\n\tdev_hold(dev);\n\tpn = l2tp_eth_pernet(dev_net(dev));\n\tspin_lock(&pn->l2tp_eth_lock);\n\tlist_add(&priv->list, &pn->l2tp_eth_dev_list);\n\tspin_unlock(&pn->l2tp_eth_lock);\n\n\treturn 0;\n\nout_del_dev:\n\tfree_netdev(dev);\n\tspriv->dev = NULL;\nout_del_session:\n\tl2tp_session_delete(session);\nout:\n\treturn rc;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2734,
            "cve_id": "CVE-2020-27675",
            "code_snippet": "static void clear_evtchn_to_irq_row(unsigned row)\n{\n\tunsigned col;\n\n\tfor (col = 0; col < EVTCHN_PER_ROW; col++)\n\t\tevtchn_to_irq[row][col] = -1;\n}",
            "detect_result": "NO\n\nExplanation: The provided code snippet defines a function `clear_evtchn_to_irq_row` that takes an unsigned integer `row` as an argument and sets all elements in the `col` dimension of a two-dimensional array `evtchn_to_irq[row][col]` to `-1`. The loop iterates from `0` to `EVTCHN_PER_ROW - 1`, which suggests that the bounds of the array are being respected within the function itself. There is no indication of buffer overflow or other common vulnerabilities in this specific code snippet. However, it's important to note that the safety of this function also depends on how `row` is validated before calling this function and the overall context in which `evtchn_to_irq` is used.",
            "final_result": 0
        },
        {
            "id": 2735,
            "cve_id": "CVE-2020-27675",
            "code_snippet": "static void xen_free_irq(unsigned irq)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\n\tif (WARN_ON(!info))\n\t\treturn;\n\n\tlist_del(&info->list);\n\n\tset_info_for_irq(irq, NULL);\n\n\tWARN_ON(info->refcnt > 0);\n\n\tkfree(info);\n\n\t/* Legacy IRQ descriptors are managed by the arch. */\n\tif (irq < nr_legacy_irqs())\n\t\treturn;\n\n\tirq_free_desc(irq);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2750,
            "cve_id": "CVE-2020-27825",
            "code_snippet": "void ring_buffer_reset_online_cpus(struct trace_buffer *buffer)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tint cpu;\n\n\tfor_each_online_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\tatomic_inc(&cpu_buffer->resize_disabled);\n\t\tatomic_inc(&cpu_buffer->record_disabled);\n\t}\n\n\t/* Make sure all commits have finished */\n\tsynchronize_rcu();\n\n\tfor_each_online_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\treset_disabled_cpu_buffer(cpu_buffer);\n\n\t\tatomic_dec(&cpu_buffer->record_disabled);\n\t\tatomic_dec(&cpu_buffer->resize_disabled);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2776,
            "cve_id": "CVE-2020-29369",
            "code_snippet": "int __do_munmap(struct mm_struct *mm, unsigned long start, size_t len,\n\t\tstruct list_head *uf, bool downgrade)\n{\n\tunsigned long end;\n\tstruct vm_area_struct *vma, *prev, *last;\n\n\tif ((offset_in_page(start)) || start > TASK_SIZE || len > TASK_SIZE-start)\n\t\treturn -EINVAL;\n\n\tlen = PAGE_ALIGN(len);\n\tend = start + len;\n\tif (len == 0)\n\t\treturn -EINVAL;\n\n\t/*\n\t * arch_unmap() might do unmaps itself.  It must be called\n\t * and finish any rbtree manipulation before this code\n\t * runs and also starts to manipulate the rbtree.\n\t */\n\tarch_unmap(mm, start, end);\n\n\t/* Find the first overlapping VMA */\n\tvma = find_vma(mm, start);\n\tif (!vma)\n\t\treturn 0;\n\tprev = vma->vm_prev;\n\t/* we have  start < vma->vm_end  */\n\n\t/* if it doesn't overlap, we have nothing.. */\n\tif (vma->vm_start >= end)\n\t\treturn 0;\n\n\t/*\n\t * If we need to split any vma, do it now to save pain later.\n\t *\n\t * Note: mremap's move_vma VM_ACCOUNT handling assumes a partially\n\t * unmapped vm_area_struct will remain in use: so lower split_vma\n\t * places tmp vma above, and higher split_vma places tmp vma below.\n\t */\n\tif (start > vma->vm_start) {\n\t\tint error;\n\n\t\t/*\n\t\t * Make sure that map_count on return from munmap() will\n\t\t * not exceed its limit; but let map_count go just above\n\t\t * its limit temporarily, to help free resources as expected.\n\t\t */\n\t\tif (end < vma->vm_end && mm->map_count >= sysctl_max_map_count)\n\t\t\treturn -ENOMEM;\n\n\t\terror = __split_vma(mm, vma, start, 0);\n\t\tif (error)\n\t\t\treturn error;\n\t\tprev = vma;\n\t}\n\n\t/* Does it split the last one? */\n\tlast = find_vma(mm, end);\n\tif (last && end > last->vm_start) {\n\t\tint error = __split_vma(mm, last, end, 1);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\tvma = prev ? prev->vm_next : mm->mmap;\n\n\tif (unlikely(uf)) {\n\t\t/*\n\t\t * If userfaultfd_unmap_prep returns an error the vmas\n\t\t * will remain splitted, but userland will get a\n\t\t * highly unexpected error anyway. This is no\n\t\t * different than the case where the first of the two\n\t\t * __split_vma fails, but we don't undo the first\n\t\t * split, despite we could. This is unlikely enough\n\t\t * failure that it's not worth optimizing it for.\n\t\t */\n\t\tint error = userfaultfd_unmap_prep(vma, start, end, uf);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\t/*\n\t * unlock any mlock()ed ranges before detaching vmas\n\t */\n\tif (mm->locked_vm) {\n\t\tstruct vm_area_struct *tmp = vma;\n\t\twhile (tmp && tmp->vm_start < end) {\n\t\t\tif (tmp->vm_flags & VM_LOCKED) {\n\t\t\t\tmm->locked_vm -= vma_pages(tmp);\n\t\t\t\tmunlock_vma_pages_all(tmp);\n\t\t\t}\n\n\t\t\ttmp = tmp->vm_next;\n\t\t}\n\t}\n\n\t/* Detach vmas from rbtree */\n\tdetach_vmas_to_be_unmapped(mm, vma, prev, end);\n\n\tif (downgrade)\n\t\tmmap_write_downgrade(mm);\n\n\tunmap_region(mm, vma, prev, start, end);\n\n\t/* Fix up all other VM information */\n\tremove_vma_list(mm, vma);\n\n\treturn downgrade ? 1 : 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2783,
            "cve_id": "CVE-2020-29374",
            "code_snippet": "static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n\t\tunsigned long start, unsigned long nr_pages,\n\t\tunsigned int gup_flags, struct page **pages,\n\t\tstruct vm_area_struct **vmas, int *locked)\n{\n\tlong ret = 0, i = 0;\n\tstruct vm_area_struct *vma = NULL;\n\tstruct follow_page_context ctx = { NULL };\n\n\tif (!nr_pages)\n\t\treturn 0;\n\n\tstart = untagged_addr(start);\n\n\tVM_BUG_ON(!!pages != !!(gup_flags & (FOLL_GET | FOLL_PIN)));\n\n\t/*\n\t * If FOLL_FORCE is set then do not force a full fault as the hinting\n\t * fault information is unrelated to the reference behaviour of a task\n\t * using the address space\n\t */\n\tif (!(gup_flags & FOLL_FORCE))\n\t\tgup_flags |= FOLL_NUMA;\n\n\tdo {\n\t\tstruct page *page;\n\t\tunsigned int foll_flags = gup_flags;\n\t\tunsigned int page_increm;\n\n\t\t/* first iteration or cross vma bound */\n\t\tif (!vma || start >= vma->vm_end) {\n\t\t\tvma = find_extend_vma(mm, start);\n\t\t\tif (!vma && in_gate_area(mm, start)) {\n\t\t\t\tret = get_gate_page(mm, start & PAGE_MASK,\n\t\t\t\t\t\tgup_flags, &vma,\n\t\t\t\t\t\tpages ? &pages[i] : NULL);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto out;\n\t\t\t\tctx.page_mask = 0;\n\t\t\t\tgoto next_page;\n\t\t\t}\n\n\t\t\tif (!vma || check_vma_flags(vma, gup_flags)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (is_vm_hugetlb_page(vma)) {\n\t\t\t\ti = follow_hugetlb_page(mm, vma, pages, vmas,\n\t\t\t\t\t\t&start, &nr_pages, i,\n\t\t\t\t\t\tgup_flags, locked);\n\t\t\t\tif (locked && *locked == 0) {\n\t\t\t\t\t/*\n\t\t\t\t\t * We've got a VM_FAULT_RETRY\n\t\t\t\t\t * and we've lost mmap_sem.\n\t\t\t\t\t * We must stop here.\n\t\t\t\t\t */\n\t\t\t\t\tBUG_ON(gup_flags & FOLL_NOWAIT);\n\t\t\t\t\tBUG_ON(ret != 0);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\nretry:\n\t\t/*\n\t\t * If we have a pending SIGKILL, don't keep faulting pages and\n\t\t * potentially allocating memory.\n\t\t */\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tgoto out;\n\t\t}\n\t\tcond_resched();\n\n\t\tpage = follow_page_mask(vma, start, foll_flags, &ctx);\n\t\tif (!page) {\n\t\t\tret = faultin_page(tsk, vma, start, &foll_flags,\n\t\t\t\t\t   locked);\n\t\t\tswitch (ret) {\n\t\t\tcase 0:\n\t\t\t\tgoto retry;\n\t\t\tcase -EBUSY:\n\t\t\t\tret = 0;\n\t\t\t\tfallthrough;\n\t\t\tcase -EFAULT:\n\t\t\tcase -ENOMEM:\n\t\t\tcase -EHWPOISON:\n\t\t\t\tgoto out;\n\t\t\tcase -ENOENT:\n\t\t\t\tgoto next_page;\n\t\t\t}\n\t\t\tBUG();\n\t\t} else if (PTR_ERR(page) == -EEXIST) {\n\t\t\t/*\n\t\t\t * Proper page table entry exists, but no corresponding\n\t\t\t * struct page.\n\t\t\t */\n\t\t\tgoto next_page;\n\t\t} else if (IS_ERR(page)) {\n\t\t\tret = PTR_ERR(page);\n\t\t\tgoto out;\n\t\t}\n\t\tif (pages) {\n\t\t\tpages[i] = page;\n\t\t\tflush_anon_page(vma, page, start);\n\t\t\tflush_dcache_page(page);\n\t\t\tctx.page_mask = 0;\n\t\t}\nnext_page:\n\t\tif (vmas) {\n\t\t\tvmas[i] = vma;\n\t\t\tctx.page_mask = 0;\n\t\t}\n\t\tpage_increm = 1 + (~(start >> PAGE_SHIFT) & ctx.page_mask);\n\t\tif (page_increm > nr_pages)\n\t\t\tpage_increm = nr_pages;\n\t\ti += page_increm;\n\t\tstart += page_increm * PAGE_SIZE;\n\t\tnr_pages -= page_increm;\n\t} while (nr_pages);\nout:\n\tif (ctx.pgmap)\n\t\tput_dev_pagemap(ctx.pgmap);\n\treturn i ? i : ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2784,
            "cve_id": "CVE-2020-29374",
            "code_snippet": "static inline bool can_follow_write_pte(pte_t pte, unsigned int flags)\n{\n\treturn pte_write(pte) ||\n\t\t((flags & FOLL_FORCE) && (flags & FOLL_COW) && pte_dirty(pte));\n}",
            "detect_result": "NO\n\nExplanation of the code behavior:\nThe function `can_follow_write_pte` checks if a page table entry (`pte`) allows for write access under certain conditions. It returns `true` if either:\n1. The page is already writable (`pte_write(pte)`).\n2. The flags include both `FOLL_FORCE` and `FOLL_COW`, and the page is marked as dirty (`pte_dirty(pte)`).\n\nVulnerability assessment:\nThe provided code snippet does not directly indicate any security vulnerabilities. It appears to be a logical check within a memory management context, likely in an operating system kernel. Without additional context about how this function is used or the broader system it operates in, it's challenging to identify specific vulnerabilities. However, based solely on the given code, there are no obvious security issues such as buffer overflows, improper input validation, or insecure memory handling.",
            "final_result": 0
        },
        {
            "id": 2876,
            "cve_id": "CVE-2020-36557",
            "code_snippet": "int vc_allocate(unsigned int currcons)\t/* return 0 on success */\n{\n\tstruct vt_notifier_param param;\n\tstruct vc_data *vc;\n\n\tWARN_CONSOLE_UNLOCKED();\n\n\tif (currcons >= MAX_NR_CONSOLES)\n\t\treturn -ENXIO;\n\n\tif (vc_cons[currcons].d)\n\t\treturn 0;\n\n\t/* due to the granularity of kmalloc, we waste some memory here */\n\t/* the alloc is done in two steps, to optimize the common situation\n\t   of a 25x80 console (structsize=216, screenbuf_size=4000) */\n\t/* although the numbers above are not valid since long ago, the\n\t   point is still up-to-date and the comment still has its value\n\t   even if only as a historical artifact.  --mj, July 1998 */\n\tparam.vc = vc = kzalloc(sizeof(struct vc_data), GFP_KERNEL);\n\tif (!vc)\n\t\treturn -ENOMEM;\n\n\tvc_cons[currcons].d = vc;\n\ttty_port_init(&vc->port);\n\tINIT_WORK(&vc_cons[currcons].SAK_work, vc_SAK);\n\n\tvisual_init(vc, currcons, 1);\n\n\tif (!*vc->vc_uni_pagedir_loc)\n\t\tcon_set_default_unimap(vc);\n\n\tvc->vc_screenbuf = kzalloc(vc->vc_screenbuf_size, GFP_KERNEL);\n\tif (!vc->vc_screenbuf)\n\t\tgoto err_free;\n\n\t/* If no drivers have overridden us and the user didn't pass a\n\t   boot option, default to displaying the cursor */\n\tif (global_cursor_default == -1)\n\t\tglobal_cursor_default = 1;\n\n\tvc_init(vc, vc->vc_rows, vc->vc_cols, 1);\n\tvcs_make_sysfs(currcons);\n\tatomic_notifier_call_chain(&vt_notifier_list, VT_ALLOCATE, &param);\n\n\treturn 0;\nerr_free:\n\tvisual_deinit(vc);\n\tkfree(vc);\n\tvc_cons[currcons].d = NULL;\n\treturn -ENOMEM;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2946,
            "cve_id": "CVE-2021-0920",
            "code_snippet": "static int unix_dgram_recvmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t      size_t size, int flags)\n{\n\tstruct scm_cookie scm;\n\tstruct sock *sk = sock->sk;\n\tstruct unix_sock *u = unix_sk(sk);\n\tstruct sk_buff *skb, *last;\n\tlong timeo;\n\tint skip;\n\tint err;\n\n\terr = -EOPNOTSUPP;\n\tif (flags&MSG_OOB)\n\t\tgoto out;\n\n\ttimeo = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\n\tdo {\n\t\tmutex_lock(&u->iolock);\n\n\t\tskip = sk_peek_offset(sk, flags);\n\t\tskb = __skb_try_recv_datagram(sk, &sk->sk_receive_queue, flags,\n\t\t\t\t\t      &skip, &err, &last);\n\t\tif (skb) {\n\t\t\tif (!(flags & MSG_PEEK))\n\t\t\t\tscm_stat_del(sk, skb);\n\t\t\tbreak;\n\t\t}\n\n\t\tmutex_unlock(&u->iolock);\n\n\t\tif (err != -EAGAIN)\n\t\t\tbreak;\n\t} while (timeo &&\n\t\t !__skb_wait_for_more_packets(sk, &sk->sk_receive_queue,\n\t\t\t\t\t      &err, &timeo, last));\n\n\tif (!skb) { /* implies iolock unlocked */\n\t\tunix_state_lock(sk);\n\t\t/* Signal EOF on disconnected non-blocking SEQPACKET socket. */\n\t\tif (sk->sk_type == SOCK_SEQPACKET && err == -EAGAIN &&\n\t\t    (sk->sk_shutdown & RCV_SHUTDOWN))\n\t\t\terr = 0;\n\t\tunix_state_unlock(sk);\n\t\tgoto out;\n\t}\n\n\tif (wq_has_sleeper(&u->peer_wait))\n\t\twake_up_interruptible_sync_poll(&u->peer_wait,\n\t\t\t\t\t\tEPOLLOUT | EPOLLWRNORM |\n\t\t\t\t\t\tEPOLLWRBAND);\n\n\tif (msg->msg_name)\n\t\tunix_copy_addr(msg, skb->sk);\n\n\tif (size > skb->len - skip)\n\t\tsize = skb->len - skip;\n\telse if (size < skb->len - skip)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\terr = skb_copy_datagram_msg(skb, skip, msg, size);\n\tif (err)\n\t\tgoto out_free;\n\n\tif (sock_flag(sk, SOCK_RCVTSTAMP))\n\t\t__sock_recv_timestamp(msg, sk, skb);\n\n\tmemset(&scm, 0, sizeof(scm));\n\n\tscm_set_cred(&scm, UNIXCB(skb).pid, UNIXCB(skb).uid, UNIXCB(skb).gid);\n\tunix_set_secdata(&scm, skb);\n\n\tif (!(flags & MSG_PEEK)) {\n\t\tif (UNIXCB(skb).fp)\n\t\t\tunix_detach_fds(&scm, skb);\n\n\t\tsk_peek_offset_bwd(sk, skb->len);\n\t} else {\n\t\t/* It is questionable: on PEEK we could:\n\t\t   - do not return fds - good, but too simple 8)\n\t\t   - return fds, and do not return them on read (old strategy,\n\t\t     apparently wrong)\n\t\t   - clone fds (I chose it for now, it is the most universal\n\t\t     solution)\n\n\t\t   POSIX 1003.1g does not actually define this clearly\n\t\t   at all. POSIX 1003.1g doesn't define a lot of things\n\t\t   clearly however!\n\n\t\t*/\n\n\t\tsk_peek_offset_fwd(sk, size);\n\n\t\tif (UNIXCB(skb).fp)\n\t\t\tscm.fp = scm_fp_dup(UNIXCB(skb).fp);\n\t}\n\terr = (flags & MSG_TRUNC) ? skb->len - skip : size;\n\n\tscm_recv(sock, msg, &scm, flags);\n\nout_free:\n\tskb_free_datagram(sk, skb);\n\tmutex_unlock(&u->iolock);\nout:\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2977,
            "cve_id": "CVE-2021-20261",
            "code_snippet": "static unsigned int floppy_check_events(struct gendisk *disk,\n\t\t\t\t\tunsigned int clearing)\n{\n\tint drive = (long)disk->private_data;\n\n\tif (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n\t    test_bit(FD_VERIFY_BIT, &UDRS->flags))\n\t\treturn DISK_EVENT_MEDIA_CHANGE;\n\n\tif (time_after(jiffies, UDRS->last_checked + UDP->checkfreq)) {\n\t\tlock_fdc(drive, false);\n\t\tpoll_drive(false, 0);\n\t\tprocess_fd_request();\n\t}\n\n\tif (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n\t    test_bit(FD_VERIFY_BIT, &UDRS->flags) ||\n\t    test_bit(drive, &fake_change) ||\n\t    drive_no_geom(drive))\n\t\treturn DISK_EVENT_MEDIA_CHANGE;\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2978,
            "cve_id": "CVE-2021-20261",
            "code_snippet": "static int user_reset_fdc(int drive, int arg, bool interruptible)\n{\n\tint ret;\n\n\tif (lock_fdc(drive, interruptible))\n\t\treturn -EINTR;\n\n\tif (arg == FD_RESET_ALWAYS)\n\t\tFDCS->reset = 1;\n\tif (FDCS->reset) {\n\t\tcont = &reset_cont;\n\t\tret = wait_til_done(reset_fdc, interruptible);\n\t\tif (ret == -EINTR)\n\t\t\treturn -EINTR;\n\t}\n\tprocess_fd_request();\n\treturn 0;\n}",
            "detect_result": "NO\n\nExplanation of the code behavior:\n- The function `user_reset_fdc` takes three parameters: `drive`, `arg`, and `interruptible`.\n- It first attempts to acquire a lock on the FDC (Floppy Disk Controller) for the specified `drive`. If the lock cannot be acquired due to an interruptible condition, it returns `-EINTR`.\n- If `arg` is equal to `FD_RESET_ALWAYS`, it sets the `reset` flag in the `FDCS` structure to 1.\n- If the `reset` flag in `FDCS` is set, it assigns a pointer to `reset_cont` to `cont` and calls `wait_til_done` with `reset_fdc` and `interruptible` as arguments. If `wait_til_done` returns `-EINTR`, the function returns `-EINTR`.\n- Finally, it calls `process_fd_request()` and returns 0.\n\nVulnerability assessment:\n- The code does not show any obvious buffer overflows, format string vulnerabilities, or other common security issues.\n- The use of `lock_fdc` suggests that there are mechanisms in place to prevent race conditions.\n- The function checks for interrupts and handles them appropriately by returning `-EINTR`.\n- There is no indication of improper input validation or handling that could lead to vulnerabilities.\n\nTherefore, based on the provided code snippet, there are no apparent vulnerabilities. However, a more comprehensive analysis would require examining the definitions and behaviors of functions like `lock_fdc`, `wait_til_done`, and `process_fd_request()`, as well as the context in which `user_reset_fdc` is used.",
            "final_result": 0
        },
        {
            "id": 3003,
            "cve_id": "CVE-2021-23133",
            "code_snippet": "static int sctp_init_sock(struct sock *sk)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct sctp_sock *sp;\n\n\tpr_debug(\"%s: sk:%p\\n\", __func__, sk);\n\n\tsp = sctp_sk(sk);\n\n\t/* Initialize the SCTP per socket area.  */\n\tswitch (sk->sk_type) {\n\tcase SOCK_SEQPACKET:\n\t\tsp->type = SCTP_SOCKET_UDP;\n\t\tbreak;\n\tcase SOCK_STREAM:\n\t\tsp->type = SCTP_SOCKET_TCP;\n\t\tbreak;\n\tdefault:\n\t\treturn -ESOCKTNOSUPPORT;\n\t}\n\n\tsk->sk_gso_type = SKB_GSO_SCTP;\n\n\t/* Initialize default send parameters. These parameters can be\n\t * modified with the SCTP_DEFAULT_SEND_PARAM socket option.\n\t */\n\tsp->default_stream = 0;\n\tsp->default_ppid = 0;\n\tsp->default_flags = 0;\n\tsp->default_context = 0;\n\tsp->default_timetolive = 0;\n\n\tsp->default_rcv_context = 0;\n\tsp->max_burst = net->sctp.max_burst;\n\n\tsp->sctp_hmac_alg = net->sctp.sctp_hmac_alg;\n\n\t/* Initialize default setup parameters. These parameters\n\t * can be modified with the SCTP_INITMSG socket option or\n\t * overridden by the SCTP_INIT CMSG.\n\t */\n\tsp->initmsg.sinit_num_ostreams   = sctp_max_outstreams;\n\tsp->initmsg.sinit_max_instreams  = sctp_max_instreams;\n\tsp->initmsg.sinit_max_attempts   = net->sctp.max_retrans_init;\n\tsp->initmsg.sinit_max_init_timeo = net->sctp.rto_max;\n\n\t/* Initialize default RTO related parameters.  These parameters can\n\t * be modified for with the SCTP_RTOINFO socket option.\n\t */\n\tsp->rtoinfo.srto_initial = net->sctp.rto_initial;\n\tsp->rtoinfo.srto_max     = net->sctp.rto_max;\n\tsp->rtoinfo.srto_min     = net->sctp.rto_min;\n\n\t/* Initialize default association related parameters. These parameters\n\t * can be modified with the SCTP_ASSOCINFO socket option.\n\t */\n\tsp->assocparams.sasoc_asocmaxrxt = net->sctp.max_retrans_association;\n\tsp->assocparams.sasoc_number_peer_destinations = 0;\n\tsp->assocparams.sasoc_peer_rwnd = 0;\n\tsp->assocparams.sasoc_local_rwnd = 0;\n\tsp->assocparams.sasoc_cookie_life = net->sctp.valid_cookie_life;\n\n\t/* Initialize default event subscriptions. By default, all the\n\t * options are off.\n\t */\n\tsp->subscribe = 0;\n\n\t/* Default Peer Address Parameters.  These defaults can\n\t * be modified via SCTP_PEER_ADDR_PARAMS\n\t */\n\tsp->hbinterval  = net->sctp.hb_interval;\n\tsp->udp_port    = htons(net->sctp.udp_port);\n\tsp->encap_port  = htons(net->sctp.encap_port);\n\tsp->pathmaxrxt  = net->sctp.max_retrans_path;\n\tsp->pf_retrans  = net->sctp.pf_retrans;\n\tsp->ps_retrans  = net->sctp.ps_retrans;\n\tsp->pf_expose   = net->sctp.pf_expose;\n\tsp->pathmtu     = 0; /* allow default discovery */\n\tsp->sackdelay   = net->sctp.sack_timeout;\n\tsp->sackfreq\t= 2;\n\tsp->param_flags = SPP_HB_ENABLE |\n\t\t\t  SPP_PMTUD_ENABLE |\n\t\t\t  SPP_SACKDELAY_ENABLE;\n\tsp->default_ss = SCTP_SS_DEFAULT;\n\n\t/* If enabled no SCTP message fragmentation will be performed.\n\t * Configure through SCTP_DISABLE_FRAGMENTS socket option.\n\t */\n\tsp->disable_fragments = 0;\n\n\t/* Enable Nagle algorithm by default.  */\n\tsp->nodelay           = 0;\n\n\tsp->recvrcvinfo = 0;\n\tsp->recvnxtinfo = 0;\n\n\t/* Enable by default. */\n\tsp->v4mapped          = 1;\n\n\t/* Auto-close idle associations after the configured\n\t * number of seconds.  A value of 0 disables this\n\t * feature.  Configure through the SCTP_AUTOCLOSE socket option,\n\t * for UDP-style sockets only.\n\t */\n\tsp->autoclose         = 0;\n\n\t/* User specified fragmentation limit. */\n\tsp->user_frag         = 0;\n\n\tsp->adaptation_ind = 0;\n\n\tsp->pf = sctp_get_pf_specific(sk->sk_family);\n\n\t/* Control variables for partial data delivery. */\n\tatomic_set(&sp->pd_mode, 0);\n\tskb_queue_head_init(&sp->pd_lobby);\n\tsp->frag_interleave = 0;\n\n\t/* Create a per socket endpoint structure.  Even if we\n\t * change the data structure relationships, this may still\n\t * be useful for storing pre-connect address information.\n\t */\n\tsp->ep = sctp_endpoint_new(sk, GFP_KERNEL);\n\tif (!sp->ep)\n\t\treturn -ENOMEM;\n\n\tsp->hmac = NULL;\n\n\tsk->sk_destruct = sctp_destruct_sock;\n\n\tSCTP_DBG_OBJCNT_INC(sock);\n\n\tlocal_bh_disable();\n\tsk_sockets_allocated_inc(sk);\n\tsock_prot_inuse_add(net, sk->sk_prot, 1);\n\n\t/* Nothing can fail after this block, otherwise\n\t * sctp_destroy_sock() will be called without addr_wq_lock held\n\t */\n\tif (net->sctp.default_auto_asconf) {\n\t\tspin_lock(&sock_net(sk)->sctp.addr_wq_lock);\n\t\tlist_add_tail(&sp->auto_asconf_list,\n\t\t    &net->sctp.auto_asconf_splist);\n\t\tsp->do_auto_asconf = 1;\n\t\tspin_unlock(&sock_net(sk)->sctp.addr_wq_lock);\n\t} else {\n\t\tsp->do_auto_asconf = 0;\n\t}\n\n\tlocal_bh_enable();\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3150,
            "cve_id": "CVE-2021-3609",
            "code_snippet": "static int bcm_delete_rx_op(struct list_head *ops, struct bcm_msg_head *mh,\n\t\t\t    int ifindex)\n{\n\tstruct bcm_op *op, *n;\n\n\tlist_for_each_entry_safe(op, n, ops, list) {\n\t\tif ((op->can_id == mh->can_id) && (op->ifindex == ifindex) &&\n\t\t    (op->flags & CAN_FD_FRAME) == (mh->flags & CAN_FD_FRAME)) {\n\n\t\t\t/*\n\t\t\t * Don't care if we're bound or not (due to netdev\n\t\t\t * problems) can_rx_unregister() is always a save\n\t\t\t * thing to do here.\n\t\t\t */\n\t\t\tif (op->ifindex) {\n\t\t\t\t/*\n\t\t\t\t * Only remove subscriptions that had not\n\t\t\t\t * been removed due to NETDEV_UNREGISTER\n\t\t\t\t * in bcm_notifier()\n\t\t\t\t */\n\t\t\t\tif (op->rx_reg_dev) {\n\t\t\t\t\tstruct net_device *dev;\n\n\t\t\t\t\tdev = dev_get_by_index(sock_net(op->sk),\n\t\t\t\t\t\t\t       op->ifindex);\n\t\t\t\t\tif (dev) {\n\t\t\t\t\t\tbcm_rx_unreg(dev, op);\n\t\t\t\t\t\tdev_put(dev);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else\n\t\t\t\tcan_rx_unregister(sock_net(op->sk), NULL,\n\t\t\t\t\t\t  op->can_id,\n\t\t\t\t\t\t  REGMASK(op->can_id),\n\t\t\t\t\t\t  bcm_rx_handler, op);\n\n\t\t\tlist_del(&op->list);\n\t\t\tbcm_remove_op(op);\n\t\t\treturn 1; /* done */\n\t\t}\n\t}\n\n\treturn 0; /* not found */\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3182,
            "cve_id": "CVE-2021-3752",
            "code_snippet": "static void l2cap_sock_destruct(struct sock *sk)\n{\n\tBT_DBG(\"sk %p\", sk);\n\n\tif (l2cap_pi(sk)->chan)\n\t\tl2cap_chan_put(l2cap_pi(sk)->chan);\n\n\tif (l2cap_pi(sk)->rx_busy_skb) {\n\t\tkfree_skb(l2cap_pi(sk)->rx_busy_skb);\n\t\tl2cap_pi(sk)->rx_busy_skb = NULL;\n\t}\n\n\tskb_queue_purge(&sk->sk_receive_queue);\n\tskb_queue_purge(&sk->sk_write_queue);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3229,
            "cve_id": "CVE-2021-39686",
            "code_snippet": "static void binder_transaction(struct binder_proc *proc,\n\t\t\t       struct binder_thread *thread,\n\t\t\t       struct binder_transaction_data *tr, int reply,\n\t\t\t       binder_size_t extra_buffers_size)\n{\n\tint ret;\n\tstruct binder_transaction *t;\n\tstruct binder_work *w;\n\tstruct binder_work *tcomplete;\n\tbinder_size_t buffer_offset = 0;\n\tbinder_size_t off_start_offset, off_end_offset;\n\tbinder_size_t off_min;\n\tbinder_size_t sg_buf_offset, sg_buf_end_offset;\n\tstruct binder_proc *target_proc = NULL;\n\tstruct binder_thread *target_thread = NULL;\n\tstruct binder_node *target_node = NULL;\n\tstruct binder_transaction *in_reply_to = NULL;\n\tstruct binder_transaction_log_entry *e;\n\tuint32_t return_error = 0;\n\tuint32_t return_error_param = 0;\n\tuint32_t return_error_line = 0;\n\tbinder_size_t last_fixup_obj_off = 0;\n\tbinder_size_t last_fixup_min_off = 0;\n\tstruct binder_context *context = proc->context;\n\tint t_debug_id = atomic_inc_return(&binder_last_id);\n\tchar *secctx = NULL;\n\tu32 secctx_sz = 0;\n\n\te = binder_transaction_log_add(&binder_transaction_log);\n\te->debug_id = t_debug_id;\n\te->call_type = reply ? 2 : !!(tr->flags & TF_ONE_WAY);\n\te->from_proc = proc->pid;\n\te->from_thread = thread->pid;\n\te->target_handle = tr->target.handle;\n\te->data_size = tr->data_size;\n\te->offsets_size = tr->offsets_size;\n\tstrscpy(e->context_name, proc->context->name, BINDERFS_MAX_NAME);\n\n\tif (reply) {\n\t\tbinder_inner_proc_lock(proc);\n\t\tin_reply_to = thread->transaction_stack;\n\t\tif (in_reply_to == NULL) {\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with no transaction stack\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_empty_call_stack;\n\t\t}\n\t\tif (in_reply_to->to_thread != thread) {\n\t\t\tspin_lock(&in_reply_to->lock);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\tproc->pid, thread->pid, in_reply_to->debug_id,\n\t\t\t\tin_reply_to->to_proc ?\n\t\t\t\tin_reply_to->to_proc->pid : 0,\n\t\t\t\tin_reply_to->to_thread ?\n\t\t\t\tin_reply_to->to_thread->pid : 0);\n\t\t\tspin_unlock(&in_reply_to->lock);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\tgoto err_bad_call_stack;\n\t\t}\n\t\tthread->transaction_stack = in_reply_to->to_parent;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_set_nice(in_reply_to->saved_priority);\n\t\ttarget_thread = binder_get_txn_from_and_acq_inner(in_reply_to);\n\t\tif (target_thread == NULL) {\n\t\t\t/* annotation for sparse */\n\t\t\t__release(&target_thread->proc->inner_lock);\n\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\tif (target_thread->transaction_stack != in_reply_to) {\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad target transaction stack %d, expected %d\\n\",\n\t\t\t\tproc->pid, thread->pid,\n\t\t\t\ttarget_thread->transaction_stack ?\n\t\t\t\ttarget_thread->transaction_stack->debug_id : 0,\n\t\t\t\tin_reply_to->debug_id);\n\t\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\ttarget_thread = NULL;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\ttarget_proc = target_thread->proc;\n\t\ttarget_proc->tmp_ref++;\n\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t} else {\n\t\tif (tr->target.handle) {\n\t\t\tstruct binder_ref *ref;\n\n\t\t\t/*\n\t\t\t * There must already be a strong ref\n\t\t\t * on this node. If so, do a strong\n\t\t\t * increment on the node to ensure it\n\t\t\t * stays alive until the transaction is\n\t\t\t * done.\n\t\t\t */\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, tr->target.handle,\n\t\t\t\t\t\t     true);\n\t\t\tif (ref) {\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\tref->node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\t} else {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to invalid handle, %u\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid, tr->target.handle);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t}\n\t\t\tbinder_proc_unlock(proc);\n\t\t} else {\n\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\ttarget_node = context->binder_context_mgr_node;\n\t\t\tif (target_node)\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\ttarget_node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\telse\n\t\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\tif (target_node && target_proc->pid == proc->pid) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to context manager from process owning it\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_invalid_target_handle;\n\t\t\t}\n\t\t}\n\t\tif (!target_node) {\n\t\t\t/*\n\t\t\t * return_error is set above\n\t\t\t */\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\te->to_node = target_node->debug_id;\n\t\tif (WARN_ON(proc == target_proc)) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_invalid_target_handle;\n\t\t}\n\t\tif (security_binder_transaction(proc->tsk,\n\t\t\t\t\t\ttarget_proc->tsk) < 0) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPERM;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_invalid_target_handle;\n\t\t}\n\t\tbinder_inner_proc_lock(proc);\n\n\t\tw = list_first_entry_or_null(&thread->todo,\n\t\t\t\t\t     struct binder_work, entry);\n\t\tif (!(tr->flags & TF_ONE_WAY) && w &&\n\t\t    w->type == BINDER_WORK_TRANSACTION) {\n\t\t\t/*\n\t\t\t * Do not allow new outgoing transaction from a\n\t\t\t * thread that has a transaction at the head of\n\t\t\t * its todo list. Only need to check the head\n\t\t\t * because binder_select_thread_ilocked picks a\n\t\t\t * thread from proc->waiting_threads to enqueue\n\t\t\t * the transaction, and nothing is queued to the\n\t\t\t * todo list while the thread is on waiting_threads.\n\t\t\t */\n\t\t\tbinder_user_error(\"%d:%d new transaction not allowed when there is a transaction on thread todo\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_todo_list;\n\t\t}\n\n\t\tif (!(tr->flags & TF_ONE_WAY) && thread->transaction_stack) {\n\t\t\tstruct binder_transaction *tmp;\n\n\t\t\ttmp = thread->transaction_stack;\n\t\t\tif (tmp->to_thread != thread) {\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tbinder_user_error(\"%d:%d got new transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, tmp->debug_id,\n\t\t\t\t\ttmp->to_proc ? tmp->to_proc->pid : 0,\n\t\t\t\t\ttmp->to_thread ?\n\t\t\t\t\ttmp->to_thread->pid : 0);\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EPROTO;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_call_stack;\n\t\t\t}\n\t\t\twhile (tmp) {\n\t\t\t\tstruct binder_thread *from;\n\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tfrom = tmp->from;\n\t\t\t\tif (from && from->proc == target_proc) {\n\t\t\t\t\tatomic_inc(&from->tmp_ref);\n\t\t\t\t\ttarget_thread = from;\n\t\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\ttmp = tmp->from_parent;\n\t\t\t}\n\t\t}\n\t\tbinder_inner_proc_unlock(proc);\n\t}\n\tif (target_thread)\n\t\te->to_thread = target_thread->pid;\n\te->to_proc = target_proc->pid;\n\n\t/* TODO: reuse incoming transaction for reply */\n\tt = kzalloc(sizeof(*t), GFP_KERNEL);\n\tif (t == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_t_failed;\n\t}\n\tINIT_LIST_HEAD(&t->fd_fixups);\n\tbinder_stats_created(BINDER_STAT_TRANSACTION);\n\tspin_lock_init(&t->lock);\n\n\ttcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL);\n\tif (tcomplete == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_tcomplete_failed;\n\t}\n\tbinder_stats_created(BINDER_STAT_TRANSACTION_COMPLETE);\n\n\tt->debug_id = t_debug_id;\n\n\tif (reply)\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_REPLY %d -> %d:%d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_thread->pid,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\telse\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_TRANSACTION %d -> %d - node %d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_node->debug_id,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\n\tif (!reply && !(tr->flags & TF_ONE_WAY))\n\t\tt->from = thread;\n\telse\n\t\tt->from = NULL;\n\tt->sender_euid = task_euid(proc->tsk);\n\tt->to_proc = target_proc;\n\tt->to_thread = target_thread;\n\tt->code = tr->code;\n\tt->flags = tr->flags;\n\tt->priority = task_nice(current);\n\n\tif (target_node && target_node->txn_security_ctx) {\n\t\tu32 secid;\n\t\tsize_t added_size;\n\n\t\t/*\n\t\t * Arguably this should be the task's subjective LSM secid but\n\t\t * we can't reliably access the subjective creds of a task\n\t\t * other than our own so we must use the objective creds, which\n\t\t * are safe to access.  The downside is that if a task is\n\t\t * temporarily overriding it's creds it will not be reflected\n\t\t * here; however, it isn't clear that binder would handle that\n\t\t * case well anyway.\n\t\t */\n\t\tsecurity_task_getsecid_obj(proc->tsk, &secid);\n\t\tret = security_secid_to_secctx(secid, &secctx, &secctx_sz);\n\t\tif (ret) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = ret;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_get_secctx_failed;\n\t\t}\n\t\tadded_size = ALIGN(secctx_sz, sizeof(u64));\n\t\textra_buffers_size += added_size;\n\t\tif (extra_buffers_size < added_size) {\n\t\t\t/* integer overflow of extra_buffers_size */\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_extra_size;\n\t\t}\n\t}\n\n\ttrace_binder_transaction(reply, t, target_node);\n\n\tt->buffer = binder_alloc_new_buf(&target_proc->alloc, tr->data_size,\n\t\ttr->offsets_size, extra_buffers_size,\n\t\t!reply && (t->flags & TF_ONE_WAY), current->tgid);\n\tif (IS_ERR(t->buffer)) {\n\t\t/*\n\t\t * -ESRCH indicates VMA cleared. The target is dying.\n\t\t */\n\t\treturn_error_param = PTR_ERR(t->buffer);\n\t\treturn_error = return_error_param == -ESRCH ?\n\t\t\tBR_DEAD_REPLY : BR_FAILED_REPLY;\n\t\treturn_error_line = __LINE__;\n\t\tt->buffer = NULL;\n\t\tgoto err_binder_alloc_buf_failed;\n\t}\n\tif (secctx) {\n\t\tint err;\n\t\tsize_t buf_offset = ALIGN(tr->data_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(tr->offsets_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(extra_buffers_size, sizeof(void *)) -\n\t\t\t\t    ALIGN(secctx_sz, sizeof(u64));\n\n\t\tt->security_ctx = (uintptr_t)t->buffer->user_data + buf_offset;\n\t\terr = binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t  t->buffer, buf_offset,\n\t\t\t\t\t\t  secctx, secctx_sz);\n\t\tif (err) {\n\t\t\tt->security_ctx = 0;\n\t\t\tWARN_ON(1);\n\t\t}\n\t\tsecurity_release_secctx(secctx, secctx_sz);\n\t\tsecctx = NULL;\n\t}\n\tt->buffer->debug_id = t->debug_id;\n\tt->buffer->transaction = t;\n\tt->buffer->target_node = target_node;\n\tt->buffer->clear_on_free = !!(t->flags & TF_CLEAR_BUF);\n\ttrace_binder_transaction_alloc_buf(t->buffer);\n\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer, 0,\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.buffer,\n\t\t\t\ttr->data_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid data ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer,\n\t\t\t\tALIGN(tr->data_size, sizeof(void *)),\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.offsets,\n\t\t\t\ttr->offsets_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (!IS_ALIGNED(tr->offsets_size, sizeof(binder_size_t))) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets size, %lld\\n\",\n\t\t\t\tproc->pid, thread->pid, (u64)tr->offsets_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\tif (!IS_ALIGNED(extra_buffers_size, sizeof(u64))) {\n\t\tbinder_user_error(\"%d:%d got transaction with unaligned buffers size, %lld\\n\",\n\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t  (u64)extra_buffers_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\toff_start_offset = ALIGN(tr->data_size, sizeof(void *));\n\tbuffer_offset = off_start_offset;\n\toff_end_offset = off_start_offset + tr->offsets_size;\n\tsg_buf_offset = ALIGN(off_end_offset, sizeof(void *));\n\tsg_buf_end_offset = sg_buf_offset + extra_buffers_size -\n\t\tALIGN(secctx_sz, sizeof(u64));\n\toff_min = 0;\n\tfor (buffer_offset = off_start_offset; buffer_offset < off_end_offset;\n\t     buffer_offset += sizeof(binder_size_t)) {\n\t\tstruct binder_object_header *hdr;\n\t\tsize_t object_size;\n\t\tstruct binder_object object;\n\t\tbinder_size_t object_offset;\n\n\t\tif (binder_alloc_copy_from_buffer(&target_proc->alloc,\n\t\t\t\t\t\t  &object_offset,\n\t\t\t\t\t\t  t->buffer,\n\t\t\t\t\t\t  buffer_offset,\n\t\t\t\t\t\t  sizeof(object_offset))) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\t\tobject_size = binder_get_object(target_proc, t->buffer,\n\t\t\t\t\t\tobject_offset, &object);\n\t\tif (object_size == 0 || object_offset < off_min) {\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offset (%lld, min %lld max %lld) or object.\\n\",\n\t\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t\t  (u64)object_offset,\n\t\t\t\t\t  (u64)off_min,\n\t\t\t\t\t  (u64)t->buffer->data_size);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\n\t\thdr = &object.hdr;\n\t\toff_min = object_offset + object_size;\n\t\tswitch (hdr->type) {\n\t\tcase BINDER_TYPE_BINDER:\n\t\tcase BINDER_TYPE_WEAK_BINDER: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_binder(fp, t, thread);\n\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\t\tcase BINDER_TYPE_HANDLE:\n\t\tcase BINDER_TYPE_WEAK_HANDLE: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_handle(fp, t, thread);\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\n\t\tcase BINDER_TYPE_FD: {\n\t\t\tstruct binder_fd_object *fp = to_binder_fd_object(hdr);\n\t\t\tbinder_size_t fd_offset = object_offset +\n\t\t\t\t(uintptr_t)&fp->fd - (uintptr_t)fp;\n\t\t\tint ret = binder_translate_fd(fp->fd, fd_offset, t,\n\t\t\t\t\t\t      thread, in_reply_to);\n\n\t\t\tfp->pad_binder = 0;\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\t\tcase BINDER_TYPE_FDA: {\n\t\t\tstruct binder_object ptr_object;\n\t\t\tbinder_size_t parent_offset;\n\t\t\tstruct binder_fd_array_object *fda =\n\t\t\t\tto_binder_fd_array_object(hdr);\n\t\t\tsize_t num_valid = (buffer_offset - off_start_offset) /\n\t\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tstruct binder_buffer_object *parent =\n\t\t\t\tbinder_validate_ptr(target_proc, t->buffer,\n\t\t\t\t\t\t    &ptr_object, fda->parent,\n\t\t\t\t\t\t    off_start_offset,\n\t\t\t\t\t\t    &parent_offset,\n\t\t\t\t\t\t    num_valid);\n\t\t\tif (!parent) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid parent offset or type\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tif (!binder_validate_fixup(target_proc, t->buffer,\n\t\t\t\t\t\t   off_start_offset,\n\t\t\t\t\t\t   parent_offset,\n\t\t\t\t\t\t   fda->parent_offset,\n\t\t\t\t\t\t   last_fixup_obj_off,\n\t\t\t\t\t\t   last_fixup_min_off)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with out-of-order buffer fixup\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tret = binder_translate_fd_array(fda, parent, t, thread,\n\t\t\t\t\t\t\tin_reply_to);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj_off = parent_offset;\n\t\t\tlast_fixup_min_off =\n\t\t\t\tfda->parent_offset + sizeof(u32) * fda->num_fds;\n\t\t} break;\n\t\tcase BINDER_TYPE_PTR: {\n\t\t\tstruct binder_buffer_object *bp =\n\t\t\t\tto_binder_buffer_object(hdr);\n\t\t\tsize_t buf_left = sg_buf_end_offset - sg_buf_offset;\n\t\t\tsize_t num_valid;\n\n\t\t\tif (bp->length > buf_left) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with too large buffer\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_offset;\n\t\t\t}\n\t\t\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t\t\t&target_proc->alloc,\n\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\tsg_buf_offset,\n\t\t\t\t\t\t(const void __user *)\n\t\t\t\t\t\t\t(uintptr_t)bp->buffer,\n\t\t\t\t\t\tbp->length)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error_param = -EFAULT;\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_copy_data_failed;\n\t\t\t}\n\t\t\t/* Fixup buffer pointer to target proc address space */\n\t\t\tbp->buffer = (uintptr_t)\n\t\t\t\tt->buffer->user_data + sg_buf_offset;\n\t\t\tsg_buf_offset += ALIGN(bp->length, sizeof(u64));\n\n\t\t\tnum_valid = (buffer_offset - off_start_offset) /\n\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tret = binder_fixup_parent(t, thread, bp,\n\t\t\t\t\t\t  off_start_offset,\n\t\t\t\t\t\t  num_valid,\n\t\t\t\t\t\t  last_fixup_obj_off,\n\t\t\t\t\t\t  last_fixup_min_off);\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tbp, sizeof(*bp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj_off = object_offset;\n\t\t\tlast_fixup_min_off = 0;\n\t\t} break;\n\t\tdefault:\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid object type, %x\\n\",\n\t\t\t\tproc->pid, thread->pid, hdr->type);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_object_type;\n\t\t}\n\t}\n\tif (t->buffer->oneway_spam_suspect)\n\t\ttcomplete->type = BINDER_WORK_TRANSACTION_ONEWAY_SPAM_SUSPECT;\n\telse\n\t\ttcomplete->type = BINDER_WORK_TRANSACTION_COMPLETE;\n\tt->work.type = BINDER_WORK_TRANSACTION;\n\n\tif (reply) {\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tbinder_inner_proc_lock(target_proc);\n\t\tif (target_thread->is_dead || target_proc->is_frozen) {\n\t\t\treturn_error = target_thread->is_dead ?\n\t\t\t\tBR_DEAD_REPLY : BR_FROZEN_REPLY;\n\t\t\tbinder_inner_proc_unlock(target_proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_pop_transaction_ilocked(target_thread, in_reply_to);\n\t\tbinder_enqueue_thread_work_ilocked(target_thread, &t->work);\n\t\ttarget_proc->outstanding_txns++;\n\t\tbinder_inner_proc_unlock(target_proc);\n\t\twake_up_interruptible_sync(&target_thread->wait);\n\t\tbinder_free_transaction(in_reply_to);\n\t} else if (!(t->flags & TF_ONE_WAY)) {\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_inner_proc_lock(proc);\n\t\t/*\n\t\t * Defer the TRANSACTION_COMPLETE, so we don't return to\n\t\t * userspace immediately; this allows the target process to\n\t\t * immediately start processing this transaction, reducing\n\t\t * latency. We will then return the TRANSACTION_COMPLETE when\n\t\t * the target replies (or there is an error).\n\t\t */\n\t\tbinder_enqueue_deferred_thread_work_ilocked(thread, tcomplete);\n\t\tt->need_reply = 1;\n\t\tt->from_parent = thread->transaction_stack;\n\t\tthread->transaction_stack = t;\n\t\tbinder_inner_proc_unlock(proc);\n\t\treturn_error = binder_proc_transaction(t,\n\t\t\t\ttarget_proc, target_thread);\n\t\tif (return_error) {\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tbinder_pop_transaction_ilocked(thread, t);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t} else {\n\t\tBUG_ON(target_node == NULL);\n\t\tBUG_ON(t->buffer->async_transaction != 1);\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\treturn_error = binder_proc_transaction(t, target_proc, NULL);\n\t\tif (return_error)\n\t\t\tgoto err_dead_proc_or_thread;\n\t}\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\t/*\n\t * write barrier to synchronize with initialization\n\t * of log entry\n\t */\n\tsmp_wmb();\n\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\treturn;\n\nerr_dead_proc_or_thread:\n\treturn_error_line = __LINE__;\n\tbinder_dequeue_work(proc, tcomplete);\nerr_translate_failed:\nerr_bad_object_type:\nerr_bad_offset:\nerr_bad_parent:\nerr_copy_data_failed:\n\tbinder_free_txn_fixups(t);\n\ttrace_binder_transaction_failed_buffer_release(t->buffer);\n\tbinder_transaction_buffer_release(target_proc, t->buffer,\n\t\t\t\t\t  buffer_offset, true);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\ttarget_node = NULL;\n\tt->buffer->transaction = NULL;\n\tbinder_alloc_free_buf(&target_proc->alloc, t->buffer);\nerr_binder_alloc_buf_failed:\nerr_bad_extra_size:\n\tif (secctx)\n\t\tsecurity_release_secctx(secctx, secctx_sz);\nerr_get_secctx_failed:\n\tkfree(tcomplete);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\nerr_alloc_tcomplete_failed:\n\tif (trace_binder_txn_latency_free_enabled())\n\t\tbinder_txn_latency_free(t);\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\nerr_alloc_t_failed:\nerr_bad_todo_list:\nerr_bad_call_stack:\nerr_empty_call_stack:\nerr_dead_binder:\nerr_invalid_target_handle:\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tif (target_proc)\n\t\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node) {\n\t\tbinder_dec_node(target_node, 1, 0);\n\t\tbinder_dec_node_tmpref(target_node);\n\t}\n\n\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t     \"%d:%d transaction failed %d/%d, size %lld-%lld line %d\\n\",\n\t\t     proc->pid, thread->pid, return_error, return_error_param,\n\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t     return_error_line);\n\n\t{\n\t\tstruct binder_transaction_log_entry *fe;\n\n\t\te->return_error = return_error;\n\t\te->return_error_param = return_error_param;\n\t\te->return_error_line = return_error_line;\n\t\tfe = binder_transaction_log_add(&binder_transaction_log_failed);\n\t\t*fe = *e;\n\t\t/*\n\t\t * write barrier to synchronize with initialization\n\t\t * of log entry\n\t\t */\n\t\tsmp_wmb();\n\t\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\t\tWRITE_ONCE(fe->debug_id_done, t_debug_id);\n\t}\n\n\tBUG_ON(thread->return_error.cmd != BR_OK);\n\tif (in_reply_to) {\n\t\tthread->return_error.cmd = BR_TRANSACTION_COMPLETE;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t\tbinder_send_failed_reply(in_reply_to, return_error);\n\t} else {\n\t\tthread->return_error.cmd = return_error;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t}\n}",
            "detect_result": "YES",
            "final_result": 1
        },
        {
            "id": 3233,
            "cve_id": "CVE-2021-39713",
            "code_snippet": "static struct tcf_block *tcf_block_find(struct net *net, struct Qdisc **q,\n\t\t\t\t\tu32 *parent, unsigned long *cl,\n\t\t\t\t\tint ifindex, u32 block_index,\n\t\t\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct tcf_block *block;\n\n\tif (ifindex == TCM_IFINDEX_MAGIC_BLOCK) {\n\t\tblock = tcf_block_lookup(net, block_index);\n\t\tif (!block) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Block of given index was not found\");\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\t} else {\n\t\tconst struct Qdisc_class_ops *cops;\n\t\tstruct net_device *dev;\n\n\t\t/* Find link */\n\t\tdev = __dev_get_by_index(net, ifindex);\n\t\tif (!dev)\n\t\t\treturn ERR_PTR(-ENODEV);\n\n\t\t/* Find qdisc */\n\t\tif (!*parent) {\n\t\t\t*q = dev->qdisc;\n\t\t\t*parent = (*q)->handle;\n\t\t} else {\n\t\t\t*q = qdisc_lookup(dev, TC_H_MAJ(*parent));\n\t\t\tif (!*q) {\n\t\t\t\tNL_SET_ERR_MSG(extack, \"Parent Qdisc doesn't exists\");\n\t\t\t\treturn ERR_PTR(-EINVAL);\n\t\t\t}\n\t\t}\n\n\t\t/* Is it classful? */\n\t\tcops = (*q)->ops->cl_ops;\n\t\tif (!cops) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Qdisc not classful\");\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\n\t\tif (!cops->tcf_block) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Class doesn't support blocks\");\n\t\t\treturn ERR_PTR(-EOPNOTSUPP);\n\t\t}\n\n\t\t/* Do we search for filter, attached to class? */\n\t\tif (TC_H_MIN(*parent)) {\n\t\t\t*cl = cops->find(*q, *parent);\n\t\t\tif (*cl == 0) {\n\t\t\t\tNL_SET_ERR_MSG(extack, \"Specified class doesn't exist\");\n\t\t\t\treturn ERR_PTR(-ENOENT);\n\t\t\t}\n\t\t}\n\n\t\t/* And the last stroke */\n\t\tblock = cops->tcf_block(*q, *cl, extack);\n\t\tif (!block)\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\tif (tcf_block_shared(block)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"This filter block is shared. Please use the block index to manipulate the filters\");\n\t\t\treturn ERR_PTR(-EOPNOTSUPP);\n\t\t}\n\t}\n\n\treturn block;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3234,
            "cve_id": "CVE-2021-39713",
            "code_snippet": "static int tc_del_tfilter(struct sk_buff *skb, struct nlmsghdr *n,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct tcmsg *t;\n\tu32 protocol;\n\tu32 prio;\n\tu32 parent;\n\tu32 chain_index;\n\tstruct Qdisc *q = NULL;\n\tstruct tcf_chain_info chain_info;\n\tstruct tcf_chain *chain = NULL;\n\tstruct tcf_block *block;\n\tstruct tcf_proto *tp = NULL;\n\tunsigned long cl = 0;\n\tvoid *fh = NULL;\n\tint err;\n\n\tif (!netlink_ns_capable(skb, net->user_ns, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*t), tca, TCA_MAX, NULL, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tt = nlmsg_data(n);\n\tprotocol = TC_H_MIN(t->tcm_info);\n\tprio = TC_H_MAJ(t->tcm_info);\n\tparent = t->tcm_parent;\n\n\tif (prio == 0 && (protocol || t->tcm_handle || tca[TCA_KIND])) {\n\t\tNL_SET_ERR_MSG(extack, \"Cannot flush filters with protocol, handle or kind set\");\n\t\treturn -ENOENT;\n\t}\n\n\t/* Find head of filter chain. */\n\n\tblock = tcf_block_find(net, &q, &parent, &cl,\n\t\t\t       t->tcm_ifindex, t->tcm_block_index, extack);\n\tif (IS_ERR(block)) {\n\t\terr = PTR_ERR(block);\n\t\tgoto errout;\n\t}\n\n\tchain_index = tca[TCA_CHAIN] ? nla_get_u32(tca[TCA_CHAIN]) : 0;\n\tif (chain_index > TC_ACT_EXT_VAL_MASK) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified chain index exceeds upper limit\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\tchain = tcf_chain_get(block, chain_index, false);\n\tif (!chain) {\n\t\t/* User requested flush on non-existent chain. Nothing to do,\n\t\t * so just return success.\n\t\t */\n\t\tif (prio == 0) {\n\t\t\terr = 0;\n\t\t\tgoto errout;\n\t\t}\n\t\tNL_SET_ERR_MSG(extack, \"Cannot find specified filter chain\");\n\t\terr = -ENOENT;\n\t\tgoto errout;\n\t}\n\n\tif (prio == 0) {\n\t\ttfilter_notify_chain(net, skb, block, q, parent, n,\n\t\t\t\t     chain, RTM_DELTFILTER);\n\t\ttcf_chain_flush(chain);\n\t\terr = 0;\n\t\tgoto errout;\n\t}\n\n\ttp = tcf_chain_tp_find(chain, &chain_info, protocol,\n\t\t\t       prio, false);\n\tif (!tp || IS_ERR(tp)) {\n\t\tNL_SET_ERR_MSG(extack, \"Filter with specified priority/protocol not found\");\n\t\terr = tp ? PTR_ERR(tp) : -ENOENT;\n\t\tgoto errout;\n\t} else if (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], tp->ops->kind)) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified filter kind does not match existing one\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\tfh = tp->ops->get(tp, t->tcm_handle);\n\n\tif (!fh) {\n\t\tif (t->tcm_handle == 0) {\n\t\t\ttcf_chain_tp_remove(chain, &chain_info, tp);\n\t\t\ttfilter_notify(net, skb, n, tp, block, q, parent, fh,\n\t\t\t\t       RTM_DELTFILTER, false);\n\t\t\ttcf_proto_destroy(tp, extack);\n\t\t\terr = 0;\n\t\t} else {\n\t\t\tNL_SET_ERR_MSG(extack, \"Specified filter handle not found\");\n\t\t\terr = -ENOENT;\n\t\t}\n\t} else {\n\t\tbool last;\n\n\t\terr = tfilter_del_notify(net, skb, n, tp, block,\n\t\t\t\t\t q, parent, fh, false, &last,\n\t\t\t\t\t extack);\n\t\tif (err)\n\t\t\tgoto errout;\n\t\tif (last) {\n\t\t\ttcf_chain_tp_remove(chain, &chain_info, tp);\n\t\t\ttcf_proto_destroy(tp, extack);\n\t\t}\n\t}\n\nerrout:\n\tif (chain)\n\t\ttcf_chain_put(chain);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3276,
            "cve_id": "CVE-2021-4203",
            "code_snippet": "static void __sk_destruct(struct rcu_head *head)\n{\n\tstruct sock *sk = container_of(head, struct sock, sk_rcu);\n\tstruct sk_filter *filter;\n\n\tif (sk->sk_destruct)\n\t\tsk->sk_destruct(sk);\n\n\tfilter = rcu_dereference_check(sk->sk_filter,\n\t\t\t\t       refcount_read(&sk->sk_wmem_alloc) == 0);\n\tif (filter) {\n\t\tsk_filter_uncharge(sk, filter);\n\t\tRCU_INIT_POINTER(sk->sk_filter, NULL);\n\t}\n\n\tsock_disable_timestamp(sk, SK_FLAGS_TIMESTAMP);\n\n#ifdef CONFIG_BPF_SYSCALL\n\tbpf_sk_storage_free(sk);\n#endif\n\n\tif (atomic_read(&sk->sk_omem_alloc))\n\t\tpr_debug(\"%s: optmem leakage (%d bytes) detected\\n\",\n\t\t\t __func__, atomic_read(&sk->sk_omem_alloc));\n\n\tif (sk->sk_frag.page) {\n\t\tput_page(sk->sk_frag.page);\n\t\tsk->sk_frag.page = NULL;\n\t}\n\n\tif (sk->sk_peer_cred)\n\t\tput_cred(sk->sk_peer_cred);\n\tput_pid(sk->sk_peer_pid);\n\tif (likely(sk->sk_net_refcnt))\n\t\tput_net(sock_net(sk));\n\tsk_prot_free(sk->sk_prot_creator, sk);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3277,
            "cve_id": "CVE-2021-4203",
            "code_snippet": "void sock_init_data(struct socket *sock, struct sock *sk)\n{\n\tsk_init_common(sk);\n\tsk->sk_send_head\t=\tNULL;\n\n\ttimer_setup(&sk->sk_timer, NULL, 0);\n\n\tsk->sk_allocation\t=\tGFP_KERNEL;\n\tsk->sk_rcvbuf\t\t=\tsysctl_rmem_default;\n\tsk->sk_sndbuf\t\t=\tsysctl_wmem_default;\n\tsk->sk_state\t\t=\tTCP_CLOSE;\n\tsk_set_socket(sk, sock);\n\n\tsock_set_flag(sk, SOCK_ZAPPED);\n\n\tif (sock) {\n\t\tsk->sk_type\t=\tsock->type;\n\t\tRCU_INIT_POINTER(sk->sk_wq, &sock->wq);\n\t\tsock->sk\t=\tsk;\n\t\tsk->sk_uid\t=\tSOCK_INODE(sock)->i_uid;\n\t} else {\n\t\tRCU_INIT_POINTER(sk->sk_wq, NULL);\n\t\tsk->sk_uid\t=\tmake_kuid(sock_net(sk)->user_ns, 0);\n\t}\n\n\trwlock_init(&sk->sk_callback_lock);\n\tif (sk->sk_kern_sock)\n\t\tlockdep_set_class_and_name(\n\t\t\t&sk->sk_callback_lock,\n\t\t\taf_kern_callback_keys + sk->sk_family,\n\t\t\taf_family_kern_clock_key_strings[sk->sk_family]);\n\telse\n\t\tlockdep_set_class_and_name(\n\t\t\t&sk->sk_callback_lock,\n\t\t\taf_callback_keys + sk->sk_family,\n\t\t\taf_family_clock_key_strings[sk->sk_family]);\n\n\tsk->sk_state_change\t=\tsock_def_wakeup;\n\tsk->sk_data_ready\t=\tsock_def_readable;\n\tsk->sk_write_space\t=\tsock_def_write_space;\n\tsk->sk_error_report\t=\tsock_def_error_report;\n\tsk->sk_destruct\t\t=\tsock_def_destruct;\n\n\tsk->sk_frag.page\t=\tNULL;\n\tsk->sk_frag.offset\t=\t0;\n\tsk->sk_peek_off\t\t=\t-1;\n\n\tsk->sk_peer_pid \t=\tNULL;\n\tsk->sk_peer_cred\t=\tNULL;\n\tsk->sk_write_pending\t=\t0;\n\tsk->sk_rcvlowat\t\t=\t1;\n\tsk->sk_rcvtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\tsk->sk_sndtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\n\tsk->sk_stamp = SK_DEFAULT_STAMP;\n#if BITS_PER_LONG==32\n\tseqlock_init(&sk->sk_stamp_seq);\n#endif\n\tatomic_set(&sk->sk_zckey, 0);\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tsk->sk_napi_id\t\t=\t0;\n\tsk->sk_ll_usec\t\t=\tsysctl_net_busy_read;\n#endif\n\n\tsk->sk_max_pacing_rate = ~0UL;\n\tsk->sk_pacing_rate = ~0UL;\n\tWRITE_ONCE(sk->sk_pacing_shift, 10);\n\tsk->sk_incoming_cpu = -1;\n\n\tsk_rx_queue_clear(sk);\n\t/*\n\t * Before updating sk_refcnt, we must commit prior changes to memory\n\t * (Documentation/RCU/rculist_nulls.rst for details)\n\t */\n\tsmp_wmb();\n\trefcount_set(&sk->sk_refcnt, 1);\n\tatomic_set(&sk->sk_drops, 0);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3297,
            "cve_id": "CVE-2021-44733",
            "code_snippet": "struct tee_shm *tee_shm_get_from_id(struct tee_context *ctx, int id)\n{\n\tstruct tee_device *teedev;\n\tstruct tee_shm *shm;\n\n\tif (!ctx)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tteedev = ctx->teedev;\n\tmutex_lock(&teedev->mutex);\n\tshm = idr_find(&teedev->idr, id);\n\tif (!shm || shm->ctx != ctx)\n\t\tshm = ERR_PTR(-EINVAL);\n\telse if (shm->flags & TEE_SHM_DMA_BUF)\n\t\tget_dma_buf(shm->dmabuf);\n\tmutex_unlock(&teedev->mutex);\n\treturn shm;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3298,
            "cve_id": "CVE-2021-44733",
            "code_snippet": "struct tee_shm *tee_shm_register(struct tee_context *ctx, unsigned long addr,\n\t\t\t\t size_t length, u32 flags)\n{\n\tstruct tee_device *teedev = ctx->teedev;\n\tconst u32 req_user_flags = TEE_SHM_DMA_BUF | TEE_SHM_USER_MAPPED;\n\tconst u32 req_kernel_flags = TEE_SHM_DMA_BUF | TEE_SHM_KERNEL_MAPPED;\n\tstruct tee_shm *shm;\n\tvoid *ret;\n\tint rc;\n\tint num_pages;\n\tunsigned long start;\n\n\tif (flags != req_user_flags && flags != req_kernel_flags)\n\t\treturn ERR_PTR(-ENOTSUPP);\n\n\tif (!tee_device_get(teedev))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (!teedev->desc->ops->shm_register ||\n\t    !teedev->desc->ops->shm_unregister) {\n\t\ttee_device_put(teedev);\n\t\treturn ERR_PTR(-ENOTSUPP);\n\t}\n\n\tteedev_ctx_get(ctx);\n\n\tshm = kzalloc(sizeof(*shm), GFP_KERNEL);\n\tif (!shm) {\n\t\tret = ERR_PTR(-ENOMEM);\n\t\tgoto err;\n\t}\n\n\tshm->flags = flags | TEE_SHM_REGISTER;\n\tshm->ctx = ctx;\n\tshm->id = -1;\n\taddr = untagged_addr(addr);\n\tstart = rounddown(addr, PAGE_SIZE);\n\tshm->offset = addr - start;\n\tshm->size = length;\n\tnum_pages = (roundup(addr + length, PAGE_SIZE) - start) / PAGE_SIZE;\n\tshm->pages = kcalloc(num_pages, sizeof(*shm->pages), GFP_KERNEL);\n\tif (!shm->pages) {\n\t\tret = ERR_PTR(-ENOMEM);\n\t\tgoto err;\n\t}\n\n\tif (flags & TEE_SHM_USER_MAPPED) {\n\t\trc = pin_user_pages_fast(start, num_pages, FOLL_WRITE,\n\t\t\t\t\t shm->pages);\n\t} else {\n\t\tstruct kvec *kiov;\n\t\tint i;\n\n\t\tkiov = kcalloc(num_pages, sizeof(*kiov), GFP_KERNEL);\n\t\tif (!kiov) {\n\t\t\tret = ERR_PTR(-ENOMEM);\n\t\t\tgoto err;\n\t\t}\n\n\t\tfor (i = 0; i < num_pages; i++) {\n\t\t\tkiov[i].iov_base = (void *)(start + i * PAGE_SIZE);\n\t\t\tkiov[i].iov_len = PAGE_SIZE;\n\t\t}\n\n\t\trc = get_kernel_pages(kiov, num_pages, 0, shm->pages);\n\t\tkfree(kiov);\n\t}\n\tif (rc > 0)\n\t\tshm->num_pages = rc;\n\tif (rc != num_pages) {\n\t\tif (rc >= 0)\n\t\t\trc = -ENOMEM;\n\t\tret = ERR_PTR(rc);\n\t\tgoto err;\n\t}\n\n\tmutex_lock(&teedev->mutex);\n\tshm->id = idr_alloc(&teedev->idr, shm, 1, 0, GFP_KERNEL);\n\tmutex_unlock(&teedev->mutex);\n\n\tif (shm->id < 0) {\n\t\tret = ERR_PTR(shm->id);\n\t\tgoto err;\n\t}\n\n\trc = teedev->desc->ops->shm_register(ctx, shm, shm->pages,\n\t\t\t\t\t     shm->num_pages, start);\n\tif (rc) {\n\t\tret = ERR_PTR(rc);\n\t\tgoto err;\n\t}\n\n\tif (flags & TEE_SHM_DMA_BUF) {\n\t\tDEFINE_DMA_BUF_EXPORT_INFO(exp_info);\n\n\t\texp_info.ops = &tee_shm_dma_buf_ops;\n\t\texp_info.size = shm->size;\n\t\texp_info.flags = O_RDWR;\n\t\texp_info.priv = shm;\n\n\t\tshm->dmabuf = dma_buf_export(&exp_info);\n\t\tif (IS_ERR(shm->dmabuf)) {\n\t\t\tret = ERR_CAST(shm->dmabuf);\n\t\t\tteedev->desc->ops->shm_unregister(ctx, shm);\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\treturn shm;\nerr:\n\tif (shm) {\n\t\tif (shm->id >= 0) {\n\t\t\tmutex_lock(&teedev->mutex);\n\t\t\tidr_remove(&teedev->idr, shm->id);\n\t\t\tmutex_unlock(&teedev->mutex);\n\t\t}\n\t\trelease_registered_pages(shm);\n\t}\n\tkfree(shm);\n\tteedev_ctx_put(ctx);\n\ttee_device_put(teedev);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3376,
            "cve_id": "CVE-2022-1048",
            "code_snippet": "void snd_pcm_detach_substream(struct snd_pcm_substream *substream)\n{\n\tstruct snd_pcm_runtime *runtime;\n\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn;\n\truntime = substream->runtime;\n\tif (runtime->private_free != NULL)\n\t\truntime->private_free(runtime);\n\tfree_pages_exact(runtime->status,\n\t\t       PAGE_ALIGN(sizeof(struct snd_pcm_mmap_status)));\n\tfree_pages_exact(runtime->control,\n\t\t       PAGE_ALIGN(sizeof(struct snd_pcm_mmap_control)));\n\tkfree(runtime->hw_constraints.rules);\n\t/* Avoid concurrent access to runtime via PCM timer interface */\n\tif (substream->timer) {\n\t\tspin_lock_irq(&substream->timer->lock);\n\t\tsubstream->runtime = NULL;\n\t\tspin_unlock_irq(&substream->timer->lock);\n\t} else {\n\t\tsubstream->runtime = NULL;\n\t}\n\tkfree(runtime);\n\tput_pid(substream->pid);\n\tsubstream->pid = NULL;\n\tsubstream->pstr->substream_opened--;\n}",
            "detect_result": "NO\n\nExplanation of the code behavior:\nThe function `snd_pcm_detach_substream` is designed to detach a PCM (Pulse-Code Modulation) substream from its runtime structure and clean up associated resources. Here's a step-by-step breakdown of what the function does:\n\n1. It first checks if the `substream` pointer is valid using the `PCM_RUNTIME_CHECK` macro. If the check fails, it returns immediately.\n\n2. It retrieves the `runtime` structure associated with the `substream`.\n\n3. If there is a custom `private_free` function set in the `runtime`, it calls this function to perform any necessary cleanup specific to that runtime.\n\n4. It then frees memory allocated for `runtime->status` and `runtime->control` using `free_pages_exact`, which is appropriate for freeing memory allocated with `alloc_pages_exact`.\n\n5. The function frees the memory allocated for `runtime->hw_constraints.rules` using `kfree`.\n\n6. To prevent concurrent access to the `runtime` via the PCM timer interface, it locks the timer's spinlock, sets `substream->runtime` to `NULL`, and then unlocks the spinlock. If there is no timer associated with the substream, it simply sets `substream->runtime` to `NULL`.\n\n7. It frees the `runtime` structure itself using `kfree`.\n\n8. Finally, it decrements the count of opened substreams in the parent stream (`substream->pstr->substream_opened`) and releases the PID associated with the substream using `put_pid`, setting `substream->pid` to `NULL`.\n\nVulnerability assessment:\nThe provided code does not appear to have any obvious vulnerabilities such as buffer overflows, use-after-free, or improper memory management issues. It properly cleans up all allocated resources and handles potential concurrency issues with the PCM timer interface. Therefore, based on the given code snippet, the answer is NO - the code is not vulnerable. However, it's always important to consider the broader context and ensure that all related functions and data structures are also correctly implemented to avoid potential vulnerabilities.",
            "final_result": 0
        },
        {
            "id": 3474,
            "cve_id": "CVE-2022-20154",
            "code_snippet": "void sctp_endpoint_hold(struct sctp_endpoint *ep)\n{\n\trefcount_inc(&ep->base.refcnt);\n}",
            "detect_result": "NO\n\nExplanation: The provided code snippet is a function named `sctp_endpoint_hold` that increments the reference count of an SCTP endpoint (`struct sctp_endpoint`). This is a common pattern used in resource management to ensure that an object is not deallocated while it is still in use. The function itself does not appear to have any obvious vulnerabilities such as buffer overflows, format string issues, or improper memory handling. However, the safety of this function would depend on the context in which `refcount_inc` and the `refcnt` field are used throughout the rest of the codebase. If the reference counting mechanism is implemented correctly and consistently, this function should be safe.",
            "final_result": 0
        },
        {
            "id": 3531,
            "cve_id": "CVE-2022-20567",
            "code_snippet": "static int pppol2tp_connect(struct socket *sock, struct sockaddr *uservaddr,\n\t\t\t    int sockaddr_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sockaddr_pppol2tp *sp = (struct sockaddr_pppol2tp *) uservaddr;\n\tstruct pppox_sock *po = pppox_sk(sk);\n\tstruct l2tp_session *session = NULL;\n\tstruct l2tp_tunnel *tunnel;\n\tstruct pppol2tp_session *ps;\n\tstruct l2tp_session_cfg cfg = { 0, };\n\tint error = 0;\n\tu32 tunnel_id, peer_tunnel_id;\n\tu32 session_id, peer_session_id;\n\tbool drop_refcnt = false;\n\tbool drop_tunnel = false;\n\tint ver = 2;\n\tint fd;\n\n\tlock_sock(sk);\n\n\terror = -EINVAL;\n\tif (sp->sa_protocol != PX_PROTO_OL2TP)\n\t\tgoto end;\n\n\t/* Check for already bound sockets */\n\terror = -EBUSY;\n\tif (sk->sk_state & PPPOX_CONNECTED)\n\t\tgoto end;\n\n\t/* We don't supporting rebinding anyway */\n\terror = -EALREADY;\n\tif (sk->sk_user_data)\n\t\tgoto end; /* socket is already attached */\n\n\t/* Get params from socket address. Handle L2TPv2 and L2TPv3.\n\t * This is nasty because there are different sockaddr_pppol2tp\n\t * structs for L2TPv2, L2TPv3, over IPv4 and IPv6. We use\n\t * the sockaddr size to determine which structure the caller\n\t * is using.\n\t */\n\tpeer_tunnel_id = 0;\n\tif (sockaddr_len == sizeof(struct sockaddr_pppol2tp)) {\n\t\tfd = sp->pppol2tp.fd;\n\t\ttunnel_id = sp->pppol2tp.s_tunnel;\n\t\tpeer_tunnel_id = sp->pppol2tp.d_tunnel;\n\t\tsession_id = sp->pppol2tp.s_session;\n\t\tpeer_session_id = sp->pppol2tp.d_session;\n\t} else if (sockaddr_len == sizeof(struct sockaddr_pppol2tpv3)) {\n\t\tstruct sockaddr_pppol2tpv3 *sp3 =\n\t\t\t(struct sockaddr_pppol2tpv3 *) sp;\n\t\tver = 3;\n\t\tfd = sp3->pppol2tp.fd;\n\t\ttunnel_id = sp3->pppol2tp.s_tunnel;\n\t\tpeer_tunnel_id = sp3->pppol2tp.d_tunnel;\n\t\tsession_id = sp3->pppol2tp.s_session;\n\t\tpeer_session_id = sp3->pppol2tp.d_session;\n\t} else if (sockaddr_len == sizeof(struct sockaddr_pppol2tpin6)) {\n\t\tstruct sockaddr_pppol2tpin6 *sp6 =\n\t\t\t(struct sockaddr_pppol2tpin6 *) sp;\n\t\tfd = sp6->pppol2tp.fd;\n\t\ttunnel_id = sp6->pppol2tp.s_tunnel;\n\t\tpeer_tunnel_id = sp6->pppol2tp.d_tunnel;\n\t\tsession_id = sp6->pppol2tp.s_session;\n\t\tpeer_session_id = sp6->pppol2tp.d_session;\n\t} else if (sockaddr_len == sizeof(struct sockaddr_pppol2tpv3in6)) {\n\t\tstruct sockaddr_pppol2tpv3in6 *sp6 =\n\t\t\t(struct sockaddr_pppol2tpv3in6 *) sp;\n\t\tver = 3;\n\t\tfd = sp6->pppol2tp.fd;\n\t\ttunnel_id = sp6->pppol2tp.s_tunnel;\n\t\tpeer_tunnel_id = sp6->pppol2tp.d_tunnel;\n\t\tsession_id = sp6->pppol2tp.s_session;\n\t\tpeer_session_id = sp6->pppol2tp.d_session;\n\t} else {\n\t\terror = -EINVAL;\n\t\tgoto end; /* bad socket address */\n\t}\n\n\t/* Don't bind if tunnel_id is 0 */\n\terror = -EINVAL;\n\tif (tunnel_id == 0)\n\t\tgoto end;\n\n\ttunnel = l2tp_tunnel_get(sock_net(sk), tunnel_id);\n\tif (tunnel)\n\t\tdrop_tunnel = true;\n\n\t/* Special case: create tunnel context if session_id and\n\t * peer_session_id is 0. Otherwise look up tunnel using supplied\n\t * tunnel id.\n\t */\n\tif ((session_id == 0) && (peer_session_id == 0)) {\n\t\tif (tunnel == NULL) {\n\t\t\tstruct l2tp_tunnel_cfg tcfg = {\n\t\t\t\t.encap = L2TP_ENCAPTYPE_UDP,\n\t\t\t\t.debug = 0,\n\t\t\t};\n\t\t\terror = l2tp_tunnel_create(sock_net(sk), fd, ver, tunnel_id, peer_tunnel_id, &tcfg, &tunnel);\n\t\t\tif (error < 0)\n\t\t\t\tgoto end;\n\t\t}\n\t} else {\n\t\t/* Error if we can't find the tunnel */\n\t\terror = -ENOENT;\n\t\tif (tunnel == NULL)\n\t\t\tgoto end;\n\n\t\t/* Error if socket is not prepped */\n\t\tif (tunnel->sock == NULL)\n\t\t\tgoto end;\n\t}\n\n\tif (tunnel->recv_payload_hook == NULL)\n\t\ttunnel->recv_payload_hook = pppol2tp_recv_payload_hook;\n\n\tif (tunnel->peer_tunnel_id == 0)\n\t\ttunnel->peer_tunnel_id = peer_tunnel_id;\n\n\tsession = l2tp_session_get(sock_net(sk), tunnel, session_id);\n\tif (session) {\n\t\tdrop_refcnt = true;\n\t\tps = l2tp_session_priv(session);\n\n\t\t/* Using a pre-existing session is fine as long as it hasn't\n\t\t * been connected yet.\n\t\t */\n\t\tmutex_lock(&ps->sk_lock);\n\t\tif (rcu_dereference_protected(ps->sk,\n\t\t\t\t\t      lockdep_is_held(&ps->sk_lock))) {\n\t\t\tmutex_unlock(&ps->sk_lock);\n\t\t\terror = -EEXIST;\n\t\t\tgoto end;\n\t\t}\n\t} else {\n\t\t/* Default MTU must allow space for UDP/L2TP/PPP headers */\n\t\tcfg.mtu = 1500 - PPPOL2TP_HEADER_OVERHEAD;\n\t\tcfg.mru = cfg.mtu;\n\n\t\tsession = l2tp_session_create(sizeof(struct pppol2tp_session),\n\t\t\t\t\t      tunnel, session_id,\n\t\t\t\t\t      peer_session_id, &cfg);\n\t\tif (IS_ERR(session)) {\n\t\t\terror = PTR_ERR(session);\n\t\t\tgoto end;\n\t\t}\n\n\t\tpppol2tp_session_init(session);\n\t\tps = l2tp_session_priv(session);\n\t\tl2tp_session_inc_refcount(session);\n\n\t\tmutex_lock(&ps->sk_lock);\n\t\terror = l2tp_session_register(session, tunnel);\n\t\tif (error < 0) {\n\t\t\tmutex_unlock(&ps->sk_lock);\n\t\t\tkfree(session);\n\t\t\tgoto end;\n\t\t}\n\t\tdrop_refcnt = true;\n\t}\n\n\t/* Special case: if source & dest session_id == 0x0000, this\n\t * socket is being created to manage the tunnel. Just set up\n\t * the internal context for use by ioctl() and sockopt()\n\t * handlers.\n\t */\n\tif ((session->session_id == 0) &&\n\t    (session->peer_session_id == 0)) {\n\t\terror = 0;\n\t\tgoto out_no_ppp;\n\t}\n\n\t/* The only header we need to worry about is the L2TP\n\t * header. This size is different depending on whether\n\t * sequence numbers are enabled for the data channel.\n\t */\n\tpo->chan.hdrlen = PPPOL2TP_L2TP_HDR_SIZE_NOSEQ;\n\n\tpo->chan.private = sk;\n\tpo->chan.ops\t = &pppol2tp_chan_ops;\n\tpo->chan.mtu\t = session->mtu;\n\n\terror = ppp_register_net_channel(sock_net(sk), &po->chan);\n\tif (error) {\n\t\tmutex_unlock(&ps->sk_lock);\n\t\tgoto end;\n\t}\n\nout_no_ppp:\n\t/* This is how we get the session context from the socket. */\n\tsk->sk_user_data = session;\n\trcu_assign_pointer(ps->sk, sk);\n\tmutex_unlock(&ps->sk_lock);\n\n\t/* Keep the reference we've grabbed on the session: sk doesn't expect\n\t * the session to disappear. pppol2tp_session_destruct() is responsible\n\t * for dropping it.\n\t */\n\tdrop_refcnt = false;\n\n\tsk->sk_state = PPPOX_CONNECTED;\n\tl2tp_info(session, L2TP_MSG_CONTROL, \"%s: created\\n\",\n\t\t  session->name);\n\nend:\n\tif (drop_refcnt)\n\t\tl2tp_session_dec_refcount(session);\n\tif (drop_tunnel)\n\t\tl2tp_tunnel_dec_refcount(tunnel);\n\trelease_sock(sk);\n\n\treturn error;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3556,
            "cve_id": "CVE-2022-23039",
            "code_snippet": "static void __del_gref(struct gntalloc_gref *gref)\n{\n\tif (gref->notify.flags & UNMAP_NOTIFY_CLEAR_BYTE) {\n\t\tuint8_t *tmp = kmap(gref->page);\n\t\ttmp[gref->notify.pgoff] = 0;\n\t\tkunmap(gref->page);\n\t}\n\tif (gref->notify.flags & UNMAP_NOTIFY_SEND_EVENT) {\n\t\tnotify_remote_via_evtchn(gref->notify.event);\n\t\tevtchn_put(gref->notify.event);\n\t}\n\n\tgref->notify.flags = 0;\n\n\tif (gref->gref_id) {\n\t\tif (gnttab_query_foreign_access(gref->gref_id))\n\t\t\treturn;\n\n\t\tif (!gnttab_end_foreign_access_ref(gref->gref_id, 0))\n\t\t\treturn;\n\n\t\tgnttab_free_grant_reference(gref->gref_id);\n\t}\n\n\tgref_size--;\n\tlist_del(&gref->next_gref);\n\n\tif (gref->page)\n\t\t__free_page(gref->page);\n\n\tkfree(gref);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3559,
            "cve_id": "CVE-2022-23042",
            "code_snippet": "static int xennet_get_responses(struct netfront_queue *queue,\n\t\t\t\tstruct netfront_rx_info *rinfo, RING_IDX rp,\n\t\t\t\tstruct sk_buff_head *list,\n\t\t\t\tbool *need_xdp_flush)\n{\n\tstruct xen_netif_rx_response *rx = &rinfo->rx, rx_local;\n\tint max = XEN_NETIF_NR_SLOTS_MIN + (rx->status <= RX_COPY_THRESHOLD);\n\tRING_IDX cons = queue->rx.rsp_cons;\n\tstruct sk_buff *skb = xennet_get_rx_skb(queue, cons);\n\tstruct xen_netif_extra_info *extras = rinfo->extras;\n\tgrant_ref_t ref = xennet_get_rx_ref(queue, cons);\n\tstruct device *dev = &queue->info->netdev->dev;\n\tstruct bpf_prog *xdp_prog;\n\tstruct xdp_buff xdp;\n\tunsigned long ret;\n\tint slots = 1;\n\tint err = 0;\n\tu32 verdict;\n\n\tif (rx->flags & XEN_NETRXF_extra_info) {\n\t\terr = xennet_get_extras(queue, extras, rp);\n\t\tif (!err) {\n\t\t\tif (extras[XEN_NETIF_EXTRA_TYPE_XDP - 1].type) {\n\t\t\t\tstruct xen_netif_extra_info *xdp;\n\n\t\t\t\txdp = &extras[XEN_NETIF_EXTRA_TYPE_XDP - 1];\n\t\t\t\trx->offset = xdp->u.xdp.headroom;\n\t\t\t}\n\t\t}\n\t\tcons = queue->rx.rsp_cons;\n\t}\n\n\tfor (;;) {\n\t\tif (unlikely(rx->status < 0 ||\n\t\t\t     rx->offset + rx->status > XEN_PAGE_SIZE)) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tdev_warn(dev, \"rx->offset: %u, size: %d\\n\",\n\t\t\t\t\t rx->offset, rx->status);\n\t\t\txennet_move_rx_slot(queue, skb, ref);\n\t\t\terr = -EINVAL;\n\t\t\tgoto next;\n\t\t}\n\n\t\t/*\n\t\t * This definitely indicates a bug, either in this driver or in\n\t\t * the backend driver. In future this should flag the bad\n\t\t * situation to the system controller to reboot the backend.\n\t\t */\n\t\tif (ref == GRANT_INVALID_REF) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tdev_warn(dev, \"Bad rx response id %d.\\n\",\n\t\t\t\t\t rx->id);\n\t\t\terr = -EINVAL;\n\t\t\tgoto next;\n\t\t}\n\n\t\tret = gnttab_end_foreign_access_ref(ref, 0);\n\t\tBUG_ON(!ret);\n\n\t\tgnttab_release_grant_reference(&queue->gref_rx_head, ref);\n\n\t\trcu_read_lock();\n\t\txdp_prog = rcu_dereference(queue->xdp_prog);\n\t\tif (xdp_prog) {\n\t\t\tif (!(rx->flags & XEN_NETRXF_more_data)) {\n\t\t\t\t/* currently only a single page contains data */\n\t\t\t\tverdict = xennet_run_xdp(queue,\n\t\t\t\t\t\t\t skb_frag_page(&skb_shinfo(skb)->frags[0]),\n\t\t\t\t\t\t\t rx, xdp_prog, &xdp, need_xdp_flush);\n\t\t\t\tif (verdict != XDP_PASS)\n\t\t\t\t\terr = -EINVAL;\n\t\t\t} else {\n\t\t\t\t/* drop the frame */\n\t\t\t\terr = -EINVAL;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\nnext:\n\t\t__skb_queue_tail(list, skb);\n\t\tif (!(rx->flags & XEN_NETRXF_more_data))\n\t\t\tbreak;\n\n\t\tif (cons + slots == rp) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tdev_warn(dev, \"Need more slots\\n\");\n\t\t\terr = -ENOENT;\n\t\t\tbreak;\n\t\t}\n\n\t\tRING_COPY_RESPONSE(&queue->rx, cons + slots, &rx_local);\n\t\trx = &rx_local;\n\t\tskb = xennet_get_rx_skb(queue, cons + slots);\n\t\tref = xennet_get_rx_ref(queue, cons + slots);\n\t\tslots++;\n\t}\n\n\tif (unlikely(slots > max)) {\n\t\tif (net_ratelimit())\n\t\t\tdev_warn(dev, \"Too many slots\\n\");\n\t\terr = -E2BIG;\n\t}\n\n\tif (unlikely(err))\n\t\txennet_set_rx_rsp_cons(queue, cons + slots);\n\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3615,
            "cve_id": "CVE-2022-2590",
            "code_snippet": "struct page *follow_devmap_pmd(struct vm_area_struct *vma, unsigned long addr,\n\t\tpmd_t *pmd, int flags, struct dev_pagemap **pgmap)\n{\n\tunsigned long pfn = pmd_pfn(*pmd);\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct page *page;\n\n\tassert_spin_locked(pmd_lockptr(mm, pmd));\n\n\t/*\n\t * When we COW a devmap PMD entry, we split it into PTEs, so we should\n\t * not be in this function with `flags & FOLL_COW` set.\n\t */\n\tWARN_ONCE(flags & FOLL_COW, \"mm: In follow_devmap_pmd with FOLL_COW set\");\n\n\t/* FOLL_GET and FOLL_PIN are mutually exclusive. */\n\tif (WARN_ON_ONCE((flags & (FOLL_PIN | FOLL_GET)) ==\n\t\t\t (FOLL_PIN | FOLL_GET)))\n\t\treturn NULL;\n\n\tif (flags & FOLL_WRITE && !pmd_write(*pmd))\n\t\treturn NULL;\n\n\tif (pmd_present(*pmd) && pmd_devmap(*pmd))\n\t\t/* pass */;\n\telse\n\t\treturn NULL;\n\n\tif (flags & FOLL_TOUCH)\n\t\ttouch_pmd(vma, addr, pmd, flags & FOLL_WRITE);\n\n\t/*\n\t * device mapped pages can only be returned if the\n\t * caller will manage the page reference count.\n\t */\n\tif (!(flags & (FOLL_GET | FOLL_PIN)))\n\t\treturn ERR_PTR(-EEXIST);\n\n\tpfn += (addr & ~PMD_MASK) >> PAGE_SHIFT;\n\t*pgmap = get_dev_pagemap(pfn, *pgmap);\n\tif (!*pgmap)\n\t\treturn ERR_PTR(-EFAULT);\n\tpage = pfn_to_page(pfn);\n\tif (!try_grab_page(page, flags))\n\t\tpage = ERR_PTR(-ENOMEM);\n\n\treturn page;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3640,
            "cve_id": "CVE-2022-28796",
            "code_snippet": "void jbd2_journal_wait_updates(journal_t *journal)\n{\n\ttransaction_t *commit_transaction = journal->j_running_transaction;\n\n\tif (!commit_transaction)\n\t\treturn;\n\n\tspin_lock(&commit_transaction->t_handle_lock);\n\twhile (atomic_read(&commit_transaction->t_updates)) {\n\t\tDEFINE_WAIT(wait);\n\n\t\tprepare_to_wait(&journal->j_wait_updates, &wait,\n\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tif (atomic_read(&commit_transaction->t_updates)) {\n\t\t\tspin_unlock(&commit_transaction->t_handle_lock);\n\t\t\twrite_unlock(&journal->j_state_lock);\n\t\t\tschedule();\n\t\t\twrite_lock(&journal->j_state_lock);\n\t\t\tspin_lock(&commit_transaction->t_handle_lock);\n\t\t}\n\t\tfinish_wait(&journal->j_wait_updates, &wait);\n\t}\n\tspin_unlock(&commit_transaction->t_handle_lock);\n}",
            "detect_result": "NO\n\nExplanation:\nThe provided code appears to be part of the JBD2 (Journaling Block Device version 2) subsystem in the Linux kernel, which handles transactions for file system journaling. The function `jbd2_journal_wait_updates` waits until all updates in the current running transaction have been completed.\n\nHere's a breakdown of the code:\n\n1. It first checks if there is a running transaction (`commit_transaction`). If not, it simply returns.\n2. It then acquires a spin lock on the transaction's handle lock (`commit_transaction->t_handle_lock`) to ensure exclusive access to the transaction's state.\n3. Inside a while loop, it checks if there are any pending updates (`t_updates`) in the transaction. If there are, it prepares to wait by defining a wait queue entry (`DEFINE_WAIT(wait)`).\n4. It sets the task state to uninterruptible sleep (`TASK_UNINTERRUPTIBLE`) and adds the wait queue entry to the journal's wait queue (`j_wait_updates`).\n5. Before going to sleep, it releases both the spin lock and the write lock on the journal's state lock (`j_state_lock`), allowing other processes to make progress.\n6. The process then calls `schedule()` to go to sleep, waiting for a wake-up event.\n7. Upon waking up, it reacquires the write lock on the journal's state lock and the spin lock on the transaction's handle lock.\n8. After the loop exits (when there are no more updates), it releases the spin lock on the transaction's handle lock.\n\nThis code does not appear to have any obvious vulnerabilities such as race conditions, deadlocks, or buffer overflows. The use of locks and wait queues is appropriate for synchronization in a multi-threaded environment. However, without seeing the surrounding code and understanding the broader context, it's difficult to rule out all possible issues. But based on the snippet provided, there are no immediate red flags.",
            "final_result": 0
        },
        {
            "id": 3657,
            "cve_id": "CVE-2022-29582",
            "code_snippet": "static int io_timeout_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe,\n\t\t\t   bool is_timeout_link)\n{\n\tstruct io_timeout_data *data;\n\tunsigned flags;\n\tu32 off = READ_ONCE(sqe->off);\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->buf_index || sqe->len != 1 ||\n\t    sqe->splice_fd_in)\n\t\treturn -EINVAL;\n\tif (off && is_timeout_link)\n\t\treturn -EINVAL;\n\tflags = READ_ONCE(sqe->timeout_flags);\n\tif (flags & ~(IORING_TIMEOUT_ABS | IORING_TIMEOUT_CLOCK_MASK |\n\t\t      IORING_TIMEOUT_ETIME_SUCCESS))\n\t\treturn -EINVAL;\n\t/* more than one clock specified is invalid, obviously */\n\tif (hweight32(flags & IORING_TIMEOUT_CLOCK_MASK) > 1)\n\t\treturn -EINVAL;\n\n\tINIT_LIST_HEAD(&req->timeout.list);\n\treq->timeout.off = off;\n\tif (unlikely(off && !req->ctx->off_timeout_used))\n\t\treq->ctx->off_timeout_used = true;\n\n\tif (WARN_ON_ONCE(req_has_async_data(req)))\n\t\treturn -EFAULT;\n\tif (io_alloc_async_data(req))\n\t\treturn -ENOMEM;\n\n\tdata = req->async_data;\n\tdata->req = req;\n\tdata->flags = flags;\n\n\tif (get_timespec64(&data->ts, u64_to_user_ptr(sqe->addr)))\n\t\treturn -EFAULT;\n\n\tif (data->ts.tv_sec < 0 || data->ts.tv_nsec < 0)\n\t\treturn -EINVAL;\n\n\tdata->mode = io_translate_timeout_mode(flags);\n\thrtimer_init(&data->timer, io_timeout_get_clock(data), data->mode);\n\n\tif (is_timeout_link) {\n\t\tstruct io_submit_link *link = &req->ctx->submit_state.link;\n\n\t\tif (!link->head)\n\t\t\treturn -EINVAL;\n\t\tif (link->last->opcode == IORING_OP_LINK_TIMEOUT)\n\t\t\treturn -EINVAL;\n\t\treq->timeout.head = link->last;\n\t\tlink->last->flags |= REQ_F_ARM_LTIMEOUT;\n\t}\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3771,
            "cve_id": "CVE-2022-3564",
            "code_snippet": "static int l2cap_stream_rx(struct l2cap_chan *chan, struct l2cap_ctrl *control,\n\t\t\t   struct sk_buff *skb)\n{\n\tBT_DBG(\"chan %p, control %p, skb %p, state %d\", chan, control, skb,\n\t       chan->rx_state);\n\n\tif (l2cap_classify_txseq(chan, control->txseq) ==\n\t    L2CAP_TXSEQ_EXPECTED) {\n\t\tl2cap_pass_to_tx(chan, control);\n\n\t\tBT_DBG(\"buffer_seq %u->%u\", chan->buffer_seq,\n\t\t       __next_seq(chan, chan->buffer_seq));\n\n\t\tchan->buffer_seq = __next_seq(chan, chan->buffer_seq);\n\n\t\tl2cap_reassemble_sdu(chan, skb, control);\n\t} else {\n\t\tif (chan->sdu) {\n\t\t\tkfree_skb(chan->sdu);\n\t\t\tchan->sdu = NULL;\n\t\t}\n\t\tchan->sdu_last_frag = NULL;\n\t\tchan->sdu_len = 0;\n\n\t\tif (skb) {\n\t\t\tBT_DBG(\"Freeing %p\", skb);\n\t\t\tkfree_skb(skb);\n\t\t}\n\t}\n\n\tchan->last_acked_seq = control->txseq;\n\tchan->expected_tx_seq = __next_seq(chan, control->txseq);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3776,
            "cve_id": "CVE-2022-3566",
            "code_snippet": "int tcp_setsockopt(struct sock *sk, int level, int optname, sockptr_t optval,\n\t\t   unsigned int optlen)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (level != SOL_TCP)\n\t\treturn icsk->icsk_af_ops->setsockopt(sk, level, optname,\n\t\t\t\t\t\t     optval, optlen);\n\treturn do_tcp_setsockopt(sk, level, optname, optval, optlen);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3780,
            "cve_id": "CVE-2022-3567",
            "code_snippet": "int sock_common_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t   char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\treturn sk->sk_prot->getsockopt(sk, level, optname, optval, optlen);\n}",
            "detect_result": "NO\n\nExplanation: The provided code snippet is a function that appears to be part of a networking stack, specifically handling the `getsockopt` operation for sockets. It takes a `socket` structure, option level, option name, user-space pointers for the option value and its length, and delegates the actual work to the protocol-specific `getsockopt` function.\n\nWithout additional context about how this function is used and the implementation details of `sk->sk_prot->getsockopt`, it's challenging to definitively state that there is a vulnerability just from this snippet alone. However, based on the given code, there are no obvious issues such as buffer overflows, improper memory access, or other common vulnerabilities. The function seems to be correctly passing parameters to the protocol-specific handler. Therefore, the answer is NO, the code snippet itself does not indicate a vulnerability.",
            "final_result": 0
        },
        {
            "id": 3781,
            "cve_id": "CVE-2022-3567",
            "code_snippet": "int inet_accept(struct socket *sock, struct socket *newsock, int flags,\n\t\tbool kern)\n{\n\tstruct sock *sk1 = sock->sk;\n\tint err = -EINVAL;\n\tstruct sock *sk2 = sk1->sk_prot->accept(sk1, flags, &err, kern);\n\n\tif (!sk2)\n\t\tgoto do_err;\n\n\tlock_sock(sk2);\n\n\tsock_rps_record_flow(sk2);\n\tWARN_ON(!((1 << sk2->sk_state) &\n\t\t  (TCPF_ESTABLISHED | TCPF_SYN_RECV |\n\t\t  TCPF_CLOSE_WAIT | TCPF_CLOSE)));\n\n\tsock_graft(sk2, newsock);\n\n\tnewsock->state = SS_CONNECTED;\n\terr = 0;\n\trelease_sock(sk2);\ndo_err:\n\treturn err;\n}",
            "detect_result": "NO\n\nThe provided code snippet appears to be part of a network stack implementation, specifically handling the acceptance of a new connection in a TCP/IP environment. Here's a breakdown of its behavior:\n\n1. **Initialization and Error Handling**: The function `inet_accept` initializes an error code `err` to `-EINVAL` (Invalid argument). It then attempts to accept a new connection using the `accept` method of the protocol associated with the socket (`sk1->sk_prot->accept`).\n\n2. **Socket Creation**: If the `accept` call is successful, it returns a pointer to a new socket structure (`sk2`). If not, the function jumps to the `do_err` label and returns the error.\n\n3. **Locking**: The new socket (`sk2`) is locked using `lock_sock`, which is necessary for thread safety when accessing or modifying the socket's state.\n\n4. **Flow Recording and State Check**: The function records the flow for RPS (Receive Packet Steering) and checks if the state of the new socket is one of the expected states (ESTABLISHED, SYN_RECV, CLOSE_WAIT, CLOSE). If the state is not one of these, a warning is issued.\n\n5. **Socket Grafting and State Update**: The new socket is grafted onto the `newsock` structure, and the state of `newsock` is set to `SS_CONNECTED`.\n\n6. **Unlocking and Return**: The socket is unlocked with `release_sock`, and the function returns `0` to indicate success.\n\n### Vulnerability Assessment\n\nBased on the provided code snippet, there are no obvious vulnerabilities such as buffer overflows, improper memory management, or insecure data handling. The code follows typical patterns for socket handling in a network stack, including proper locking and state checking. However, without the full context of the surrounding code and the specific implementation details of functions like `lock_sock`, `sock_graft`, and `release_sock`, it's challenging to rule out all potential issues. \n\nGiven the information provided, the code does not exhibit any immediate signs of vulnerability. Therefore, the answer is NO.",
            "final_result": 0
        },
        {
            "id": 3792,
            "cve_id": "CVE-2022-3623",
            "code_snippet": "static struct page *follow_pmd_mask(struct vm_area_struct *vma,\n\t\t\t\t    unsigned long address, pud_t *pudp,\n\t\t\t\t    unsigned int flags,\n\t\t\t\t    struct follow_page_context *ctx)\n{\n\tpmd_t *pmd, pmdval;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\tstruct mm_struct *mm = vma->vm_mm;\n\n\tpmd = pmd_offset(pudp, address);\n\t/*\n\t * The READ_ONCE() will stabilize the pmdval in a register or\n\t * on the stack so that it will stop changing under the code.\n\t */\n\tpmdval = READ_ONCE(*pmd);\n\tif (pmd_none(pmdval))\n\t\treturn no_page_table(vma, flags);\n\tif (pmd_huge(pmdval) && is_vm_hugetlb_page(vma)) {\n\t\tpage = follow_huge_pmd(mm, address, pmd, flags);\n\t\tif (page)\n\t\t\treturn page;\n\t\treturn no_page_table(vma, flags);\n\t}\n\tif (is_hugepd(__hugepd(pmd_val(pmdval)))) {\n\t\tpage = follow_huge_pd(vma, address,\n\t\t\t\t      __hugepd(pmd_val(pmdval)), flags,\n\t\t\t\t      PMD_SHIFT);\n\t\tif (page)\n\t\t\treturn page;\n\t\treturn no_page_table(vma, flags);\n\t}\nretry:\n\tif (!pmd_present(pmdval)) {\n\t\t/*\n\t\t * Should never reach here, if thp migration is not supported;\n\t\t * Otherwise, it must be a thp migration entry.\n\t\t */\n\t\tVM_BUG_ON(!thp_migration_supported() ||\n\t\t\t\t  !is_pmd_migration_entry(pmdval));\n\n\t\tif (likely(!(flags & FOLL_MIGRATION)))\n\t\t\treturn no_page_table(vma, flags);\n\n\t\tpmd_migration_entry_wait(mm, pmd);\n\t\tpmdval = READ_ONCE(*pmd);\n\t\t/*\n\t\t * MADV_DONTNEED may convert the pmd to null because\n\t\t * mmap_lock is held in read mode\n\t\t */\n\t\tif (pmd_none(pmdval))\n\t\t\treturn no_page_table(vma, flags);\n\t\tgoto retry;\n\t}\n\tif (pmd_devmap(pmdval)) {\n\t\tptl = pmd_lock(mm, pmd);\n\t\tpage = follow_devmap_pmd(vma, address, pmd, flags, &ctx->pgmap);\n\t\tspin_unlock(ptl);\n\t\tif (page)\n\t\t\treturn page;\n\t}\n\tif (likely(!pmd_trans_huge(pmdval)))\n\t\treturn follow_page_pte(vma, address, pmd, flags, &ctx->pgmap);\n\n\tif ((flags & FOLL_NUMA) && pmd_protnone(pmdval))\n\t\treturn no_page_table(vma, flags);\n\nretry_locked:\n\tptl = pmd_lock(mm, pmd);\n\tif (unlikely(pmd_none(*pmd))) {\n\t\tspin_unlock(ptl);\n\t\treturn no_page_table(vma, flags);\n\t}\n\tif (unlikely(!pmd_present(*pmd))) {\n\t\tspin_unlock(ptl);\n\t\tif (likely(!(flags & FOLL_MIGRATION)))\n\t\t\treturn no_page_table(vma, flags);\n\t\tpmd_migration_entry_wait(mm, pmd);\n\t\tgoto retry_locked;\n\t}\n\tif (unlikely(!pmd_trans_huge(*pmd))) {\n\t\tspin_unlock(ptl);\n\t\treturn follow_page_pte(vma, address, pmd, flags, &ctx->pgmap);\n\t}\n\tif (flags & FOLL_SPLIT_PMD) {\n\t\tint ret;\n\t\tpage = pmd_page(*pmd);\n\t\tif (is_huge_zero_page(page)) {\n\t\t\tspin_unlock(ptl);\n\t\t\tret = 0;\n\t\t\tsplit_huge_pmd(vma, pmd, address);\n\t\t\tif (pmd_trans_unstable(pmd))\n\t\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tspin_unlock(ptl);\n\t\t\tsplit_huge_pmd(vma, pmd, address);\n\t\t\tret = pte_alloc(mm, pmd) ? -ENOMEM : 0;\n\t\t}\n\n\t\treturn ret ? ERR_PTR(ret) :\n\t\t\tfollow_page_pte(vma, address, pmd, flags, &ctx->pgmap);\n\t}\n\tpage = follow_trans_huge_pmd(vma, address, pmd, flags);\n\tspin_unlock(ptl);\n\tctx->page_mask = HPAGE_PMD_NR - 1;\n\treturn page;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3827,
            "cve_id": "CVE-2022-39188",
            "code_snippet": "static inline void\ntlb_update_vma_flags(struct mmu_gather *tlb, struct vm_area_struct *vma) { }",
            "detect_result": "NO\n\nThe provided code snippet defines an empty static inline function `tlb_update_vma_flags` that takes two parameters: a pointer to a `mmu_gather` structure and a pointer to a `vm_area_struct` structure. Since the function body is empty, it does not perform any operations and therefore cannot introduce vulnerabilities on its own. However, vulnerabilities could potentially arise from how this function is used elsewhere in the codebase, but based solely on the given snippet, there is no indication of a vulnerability.",
            "final_result": 0
        },
        {
            "id": 3867,
            "cve_id": "CVE-2022-41849",
            "code_snippet": "static int ufx_ops_open(struct fb_info *info, int user)\n{\n\tstruct ufx_data *dev = info->par;\n\n\t/* fbcon aggressively connects to first framebuffer it finds,\n\t * preventing other clients (X) from working properly. Usually\n\t * not what the user wants. Fail by default with option to enable. */\n\tif (user == 0 && !console)\n\t\treturn -EBUSY;\n\n\t/* If the USB device is gone, we don't accept new opens */\n\tif (dev->virtualized)\n\t\treturn -ENODEV;\n\n\tdev->fb_count++;\n\n\tkref_get(&dev->kref);\n\n\tif (fb_defio && (info->fbdefio == NULL)) {\n\t\t/* enable defio at last moment if not disabled by client */\n\n\t\tstruct fb_deferred_io *fbdefio;\n\n\t\tfbdefio = kzalloc(sizeof(*fbdefio), GFP_KERNEL);\n\t\tif (fbdefio) {\n\t\t\tfbdefio->delay = UFX_DEFIO_WRITE_DELAY;\n\t\t\tfbdefio->deferred_io = ufx_dpy_deferred_io;\n\t\t}\n\n\t\tinfo->fbdefio = fbdefio;\n\t\tfb_deferred_io_init(info);\n\t}\n\n\tpr_debug(\"open /dev/fb%d user=%d fb_info=%p count=%d\",\n\t\tinfo->node, user, info, dev->fb_count);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3899,
            "cve_id": "CVE-2022-45869",
            "code_snippet": "static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)\n{\n\tbool is_tdp_mmu_fault = is_tdp_mmu(vcpu->arch.mmu);\n\n\tunsigned long mmu_seq;\n\tint r;\n\n\tfault->gfn = fault->addr >> PAGE_SHIFT;\n\tfault->slot = kvm_vcpu_gfn_to_memslot(vcpu, fault->gfn);\n\n\tif (page_fault_handle_page_track(vcpu, fault))\n\t\treturn RET_PF_EMULATE;\n\n\tr = fast_page_fault(vcpu, fault);\n\tif (r != RET_PF_INVALID)\n\t\treturn r;\n\n\tr = mmu_topup_memory_caches(vcpu, false);\n\tif (r)\n\t\treturn r;\n\n\tmmu_seq = vcpu->kvm->mmu_invalidate_seq;\n\tsmp_rmb();\n\n\tr = kvm_faultin_pfn(vcpu, fault);\n\tif (r != RET_PF_CONTINUE)\n\t\treturn r;\n\n\tr = handle_abnormal_pfn(vcpu, fault, ACC_ALL);\n\tif (r != RET_PF_CONTINUE)\n\t\treturn r;\n\n\tr = RET_PF_RETRY;\n\n\tif (is_tdp_mmu_fault)\n\t\tread_lock(&vcpu->kvm->mmu_lock);\n\telse\n\t\twrite_lock(&vcpu->kvm->mmu_lock);\n\n\tif (is_page_fault_stale(vcpu, fault, mmu_seq))\n\t\tgoto out_unlock;\n\n\tr = make_mmu_pages_available(vcpu);\n\tif (r)\n\t\tgoto out_unlock;\n\n\tif (is_tdp_mmu_fault)\n\t\tr = kvm_tdp_mmu_map(vcpu, fault);\n\telse\n\t\tr = __direct_map(vcpu, fault);\n\nout_unlock:\n\tif (is_tdp_mmu_fault)\n\t\tread_unlock(&vcpu->kvm->mmu_lock);\n\telse\n\t\twrite_unlock(&vcpu->kvm->mmu_lock);\n\tkvm_release_pfn_clean(fault->pfn);\n\treturn r;\n}",
            "detect_result": "NO\n\nThe provided code appears to be handling page faults in a KVM (Kernel-based Virtual Machine) environment. It includes several checks and operations to manage memory mappings and faults efficiently. The code also uses locking mechanisms (`read_lock`, `write_lock`, `read_unlock`, `write_unlock`) to ensure thread safety when accessing shared resources.\n\nWhile the code is complex and involves low-level operations, there are no obvious vulnerabilities such as buffer overflows, improper input validation, or insecure memory access patterns visible from the snippet provided. However, it's important to note that the security of this function would depend on the correctness and security of the functions it calls (e.g., `fast_page_fault`, `kvm_faultin_pfn`, `handle_abnormal_pfn`, `make_mmu_pages_available`, `kvm_tdp_mmu_map`, `__direct_map`), as well as the overall context in which this function is used.\n\nFor a comprehensive security assessment, one would need to review the entire codebase and understand how this function interacts with other parts of the system.",
            "final_result": 0
        },
        {
            "id": 3902,
            "cve_id": "CVE-2022-45888",
            "code_snippet": "static void xillyusb_disconnect(struct usb_interface *interface)\n{\n\tstruct xillyusb_dev *xdev = usb_get_intfdata(interface);\n\tstruct xillyusb_endpoint *msg_ep = xdev->msg_ep;\n\tstruct xillyfifo *fifo = &msg_ep->fifo;\n\tint rc;\n\tint i;\n\n\txillybus_cleanup_chrdev(xdev, &interface->dev);\n\n\t/*\n\t * Try to send OPCODE_QUIESCE, which will fail silently if the device\n\t * was disconnected, but makes sense on module unload.\n\t */\n\n\tmsg_ep->wake_on_drain = true;\n\txillyusb_send_opcode(xdev, ~0, OPCODE_QUIESCE, 0);\n\n\t/*\n\t * If the device has been disconnected, sending the opcode causes\n\t * a global device error with xdev->error, if such error didn't\n\t * occur earlier. Hence timing out means that the USB link is fine,\n\t * but somehow the message wasn't sent. Should never happen.\n\t */\n\n\trc = wait_event_interruptible_timeout(fifo->waitq,\n\t\t\t\t\t      msg_ep->drained || xdev->error,\n\t\t\t\t\t      XILLY_RESPONSE_TIMEOUT);\n\n\tif (!rc)\n\t\tdev_err(&interface->dev,\n\t\t\t\"Weird timeout condition on sending quiesce request.\\n\");\n\n\treport_io_error(xdev, -ENODEV); /* Discourage further activity */\n\n\t/*\n\t * This device driver is declared with soft_unbind set, or else\n\t * sending OPCODE_QUIESCE above would always fail. The price is\n\t * that the USB framework didn't kill outstanding URBs, so it has\n\t * to be done explicitly before returning from this call.\n\t */\n\n\tfor (i = 0; i < xdev->num_channels; i++) {\n\t\tstruct xillyusb_channel *chan = &xdev->channels[i];\n\n\t\t/*\n\t\t * Lock taken to prevent chan->out_ep from changing. It also\n\t\t * ensures xillyusb_open() and xillyusb_flush() don't access\n\t\t * xdev->dev after being nullified below.\n\t\t */\n\t\tmutex_lock(&chan->lock);\n\t\tif (chan->out_ep)\n\t\t\tendpoint_quiesce(chan->out_ep);\n\t\tmutex_unlock(&chan->lock);\n\t}\n\n\tendpoint_quiesce(xdev->in_ep);\n\tendpoint_quiesce(xdev->msg_ep);\n\n\tusb_set_intfdata(interface, NULL);\n\n\txdev->dev = NULL;\n\n\tkref_put(&xdev->kref, cleanup_dev);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4018,
            "cve_id": "CVE-2023-1582",
            "code_snippet": "static void smaps_pte_entry(pte_t *pte, unsigned long addr,\n\t\tstruct mm_walk *walk)\n{\n\tstruct mem_size_stats *mss = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tbool locked = !!(vma->vm_flags & VM_LOCKED);\n\tstruct page *page = NULL;\n\n\tif (pte_present(*pte)) {\n\t\tpage = vm_normal_page(vma, addr, *pte);\n\t} else if (is_swap_pte(*pte)) {\n\t\tswp_entry_t swpent = pte_to_swp_entry(*pte);\n\n\t\tif (!non_swap_entry(swpent)) {\n\t\t\tint mapcount;\n\n\t\t\tmss->swap += PAGE_SIZE;\n\t\t\tmapcount = swp_swapcount(swpent);\n\t\t\tif (mapcount >= 2) {\n\t\t\t\tu64 pss_delta = (u64)PAGE_SIZE << PSS_SHIFT;\n\n\t\t\t\tdo_div(pss_delta, mapcount);\n\t\t\t\tmss->swap_pss += pss_delta;\n\t\t\t} else {\n\t\t\t\tmss->swap_pss += (u64)PAGE_SIZE << PSS_SHIFT;\n\t\t\t}\n\t\t} else if (is_pfn_swap_entry(swpent))\n\t\t\tpage = pfn_swap_entry_to_page(swpent);\n\t} else {\n\t\tsmaps_pte_hole_lookup(addr, walk);\n\t\treturn;\n\t}\n\n\tif (!page)\n\t\treturn;\n\n\tsmaps_account(mss, page, false, pte_young(*pte), pte_dirty(*pte), locked);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4043,
            "cve_id": "CVE-2023-2006",
            "code_snippet": "static struct rxrpc_bundle *rxrpc_alloc_bundle(struct rxrpc_conn_parameters *cp,\n\t\t\t\t\t       gfp_t gfp)\n{\n\tstruct rxrpc_bundle *bundle;\n\n\tbundle = kzalloc(sizeof(*bundle), gfp);\n\tif (bundle) {\n\t\tbundle->params = *cp;\n\t\trxrpc_get_peer(bundle->params.peer);\n\t\trefcount_set(&bundle->ref, 1);\n\t\tspin_lock_init(&bundle->channel_lock);\n\t\tINIT_LIST_HEAD(&bundle->waiting_calls);\n\t}\n\treturn bundle;\n}",
            "detect_result": "NO\n\nExplanation:\nThe provided code snippet defines a function `rxrpc_alloc_bundle` that allocates and initializes a `struct rxrpc_bundle`. Here's a breakdown of its behavior:\n\n1. It attempts to allocate memory for a `struct rxrpc_bundle` using `kzalloc`, which also initializes the allocated memory to zero.\n2. If the allocation is successful (`if (bundle)`), it proceeds to initialize various fields of the `bundle` structure:\n   - Copies the connection parameters from `cp` to `bundle->params`.\n   - Increments the reference count of the peer associated with the connection parameters using `rxrpc_get_peer`.\n   - Initializes a reference count for the bundle itself using `refcount_set`.\n   - Initializes a spinlock for synchronizing access to the bundle's channels.\n   - Initializes a list head for managing waiting calls.\n\nFrom the provided code, there are no obvious vulnerabilities such as buffer overflows, use-after-free, or improper memory management. The code follows typical patterns for memory allocation and initialization in kernel space. However, without more context about how this function is used and the definitions of other functions and structures involved, it's not possible to identify potential vulnerabilities that might arise from interactions with other parts of the system.",
            "final_result": 0
        },
        {
            "id": 4044,
            "cve_id": "CVE-2023-2006",
            "code_snippet": "static void rxrpc_unbundle_conn(struct rxrpc_connection *conn)\n{\n\tstruct rxrpc_bundle *bundle = conn->bundle;\n\tstruct rxrpc_local *local = bundle->params.local;\n\tunsigned int bindex;\n\tbool need_drop = false, need_put = false;\n\tint i;\n\n\t_enter(\"C=%x\", conn->debug_id);\n\n\tif (conn->flags & RXRPC_CONN_FINAL_ACK_MASK)\n\t\trxrpc_process_delayed_final_acks(conn, true);\n\n\tspin_lock(&bundle->channel_lock);\n\tbindex = conn->bundle_shift / RXRPC_MAXCALLS;\n\tif (bundle->conns[bindex] == conn) {\n\t\t_debug(\"clear slot %u\", bindex);\n\t\tbundle->conns[bindex] = NULL;\n\t\tfor (i = 0; i < RXRPC_MAXCALLS; i++)\n\t\t\tclear_bit(conn->bundle_shift + i, &bundle->avail_chans);\n\t\tneed_drop = true;\n\t}\n\tspin_unlock(&bundle->channel_lock);\n\n\t/* If there are no more connections, remove the bundle */\n\tif (!bundle->avail_chans) {\n\t\t_debug(\"maybe unbundle\");\n\t\tspin_lock(&local->client_bundles_lock);\n\n\t\tfor (i = 0; i < ARRAY_SIZE(bundle->conns); i++)\n\t\t\tif (bundle->conns[i])\n\t\t\t\tbreak;\n\t\tif (i == ARRAY_SIZE(bundle->conns) && !bundle->params.exclusive) {\n\t\t\t_debug(\"erase bundle\");\n\t\t\trb_erase(&bundle->local_node, &local->client_bundles);\n\t\t\tneed_put = true;\n\t\t}\n\n\t\tspin_unlock(&local->client_bundles_lock);\n\t\tif (need_put)\n\t\t\trxrpc_put_bundle(bundle);\n\t}\n\n\tif (need_drop)\n\t\trxrpc_put_connection(conn);\n\t_leave(\"\");\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4157,
            "cve_id": "CVE-2023-3108",
            "code_snippet": "static int skcipher_recvmsg(struct kiocb *unused, struct socket *sock,\n\t\t\t    struct msghdr *msg, size_t ignored, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct alg_sock *ask = alg_sk(sk);\n\tstruct skcipher_ctx *ctx = ask->private;\n\tunsigned bs = crypto_ablkcipher_blocksize(crypto_ablkcipher_reqtfm(\n\t\t&ctx->req));\n\tstruct skcipher_sg_list *sgl;\n\tstruct scatterlist *sg;\n\tint err = -EAGAIN;\n\tint used;\n\tlong copied = 0;\n\n\tlock_sock(sk);\n\twhile (iov_iter_count(&msg->msg_iter)) {\n\t\tsgl = list_first_entry(&ctx->tsgl,\n\t\t\t\t       struct skcipher_sg_list, list);\n\t\tsg = sgl->sg;\n\n\t\twhile (!sg->length)\n\t\t\tsg++;\n\n\t\tused = ctx->used;\n\t\tif (!used) {\n\t\t\terr = skcipher_wait_for_data(sk, flags);\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\t\t}\n\n\t\tused = min_t(unsigned long, used, iov_iter_count(&msg->msg_iter));\n\n\t\tused = af_alg_make_sg(&ctx->rsgl, &msg->msg_iter, used);\n\t\terr = used;\n\t\tif (err < 0)\n\t\t\tgoto unlock;\n\n\t\tif (ctx->more || used < ctx->used)\n\t\t\tused -= used % bs;\n\n\t\terr = -EINVAL;\n\t\tif (!used)\n\t\t\tgoto free;\n\n\t\tablkcipher_request_set_crypt(&ctx->req, sg,\n\t\t\t\t\t     ctx->rsgl.sg, used,\n\t\t\t\t\t     ctx->iv);\n\n\t\terr = af_alg_wait_for_completion(\n\t\t\t\tctx->enc ?\n\t\t\t\t\tcrypto_ablkcipher_encrypt(&ctx->req) :\n\t\t\t\t\tcrypto_ablkcipher_decrypt(&ctx->req),\n\t\t\t\t&ctx->completion);\n\nfree:\n\t\taf_alg_free_sg(&ctx->rsgl);\n\n\t\tif (err)\n\t\t\tgoto unlock;\n\n\t\tcopied += used;\n\t\tskcipher_pull_sgl(sk, used);\n\t\tiov_iter_advance(&msg->msg_iter, used);\n\t}\n\n\terr = 0;\n\nunlock:\n\tskcipher_wmem_wakeup(sk);\n\trelease_sock(sk);\n\n\treturn copied ?: err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4181,
            "cve_id": "CVE-2023-32250",
            "code_snippet": "static void ksmbd_conn_lock(struct ksmbd_conn *conn)\n{\n\tmutex_lock(&conn->srv_mutex);\n}",
            "detect_result": "NO\n\nExplanation: The provided code snippet is a function that locks a mutex associated with a connection structure (`ksmbd_conn`). The function `mutex_lock` is used to acquire the lock, which is generally a safe operation. However, without additional context about how this function is used within the broader codebase, it's not possible to definitively say if there are vulnerabilities related to this specific function call. The function itself does not introduce any obvious vulnerabilities such as buffer overflows, format string issues, or improper error handling. It simply attempts to acquire a lock, which is a standard synchronization mechanism. If the mutex is properly initialized and used elsewhere in the code, this function should be safe.",
            "final_result": 0
        },
        {
            "id": 4182,
            "cve_id": "CVE-2023-32250",
            "code_snippet": "bool ksmbd_conn_alive(struct ksmbd_conn *conn)\n{\n\tif (!ksmbd_server_running())\n\t\treturn false;\n\n\tif (conn->status == KSMBD_SESS_EXITING)\n\t\treturn false;\n\n\tif (kthread_should_stop())\n\t\treturn false;\n\n\tif (atomic_read(&conn->stats.open_files_count) > 0)\n\t\treturn true;\n\n\t/*\n\t * Stop current session if the time that get last request from client\n\t * is bigger than deadtime user configured and opening file count is\n\t * zero.\n\t */\n\tif (server_conf.deadtime > 0 &&\n\t    time_after(jiffies, conn->last_active + server_conf.deadtime)) {\n\t\tksmbd_debug(CONN, \"No response from client in %lu minutes\\n\",\n\t\t\t    server_conf.deadtime / SMB_ECHO_INTERVAL);\n\t\treturn false;\n\t}\n\treturn true;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4183,
            "cve_id": "CVE-2023-32250",
            "code_snippet": "static inline int check_conn_state(struct ksmbd_work *work)\n{\n\tstruct smb_hdr *rsp_hdr;\n\n\tif (ksmbd_conn_exiting(work) || ksmbd_conn_need_reconnect(work)) {\n\t\trsp_hdr = work->response_buf;\n\t\trsp_hdr->Status.CifsError = STATUS_CONNECTION_DISCONNECTED;\n\t\treturn 1;\n\t}\n\treturn 0;\n}",
            "detect_result": "NO\n\nExplanation: The provided code snippet defines a function `check_conn_state` that checks if a connection associated with a `struct ksmbd_work` object is exiting or needs to be reconnected. If either condition is true, it sets an error status in the response header and returns 1. Otherwise, it returns 0. This function does not appear to have any obvious vulnerabilities such as buffer overflows, use-after-free, or improper input validation based on the given code. However, without more context about how this function is used and the definitions of `ksmbd_conn_exiting`, `ksmbd_conn_need_reconnect`, and `struct smb_hdr`, a more comprehensive security analysis cannot be performed.",
            "final_result": 0
        },
        {
            "id": 4242,
            "cve_id": "CVE-2023-33951",
            "code_snippet": "int vmw_user_bo_synccpu_ioctl(struct drm_device *dev, void *data,\n\t\t\t      struct drm_file *file_priv)\n{\n\tstruct drm_vmw_synccpu_arg *arg =\n\t\t(struct drm_vmw_synccpu_arg *) data;\n\tstruct vmw_bo *vbo;\n\tint ret;\n\n\tif ((arg->flags & (drm_vmw_synccpu_read | drm_vmw_synccpu_write)) == 0\n\t    || (arg->flags & ~(drm_vmw_synccpu_read | drm_vmw_synccpu_write |\n\t\t\t       drm_vmw_synccpu_dontblock |\n\t\t\t       drm_vmw_synccpu_allow_cs)) != 0) {\n\t\tDRM_ERROR(\"Illegal synccpu flags.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (arg->op) {\n\tcase drm_vmw_synccpu_grab:\n\t\tret = vmw_user_bo_lookup(file_priv, arg->handle, &vbo);\n\t\tif (unlikely(ret != 0))\n\t\t\treturn ret;\n\n\t\tret = vmw_user_bo_synccpu_grab(vbo, arg->flags);\n\t\tvmw_bo_unreference(&vbo);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tif (ret == -ERESTARTSYS || ret == -EBUSY)\n\t\t\t\treturn -EBUSY;\n\t\t\tDRM_ERROR(\"Failed synccpu grab on handle 0x%08x.\\n\",\n\t\t\t\t  (unsigned int) arg->handle);\n\t\t\treturn ret;\n\t\t}\n\t\tbreak;\n\tcase drm_vmw_synccpu_release:\n\t\tret = vmw_user_bo_synccpu_release(file_priv,\n\t\t\t\t\t\t  arg->handle,\n\t\t\t\t\t\t  arg->flags);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Failed synccpu release on handle 0x%08x.\\n\",\n\t\t\t\t  (unsigned int) arg->handle);\n\t\t\treturn ret;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Invalid synccpu operation.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4243,
            "cve_id": "CVE-2023-33951",
            "code_snippet": "static int vmw_user_bo_synccpu_release(struct drm_file *filp,\n\t\t\t\t       uint32_t handle,\n\t\t\t\t       uint32_t flags)\n{\n\tstruct vmw_bo *vmw_bo;\n\tint ret = vmw_user_bo_lookup(filp, handle, &vmw_bo);\n\n\tif (!ret) {\n\t\tif (!(flags & drm_vmw_synccpu_allow_cs)) {\n\t\t\tatomic_dec(&vmw_bo->cpu_writers);\n\t\t}\n\t\tttm_bo_put(&vmw_bo->tbo);\n\t}\n\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4244,
            "cve_id": "CVE-2023-33951",
            "code_snippet": "static int vmw_translate_guest_ptr(struct vmw_private *dev_priv,\n\t\t\t\t   struct vmw_sw_context *sw_context,\n\t\t\t\t   SVGAGuestPtr *ptr,\n\t\t\t\t   struct vmw_bo **vmw_bo_p)\n{\n\tstruct vmw_bo *vmw_bo;\n\tuint32_t handle = ptr->gmrId;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use GMR region.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM,\n\t\t\t     VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM);\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n\tttm_bo_put(&vmw_bo->tbo);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->location = ptr;\n\treloc->vbo = vmw_bo;\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
            "detect_result": "NO\n\nExplanation of the code behavior:\nThe function `vmw_translate_guest_ptr` is designed to translate a guest pointer (`SVGAGuestPtr`) into a buffer object (`vmw_bo`) that can be used within the VMware graphics driver context. Here's a step-by-step breakdown of what the function does:\n\n1. It retrieves the buffer object handle from the guest pointer structure (`ptr->gmrId`).\n2. It attempts to look up the buffer object using this handle via `vmw_user_bo_lookup`. If the lookup fails, it logs an error message and returns an error code.\n3. If the buffer object is successfully found, it sets the placement of the buffer object to either GMR (Guest Memory Region) or VRAM (Video RAM) domains.\n4. The buffer object is then added to the validation context for further processing with `vmw_validation_add_bo`.\n5. After adding the buffer object to the validation context, the reference count of the buffer object is decremented with `ttm_bo_put`.\n6. A relocation structure is allocated to store information about the buffer object and its location.\n7. If memory allocation for the relocation structure fails, the function returns `-ENOMEM`.\n8. The relocation structure is populated with the buffer object and its location, and it is added to a list of relocations in the software context.\n9. Finally, the function returns 0 to indicate success.\n\nVulnerability assessment:\nThe code does not appear to have obvious vulnerabilities such as buffer overflows, use-after-free, or improper input validation. However, there are a few considerations:\n- The function assumes that `vmw_user_bo_lookup` correctly handles all possible values of `handle` and returns valid pointers or appropriate error codes.\n- The function does not perform extensive validation on the `ptr` structure beyond checking the return value of `vmw_user_bo_lookup`.\n- The use of `ttm_bo_put` after adding the buffer object to the validation context might be problematic if the validation process requires the buffer object to remain valid until the end of the operation. However, without more context about how `vmw_validation_add_bo` works, it's difficult to say for certain.\n\nGiven the provided code snippet, there are no immediate signs of critical vulnerabilities, but further analysis would be required to ensure the safety of the entire system.",
            "final_result": 0
        },
        {
            "id": 4245,
            "cve_id": "CVE-2023-33951",
            "code_snippet": "static int vmw_translate_mob_ptr(struct vmw_private *dev_priv,\n\t\t\t\t struct vmw_sw_context *sw_context,\n\t\t\t\t SVGAMobId *id,\n\t\t\t\t struct vmw_bo **vmw_bo_p)\n{\n\tstruct vmw_bo *vmw_bo;\n\tuint32_t handle = *id;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use MOB buffer.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_MOB, VMW_BO_DOMAIN_MOB);\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n\tttm_bo_put(&vmw_bo->tbo);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->mob_loc = id;\n\treloc->vbo = vmw_bo;\n\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
            "detect_result": "NO\n\nExplanation of the code behavior:\n1. The function `vmw_translate_mob_ptr` takes several parameters including a pointer to a `vmw_private` structure, a pointer to a `vmw_sw_context` structure, a pointer to an `SVGAMobId`, and a pointer to a `vmw_bo` pointer.\n2. It initializes a `vmw_bo` pointer and sets a `handle` variable to the value pointed to by `id`.\n3. It then attempts to look up a user buffer object (BO) using the `vmw_user_bo_lookup` function with the file pointer from the `sw_context` and the `handle`. If this lookup fails, it logs a debug message and returns an error code.\n4. If the lookup is successful, it sets the placement of the BO to be within the MOB domain and adds the BO to the validation context associated with the `sw_context`.\n5. After adding the BO to the validation context, it releases the reference to the BO using `ttm_bo_put`.\n6. It allocates memory for a `vmw_relocation` structure and checks if the allocation was successful. If not, it returns `-ENOMEM`.\n7. If the allocation is successful, it sets the `mob_loc` field of the `reloc` structure to point to the `id` and the `vbo` field to point to the `vmw_bo`.\n8. It assigns the `vmw_bo` to the pointer provided by the caller (`vmw_bo_p`) and adds the `reloc` structure to the list of BO relocations in the `sw_context`.\n9. Finally, it returns 0 indicating success.\n\nVulnerability assessment:\nThe code does not appear to have obvious vulnerabilities such as buffer overflows, use-after-free, or improper input validation based on the provided snippet. However, without more context about the surrounding code and the definitions of the functions and structures used, a thorough security analysis cannot be performed. Nonetheless, the code does release the BO reference with `ttm_bo_put` before adding it to the relocation list, which could potentially lead to issues if the BO is accessed after its reference count reaches zero. But given the current context, there is no clear indication of a vulnerability.",
            "final_result": 0
        },
        {
            "id": 4273,
            "cve_id": "CVE-2023-35823",
            "code_snippet": "int saa7134_vbi_fini(struct saa7134_dev *dev)\n{\n\t/* nothing */\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4407,
            "cve_id": "CVE-2023-4732",
            "code_snippet": "void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)\n{\n\tstruct vm_area_struct *vma = pvmw->vma;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long address = pvmw->address;\n\tunsigned long mmun_start = address & HPAGE_PMD_MASK;\n\tpmd_t pmde;\n\tswp_entry_t entry;\n\n\tif (!(pvmw->pmd && !pvmw->pte))\n\t\treturn;\n\n\tentry = pmd_to_swp_entry(*pvmw->pmd);\n\tget_page(new);\n\tpmde = pmd_mkold(mk_huge_pmd(new, vma->vm_page_prot));\n\tif (pmd_swp_soft_dirty(*pvmw->pmd))\n\t\tpmde = pmd_mksoft_dirty(pmde);\n\tif (is_write_migration_entry(entry))\n\t\tpmde = maybe_pmd_mkwrite(pmde, vma);\n\n\tflush_cache_range(vma, mmun_start, mmun_start + HPAGE_PMD_SIZE);\n\tif (PageAnon(new))\n\t\tpage_add_anon_rmap(new, vma, mmun_start, true);\n\telse\n\t\tpage_add_file_rmap(new, true);\n\tset_pmd_at(mm, mmun_start, pvmw->pmd, pmde);\n\tif ((vma->vm_flags & VM_LOCKED) && !PageDoubleMap(new))\n\t\tmlock_vma_page(new);\n\tupdate_mmu_cache_pmd(vma, address, pvmw->pmd);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4408,
            "cve_id": "CVE-2023-4732",
            "code_snippet": "static inline int\ncopy_present_pte(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,\n\t\t pte_t *dst_pte, pte_t *src_pte, unsigned long addr, int *rss,\n\t\t struct page **prealloc)\n{\n\tstruct mm_struct *src_mm = src_vma->vm_mm;\n\tunsigned long vm_flags = src_vma->vm_flags;\n\tpte_t pte = *src_pte;\n\tstruct page *page;\n\n\tpage = vm_normal_page(src_vma, addr, pte);\n\tif (page) {\n\t\tint retval;\n\n\t\tretval = copy_present_page(dst_vma, src_vma, dst_pte, src_pte,\n\t\t\t\t\t   addr, rss, prealloc, pte, page);\n\t\tif (retval <= 0)\n\t\t\treturn retval;\n\n\t\tget_page(page);\n\t\tpage_dup_rmap(page, false);\n\t\trss[mm_counter(page)]++;\n\t}\n\n\t/*\n\t * If it's a COW mapping, write protect it both\n\t * in the parent and the child\n\t */\n\tif (is_cow_mapping(vm_flags) && pte_write(pte)) {\n\t\tptep_set_wrprotect(src_mm, addr, src_pte);\n\t\tpte = pte_wrprotect(pte);\n\t}\n\n\t/*\n\t * If it's a shared mapping, mark it clean in\n\t * the child\n\t */\n\tif (vm_flags & VM_SHARED)\n\t\tpte = pte_mkclean(pte);\n\tpte = pte_mkold(pte);\n\n\t/*\n\t * Make sure the _PAGE_UFFD_WP bit is cleared if the new VMA\n\t * does not have the VM_UFFD_WP, which means that the uffd\n\t * fork event is not enabled.\n\t */\n\tif (!(vm_flags & VM_UFFD_WP))\n\t\tpte = pte_clear_uffd_wp(pte);\n\n\tset_pte_at(dst_vma->vm_mm, addr, dst_pte, pte);\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        }
    ],
    "non_vul_data": [
        {
            "id": 65,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "int dccp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tconst struct sockaddr_in *usin = (struct sockaddr_in *)uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\t__be16 orig_sport, orig_dport;\n\t__be32 daddr, nexthop;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\tint err;\n\tstruct ip_options_rcu *inet_opt;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < sizeof(struct sockaddr_in))\n\t\treturn -EINVAL;\n\n\tif (usin->sin_family != AF_INET)\n\t\treturn -EAFNOSUPPORT;\n\n\tnexthop = daddr = usin->sin_addr.s_addr;\n\n\tinet_opt = rcu_dereference_protected(inet->inet_opt,\n\t\t\t\t\t     sock_owned_by_user(sk));\n\tif (inet_opt != NULL && inet_opt->opt.srr) {\n\t\tif (daddr == 0)\n\t\t\treturn -EINVAL;\n\t\tnexthop = inet_opt->opt.faddr;\n\t}\n\n\torig_sport = inet->inet_sport;\n\torig_dport = usin->sin_port;\n\trt = ip_route_connect(&fl4, nexthop, inet->inet_saddr,\n\t\t\t      RT_CONN_FLAGS(sk), sk->sk_bound_dev_if,\n\t\t\t      IPPROTO_DCCP,\n\t\t\t      orig_sport, orig_dport, sk, true);\n\tif (IS_ERR(rt))\n\t\treturn PTR_ERR(rt);\n\n\tif (rt->rt_flags & (RTCF_MULTICAST | RTCF_BROADCAST)) {\n\t\tip_rt_put(rt);\n\t\treturn -ENETUNREACH;\n\t}\n\n\tif (inet_opt == NULL || !inet_opt->opt.srr)\n\t\tdaddr = rt->rt_dst;\n\n\tif (inet->inet_saddr == 0)\n\t\tinet->inet_saddr = rt->rt_src;\n\tinet->inet_rcv_saddr = inet->inet_saddr;\n\n\tinet->inet_dport = usin->sin_port;\n\tinet->inet_daddr = daddr;\n\n\tinet_csk(sk)->icsk_ext_hdr_len = 0;\n\tif (inet_opt)\n\t\tinet_csk(sk)->icsk_ext_hdr_len = inet_opt->opt.optlen;\n\t/*\n\t * Socket identity is still unknown (sport may be zero).\n\t * However we set state to DCCP_REQUESTING and not releasing socket\n\t * lock select source port, enter ourselves into the hash tables and\n\t * complete initialization after this.\n\t */\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet_hash_connect(&dccp_death_row, sk);\n\tif (err != 0)\n\t\tgoto failure;\n\n\trt = ip_route_newports(&fl4, rt, orig_sport, orig_dport,\n\t\t\t       inet->inet_sport, inet->inet_dport, sk);\n\tif (IS_ERR(rt)) {\n\t\trt = NULL;\n\t\tgoto failure;\n\t}\n\t/* OK, now commit destination to socket.  */\n\tsk_setup_caps(sk, &rt->dst);\n\n\tdp->dccps_iss = secure_dccp_sequence_number(inet->inet_saddr,\n\t\t\t\t\t\t    inet->inet_daddr,\n\t\t\t\t\t\t    inet->inet_sport,\n\t\t\t\t\t\t    inet->inet_dport);\n\tinet->inet_id = dp->dccps_iss ^ jiffies;\n\n\terr = dccp_connect(sk);\n\trt = NULL;\n\tif (err != 0)\n\t\tgoto failure;\nout:\n\treturn err;\nfailure:\n\t/*\n\t * This unhashes the socket and releases the local port, if necessary.\n\t */\n\tdccp_set_state(sk, DCCP_CLOSED);\n\tip_rt_put(rt);\n\tsk->sk_route_caps = 0;\n\tinet->inet_dport = 0;\n\tgoto out;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 66,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "static struct sock *dccp_v6_request_recv_sock(struct sock *sk,\n\t\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t\t      struct request_sock *req,\n\t\t\t\t\t      struct dst_entry *dst)\n{\n\tstruct inet6_request_sock *ireq6 = inet6_rsk(req);\n\tstruct ipv6_pinfo *newnp, *np = inet6_sk(sk);\n\tstruct inet_sock *newinet;\n\tstruct dccp6_sock *newdp6;\n\tstruct sock *newsk;\n\tstruct ipv6_txoptions *opt;\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t/*\n\t\t *\tv6 mapped\n\t\t */\n\t\tnewsk = dccp_v4_request_recv_sock(sk, skb, req, dst);\n\t\tif (newsk == NULL)\n\t\t\treturn NULL;\n\n\t\tnewdp6 = (struct dccp6_sock *)newsk;\n\t\tnewinet = inet_sk(newsk);\n\t\tnewinet->pinet6 = &newdp6->inet6;\n\t\tnewnp = inet6_sk(newsk);\n\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\t\tipv6_addr_set_v4mapped(newinet->inet_daddr, &newnp->daddr);\n\n\t\tipv6_addr_set_v4mapped(newinet->inet_saddr, &newnp->saddr);\n\n\t\tipv6_addr_copy(&newnp->rcv_saddr, &newnp->saddr);\n\n\t\tinet_csk(newsk)->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tnewsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\t\tnewnp->pktoptions  = NULL;\n\t\tnewnp->opt\t   = NULL;\n\t\tnewnp->mcast_oif   = inet6_iif(skb);\n\t\tnewnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;\n\n\t\t/*\n\t\t * No need to charge this sock to the relevant IPv6 refcnt debug socks count\n\t\t * here, dccp_create_openreq_child now does this for us, see the comment in\n\t\t * that function for the gory details. -acme\n\t\t */\n\n\t\t/* It is tricky place. Until this moment IPv4 tcp\n\t\t   worked with IPv6 icsk.icsk_af_ops.\n\t\t   Sync it now.\n\t\t */\n\t\tdccp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);\n\n\t\treturn newsk;\n\t}\n\n\topt = np->opt;\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto out_overflow;\n\n\tif (dst == NULL) {\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = IPPROTO_DCCP;\n\t\tipv6_addr_copy(&fl6.daddr, &ireq6->rmt_addr);\n\t\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\t\tipv6_addr_copy(&fl6.saddr, &ireq6->loc_addr);\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.fl6_dport = inet_rsk(req)->rmt_port;\n\t\tfl6.fl6_sport = inet_rsk(req)->loc_port;\n\t\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p, false);\n\t\tif (IS_ERR(dst))\n\t\t\tgoto out;\n\t}\n\n\tnewsk = dccp_create_openreq_child(sk, req, skb);\n\tif (newsk == NULL)\n\t\tgoto out_nonewsk;\n\n\t/*\n\t * No need to charge this sock to the relevant IPv6 refcnt debug socks\n\t * count here, dccp_create_openreq_child now does this for us, see the\n\t * comment in that function for the gory details. -acme\n\t */\n\n\t__ip6_dst_store(newsk, dst, NULL, NULL);\n\tnewsk->sk_route_caps = dst->dev->features & ~(NETIF_F_IP_CSUM |\n\t\t\t\t\t\t      NETIF_F_TSO);\n\tnewdp6 = (struct dccp6_sock *)newsk;\n\tnewinet = inet_sk(newsk);\n\tnewinet->pinet6 = &newdp6->inet6;\n\tnewnp = inet6_sk(newsk);\n\n\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\tipv6_addr_copy(&newnp->daddr, &ireq6->rmt_addr);\n\tipv6_addr_copy(&newnp->saddr, &ireq6->loc_addr);\n\tipv6_addr_copy(&newnp->rcv_saddr, &ireq6->loc_addr);\n\tnewsk->sk_bound_dev_if = ireq6->iif;\n\n\t/* Now IPv6 options...\n\n\t   First: no IPv4 options.\n\t */\n\tnewinet->inet_opt = NULL;\n\n\t/* Clone RX bits */\n\tnewnp->rxopt.all = np->rxopt.all;\n\n\t/* Clone pktoptions received with SYN */\n\tnewnp->pktoptions = NULL;\n\tif (ireq6->pktopts != NULL) {\n\t\tnewnp->pktoptions = skb_clone(ireq6->pktopts, GFP_ATOMIC);\n\t\tkfree_skb(ireq6->pktopts);\n\t\tireq6->pktopts = NULL;\n\t\tif (newnp->pktoptions)\n\t\t\tskb_set_owner_r(newnp->pktoptions, newsk);\n\t}\n\tnewnp->opt\t  = NULL;\n\tnewnp->mcast_oif  = inet6_iif(skb);\n\tnewnp->mcast_hops = ipv6_hdr(skb)->hop_limit;\n\n\t/*\n\t * Clone native IPv6 options from listening socket (if any)\n\t *\n\t * Yes, keeping reference count would be much more clever, but we make\n\t * one more one thing there: reattach optmem to newsk.\n\t */\n\tif (opt != NULL) {\n\t\tnewnp->opt = ipv6_dup_options(newsk, opt);\n\t\tif (opt != np->opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t}\n\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (newnp->opt != NULL)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +\n\t\t\t\t\t\t     newnp->opt->opt_flen);\n\n\tdccp_sync_mss(newsk, dst_mtu(dst));\n\n\tnewinet->inet_daddr = newinet->inet_saddr = LOOPBACK4_IPV6;\n\tnewinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tsock_put(newsk);\n\t\tgoto out;\n\t}\n\t__inet6_hash(newsk, NULL);\n\n\treturn newsk;\n\nout_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nout_nonewsk:\n\tdst_release(dst);\nout:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\tif (opt != NULL && opt != np->opt)\n\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\treturn NULL;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 67,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "int inet_sk_rebuild_header(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct rtable *rt = (struct rtable *)__sk_dst_check(sk, 0);\n\t__be32 daddr;\n\tstruct ip_options_rcu *inet_opt;\n\tint err;\n\n\t/* Route is OK, nothing to do. */\n\tif (rt)\n\t\treturn 0;\n\n\t/* Reroute. */\n\trcu_read_lock();\n\tinet_opt = rcu_dereference(inet->inet_opt);\n\tdaddr = inet->inet_daddr;\n\tif (inet_opt && inet_opt->opt.srr)\n\t\tdaddr = inet_opt->opt.faddr;\n\trcu_read_unlock();\n\trt = ip_route_output_ports(sock_net(sk), sk, daddr, inet->inet_saddr,\n\t\t\t\t   inet->inet_dport, inet->inet_sport,\n\t\t\t\t   sk->sk_protocol, RT_CONN_FLAGS(sk),\n\t\t\t\t   sk->sk_bound_dev_if);\n\tif (!IS_ERR(rt)) {\n\t\terr = 0;\n\t\tsk_setup_caps(sk, &rt->dst);\n\t} else {\n\t\terr = PTR_ERR(rt);\n\n\t\t/* Routing failed... */\n\t\tsk->sk_route_caps = 0;\n\t\t/*\n\t\t * Other protocols have to map its equivalent state to TCP_SYN_SENT.\n\t\t * DCCP maps its DCCP_REQUESTING state to TCP_SYN_SENT. -acme\n\t\t */\n\t\tif (!sysctl_ip_dynaddr ||\n\t\t    sk->sk_state != TCP_SYN_SENT ||\n\t\t    (sk->sk_userlocks & SOCK_BINDADDR_LOCK) ||\n\t\t    (err = inet_sk_reselect_saddr(sk)) != 0)\n\t\t\tsk->sk_err_soft = -err;\n\t}\n\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 68,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "void inet_sock_destruct(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\t__skb_queue_purge(&sk->sk_receive_queue);\n\t__skb_queue_purge(&sk->sk_error_queue);\n\n\tsk_mem_reclaim(sk);\n\n\tif (sk->sk_type == SOCK_STREAM && sk->sk_state != TCP_CLOSE) {\n\t\tpr_err(\"Attempt to release TCP socket in state %d %p\\n\",\n\t\t       sk->sk_state, sk);\n\t\treturn;\n\t}\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tpr_err(\"Attempt to release alive inet socket %p\\n\", sk);\n\t\treturn;\n\t}\n\n\tWARN_ON(atomic_read(&sk->sk_rmem_alloc));\n\tWARN_ON(atomic_read(&sk->sk_wmem_alloc));\n\tWARN_ON(sk->sk_wmem_queued);\n\tWARN_ON(sk->sk_forward_alloc);\n\n\tkfree(rcu_dereference_protected(inet->inet_opt, 1));\n\tdst_release(rcu_dereference_check(sk->sk_dst_cache, 1));\n\tsk_refcnt_debug_dec(sk);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 69,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "static int inet_sk_reselect_saddr(struct sock *sk)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\t__be32 old_saddr = inet->inet_saddr;\n\t__be32 daddr = inet->inet_daddr;\n\tstruct flowi4 fl4;\n\tstruct rtable *rt;\n\t__be32 new_saddr;\n\tstruct ip_options_rcu *inet_opt;\n\n\tinet_opt = rcu_dereference_protected(inet->inet_opt,\n\t\t\t\t\t     sock_owned_by_user(sk));\n\tif (inet_opt && inet_opt->opt.srr)\n\t\tdaddr = inet_opt->opt.faddr;\n\n\t/* Query new route. */\n\trt = ip_route_connect(&fl4, daddr, 0, RT_CONN_FLAGS(sk),\n\t\t\t      sk->sk_bound_dev_if, sk->sk_protocol,\n\t\t\t      inet->inet_sport, inet->inet_dport, sk, false);\n\tif (IS_ERR(rt))\n\t\treturn PTR_ERR(rt);\n\n\tsk_setup_caps(sk, &rt->dst);\n\n\tnew_saddr = rt->rt_src;\n\n\tif (new_saddr == old_saddr)\n\t\treturn 0;\n\n\tif (sysctl_ip_dynaddr > 1) {\n\t\tprintk(KERN_INFO \"%s(): shifting inet->saddr from %pI4 to %pI4\\n\",\n\t\t       __func__, &old_saddr, &new_saddr);\n\t}\n\n\tinet->inet_saddr = inet->inet_rcv_saddr = new_saddr;\n\n\t/*\n\t * XXX The only one ugly spot where we need to\n\t * XXX really change the sockets identity after\n\t * XXX it has entered the hashes. -DaveM\n\t *\n\t * Besides that, it does not check for connection\n\t * uniqueness. Wait for troubles.\n\t */\n\t__sk_prot_rehash(sk);\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 70,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "void cipso_v4_req_delattr(struct request_sock *req)\n{\n\tstruct ip_options_rcu *opt;\n\tstruct inet_request_sock *req_inet;\n\n\treq_inet = inet_rsk(req);\n\topt = req_inet->opt;\n\tif (opt == NULL || opt->opt.cipso == 0)\n\t\treturn;\n\n\tcipso_v4_delopt(&req_inet->opt);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 71,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "void cipso_v4_sock_delattr(struct sock *sk)\n{\n\tint hdr_delta;\n\tstruct ip_options_rcu *opt;\n\tstruct inet_sock *sk_inet;\n\n\tsk_inet = inet_sk(sk);\n\topt = rcu_dereference_protected(sk_inet->inet_opt, 1);\n\tif (opt == NULL || opt->opt.cipso == 0)\n\t\treturn;\n\n\thdr_delta = cipso_v4_delopt(&sk_inet->inet_opt);\n\tif (sk_inet->is_icsk && hdr_delta > 0) {\n\t\tstruct inet_connection_sock *sk_conn = inet_csk(sk);\n\t\tsk_conn->icsk_ext_hdr_len -= hdr_delta;\n\t\tsk_conn->icsk_sync_mss(sk, sk_conn->icsk_pmtu_cookie);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 72,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "int cipso_v4_sock_getattr(struct sock *sk, struct netlbl_lsm_secattr *secattr)\n{\n\tstruct ip_options_rcu *opt;\n\tint res = -ENOMSG;\n\n\trcu_read_lock();\n\topt = rcu_dereference(inet_sk(sk)->inet_opt);\n\tif (opt && opt->opt.cipso)\n\t\tres = cipso_v4_getattr(opt->opt.__data +\n\t\t\t\t\t\topt->opt.cipso -\n\t\t\t\t\t\tsizeof(struct iphdr),\n\t\t\t\t       secattr);\n\trcu_read_unlock();\n\treturn res;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 73,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "int cipso_v4_sock_setattr(struct sock *sk,\n\t\t\t  const struct cipso_v4_doi *doi_def,\n\t\t\t  const struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val = -EPERM;\n\tunsigned char *buf = NULL;\n\tu32 buf_len;\n\tu32 opt_len;\n\tstruct ip_options_rcu *old, *opt = NULL;\n\tstruct inet_sock *sk_inet;\n\tstruct inet_connection_sock *sk_conn;\n\n\t/* In the case of sock_create_lite(), the sock->sk field is not\n\t * defined yet but it is not a problem as the only users of these\n\t * \"lite\" PF_INET sockets are functions which do an accept() call\n\t * afterwards so we will label the socket as part of the accept(). */\n\tif (sk == NULL)\n\t\treturn 0;\n\n\t/* We allocate the maximum CIPSO option size here so we are probably\n\t * being a little wasteful, but it makes our life _much_ easier later\n\t * on and after all we are only talking about 40 bytes. */\n\tbuf_len = CIPSO_V4_OPT_LEN_MAX;\n\tbuf = kmalloc(buf_len, GFP_ATOMIC);\n\tif (buf == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto socket_setattr_failure;\n\t}\n\n\tret_val = cipso_v4_genopt(buf, buf_len, doi_def, secattr);\n\tif (ret_val < 0)\n\t\tgoto socket_setattr_failure;\n\tbuf_len = ret_val;\n\n\t/* We can't use ip_options_get() directly because it makes a call to\n\t * ip_options_get_alloc() which allocates memory with GFP_KERNEL and\n\t * we won't always have CAP_NET_RAW even though we _always_ want to\n\t * set the IPOPT_CIPSO option. */\n\topt_len = (buf_len + 3) & ~3;\n\topt = kzalloc(sizeof(*opt) + opt_len, GFP_ATOMIC);\n\tif (opt == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto socket_setattr_failure;\n\t}\n\tmemcpy(opt->opt.__data, buf, buf_len);\n\topt->opt.optlen = opt_len;\n\topt->opt.cipso = sizeof(struct iphdr);\n\tkfree(buf);\n\tbuf = NULL;\n\n\tsk_inet = inet_sk(sk);\n\n\told = rcu_dereference_protected(sk_inet->inet_opt, sock_owned_by_user(sk));\n\tif (sk_inet->is_icsk) {\n\t\tsk_conn = inet_csk(sk);\n\t\tif (old)\n\t\t\tsk_conn->icsk_ext_hdr_len -= old->opt.optlen;\n\t\tsk_conn->icsk_ext_hdr_len += opt->opt.optlen;\n\t\tsk_conn->icsk_sync_mss(sk, sk_conn->icsk_pmtu_cookie);\n\t}\n\trcu_assign_pointer(sk_inet->inet_opt, opt);\n\tif (old)\n\t\tcall_rcu(&old->rcu, opt_kfree_rcu);\n\n\treturn 0;\n\nsocket_setattr_failure:\n\tkfree(buf);\n\tkfree(opt);\n\treturn ret_val;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 74,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "int cipso_v4_req_setattr(struct request_sock *req,\n\t\t\t const struct cipso_v4_doi *doi_def,\n\t\t\t const struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val = -EPERM;\n\tunsigned char *buf = NULL;\n\tu32 buf_len;\n\tu32 opt_len;\n\tstruct ip_options_rcu *opt = NULL;\n\tstruct inet_request_sock *req_inet;\n\n\t/* We allocate the maximum CIPSO option size here so we are probably\n\t * being a little wasteful, but it makes our life _much_ easier later\n\t * on and after all we are only talking about 40 bytes. */\n\tbuf_len = CIPSO_V4_OPT_LEN_MAX;\n\tbuf = kmalloc(buf_len, GFP_ATOMIC);\n\tif (buf == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto req_setattr_failure;\n\t}\n\n\tret_val = cipso_v4_genopt(buf, buf_len, doi_def, secattr);\n\tif (ret_val < 0)\n\t\tgoto req_setattr_failure;\n\tbuf_len = ret_val;\n\n\t/* We can't use ip_options_get() directly because it makes a call to\n\t * ip_options_get_alloc() which allocates memory with GFP_KERNEL and\n\t * we won't always have CAP_NET_RAW even though we _always_ want to\n\t * set the IPOPT_CIPSO option. */\n\topt_len = (buf_len + 3) & ~3;\n\topt = kzalloc(sizeof(*opt) + opt_len, GFP_ATOMIC);\n\tif (opt == NULL) {\n\t\tret_val = -ENOMEM;\n\t\tgoto req_setattr_failure;\n\t}\n\tmemcpy(opt->opt.__data, buf, buf_len);\n\topt->opt.optlen = opt_len;\n\topt->opt.cipso = sizeof(struct iphdr);\n\tkfree(buf);\n\tbuf = NULL;\n\n\treq_inet = inet_rsk(req);\n\topt = xchg(&req_inet->opt, opt);\n\tif (opt)\n\t\tcall_rcu(&opt->rcu, opt_kfree_rcu);\n\n\treturn 0;\n\nreq_setattr_failure:\n\tkfree(buf);\n\tkfree(opt);\n\treturn ret_val;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 75,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "static void icmp_reply(struct icmp_bxm *icmp_param, struct sk_buff *skb)\n{\n\tstruct ipcm_cookie ipc;\n\tstruct rtable *rt = skb_rtable(skb);\n\tstruct net *net = dev_net(rt->dst.dev);\n\tstruct sock *sk;\n\tstruct inet_sock *inet;\n\t__be32 daddr;\n\n\tif (ip_options_echo(&icmp_param->replyopts.opt.opt, skb))\n\t\treturn;\n\n\tsk = icmp_xmit_lock(net);\n\tif (sk == NULL)\n\t\treturn;\n\tinet = inet_sk(sk);\n\n\ticmp_param->data.icmph.checksum = 0;\n\n\tinet->tos = ip_hdr(skb)->tos;\n\tdaddr = ipc.addr = rt->rt_src;\n\tipc.opt = NULL;\n\tipc.tx_flags = 0;\n\tif (icmp_param->replyopts.opt.opt.optlen) {\n\t\tipc.opt = &icmp_param->replyopts.opt;\n\t\tif (ipc.opt->opt.srr)\n\t\t\tdaddr = icmp_param->replyopts.opt.opt.faddr;\n\t}\n\t{\n\t\tstruct flowi4 fl4 = {\n\t\t\t.daddr = daddr,\n\t\t\t.saddr = rt->rt_spec_dst,\n\t\t\t.flowi4_tos = RT_TOS(ip_hdr(skb)->tos),\n\t\t\t.flowi4_proto = IPPROTO_ICMP,\n\t\t};\n\t\tsecurity_skb_classify_flow(skb, flowi4_to_flowi(&fl4));\n\t\trt = ip_route_output_key(net, &fl4);\n\t\tif (IS_ERR(rt))\n\t\t\tgoto out_unlock;\n\t}\n\tif (icmpv4_xrlim_allow(net, rt, icmp_param->data.icmph.type,\n\t\t\t       icmp_param->data.icmph.code))\n\t\ticmp_push_reply(icmp_param, &ipc, &rt);\n\tip_rt_put(rt);\nout_unlock:\n\ticmp_xmit_unlock(sk);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 76,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "void icmp_send(struct sk_buff *skb_in, int type, int code, __be32 info)\n{\n\tstruct iphdr *iph;\n\tint room;\n\tstruct icmp_bxm icmp_param;\n\tstruct rtable *rt = skb_rtable(skb_in);\n\tstruct ipcm_cookie ipc;\n\t__be32 saddr;\n\tu8  tos;\n\tstruct net *net;\n\tstruct sock *sk;\n\n\tif (!rt)\n\t\tgoto out;\n\tnet = dev_net(rt->dst.dev);\n\n\t/*\n\t *\tFind the original header. It is expected to be valid, of course.\n\t *\tCheck this, icmp_send is called from the most obscure devices\n\t *\tsometimes.\n\t */\n\tiph = ip_hdr(skb_in);\n\n\tif ((u8 *)iph < skb_in->head ||\n\t    (skb_in->network_header + sizeof(*iph)) > skb_in->tail)\n\t\tgoto out;\n\n\t/*\n\t *\tNo replies to physical multicast/broadcast\n\t */\n\tif (skb_in->pkt_type != PACKET_HOST)\n\t\tgoto out;\n\n\t/*\n\t *\tNow check at the protocol level\n\t */\n\tif (rt->rt_flags & (RTCF_BROADCAST | RTCF_MULTICAST))\n\t\tgoto out;\n\n\t/*\n\t *\tOnly reply to fragment 0. We byte re-order the constant\n\t *\tmask for efficiency.\n\t */\n\tif (iph->frag_off & htons(IP_OFFSET))\n\t\tgoto out;\n\n\t/*\n\t *\tIf we send an ICMP error to an ICMP error a mess would result..\n\t */\n\tif (icmp_pointers[type].error) {\n\t\t/*\n\t\t *\tWe are an error, check if we are replying to an\n\t\t *\tICMP error\n\t\t */\n\t\tif (iph->protocol == IPPROTO_ICMP) {\n\t\t\tu8 _inner_type, *itp;\n\n\t\t\titp = skb_header_pointer(skb_in,\n\t\t\t\t\t\t skb_network_header(skb_in) +\n\t\t\t\t\t\t (iph->ihl << 2) +\n\t\t\t\t\t\t offsetof(struct icmphdr,\n\t\t\t\t\t\t\t  type) -\n\t\t\t\t\t\t skb_in->data,\n\t\t\t\t\t\t sizeof(_inner_type),\n\t\t\t\t\t\t &_inner_type);\n\t\t\tif (itp == NULL)\n\t\t\t\tgoto out;\n\n\t\t\t/*\n\t\t\t *\tAssume any unknown ICMP type is an error. This\n\t\t\t *\tisn't specified by the RFC, but think about it..\n\t\t\t */\n\t\t\tif (*itp > NR_ICMP_TYPES ||\n\t\t\t    icmp_pointers[*itp].error)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk = icmp_xmit_lock(net);\n\tif (sk == NULL)\n\t\treturn;\n\n\t/*\n\t *\tConstruct source address and options.\n\t */\n\n\tsaddr = iph->daddr;\n\tif (!(rt->rt_flags & RTCF_LOCAL)) {\n\t\tstruct net_device *dev = NULL;\n\n\t\trcu_read_lock();\n\t\tif (rt_is_input_route(rt) &&\n\t\t    net->ipv4.sysctl_icmp_errors_use_inbound_ifaddr)\n\t\t\tdev = dev_get_by_index_rcu(net, rt->rt_iif);\n\n\t\tif (dev)\n\t\t\tsaddr = inet_select_addr(dev, 0, RT_SCOPE_LINK);\n\t\telse\n\t\t\tsaddr = 0;\n\t\trcu_read_unlock();\n\t}\n\n\ttos = icmp_pointers[type].error ? ((iph->tos & IPTOS_TOS_MASK) |\n\t\t\t\t\t   IPTOS_PREC_INTERNETCONTROL) :\n\t\t\t\t\t  iph->tos;\n\n\tif (ip_options_echo(&icmp_param.replyopts.opt.opt, skb_in))\n\t\tgoto out_unlock;\n\n\n\t/*\n\t *\tPrepare data for ICMP header.\n\t */\n\n\ticmp_param.data.icmph.type\t = type;\n\ticmp_param.data.icmph.code\t = code;\n\ticmp_param.data.icmph.un.gateway = info;\n\ticmp_param.data.icmph.checksum\t = 0;\n\ticmp_param.skb\t  = skb_in;\n\ticmp_param.offset = skb_network_offset(skb_in);\n\tinet_sk(sk)->tos = tos;\n\tipc.addr = iph->saddr;\n\tipc.opt = &icmp_param.replyopts.opt;\n\tipc.tx_flags = 0;\n\n\trt = icmp_route_lookup(net, skb_in, iph, saddr, tos,\n\t\t\t       type, code, &icmp_param);\n\tif (IS_ERR(rt))\n\t\tgoto out_unlock;\n\n\tif (!icmpv4_xrlim_allow(net, rt, type, code))\n\t\tgoto ende;\n\n\t/* RFC says return as much as we can without exceeding 576 bytes. */\n\n\troom = dst_mtu(&rt->dst);\n\tif (room > 576)\n\t\troom = 576;\n\troom -= sizeof(struct iphdr) + icmp_param.replyopts.opt.opt.optlen;\n\troom -= sizeof(struct icmphdr);\n\n\ticmp_param.data_len = skb_in->len - icmp_param.offset;\n\tif (icmp_param.data_len > room)\n\t\ticmp_param.data_len = room;\n\ticmp_param.head_len = sizeof(struct icmphdr);\n\n\ticmp_push_reply(&icmp_param, &ipc, &rt);\nende:\n\tip_rt_put(rt);\nout_unlock:\n\ticmp_xmit_unlock(sk);\nout:;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 77,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "static struct rtable *icmp_route_lookup(struct net *net, struct sk_buff *skb_in,\n\t\t\t\t\tconst struct iphdr *iph,\n\t\t\t\t\t__be32 saddr, u8 tos,\n\t\t\t\t\tint type, int code,\n\t\t\t\t\tstruct icmp_bxm *param)\n{\n\tstruct flowi4 fl4 = {\n\t\t.daddr = (param->replyopts.opt.opt.srr ?\n\t\t\t  param->replyopts.opt.opt.faddr : iph->saddr),\n\t\t.saddr = saddr,\n\t\t.flowi4_tos = RT_TOS(tos),\n\t\t.flowi4_proto = IPPROTO_ICMP,\n\t\t.fl4_icmp_type = type,\n\t\t.fl4_icmp_code = code,\n\t};\n\tstruct rtable *rt, *rt2;\n\tint err;\n\n\tsecurity_skb_classify_flow(skb_in, flowi4_to_flowi(&fl4));\n\trt = __ip_route_output_key(net, &fl4);\n\tif (IS_ERR(rt))\n\t\treturn rt;\n\n\t/* No need to clone since we're just using its address. */\n\trt2 = rt;\n\n\tif (!fl4.saddr)\n\t\tfl4.saddr = rt->rt_src;\n\n\trt = (struct rtable *) xfrm_lookup(net, &rt->dst,\n\t\t\t\t\t   flowi4_to_flowi(&fl4), NULL, 0);\n\tif (!IS_ERR(rt)) {\n\t\tif (rt != rt2)\n\t\t\treturn rt;\n\t} else if (PTR_ERR(rt) == -EPERM) {\n\t\trt = NULL;\n\t} else\n\t\treturn rt;\n\n\terr = xfrm_decode_session_reverse(skb_in, flowi4_to_flowi(&fl4), AF_INET);\n\tif (err)\n\t\tgoto relookup_failed;\n\n\tif (inet_addr_type(net, fl4.saddr) == RTN_LOCAL) {\n\t\trt2 = __ip_route_output_key(net, &fl4);\n\t\tif (IS_ERR(rt2))\n\t\t\terr = PTR_ERR(rt2);\n\t} else {\n\t\tstruct flowi4 fl4_2 = {};\n\t\tunsigned long orefdst;\n\n\t\tfl4_2.daddr = fl4.saddr;\n\t\trt2 = ip_route_output_key(net, &fl4_2);\n\t\tif (IS_ERR(rt2)) {\n\t\t\terr = PTR_ERR(rt2);\n\t\t\tgoto relookup_failed;\n\t\t}\n\t\t/* Ugh! */\n\t\torefdst = skb_in->_skb_refdst; /* save old refdst */\n\t\terr = ip_route_input(skb_in, fl4.daddr, fl4.saddr,\n\t\t\t\t     RT_TOS(tos), rt2->dst.dev);\n\n\t\tdst_release(&rt2->dst);\n\t\trt2 = skb_rtable(skb_in);\n\t\tskb_in->_skb_refdst = orefdst; /* restore old refdst */\n\t}\n\n\tif (err)\n\t\tgoto relookup_failed;\n\n\trt2 = (struct rtable *) xfrm_lookup(net, &rt2->dst,\n\t\t\t\t\t    flowi4_to_flowi(&fl4), NULL,\n\t\t\t\t\t    XFRM_LOOKUP_ICMP);\n\tif (!IS_ERR(rt2)) {\n\t\tdst_release(&rt->dst);\n\t\trt = rt2;\n\t} else if (PTR_ERR(rt2) == -EPERM) {\n\t\tif (rt)\n\t\t\tdst_release(&rt->dst);\n\t\treturn rt2;\n\t} else {\n\t\terr = PTR_ERR(rt2);\n\t\tgoto relookup_failed;\n\t}\n\treturn rt;\n\nrelookup_failed:\n\tif (rt)\n\t\treturn rt;\n\treturn ERR_PTR(err);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 78,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "struct dst_entry *inet_csk_route_req(struct sock *sk,\n\t\t\t\t     const struct request_sock *req)\n{\n\tstruct rtable *rt;\n\tconst struct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ip_options_rcu *opt = inet_rsk(req)->opt;\n\tstruct net *net = sock_net(sk);\n\tstruct flowi4 fl4;\n\n\tflowi4_init_output(&fl4, sk->sk_bound_dev_if, sk->sk_mark,\n\t\t\t   RT_CONN_FLAGS(sk), RT_SCOPE_UNIVERSE,\n\t\t\t   sk->sk_protocol, inet_sk_flowi_flags(sk),\n\t\t\t   (opt && opt->opt.srr) ? opt->opt.faddr : ireq->rmt_addr,\n\t\t\t   ireq->loc_addr, ireq->rmt_port, inet_sk(sk)->inet_sport);\n\tsecurity_req_classify_flow(req, flowi4_to_flowi(&fl4));\n\trt = ip_route_output_flow(net, &fl4, sk);\n\tif (IS_ERR(rt))\n\t\tgoto no_route;\n\tif (opt && opt->opt.is_strictroute && rt->rt_dst != rt->rt_gateway)\n\t\tgoto route_err;\n\treturn &rt->dst;\n\nroute_err:\n\tip_rt_put(rt);\nno_route:\n\tIP_INC_STATS_BH(net, IPSTATS_MIB_OUTNOROUTES);\n\treturn NULL;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 79,
            "cve_id": "CVE-2012-3552",
            "code_snippet": "void ip_options_build(struct sk_buff *skb, struct ip_options *opt,\n\t\t\t    __be32 daddr, struct rtable *rt, int is_frag)\n{\n\tunsigned char *iph = skb_network_header(skb);\n\n\tmemcpy(&(IPCB(skb)->opt), opt, sizeof(struct ip_options));\n\tmemcpy(iph+sizeof(struct iphdr), opt->__data, opt->optlen);\n\topt = &(IPCB(skb)->opt);\n\n\tif (opt->srr)\n\t\tmemcpy(iph+opt->srr+iph[opt->srr+1]-4, &daddr, 4);\n\n\tif (!is_frag) {\n\t\tif (opt->rr_needaddr)\n\t\t\tip_rt_get_source(iph+opt->rr+iph[opt->rr+2]-5, rt);\n\t\tif (opt->ts_needaddr)\n\t\t\tip_rt_get_source(iph+opt->ts+iph[opt->ts+2]-9, rt);\n\t\tif (opt->ts_needtime) {\n\t\t\tstruct timespec tv;\n\t\t\t__be32 midtime;\n\t\t\tgetnstimeofday(&tv);\n\t\t\tmidtime = htonl((tv.tv_sec % 86400) * MSEC_PER_SEC + tv.tv_nsec / NSEC_PER_MSEC);\n\t\t\tmemcpy(iph+opt->ts+iph[opt->ts+2]-5, &midtime, 4);\n\t\t}\n\t\treturn;\n\t}\n\tif (opt->rr) {\n\t\tmemset(iph+opt->rr, IPOPT_NOP, iph[opt->rr+1]);\n\t\topt->rr = 0;\n\t\topt->rr_needaddr = 0;\n\t}\n\tif (opt->ts) {\n\t\tmemset(iph+opt->ts, IPOPT_NOP, iph[opt->ts+1]);\n\t\topt->ts = 0;\n\t\topt->ts_needaddr = opt->ts_needtime = 0;\n\t}\n}",
            "detect_result": "YES",
            "final_result": 1
        },
        {
            "id": 107,
            "cve_id": "CVE-2012-4508",
            "code_snippet": "static int ext4_split_extent_at(handle_t *handle,\n\t\t\t     struct inode *inode,\n\t\t\t     struct ext4_ext_path *path,\n\t\t\t     ext4_lblk_t split,\n\t\t\t     int split_flag,\n\t\t\t     int flags)\n{\n\text4_fsblk_t newblock;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex, newex, orig_ex;\n\tstruct ext4_extent *ex2 = NULL;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\n\tBUG_ON((split_flag & (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2)) ==\n\t       (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2));\n\n\text_debug(\"ext4_split_extents_at: inode %lu, logical\"\n\t\t\"block %llu\\n\", inode->i_ino, (unsigned long long)split);\n\n\text4_ext_show_leaf(inode, path);\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tnewblock = split - ee_block + ext4_ext_pblock(ex);\n\n\tBUG_ON(split < ee_block || split >= (ee_block + ee_len));\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\n\tif (split == ee_block) {\n\t\t/*\n\t\t * case b: block @split is the block that the extent begins with\n\t\t * then we just change the state of the extent, and splitting\n\t\t * is not needed.\n\t\t */\n\t\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\t\text4_ext_mark_uninitialized(ex);\n\t\telse\n\t\t\text4_ext_mark_initialized(ex);\n\n\t\tif (!(flags & EXT4_GET_BLOCKS_PRE_IO))\n\t\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tgoto out;\n\t}\n\n\t/* case a */\n\tmemcpy(&orig_ex, ex, sizeof(orig_ex));\n\tex->ee_len = cpu_to_le16(split - ee_block);\n\tif (split_flag & EXT4_EXT_MARK_UNINIT1)\n\t\text4_ext_mark_uninitialized(ex);\n\n\t/*\n\t * path may lead to new leaf, not to original leaf any more\n\t * after ext4_ext_insert_extent() returns,\n\t */\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\tif (err)\n\t\tgoto fix_extent_len;\n\n\tex2 = &newex;\n\tex2->ee_block = cpu_to_le32(split);\n\tex2->ee_len   = cpu_to_le16(ee_len - (split - ee_block));\n\text4_ext_store_pblock(ex2, newblock);\n\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\text4_ext_mark_uninitialized(ex2);\n\n\terr = ext4_ext_insert_extent(handle, inode, path, &newex, flags);\n\tif (err == -ENOSPC && (EXT4_EXT_MAY_ZEROOUT & split_flag)) {\n\t\tif (split_flag & (EXT4_EXT_DATA_VALID1|EXT4_EXT_DATA_VALID2)) {\n\t\t\tif (split_flag & EXT4_EXT_DATA_VALID1)\n\t\t\t\terr = ext4_ext_zeroout(inode, ex2);\n\t\t\telse\n\t\t\t\terr = ext4_ext_zeroout(inode, ex);\n\t\t} else\n\t\t\terr = ext4_ext_zeroout(inode, &orig_ex);\n\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\t\t/* update the extent length and mark as initialized */\n\t\tex->ee_len = cpu_to_le16(ee_len);\n\t\text4_ext_try_to_merge(handle, inode, path, ex);\n\t\terr = ext4_ext_dirty(handle, inode, path + path->p_depth);\n\t\tgoto out;\n\t} else if (err)\n\t\tgoto fix_extent_len;\n\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n\nfix_extent_len:\n\tex->ee_len = orig_ex.ee_len;\n\text4_ext_dirty(handle, inode, path + depth);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 167,
            "cve_id": "CVE-2013-0871",
            "code_snippet": "void __ptrace_unlink(struct task_struct *child)\n{\n\tBUG_ON(!child->ptrace);\n\n\tchild->ptrace = 0;\n\tchild->parent = child->real_parent;\n\tlist_del_init(&child->ptrace_entry);\n\n\tspin_lock(&child->sighand->siglock);\n\n\t/*\n\t * Clear all pending traps and TRAPPING.  TRAPPING should be\n\t * cleared regardless of JOBCTL_STOP_PENDING.  Do it explicitly.\n\t */\n\ttask_clear_jobctl_pending(child, JOBCTL_TRAP_MASK);\n\ttask_clear_jobctl_trapping(child);\n\n\t/*\n\t * Reinstate JOBCTL_STOP_PENDING if group stop is in effect and\n\t * @child isn't dead.\n\t */\n\tif (!(child->flags & PF_EXITING) &&\n\t    (child->signal->flags & SIGNAL_STOP_STOPPED ||\n\t     child->signal->group_stop_count)) {\n\t\tchild->jobctl |= JOBCTL_STOP_PENDING;\n\n\t\t/*\n\t\t * This is only possible if this thread was cloned by the\n\t\t * traced task running in the stopped group, set the signal\n\t\t * for the future reports.\n\t\t * FIXME: we should change ptrace_init_task() to handle this\n\t\t * case.\n\t\t */\n\t\tif (!(child->jobctl & JOBCTL_STOP_SIGMASK))\n\t\t\tchild->jobctl |= SIGSTOP;\n\t}\n\n\t/*\n\t * If transition to TASK_STOPPED is pending or in TASK_TRACED, kick\n\t * @child in the butt.  Note that @resume should be used iff @child\n\t * is in TASK_TRACED; otherwise, we might unduly disrupt\n\t * TASK_KILLABLE sleeps.\n\t */\n\tif (child->jobctl & JOBCTL_STOP_PENDING || task_is_traced(child))\n\t\tptrace_signal_wake_up(child, true);\n\n\tspin_unlock(&child->sighand->siglock);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 288,
            "cve_id": "CVE-2013-3302",
            "code_snippet": "static int\nsmb_send_kvec(struct TCP_Server_Info *server, struct kvec *iov, size_t n_vec,\n\t\tsize_t *sent)\n{\n\tint rc = 0;\n\tint i = 0;\n\tstruct msghdr smb_msg;\n\tunsigned int remaining;\n\tsize_t first_vec = 0;\n\tstruct socket *ssocket = server->ssocket;\n\n\t*sent = 0;\n\n\tsmb_msg.msg_name = (struct sockaddr *) &server->dstaddr;\n\tsmb_msg.msg_namelen = sizeof(struct sockaddr);\n\tsmb_msg.msg_control = NULL;\n\tsmb_msg.msg_controllen = 0;\n\tif (server->noblocksnd)\n\t\tsmb_msg.msg_flags = MSG_DONTWAIT + MSG_NOSIGNAL;\n\telse\n\t\tsmb_msg.msg_flags = MSG_NOSIGNAL;\n\n\tremaining = 0;\n\tfor (i = 0; i < n_vec; i++)\n\t\tremaining += iov[i].iov_len;\n\n\ti = 0;\n\twhile (remaining) {\n\t\t/*\n\t\t * If blocking send, we try 3 times, since each can block\n\t\t * for 5 seconds. For nonblocking  we have to try more\n\t\t * but wait increasing amounts of time allowing time for\n\t\t * socket to clear.  The overall time we wait in either\n\t\t * case to send on the socket is about 15 seconds.\n\t\t * Similarly we wait for 15 seconds for a response from\n\t\t * the server in SendReceive[2] for the server to send\n\t\t * a response back for most types of requests (except\n\t\t * SMB Write past end of file which can be slow, and\n\t\t * blocking lock operations). NFS waits slightly longer\n\t\t * than CIFS, but this can make it take longer for\n\t\t * nonresponsive servers to be detected and 15 seconds\n\t\t * is more than enough time for modern networks to\n\t\t * send a packet.  In most cases if we fail to send\n\t\t * after the retries we will kill the socket and\n\t\t * reconnect which may clear the network problem.\n\t\t */\n\t\trc = kernel_sendmsg(ssocket, &smb_msg, &iov[first_vec],\n\t\t\t\t    n_vec - first_vec, remaining);\n\t\tif (rc == -ENOSPC || rc == -EAGAIN) {\n\t\t\t/*\n\t\t\t * Catch if a low level driver returns -ENOSPC. This\n\t\t\t * WARN_ON will be removed by 3.10 if no one reports\n\t\t\t * seeing this.\n\t\t\t */\n\t\t\tWARN_ON_ONCE(rc == -ENOSPC);\n\t\t\ti++;\n\t\t\tif (i >= 14 || (!server->noblocksnd && (i > 2))) {\n\t\t\t\tcERROR(1, \"sends on sock %p stuck for 15 \"\n\t\t\t\t\t  \"seconds\", ssocket);\n\t\t\t\trc = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmsleep(1 << i);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (rc < 0)\n\t\t\tbreak;\n\n\t\t/* send was at least partially successful */\n\t\t*sent += rc;\n\n\t\tif (rc == remaining) {\n\t\t\tremaining = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (rc > remaining) {\n\t\t\tcERROR(1, \"sent %d requested %d\", rc, remaining);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (rc == 0) {\n\t\t\t/* should never happen, letting socket clear before\n\t\t\t   retrying is our only obvious option here */\n\t\t\tcERROR(1, \"tcp sent no data\");\n\t\t\tmsleep(500);\n\t\t\tcontinue;\n\t\t}\n\n\t\tremaining -= rc;\n\n\t\t/* the line below resets i */\n\t\tfor (i = first_vec; i < n_vec; i++) {\n\t\t\tif (iov[i].iov_len) {\n\t\t\t\tif (rc > iov[i].iov_len) {\n\t\t\t\t\trc -= iov[i].iov_len;\n\t\t\t\t\tiov[i].iov_len = 0;\n\t\t\t\t} else {\n\t\t\t\t\tiov[i].iov_base += rc;\n\t\t\t\t\tiov[i].iov_len -= rc;\n\t\t\t\t\tfirst_vec = i;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\ti = 0; /* in case we get ENOSPC on the next send */\n\t\trc = 0;\n\t}\n\treturn rc;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 366,
            "cve_id": "CVE-2013-7026",
            "code_snippet": "\nSYSCALL_DEFINE3(shmctl, int, shmid, int, cmd, struct shmid_ds __user *, buf)\n{\n\tstruct shmid_kernel *shp;\n\tint err, version;\n\tstruct ipc_namespace *ns;\n\n\tif (cmd < 0 || shmid < 0)\n\t\treturn -EINVAL;\n\n\tversion = ipc_parse_version(&cmd);\n\tns = current->nsproxy->ipc_ns;\n\n\tswitch (cmd) {\n\tcase IPC_INFO:\n\tcase SHM_INFO:\n\tcase SHM_STAT:\n\tcase IPC_STAT:\n\t\treturn shmctl_nolock(ns, shmid, cmd, version, buf);\n\tcase IPC_RMID:\n\tcase IPC_SET:\n\t\treturn shmctl_down(ns, shmid, cmd, buf, version);\n\tcase SHM_LOCK:\n\tcase SHM_UNLOCK:\n\t{\n\t\tstruct file *shm_file;\n\n\t\trcu_read_lock();\n\t\tshp = shm_obtain_object_check(ns, shmid);\n\t\tif (IS_ERR(shp)) {\n\t\t\terr = PTR_ERR(shp);\n\t\t\tgoto out_unlock1;\n\t\t}\n\n\t\taudit_ipc_obj(&(shp->shm_perm));\n\t\terr = security_shm_shmctl(shp, cmd);\n\t\tif (err)\n\t\t\tgoto out_unlock1;\n\n\t\tipc_lock_object(&shp->shm_perm);\n\t\tif (!ns_capable(ns->user_ns, CAP_IPC_LOCK)) {\n\t\t\tkuid_t euid = current_euid();\n\t\t\terr = -EPERM;\n\t\t\tif (!uid_eq(euid, shp->shm_perm.uid) &&\n\t\t\t    !uid_eq(euid, shp->shm_perm.cuid))\n\t\t\t\tgoto out_unlock0;\n\t\t\tif (cmd == SHM_LOCK && !rlimit(RLIMIT_MEMLOCK))\n\t\t\t\tgoto out_unlock0;\n\t\t}\n\n\t\tshm_file = shp->shm_file;\n\n\t\t/* check if shm_destroy() is tearing down shp */\n\t\tif (shm_file == NULL) {\n\t\t\terr = -EIDRM;\n\t\t\tgoto out_unlock0;\n\t\t}\n\n\t\tif (is_file_hugepages(shm_file))\n\t\t\tgoto out_unlock0;\n\n\t\tif (cmd == SHM_LOCK) {\n\t\t\tstruct user_struct *user = current_user();\n\t\t\terr = shmem_lock(shm_file, 1, user);\n\t\t\tif (!err && !(shp->shm_perm.mode & SHM_LOCKED)) {\n\t\t\t\tshp->shm_perm.mode |= SHM_LOCKED;\n\t\t\t\tshp->mlock_user = user;\n\t\t\t}\n\t\t\tgoto out_unlock0;\n\t\t}\n\n\t\t/* SHM_UNLOCK */\n\t\tif (!(shp->shm_perm.mode & SHM_LOCKED))\n\t\t\tgoto out_unlock0;\n\t\tshmem_lock(shm_file, 0, shp->mlock_user);\n\t\tshp->shm_perm.mode &= ~SHM_LOCKED;\n\t\tshp->mlock_user = NULL;\n\t\tget_file(shm_file);\n\t\tipc_unlock_object(&shp->shm_perm);\n\t\trcu_read_unlock();\n\t\tshmem_unlock_mapping(shm_file->f_mapping);\n\n\t\tfput(shm_file);\n\t\treturn err;\n\t}\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\nout_unlock0:\n\tipc_unlock_object(&shp->shm_perm);\nout_unlock1:\n\trcu_read_unlock();\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 560,
            "cve_id": "CVE-2014-2706",
            "code_snippet": "void ieee80211_sta_ps_deliver_wakeup(struct sta_info *sta)\n{\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct sk_buff_head pending;\n\tint filtered = 0, buffered = 0, ac;\n\tunsigned long flags;\n\n\tclear_sta_flag(sta, WLAN_STA_SP);\n\n\tBUILD_BUG_ON(BITS_TO_LONGS(IEEE80211_NUM_TIDS) > 1);\n\tsta->driver_buffered_tids = 0;\n\n\tif (!(local->hw.flags & IEEE80211_HW_AP_LINK_PS))\n\t\tdrv_sta_notify(local, sdata, STA_NOTIFY_AWAKE, &sta->sta);\n\n\tskb_queue_head_init(&pending);\n\n\t/* sync with ieee80211_tx_h_unicast_ps_buf */\n\tspin_lock(&sta->ps_lock);\n\t/* Send all buffered frames to the station */\n\tfor (ac = 0; ac < IEEE80211_NUM_ACS; ac++) {\n\t\tint count = skb_queue_len(&pending), tmp;\n\n\t\tspin_lock_irqsave(&sta->tx_filtered[ac].lock, flags);\n\t\tskb_queue_splice_tail_init(&sta->tx_filtered[ac], &pending);\n\t\tspin_unlock_irqrestore(&sta->tx_filtered[ac].lock, flags);\n\t\ttmp = skb_queue_len(&pending);\n\t\tfiltered += tmp - count;\n\t\tcount = tmp;\n\n\t\tspin_lock_irqsave(&sta->ps_tx_buf[ac].lock, flags);\n\t\tskb_queue_splice_tail_init(&sta->ps_tx_buf[ac], &pending);\n\t\tspin_unlock_irqrestore(&sta->ps_tx_buf[ac].lock, flags);\n\t\ttmp = skb_queue_len(&pending);\n\t\tbuffered += tmp - count;\n\t}\n\n\tieee80211_add_pending_skbs_fn(local, &pending, clear_sta_ps_flags, sta);\n\tspin_unlock(&sta->ps_lock);\n\n\t/* This station just woke up and isn't aware of our SMPS state */\n\tif (!ieee80211_smps_is_restrictive(sta->known_smps_mode,\n\t\t\t\t\t   sdata->smps_mode) &&\n\t    sta->known_smps_mode != sdata->bss->req_smps &&\n\t    sta_info_tx_streams(sta) != 1) {\n\t\tht_dbg(sdata,\n\t\t       \"%pM just woke up and MIMO capable - update SMPS\\n\",\n\t\t       sta->sta.addr);\n\t\tieee80211_send_smps_action(sdata, sdata->bss->req_smps,\n\t\t\t\t\t   sta->sta.addr,\n\t\t\t\t\t   sdata->vif.bss_conf.bssid);\n\t}\n\n\tlocal->total_ps_buffered -= buffered;\n\n\tsta_info_recalc_tim(sta);\n\n\tps_dbg(sdata,\n\t       \"STA %pM aid %d sending %d filtered/%d PS frames since STA not sleeping anymore\\n\",\n\t       sta->sta.addr, sta->sta.aid, filtered, buffered);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 629,
            "cve_id": "CVE-2014-4652",
            "code_snippet": "static int snd_ctl_elem_user_put(struct snd_kcontrol *kcontrol,\n\t\t\t\t struct snd_ctl_elem_value *ucontrol)\n{\n\tint change;\n\tstruct user_element *ue = kcontrol->private_data;\n\n\tmutex_lock(&ue->card->user_ctl_lock);\n\tchange = memcmp(&ucontrol->value, ue->elem_data, ue->elem_data_size) != 0;\n\tif (change)\n\t\tmemcpy(ue->elem_data, &ucontrol->value, ue->elem_data_size);\n\tmutex_unlock(&ue->card->user_ctl_lock);\n\treturn change;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 756,
            "cve_id": "CVE-2014-9710",
            "code_snippet": "struct btrfs_dir_item *btrfs_match_dir_item_name(struct btrfs_root *root,\n\t\t\t\t\t\t struct btrfs_path *path,\n\t\t\t\t\t\t const char *name, int name_len)\n{\n\tstruct btrfs_dir_item *dir_item;\n\tunsigned long name_ptr;\n\tu32 total_len;\n\tu32 cur = 0;\n\tu32 this_len;\n\tstruct extent_buffer *leaf;\n\n\tleaf = path->nodes[0];\n\tdir_item = btrfs_item_ptr(leaf, path->slots[0], struct btrfs_dir_item);\n\tif (verify_dir_item(root, leaf, dir_item))\n\t\treturn NULL;\n\n\ttotal_len = btrfs_item_size_nr(leaf, path->slots[0]);\n\twhile (cur < total_len) {\n\t\tthis_len = sizeof(*dir_item) +\n\t\t\tbtrfs_dir_name_len(leaf, dir_item) +\n\t\t\tbtrfs_dir_data_len(leaf, dir_item);\n\t\tname_ptr = (unsigned long)(dir_item + 1);\n\n\t\tif (btrfs_dir_name_len(leaf, dir_item) == name_len &&\n\t\t    memcmp_extent_buffer(leaf, name, name_ptr, name_len) == 0)\n\t\t\treturn dir_item;\n\n\t\tcur += this_len;\n\t\tdir_item = (struct btrfs_dir_item *)((char *)dir_item +\n\t\t\t\t\t\t     this_len);\n\t}\n\treturn NULL;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 817,
            "cve_id": "CVE-2015-3212",
            "code_snippet": "static int sctp_init_sock(struct sock *sk)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct sctp_sock *sp;\n\n\tpr_debug(\"%s: sk:%p\\n\", __func__, sk);\n\n\tsp = sctp_sk(sk);\n\n\t/* Initialize the SCTP per socket area.  */\n\tswitch (sk->sk_type) {\n\tcase SOCK_SEQPACKET:\n\t\tsp->type = SCTP_SOCKET_UDP;\n\t\tbreak;\n\tcase SOCK_STREAM:\n\t\tsp->type = SCTP_SOCKET_TCP;\n\t\tbreak;\n\tdefault:\n\t\treturn -ESOCKTNOSUPPORT;\n\t}\n\n\t/* Initialize default send parameters. These parameters can be\n\t * modified with the SCTP_DEFAULT_SEND_PARAM socket option.\n\t */\n\tsp->default_stream = 0;\n\tsp->default_ppid = 0;\n\tsp->default_flags = 0;\n\tsp->default_context = 0;\n\tsp->default_timetolive = 0;\n\n\tsp->default_rcv_context = 0;\n\tsp->max_burst = net->sctp.max_burst;\n\n\tsp->sctp_hmac_alg = net->sctp.sctp_hmac_alg;\n\n\t/* Initialize default setup parameters. These parameters\n\t * can be modified with the SCTP_INITMSG socket option or\n\t * overridden by the SCTP_INIT CMSG.\n\t */\n\tsp->initmsg.sinit_num_ostreams   = sctp_max_outstreams;\n\tsp->initmsg.sinit_max_instreams  = sctp_max_instreams;\n\tsp->initmsg.sinit_max_attempts   = net->sctp.max_retrans_init;\n\tsp->initmsg.sinit_max_init_timeo = net->sctp.rto_max;\n\n\t/* Initialize default RTO related parameters.  These parameters can\n\t * be modified for with the SCTP_RTOINFO socket option.\n\t */\n\tsp->rtoinfo.srto_initial = net->sctp.rto_initial;\n\tsp->rtoinfo.srto_max     = net->sctp.rto_max;\n\tsp->rtoinfo.srto_min     = net->sctp.rto_min;\n\n\t/* Initialize default association related parameters. These parameters\n\t * can be modified with the SCTP_ASSOCINFO socket option.\n\t */\n\tsp->assocparams.sasoc_asocmaxrxt = net->sctp.max_retrans_association;\n\tsp->assocparams.sasoc_number_peer_destinations = 0;\n\tsp->assocparams.sasoc_peer_rwnd = 0;\n\tsp->assocparams.sasoc_local_rwnd = 0;\n\tsp->assocparams.sasoc_cookie_life = net->sctp.valid_cookie_life;\n\n\t/* Initialize default event subscriptions. By default, all the\n\t * options are off.\n\t */\n\tmemset(&sp->subscribe, 0, sizeof(struct sctp_event_subscribe));\n\n\t/* Default Peer Address Parameters.  These defaults can\n\t * be modified via SCTP_PEER_ADDR_PARAMS\n\t */\n\tsp->hbinterval  = net->sctp.hb_interval;\n\tsp->pathmaxrxt  = net->sctp.max_retrans_path;\n\tsp->pathmtu     = 0; /* allow default discovery */\n\tsp->sackdelay   = net->sctp.sack_timeout;\n\tsp->sackfreq\t= 2;\n\tsp->param_flags = SPP_HB_ENABLE |\n\t\t\t  SPP_PMTUD_ENABLE |\n\t\t\t  SPP_SACKDELAY_ENABLE;\n\n\t/* If enabled no SCTP message fragmentation will be performed.\n\t * Configure through SCTP_DISABLE_FRAGMENTS socket option.\n\t */\n\tsp->disable_fragments = 0;\n\n\t/* Enable Nagle algorithm by default.  */\n\tsp->nodelay           = 0;\n\n\tsp->recvrcvinfo = 0;\n\tsp->recvnxtinfo = 0;\n\n\t/* Enable by default. */\n\tsp->v4mapped          = 1;\n\n\t/* Auto-close idle associations after the configured\n\t * number of seconds.  A value of 0 disables this\n\t * feature.  Configure through the SCTP_AUTOCLOSE socket option,\n\t * for UDP-style sockets only.\n\t */\n\tsp->autoclose         = 0;\n\n\t/* User specified fragmentation limit. */\n\tsp->user_frag         = 0;\n\n\tsp->adaptation_ind = 0;\n\n\tsp->pf = sctp_get_pf_specific(sk->sk_family);\n\n\t/* Control variables for partial data delivery. */\n\tatomic_set(&sp->pd_mode, 0);\n\tskb_queue_head_init(&sp->pd_lobby);\n\tsp->frag_interleave = 0;\n\n\t/* Create a per socket endpoint structure.  Even if we\n\t * change the data structure relationships, this may still\n\t * be useful for storing pre-connect address information.\n\t */\n\tsp->ep = sctp_endpoint_new(sk, GFP_KERNEL);\n\tif (!sp->ep)\n\t\treturn -ENOMEM;\n\n\tsp->hmac = NULL;\n\n\tsk->sk_destruct = sctp_destruct_sock;\n\n\tSCTP_DBG_OBJCNT_INC(sock);\n\n\tlocal_bh_disable();\n\tpercpu_counter_inc(&sctp_sockets_allocated);\n\tsock_prot_inuse_add(net, sk->sk_prot, 1);\n\n\t/* Nothing can fail after this block, otherwise\n\t * sctp_destroy_sock() will be called without addr_wq_lock held\n\t */\n\tif (net->sctp.default_auto_asconf) {\n\t\tspin_lock(&sock_net(sk)->sctp.addr_wq_lock);\n\t\tlist_add_tail(&sp->auto_asconf_list,\n\t\t    &net->sctp.auto_asconf_splist);\n\t\tsp->do_auto_asconf = 1;\n\t\tspin_unlock(&sock_net(sk)->sctp.addr_wq_lock);\n\t} else {\n\t\tsp->do_auto_asconf = 0;\n\t}\n\n\tlocal_bh_enable();\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 856,
            "cve_id": "CVE-2015-7613",
            "code_snippet": "static int newseg(struct ipc_namespace *ns, struct ipc_params *params)\n{\n\tkey_t key = params->key;\n\tint shmflg = params->flg;\n\tsize_t size = params->u.size;\n\tint error;\n\tstruct shmid_kernel *shp;\n\tsize_t numpages = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tstruct file *file;\n\tchar name[13];\n\tint id;\n\tvm_flags_t acctflag = 0;\n\n\tif (size < SHMMIN || size > ns->shm_ctlmax)\n\t\treturn -EINVAL;\n\n\tif (numpages << PAGE_SHIFT < size)\n\t\treturn -ENOSPC;\n\n\tif (ns->shm_tot + numpages < ns->shm_tot ||\n\t\t\tns->shm_tot + numpages > ns->shm_ctlall)\n\t\treturn -ENOSPC;\n\n\tshp = ipc_rcu_alloc(sizeof(*shp));\n\tif (!shp)\n\t\treturn -ENOMEM;\n\n\tshp->shm_perm.key = key;\n\tshp->shm_perm.mode = (shmflg & S_IRWXUGO);\n\tshp->mlock_user = NULL;\n\n\tshp->shm_perm.security = NULL;\n\terror = security_shm_alloc(shp);\n\tif (error) {\n\t\tipc_rcu_putref(shp, ipc_rcu_free);\n\t\treturn error;\n\t}\n\n\tsprintf(name, \"SYSV%08x\", key);\n\tif (shmflg & SHM_HUGETLB) {\n\t\tstruct hstate *hs;\n\t\tsize_t hugesize;\n\n\t\ths = hstate_sizelog((shmflg >> SHM_HUGE_SHIFT) & SHM_HUGE_MASK);\n\t\tif (!hs) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto no_file;\n\t\t}\n\t\thugesize = ALIGN(size, huge_page_size(hs));\n\n\t\t/* hugetlb_file_setup applies strict accounting */\n\t\tif (shmflg & SHM_NORESERVE)\n\t\t\tacctflag = VM_NORESERVE;\n\t\tfile = hugetlb_file_setup(name, hugesize, acctflag,\n\t\t\t\t  &shp->mlock_user, HUGETLB_SHMFS_INODE,\n\t\t\t\t(shmflg >> SHM_HUGE_SHIFT) & SHM_HUGE_MASK);\n\t} else {\n\t\t/*\n\t\t * Do not allow no accounting for OVERCOMMIT_NEVER, even\n\t\t * if it's asked for.\n\t\t */\n\t\tif  ((shmflg & SHM_NORESERVE) &&\n\t\t\t\tsysctl_overcommit_memory != OVERCOMMIT_NEVER)\n\t\t\tacctflag = VM_NORESERVE;\n\t\tfile = shmem_kernel_file_setup(name, size, acctflag);\n\t}\n\terror = PTR_ERR(file);\n\tif (IS_ERR(file))\n\t\tgoto no_file;\n\n\tshp->shm_cprid = task_tgid_vnr(current);\n\tshp->shm_lprid = 0;\n\tshp->shm_atim = shp->shm_dtim = 0;\n\tshp->shm_ctim = get_seconds();\n\tshp->shm_segsz = size;\n\tshp->shm_nattch = 0;\n\tshp->shm_file = file;\n\tshp->shm_creator = current;\n\n\tid = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);\n\tif (id < 0) {\n\t\terror = id;\n\t\tgoto no_id;\n\t}\n\n\tlist_add(&shp->shm_clist, &current->sysvshm.shm_clist);\n\n\t/*\n\t * shmid gets reported as \"inode#\" in /proc/pid/maps.\n\t * proc-ps tools use this. Changing this will break them.\n\t */\n\tfile_inode(file)->i_ino = shp->shm_perm.id;\n\n\tns->shm_tot += numpages;\n\terror = shp->shm_perm.id;\n\n\tipc_unlock_object(&shp->shm_perm);\n\trcu_read_unlock();\n\treturn error;\n\nno_id:\n\tif (is_file_hugepages(file) && shp->mlock_user)\n\t\tuser_shm_unlock(size, shp->mlock_user);\n\tfput(file);\nno_file:\n\tipc_rcu_putref(shp, shm_rcu_free);\n\treturn error;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 864,
            "cve_id": "CVE-2015-7990",
            "code_snippet": "int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct rds_sock *rs = rds_sk_to_rs(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);\n\t__be32 daddr;\n\t__be16 dport;\n\tstruct rds_message *rm = NULL;\n\tstruct rds_connection *conn;\n\tint ret = 0;\n\tint queued = 0, allocated_mr = 0;\n\tint nonblock = msg->msg_flags & MSG_DONTWAIT;\n\tlong timeo = sock_sndtimeo(sk, nonblock);\n\n\t/* Mirror Linux UDP mirror of BSD error message compatibility */\n\t/* XXX: Perhaps MSG_MORE someday */\n\tif (msg->msg_flags & ~(MSG_DONTWAIT | MSG_CMSG_COMPAT)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tif (msg->msg_namelen) {\n\t\t/* XXX fail non-unicast destination IPs? */\n\t\tif (msg->msg_namelen < sizeof(*usin) || usin->sin_family != AF_INET) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tdaddr = usin->sin_addr.s_addr;\n\t\tdport = usin->sin_port;\n\t} else {\n\t\t/* We only care about consistency with ->connect() */\n\t\tlock_sock(sk);\n\t\tdaddr = rs->rs_conn_addr;\n\t\tdport = rs->rs_conn_port;\n\t\trelease_sock(sk);\n\t}\n\n\tlock_sock(sk);\n\tif (daddr == 0 || rs->rs_bound_addr == 0) {\n\t\trelease_sock(sk);\n\t\tret = -ENOTCONN; /* XXX not a great errno */\n\t\tgoto out;\n\t}\n\trelease_sock(sk);\n\n\tif (payload_len > rds_sk_sndbuf(rs)) {\n\t\tret = -EMSGSIZE;\n\t\tgoto out;\n\t}\n\n\t/* size of rm including all sgs */\n\tret = rds_rm_size(msg, payload_len);\n\tif (ret < 0)\n\t\tgoto out;\n\n\trm = rds_message_alloc(ret, GFP_KERNEL);\n\tif (!rm) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t/* Attach data to the rm */\n\tif (payload_len) {\n\t\trm->data.op_sg = rds_message_alloc_sgs(rm, ceil(payload_len, PAGE_SIZE));\n\t\tif (!rm->data.op_sg) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tret = rds_message_copy_from_user(rm, &msg->msg_iter);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\trm->data.op_active = 1;\n\n\trm->m_daddr = daddr;\n\n\t/* rds_conn_create has a spinlock that runs with IRQ off.\n\t * Caching the conn in the socket helps a lot. */\n\tif (rs->rs_conn && rs->rs_conn->c_faddr == daddr)\n\t\tconn = rs->rs_conn;\n\telse {\n\t\tconn = rds_conn_create_outgoing(sock_net(sock->sk),\n\t\t\t\t\t\trs->rs_bound_addr, daddr,\n\t\t\t\t\trs->rs_transport,\n\t\t\t\t\tsock->sk->sk_allocation);\n\t\tif (IS_ERR(conn)) {\n\t\t\tret = PTR_ERR(conn);\n\t\t\tgoto out;\n\t\t}\n\t\trs->rs_conn = conn;\n\t}\n\n\t/* Parse any control messages the user may have included. */\n\tret = rds_cmsg_send(rs, rm, msg, &allocated_mr);\n\tif (ret)\n\t\tgoto out;\n\n\tif (rm->rdma.op_active && !conn->c_trans->xmit_rdma) {\n\t\tprintk_ratelimited(KERN_NOTICE \"rdma_op %p conn xmit_rdma %p\\n\",\n\t\t\t       &rm->rdma, conn->c_trans->xmit_rdma);\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tif (rm->atomic.op_active && !conn->c_trans->xmit_atomic) {\n\t\tprintk_ratelimited(KERN_NOTICE \"atomic_op %p conn xmit_atomic %p\\n\",\n\t\t\t       &rm->atomic, conn->c_trans->xmit_atomic);\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\trds_conn_connect_if_down(conn);\n\n\tret = rds_cong_wait(conn->c_fcong, dport, nonblock, rs);\n\tif (ret) {\n\t\trs->rs_seen_congestion = 1;\n\t\tgoto out;\n\t}\n\n\twhile (!rds_send_queue_rm(rs, conn, rm, rs->rs_bound_port,\n\t\t\t\t  dport, &queued)) {\n\t\trds_stats_inc(s_send_queue_full);\n\n\t\tif (nonblock) {\n\t\t\tret = -EAGAIN;\n\t\t\tgoto out;\n\t\t}\n\n\t\ttimeo = wait_event_interruptible_timeout(*sk_sleep(sk),\n\t\t\t\t\trds_send_queue_rm(rs, conn, rm,\n\t\t\t\t\t\t\t  rs->rs_bound_port,\n\t\t\t\t\t\t\t  dport,\n\t\t\t\t\t\t\t  &queued),\n\t\t\t\t\ttimeo);\n\t\trdsdebug(\"sendmsg woke queued %d timeo %ld\\n\", queued, timeo);\n\t\tif (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)\n\t\t\tcontinue;\n\n\t\tret = timeo;\n\t\tif (ret == 0)\n\t\t\tret = -ETIMEDOUT;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * By now we've committed to the send.  We reuse rds_send_worker()\n\t * to retry sends in the rds thread if the transport asks us to.\n\t */\n\trds_stats_inc(s_send_queued);\n\n\tret = rds_send_xmit(conn);\n\tif (ret == -ENOMEM || ret == -EAGAIN)\n\t\tqueue_delayed_work(rds_wq, &conn->c_send_w, 1);\n\n\trds_message_put(rm);\n\treturn payload_len;\n\nout:\n\t/* If the user included a RDMA_MAP cmsg, we allocated a MR on the fly.\n\t * If the sendmsg goes through, we keep the MR. If it fails with EAGAIN\n\t * or in any other way, we need to destroy the MR again */\n\tif (allocated_mr)\n\t\trds_rdma_unuse(rs, rds_rdma_cookie_key(rm->m_rdma_cookie), 1);\n\n\tif (rm)\n\t\trds_message_put(rm);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 908,
            "cve_id": "CVE-2015-8767",
            "code_snippet": "void sctp_generate_proto_unreach_event(unsigned long data)\n{\n\tstruct sctp_transport *transport = (struct sctp_transport *) data;\n\tstruct sctp_association *asoc = transport->asoc;\n\tstruct sock *sk = asoc->base.sk;\n\tstruct net *net = sock_net(sk);\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk)) {\n\t\tpr_debug(\"%s: sock is busy\\n\", __func__);\n\n\t\t/* Try again later.  */\n\t\tif (!mod_timer(&transport->proto_unreach_timer,\n\t\t\t\tjiffies + (HZ/20)))\n\t\t\tsctp_association_hold(asoc);\n\t\tgoto out_unlock;\n\t}\n\n\t/* Is this structure just waiting around for us to actually\n\t * get destroyed?\n\t */\n\tif (asoc->base.dead)\n\t\tgoto out_unlock;\n\n\tsctp_do_sm(net, SCTP_EVENT_T_OTHER,\n\t\t   SCTP_ST_OTHER(SCTP_EVENT_ICMP_PROTO_UNREACH),\n\t\t   asoc->state, asoc->ep, asoc, transport, GFP_ATOMIC);\n\nout_unlock:\n\tbh_unlock_sock(sk);\n\tsctp_association_put(asoc);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 918,
            "cve_id": "CVE-2015-8839",
            "code_snippet": "int ext4_collapse_range(struct inode *inode, loff_t offset, loff_t len)\n{\n\tstruct super_block *sb = inode->i_sb;\n\text4_lblk_t punch_start, punch_stop;\n\thandle_t *handle;\n\tunsigned int credits;\n\tloff_t new_size, ioffset;\n\tint ret;\n\n\t/*\n\t * We need to test this early because xfstests assumes that a\n\t * collapse range of (0, 1) will return EOPNOTSUPP if the file\n\t * system does not support collapse range.\n\t */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\treturn -EOPNOTSUPP;\n\n\t/* Collapse range works only on fs block size aligned offsets. */\n\tif (offset & (EXT4_CLUSTER_SIZE(sb) - 1) ||\n\t    len & (EXT4_CLUSTER_SIZE(sb) - 1))\n\t\treturn -EINVAL;\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EINVAL;\n\n\ttrace_ext4_collapse_range(inode, offset, len);\n\n\tpunch_start = offset >> EXT4_BLOCK_SIZE_BITS(sb);\n\tpunch_stop = (offset + len) >> EXT4_BLOCK_SIZE_BITS(sb);\n\n\t/* Call ext4_force_commit to flush all data in case of data=journal. */\n\tif (ext4_should_journal_data(inode)) {\n\t\tret = ext4_force_commit(inode->i_sb);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Need to round down offset to be aligned with page size boundary\n\t * for page size > block size.\n\t */\n\tioffset = round_down(offset, PAGE_SIZE);\n\n\t/* Write out all dirty pages */\n\tret = filemap_write_and_wait_range(inode->i_mapping, ioffset,\n\t\t\t\t\t   LLONG_MAX);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Take mutex lock */\n\tmutex_lock(&inode->i_mutex);\n\n\t/*\n\t * There is no need to overlap collapse range with EOF, in which case\n\t * it is effectively a truncate operation\n\t */\n\tif (offset + len >= i_size_read(inode)) {\n\t\tret = -EINVAL;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Currently just for extent based files */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Wait for existing dio to complete */\n\text4_inode_block_unlocked_dio(inode);\n\tinode_dio_wait(inode);\n\n\t/*\n\t * Prevent page faults from reinstantiating pages we have released from\n\t * page cache.\n\t */\n\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n\ttruncate_pagecache(inode, ioffset);\n\n\tcredits = ext4_writepage_trans_blocks(inode);\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\tgoto out_mmap;\n\t}\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_discard_preallocations(inode);\n\n\tret = ext4_es_remove_extent(inode, punch_start,\n\t\t\t\t    EXT_MAX_BLOCKS - punch_start);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\tret = ext4_ext_remove_space(inode, punch_start, punch_stop - 1);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\text4_discard_preallocations(inode);\n\n\tret = ext4_ext_shift_extents(inode, handle, punch_stop,\n\t\t\t\t     punch_stop - punch_start, SHIFT_LEFT);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\tnew_size = i_size_read(inode) - len;\n\ti_size_write(inode, new_size);\n\tEXT4_I(inode)->i_disksize = new_size;\n\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\nout_stop:\n\text4_journal_stop(handle);\nout_mmap:\n\tup_write(&EXT4_I(inode)->i_mmap_sem);\n\text4_inode_resume_unlocked_dio(inode);\nout_mutex:\n\tmutex_unlock(&inode->i_mutex);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 919,
            "cve_id": "CVE-2015-8839",
            "code_snippet": "int ext4_insert_range(struct inode *inode, loff_t offset, loff_t len)\n{\n\tstruct super_block *sb = inode->i_sb;\n\thandle_t *handle;\n\tstruct ext4_ext_path *path;\n\tstruct ext4_extent *extent;\n\text4_lblk_t offset_lblk, len_lblk, ee_start_lblk = 0;\n\tunsigned int credits, ee_len;\n\tint ret = 0, depth, split_flag = 0;\n\tloff_t ioffset;\n\n\t/*\n\t * We need to test this early because xfstests assumes that an\n\t * insert range of (0, 1) will return EOPNOTSUPP if the file\n\t * system does not support insert range.\n\t */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))\n\t\treturn -EOPNOTSUPP;\n\n\t/* Insert range works only on fs block size aligned offsets. */\n\tif (offset & (EXT4_CLUSTER_SIZE(sb) - 1) ||\n\t\t\tlen & (EXT4_CLUSTER_SIZE(sb) - 1))\n\t\treturn -EINVAL;\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EOPNOTSUPP;\n\n\ttrace_ext4_insert_range(inode, offset, len);\n\n\toffset_lblk = offset >> EXT4_BLOCK_SIZE_BITS(sb);\n\tlen_lblk = len >> EXT4_BLOCK_SIZE_BITS(sb);\n\n\t/* Call ext4_force_commit to flush all data in case of data=journal */\n\tif (ext4_should_journal_data(inode)) {\n\t\tret = ext4_force_commit(inode->i_sb);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * Need to round down to align start offset to page size boundary\n\t * for page size > block size.\n\t */\n\tioffset = round_down(offset, PAGE_SIZE);\n\n\t/* Write out all dirty pages */\n\tret = filemap_write_and_wait_range(inode->i_mapping, ioffset,\n\t\t\tLLONG_MAX);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Take mutex lock */\n\tmutex_lock(&inode->i_mutex);\n\n\t/* Currently just for extent based files */\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Check for wrap through zero */\n\tif (inode->i_size + len > inode->i_sb->s_maxbytes) {\n\t\tret = -EFBIG;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Offset should be less than i_size */\n\tif (offset >= i_size_read(inode)) {\n\t\tret = -EINVAL;\n\t\tgoto out_mutex;\n\t}\n\n\t/* Wait for existing dio to complete */\n\text4_inode_block_unlocked_dio(inode);\n\tinode_dio_wait(inode);\n\n\t/*\n\t * Prevent page faults from reinstantiating pages we have released from\n\t * page cache.\n\t */\n\tdown_write(&EXT4_I(inode)->i_mmap_sem);\n\ttruncate_pagecache(inode, ioffset);\n\n\tcredits = ext4_writepage_trans_blocks(inode);\n\thandle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);\n\tif (IS_ERR(handle)) {\n\t\tret = PTR_ERR(handle);\n\t\tgoto out_mmap;\n\t}\n\n\t/* Expand file to avoid data loss if there is error while shifting */\n\tinode->i_size += len;\n\tEXT4_I(inode)->i_disksize += len;\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\tret = ext4_mark_inode_dirty(handle, inode);\n\tif (ret)\n\t\tgoto out_stop;\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_discard_preallocations(inode);\n\n\tpath = ext4_find_extent(inode, offset_lblk, NULL, 0);\n\tif (IS_ERR(path)) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\tdepth = ext_depth(inode);\n\textent = path[depth].p_ext;\n\tif (extent) {\n\t\tee_start_lblk = le32_to_cpu(extent->ee_block);\n\t\tee_len = ext4_ext_get_actual_len(extent);\n\n\t\t/*\n\t\t * If offset_lblk is not the starting block of extent, split\n\t\t * the extent @offset_lblk\n\t\t */\n\t\tif ((offset_lblk > ee_start_lblk) &&\n\t\t\t\t(offset_lblk < (ee_start_lblk + ee_len))) {\n\t\t\tif (ext4_ext_is_unwritten(extent))\n\t\t\t\tsplit_flag = EXT4_EXT_MARK_UNWRIT1 |\n\t\t\t\t\tEXT4_EXT_MARK_UNWRIT2;\n\t\t\tret = ext4_split_extent_at(handle, inode, &path,\n\t\t\t\t\toffset_lblk, split_flag,\n\t\t\t\t\tEXT4_EX_NOCACHE |\n\t\t\t\t\tEXT4_GET_BLOCKS_PRE_IO |\n\t\t\t\t\tEXT4_GET_BLOCKS_METADATA_NOFAIL);\n\t\t}\n\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t\tif (ret < 0) {\n\t\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\t\tgoto out_stop;\n\t\t}\n\t}\n\n\tret = ext4_es_remove_extent(inode, offset_lblk,\n\t\t\tEXT_MAX_BLOCKS - offset_lblk);\n\tif (ret) {\n\t\tup_write(&EXT4_I(inode)->i_data_sem);\n\t\tgoto out_stop;\n\t}\n\n\t/*\n\t * if offset_lblk lies in a hole which is at start of file, use\n\t * ee_start_lblk to shift extents\n\t */\n\tret = ext4_ext_shift_extents(inode, handle,\n\t\tee_start_lblk > offset_lblk ? ee_start_lblk : offset_lblk,\n\t\tlen_lblk, SHIFT_RIGHT);\n\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\nout_stop:\n\text4_journal_stop(handle);\nout_mmap:\n\tup_write(&EXT4_I(inode)->i_mmap_sem);\n\text4_inode_resume_unlocked_dio(inode);\nout_mutex:\n\tmutex_unlock(&inode->i_mutex);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 920,
            "cve_id": "CVE-2015-8839",
            "code_snippet": "static int ext4_dax_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tint result;\n\thandle_t *handle = NULL;\n\tstruct inode *inode = file_inode(vma->vm_file);\n\tstruct super_block *sb = inode->i_sb;\n\tbool write = vmf->flags & FAULT_FLAG_WRITE;\n\n\tif (write) {\n\t\tsb_start_pagefault(sb);\n\t\tfile_update_time(vma->vm_file);\n\t\tdown_read(&EXT4_I(inode)->i_mmap_sem);\n\t\thandle = ext4_journal_start_sb(sb, EXT4_HT_WRITE_PAGE,\n\t\t\t\t\t\tEXT4_DATA_TRANS_BLOCKS(sb));\n\t} else\n\t\tdown_read(&EXT4_I(inode)->i_mmap_sem);\n\n\tif (IS_ERR(handle))\n\t\tresult = VM_FAULT_SIGBUS;\n\telse\n\t\tresult = __dax_fault(vma, vmf, ext4_get_block_dax,\n\t\t\t\t\t\text4_end_io_unwritten);\n\n\tif (write) {\n\t\tif (!IS_ERR(handle))\n\t\t\text4_journal_stop(handle);\n\t\tup_read(&EXT4_I(inode)->i_mmap_sem);\n\t\tsb_end_pagefault(sb);\n\t} else\n\t\tup_read(&EXT4_I(inode)->i_mmap_sem);\n\n\treturn result;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 921,
            "cve_id": "CVE-2015-8839",
            "code_snippet": "static int ext4_dax_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tint err;\n\tstruct inode *inode = file_inode(vma->vm_file);\n\n\tsb_start_pagefault(inode->i_sb);\n\tfile_update_time(vma->vm_file);\n\tdown_read(&EXT4_I(inode)->i_mmap_sem);\n\terr = __dax_mkwrite(vma, vmf, ext4_get_block_dax,\n\t\t\t    ext4_end_io_unwritten);\n\tup_read(&EXT4_I(inode)->i_mmap_sem);\n\tsb_end_pagefault(inode->i_sb);\n\n\treturn err;\n}",
            "detect_result": "NO\n\nExplanation of the code behavior:\n- The function `ext4_dax_mkwrite` is designed to handle write faults for Direct Access (DAX) mappings in the ext4 filesystem.\n- It starts by obtaining the `inode` associated with the file being accessed through the `vm_area_struct`.\n- It then calls `sb_start_pagefault` and `file_update_time` to prepare for handling the page fault and update the file's last access time, respectively.\n- The function acquires a read lock on the `i_mmap_sem` semaphore associated with the inode to ensure that the memory mapping is stable during the operation.\n- It proceeds to call `__dax_mkwrite`, which performs the actual write fault handling using the provided block mapping and end I/O functions (`ext4_get_block_dax` and `ext4_end_io_unwritten`).\n- After the write fault handling is complete, it releases the read lock on `i_mmap_sem`.\n- Finally, it calls `sb_end_pagefault` to conclude the page fault handling process and returns the result of the operation.\n\nVulnerability assessment:\n- The code appears to follow proper locking mechanisms (`down_read` and `up_read`) to protect shared resources during concurrent access.\n- There are no obvious buffer overflows, improper memory management, or other common vulnerabilities present in the snippet.\n- However, without a full context of how this function is used within the broader ext4 filesystem implementation, it is challenging to identify potential vulnerabilities definitively.\n- Given the provided code alone, there are no evident security issues, leading to the conclusion that the code is not vulnerable based on the information available.",
            "final_result": 0
        },
        {
            "id": 922,
            "cve_id": "CVE-2015-8839",
            "code_snippet": "static int ext4_dax_pmd_fault(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t\t\t\tpmd_t *pmd, unsigned int flags)\n{\n\tint result;\n\thandle_t *handle = NULL;\n\tstruct inode *inode = file_inode(vma->vm_file);\n\tstruct super_block *sb = inode->i_sb;\n\tbool write = flags & FAULT_FLAG_WRITE;\n\n\tif (write) {\n\t\tsb_start_pagefault(sb);\n\t\tfile_update_time(vma->vm_file);\n\t\tdown_read(&EXT4_I(inode)->i_mmap_sem);\n\t\thandle = ext4_journal_start_sb(sb, EXT4_HT_WRITE_PAGE,\n\t\t\t\text4_chunk_trans_blocks(inode,\n\t\t\t\t\t\t\tPMD_SIZE / PAGE_SIZE));\n\t} else\n\t\tdown_read(&EXT4_I(inode)->i_mmap_sem);\n\n\tif (IS_ERR(handle))\n\t\tresult = VM_FAULT_SIGBUS;\n\telse\n\t\tresult = __dax_pmd_fault(vma, addr, pmd, flags,\n\t\t\t\text4_get_block_dax, ext4_end_io_unwritten);\n\n\tif (write) {\n\t\tif (!IS_ERR(handle))\n\t\t\text4_journal_stop(handle);\n\t\tup_read(&EXT4_I(inode)->i_mmap_sem);\n\t\tsb_end_pagefault(sb);\n\t} else\n\t\tup_read(&EXT4_I(inode)->i_mmap_sem);\n\n\treturn result;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 939,
            "cve_id": "CVE-2015-8963",
            "code_snippet": "static void perf_event_init_cpu(int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\n\tmutex_lock(&swhash->hlist_mutex);\n\tif (swhash->hlist_refcount > 0) {\n\t\tstruct swevent_hlist *hlist;\n\n\t\thlist = kzalloc_node(sizeof(*hlist), GFP_KERNEL, cpu_to_node(cpu));\n\t\tWARN_ON(!hlist);\n\t\trcu_assign_pointer(swhash->swevent_hlist, hlist);\n\t}\n\tmutex_unlock(&swhash->hlist_mutex);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 949,
            "cve_id": "CVE-2015-9016",
            "code_snippet": "static bool blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq)\n{\n\tstruct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];\n\tstruct request *first_rq =\n\t\tlist_first_entry(pending, struct request, flush.list);\n\tstruct request *flush_rq = fq->flush_rq;\n\n\t/* C1 described at the top of this file */\n\tif (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))\n\t\treturn false;\n\n\t/* C2 and C3 */\n\tif (!list_empty(&fq->flush_data_in_flight) &&\n\t    time_before(jiffies,\n\t\t\tfq->flush_pending_since + FLUSH_PENDING_TIMEOUT))\n\t\treturn false;\n\n\t/*\n\t * Issue flush and toggle pending_idx.  This makes pending_idx\n\t * different from running_idx, which means flush is in flight.\n\t */\n\tfq->flush_pending_idx ^= 1;\n\n\tblk_rq_init(q, flush_rq);\n\n\t/*\n\t * Borrow tag from the first request since they can't\n\t * be in flight at the same time. And acquire the tag's\n\t * ownership for flush req.\n\t */\n\tif (q->mq_ops) {\n\t\tstruct blk_mq_hw_ctx *hctx;\n\n\t\tflush_rq->mq_ctx = first_rq->mq_ctx;\n\t\tflush_rq->tag = first_rq->tag;\n\t\tfq->orig_rq = first_rq;\n\n\t\thctx = q->mq_ops->map_queue(q, first_rq->mq_ctx->cpu);\n\t\tblk_mq_tag_set_rq(hctx, first_rq->tag, flush_rq);\n\t}\n\n\tflush_rq->cmd_type = REQ_TYPE_FS;\n\tflush_rq->cmd_flags = WRITE_FLUSH | REQ_FLUSH_SEQ;\n\tflush_rq->rq_disk = first_rq->rq_disk;\n\tflush_rq->end_io = flush_end_io;\n\n\treturn blk_flush_queue_rq(flush_rq, false);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 950,
            "cve_id": "CVE-2015-9016",
            "code_snippet": "static void bt_for_each(struct blk_mq_hw_ctx *hctx,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t\trq = hctx->tags->rqs[off + bit];\n\t\t\tif (rq->q == hctx->queue)\n\t\t\t\tfn(hctx, rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 966,
            "cve_id": "CVE-2016-10200",
            "code_snippet": "static int l2tp_ip6_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_l2tpip6 *addr = (struct sockaddr_l2tpip6 *) uaddr;\n\tstruct net *net = sock_net(sk);\n\t__be32 v4addr = 0;\n\tint addr_type;\n\tint err;\n\n\tif (addr->l2tp_family != AF_INET6)\n\t\treturn -EINVAL;\n\tif (addr_len < sizeof(*addr))\n\t\treturn -EINVAL;\n\n\taddr_type = ipv6_addr_type(&addr->l2tp_addr);\n\n\t/* l2tp_ip6 sockets are IPv6 only */\n\tif (addr_type == IPV6_ADDR_MAPPED)\n\t\treturn -EADDRNOTAVAIL;\n\n\t/* L2TP is point-point, not multicast */\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -EADDRNOTAVAIL;\n\n\terr = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip6_lock);\n\tif (__l2tp_ip6_bind_lookup(net, &addr->l2tp_addr,\n\t\t\t\t   sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\tread_unlock_bh(&l2tp_ip6_lock);\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\tgoto out_unlock;\n\n\tif (sk->sk_state != TCP_CLOSE)\n\t\tgoto out_unlock;\n\n\t/* Check if the address belongs to the host. */\n\trcu_read_lock();\n\tif (addr_type != IPV6_ADDR_ANY) {\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t\t    addr->l2tp_scope_id) {\n\t\t\t\t/* Override any existing binding, if another\n\t\t\t\t * one is supplied by user.\n\t\t\t\t */\n\t\t\t\tsk->sk_bound_dev_if = addr->l2tp_scope_id;\n\t\t\t}\n\n\t\t\t/* Binding to link-local address requires an\n\t\t\t   interface */\n\t\t\tif (!sk->sk_bound_dev_if)\n\t\t\t\tgoto out_unlock_rcu;\n\n\t\t\terr = -ENODEV;\n\t\t\tdev = dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\t\tif (!dev)\n\t\t\t\tgoto out_unlock_rcu;\n\t\t}\n\n\t\t/* ipv4 addr of the socket is invalid.  Only the\n\t\t * unspecified and mapped address have a v4 equivalent.\n\t\t */\n\t\tv4addr = LOOPBACK4_IPV6;\n\t\terr = -EADDRNOTAVAIL;\n\t\tif (!ipv6_chk_addr(sock_net(sk), &addr->l2tp_addr, dev, 0))\n\t\t\tgoto out_unlock_rcu;\n\t}\n\trcu_read_unlock();\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = v4addr;\n\tsk->sk_v6_rcv_saddr = addr->l2tp_addr;\n\tnp->saddr = addr->l2tp_addr;\n\n\tl2tp_ip6_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip6_lock);\n\tsk_add_bind_node(sk, &l2tp_ip6_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip6_lock);\n\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\trelease_sock(sk);\n\treturn 0;\n\nout_unlock_rcu:\n\trcu_read_unlock();\nout_unlock:\n\trelease_sock(sk);\n\treturn err;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip6_lock);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 977,
            "cve_id": "CVE-2016-10906",
            "code_snippet": "static void arc_emac_tx_clean(struct net_device *ndev)\n{\n\tstruct arc_emac_priv *priv = netdev_priv(ndev);\n\tstruct net_device_stats *stats = &ndev->stats;\n\tunsigned int i;\n\n\tfor (i = 0; i < TX_BD_NUM; i++) {\n\t\tunsigned int *txbd_dirty = &priv->txbd_dirty;\n\t\tstruct arc_emac_bd *txbd = &priv->txbd[*txbd_dirty];\n\t\tstruct buffer_state *tx_buff = &priv->tx_buff[*txbd_dirty];\n\t\tstruct sk_buff *skb = tx_buff->skb;\n\t\tunsigned int info = le32_to_cpu(txbd->info);\n\n\t\tif ((info & FOR_EMAC) || !txbd->data || !skb)\n\t\t\tbreak;\n\n\t\tif (unlikely(info & (DROP | DEFR | LTCL | UFLO))) {\n\t\t\tstats->tx_errors++;\n\t\t\tstats->tx_dropped++;\n\n\t\t\tif (info & DEFR)\n\t\t\t\tstats->tx_carrier_errors++;\n\n\t\t\tif (info & LTCL)\n\t\t\t\tstats->collisions++;\n\n\t\t\tif (info & UFLO)\n\t\t\t\tstats->tx_fifo_errors++;\n\t\t} else if (likely(info & FIRST_OR_LAST_MASK)) {\n\t\t\tstats->tx_packets++;\n\t\t\tstats->tx_bytes += skb->len;\n\t\t}\n\n\t\tdma_unmap_single(&ndev->dev, dma_unmap_addr(tx_buff, addr),\n\t\t\t\t dma_unmap_len(tx_buff, len), DMA_TO_DEVICE);\n\n\t\t/* return the sk_buff to system */\n\t\tdev_kfree_skb_irq(skb);\n\n\t\ttxbd->data = 0;\n\t\ttxbd->info = 0;\n\t\ttx_buff->skb = NULL;\n\n\t\t*txbd_dirty = (*txbd_dirty + 1) % TX_BD_NUM;\n\t}\n\n\t/* Ensure that txbd_dirty is visible to tx() before checking\n\t * for queue stopped.\n\t */\n\tsmp_mb();\n\n\tif (netif_queue_stopped(ndev) && arc_emac_tx_avail(priv))\n\t\tnetif_wake_queue(ndev);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 984,
            "cve_id": "CVE-2016-2069",
            "code_snippet": "void flush_tlb_current_task(void)\n{\n\tstruct mm_struct *mm = current->mm;\n\n\tpreempt_disable();\n\n\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);\n\n\t/* This is an implicit full barrier that synchronizes with switch_mm. */\n\tlocal_flush_tlb();\n\n\ttrace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);\n\tif (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)\n\t\tflush_tlb_others(mm_cpumask(mm), mm, 0UL, TLB_FLUSH_ALL);\n\tpreempt_enable();\n}",
            "detect_result": "NO\n\nThe provided code appears to be part of a Linux kernel function responsible for flushing the Translation Lookaside Buffer (TLB) for the current task. The function ensures that the TLB is synchronized across all relevant CPUs, which is crucial for maintaining memory consistency in a multi-processor environment.\n\nHere's a breakdown of the code:\n\n1. `struct mm_struct *mm = current->mm;` - Retrieves the memory descriptor structure for the current task.\n2. `preempt_disable();` - Disables preemption to prevent the current task from being interrupted, ensuring that the TLB flush operation is atomic.\n3. `count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);` - Increments a counter to track the number of local TLB flushes.\n4. `local_flush_tlb();` - Flushes the TLB on the local CPU.\n5. `trace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);` - Traces the TLB flush event for debugging and performance analysis purposes.\n6. The `if` statement checks if there are any other CPUs that need to have their TLBs flushed for this memory descriptor. If so, it calls `flush_tlb_others()` to perform the flush on those CPUs.\n7. `preempt_enable();` - Re-enables preemption after the TLB flush operations are complete.\n\nThe code does not appear to contain any obvious vulnerabilities such as buffer overflows, use-after-free errors, or improper synchronization issues. It follows standard practices for handling TLB flushes in a multi-processor environment. Therefore, based on the provided code snippet, it is not vulnerable.",
            "final_result": 0
        },
        {
            "id": 1004,
            "cve_id": "CVE-2016-2546",
            "code_snippet": "static long snd_timer_user_ioctl(struct file *file, unsigned int cmd,\n\t\t\t\t unsigned long arg)\n{\n\tstruct snd_timer_user *tu = file->private_data;\n\tlong ret;\n\n\tmutex_lock(&tu->ioctl_lock);\n\tret = __snd_timer_user_ioctl(file, cmd, arg);\n\tmutex_unlock(&tu->ioctl_lock);\n\treturn ret;\n}",
            "detect_result": "NO\n\nExplanation: The provided code snippet appears to be a part of a Linux kernel module, specifically handling an ioctl (input/output control) operation for a sound timer user interface. The function `snd_timer_user_ioctl` locks a mutex (`tu->ioctl_lock`) before calling another function `__snd_timer_user_ioctl` to perform the actual ioctl operation, and then unlocks the mutex afterward. This locking mechanism is used to ensure that only one thread can execute the `__snd_timer_user_ioctl` function at a time, preventing race conditions. Without additional context about the implementation of `__snd_timer_user_ioctl`, it's not possible to identify any specific vulnerabilities in this snippet alone. However, based on the given code, there are no obvious issues such as buffer overflows, improper memory management, or lack of input validation that would make this function vulnerable.",
            "final_result": 0
        },
        {
            "id": 1008,
            "cve_id": "CVE-2016-2547",
            "code_snippet": "static int snd_timer_start_slave(struct snd_timer_instance *timeri)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&slave_active_lock, flags);\n\ttimeri->flags |= SNDRV_TIMER_IFLG_RUNNING;\n\tif (timeri->master && timeri->timer) {\n\t\tspin_lock(&timeri->timer->lock);\n\t\tlist_add_tail(&timeri->active_list,\n\t\t\t      &timeri->master->slave_active_head);\n\t\tspin_unlock(&timeri->timer->lock);\n\t}\n\tspin_unlock_irqrestore(&slave_active_lock, flags);\n\treturn 1; /* delayed start */\n}",
            "detect_result": "NO\n\nExplanation of the code behavior:\nThe function `snd_timer_start_slave` is responsible for starting a slave timer instance in a sound subsystem. Here's a step-by-step breakdown of what the function does:\n\n1. It acquires a spinlock (`slave_active_lock`) to ensure that modifications to the shared data structure `slave_active_head` are atomic and thread-safe. The `irqsave` variant of the lock also saves the current interrupt state and disables interrupts, which is necessary when the lock can be acquired in both process and interrupt contexts.\n\n2. It sets the `SNDRV_TIMER_IFLG_RUNNING` flag in the `flags` field of the `timeri` structure, indicating that the timer instance is now running.\n\n3. It checks if the `timeri` has a master timer and an associated timer structure. If both conditions are true, it proceeds to add the `timeri` to the list of active slaves of its master timer.\n\n4. To safely modify the list of active slaves, it acquires another spinlock (`timeri->timer->lock`). This ensures that no other thread can modify the list at the same time.\n\n5. It adds the `timeri` to the end of the `slave_active_head` list using `list_add_tail`.\n\n6. After modifying the list, it releases the `timeri->timer->lock`.\n\n7. Finally, it releases the `slave_active_lock` and restores the previous interrupt state.\n\n8. The function returns 1, indicating a delayed start of the timer.\n\nVulnerability assessment:\nThe provided code does not appear to have any obvious vulnerabilities such as buffer overflows, use-after-free, or race conditions that could lead to security issues. The use of spinlocks ensures that critical sections are protected from concurrent access, and the function correctly handles the locking and unlocking of these locks. However, without more context about how this function is used and the overall design of the system, it is difficult to rule out all potential vulnerabilities. But based on the given code snippet alone, there are no apparent issues.",
            "final_result": 0
        },
        {
            "id": 1119,
            "cve_id": "CVE-2016-5195",
            "code_snippet": "static struct page *follow_page_pte(struct vm_area_struct *vma,\n\t\tunsigned long address, pmd_t *pmd, unsigned int flags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct dev_pagemap *pgmap = NULL;\n\tstruct page *page;\n\tspinlock_t *ptl;\n\tpte_t *ptep, pte;\n\nretry:\n\tif (unlikely(pmd_bad(*pmd)))\n\t\treturn no_page_table(vma, flags);\n\n\tptep = pte_offset_map_lock(mm, pmd, address, &ptl);\n\tpte = *ptep;\n\tif (!pte_present(pte)) {\n\t\tswp_entry_t entry;\n\t\t/*\n\t\t * KSM's break_ksm() relies upon recognizing a ksm page\n\t\t * even while it is being migrated, so for that case we\n\t\t * need migration_entry_wait().\n\t\t */\n\t\tif (likely(!(flags & FOLL_MIGRATION)))\n\t\t\tgoto no_page;\n\t\tif (pte_none(pte))\n\t\t\tgoto no_page;\n\t\tentry = pte_to_swp_entry(pte);\n\t\tif (!is_migration_entry(entry))\n\t\t\tgoto no_page;\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tmigration_entry_wait(mm, pmd, address);\n\t\tgoto retry;\n\t}\n\tif ((flags & FOLL_NUMA) && pte_protnone(pte))\n\t\tgoto no_page;\n\tif ((flags & FOLL_WRITE) && !can_follow_write_pte(pte, flags)) {\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\treturn NULL;\n\t}\n\n\tpage = vm_normal_page(vma, address, pte);\n\tif (!page && pte_devmap(pte) && (flags & FOLL_GET)) {\n\t\t/*\n\t\t * Only return device mapping pages in the FOLL_GET case since\n\t\t * they are only valid while holding the pgmap reference.\n\t\t */\n\t\tpgmap = get_dev_pagemap(pte_pfn(pte), NULL);\n\t\tif (pgmap)\n\t\t\tpage = pte_page(pte);\n\t\telse\n\t\t\tgoto no_page;\n\t} else if (unlikely(!page)) {\n\t\tif (flags & FOLL_DUMP) {\n\t\t\t/* Avoid special (like zero) pages in core dumps */\n\t\t\tpage = ERR_PTR(-EFAULT);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (is_zero_pfn(pte_pfn(pte))) {\n\t\t\tpage = pte_page(pte);\n\t\t} else {\n\t\t\tint ret;\n\n\t\t\tret = follow_pfn_pte(vma, address, ptep, flags);\n\t\t\tpage = ERR_PTR(ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (flags & FOLL_SPLIT && PageTransCompound(page)) {\n\t\tint ret;\n\t\tget_page(page);\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tlock_page(page);\n\t\tret = split_huge_page(page);\n\t\tunlock_page(page);\n\t\tput_page(page);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t\tgoto retry;\n\t}\n\n\tif (flags & FOLL_GET) {\n\t\tget_page(page);\n\n\t\t/* drop the pgmap reference now that we hold the page */\n\t\tif (pgmap) {\n\t\t\tput_dev_pagemap(pgmap);\n\t\t\tpgmap = NULL;\n\t\t}\n\t}\n\tif (flags & FOLL_TOUCH) {\n\t\tif ((flags & FOLL_WRITE) &&\n\t\t    !pte_dirty(pte) && !PageDirty(page))\n\t\t\tset_page_dirty(page);\n\t\t/*\n\t\t * pte_mkyoung() would be more correct here, but atomic care\n\t\t * is needed to avoid losing the dirty bit: it is easier to use\n\t\t * mark_page_accessed().\n\t\t */\n\t\tmark_page_accessed(page);\n\t}\n\tif ((flags & FOLL_MLOCK) && (vma->vm_flags & VM_LOCKED)) {\n\t\t/* Do not mlock pte-mapped THP */\n\t\tif (PageTransCompound(page))\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * The preliminary mapping check is mainly to avoid the\n\t\t * pointless overhead of lock_page on the ZERO_PAGE\n\t\t * which might bounce very badly if there is contention.\n\t\t *\n\t\t * If the page is already locked, we don't need to\n\t\t * handle it now - vmscan will handle it later if and\n\t\t * when it attempts to reclaim the page.\n\t\t */\n\t\tif (page->mapping && trylock_page(page)) {\n\t\t\tlru_add_drain();  /* push cached pages to LRU */\n\t\t\t/*\n\t\t\t * Because we lock page here, and migration is\n\t\t\t * blocked by the pte's page reference, and we\n\t\t\t * know the page is still mapped, we don't even\n\t\t\t * need to check for file-cache page truncation.\n\t\t\t */\n\t\t\tmlock_vma_page(page);\n\t\t\tunlock_page(page);\n\t\t}\n\t}\nout:\n\tpte_unmap_unlock(ptep, ptl);\n\treturn page;\nno_page:\n\tpte_unmap_unlock(ptep, ptl);\n\tif (!pte_none(pte))\n\t\treturn NULL;\n\treturn no_page_table(vma, flags);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1212,
            "cve_id": "CVE-2016-8655",
            "code_snippet": "static int\npacket_setsockopt(struct socket *sock, int level, int optname, char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint ret;\n\n\tif (level != SOL_PACKET)\n\t\treturn -ENOPROTOOPT;\n\n\tswitch (optname) {\n\tcase PACKET_ADD_MEMBERSHIP:\n\tcase PACKET_DROP_MEMBERSHIP:\n\t{\n\t\tstruct packet_mreq_max mreq;\n\t\tint len = optlen;\n\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\tif (len < sizeof(struct packet_mreq))\n\t\t\treturn -EINVAL;\n\t\tif (len > sizeof(mreq))\n\t\t\tlen = sizeof(mreq);\n\t\tif (copy_from_user(&mreq, optval, len))\n\t\t\treturn -EFAULT;\n\t\tif (len < (mreq.mr_alen + offsetof(struct packet_mreq, mr_address)))\n\t\t\treturn -EINVAL;\n\t\tif (optname == PACKET_ADD_MEMBERSHIP)\n\t\t\tret = packet_mc_add(sk, &mreq);\n\t\telse\n\t\t\tret = packet_mc_drop(sk, &mreq);\n\t\treturn ret;\n\t}\n\n\tcase PACKET_RX_RING:\n\tcase PACKET_TX_RING:\n\t{\n\t\tunion tpacket_req_u req_u;\n\t\tint len;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\t\tlen = sizeof(req_u.req);\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\tdefault:\n\t\t\tlen = sizeof(req_u.req3);\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&req_u.req, optval, len))\n\t\t\treturn -EFAULT;\n\t\treturn packet_set_ring(sk, &req_u, 0,\n\t\t\toptname == PACKET_TX_RING);\n\t}\n\tcase PACKET_COPY_THRESH:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpkt_sk(sk)->copy_thresh = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VERSION:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tswitch (val) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\tcase TPACKET_V3:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tlock_sock(sk);\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {\n\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tpo->tp_version = val;\n\t\t\tret = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t\treturn ret;\n\t}\n\tcase PACKET_RESERVE:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_reserve = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_LOSS:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_loss = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_AUXDATA:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->auxdata = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_ORIGDEV:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->origdev = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VNET_HDR:\n\t{\n\t\tint val;\n\n\t\tif (sock->type != SOCK_RAW)\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->has_vnet_hdr = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_TIMESTAMP:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->tp_tstamp = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_FANOUT:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\treturn fanout_add(sk, val & 0xffff, val >> 16);\n\t}\n\tcase PACKET_FANOUT_DATA:\n\t{\n\t\tif (!po->fanout)\n\t\t\treturn -EINVAL;\n\n\t\treturn fanout_set_data(po, optval, optlen);\n\t}\n\tcase PACKET_TX_HAS_OFF:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_tx_has_off = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_QDISC_BYPASS:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->xmit = val ? packet_direct_xmit : dev_queue_xmit;\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1262,
            "cve_id": "CVE-2017-1000112",
            "code_snippet": "ssize_t\tip_append_page(struct sock *sk, struct flowi4 *fl4, struct page *page,\n\t\t       int offset, size_t size, int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct rtable *rt;\n\tstruct ip_options *opt = NULL;\n\tstruct inet_cork *cork;\n\tint hh_len;\n\tint mtu;\n\tint len;\n\tint err;\n\tunsigned int maxfraglen, fragheaderlen, fraggap, maxnonfragsize;\n\n\tif (inet->hdrincl)\n\t\treturn -EPERM;\n\n\tif (flags&MSG_PROBE)\n\t\treturn 0;\n\n\tif (skb_queue_empty(&sk->sk_write_queue))\n\t\treturn -EINVAL;\n\n\tcork = &inet->cork.base;\n\trt = (struct rtable *)cork->dst;\n\tif (cork->flags & IPCORK_OPT)\n\t\topt = cork->opt;\n\n\tif (!(rt->dst.dev->features&NETIF_F_SG))\n\t\treturn -EOPNOTSUPP;\n\n\thh_len = LL_RESERVED_SPACE(rt->dst.dev);\n\tmtu = cork->fragsize;\n\n\tfragheaderlen = sizeof(struct iphdr) + (opt ? opt->optlen : 0);\n\tmaxfraglen = ((mtu - fragheaderlen) & ~7) + fragheaderlen;\n\tmaxnonfragsize = ip_sk_ignore_df(sk) ? 0xFFFF : mtu;\n\n\tif (cork->length + size > maxnonfragsize - fragheaderlen) {\n\t\tip_local_error(sk, EMSGSIZE, fl4->daddr, inet->inet_dport,\n\t\t\t       mtu - (opt ? opt->optlen : 0));\n\t\treturn -EMSGSIZE;\n\t}\n\n\tskb = skb_peek_tail(&sk->sk_write_queue);\n\tif (!skb)\n\t\treturn -EINVAL;\n\n\tif ((size + skb->len > mtu) &&\n\t    (skb_queue_len(&sk->sk_write_queue) == 1) &&\n\t    (sk->sk_protocol == IPPROTO_UDP) &&\n\t    (rt->dst.dev->features & NETIF_F_UFO)) {\n\t\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tskb_shinfo(skb)->gso_size = mtu - fragheaderlen;\n\t\tskb_shinfo(skb)->gso_type = SKB_GSO_UDP;\n\t}\n\tcork->length += size;\n\n\twhile (size > 0) {\n\t\tif (skb_is_gso(skb)) {\n\t\t\tlen = size;\n\t\t} else {\n\n\t\t\t/* Check if the remaining data fits into current packet. */\n\t\t\tlen = mtu - skb->len;\n\t\t\tif (len < size)\n\t\t\t\tlen = maxfraglen - skb->len;\n\t\t}\n\t\tif (len <= 0) {\n\t\t\tstruct sk_buff *skb_prev;\n\t\t\tint alloclen;\n\n\t\t\tskb_prev = skb;\n\t\t\tfraggap = skb_prev->len - maxfraglen;\n\n\t\t\talloclen = fragheaderlen + hh_len + fraggap + 15;\n\t\t\tskb = sock_wmalloc(sk, alloclen, 1, sk->sk_allocation);\n\t\t\tif (unlikely(!skb)) {\n\t\t\t\terr = -ENOBUFS;\n\t\t\t\tgoto error;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t *\tFill in the control structures\n\t\t\t */\n\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\t\tskb->csum = 0;\n\t\t\tskb_reserve(skb, hh_len);\n\n\t\t\t/*\n\t\t\t *\tFind where to start putting bytes.\n\t\t\t */\n\t\t\tskb_put(skb, fragheaderlen + fraggap);\n\t\t\tskb_reset_network_header(skb);\n\t\t\tskb->transport_header = (skb->network_header +\n\t\t\t\t\t\t fragheaderlen);\n\t\t\tif (fraggap) {\n\t\t\t\tskb->csum = skb_copy_and_csum_bits(skb_prev,\n\t\t\t\t\t\t\t\t   maxfraglen,\n\t\t\t\t\t\t    skb_transport_header(skb),\n\t\t\t\t\t\t\t\t   fraggap, 0);\n\t\t\t\tskb_prev->csum = csum_sub(skb_prev->csum,\n\t\t\t\t\t\t\t  skb->csum);\n\t\t\t\tpskb_trim_unique(skb_prev, maxfraglen);\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Put the packet on the pending queue.\n\t\t\t */\n\t\t\t__skb_queue_tail(&sk->sk_write_queue, skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (len > size)\n\t\t\tlen = size;\n\n\t\tif (skb_append_pagefrags(skb, page, offset, len)) {\n\t\t\terr = -EMSGSIZE;\n\t\t\tgoto error;\n\t\t}\n\n\t\tif (skb->ip_summed == CHECKSUM_NONE) {\n\t\t\t__wsum csum;\n\t\t\tcsum = csum_page(page, offset, len);\n\t\t\tskb->csum = csum_block_add(skb->csum, csum, skb->len);\n\t\t}\n\n\t\tskb->len += len;\n\t\tskb->data_len += len;\n\t\tskb->truesize += len;\n\t\trefcount_add(len, &sk->sk_wmem_alloc);\n\t\toffset += len;\n\t\tsize -= len;\n\t}\n\treturn 0;\n\nerror:\n\tcork->length -= size;\n\tIP_INC_STATS(sock_net(sk), IPSTATS_MIB_OUTDISCARDS);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1320,
            "cve_id": "CVE-2017-1000405",
            "code_snippet": "struct page *follow_devmap_pmd(struct vm_area_struct *vma, unsigned long addr,\n\t\tpmd_t *pmd, int flags)\n{\n\tunsigned long pfn = pmd_pfn(*pmd);\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct dev_pagemap *pgmap;\n\tstruct page *page;\n\n\tassert_spin_locked(pmd_lockptr(mm, pmd));\n\n\t/*\n\t * When we COW a devmap PMD entry, we split it into PTEs, so we should\n\t * not be in this function with `flags & FOLL_COW` set.\n\t */\n\tWARN_ONCE(flags & FOLL_COW, \"mm: In follow_devmap_pmd with FOLL_COW set\");\n\n\tif (flags & FOLL_WRITE && !pmd_write(*pmd))\n\t\treturn NULL;\n\n\tif (pmd_present(*pmd) && pmd_devmap(*pmd))\n\t\t/* pass */;\n\telse\n\t\treturn NULL;\n\n\tif (flags & FOLL_TOUCH)\n\t\ttouch_pmd(vma, addr, pmd, flags);\n\n\t/*\n\t * device mapped pages can only be returned if the\n\t * caller will manage the page reference count.\n\t */\n\tif (!(flags & FOLL_GET))\n\t\treturn ERR_PTR(-EEXIST);\n\n\tpfn += (addr & ~PMD_MASK) >> PAGE_SHIFT;\n\tpgmap = get_dev_pagemap(pfn, NULL);\n\tif (!pgmap)\n\t\treturn ERR_PTR(-EFAULT);\n\tpage = pfn_to_page(pfn);\n\tget_page(page);\n\tput_dev_pagemap(pgmap);\n\n\treturn page;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1336,
            "cve_id": "CVE-2017-12146",
            "code_snippet": "static ssize_t driver_override_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct platform_device *pdev = to_platform_device(dev);\n\tssize_t len;\n\n\tdevice_lock(dev);\n\tlen = sprintf(buf, \"%s\\n\", pdev->driver_override);\n\tdevice_unlock(dev);\n\treturn len;\n}",
            "detect_result": "NO\n\nExplanation: The provided code snippet defines a function `driver_override_show` which is used to display the value of `driver_override` for a given platform device. It locks the device to ensure thread safety while accessing the `driver_override` field, formats the string using `sprintf`, and then unlocks the device. There are no obvious vulnerabilities such as buffer overflows, format string vulnerabilities, or improper memory handling in this snippet. However, it's always important to consider the context in which this function is used and the overall security of the system.",
            "final_result": 0
        },
        {
            "id": 1382,
            "cve_id": "CVE-2017-15265",
            "code_snippet": "struct snd_seq_client_port *snd_seq_create_port(struct snd_seq_client *client,\n\t\t\t\t\t\tint port)\n{\n\tunsigned long flags;\n\tstruct snd_seq_client_port *new_port, *p;\n\tint num = -1;\n\t\n\t/* sanity check */\n\tif (snd_BUG_ON(!client))\n\t\treturn NULL;\n\n\tif (client->num_ports >= SNDRV_SEQ_MAX_PORTS) {\n\t\tpr_warn(\"ALSA: seq: too many ports for client %d\\n\", client->number);\n\t\treturn NULL;\n\t}\n\n\t/* create a new port */\n\tnew_port = kzalloc(sizeof(*new_port), GFP_KERNEL);\n\tif (!new_port)\n\t\treturn NULL;\t/* failure, out of memory */\n\t/* init port data */\n\tnew_port->addr.client = client->number;\n\tnew_port->addr.port = -1;\n\tnew_port->owner = THIS_MODULE;\n\tsprintf(new_port->name, \"port-%d\", num);\n\tsnd_use_lock_init(&new_port->use_lock);\n\tport_subs_info_init(&new_port->c_src);\n\tport_subs_info_init(&new_port->c_dest);\n\tsnd_use_lock_use(&new_port->use_lock);\n\n\tnum = port >= 0 ? port : 0;\n\tmutex_lock(&client->ports_mutex);\n\twrite_lock_irqsave(&client->ports_lock, flags);\n\tlist_for_each_entry(p, &client->ports_list_head, list) {\n\t\tif (p->addr.port > num)\n\t\t\tbreak;\n\t\tif (port < 0) /* auto-probe mode */\n\t\t\tnum = p->addr.port + 1;\n\t}\n\t/* insert the new port */\n\tlist_add_tail(&new_port->list, &p->list);\n\tclient->num_ports++;\n\tnew_port->addr.port = num;\t/* store the port number in the port */\n\tsprintf(new_port->name, \"port-%d\", num);\n\twrite_unlock_irqrestore(&client->ports_lock, flags);\n\tmutex_unlock(&client->ports_mutex);\n\n\treturn new_port;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1569,
            "cve_id": "CVE-2017-6346",
            "code_snippet": "static void fanout_release(struct sock *sk)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f;\n\n\tmutex_lock(&fanout_mutex);\n\tf = po->fanout;\n\tif (f) {\n\t\tpo->fanout = NULL;\n\n\t\tif (atomic_dec_and_test(&f->sk_ref)) {\n\t\t\tlist_del(&f->list);\n\t\t\tdev_remove_pack(&f->prot_hook);\n\t\t\tfanout_release_data(f);\n\t\t\tkfree(f);\n\t\t}\n\n\t\tif (po->rollover)\n\t\t\tkfree_rcu(po->rollover, rcu);\n\t}\n\tmutex_unlock(&fanout_mutex);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1574,
            "cve_id": "CVE-2017-6874",
            "code_snippet": "static struct ucounts *get_ucounts(struct user_namespace *ns, kuid_t uid)\n{\n\tstruct hlist_head *hashent = ucounts_hashentry(ns, uid);\n\tstruct ucounts *ucounts, *new;\n\n\tspin_lock_irq(&ucounts_lock);\n\tucounts = find_ucounts(ns, uid, hashent);\n\tif (!ucounts) {\n\t\tspin_unlock_irq(&ucounts_lock);\n\n\t\tnew = kzalloc(sizeof(*new), GFP_KERNEL);\n\t\tif (!new)\n\t\t\treturn NULL;\n\n\t\tnew->ns = ns;\n\t\tnew->uid = uid;\n\t\tnew->count = 0;\n\n\t\tspin_lock_irq(&ucounts_lock);\n\t\tucounts = find_ucounts(ns, uid, hashent);\n\t\tif (ucounts) {\n\t\t\tkfree(new);\n\t\t} else {\n\t\t\thlist_add_head(&new->node, hashent);\n\t\t\tucounts = new;\n\t\t}\n\t}\n\tif (ucounts->count == INT_MAX)\n\t\tucounts = NULL;\n\telse\n\t\tucounts->count += 1;\n\tspin_unlock_irq(&ucounts_lock);\n\treturn ucounts;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1597,
            "cve_id": "CVE-2017-7533",
            "code_snippet": "int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,\n\t       struct inode *new_dir, struct dentry *new_dentry,\n\t       struct inode **delegated_inode, unsigned int flags)\n{\n\tint error;\n\tbool is_dir = d_is_dir(old_dentry);\n\tstruct inode *source = old_dentry->d_inode;\n\tstruct inode *target = new_dentry->d_inode;\n\tbool new_is_dir = false;\n\tunsigned max_links = new_dir->i_sb->s_max_links;\n\tstruct name_snapshot old_name;\n\n\tif (source == target)\n\t\treturn 0;\n\n\terror = may_delete(old_dir, old_dentry, is_dir);\n\tif (error)\n\t\treturn error;\n\n\tif (!target) {\n\t\terror = may_create(new_dir, new_dentry);\n\t} else {\n\t\tnew_is_dir = d_is_dir(new_dentry);\n\n\t\tif (!(flags & RENAME_EXCHANGE))\n\t\t\terror = may_delete(new_dir, new_dentry, is_dir);\n\t\telse\n\t\t\terror = may_delete(new_dir, new_dentry, new_is_dir);\n\t}\n\tif (error)\n\t\treturn error;\n\n\tif (!old_dir->i_op->rename)\n\t\treturn -EPERM;\n\n\t/*\n\t * If we are going to change the parent - check write permissions,\n\t * we'll need to flip '..'.\n\t */\n\tif (new_dir != old_dir) {\n\t\tif (is_dir) {\n\t\t\terror = inode_permission(source, MAY_WRITE);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\t\tif ((flags & RENAME_EXCHANGE) && new_is_dir) {\n\t\t\terror = inode_permission(target, MAY_WRITE);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\t}\n\n\terror = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,\n\t\t\t\t      flags);\n\tif (error)\n\t\treturn error;\n\n\ttake_dentry_name_snapshot(&old_name, old_dentry);\n\tdget(new_dentry);\n\tif (!is_dir || (flags & RENAME_EXCHANGE))\n\t\tlock_two_nondirectories(source, target);\n\telse if (target)\n\t\tinode_lock(target);\n\n\terror = -EBUSY;\n\tif (is_local_mountpoint(old_dentry) || is_local_mountpoint(new_dentry))\n\t\tgoto out;\n\n\tif (max_links && new_dir != old_dir) {\n\t\terror = -EMLINK;\n\t\tif (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)\n\t\t\tgoto out;\n\t\tif ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&\n\t\t    old_dir->i_nlink >= max_links)\n\t\t\tgoto out;\n\t}\n\tif (is_dir && !(flags & RENAME_EXCHANGE) && target)\n\t\tshrink_dcache_parent(new_dentry);\n\tif (!is_dir) {\n\t\terror = try_break_deleg(source, delegated_inode);\n\t\tif (error)\n\t\t\tgoto out;\n\t}\n\tif (target && !new_is_dir) {\n\t\terror = try_break_deleg(target, delegated_inode);\n\t\tif (error)\n\t\t\tgoto out;\n\t}\n\terror = old_dir->i_op->rename(old_dir, old_dentry,\n\t\t\t\t       new_dir, new_dentry, flags);\n\tif (error)\n\t\tgoto out;\n\n\tif (!(flags & RENAME_EXCHANGE) && target) {\n\t\tif (is_dir)\n\t\t\ttarget->i_flags |= S_DEAD;\n\t\tdont_mount(new_dentry);\n\t\tdetach_mounts(new_dentry);\n\t}\n\tif (!(old_dir->i_sb->s_type->fs_flags & FS_RENAME_DOES_D_MOVE)) {\n\t\tif (!(flags & RENAME_EXCHANGE))\n\t\t\td_move(old_dentry, new_dentry);\n\t\telse\n\t\t\td_exchange(old_dentry, new_dentry);\n\t}\nout:\n\tif (!is_dir || (flags & RENAME_EXCHANGE))\n\t\tunlock_two_nondirectories(source, target);\n\telse if (target)\n\t\tinode_unlock(target);\n\tdput(new_dentry);\n\tif (!error) {\n\t\tfsnotify_move(old_dir, new_dir, old_name.name, is_dir,\n\t\t\t      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);\n\t\tif (flags & RENAME_EXCHANGE) {\n\t\t\tfsnotify_move(new_dir, old_dir, old_dentry->d_name.name,\n\t\t\t\t      new_is_dir, NULL, new_dentry);\n\t\t}\n\t}\n\trelease_dentry_name_snapshot(&old_name);\n\n\treturn error;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1664,
            "cve_id": "CVE-2018-1000004",
            "code_snippet": "static struct snd_seq_client *seq_create_client1(int client_index, int poolsize)\n{\n\tunsigned long flags;\n\tint c;\n\tstruct snd_seq_client *client;\n\n\t/* init client data */\n\tclient = kzalloc(sizeof(*client), GFP_KERNEL);\n\tif (client == NULL)\n\t\treturn NULL;\n\tclient->pool = snd_seq_pool_new(poolsize);\n\tif (client->pool == NULL) {\n\t\tkfree(client);\n\t\treturn NULL;\n\t}\n\tclient->type = NO_CLIENT;\n\tsnd_use_lock_init(&client->use_lock);\n\trwlock_init(&client->ports_lock);\n\tmutex_init(&client->ports_mutex);\n\tINIT_LIST_HEAD(&client->ports_list_head);\n\tmutex_init(&client->ioctl_mutex);\n\n\t/* find free slot in the client table */\n\tspin_lock_irqsave(&clients_lock, flags);\n\tif (client_index < 0) {\n\t\tfor (c = SNDRV_SEQ_DYNAMIC_CLIENTS_BEGIN;\n\t\t     c < SNDRV_SEQ_MAX_CLIENTS;\n\t\t     c++) {\n\t\t\tif (clienttab[c] || clienttablock[c])\n\t\t\t\tcontinue;\n\t\t\tclienttab[client->number = c] = client;\n\t\t\tspin_unlock_irqrestore(&clients_lock, flags);\n\t\t\treturn client;\n\t\t}\n\t} else {\n\t\tif (clienttab[client_index] == NULL && !clienttablock[client_index]) {\n\t\t\tclienttab[client->number = client_index] = client;\n\t\t\tspin_unlock_irqrestore(&clients_lock, flags);\n\t\t\treturn client;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&clients_lock, flags);\n\tsnd_seq_pool_delete(&client->pool);\n\tkfree(client);\n\treturn NULL;\t/* no free slot found or busy, return failure code */\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1742,
            "cve_id": "CVE-2018-12232",
            "code_snippet": "static int sock_close(struct inode *inode, struct file *filp)\n{\n\t__sock_release(SOCKET_I(inode), inode);\n\treturn 0;\n}",
            "detect_result": "NO\n\nExplanation: The provided code snippet is a function `sock_close` that appears to be part of a kernel module or a similar low-level system component, likely related to socket handling in a Unix-like operating system. The function takes two parameters: `inode`, which represents an index node in the filesystem, and `filp`, which represents a file structure.\n\nThe function calls `__sock_release` with `SOCKET_I(inode)` and `inode` as arguments. The purpose of this function seems to be to release resources associated with a socket when it is closed. The return value of `sock_close` is always 0, indicating successful execution.\n\nBased on the provided code snippet alone, there are no obvious vulnerabilities such as buffer overflows, use-after-free errors, or improper input validation. However, it's important to note that without the full context of the surrounding code, including the implementation of `__sock_release` and how `sock_close` is used within the larger system, it's not possible to definitively rule out all potential vulnerabilities. But given the snippet provided, there is no immediate indication of a vulnerability.",
            "final_result": 0
        },
        {
            "id": 1875,
            "cve_id": "CVE-2018-20836",
            "code_snippet": "static void smp_task_done(struct sas_task *task)\n{\n\tdel_timer(&task->slow_task->timer);\n\tcomplete(&task->slow_task->completion);\n}",
            "detect_result": "NO\n\nExplanation: The provided code snippet appears to be part of a function that handles the completion of an SAS (Serial Attached SCSI) task. It deletes a timer associated with the task and completes a completion object, which is typically used to signal that the task has finished.\n\nBased on the given code alone, there are no obvious vulnerabilities such as buffer overflows, use-after-free, or improper input validation. However, without more context about how `sas_task` and its members are managed and used elsewhere in the codebase, it's challenging to definitively state that there are no vulnerabilities. But from this isolated function, there is no evident issue.",
            "final_result": 0
        },
        {
            "id": 1922,
            "cve_id": "CVE-2018-5814",
            "code_snippet": "static void stub_disconnect(struct usb_device *udev)\n{\n\tstruct stub_device *sdev;\n\tconst char *udev_busid = dev_name(&udev->dev);\n\tstruct bus_id_priv *busid_priv;\n\tint rc;\n\n\tdev_dbg(&udev->dev, \"Enter disconnect\\n\");\n\n\tbusid_priv = get_busid_priv(udev_busid);\n\tif (!busid_priv) {\n\t\tBUG();\n\t\treturn;\n\t}\n\n\tsdev = dev_get_drvdata(&udev->dev);\n\n\t/* get stub_device */\n\tif (!sdev) {\n\t\tdev_err(&udev->dev, \"could not get device\");\n\t\tgoto call_put_busid_priv;\n\t}\n\n\tdev_set_drvdata(&udev->dev, NULL);\n\n\t/*\n\t * NOTE: rx/tx threads are invoked for each usb_device.\n\t */\n\tstub_remove_files(&udev->dev);\n\n\t/* release port */\n\trc = usb_hub_release_port(udev->parent, udev->portnum,\n\t\t\t\t  (struct usb_dev_state *) udev);\n\tif (rc) {\n\t\tdev_dbg(&udev->dev, \"unable to release port\\n\");\n\t\tgoto call_put_busid_priv;\n\t}\n\n\t/* If usb reset is called from event handler */\n\tif (usbip_in_eh(current))\n\t\tgoto call_put_busid_priv;\n\n\t/* shutdown the current connection */\n\tshutdown_busid(busid_priv);\n\n\tusb_put_dev(sdev->udev);\n\n\t/* free sdev */\n\tbusid_priv->sdev = NULL;\n\tstub_device_free(sdev);\n\n\tif (busid_priv->status == STUB_BUSID_ALLOC)\n\t\tbusid_priv->status = STUB_BUSID_ADDED;\n\ncall_put_busid_priv:\n\tput_busid_priv(busid_priv);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1923,
            "cve_id": "CVE-2018-5814",
            "code_snippet": "int del_match_busid(char *busid)\n{\n\tint idx;\n\tint ret = -1;\n\n\tspin_lock(&busid_table_lock);\n\tidx = get_busid_idx(busid);\n\tif (idx < 0)\n\t\tgoto out;\n\n\t/* found */\n\tret = 0;\n\n\tspin_lock(&busid_table[idx].busid_lock);\n\n\tif (busid_table[idx].status == STUB_BUSID_OTHER)\n\t\tmemset(busid_table[idx].name, 0, BUSID_SIZE);\n\n\tif ((busid_table[idx].status != STUB_BUSID_OTHER) &&\n\t    (busid_table[idx].status != STUB_BUSID_ADDED))\n\t\tbusid_table[idx].status = STUB_BUSID_REMOV;\n\n\tspin_unlock(&busid_table[idx].busid_lock);\nout:\n\tspin_unlock(&busid_table_lock);\n\n\treturn ret;\n}",
            "detect_result": "NO\n\nExplanation:\nThe provided code appears to be managing a table of bus IDs with locking mechanisms to ensure thread safety. Here's a breakdown of its behavior:\n\n1. The function `del_match_busid` takes a `char *busid` as an argument.\n2. It locks a global `busid_table_lock` to safely access the `busid_table`.\n3. It searches for the index of the given `busid` in the `busid_table` using `get_busid_idx`.\n4. If the `busid` is not found (`idx < 0`), it skips to the `out` label and unlocks the `busid_table_lock`.\n5. If the `busid` is found, it sets `ret` to 0 indicating success.\n6. It then locks the specific `busid_lock` for the found entry in the `busid_table`.\n7. Depending on the `status` of the entry, it either clears the `name` field if the status is `STUB_BUSID_OTHER`, or sets the status to `STUB_BUSID_REMOV` if the status is neither `STUB_BUSID_OTHER` nor `STUB_BUSID_ADDED`.\n8. It unlocks the specific `busid_lock`.\n9. Finally, it unlocks the global `busid_table_lock` and returns `ret`.\n\nThe code uses proper locking mechanisms (`spin_lock` and `spin_unlock`) to protect shared data structures from concurrent access, which helps prevent race conditions. Therefore, based on the provided code snippet, there are no obvious vulnerabilities related to concurrency or data integrity. However, without more context about the rest of the system, such as how `busid_table` and `busid_table_lock` are initialized and used elsewhere, it's impossible to rule out all potential issues. But within the scope of this function, it appears to be correctly implemented.",
            "final_result": 0
        },
        {
            "id": 1924,
            "cve_id": "CVE-2018-5814",
            "code_snippet": "static ssize_t match_busid_show(struct device_driver *drv, char *buf)\n{\n\tint i;\n\tchar *out = buf;\n\n\tspin_lock(&busid_table_lock);\n\tfor (i = 0; i < MAX_BUSID; i++) {\n\t\tspin_lock(&busid_table[i].busid_lock);\n\t\tif (busid_table[i].name[0])\n\t\t\tout += sprintf(out, \"%s \", busid_table[i].name);\n\t\tspin_unlock(&busid_table[i].busid_lock);\n\t}\n\tspin_unlock(&busid_table_lock);\n\tout += sprintf(out, \"\\n\");\n\n\treturn out - buf;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1925,
            "cve_id": "CVE-2018-5814",
            "code_snippet": "struct bus_id_priv *get_busid_priv(const char *busid)\n{\n\tint idx;\n\tstruct bus_id_priv *bid = NULL;\n\n\tspin_lock(&busid_table_lock);\n\tidx = get_busid_idx(busid);\n\tif (idx >= 0) {\n\t\tbid = &(busid_table[idx]);\n\t\t/* get busid_lock before returning */\n\t\tspin_lock(&bid->busid_lock);\n\t}\n\tspin_unlock(&busid_table_lock);\n\n\treturn bid;\n}",
            "detect_result": "NO\n\nExplanation:\nThe provided code snippet appears to be part of a function that retrieves a `bus_id_priv` structure based on a given `busid`. The function uses locking mechanisms (`spin_lock` and `spin_unlock`) to ensure thread safety when accessing shared data structures (`busid_table` and `bid->busid_lock`). \n\nHere's a breakdown of the code behavior:\n1. It locks the `busid_table_lock` to prevent other threads from modifying the `busid_table` while it is being accessed.\n2. It calculates an index (`idx`) for the given `busid` using the `get_busid_idx` function.\n3. If the index is valid (i.e., `idx >= 0`), it sets `bid` to point to the corresponding entry in the `busid_table`.\n4. Before returning the pointer to the `bus_id_priv` structure, it locks the `busid_lock` associated with the specific `bus_id_priv` entry to ensure that no other thread can modify this entry while it is being used.\n5. Finally, it unlocks the `busid_table_lock` to allow other threads to access the table.\n\nThe code does not show any obvious vulnerabilities such as buffer overflows, use-after-free, or improper memory management. However, without more context about the rest of the system, it's impossible to rule out all potential issues. But based solely on the provided code snippet, there are no apparent vulnerabilities related to race conditions or data corruption due to improper synchronization.",
            "final_result": 0
        },
        {
            "id": 1957,
            "cve_id": "CVE-2018-7995",
            "code_snippet": "static ssize_t set_cmci_disabled(struct device *s,\n\t\t\t\t struct device_attribute *attr,\n\t\t\t\t const char *buf, size_t size)\n{\n\tu64 new;\n\n\tif (kstrtou64(buf, 0, &new) < 0)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&mce_sysfs_mutex);\n\tif (mca_cfg.cmci_disabled ^ !!new) {\n\t\tif (new) {\n\t\t\t/* disable cmci */\n\t\t\ton_each_cpu(mce_disable_cmci, NULL, 1);\n\t\t\tmca_cfg.cmci_disabled = true;\n\t\t} else {\n\t\t\t/* enable cmci */\n\t\t\tmca_cfg.cmci_disabled = false;\n\t\t\ton_each_cpu(mce_enable_ce, NULL, 1);\n\t\t}\n\t}\n\tmutex_unlock(&mce_sysfs_mutex);\n\n\treturn size;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2059,
            "cve_id": "CVE-2019-13233",
            "code_snippet": "int insn_get_code_seg_params(struct pt_regs *regs)\n{\n\tstruct desc_struct desc;\n\tshort sel;\n\n\tif (v8086_mode(regs))\n\t\t/* Address and operand size are both 16-bit. */\n\t\treturn INSN_CODE_SEG_PARAMS(2, 2);\n\n\tsel = get_segment_selector(regs, INAT_SEG_REG_CS);\n\tif (sel < 0)\n\t\treturn sel;\n\n\tif (!get_desc(&desc, sel))\n\t\treturn -EINVAL;\n\n\t/*\n\t * The most significant byte of the Type field of the segment descriptor\n\t * determines whether a segment contains data or code. If this is a data\n\t * segment, return error.\n\t */\n\tif (!(desc.type & BIT(3)))\n\t\treturn -EINVAL;\n\n\tswitch ((desc.l << 1) | desc.d) {\n\tcase 0: /*\n\t\t * Legacy mode. CS.L=0, CS.D=0. Address and operand size are\n\t\t * both 16-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(2, 2);\n\tcase 1: /*\n\t\t * Legacy mode. CS.L=0, CS.D=1. Address and operand size are\n\t\t * both 32-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(4, 4);\n\tcase 2: /*\n\t\t * IA-32e 64-bit mode. CS.L=1, CS.D=0. Address size is 64-bit;\n\t\t * operand size is 32-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(4, 8);\n\tcase 3: /* Invalid setting. CS.L=1, CS.D=1 */\n\t\t/* fall through */\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2200,
            "cve_id": "CVE-2019-18683",
            "code_snippet": "static int vivid_thread_vid_cap(void *data)\n{\n\tstruct vivid_dev *dev = data;\n\tu64 numerators_since_start;\n\tu64 buffers_since_start;\n\tu64 next_jiffies_since_start;\n\tunsigned long jiffies_since_start;\n\tunsigned long cur_jiffies;\n\tunsigned wait_jiffies;\n\tunsigned numerator;\n\tunsigned denominator;\n\tint dropped_bufs;\n\n\tdprintk(dev, 1, \"Video Capture Thread Start\\n\");\n\n\tset_freezable();\n\n\t/* Resets frame counters */\n\tdev->cap_seq_offset = 0;\n\tdev->cap_seq_count = 0;\n\tdev->cap_seq_resync = false;\n\tdev->jiffies_vid_cap = jiffies;\n\tdev->cap_stream_start = ktime_get_ns();\n\tvivid_cap_update_frame_period(dev);\n\n\tfor (;;) {\n\t\ttry_to_freeze();\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tif (!mutex_trylock(&dev->mutex)) {\n\t\t\tschedule_timeout_uninterruptible(1);\n\t\t\tcontinue;\n\t\t}\n\n\t\tcur_jiffies = jiffies;\n\t\tif (dev->cap_seq_resync) {\n\t\t\tdev->jiffies_vid_cap = cur_jiffies;\n\t\t\tdev->cap_seq_offset = dev->cap_seq_count + 1;\n\t\t\tdev->cap_seq_count = 0;\n\t\t\tdev->cap_stream_start += dev->cap_frame_period *\n\t\t\t\t\t\t dev->cap_seq_offset;\n\t\t\tvivid_cap_update_frame_period(dev);\n\t\t\tdev->cap_seq_resync = false;\n\t\t}\n\t\tnumerator = dev->timeperframe_vid_cap.numerator;\n\t\tdenominator = dev->timeperframe_vid_cap.denominator;\n\n\t\tif (dev->field_cap == V4L2_FIELD_ALTERNATE)\n\t\t\tdenominator *= 2;\n\n\t\t/* Calculate the number of jiffies since we started streaming */\n\t\tjiffies_since_start = cur_jiffies - dev->jiffies_vid_cap;\n\t\t/* Get the number of buffers streamed since the start */\n\t\tbuffers_since_start = (u64)jiffies_since_start * denominator +\n\t\t\t\t      (HZ * numerator) / 2;\n\t\tdo_div(buffers_since_start, HZ * numerator);\n\n\t\t/*\n\t\t * After more than 0xf0000000 (rounded down to a multiple of\n\t\t * 'jiffies-per-day' to ease jiffies_to_msecs calculation)\n\t\t * jiffies have passed since we started streaming reset the\n\t\t * counters and keep track of the sequence offset.\n\t\t */\n\t\tif (jiffies_since_start > JIFFIES_RESYNC) {\n\t\t\tdev->jiffies_vid_cap = cur_jiffies;\n\t\t\tdev->cap_seq_offset = buffers_since_start;\n\t\t\tbuffers_since_start = 0;\n\t\t}\n\t\tdropped_bufs = buffers_since_start + dev->cap_seq_offset - dev->cap_seq_count;\n\t\tdev->cap_seq_count = buffers_since_start + dev->cap_seq_offset;\n\t\tdev->vid_cap_seq_count = dev->cap_seq_count - dev->vid_cap_seq_start;\n\t\tdev->vbi_cap_seq_count = dev->cap_seq_count - dev->vbi_cap_seq_start;\n\t\tdev->meta_cap_seq_count = dev->cap_seq_count - dev->meta_cap_seq_start;\n\n\t\tvivid_thread_vid_cap_tick(dev, dropped_bufs);\n\n\t\t/*\n\t\t * Calculate the number of 'numerators' streamed since we started,\n\t\t * including the current buffer.\n\t\t */\n\t\tnumerators_since_start = ++buffers_since_start * numerator;\n\n\t\t/* And the number of jiffies since we started */\n\t\tjiffies_since_start = jiffies - dev->jiffies_vid_cap;\n\n\t\tmutex_unlock(&dev->mutex);\n\n\t\t/*\n\t\t * Calculate when that next buffer is supposed to start\n\t\t * in jiffies since we started streaming.\n\t\t */\n\t\tnext_jiffies_since_start = numerators_since_start * HZ +\n\t\t\t\t\t   denominator / 2;\n\t\tdo_div(next_jiffies_since_start, denominator);\n\t\t/* If it is in the past, then just schedule asap */\n\t\tif (next_jiffies_since_start < jiffies_since_start)\n\t\t\tnext_jiffies_since_start = jiffies_since_start;\n\n\t\twait_jiffies = next_jiffies_since_start - jiffies_since_start;\n\t\tschedule_timeout_interruptible(wait_jiffies ? wait_jiffies : 1);\n\t}\n\tdprintk(dev, 1, \"Video Capture Thread End\\n\");\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2201,
            "cve_id": "CVE-2019-18683",
            "code_snippet": "static int vivid_thread_vid_out(void *data)\n{\n\tstruct vivid_dev *dev = data;\n\tu64 numerators_since_start;\n\tu64 buffers_since_start;\n\tu64 next_jiffies_since_start;\n\tunsigned long jiffies_since_start;\n\tunsigned long cur_jiffies;\n\tunsigned wait_jiffies;\n\tunsigned numerator;\n\tunsigned denominator;\n\n\tdprintk(dev, 1, \"Video Output Thread Start\\n\");\n\n\tset_freezable();\n\n\t/* Resets frame counters */\n\tdev->out_seq_offset = 0;\n\tif (dev->seq_wrap)\n\t\tdev->out_seq_count = 0xffffff80U;\n\tdev->jiffies_vid_out = jiffies;\n\tdev->vid_out_seq_start = dev->vbi_out_seq_start = 0;\n\tdev->meta_out_seq_start = 0;\n\tdev->out_seq_resync = false;\n\n\tfor (;;) {\n\t\ttry_to_freeze();\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tif (!mutex_trylock(&dev->mutex)) {\n\t\t\tschedule_timeout_uninterruptible(1);\n\t\t\tcontinue;\n\t\t}\n\n\t\tcur_jiffies = jiffies;\n\t\tif (dev->out_seq_resync) {\n\t\t\tdev->jiffies_vid_out = cur_jiffies;\n\t\t\tdev->out_seq_offset = dev->out_seq_count + 1;\n\t\t\tdev->out_seq_count = 0;\n\t\t\tdev->out_seq_resync = false;\n\t\t}\n\t\tnumerator = dev->timeperframe_vid_out.numerator;\n\t\tdenominator = dev->timeperframe_vid_out.denominator;\n\n\t\tif (dev->field_out == V4L2_FIELD_ALTERNATE)\n\t\t\tdenominator *= 2;\n\n\t\t/* Calculate the number of jiffies since we started streaming */\n\t\tjiffies_since_start = cur_jiffies - dev->jiffies_vid_out;\n\t\t/* Get the number of buffers streamed since the start */\n\t\tbuffers_since_start = (u64)jiffies_since_start * denominator +\n\t\t\t\t      (HZ * numerator) / 2;\n\t\tdo_div(buffers_since_start, HZ * numerator);\n\n\t\t/*\n\t\t * After more than 0xf0000000 (rounded down to a multiple of\n\t\t * 'jiffies-per-day' to ease jiffies_to_msecs calculation)\n\t\t * jiffies have passed since we started streaming reset the\n\t\t * counters and keep track of the sequence offset.\n\t\t */\n\t\tif (jiffies_since_start > JIFFIES_RESYNC) {\n\t\t\tdev->jiffies_vid_out = cur_jiffies;\n\t\t\tdev->out_seq_offset = buffers_since_start;\n\t\t\tbuffers_since_start = 0;\n\t\t}\n\t\tdev->out_seq_count = buffers_since_start + dev->out_seq_offset;\n\t\tdev->vid_out_seq_count = dev->out_seq_count - dev->vid_out_seq_start;\n\t\tdev->vbi_out_seq_count = dev->out_seq_count - dev->vbi_out_seq_start;\n\t\tdev->meta_out_seq_count = dev->out_seq_count - dev->meta_out_seq_start;\n\n\t\tvivid_thread_vid_out_tick(dev);\n\t\tmutex_unlock(&dev->mutex);\n\n\t\t/*\n\t\t * Calculate the number of 'numerators' streamed since we started,\n\t\t * not including the current buffer.\n\t\t */\n\t\tnumerators_since_start = buffers_since_start * numerator;\n\n\t\t/* And the number of jiffies since we started */\n\t\tjiffies_since_start = jiffies - dev->jiffies_vid_out;\n\n\t\t/* Increase by the 'numerator' of one buffer */\n\t\tnumerators_since_start += numerator;\n\t\t/*\n\t\t * Calculate when that next buffer is supposed to start\n\t\t * in jiffies since we started streaming.\n\t\t */\n\t\tnext_jiffies_since_start = numerators_since_start * HZ +\n\t\t\t\t\t   denominator / 2;\n\t\tdo_div(next_jiffies_since_start, denominator);\n\t\t/* If it is in the past, then just schedule asap */\n\t\tif (next_jiffies_since_start < jiffies_since_start)\n\t\t\tnext_jiffies_since_start = jiffies_since_start;\n\n\t\twait_jiffies = next_jiffies_since_start - jiffies_since_start;\n\t\tschedule_timeout_interruptible(wait_jiffies ? wait_jiffies : 1);\n\t}\n\tdprintk(dev, 1, \"Video Output Thread End\\n\");\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2328,
            "cve_id": "CVE-2019-19537",
            "code_snippet": "int usb_register_dev(struct usb_interface *intf,\n\t\t     struct usb_class_driver *class_driver)\n{\n\tint retval;\n\tint minor_base = class_driver->minor_base;\n\tint minor;\n\tchar name[20];\n\n#ifdef CONFIG_USB_DYNAMIC_MINORS\n\t/*\n\t * We don't care what the device tries to start at, we want to start\n\t * at zero to pack the devices into the smallest available space with\n\t * no holes in the minor range.\n\t */\n\tminor_base = 0;\n#endif\n\n\tif (class_driver->fops == NULL)\n\t\treturn -EINVAL;\n\tif (intf->minor >= 0)\n\t\treturn -EADDRINUSE;\n\n\tmutex_lock(&init_usb_class_mutex);\n\tretval = init_usb_class();\n\tmutex_unlock(&init_usb_class_mutex);\n\n\tif (retval)\n\t\treturn retval;\n\n\tdev_dbg(&intf->dev, \"looking for a minor, starting at %d\\n\", minor_base);\n\n\tdown_write(&minor_rwsem);\n\tfor (minor = minor_base; minor < MAX_USB_MINORS; ++minor) {\n\t\tif (usb_minors[minor])\n\t\t\tcontinue;\n\n\t\tusb_minors[minor] = class_driver->fops;\n\t\tintf->minor = minor;\n\t\tbreak;\n\t}\n\tif (intf->minor < 0) {\n\t\tup_write(&minor_rwsem);\n\t\treturn -EXFULL;\n\t}\n\n\t/* create a usb class device for this usb interface */\n\tsnprintf(name, sizeof(name), class_driver->name, minor - minor_base);\n\tintf->usb_dev = device_create(usb_class->class, &intf->dev,\n\t\t\t\t      MKDEV(USB_MAJOR, minor), class_driver,\n\t\t\t\t      \"%s\", kbasename(name));\n\tif (IS_ERR(intf->usb_dev)) {\n\t\tusb_minors[minor] = NULL;\n\t\tintf->minor = -1;\n\t\tretval = PTR_ERR(intf->usb_dev);\n\t}\n\tup_write(&minor_rwsem);\n\treturn retval;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2403,
            "cve_id": "CVE-2019-2213",
            "code_snippet": "static void\nbinder_free_buf(struct binder_proc *proc, struct binder_buffer *buffer)\n{\n\tbinder_inner_proc_lock(proc);\n\tif (buffer->transaction) {\n\t\tbuffer->transaction->buffer = NULL;\n\t\tbuffer->transaction = NULL;\n\t}\n\tbinder_inner_proc_unlock(proc);\n\tif (buffer->async_transaction && buffer->target_node) {\n\t\tstruct binder_node *buf_node;\n\t\tstruct binder_work *w;\n\n\t\tbuf_node = buffer->target_node;\n\t\tbinder_node_inner_lock(buf_node);\n\t\tBUG_ON(!buf_node->has_async_transaction);\n\t\tBUG_ON(buf_node->proc != proc);\n\t\tw = binder_dequeue_work_head_ilocked(\n\t\t\t\t&buf_node->async_todo);\n\t\tif (!w) {\n\t\t\tbuf_node->has_async_transaction = false;\n\t\t} else {\n\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\tw, &proc->todo);\n\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t}\n\t\tbinder_node_inner_unlock(buf_node);\n\t}\n\ttrace_binder_transaction_buffer_release(buffer);\n\tbinder_transaction_buffer_release(proc, buffer, 0, false);\n\tbinder_alloc_free_buf(&proc->alloc, buffer);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2549,
            "cve_id": "CVE-2020-11884",
            "code_snippet": "static void __crst_table_upgrade(void *arg)\n{\n\tstruct mm_struct *mm = arg;\n\n\t/* we must change all active ASCEs to avoid the creation of new TLBs */\n\tif (current->active_mm == mm) {\n\t\tS390_lowcore.user_asce = mm->context.asce;\n\t\tif (current->thread.mm_segment == USER_DS) {\n\t\t\t__ctl_load(S390_lowcore.user_asce, 1, 1);\n\t\t\t/* Mark user-ASCE present in CR1 */\n\t\t\tclear_cpu_flag(CIF_ASCE_PRIMARY);\n\t\t}\n\t\tif (current->thread.mm_segment == USER_DS_SACF) {\n\t\t\t__ctl_load(S390_lowcore.user_asce, 7, 7);\n\t\t\t/* enable_sacf_uaccess does all or nothing */\n\t\t\tWARN_ON(!test_cpu_flag(CIF_ASCE_SECONDARY));\n\t\t}\n\t}\n\t__tlb_flush_local();\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2551,
            "cve_id": "CVE-2020-12114",
            "code_snippet": "static void drop_mountpoint(struct fs_pin *p)\n{\n\tstruct mount *m = container_of(p, struct mount, mnt_umount);\n\tpin_remove(p);\n\tmntput(&m->mnt);\n}",
            "detect_result": "NO\n\nExplanation: The provided code snippet appears to be part of a function responsible for dropping a mount point in a file system context. It retrieves a `struct mount` pointer from a `struct fs_pin` pointer using `container_of`, removes a pin on the file system object with `pin_remove`, and then decrements the reference count of the mount structure with `mntput`.\n\nBased on the given code alone, there are no obvious vulnerabilities such as buffer overflows, use-after-free, or improper memory management that can be identified. However, it's important to note that the safety of this function would depend on the correct usage and implementation details of the functions `container_of`, `pin_remove`, and `mntput`, as well as the overall context in which `drop_mountpoint` is called. Without additional information about these functions and their implementations, we cannot conclusively state that there are vulnerabilities, but based on the snippet provided, the answer is NO.",
            "final_result": 0
        },
        {
            "id": 2552,
            "cve_id": "CVE-2020-12114",
            "code_snippet": "static struct mountpoint *get_mountpoint(struct dentry *dentry)\n{\n\tstruct mountpoint *mp, *new = NULL;\n\tint ret;\n\n\tif (d_mountpoint(dentry)) {\n\t\t/* might be worth a WARN_ON() */\n\t\tif (d_unlinked(dentry))\n\t\t\treturn ERR_PTR(-ENOENT);\nmountpoint:\n\t\tread_seqlock_excl(&mount_lock);\n\t\tmp = lookup_mountpoint(dentry);\n\t\tread_sequnlock_excl(&mount_lock);\n\t\tif (mp)\n\t\t\tgoto done;\n\t}\n\n\tif (!new)\n\t\tnew = kmalloc(sizeof(struct mountpoint), GFP_KERNEL);\n\tif (!new)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\n\t/* Exactly one processes may set d_mounted */\n\tret = d_set_mounted(dentry);\n\n\t/* Someone else set d_mounted? */\n\tif (ret == -EBUSY)\n\t\tgoto mountpoint;\n\n\t/* The dentry is not available as a mountpoint? */\n\tmp = ERR_PTR(ret);\n\tif (ret)\n\t\tgoto done;\n\n\t/* Add the new mountpoint to the hash table */\n\tread_seqlock_excl(&mount_lock);\n\tnew->m_dentry = dget(dentry);\n\tnew->m_count = 1;\n\thlist_add_head(&new->m_hash, mp_hash(dentry));\n\tINIT_HLIST_HEAD(&new->m_list);\n\tread_sequnlock_excl(&mount_lock);\n\n\tmp = new;\n\tnew = NULL;\ndone:\n\tkfree(new);\n\treturn mp;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2553,
            "cve_id": "CVE-2020-12114",
            "code_snippet": "void mnt_set_mountpoint(struct mount *mnt,\n\t\t\tstruct mountpoint *mp,\n\t\t\tstruct mount *child_mnt)\n{\n\tmp->m_count++;\n\tmnt_add_count(mnt, 1);\t/* essentially, that's mntget */\n\tchild_mnt->mnt_mountpoint = mp->m_dentry;\n\tchild_mnt->mnt_parent = mnt;\n\tchild_mnt->mnt_mp = mp;\n\thlist_add_head(&child_mnt->mnt_mp_list, &mp->m_list);\n}",
            "detect_result": "NO\n\nExplanation: The provided code snippet appears to be part of a Linux kernel function responsible for setting up a mount point relationship between file system mounts. It increments reference counts, sets pointers, and adds entries to a hash list. However, without additional context about how this function is called and used within the broader codebase, it's challenging to identify specific vulnerabilities. The code itself does not show obvious issues such as buffer overflows, improper memory management, or insecure data handling. Therefore, based on the given snippet alone, there is no clear indication of a vulnerability.",
            "final_result": 0
        },
        {
            "id": 2554,
            "cve_id": "CVE-2020-12114",
            "code_snippet": "static void mntput_no_expire(struct mount *mnt)\n{\n\tLIST_HEAD(list);\n\n\trcu_read_lock();\n\tif (likely(READ_ONCE(mnt->mnt_ns))) {\n\t\t/*\n\t\t * Since we don't do lock_mount_hash() here,\n\t\t * ->mnt_ns can change under us.  However, if it's\n\t\t * non-NULL, then there's a reference that won't\n\t\t * be dropped until after an RCU delay done after\n\t\t * turning ->mnt_ns NULL.  So if we observe it\n\t\t * non-NULL under rcu_read_lock(), the reference\n\t\t * we are dropping is not the final one.\n\t\t */\n\t\tmnt_add_count(mnt, -1);\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\tlock_mount_hash();\n\t/*\n\t * make sure that if __legitimize_mnt() has not seen us grab\n\t * mount_lock, we'll see their refcount increment here.\n\t */\n\tsmp_mb();\n\tmnt_add_count(mnt, -1);\n\tif (mnt_get_count(mnt)) {\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\treturn;\n\t}\n\tif (unlikely(mnt->mnt.mnt_flags & MNT_DOOMED)) {\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\treturn;\n\t}\n\tmnt->mnt.mnt_flags |= MNT_DOOMED;\n\trcu_read_unlock();\n\n\tlist_del(&mnt->mnt_instance);\n\n\tif (unlikely(!list_empty(&mnt->mnt_mounts))) {\n\t\tstruct mount *p, *tmp;\n\t\tlist_for_each_entry_safe(p, tmp, &mnt->mnt_mounts,  mnt_child) {\n\t\t\t__put_mountpoint(unhash_mnt(p), &list);\n\t\t}\n\t}\n\tunlock_mount_hash();\n\tshrink_dentry_list(&list);\n\n\tif (likely(!(mnt->mnt.mnt_flags & MNT_INTERNAL))) {\n\t\tstruct task_struct *task = current;\n\t\tif (likely(!(task->flags & PF_KTHREAD))) {\n\t\t\tinit_task_work(&mnt->mnt_rcu, __cleanup_mnt);\n\t\t\tif (!task_work_add(task, &mnt->mnt_rcu, true))\n\t\t\t\treturn;\n\t\t}\n\t\tif (llist_add(&mnt->mnt_llist, &delayed_mntput_list))\n\t\t\tschedule_delayed_work(&delayed_mntput_work, 1);\n\t\treturn;\n\t}\n\tcleanup_mnt(mnt);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2585,
            "cve_id": "CVE-2020-12652",
            "code_snippet": "static long\n__mptctl_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tmpt_ioctl_header __user *uhdr = (void __user *) arg;\n\tmpt_ioctl_header\t khdr;\n\tint iocnum;\n\tunsigned iocnumX;\n\tint nonblock = (file->f_flags & O_NONBLOCK);\n\tint ret;\n\tMPT_ADAPTER *iocp = NULL;\n\n\tif (copy_from_user(&khdr, uhdr, sizeof(khdr))) {\n\t\tprintk(KERN_ERR MYNAM \"%s::mptctl_ioctl() @%d - \"\n\t\t\t\t\"Unable to copy mpt_ioctl_header data @ %p\\n\",\n\t\t\t\t__FILE__, __LINE__, uhdr);\n\t\treturn -EFAULT;\n\t}\n\tret = -ENXIO;\t\t\t\t/* (-6) No such device or address */\n\n\t/* Verify intended MPT adapter - set iocnum and the adapter\n\t * pointer (iocp)\n\t */\n\tiocnumX = khdr.iocnum & 0xFF;\n\tif (((iocnum = mpt_verify_adapter(iocnumX, &iocp)) < 0) ||\n\t    (iocp == NULL))\n\t\treturn -ENODEV;\n\n\tif (!iocp->active) {\n\t\tprintk(KERN_DEBUG MYNAM \"%s::mptctl_ioctl() @%d - Controller disabled.\\n\",\n\t\t\t\t__FILE__, __LINE__);\n\t\treturn -EFAULT;\n\t}\n\n\t/* Handle those commands that are just returning\n\t * information stored in the driver.\n\t * These commands should never time out and are unaffected\n\t * by TM and FW reloads.\n\t */\n\tif ((cmd & ~IOCSIZE_MASK) == (MPTIOCINFO & ~IOCSIZE_MASK)) {\n\t\treturn mptctl_getiocinfo(iocp, arg, _IOC_SIZE(cmd));\n\t} else if (cmd == MPTTARGETINFO) {\n\t\treturn mptctl_gettargetinfo(iocp, arg);\n\t} else if (cmd == MPTTEST) {\n\t\treturn mptctl_readtest(iocp, arg);\n\t} else if (cmd == MPTEVENTQUERY) {\n\t\treturn mptctl_eventquery(iocp, arg);\n\t} else if (cmd == MPTEVENTENABLE) {\n\t\treturn mptctl_eventenable(iocp, arg);\n\t} else if (cmd == MPTEVENTREPORT) {\n\t\treturn mptctl_eventreport(iocp, arg);\n\t} else if (cmd == MPTFWREPLACE) {\n\t\treturn mptctl_replace_fw(iocp, arg);\n\t}\n\n\t/* All of these commands require an interrupt or\n\t * are unknown/illegal.\n\t */\n\tif ((ret = mptctl_syscall_down(iocp, nonblock)) != 0)\n\t\treturn ret;\n\n\tif (cmd == MPTFWDOWNLOAD)\n\t\tret = mptctl_fw_download(iocp, arg);\n\telse if (cmd == MPTCOMMAND)\n\t\tret = mptctl_mpt_command(iocp, arg);\n\telse if (cmd == MPTHARDRESET)\n\t\tret = mptctl_do_reset(iocp, arg);\n\telse if ((cmd & ~IOCSIZE_MASK) == (HP_GETHOSTINFO & ~IOCSIZE_MASK))\n\t\tret = mptctl_hp_hostinfo(iocp, arg, _IOC_SIZE(cmd));\n\telse if (cmd == HP_GETTARGETINFO)\n\t\tret = mptctl_hp_targetinfo(iocp, arg);\n\telse\n\t\tret = -EINVAL;\n\n\tmutex_unlock(&iocp->ioctl_cmds.mutex);\n\n\treturn ret;\n}",
            "detect_result": "YES",
            "final_result": 1
        },
        {
            "id": 2653,
            "cve_id": "CVE-2020-14416",
            "code_snippet": "static void slcan_write_wakeup(struct tty_struct *tty)\n{\n\tstruct slcan *sl;\n\n\trcu_read_lock();\n\tsl = rcu_dereference(tty->disc_data);\n\tif (!sl)\n\t\tgoto out;\n\n\tschedule_work(&sl->tx_work);\nout:\n\trcu_read_unlock();\n}",
            "detect_result": "NO\n\nExplanation: The provided code snippet appears to be part of a Linux kernel module, specifically related to the SLCAN (Serial Line CAN) driver. The function `slcan_write_wakeup` is responsible for waking up the transmission work queue when there is data to be sent over the CAN bus.\n\nHere's a breakdown of the code:\n\n1. `rcu_read_lock();` - This acquires a read lock using RCU (Read-Copy-Update), which is a synchronization mechanism used in the Linux kernel to safely read shared data structures that are also modified by other threads.\n\n2. `sl = rcu_dereference(tty->disc_data);` - This line safely dereferences the `disc_data` pointer from the `tty_struct` structure using `rcu_dereference`. This ensures that the pointer is read in a way that is safe with respect to concurrent modifications.\n\n3. `if (!sl) goto out;` - If the `sl` pointer is NULL, the function skips to the `out` label, effectively doing nothing and releasing the RCU read lock.\n\n4. `schedule_work(&sl->tx_work);` - If `sl` is not NULL, this schedules the `tx_work` work item for execution. This work item is likely responsible for handling the actual transmission of data over the CAN bus.\n\n5. `out:` - This is a label used to mark the end of the function where the RCU read lock is released.\n\n6. `rcu_read_unlock();` - This releases the RCU read lock that was acquired at the beginning of the function.\n\nThe code does not appear to have any obvious vulnerabilities such as buffer overflows, use-after-free, or race conditions that could be exploited. The use of RCU for reading the `disc_data` pointer ensures that the read operation is safe even if the pointer is being modified concurrently by another thread. Therefore, based on the provided code snippet, it does not seem to be vulnerable.",
            "final_result": 0
        },
        {
            "id": 2699,
            "cve_id": "CVE-2020-25285",
            "code_snippet": "static int hugetlb_sysctl_handler_common(bool obey_mempolicy,\n\t\t\t struct ctl_table *table, int write,\n\t\t\t void *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp = h->max_huge_pages;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\tret = proc_hugetlb_doulongvec_minmax(table, write, buffer, length, ppos,\n\t\t\t\t\t     &tmp);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write)\n\t\tret = __nr_hugepages_store_common(obey_mempolicy, h,\n\t\t\t\t\t\t  NUMA_NO_NODE, tmp, *length);\nout:\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2724,
            "cve_id": "CVE-2020-27067",
            "code_snippet": "static int l2tp_eth_create(struct net *net, struct l2tp_tunnel *tunnel,\n\t\t\t   u32 session_id, u32 peer_session_id,\n\t\t\t   struct l2tp_session_cfg *cfg)\n{\n\tunsigned char name_assign_type;\n\tstruct net_device *dev;\n\tchar name[IFNAMSIZ];\n\tstruct l2tp_session *session;\n\tstruct l2tp_eth *priv;\n\tstruct l2tp_eth_sess *spriv;\n\tint rc;\n\n\tif (cfg->ifname) {\n\t\tstrlcpy(name, cfg->ifname, IFNAMSIZ);\n\t\tname_assign_type = NET_NAME_USER;\n\t} else {\n\t\tstrcpy(name, L2TP_ETH_DEV_NAME);\n\t\tname_assign_type = NET_NAME_ENUM;\n\t}\n\n\tsession = l2tp_session_create(sizeof(*spriv), tunnel, session_id,\n\t\t\t\t      peer_session_id, cfg);\n\tif (IS_ERR(session)) {\n\t\trc = PTR_ERR(session);\n\t\tgoto out;\n\t}\n\n\tdev = alloc_netdev(sizeof(*priv), name, name_assign_type,\n\t\t\t   l2tp_eth_dev_setup);\n\tif (!dev) {\n\t\trc = -ENOMEM;\n\t\tgoto out_del_session;\n\t}\n\n\tdev_net_set(dev, net);\n\tdev->min_mtu = 0;\n\tdev->max_mtu = ETH_MAX_MTU;\n\tl2tp_eth_adjust_mtu(tunnel, session, dev);\n\n\tpriv = netdev_priv(dev);\n\tpriv->dev = dev;\n\tpriv->session = session;\n\n\tpriv->tunnel_sock = tunnel->sock;\n\tsession->recv_skb = l2tp_eth_dev_recv;\n\tsession->session_close = l2tp_eth_delete;\n#if IS_ENABLED(CONFIG_L2TP_DEBUGFS)\n\tsession->show = l2tp_eth_show;\n#endif\n\n\tspriv = l2tp_session_priv(session);\n\tspriv->dev = dev;\n\n\trc = register_netdev(dev);\n\tif (rc < 0)\n\t\tgoto out_del_dev;\n\n\t__module_get(THIS_MODULE);\n\t/* Must be done after register_netdev() */\n\tstrlcpy(session->ifname, dev->name, IFNAMSIZ);\n\n\tdev_hold(dev);\n\n\treturn 0;\n\nout_del_dev:\n\tfree_netdev(dev);\n\tspriv->dev = NULL;\nout_del_session:\n\tl2tp_session_delete(session);\nout:\n\treturn rc;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2734,
            "cve_id": "CVE-2020-27675",
            "code_snippet": "static void clear_evtchn_to_irq_row(unsigned row)\n{\n\tunsigned col;\n\n\tfor (col = 0; col < EVTCHN_PER_ROW; col++)\n\t\tWRITE_ONCE(evtchn_to_irq[row][col], -1);\n}",
            "detect_result": "NO\n\nExplanation: The provided code snippet defines a function `clear_evtchn_to_irq_row` that takes an unsigned integer `row` as an argument. It iterates over a predefined number of columns (`EVTCHN_PER_ROW`) and sets each element in the `evtchn_to_irq` array at the specified row to `-1`. This operation does not involve any input from external sources or user input, and it does not perform any operations that could lead to vulnerabilities such as buffer overflows, format string vulnerabilities, or improper memory access. Therefore, based on the given code snippet alone, there are no apparent vulnerabilities. However, it's important to note that vulnerabilities can sometimes arise from how functions are used or from interactions with other parts of the codebase.",
            "final_result": 0
        },
        {
            "id": 2735,
            "cve_id": "CVE-2020-27675",
            "code_snippet": "static void xen_free_irq(unsigned irq)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\tunsigned long flags;\n\n\tif (WARN_ON(!info))\n\t\treturn;\n\n\twrite_lock_irqsave(&evtchn_rwlock, flags);\n\n\tlist_del(&info->list);\n\n\tset_info_for_irq(irq, NULL);\n\n\tWARN_ON(info->refcnt > 0);\n\n\twrite_unlock_irqrestore(&evtchn_rwlock, flags);\n\n\tkfree(info);\n\n\t/* Legacy IRQ descriptors are managed by the arch. */\n\tif (irq < nr_legacy_irqs())\n\t\treturn;\n\n\tirq_free_desc(irq);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2750,
            "cve_id": "CVE-2020-27825",
            "code_snippet": "void ring_buffer_reset_online_cpus(struct trace_buffer *buffer)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tint cpu;\n\n\t/* prevent another thread from changing buffer sizes */\n\tmutex_lock(&buffer->mutex);\n\n\tfor_each_online_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\tatomic_inc(&cpu_buffer->resize_disabled);\n\t\tatomic_inc(&cpu_buffer->record_disabled);\n\t}\n\n\t/* Make sure all commits have finished */\n\tsynchronize_rcu();\n\n\tfor_each_online_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\treset_disabled_cpu_buffer(cpu_buffer);\n\n\t\tatomic_dec(&cpu_buffer->record_disabled);\n\t\tatomic_dec(&cpu_buffer->resize_disabled);\n\t}\n\n\tmutex_unlock(&buffer->mutex);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2776,
            "cve_id": "CVE-2020-29369",
            "code_snippet": "int __do_munmap(struct mm_struct *mm, unsigned long start, size_t len,\n\t\tstruct list_head *uf, bool downgrade)\n{\n\tunsigned long end;\n\tstruct vm_area_struct *vma, *prev, *last;\n\n\tif ((offset_in_page(start)) || start > TASK_SIZE || len > TASK_SIZE-start)\n\t\treturn -EINVAL;\n\n\tlen = PAGE_ALIGN(len);\n\tend = start + len;\n\tif (len == 0)\n\t\treturn -EINVAL;\n\n\t/*\n\t * arch_unmap() might do unmaps itself.  It must be called\n\t * and finish any rbtree manipulation before this code\n\t * runs and also starts to manipulate the rbtree.\n\t */\n\tarch_unmap(mm, start, end);\n\n\t/* Find the first overlapping VMA */\n\tvma = find_vma(mm, start);\n\tif (!vma)\n\t\treturn 0;\n\tprev = vma->vm_prev;\n\t/* we have  start < vma->vm_end  */\n\n\t/* if it doesn't overlap, we have nothing.. */\n\tif (vma->vm_start >= end)\n\t\treturn 0;\n\n\t/*\n\t * If we need to split any vma, do it now to save pain later.\n\t *\n\t * Note: mremap's move_vma VM_ACCOUNT handling assumes a partially\n\t * unmapped vm_area_struct will remain in use: so lower split_vma\n\t * places tmp vma above, and higher split_vma places tmp vma below.\n\t */\n\tif (start > vma->vm_start) {\n\t\tint error;\n\n\t\t/*\n\t\t * Make sure that map_count on return from munmap() will\n\t\t * not exceed its limit; but let map_count go just above\n\t\t * its limit temporarily, to help free resources as expected.\n\t\t */\n\t\tif (end < vma->vm_end && mm->map_count >= sysctl_max_map_count)\n\t\t\treturn -ENOMEM;\n\n\t\terror = __split_vma(mm, vma, start, 0);\n\t\tif (error)\n\t\t\treturn error;\n\t\tprev = vma;\n\t}\n\n\t/* Does it split the last one? */\n\tlast = find_vma(mm, end);\n\tif (last && end > last->vm_start) {\n\t\tint error = __split_vma(mm, last, end, 1);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\tvma = prev ? prev->vm_next : mm->mmap;\n\n\tif (unlikely(uf)) {\n\t\t/*\n\t\t * If userfaultfd_unmap_prep returns an error the vmas\n\t\t * will remain splitted, but userland will get a\n\t\t * highly unexpected error anyway. This is no\n\t\t * different than the case where the first of the two\n\t\t * __split_vma fails, but we don't undo the first\n\t\t * split, despite we could. This is unlikely enough\n\t\t * failure that it's not worth optimizing it for.\n\t\t */\n\t\tint error = userfaultfd_unmap_prep(vma, start, end, uf);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\t/*\n\t * unlock any mlock()ed ranges before detaching vmas\n\t */\n\tif (mm->locked_vm) {\n\t\tstruct vm_area_struct *tmp = vma;\n\t\twhile (tmp && tmp->vm_start < end) {\n\t\t\tif (tmp->vm_flags & VM_LOCKED) {\n\t\t\t\tmm->locked_vm -= vma_pages(tmp);\n\t\t\t\tmunlock_vma_pages_all(tmp);\n\t\t\t}\n\n\t\t\ttmp = tmp->vm_next;\n\t\t}\n\t}\n\n\t/* Detach vmas from rbtree */\n\tif (!detach_vmas_to_be_unmapped(mm, vma, prev, end))\n\t\tdowngrade = false;\n\n\tif (downgrade)\n\t\tmmap_write_downgrade(mm);\n\n\tunmap_region(mm, vma, prev, start, end);\n\n\t/* Fix up all other VM information */\n\tremove_vma_list(mm, vma);\n\n\treturn downgrade ? 1 : 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2783,
            "cve_id": "CVE-2020-29374",
            "code_snippet": "static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n\t\tunsigned long start, unsigned long nr_pages,\n\t\tunsigned int gup_flags, struct page **pages,\n\t\tstruct vm_area_struct **vmas, int *locked)\n{\n\tlong ret = 0, i = 0;\n\tstruct vm_area_struct *vma = NULL;\n\tstruct follow_page_context ctx = { NULL };\n\n\tif (!nr_pages)\n\t\treturn 0;\n\n\tstart = untagged_addr(start);\n\n\tVM_BUG_ON(!!pages != !!(gup_flags & (FOLL_GET | FOLL_PIN)));\n\n\t/*\n\t * If FOLL_FORCE is set then do not force a full fault as the hinting\n\t * fault information is unrelated to the reference behaviour of a task\n\t * using the address space\n\t */\n\tif (!(gup_flags & FOLL_FORCE))\n\t\tgup_flags |= FOLL_NUMA;\n\n\tdo {\n\t\tstruct page *page;\n\t\tunsigned int foll_flags = gup_flags;\n\t\tunsigned int page_increm;\n\n\t\t/* first iteration or cross vma bound */\n\t\tif (!vma || start >= vma->vm_end) {\n\t\t\tvma = find_extend_vma(mm, start);\n\t\t\tif (!vma && in_gate_area(mm, start)) {\n\t\t\t\tret = get_gate_page(mm, start & PAGE_MASK,\n\t\t\t\t\t\tgup_flags, &vma,\n\t\t\t\t\t\tpages ? &pages[i] : NULL);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto out;\n\t\t\t\tctx.page_mask = 0;\n\t\t\t\tgoto next_page;\n\t\t\t}\n\n\t\t\tif (!vma || check_vma_flags(vma, gup_flags)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (is_vm_hugetlb_page(vma)) {\n\t\t\t\tif (should_force_cow_break(vma, foll_flags))\n\t\t\t\t\tfoll_flags |= FOLL_WRITE;\n\t\t\t\ti = follow_hugetlb_page(mm, vma, pages, vmas,\n\t\t\t\t\t\t&start, &nr_pages, i,\n\t\t\t\t\t\tfoll_flags, locked);\n\t\t\t\tif (locked && *locked == 0) {\n\t\t\t\t\t/*\n\t\t\t\t\t * We've got a VM_FAULT_RETRY\n\t\t\t\t\t * and we've lost mmap_sem.\n\t\t\t\t\t * We must stop here.\n\t\t\t\t\t */\n\t\t\t\t\tBUG_ON(gup_flags & FOLL_NOWAIT);\n\t\t\t\t\tBUG_ON(ret != 0);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (should_force_cow_break(vma, foll_flags))\n\t\t\tfoll_flags |= FOLL_WRITE;\n\nretry:\n\t\t/*\n\t\t * If we have a pending SIGKILL, don't keep faulting pages and\n\t\t * potentially allocating memory.\n\t\t */\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tgoto out;\n\t\t}\n\t\tcond_resched();\n\n\t\tpage = follow_page_mask(vma, start, foll_flags, &ctx);\n\t\tif (!page) {\n\t\t\tret = faultin_page(tsk, vma, start, &foll_flags,\n\t\t\t\t\t   locked);\n\t\t\tswitch (ret) {\n\t\t\tcase 0:\n\t\t\t\tgoto retry;\n\t\t\tcase -EBUSY:\n\t\t\t\tret = 0;\n\t\t\t\tfallthrough;\n\t\t\tcase -EFAULT:\n\t\t\tcase -ENOMEM:\n\t\t\tcase -EHWPOISON:\n\t\t\t\tgoto out;\n\t\t\tcase -ENOENT:\n\t\t\t\tgoto next_page;\n\t\t\t}\n\t\t\tBUG();\n\t\t} else if (PTR_ERR(page) == -EEXIST) {\n\t\t\t/*\n\t\t\t * Proper page table entry exists, but no corresponding\n\t\t\t * struct page.\n\t\t\t */\n\t\t\tgoto next_page;\n\t\t} else if (IS_ERR(page)) {\n\t\t\tret = PTR_ERR(page);\n\t\t\tgoto out;\n\t\t}\n\t\tif (pages) {\n\t\t\tpages[i] = page;\n\t\t\tflush_anon_page(vma, page, start);\n\t\t\tflush_dcache_page(page);\n\t\t\tctx.page_mask = 0;\n\t\t}\nnext_page:\n\t\tif (vmas) {\n\t\t\tvmas[i] = vma;\n\t\t\tctx.page_mask = 0;\n\t\t}\n\t\tpage_increm = 1 + (~(start >> PAGE_SHIFT) & ctx.page_mask);\n\t\tif (page_increm > nr_pages)\n\t\t\tpage_increm = nr_pages;\n\t\ti += page_increm;\n\t\tstart += page_increm * PAGE_SIZE;\n\t\tnr_pages -= page_increm;\n\t} while (nr_pages);\nout:\n\tif (ctx.pgmap)\n\t\tput_dev_pagemap(ctx.pgmap);\n\treturn i ? i : ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2784,
            "cve_id": "CVE-2020-29374",
            "code_snippet": "static inline bool can_follow_write_pte(pte_t pte, unsigned int flags)\n{\n\treturn pte_write(pte) || ((flags & FOLL_COW) && pte_dirty(pte));\n}",
            "detect_result": "NO.\n\nExplanation: The provided function `can_follow_write_pte` appears to be part of a memory management system, likely in an operating system kernel. It checks if a page table entry (PTE) allows for writing under certain conditions. The function returns true if the PTE is writable (`pte_write(pte)`) or if the flags include `FOLL_COW` and the PTE is dirty (`pte_dirty(pte)`).\n\nThis specific function does not directly handle user input or perform operations that could lead to vulnerabilities such as buffer overflows, format string vulnerabilities, or improper memory access. It simply evaluates the state of a PTE and some flags, which are typically controlled by the kernel itself rather than untrusted user input. Therefore, based on the snippet provided, there is no obvious vulnerability in this function.",
            "final_result": 0
        },
        {
            "id": 2876,
            "cve_id": "CVE-2020-36557",
            "code_snippet": "int vc_allocate(unsigned int currcons)\t/* return 0 on success */\n{\n\tstruct vt_notifier_param param;\n\tstruct vc_data *vc;\n\n\tWARN_CONSOLE_UNLOCKED();\n\n\tif (currcons >= MAX_NR_CONSOLES)\n\t\treturn -ENXIO;\n\n\tif (vc_cons[currcons].d)\n\t\treturn 0;\n\n\t/* due to the granularity of kmalloc, we waste some memory here */\n\t/* the alloc is done in two steps, to optimize the common situation\n\t   of a 25x80 console (structsize=216, screenbuf_size=4000) */\n\t/* although the numbers above are not valid since long ago, the\n\t   point is still up-to-date and the comment still has its value\n\t   even if only as a historical artifact.  --mj, July 1998 */\n\tparam.vc = vc = kzalloc(sizeof(struct vc_data), GFP_KERNEL);\n\tif (!vc)\n\t\treturn -ENOMEM;\n\n\tvc_cons[currcons].d = vc;\n\ttty_port_init(&vc->port);\n\tvc->port.ops = &vc_port_ops;\n\tINIT_WORK(&vc_cons[currcons].SAK_work, vc_SAK);\n\n\tvisual_init(vc, currcons, 1);\n\n\tif (!*vc->vc_uni_pagedir_loc)\n\t\tcon_set_default_unimap(vc);\n\n\tvc->vc_screenbuf = kzalloc(vc->vc_screenbuf_size, GFP_KERNEL);\n\tif (!vc->vc_screenbuf)\n\t\tgoto err_free;\n\n\t/* If no drivers have overridden us and the user didn't pass a\n\t   boot option, default to displaying the cursor */\n\tif (global_cursor_default == -1)\n\t\tglobal_cursor_default = 1;\n\n\tvc_init(vc, vc->vc_rows, vc->vc_cols, 1);\n\tvcs_make_sysfs(currcons);\n\tatomic_notifier_call_chain(&vt_notifier_list, VT_ALLOCATE, &param);\n\n\treturn 0;\nerr_free:\n\tvisual_deinit(vc);\n\tkfree(vc);\n\tvc_cons[currcons].d = NULL;\n\treturn -ENOMEM;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2946,
            "cve_id": "CVE-2021-0920",
            "code_snippet": "static int unix_dgram_recvmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t      size_t size, int flags)\n{\n\tstruct scm_cookie scm;\n\tstruct sock *sk = sock->sk;\n\tstruct unix_sock *u = unix_sk(sk);\n\tstruct sk_buff *skb, *last;\n\tlong timeo;\n\tint skip;\n\tint err;\n\n\terr = -EOPNOTSUPP;\n\tif (flags&MSG_OOB)\n\t\tgoto out;\n\n\ttimeo = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\n\tdo {\n\t\tmutex_lock(&u->iolock);\n\n\t\tskip = sk_peek_offset(sk, flags);\n\t\tskb = __skb_try_recv_datagram(sk, &sk->sk_receive_queue, flags,\n\t\t\t\t\t      &skip, &err, &last);\n\t\tif (skb) {\n\t\t\tif (!(flags & MSG_PEEK))\n\t\t\t\tscm_stat_del(sk, skb);\n\t\t\tbreak;\n\t\t}\n\n\t\tmutex_unlock(&u->iolock);\n\n\t\tif (err != -EAGAIN)\n\t\t\tbreak;\n\t} while (timeo &&\n\t\t !__skb_wait_for_more_packets(sk, &sk->sk_receive_queue,\n\t\t\t\t\t      &err, &timeo, last));\n\n\tif (!skb) { /* implies iolock unlocked */\n\t\tunix_state_lock(sk);\n\t\t/* Signal EOF on disconnected non-blocking SEQPACKET socket. */\n\t\tif (sk->sk_type == SOCK_SEQPACKET && err == -EAGAIN &&\n\t\t    (sk->sk_shutdown & RCV_SHUTDOWN))\n\t\t\terr = 0;\n\t\tunix_state_unlock(sk);\n\t\tgoto out;\n\t}\n\n\tif (wq_has_sleeper(&u->peer_wait))\n\t\twake_up_interruptible_sync_poll(&u->peer_wait,\n\t\t\t\t\t\tEPOLLOUT | EPOLLWRNORM |\n\t\t\t\t\t\tEPOLLWRBAND);\n\n\tif (msg->msg_name)\n\t\tunix_copy_addr(msg, skb->sk);\n\n\tif (size > skb->len - skip)\n\t\tsize = skb->len - skip;\n\telse if (size < skb->len - skip)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\terr = skb_copy_datagram_msg(skb, skip, msg, size);\n\tif (err)\n\t\tgoto out_free;\n\n\tif (sock_flag(sk, SOCK_RCVTSTAMP))\n\t\t__sock_recv_timestamp(msg, sk, skb);\n\n\tmemset(&scm, 0, sizeof(scm));\n\n\tscm_set_cred(&scm, UNIXCB(skb).pid, UNIXCB(skb).uid, UNIXCB(skb).gid);\n\tunix_set_secdata(&scm, skb);\n\n\tif (!(flags & MSG_PEEK)) {\n\t\tif (UNIXCB(skb).fp)\n\t\t\tunix_detach_fds(&scm, skb);\n\n\t\tsk_peek_offset_bwd(sk, skb->len);\n\t} else {\n\t\t/* It is questionable: on PEEK we could:\n\t\t   - do not return fds - good, but too simple 8)\n\t\t   - return fds, and do not return them on read (old strategy,\n\t\t     apparently wrong)\n\t\t   - clone fds (I chose it for now, it is the most universal\n\t\t     solution)\n\n\t\t   POSIX 1003.1g does not actually define this clearly\n\t\t   at all. POSIX 1003.1g doesn't define a lot of things\n\t\t   clearly however!\n\n\t\t*/\n\n\t\tsk_peek_offset_fwd(sk, size);\n\n\t\tif (UNIXCB(skb).fp)\n\t\t\tunix_peek_fds(&scm, skb);\n\t}\n\terr = (flags & MSG_TRUNC) ? skb->len - skip : size;\n\n\tscm_recv(sock, msg, &scm, flags);\n\nout_free:\n\tskb_free_datagram(sk, skb);\n\tmutex_unlock(&u->iolock);\nout:\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2977,
            "cve_id": "CVE-2021-20261",
            "code_snippet": "static unsigned int floppy_check_events(struct gendisk *disk,\n\t\t\t\t\tunsigned int clearing)\n{\n\tint drive = (long)disk->private_data;\n\n\tif (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n\t    test_bit(FD_VERIFY_BIT, &UDRS->flags))\n\t\treturn DISK_EVENT_MEDIA_CHANGE;\n\n\tif (time_after(jiffies, UDRS->last_checked + UDP->checkfreq)) {\n\t\tif (lock_fdc(drive))\n\t\t\treturn -EINTR;\n\t\tpoll_drive(false, 0);\n\t\tprocess_fd_request();\n\t}\n\n\tif (test_bit(FD_DISK_CHANGED_BIT, &UDRS->flags) ||\n\t    test_bit(FD_VERIFY_BIT, &UDRS->flags) ||\n\t    test_bit(drive, &fake_change) ||\n\t    drive_no_geom(drive))\n\t\treturn DISK_EVENT_MEDIA_CHANGE;\n\treturn 0;\n}",
            "detect_result": "NO\n\nExplanation of the code behavior:\n- The function `floppy_check_events` checks for events related to a floppy disk drive.\n- It first retrieves the drive number from the `disk` structure's private data.\n- It then checks if either the `FD_DISK_CHANGED_BIT` or `FD_VERIFY_BIT` is set in the drive's flags. If so, it returns `DISK_EVENT_MEDIA_CHANGE`, indicating that the media has changed or needs verification.\n- If the current time (`jiffies`) is greater than the last checked time plus the check frequency (`UDP->checkfreq`), it locks the FDC (Floppy Disk Controller) for the drive. If locking fails, it returns `-EINTR`.\n- It then polls the drive and processes any pending requests.\n- After polling and processing, it again checks the drive's flags and a `fake_change` bit to see if the media has changed or if there is no geometry information for the drive. If any of these conditions are true, it returns `DISK_EVENT_MEDIA_CHANGE`.\n- If none of the above conditions are met, it returns `0`, indicating no event.\n\nVulnerability assessment:\n- The code does not appear to have obvious buffer overflows, format string vulnerabilities, or other common security issues.\n- The use of `lock_fdc` suggests that the code attempts to handle concurrency properly.\n- There are no direct inputs from untrusted sources that are not validated or sanitized.\n- The function seems to be designed to handle specific hardware events and does not perform complex operations that could lead to vulnerabilities.\n\nHowever, without more context about how this function is used and the definitions of the macros and structures involved, a thorough security analysis cannot be performed. But based on the provided code snippet alone, there are no apparent vulnerabilities.",
            "final_result": 0
        },
        {
            "id": 2978,
            "cve_id": "CVE-2021-20261",
            "code_snippet": "static int user_reset_fdc(int drive, int arg, bool interruptible)\n{\n\tint ret;\n\n\tif (lock_fdc(drive))\n\t\treturn -EINTR;\n\n\tif (arg == FD_RESET_ALWAYS)\n\t\tFDCS->reset = 1;\n\tif (FDCS->reset) {\n\t\tcont = &reset_cont;\n\t\tret = wait_til_done(reset_fdc, interruptible);\n\t\tif (ret == -EINTR)\n\t\t\treturn -EINTR;\n\t}\n\tprocess_fd_request();\n\treturn 0;\n}",
            "detect_result": "NO\n\nExplanation of the code behavior:\n- The function `user_reset_fdc` takes three parameters: `drive`, `arg`, and `interruptible`.\n- It first attempts to acquire a lock on the FDC (Floppy Disk Controller) for the specified `drive`. If the lock cannot be acquired and `interruptible` is true, it returns `-EINTR` indicating an interruption.\n- If `arg` is equal to `FD_RESET_ALWAYS`, it sets the `reset` flag in the `FDCS` structure to 1.\n- If the `reset` flag in `FDCS` is set, it prepares a continuation function `reset_cont` and calls `wait_til_done` with this function and the `interruptible` flag. If `wait_til_done` returns `-EINTR`, the function returns `-EINTR`.\n- Finally, it calls `process_fd_request` and returns 0.\n\nVulnerability assessment:\n- The code does not show any obvious buffer overflows, format string vulnerabilities, or other common security issues.\n- The use of a lock (`lock_fdc`) suggests that the code is attempting to prevent race conditions, which is a good practice.\n- The function checks for interruptions and handles them appropriately by returning `-EINTR`.\n\nHowever, without more context about the surrounding code, such as the implementation of `lock_fdc`, `wait_til_done`, and `process_fd_request`, it's difficult to conclusively state there are no vulnerabilities. But based on the provided code snippet alone, there are no apparent vulnerabilities.",
            "final_result": 0
        },
        {
            "id": 3003,
            "cve_id": "CVE-2021-23133",
            "code_snippet": "static int sctp_init_sock(struct sock *sk)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct sctp_sock *sp;\n\n\tpr_debug(\"%s: sk:%p\\n\", __func__, sk);\n\n\tsp = sctp_sk(sk);\n\n\t/* Initialize the SCTP per socket area.  */\n\tswitch (sk->sk_type) {\n\tcase SOCK_SEQPACKET:\n\t\tsp->type = SCTP_SOCKET_UDP;\n\t\tbreak;\n\tcase SOCK_STREAM:\n\t\tsp->type = SCTP_SOCKET_TCP;\n\t\tbreak;\n\tdefault:\n\t\treturn -ESOCKTNOSUPPORT;\n\t}\n\n\tsk->sk_gso_type = SKB_GSO_SCTP;\n\n\t/* Initialize default send parameters. These parameters can be\n\t * modified with the SCTP_DEFAULT_SEND_PARAM socket option.\n\t */\n\tsp->default_stream = 0;\n\tsp->default_ppid = 0;\n\tsp->default_flags = 0;\n\tsp->default_context = 0;\n\tsp->default_timetolive = 0;\n\n\tsp->default_rcv_context = 0;\n\tsp->max_burst = net->sctp.max_burst;\n\n\tsp->sctp_hmac_alg = net->sctp.sctp_hmac_alg;\n\n\t/* Initialize default setup parameters. These parameters\n\t * can be modified with the SCTP_INITMSG socket option or\n\t * overridden by the SCTP_INIT CMSG.\n\t */\n\tsp->initmsg.sinit_num_ostreams   = sctp_max_outstreams;\n\tsp->initmsg.sinit_max_instreams  = sctp_max_instreams;\n\tsp->initmsg.sinit_max_attempts   = net->sctp.max_retrans_init;\n\tsp->initmsg.sinit_max_init_timeo = net->sctp.rto_max;\n\n\t/* Initialize default RTO related parameters.  These parameters can\n\t * be modified for with the SCTP_RTOINFO socket option.\n\t */\n\tsp->rtoinfo.srto_initial = net->sctp.rto_initial;\n\tsp->rtoinfo.srto_max     = net->sctp.rto_max;\n\tsp->rtoinfo.srto_min     = net->sctp.rto_min;\n\n\t/* Initialize default association related parameters. These parameters\n\t * can be modified with the SCTP_ASSOCINFO socket option.\n\t */\n\tsp->assocparams.sasoc_asocmaxrxt = net->sctp.max_retrans_association;\n\tsp->assocparams.sasoc_number_peer_destinations = 0;\n\tsp->assocparams.sasoc_peer_rwnd = 0;\n\tsp->assocparams.sasoc_local_rwnd = 0;\n\tsp->assocparams.sasoc_cookie_life = net->sctp.valid_cookie_life;\n\n\t/* Initialize default event subscriptions. By default, all the\n\t * options are off.\n\t */\n\tsp->subscribe = 0;\n\n\t/* Default Peer Address Parameters.  These defaults can\n\t * be modified via SCTP_PEER_ADDR_PARAMS\n\t */\n\tsp->hbinterval  = net->sctp.hb_interval;\n\tsp->udp_port    = htons(net->sctp.udp_port);\n\tsp->encap_port  = htons(net->sctp.encap_port);\n\tsp->pathmaxrxt  = net->sctp.max_retrans_path;\n\tsp->pf_retrans  = net->sctp.pf_retrans;\n\tsp->ps_retrans  = net->sctp.ps_retrans;\n\tsp->pf_expose   = net->sctp.pf_expose;\n\tsp->pathmtu     = 0; /* allow default discovery */\n\tsp->sackdelay   = net->sctp.sack_timeout;\n\tsp->sackfreq\t= 2;\n\tsp->param_flags = SPP_HB_ENABLE |\n\t\t\t  SPP_PMTUD_ENABLE |\n\t\t\t  SPP_SACKDELAY_ENABLE;\n\tsp->default_ss = SCTP_SS_DEFAULT;\n\n\t/* If enabled no SCTP message fragmentation will be performed.\n\t * Configure through SCTP_DISABLE_FRAGMENTS socket option.\n\t */\n\tsp->disable_fragments = 0;\n\n\t/* Enable Nagle algorithm by default.  */\n\tsp->nodelay           = 0;\n\n\tsp->recvrcvinfo = 0;\n\tsp->recvnxtinfo = 0;\n\n\t/* Enable by default. */\n\tsp->v4mapped          = 1;\n\n\t/* Auto-close idle associations after the configured\n\t * number of seconds.  A value of 0 disables this\n\t * feature.  Configure through the SCTP_AUTOCLOSE socket option,\n\t * for UDP-style sockets only.\n\t */\n\tsp->autoclose         = 0;\n\n\t/* User specified fragmentation limit. */\n\tsp->user_frag         = 0;\n\n\tsp->adaptation_ind = 0;\n\n\tsp->pf = sctp_get_pf_specific(sk->sk_family);\n\n\t/* Control variables for partial data delivery. */\n\tatomic_set(&sp->pd_mode, 0);\n\tskb_queue_head_init(&sp->pd_lobby);\n\tsp->frag_interleave = 0;\n\n\t/* Create a per socket endpoint structure.  Even if we\n\t * change the data structure relationships, this may still\n\t * be useful for storing pre-connect address information.\n\t */\n\tsp->ep = sctp_endpoint_new(sk, GFP_KERNEL);\n\tif (!sp->ep)\n\t\treturn -ENOMEM;\n\n\tsp->hmac = NULL;\n\n\tsk->sk_destruct = sctp_destruct_sock;\n\n\tSCTP_DBG_OBJCNT_INC(sock);\n\n\tlocal_bh_disable();\n\tsk_sockets_allocated_inc(sk);\n\tsock_prot_inuse_add(net, sk->sk_prot, 1);\n\n\tif (net->sctp.default_auto_asconf) {\n\t\tspin_lock(&sock_net(sk)->sctp.addr_wq_lock);\n\t\tlist_add_tail(&sp->auto_asconf_list,\n\t\t    &net->sctp.auto_asconf_splist);\n\t\tsp->do_auto_asconf = 1;\n\t\tspin_unlock(&sock_net(sk)->sctp.addr_wq_lock);\n\t} else {\n\t\tsp->do_auto_asconf = 0;\n\t}\n\n\tlocal_bh_enable();\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3150,
            "cve_id": "CVE-2021-3609",
            "code_snippet": "static int bcm_delete_rx_op(struct list_head *ops, struct bcm_msg_head *mh,\n\t\t\t    int ifindex)\n{\n\tstruct bcm_op *op, *n;\n\n\tlist_for_each_entry_safe(op, n, ops, list) {\n\t\tif ((op->can_id == mh->can_id) && (op->ifindex == ifindex) &&\n\t\t    (op->flags & CAN_FD_FRAME) == (mh->flags & CAN_FD_FRAME)) {\n\n\t\t\t/*\n\t\t\t * Don't care if we're bound or not (due to netdev\n\t\t\t * problems) can_rx_unregister() is always a save\n\t\t\t * thing to do here.\n\t\t\t */\n\t\t\tif (op->ifindex) {\n\t\t\t\t/*\n\t\t\t\t * Only remove subscriptions that had not\n\t\t\t\t * been removed due to NETDEV_UNREGISTER\n\t\t\t\t * in bcm_notifier()\n\t\t\t\t */\n\t\t\t\tif (op->rx_reg_dev) {\n\t\t\t\t\tstruct net_device *dev;\n\n\t\t\t\t\tdev = dev_get_by_index(sock_net(op->sk),\n\t\t\t\t\t\t\t       op->ifindex);\n\t\t\t\t\tif (dev) {\n\t\t\t\t\t\tbcm_rx_unreg(dev, op);\n\t\t\t\t\t\tdev_put(dev);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else\n\t\t\t\tcan_rx_unregister(sock_net(op->sk), NULL,\n\t\t\t\t\t\t  op->can_id,\n\t\t\t\t\t\t  REGMASK(op->can_id),\n\t\t\t\t\t\t  bcm_rx_handler, op);\n\n\t\t\tlist_del(&op->list);\n\t\t\tsynchronize_rcu();\n\t\t\tbcm_remove_op(op);\n\t\t\treturn 1; /* done */\n\t\t}\n\t}\n\n\treturn 0; /* not found */\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3182,
            "cve_id": "CVE-2021-3752",
            "code_snippet": "static void l2cap_sock_destruct(struct sock *sk)\n{\n\tBT_DBG(\"sk %p\", sk);\n\n\tif (l2cap_pi(sk)->chan) {\n\t\tl2cap_pi(sk)->chan->data = NULL;\n\t\tl2cap_chan_put(l2cap_pi(sk)->chan);\n\t}\n\n\tif (l2cap_pi(sk)->rx_busy_skb) {\n\t\tkfree_skb(l2cap_pi(sk)->rx_busy_skb);\n\t\tl2cap_pi(sk)->rx_busy_skb = NULL;\n\t}\n\n\tskb_queue_purge(&sk->sk_receive_queue);\n\tskb_queue_purge(&sk->sk_write_queue);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3229,
            "cve_id": "CVE-2021-39686",
            "code_snippet": "static void binder_transaction(struct binder_proc *proc,\n\t\t\t       struct binder_thread *thread,\n\t\t\t       struct binder_transaction_data *tr, int reply,\n\t\t\t       binder_size_t extra_buffers_size)\n{\n\tint ret;\n\tstruct binder_transaction *t;\n\tstruct binder_work *w;\n\tstruct binder_work *tcomplete;\n\tbinder_size_t buffer_offset = 0;\n\tbinder_size_t off_start_offset, off_end_offset;\n\tbinder_size_t off_min;\n\tbinder_size_t sg_buf_offset, sg_buf_end_offset;\n\tstruct binder_proc *target_proc = NULL;\n\tstruct binder_thread *target_thread = NULL;\n\tstruct binder_node *target_node = NULL;\n\tstruct binder_transaction *in_reply_to = NULL;\n\tstruct binder_transaction_log_entry *e;\n\tuint32_t return_error = 0;\n\tuint32_t return_error_param = 0;\n\tuint32_t return_error_line = 0;\n\tbinder_size_t last_fixup_obj_off = 0;\n\tbinder_size_t last_fixup_min_off = 0;\n\tstruct binder_context *context = proc->context;\n\tint t_debug_id = atomic_inc_return(&binder_last_id);\n\tchar *secctx = NULL;\n\tu32 secctx_sz = 0;\n\n\te = binder_transaction_log_add(&binder_transaction_log);\n\te->debug_id = t_debug_id;\n\te->call_type = reply ? 2 : !!(tr->flags & TF_ONE_WAY);\n\te->from_proc = proc->pid;\n\te->from_thread = thread->pid;\n\te->target_handle = tr->target.handle;\n\te->data_size = tr->data_size;\n\te->offsets_size = tr->offsets_size;\n\tstrscpy(e->context_name, proc->context->name, BINDERFS_MAX_NAME);\n\n\tif (reply) {\n\t\tbinder_inner_proc_lock(proc);\n\t\tin_reply_to = thread->transaction_stack;\n\t\tif (in_reply_to == NULL) {\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with no transaction stack\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_empty_call_stack;\n\t\t}\n\t\tif (in_reply_to->to_thread != thread) {\n\t\t\tspin_lock(&in_reply_to->lock);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\tproc->pid, thread->pid, in_reply_to->debug_id,\n\t\t\t\tin_reply_to->to_proc ?\n\t\t\t\tin_reply_to->to_proc->pid : 0,\n\t\t\t\tin_reply_to->to_thread ?\n\t\t\t\tin_reply_to->to_thread->pid : 0);\n\t\t\tspin_unlock(&in_reply_to->lock);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\tgoto err_bad_call_stack;\n\t\t}\n\t\tthread->transaction_stack = in_reply_to->to_parent;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_set_nice(in_reply_to->saved_priority);\n\t\ttarget_thread = binder_get_txn_from_and_acq_inner(in_reply_to);\n\t\tif (target_thread == NULL) {\n\t\t\t/* annotation for sparse */\n\t\t\t__release(&target_thread->proc->inner_lock);\n\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\tif (target_thread->transaction_stack != in_reply_to) {\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad target transaction stack %d, expected %d\\n\",\n\t\t\t\tproc->pid, thread->pid,\n\t\t\t\ttarget_thread->transaction_stack ?\n\t\t\t\ttarget_thread->transaction_stack->debug_id : 0,\n\t\t\t\tin_reply_to->debug_id);\n\t\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\ttarget_thread = NULL;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\ttarget_proc = target_thread->proc;\n\t\ttarget_proc->tmp_ref++;\n\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t} else {\n\t\tif (tr->target.handle) {\n\t\t\tstruct binder_ref *ref;\n\n\t\t\t/*\n\t\t\t * There must already be a strong ref\n\t\t\t * on this node. If so, do a strong\n\t\t\t * increment on the node to ensure it\n\t\t\t * stays alive until the transaction is\n\t\t\t * done.\n\t\t\t */\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, tr->target.handle,\n\t\t\t\t\t\t     true);\n\t\t\tif (ref) {\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\tref->node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\t} else {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to invalid handle, %u\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid, tr->target.handle);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t}\n\t\t\tbinder_proc_unlock(proc);\n\t\t} else {\n\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\ttarget_node = context->binder_context_mgr_node;\n\t\t\tif (target_node)\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\ttarget_node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\telse\n\t\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\tif (target_node && target_proc->pid == proc->pid) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to context manager from process owning it\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_invalid_target_handle;\n\t\t\t}\n\t\t}\n\t\tif (!target_node) {\n\t\t\t/*\n\t\t\t * return_error is set above\n\t\t\t */\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\te->to_node = target_node->debug_id;\n\t\tif (WARN_ON(proc == target_proc)) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_invalid_target_handle;\n\t\t}\n\t\tif (security_binder_transaction(proc->tsk,\n\t\t\t\t\t\ttarget_proc->tsk) < 0) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPERM;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_invalid_target_handle;\n\t\t}\n\t\tbinder_inner_proc_lock(proc);\n\n\t\tw = list_first_entry_or_null(&thread->todo,\n\t\t\t\t\t     struct binder_work, entry);\n\t\tif (!(tr->flags & TF_ONE_WAY) && w &&\n\t\t    w->type == BINDER_WORK_TRANSACTION) {\n\t\t\t/*\n\t\t\t * Do not allow new outgoing transaction from a\n\t\t\t * thread that has a transaction at the head of\n\t\t\t * its todo list. Only need to check the head\n\t\t\t * because binder_select_thread_ilocked picks a\n\t\t\t * thread from proc->waiting_threads to enqueue\n\t\t\t * the transaction, and nothing is queued to the\n\t\t\t * todo list while the thread is on waiting_threads.\n\t\t\t */\n\t\t\tbinder_user_error(\"%d:%d new transaction not allowed when there is a transaction on thread todo\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_todo_list;\n\t\t}\n\n\t\tif (!(tr->flags & TF_ONE_WAY) && thread->transaction_stack) {\n\t\t\tstruct binder_transaction *tmp;\n\n\t\t\ttmp = thread->transaction_stack;\n\t\t\tif (tmp->to_thread != thread) {\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tbinder_user_error(\"%d:%d got new transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, tmp->debug_id,\n\t\t\t\t\ttmp->to_proc ? tmp->to_proc->pid : 0,\n\t\t\t\t\ttmp->to_thread ?\n\t\t\t\t\ttmp->to_thread->pid : 0);\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EPROTO;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_call_stack;\n\t\t\t}\n\t\t\twhile (tmp) {\n\t\t\t\tstruct binder_thread *from;\n\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tfrom = tmp->from;\n\t\t\t\tif (from && from->proc == target_proc) {\n\t\t\t\t\tatomic_inc(&from->tmp_ref);\n\t\t\t\t\ttarget_thread = from;\n\t\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\ttmp = tmp->from_parent;\n\t\t\t}\n\t\t}\n\t\tbinder_inner_proc_unlock(proc);\n\t}\n\tif (target_thread)\n\t\te->to_thread = target_thread->pid;\n\te->to_proc = target_proc->pid;\n\n\t/* TODO: reuse incoming transaction for reply */\n\tt = kzalloc(sizeof(*t), GFP_KERNEL);\n\tif (t == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_t_failed;\n\t}\n\tINIT_LIST_HEAD(&t->fd_fixups);\n\tbinder_stats_created(BINDER_STAT_TRANSACTION);\n\tspin_lock_init(&t->lock);\n\n\ttcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL);\n\tif (tcomplete == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_tcomplete_failed;\n\t}\n\tbinder_stats_created(BINDER_STAT_TRANSACTION_COMPLETE);\n\n\tt->debug_id = t_debug_id;\n\n\tif (reply)\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_REPLY %d -> %d:%d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_thread->pid,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\telse\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_TRANSACTION %d -> %d - node %d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_node->debug_id,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\n\tif (!reply && !(tr->flags & TF_ONE_WAY))\n\t\tt->from = thread;\n\telse\n\t\tt->from = NULL;\n\tt->sender_euid = proc->cred->euid;\n\tt->to_proc = target_proc;\n\tt->to_thread = target_thread;\n\tt->code = tr->code;\n\tt->flags = tr->flags;\n\tt->priority = task_nice(current);\n\n\tif (target_node && target_node->txn_security_ctx) {\n\t\tu32 secid;\n\t\tsize_t added_size;\n\n\t\t/*\n\t\t * Arguably this should be the task's subjective LSM secid but\n\t\t * we can't reliably access the subjective creds of a task\n\t\t * other than our own so we must use the objective creds, which\n\t\t * are safe to access.  The downside is that if a task is\n\t\t * temporarily overriding it's creds it will not be reflected\n\t\t * here; however, it isn't clear that binder would handle that\n\t\t * case well anyway.\n\t\t */\n\t\tsecurity_task_getsecid_obj(proc->tsk, &secid);\n\t\tret = security_secid_to_secctx(secid, &secctx, &secctx_sz);\n\t\tif (ret) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = ret;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_get_secctx_failed;\n\t\t}\n\t\tadded_size = ALIGN(secctx_sz, sizeof(u64));\n\t\textra_buffers_size += added_size;\n\t\tif (extra_buffers_size < added_size) {\n\t\t\t/* integer overflow of extra_buffers_size */\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_extra_size;\n\t\t}\n\t}\n\n\ttrace_binder_transaction(reply, t, target_node);\n\n\tt->buffer = binder_alloc_new_buf(&target_proc->alloc, tr->data_size,\n\t\ttr->offsets_size, extra_buffers_size,\n\t\t!reply && (t->flags & TF_ONE_WAY), current->tgid);\n\tif (IS_ERR(t->buffer)) {\n\t\t/*\n\t\t * -ESRCH indicates VMA cleared. The target is dying.\n\t\t */\n\t\treturn_error_param = PTR_ERR(t->buffer);\n\t\treturn_error = return_error_param == -ESRCH ?\n\t\t\tBR_DEAD_REPLY : BR_FAILED_REPLY;\n\t\treturn_error_line = __LINE__;\n\t\tt->buffer = NULL;\n\t\tgoto err_binder_alloc_buf_failed;\n\t}\n\tif (secctx) {\n\t\tint err;\n\t\tsize_t buf_offset = ALIGN(tr->data_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(tr->offsets_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(extra_buffers_size, sizeof(void *)) -\n\t\t\t\t    ALIGN(secctx_sz, sizeof(u64));\n\n\t\tt->security_ctx = (uintptr_t)t->buffer->user_data + buf_offset;\n\t\terr = binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t  t->buffer, buf_offset,\n\t\t\t\t\t\t  secctx, secctx_sz);\n\t\tif (err) {\n\t\t\tt->security_ctx = 0;\n\t\t\tWARN_ON(1);\n\t\t}\n\t\tsecurity_release_secctx(secctx, secctx_sz);\n\t\tsecctx = NULL;\n\t}\n\tt->buffer->debug_id = t->debug_id;\n\tt->buffer->transaction = t;\n\tt->buffer->target_node = target_node;\n\tt->buffer->clear_on_free = !!(t->flags & TF_CLEAR_BUF);\n\ttrace_binder_transaction_alloc_buf(t->buffer);\n\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer, 0,\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.buffer,\n\t\t\t\ttr->data_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid data ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer,\n\t\t\t\tALIGN(tr->data_size, sizeof(void *)),\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.offsets,\n\t\t\t\ttr->offsets_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (!IS_ALIGNED(tr->offsets_size, sizeof(binder_size_t))) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets size, %lld\\n\",\n\t\t\t\tproc->pid, thread->pid, (u64)tr->offsets_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\tif (!IS_ALIGNED(extra_buffers_size, sizeof(u64))) {\n\t\tbinder_user_error(\"%d:%d got transaction with unaligned buffers size, %lld\\n\",\n\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t  (u64)extra_buffers_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\toff_start_offset = ALIGN(tr->data_size, sizeof(void *));\n\tbuffer_offset = off_start_offset;\n\toff_end_offset = off_start_offset + tr->offsets_size;\n\tsg_buf_offset = ALIGN(off_end_offset, sizeof(void *));\n\tsg_buf_end_offset = sg_buf_offset + extra_buffers_size -\n\t\tALIGN(secctx_sz, sizeof(u64));\n\toff_min = 0;\n\tfor (buffer_offset = off_start_offset; buffer_offset < off_end_offset;\n\t     buffer_offset += sizeof(binder_size_t)) {\n\t\tstruct binder_object_header *hdr;\n\t\tsize_t object_size;\n\t\tstruct binder_object object;\n\t\tbinder_size_t object_offset;\n\n\t\tif (binder_alloc_copy_from_buffer(&target_proc->alloc,\n\t\t\t\t\t\t  &object_offset,\n\t\t\t\t\t\t  t->buffer,\n\t\t\t\t\t\t  buffer_offset,\n\t\t\t\t\t\t  sizeof(object_offset))) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\t\tobject_size = binder_get_object(target_proc, t->buffer,\n\t\t\t\t\t\tobject_offset, &object);\n\t\tif (object_size == 0 || object_offset < off_min) {\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offset (%lld, min %lld max %lld) or object.\\n\",\n\t\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t\t  (u64)object_offset,\n\t\t\t\t\t  (u64)off_min,\n\t\t\t\t\t  (u64)t->buffer->data_size);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\n\t\thdr = &object.hdr;\n\t\toff_min = object_offset + object_size;\n\t\tswitch (hdr->type) {\n\t\tcase BINDER_TYPE_BINDER:\n\t\tcase BINDER_TYPE_WEAK_BINDER: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_binder(fp, t, thread);\n\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\t\tcase BINDER_TYPE_HANDLE:\n\t\tcase BINDER_TYPE_WEAK_HANDLE: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_handle(fp, t, thread);\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\n\t\tcase BINDER_TYPE_FD: {\n\t\t\tstruct binder_fd_object *fp = to_binder_fd_object(hdr);\n\t\t\tbinder_size_t fd_offset = object_offset +\n\t\t\t\t(uintptr_t)&fp->fd - (uintptr_t)fp;\n\t\t\tint ret = binder_translate_fd(fp->fd, fd_offset, t,\n\t\t\t\t\t\t      thread, in_reply_to);\n\n\t\t\tfp->pad_binder = 0;\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\t\tcase BINDER_TYPE_FDA: {\n\t\t\tstruct binder_object ptr_object;\n\t\t\tbinder_size_t parent_offset;\n\t\t\tstruct binder_fd_array_object *fda =\n\t\t\t\tto_binder_fd_array_object(hdr);\n\t\t\tsize_t num_valid = (buffer_offset - off_start_offset) /\n\t\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tstruct binder_buffer_object *parent =\n\t\t\t\tbinder_validate_ptr(target_proc, t->buffer,\n\t\t\t\t\t\t    &ptr_object, fda->parent,\n\t\t\t\t\t\t    off_start_offset,\n\t\t\t\t\t\t    &parent_offset,\n\t\t\t\t\t\t    num_valid);\n\t\t\tif (!parent) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid parent offset or type\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tif (!binder_validate_fixup(target_proc, t->buffer,\n\t\t\t\t\t\t   off_start_offset,\n\t\t\t\t\t\t   parent_offset,\n\t\t\t\t\t\t   fda->parent_offset,\n\t\t\t\t\t\t   last_fixup_obj_off,\n\t\t\t\t\t\t   last_fixup_min_off)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with out-of-order buffer fixup\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tret = binder_translate_fd_array(fda, parent, t, thread,\n\t\t\t\t\t\t\tin_reply_to);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj_off = parent_offset;\n\t\t\tlast_fixup_min_off =\n\t\t\t\tfda->parent_offset + sizeof(u32) * fda->num_fds;\n\t\t} break;\n\t\tcase BINDER_TYPE_PTR: {\n\t\t\tstruct binder_buffer_object *bp =\n\t\t\t\tto_binder_buffer_object(hdr);\n\t\t\tsize_t buf_left = sg_buf_end_offset - sg_buf_offset;\n\t\t\tsize_t num_valid;\n\n\t\t\tif (bp->length > buf_left) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with too large buffer\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_offset;\n\t\t\t}\n\t\t\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t\t\t&target_proc->alloc,\n\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\tsg_buf_offset,\n\t\t\t\t\t\t(const void __user *)\n\t\t\t\t\t\t\t(uintptr_t)bp->buffer,\n\t\t\t\t\t\tbp->length)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error_param = -EFAULT;\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_copy_data_failed;\n\t\t\t}\n\t\t\t/* Fixup buffer pointer to target proc address space */\n\t\t\tbp->buffer = (uintptr_t)\n\t\t\t\tt->buffer->user_data + sg_buf_offset;\n\t\t\tsg_buf_offset += ALIGN(bp->length, sizeof(u64));\n\n\t\t\tnum_valid = (buffer_offset - off_start_offset) /\n\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tret = binder_fixup_parent(t, thread, bp,\n\t\t\t\t\t\t  off_start_offset,\n\t\t\t\t\t\t  num_valid,\n\t\t\t\t\t\t  last_fixup_obj_off,\n\t\t\t\t\t\t  last_fixup_min_off);\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tbp, sizeof(*bp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj_off = object_offset;\n\t\t\tlast_fixup_min_off = 0;\n\t\t} break;\n\t\tdefault:\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid object type, %x\\n\",\n\t\t\t\tproc->pid, thread->pid, hdr->type);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_object_type;\n\t\t}\n\t}\n\tif (t->buffer->oneway_spam_suspect)\n\t\ttcomplete->type = BINDER_WORK_TRANSACTION_ONEWAY_SPAM_SUSPECT;\n\telse\n\t\ttcomplete->type = BINDER_WORK_TRANSACTION_COMPLETE;\n\tt->work.type = BINDER_WORK_TRANSACTION;\n\n\tif (reply) {\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tbinder_inner_proc_lock(target_proc);\n\t\tif (target_thread->is_dead || target_proc->is_frozen) {\n\t\t\treturn_error = target_thread->is_dead ?\n\t\t\t\tBR_DEAD_REPLY : BR_FROZEN_REPLY;\n\t\t\tbinder_inner_proc_unlock(target_proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_pop_transaction_ilocked(target_thread, in_reply_to);\n\t\tbinder_enqueue_thread_work_ilocked(target_thread, &t->work);\n\t\ttarget_proc->outstanding_txns++;\n\t\tbinder_inner_proc_unlock(target_proc);\n\t\twake_up_interruptible_sync(&target_thread->wait);\n\t\tbinder_free_transaction(in_reply_to);\n\t} else if (!(t->flags & TF_ONE_WAY)) {\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_inner_proc_lock(proc);\n\t\t/*\n\t\t * Defer the TRANSACTION_COMPLETE, so we don't return to\n\t\t * userspace immediately; this allows the target process to\n\t\t * immediately start processing this transaction, reducing\n\t\t * latency. We will then return the TRANSACTION_COMPLETE when\n\t\t * the target replies (or there is an error).\n\t\t */\n\t\tbinder_enqueue_deferred_thread_work_ilocked(thread, tcomplete);\n\t\tt->need_reply = 1;\n\t\tt->from_parent = thread->transaction_stack;\n\t\tthread->transaction_stack = t;\n\t\tbinder_inner_proc_unlock(proc);\n\t\treturn_error = binder_proc_transaction(t,\n\t\t\t\ttarget_proc, target_thread);\n\t\tif (return_error) {\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tbinder_pop_transaction_ilocked(thread, t);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t} else {\n\t\tBUG_ON(target_node == NULL);\n\t\tBUG_ON(t->buffer->async_transaction != 1);\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\treturn_error = binder_proc_transaction(t, target_proc, NULL);\n\t\tif (return_error)\n\t\t\tgoto err_dead_proc_or_thread;\n\t}\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\t/*\n\t * write barrier to synchronize with initialization\n\t * of log entry\n\t */\n\tsmp_wmb();\n\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\treturn;\n\nerr_dead_proc_or_thread:\n\treturn_error_line = __LINE__;\n\tbinder_dequeue_work(proc, tcomplete);\nerr_translate_failed:\nerr_bad_object_type:\nerr_bad_offset:\nerr_bad_parent:\nerr_copy_data_failed:\n\tbinder_free_txn_fixups(t);\n\ttrace_binder_transaction_failed_buffer_release(t->buffer);\n\tbinder_transaction_buffer_release(target_proc, t->buffer,\n\t\t\t\t\t  buffer_offset, true);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\ttarget_node = NULL;\n\tt->buffer->transaction = NULL;\n\tbinder_alloc_free_buf(&target_proc->alloc, t->buffer);\nerr_binder_alloc_buf_failed:\nerr_bad_extra_size:\n\tif (secctx)\n\t\tsecurity_release_secctx(secctx, secctx_sz);\nerr_get_secctx_failed:\n\tkfree(tcomplete);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\nerr_alloc_tcomplete_failed:\n\tif (trace_binder_txn_latency_free_enabled())\n\t\tbinder_txn_latency_free(t);\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\nerr_alloc_t_failed:\nerr_bad_todo_list:\nerr_bad_call_stack:\nerr_empty_call_stack:\nerr_dead_binder:\nerr_invalid_target_handle:\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tif (target_proc)\n\t\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node) {\n\t\tbinder_dec_node(target_node, 1, 0);\n\t\tbinder_dec_node_tmpref(target_node);\n\t}\n\n\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t     \"%d:%d transaction failed %d/%d, size %lld-%lld line %d\\n\",\n\t\t     proc->pid, thread->pid, return_error, return_error_param,\n\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t     return_error_line);\n\n\t{\n\t\tstruct binder_transaction_log_entry *fe;\n\n\t\te->return_error = return_error;\n\t\te->return_error_param = return_error_param;\n\t\te->return_error_line = return_error_line;\n\t\tfe = binder_transaction_log_add(&binder_transaction_log_failed);\n\t\t*fe = *e;\n\t\t/*\n\t\t * write barrier to synchronize with initialization\n\t\t * of log entry\n\t\t */\n\t\tsmp_wmb();\n\t\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\t\tWRITE_ONCE(fe->debug_id_done, t_debug_id);\n\t}\n\n\tBUG_ON(thread->return_error.cmd != BR_OK);\n\tif (in_reply_to) {\n\t\tthread->return_error.cmd = BR_TRANSACTION_COMPLETE;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t\tbinder_send_failed_reply(in_reply_to, return_error);\n\t} else {\n\t\tthread->return_error.cmd = return_error;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t}\n}",
            "detect_result": "YES",
            "final_result": 1
        },
        {
            "id": 3233,
            "cve_id": "CVE-2021-39713",
            "code_snippet": "static struct tcf_block *tcf_block_find(struct net *net, struct Qdisc **q,\n\t\t\t\t\tu32 *parent, unsigned long *cl,\n\t\t\t\t\tint ifindex, u32 block_index,\n\t\t\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct tcf_block *block;\n\tint err = 0;\n\n\tif (ifindex == TCM_IFINDEX_MAGIC_BLOCK) {\n\t\tblock = tcf_block_lookup(net, block_index);\n\t\tif (!block) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Block of given index was not found\");\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\t} else {\n\t\tconst struct Qdisc_class_ops *cops;\n\t\tstruct net_device *dev;\n\n\t\trcu_read_lock();\n\n\t\t/* Find link */\n\t\tdev = dev_get_by_index_rcu(net, ifindex);\n\t\tif (!dev) {\n\t\t\trcu_read_unlock();\n\t\t\treturn ERR_PTR(-ENODEV);\n\t\t}\n\n\t\t/* Find qdisc */\n\t\tif (!*parent) {\n\t\t\t*q = dev->qdisc;\n\t\t\t*parent = (*q)->handle;\n\t\t} else {\n\t\t\t*q = qdisc_lookup_rcu(dev, TC_H_MAJ(*parent));\n\t\t\tif (!*q) {\n\t\t\t\tNL_SET_ERR_MSG(extack, \"Parent Qdisc doesn't exists\");\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto errout_rcu;\n\t\t\t}\n\t\t}\n\n\t\t*q = qdisc_refcount_inc_nz(*q);\n\t\tif (!*q) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Parent Qdisc doesn't exists\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout_rcu;\n\t\t}\n\n\t\t/* Is it classful? */\n\t\tcops = (*q)->ops->cl_ops;\n\t\tif (!cops) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Qdisc not classful\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout_rcu;\n\t\t}\n\n\t\tif (!cops->tcf_block) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Class doesn't support blocks\");\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto errout_rcu;\n\t\t}\n\n\t\t/* At this point we know that qdisc is not noop_qdisc,\n\t\t * which means that qdisc holds a reference to net_device\n\t\t * and we hold a reference to qdisc, so it is safe to release\n\t\t * rcu read lock.\n\t\t */\n\t\trcu_read_unlock();\n\n\t\t/* Do we search for filter, attached to class? */\n\t\tif (TC_H_MIN(*parent)) {\n\t\t\t*cl = cops->find(*q, *parent);\n\t\t\tif (*cl == 0) {\n\t\t\t\tNL_SET_ERR_MSG(extack, \"Specified class doesn't exist\");\n\t\t\t\terr = -ENOENT;\n\t\t\t\tgoto errout_qdisc;\n\t\t\t}\n\t\t}\n\n\t\t/* And the last stroke */\n\t\tblock = cops->tcf_block(*q, *cl, extack);\n\t\tif (!block) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout_qdisc;\n\t\t}\n\t\tif (tcf_block_shared(block)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"This filter block is shared. Please use the block index to manipulate the filters\");\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto errout_qdisc;\n\t\t}\n\t}\n\n\treturn block;\n\nerrout_rcu:\n\trcu_read_unlock();\nerrout_qdisc:\n\tif (*q)\n\t\tqdisc_put(*q);\n\treturn ERR_PTR(err);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3234,
            "cve_id": "CVE-2021-39713",
            "code_snippet": "static int tc_del_tfilter(struct sk_buff *skb, struct nlmsghdr *n,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct tcmsg *t;\n\tu32 protocol;\n\tu32 prio;\n\tu32 parent;\n\tu32 chain_index;\n\tstruct Qdisc *q = NULL;\n\tstruct tcf_chain_info chain_info;\n\tstruct tcf_chain *chain = NULL;\n\tstruct tcf_block *block;\n\tstruct tcf_proto *tp = NULL;\n\tunsigned long cl = 0;\n\tvoid *fh = NULL;\n\tint err;\n\n\tif (!netlink_ns_capable(skb, net->user_ns, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*t), tca, TCA_MAX, NULL, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tt = nlmsg_data(n);\n\tprotocol = TC_H_MIN(t->tcm_info);\n\tprio = TC_H_MAJ(t->tcm_info);\n\tparent = t->tcm_parent;\n\n\tif (prio == 0 && (protocol || t->tcm_handle || tca[TCA_KIND])) {\n\t\tNL_SET_ERR_MSG(extack, \"Cannot flush filters with protocol, handle or kind set\");\n\t\treturn -ENOENT;\n\t}\n\n\t/* Find head of filter chain. */\n\n\tblock = tcf_block_find(net, &q, &parent, &cl,\n\t\t\t       t->tcm_ifindex, t->tcm_block_index, extack);\n\tif (IS_ERR(block)) {\n\t\terr = PTR_ERR(block);\n\t\tgoto errout;\n\t}\n\n\tchain_index = tca[TCA_CHAIN] ? nla_get_u32(tca[TCA_CHAIN]) : 0;\n\tif (chain_index > TC_ACT_EXT_VAL_MASK) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified chain index exceeds upper limit\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\tchain = tcf_chain_get(block, chain_index, false);\n\tif (!chain) {\n\t\t/* User requested flush on non-existent chain. Nothing to do,\n\t\t * so just return success.\n\t\t */\n\t\tif (prio == 0) {\n\t\t\terr = 0;\n\t\t\tgoto errout;\n\t\t}\n\t\tNL_SET_ERR_MSG(extack, \"Cannot find specified filter chain\");\n\t\terr = -ENOENT;\n\t\tgoto errout;\n\t}\n\n\tif (prio == 0) {\n\t\ttfilter_notify_chain(net, skb, block, q, parent, n,\n\t\t\t\t     chain, RTM_DELTFILTER);\n\t\ttcf_chain_flush(chain);\n\t\terr = 0;\n\t\tgoto errout;\n\t}\n\n\ttp = tcf_chain_tp_find(chain, &chain_info, protocol,\n\t\t\t       prio, false);\n\tif (!tp || IS_ERR(tp)) {\n\t\tNL_SET_ERR_MSG(extack, \"Filter with specified priority/protocol not found\");\n\t\terr = tp ? PTR_ERR(tp) : -ENOENT;\n\t\tgoto errout;\n\t} else if (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], tp->ops->kind)) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified filter kind does not match existing one\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\tfh = tp->ops->get(tp, t->tcm_handle);\n\n\tif (!fh) {\n\t\tif (t->tcm_handle == 0) {\n\t\t\ttcf_chain_tp_remove(chain, &chain_info, tp);\n\t\t\ttfilter_notify(net, skb, n, tp, block, q, parent, fh,\n\t\t\t\t       RTM_DELTFILTER, false);\n\t\t\ttcf_proto_destroy(tp, extack);\n\t\t\terr = 0;\n\t\t} else {\n\t\t\tNL_SET_ERR_MSG(extack, \"Specified filter handle not found\");\n\t\t\terr = -ENOENT;\n\t\t}\n\t} else {\n\t\tbool last;\n\n\t\terr = tfilter_del_notify(net, skb, n, tp, block,\n\t\t\t\t\t q, parent, fh, false, &last,\n\t\t\t\t\t extack);\n\t\tif (err)\n\t\t\tgoto errout;\n\t\tif (last) {\n\t\t\ttcf_chain_tp_remove(chain, &chain_info, tp);\n\t\t\ttcf_proto_destroy(tp, extack);\n\t\t}\n\t}\n\nerrout:\n\tif (chain)\n\t\ttcf_chain_put(chain);\n\ttcf_block_release(q, block);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3276,
            "cve_id": "CVE-2021-4203",
            "code_snippet": "static void __sk_destruct(struct rcu_head *head)\n{\n\tstruct sock *sk = container_of(head, struct sock, sk_rcu);\n\tstruct sk_filter *filter;\n\n\tif (sk->sk_destruct)\n\t\tsk->sk_destruct(sk);\n\n\tfilter = rcu_dereference_check(sk->sk_filter,\n\t\t\t\t       refcount_read(&sk->sk_wmem_alloc) == 0);\n\tif (filter) {\n\t\tsk_filter_uncharge(sk, filter);\n\t\tRCU_INIT_POINTER(sk->sk_filter, NULL);\n\t}\n\n\tsock_disable_timestamp(sk, SK_FLAGS_TIMESTAMP);\n\n#ifdef CONFIG_BPF_SYSCALL\n\tbpf_sk_storage_free(sk);\n#endif\n\n\tif (atomic_read(&sk->sk_omem_alloc))\n\t\tpr_debug(\"%s: optmem leakage (%d bytes) detected\\n\",\n\t\t\t __func__, atomic_read(&sk->sk_omem_alloc));\n\n\tif (sk->sk_frag.page) {\n\t\tput_page(sk->sk_frag.page);\n\t\tsk->sk_frag.page = NULL;\n\t}\n\n\t/* We do not need to acquire sk->sk_peer_lock, we are the last user. */\n\tput_cred(sk->sk_peer_cred);\n\tput_pid(sk->sk_peer_pid);\n\n\tif (likely(sk->sk_net_refcnt))\n\t\tput_net(sock_net(sk));\n\tsk_prot_free(sk->sk_prot_creator, sk);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3277,
            "cve_id": "CVE-2021-4203",
            "code_snippet": "void sock_init_data(struct socket *sock, struct sock *sk)\n{\n\tsk_init_common(sk);\n\tsk->sk_send_head\t=\tNULL;\n\n\ttimer_setup(&sk->sk_timer, NULL, 0);\n\n\tsk->sk_allocation\t=\tGFP_KERNEL;\n\tsk->sk_rcvbuf\t\t=\tsysctl_rmem_default;\n\tsk->sk_sndbuf\t\t=\tsysctl_wmem_default;\n\tsk->sk_state\t\t=\tTCP_CLOSE;\n\tsk_set_socket(sk, sock);\n\n\tsock_set_flag(sk, SOCK_ZAPPED);\n\n\tif (sock) {\n\t\tsk->sk_type\t=\tsock->type;\n\t\tRCU_INIT_POINTER(sk->sk_wq, &sock->wq);\n\t\tsock->sk\t=\tsk;\n\t\tsk->sk_uid\t=\tSOCK_INODE(sock)->i_uid;\n\t} else {\n\t\tRCU_INIT_POINTER(sk->sk_wq, NULL);\n\t\tsk->sk_uid\t=\tmake_kuid(sock_net(sk)->user_ns, 0);\n\t}\n\n\trwlock_init(&sk->sk_callback_lock);\n\tif (sk->sk_kern_sock)\n\t\tlockdep_set_class_and_name(\n\t\t\t&sk->sk_callback_lock,\n\t\t\taf_kern_callback_keys + sk->sk_family,\n\t\t\taf_family_kern_clock_key_strings[sk->sk_family]);\n\telse\n\t\tlockdep_set_class_and_name(\n\t\t\t&sk->sk_callback_lock,\n\t\t\taf_callback_keys + sk->sk_family,\n\t\t\taf_family_clock_key_strings[sk->sk_family]);\n\n\tsk->sk_state_change\t=\tsock_def_wakeup;\n\tsk->sk_data_ready\t=\tsock_def_readable;\n\tsk->sk_write_space\t=\tsock_def_write_space;\n\tsk->sk_error_report\t=\tsock_def_error_report;\n\tsk->sk_destruct\t\t=\tsock_def_destruct;\n\n\tsk->sk_frag.page\t=\tNULL;\n\tsk->sk_frag.offset\t=\t0;\n\tsk->sk_peek_off\t\t=\t-1;\n\n\tsk->sk_peer_pid \t=\tNULL;\n\tsk->sk_peer_cred\t=\tNULL;\n\tspin_lock_init(&sk->sk_peer_lock);\n\n\tsk->sk_write_pending\t=\t0;\n\tsk->sk_rcvlowat\t\t=\t1;\n\tsk->sk_rcvtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\tsk->sk_sndtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\n\tsk->sk_stamp = SK_DEFAULT_STAMP;\n#if BITS_PER_LONG==32\n\tseqlock_init(&sk->sk_stamp_seq);\n#endif\n\tatomic_set(&sk->sk_zckey, 0);\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tsk->sk_napi_id\t\t=\t0;\n\tsk->sk_ll_usec\t\t=\tsysctl_net_busy_read;\n#endif\n\n\tsk->sk_max_pacing_rate = ~0UL;\n\tsk->sk_pacing_rate = ~0UL;\n\tWRITE_ONCE(sk->sk_pacing_shift, 10);\n\tsk->sk_incoming_cpu = -1;\n\n\tsk_rx_queue_clear(sk);\n\t/*\n\t * Before updating sk_refcnt, we must commit prior changes to memory\n\t * (Documentation/RCU/rculist_nulls.rst for details)\n\t */\n\tsmp_wmb();\n\trefcount_set(&sk->sk_refcnt, 1);\n\tatomic_set(&sk->sk_drops, 0);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3297,
            "cve_id": "CVE-2021-44733",
            "code_snippet": "struct tee_shm *tee_shm_get_from_id(struct tee_context *ctx, int id)\n{\n\tstruct tee_device *teedev;\n\tstruct tee_shm *shm;\n\n\tif (!ctx)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tteedev = ctx->teedev;\n\tmutex_lock(&teedev->mutex);\n\tshm = idr_find(&teedev->idr, id);\n\t/*\n\t * If the tee_shm was found in the IDR it must have a refcount\n\t * larger than 0 due to the guarantee in tee_shm_put() below. So\n\t * it's safe to use refcount_inc().\n\t */\n\tif (!shm || shm->ctx != ctx)\n\t\tshm = ERR_PTR(-EINVAL);\n\telse\n\t\trefcount_inc(&shm->refcount);\n\tmutex_unlock(&teedev->mutex);\n\treturn shm;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3298,
            "cve_id": "CVE-2021-44733",
            "code_snippet": "struct tee_shm *tee_shm_register(struct tee_context *ctx, unsigned long addr,\n\t\t\t\t size_t length, u32 flags)\n{\n\tstruct tee_device *teedev = ctx->teedev;\n\tconst u32 req_user_flags = TEE_SHM_DMA_BUF | TEE_SHM_USER_MAPPED;\n\tconst u32 req_kernel_flags = TEE_SHM_DMA_BUF | TEE_SHM_KERNEL_MAPPED;\n\tstruct tee_shm *shm;\n\tvoid *ret;\n\tint rc;\n\tint num_pages;\n\tunsigned long start;\n\n\tif (flags != req_user_flags && flags != req_kernel_flags)\n\t\treturn ERR_PTR(-ENOTSUPP);\n\n\tif (!tee_device_get(teedev))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (!teedev->desc->ops->shm_register ||\n\t    !teedev->desc->ops->shm_unregister) {\n\t\ttee_device_put(teedev);\n\t\treturn ERR_PTR(-ENOTSUPP);\n\t}\n\n\tteedev_ctx_get(ctx);\n\n\tshm = kzalloc(sizeof(*shm), GFP_KERNEL);\n\tif (!shm) {\n\t\tret = ERR_PTR(-ENOMEM);\n\t\tgoto err;\n\t}\n\n\trefcount_set(&shm->refcount, 1);\n\tshm->flags = flags | TEE_SHM_REGISTER;\n\tshm->ctx = ctx;\n\tshm->id = -1;\n\taddr = untagged_addr(addr);\n\tstart = rounddown(addr, PAGE_SIZE);\n\tshm->offset = addr - start;\n\tshm->size = length;\n\tnum_pages = (roundup(addr + length, PAGE_SIZE) - start) / PAGE_SIZE;\n\tshm->pages = kcalloc(num_pages, sizeof(*shm->pages), GFP_KERNEL);\n\tif (!shm->pages) {\n\t\tret = ERR_PTR(-ENOMEM);\n\t\tgoto err;\n\t}\n\n\tif (flags & TEE_SHM_USER_MAPPED) {\n\t\trc = pin_user_pages_fast(start, num_pages, FOLL_WRITE,\n\t\t\t\t\t shm->pages);\n\t} else {\n\t\tstruct kvec *kiov;\n\t\tint i;\n\n\t\tkiov = kcalloc(num_pages, sizeof(*kiov), GFP_KERNEL);\n\t\tif (!kiov) {\n\t\t\tret = ERR_PTR(-ENOMEM);\n\t\t\tgoto err;\n\t\t}\n\n\t\tfor (i = 0; i < num_pages; i++) {\n\t\t\tkiov[i].iov_base = (void *)(start + i * PAGE_SIZE);\n\t\t\tkiov[i].iov_len = PAGE_SIZE;\n\t\t}\n\n\t\trc = get_kernel_pages(kiov, num_pages, 0, shm->pages);\n\t\tkfree(kiov);\n\t}\n\tif (rc > 0)\n\t\tshm->num_pages = rc;\n\tif (rc != num_pages) {\n\t\tif (rc >= 0)\n\t\t\trc = -ENOMEM;\n\t\tret = ERR_PTR(rc);\n\t\tgoto err;\n\t}\n\n\tmutex_lock(&teedev->mutex);\n\tshm->id = idr_alloc(&teedev->idr, shm, 1, 0, GFP_KERNEL);\n\tmutex_unlock(&teedev->mutex);\n\n\tif (shm->id < 0) {\n\t\tret = ERR_PTR(shm->id);\n\t\tgoto err;\n\t}\n\n\trc = teedev->desc->ops->shm_register(ctx, shm, shm->pages,\n\t\t\t\t\t     shm->num_pages, start);\n\tif (rc) {\n\t\tret = ERR_PTR(rc);\n\t\tgoto err;\n\t}\n\n\treturn shm;\nerr:\n\tif (shm) {\n\t\tif (shm->id >= 0) {\n\t\t\tmutex_lock(&teedev->mutex);\n\t\t\tidr_remove(&teedev->idr, shm->id);\n\t\t\tmutex_unlock(&teedev->mutex);\n\t\t}\n\t\trelease_registered_pages(shm);\n\t}\n\tkfree(shm);\n\tteedev_ctx_put(ctx);\n\ttee_device_put(teedev);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3376,
            "cve_id": "CVE-2022-1048",
            "code_snippet": "void snd_pcm_detach_substream(struct snd_pcm_substream *substream)\n{\n\tstruct snd_pcm_runtime *runtime;\n\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn;\n\truntime = substream->runtime;\n\tif (runtime->private_free != NULL)\n\t\truntime->private_free(runtime);\n\tfree_pages_exact(runtime->status,\n\t\t       PAGE_ALIGN(sizeof(struct snd_pcm_mmap_status)));\n\tfree_pages_exact(runtime->control,\n\t\t       PAGE_ALIGN(sizeof(struct snd_pcm_mmap_control)));\n\tkfree(runtime->hw_constraints.rules);\n\t/* Avoid concurrent access to runtime via PCM timer interface */\n\tif (substream->timer) {\n\t\tspin_lock_irq(&substream->timer->lock);\n\t\tsubstream->runtime = NULL;\n\t\tspin_unlock_irq(&substream->timer->lock);\n\t} else {\n\t\tsubstream->runtime = NULL;\n\t}\n\tmutex_destroy(&runtime->buffer_mutex);\n\tkfree(runtime);\n\tput_pid(substream->pid);\n\tsubstream->pid = NULL;\n\tsubstream->pstr->substream_opened--;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3474,
            "cve_id": "CVE-2022-20154",
            "code_snippet": "int sctp_endpoint_hold(struct sctp_endpoint *ep)\n{\n\treturn refcount_inc_not_zero(&ep->base.refcnt);\n}",
            "detect_result": "NO\n\nExplanation: The function `sctp_endpoint_hold` appears to be incrementing a reference count for an SCTP endpoint only if it is not zero. This is a common pattern used in resource management to ensure that a resource is not freed while it is still in use. The function `refcount_inc_not_zero` is designed to safely increment a reference count without causing it to wrap around or become negative, which can prevent use-after-free vulnerabilities. Therefore, based on the provided code snippet, there does not appear to be any obvious vulnerability. However, it's important to note that this assessment is based solely on the given function and does not consider the broader context or potential issues elsewhere in the codebase.",
            "final_result": 0
        },
        {
            "id": 3531,
            "cve_id": "CVE-2022-20567",
            "code_snippet": "static int pppol2tp_connect(struct socket *sock, struct sockaddr *uservaddr,\n\t\t\t    int sockaddr_len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sockaddr_pppol2tp *sp = (struct sockaddr_pppol2tp *) uservaddr;\n\tstruct pppox_sock *po = pppox_sk(sk);\n\tstruct l2tp_session *session = NULL;\n\tstruct l2tp_tunnel *tunnel;\n\tstruct pppol2tp_session *ps;\n\tstruct l2tp_session_cfg cfg = { 0, };\n\tint error = 0;\n\tu32 tunnel_id, peer_tunnel_id;\n\tu32 session_id, peer_session_id;\n\tbool drop_refcnt = false;\n\tbool drop_tunnel = false;\n\tint ver = 2;\n\tint fd;\n\n\tlock_sock(sk);\n\n\terror = -EINVAL;\n\tif (sp->sa_protocol != PX_PROTO_OL2TP)\n\t\tgoto end;\n\n\t/* Check for already bound sockets */\n\terror = -EBUSY;\n\tif (sk->sk_state & PPPOX_CONNECTED)\n\t\tgoto end;\n\n\t/* We don't supporting rebinding anyway */\n\terror = -EALREADY;\n\tif (sk->sk_user_data)\n\t\tgoto end; /* socket is already attached */\n\n\t/* Get params from socket address. Handle L2TPv2 and L2TPv3.\n\t * This is nasty because there are different sockaddr_pppol2tp\n\t * structs for L2TPv2, L2TPv3, over IPv4 and IPv6. We use\n\t * the sockaddr size to determine which structure the caller\n\t * is using.\n\t */\n\tpeer_tunnel_id = 0;\n\tif (sockaddr_len == sizeof(struct sockaddr_pppol2tp)) {\n\t\tfd = sp->pppol2tp.fd;\n\t\ttunnel_id = sp->pppol2tp.s_tunnel;\n\t\tpeer_tunnel_id = sp->pppol2tp.d_tunnel;\n\t\tsession_id = sp->pppol2tp.s_session;\n\t\tpeer_session_id = sp->pppol2tp.d_session;\n\t} else if (sockaddr_len == sizeof(struct sockaddr_pppol2tpv3)) {\n\t\tstruct sockaddr_pppol2tpv3 *sp3 =\n\t\t\t(struct sockaddr_pppol2tpv3 *) sp;\n\t\tver = 3;\n\t\tfd = sp3->pppol2tp.fd;\n\t\ttunnel_id = sp3->pppol2tp.s_tunnel;\n\t\tpeer_tunnel_id = sp3->pppol2tp.d_tunnel;\n\t\tsession_id = sp3->pppol2tp.s_session;\n\t\tpeer_session_id = sp3->pppol2tp.d_session;\n\t} else if (sockaddr_len == sizeof(struct sockaddr_pppol2tpin6)) {\n\t\tstruct sockaddr_pppol2tpin6 *sp6 =\n\t\t\t(struct sockaddr_pppol2tpin6 *) sp;\n\t\tfd = sp6->pppol2tp.fd;\n\t\ttunnel_id = sp6->pppol2tp.s_tunnel;\n\t\tpeer_tunnel_id = sp6->pppol2tp.d_tunnel;\n\t\tsession_id = sp6->pppol2tp.s_session;\n\t\tpeer_session_id = sp6->pppol2tp.d_session;\n\t} else if (sockaddr_len == sizeof(struct sockaddr_pppol2tpv3in6)) {\n\t\tstruct sockaddr_pppol2tpv3in6 *sp6 =\n\t\t\t(struct sockaddr_pppol2tpv3in6 *) sp;\n\t\tver = 3;\n\t\tfd = sp6->pppol2tp.fd;\n\t\ttunnel_id = sp6->pppol2tp.s_tunnel;\n\t\tpeer_tunnel_id = sp6->pppol2tp.d_tunnel;\n\t\tsession_id = sp6->pppol2tp.s_session;\n\t\tpeer_session_id = sp6->pppol2tp.d_session;\n\t} else {\n\t\terror = -EINVAL;\n\t\tgoto end; /* bad socket address */\n\t}\n\n\t/* Don't bind if tunnel_id is 0 */\n\terror = -EINVAL;\n\tif (tunnel_id == 0)\n\t\tgoto end;\n\n\ttunnel = l2tp_tunnel_get(sock_net(sk), tunnel_id);\n\tif (tunnel)\n\t\tdrop_tunnel = true;\n\n\t/* Special case: create tunnel context if session_id and\n\t * peer_session_id is 0. Otherwise look up tunnel using supplied\n\t * tunnel id.\n\t */\n\tif ((session_id == 0) && (peer_session_id == 0)) {\n\t\tif (tunnel == NULL) {\n\t\t\tstruct l2tp_tunnel_cfg tcfg = {\n\t\t\t\t.encap = L2TP_ENCAPTYPE_UDP,\n\t\t\t\t.debug = 0,\n\t\t\t};\n\t\t\terror = l2tp_tunnel_create(sock_net(sk), fd, ver, tunnel_id, peer_tunnel_id, &tcfg, &tunnel);\n\t\t\tif (error < 0)\n\t\t\t\tgoto end;\n\t\t}\n\t} else {\n\t\t/* Error if we can't find the tunnel */\n\t\terror = -ENOENT;\n\t\tif (tunnel == NULL)\n\t\t\tgoto end;\n\n\t\t/* Error if socket is not prepped */\n\t\tif (tunnel->sock == NULL)\n\t\t\tgoto end;\n\t}\n\n\tif (tunnel->recv_payload_hook == NULL)\n\t\ttunnel->recv_payload_hook = pppol2tp_recv_payload_hook;\n\n\tif (tunnel->peer_tunnel_id == 0)\n\t\ttunnel->peer_tunnel_id = peer_tunnel_id;\n\n\tsession = l2tp_session_get(sock_net(sk), tunnel, session_id);\n\tif (session) {\n\t\tdrop_refcnt = true;\n\t\tps = l2tp_session_priv(session);\n\n\t\t/* Using a pre-existing session is fine as long as it hasn't\n\t\t * been connected yet.\n\t\t */\n\t\tmutex_lock(&ps->sk_lock);\n\t\tif (rcu_dereference_protected(ps->sk,\n\t\t\t\t\t      lockdep_is_held(&ps->sk_lock))) {\n\t\t\tmutex_unlock(&ps->sk_lock);\n\t\t\terror = -EEXIST;\n\t\t\tgoto end;\n\t\t}\n\t} else {\n\t\t/* Default MTU must allow space for UDP/L2TP/PPP headers */\n\t\tcfg.mtu = 1500 - PPPOL2TP_HEADER_OVERHEAD;\n\t\tcfg.mru = cfg.mtu;\n\n\t\tsession = l2tp_session_create(sizeof(struct pppol2tp_session),\n\t\t\t\t\t      tunnel, session_id,\n\t\t\t\t\t      peer_session_id, &cfg);\n\t\tif (IS_ERR(session)) {\n\t\t\terror = PTR_ERR(session);\n\t\t\tgoto end;\n\t\t}\n\n\t\tpppol2tp_session_init(session);\n\t\tps = l2tp_session_priv(session);\n\t\tl2tp_session_inc_refcount(session);\n\n\t\tmutex_lock(&ps->sk_lock);\n\t\terror = l2tp_session_register(session, tunnel);\n\t\tif (error < 0) {\n\t\t\tmutex_unlock(&ps->sk_lock);\n\t\t\tkfree(session);\n\t\t\tgoto end;\n\t\t}\n\t\tdrop_refcnt = true;\n\t}\n\n\t/* Special case: if source & dest session_id == 0x0000, this\n\t * socket is being created to manage the tunnel. Just set up\n\t * the internal context for use by ioctl() and sockopt()\n\t * handlers.\n\t */\n\tif ((session->session_id == 0) &&\n\t    (session->peer_session_id == 0)) {\n\t\terror = 0;\n\t\tgoto out_no_ppp;\n\t}\n\n\t/* The only header we need to worry about is the L2TP\n\t * header. This size is different depending on whether\n\t * sequence numbers are enabled for the data channel.\n\t */\n\tpo->chan.hdrlen = PPPOL2TP_L2TP_HDR_SIZE_NOSEQ;\n\n\tpo->chan.private = sk;\n\tpo->chan.ops\t = &pppol2tp_chan_ops;\n\tpo->chan.mtu\t = session->mtu;\n\n\terror = ppp_register_net_channel(sock_net(sk), &po->chan);\n\tif (error) {\n\t\tmutex_unlock(&ps->sk_lock);\n\t\tgoto end;\n\t}\n\nout_no_ppp:\n\t/* This is how we get the session context from the socket. */\n\tsock_hold(sk);\n\tsk->sk_user_data = session;\n\trcu_assign_pointer(ps->sk, sk);\n\tmutex_unlock(&ps->sk_lock);\n\n\t/* Keep the reference we've grabbed on the session: sk doesn't expect\n\t * the session to disappear. pppol2tp_session_destruct() is responsible\n\t * for dropping it.\n\t */\n\tdrop_refcnt = false;\n\n\tsk->sk_state = PPPOX_CONNECTED;\n\tl2tp_info(session, L2TP_MSG_CONTROL, \"%s: created\\n\",\n\t\t  session->name);\n\nend:\n\tif (drop_refcnt)\n\t\tl2tp_session_dec_refcount(session);\n\tif (drop_tunnel)\n\t\tl2tp_tunnel_dec_refcount(tunnel);\n\trelease_sock(sk);\n\n\treturn error;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3556,
            "cve_id": "CVE-2022-23039",
            "code_snippet": "static void __del_gref(struct gntalloc_gref *gref)\n{\n\tunsigned long addr;\n\n\tif (gref->notify.flags & UNMAP_NOTIFY_CLEAR_BYTE) {\n\t\tuint8_t *tmp = kmap(gref->page);\n\t\ttmp[gref->notify.pgoff] = 0;\n\t\tkunmap(gref->page);\n\t}\n\tif (gref->notify.flags & UNMAP_NOTIFY_SEND_EVENT) {\n\t\tnotify_remote_via_evtchn(gref->notify.event);\n\t\tevtchn_put(gref->notify.event);\n\t}\n\n\tgref->notify.flags = 0;\n\n\tif (gref->gref_id) {\n\t\tif (gref->page) {\n\t\t\taddr = (unsigned long)page_to_virt(gref->page);\n\t\t\tgnttab_end_foreign_access(gref->gref_id, 0, addr);\n\t\t} else\n\t\t\tgnttab_free_grant_reference(gref->gref_id);\n\t}\n\n\tgref_size--;\n\tlist_del(&gref->next_gref);\n\n\tkfree(gref);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3559,
            "cve_id": "CVE-2022-23042",
            "code_snippet": "static int xennet_get_responses(struct netfront_queue *queue,\n\t\t\t\tstruct netfront_rx_info *rinfo, RING_IDX rp,\n\t\t\t\tstruct sk_buff_head *list,\n\t\t\t\tbool *need_xdp_flush)\n{\n\tstruct xen_netif_rx_response *rx = &rinfo->rx, rx_local;\n\tint max = XEN_NETIF_NR_SLOTS_MIN + (rx->status <= RX_COPY_THRESHOLD);\n\tRING_IDX cons = queue->rx.rsp_cons;\n\tstruct sk_buff *skb = xennet_get_rx_skb(queue, cons);\n\tstruct xen_netif_extra_info *extras = rinfo->extras;\n\tgrant_ref_t ref = xennet_get_rx_ref(queue, cons);\n\tstruct device *dev = &queue->info->netdev->dev;\n\tstruct bpf_prog *xdp_prog;\n\tstruct xdp_buff xdp;\n\tint slots = 1;\n\tint err = 0;\n\tu32 verdict;\n\n\tif (rx->flags & XEN_NETRXF_extra_info) {\n\t\terr = xennet_get_extras(queue, extras, rp);\n\t\tif (!err) {\n\t\t\tif (extras[XEN_NETIF_EXTRA_TYPE_XDP - 1].type) {\n\t\t\t\tstruct xen_netif_extra_info *xdp;\n\n\t\t\t\txdp = &extras[XEN_NETIF_EXTRA_TYPE_XDP - 1];\n\t\t\t\trx->offset = xdp->u.xdp.headroom;\n\t\t\t}\n\t\t}\n\t\tcons = queue->rx.rsp_cons;\n\t}\n\n\tfor (;;) {\n\t\tif (unlikely(rx->status < 0 ||\n\t\t\t     rx->offset + rx->status > XEN_PAGE_SIZE)) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tdev_warn(dev, \"rx->offset: %u, size: %d\\n\",\n\t\t\t\t\t rx->offset, rx->status);\n\t\t\txennet_move_rx_slot(queue, skb, ref);\n\t\t\terr = -EINVAL;\n\t\t\tgoto next;\n\t\t}\n\n\t\t/*\n\t\t * This definitely indicates a bug, either in this driver or in\n\t\t * the backend driver. In future this should flag the bad\n\t\t * situation to the system controller to reboot the backend.\n\t\t */\n\t\tif (ref == GRANT_INVALID_REF) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tdev_warn(dev, \"Bad rx response id %d.\\n\",\n\t\t\t\t\t rx->id);\n\t\t\terr = -EINVAL;\n\t\t\tgoto next;\n\t\t}\n\n\t\tif (!gnttab_end_foreign_access_ref(ref, 0)) {\n\t\t\tdev_alert(dev,\n\t\t\t\t  \"Grant still in use by backend domain\\n\");\n\t\t\tqueue->info->broken = true;\n\t\t\tdev_alert(dev, \"Disabled for further use\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tgnttab_release_grant_reference(&queue->gref_rx_head, ref);\n\n\t\trcu_read_lock();\n\t\txdp_prog = rcu_dereference(queue->xdp_prog);\n\t\tif (xdp_prog) {\n\t\t\tif (!(rx->flags & XEN_NETRXF_more_data)) {\n\t\t\t\t/* currently only a single page contains data */\n\t\t\t\tverdict = xennet_run_xdp(queue,\n\t\t\t\t\t\t\t skb_frag_page(&skb_shinfo(skb)->frags[0]),\n\t\t\t\t\t\t\t rx, xdp_prog, &xdp, need_xdp_flush);\n\t\t\t\tif (verdict != XDP_PASS)\n\t\t\t\t\terr = -EINVAL;\n\t\t\t} else {\n\t\t\t\t/* drop the frame */\n\t\t\t\terr = -EINVAL;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\nnext:\n\t\t__skb_queue_tail(list, skb);\n\t\tif (!(rx->flags & XEN_NETRXF_more_data))\n\t\t\tbreak;\n\n\t\tif (cons + slots == rp) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tdev_warn(dev, \"Need more slots\\n\");\n\t\t\terr = -ENOENT;\n\t\t\tbreak;\n\t\t}\n\n\t\tRING_COPY_RESPONSE(&queue->rx, cons + slots, &rx_local);\n\t\trx = &rx_local;\n\t\tskb = xennet_get_rx_skb(queue, cons + slots);\n\t\tref = xennet_get_rx_ref(queue, cons + slots);\n\t\tslots++;\n\t}\n\n\tif (unlikely(slots > max)) {\n\t\tif (net_ratelimit())\n\t\t\tdev_warn(dev, \"Too many slots\\n\");\n\t\terr = -E2BIG;\n\t}\n\n\tif (unlikely(err))\n\t\txennet_set_rx_rsp_cons(queue, cons + slots);\n\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3615,
            "cve_id": "CVE-2022-2590",
            "code_snippet": "struct page *follow_devmap_pmd(struct vm_area_struct *vma, unsigned long addr,\n\t\tpmd_t *pmd, int flags, struct dev_pagemap **pgmap)\n{\n\tunsigned long pfn = pmd_pfn(*pmd);\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct page *page;\n\n\tassert_spin_locked(pmd_lockptr(mm, pmd));\n\n\t/* FOLL_GET and FOLL_PIN are mutually exclusive. */\n\tif (WARN_ON_ONCE((flags & (FOLL_PIN | FOLL_GET)) ==\n\t\t\t (FOLL_PIN | FOLL_GET)))\n\t\treturn NULL;\n\n\tif (flags & FOLL_WRITE && !pmd_write(*pmd))\n\t\treturn NULL;\n\n\tif (pmd_present(*pmd) && pmd_devmap(*pmd))\n\t\t/* pass */;\n\telse\n\t\treturn NULL;\n\n\tif (flags & FOLL_TOUCH)\n\t\ttouch_pmd(vma, addr, pmd, flags & FOLL_WRITE);\n\n\t/*\n\t * device mapped pages can only be returned if the\n\t * caller will manage the page reference count.\n\t */\n\tif (!(flags & (FOLL_GET | FOLL_PIN)))\n\t\treturn ERR_PTR(-EEXIST);\n\n\tpfn += (addr & ~PMD_MASK) >> PAGE_SHIFT;\n\t*pgmap = get_dev_pagemap(pfn, *pgmap);\n\tif (!*pgmap)\n\t\treturn ERR_PTR(-EFAULT);\n\tpage = pfn_to_page(pfn);\n\tif (!try_grab_page(page, flags))\n\t\tpage = ERR_PTR(-ENOMEM);\n\n\treturn page;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3640,
            "cve_id": "CVE-2022-28796",
            "code_snippet": "void jbd2_journal_wait_updates(journal_t *journal)\n{\n\tDEFINE_WAIT(wait);\n\n\twhile (1) {\n\t\t/*\n\t\t * Note that the running transaction can get freed under us if\n\t\t * this transaction is getting committed in\n\t\t * jbd2_journal_commit_transaction() ->\n\t\t * jbd2_journal_free_transaction(). This can only happen when we\n\t\t * release j_state_lock -> schedule() -> acquire j_state_lock.\n\t\t * Hence we should everytime retrieve new j_running_transaction\n\t\t * value (after j_state_lock release acquire cycle), else it may\n\t\t * lead to use-after-free of old freed transaction.\n\t\t */\n\t\ttransaction_t *transaction = journal->j_running_transaction;\n\n\t\tif (!transaction)\n\t\t\tbreak;\n\n\t\tspin_lock(&transaction->t_handle_lock);\n\t\tprepare_to_wait(&journal->j_wait_updates, &wait,\n\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tif (!atomic_read(&transaction->t_updates)) {\n\t\t\tspin_unlock(&transaction->t_handle_lock);\n\t\t\tfinish_wait(&journal->j_wait_updates, &wait);\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&transaction->t_handle_lock);\n\t\twrite_unlock(&journal->j_state_lock);\n\t\tschedule();\n\t\tfinish_wait(&journal->j_wait_updates, &wait);\n\t\twrite_lock(&journal->j_state_lock);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3657,
            "cve_id": "CVE-2022-29582",
            "code_snippet": "static int io_timeout_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe,\n\t\t\t   bool is_timeout_link)\n{\n\tstruct io_timeout_data *data;\n\tunsigned flags;\n\tu32 off = READ_ONCE(sqe->off);\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->buf_index || sqe->len != 1 ||\n\t    sqe->splice_fd_in)\n\t\treturn -EINVAL;\n\tif (off && is_timeout_link)\n\t\treturn -EINVAL;\n\tflags = READ_ONCE(sqe->timeout_flags);\n\tif (flags & ~(IORING_TIMEOUT_ABS | IORING_TIMEOUT_CLOCK_MASK |\n\t\t      IORING_TIMEOUT_ETIME_SUCCESS))\n\t\treturn -EINVAL;\n\t/* more than one clock specified is invalid, obviously */\n\tif (hweight32(flags & IORING_TIMEOUT_CLOCK_MASK) > 1)\n\t\treturn -EINVAL;\n\n\tINIT_LIST_HEAD(&req->timeout.list);\n\treq->timeout.off = off;\n\tif (unlikely(off && !req->ctx->off_timeout_used))\n\t\treq->ctx->off_timeout_used = true;\n\n\tif (WARN_ON_ONCE(req_has_async_data(req)))\n\t\treturn -EFAULT;\n\tif (io_alloc_async_data(req))\n\t\treturn -ENOMEM;\n\n\tdata = req->async_data;\n\tdata->req = req;\n\tdata->flags = flags;\n\n\tif (get_timespec64(&data->ts, u64_to_user_ptr(sqe->addr)))\n\t\treturn -EFAULT;\n\n\tif (data->ts.tv_sec < 0 || data->ts.tv_nsec < 0)\n\t\treturn -EINVAL;\n\n\tINIT_LIST_HEAD(&req->timeout.list);\n\tdata->mode = io_translate_timeout_mode(flags);\n\thrtimer_init(&data->timer, io_timeout_get_clock(data), data->mode);\n\n\tif (is_timeout_link) {\n\t\tstruct io_submit_link *link = &req->ctx->submit_state.link;\n\n\t\tif (!link->head)\n\t\t\treturn -EINVAL;\n\t\tif (link->last->opcode == IORING_OP_LINK_TIMEOUT)\n\t\t\treturn -EINVAL;\n\t\treq->timeout.head = link->last;\n\t\tlink->last->flags |= REQ_F_ARM_LTIMEOUT;\n\t}\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3771,
            "cve_id": "CVE-2022-3564",
            "code_snippet": "static int l2cap_stream_rx(struct l2cap_chan *chan, struct l2cap_ctrl *control,\n\t\t\t   struct sk_buff *skb)\n{\n\t/* l2cap_reassemble_sdu may free skb, hence invalidate control, so store\n\t * the txseq field in advance to use it after l2cap_reassemble_sdu\n\t * returns and to avoid the race condition, for example:\n\t *\n\t * The current thread calls:\n\t *   l2cap_reassemble_sdu\n\t *     chan->ops->recv == l2cap_sock_recv_cb\n\t *       __sock_queue_rcv_skb\n\t * Another thread calls:\n\t *   bt_sock_recvmsg\n\t *     skb_recv_datagram\n\t *     skb_free_datagram\n\t * Then the current thread tries to access control, but it was freed by\n\t * skb_free_datagram.\n\t */\n\tu16 txseq = control->txseq;\n\n\tBT_DBG(\"chan %p, control %p, skb %p, state %d\", chan, control, skb,\n\t       chan->rx_state);\n\n\tif (l2cap_classify_txseq(chan, txseq) == L2CAP_TXSEQ_EXPECTED) {\n\t\tl2cap_pass_to_tx(chan, control);\n\n\t\tBT_DBG(\"buffer_seq %u->%u\", chan->buffer_seq,\n\t\t       __next_seq(chan, chan->buffer_seq));\n\n\t\tchan->buffer_seq = __next_seq(chan, chan->buffer_seq);\n\n\t\tl2cap_reassemble_sdu(chan, skb, control);\n\t} else {\n\t\tif (chan->sdu) {\n\t\t\tkfree_skb(chan->sdu);\n\t\t\tchan->sdu = NULL;\n\t\t}\n\t\tchan->sdu_last_frag = NULL;\n\t\tchan->sdu_len = 0;\n\n\t\tif (skb) {\n\t\t\tBT_DBG(\"Freeing %p\", skb);\n\t\t\tkfree_skb(skb);\n\t\t}\n\t}\n\n\tchan->last_acked_seq = txseq;\n\tchan->expected_tx_seq = __next_seq(chan, txseq);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3776,
            "cve_id": "CVE-2022-3566",
            "code_snippet": "int tcp_setsockopt(struct sock *sk, int level, int optname, sockptr_t optval,\n\t\t   unsigned int optlen)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (level != SOL_TCP)\n\t\t/* Paired with WRITE_ONCE() in do_ipv6_setsockopt() and tcp_v6_connect() */\n\t\treturn READ_ONCE(icsk->icsk_af_ops)->setsockopt(sk, level, optname,\n\t\t\t\t\t\t\t\toptval, optlen);\n\treturn do_tcp_setsockopt(sk, level, optname, optval, optlen);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3780,
            "cve_id": "CVE-2022-3567",
            "code_snippet": "int sock_common_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t   char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n\treturn READ_ONCE(sk->sk_prot)->getsockopt(sk, level, optname, optval, optlen);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3781,
            "cve_id": "CVE-2022-3567",
            "code_snippet": "int inet_accept(struct socket *sock, struct socket *newsock, int flags,\n\t\tbool kern)\n{\n\tstruct sock *sk1 = sock->sk, *sk2;\n\tint err = -EINVAL;\n\n\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n\tsk2 = READ_ONCE(sk1->sk_prot)->accept(sk1, flags, &err, kern);\n\tif (!sk2)\n\t\tgoto do_err;\n\n\tlock_sock(sk2);\n\n\tsock_rps_record_flow(sk2);\n\tWARN_ON(!((1 << sk2->sk_state) &\n\t\t  (TCPF_ESTABLISHED | TCPF_SYN_RECV |\n\t\t  TCPF_CLOSE_WAIT | TCPF_CLOSE)));\n\n\tsock_graft(sk2, newsock);\n\n\tnewsock->state = SS_CONNECTED;\n\terr = 0;\n\trelease_sock(sk2);\ndo_err:\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3792,
            "cve_id": "CVE-2022-3623",
            "code_snippet": "static struct page *follow_pmd_mask(struct vm_area_struct *vma,\n\t\t\t\t    unsigned long address, pud_t *pudp,\n\t\t\t\t    unsigned int flags,\n\t\t\t\t    struct follow_page_context *ctx)\n{\n\tpmd_t *pmd, pmdval;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\tstruct mm_struct *mm = vma->vm_mm;\n\n\tpmd = pmd_offset(pudp, address);\n\t/*\n\t * The READ_ONCE() will stabilize the pmdval in a register or\n\t * on the stack so that it will stop changing under the code.\n\t */\n\tpmdval = READ_ONCE(*pmd);\n\tif (pmd_none(pmdval))\n\t\treturn no_page_table(vma, flags);\n\tif (pmd_huge(pmdval) && is_vm_hugetlb_page(vma)) {\n\t\tpage = follow_huge_pmd_pte(vma, address, flags);\n\t\tif (page)\n\t\t\treturn page;\n\t\treturn no_page_table(vma, flags);\n\t}\n\tif (is_hugepd(__hugepd(pmd_val(pmdval)))) {\n\t\tpage = follow_huge_pd(vma, address,\n\t\t\t\t      __hugepd(pmd_val(pmdval)), flags,\n\t\t\t\t      PMD_SHIFT);\n\t\tif (page)\n\t\t\treturn page;\n\t\treturn no_page_table(vma, flags);\n\t}\nretry:\n\tif (!pmd_present(pmdval)) {\n\t\t/*\n\t\t * Should never reach here, if thp migration is not supported;\n\t\t * Otherwise, it must be a thp migration entry.\n\t\t */\n\t\tVM_BUG_ON(!thp_migration_supported() ||\n\t\t\t\t  !is_pmd_migration_entry(pmdval));\n\n\t\tif (likely(!(flags & FOLL_MIGRATION)))\n\t\t\treturn no_page_table(vma, flags);\n\n\t\tpmd_migration_entry_wait(mm, pmd);\n\t\tpmdval = READ_ONCE(*pmd);\n\t\t/*\n\t\t * MADV_DONTNEED may convert the pmd to null because\n\t\t * mmap_lock is held in read mode\n\t\t */\n\t\tif (pmd_none(pmdval))\n\t\t\treturn no_page_table(vma, flags);\n\t\tgoto retry;\n\t}\n\tif (pmd_devmap(pmdval)) {\n\t\tptl = pmd_lock(mm, pmd);\n\t\tpage = follow_devmap_pmd(vma, address, pmd, flags, &ctx->pgmap);\n\t\tspin_unlock(ptl);\n\t\tif (page)\n\t\t\treturn page;\n\t}\n\tif (likely(!pmd_trans_huge(pmdval)))\n\t\treturn follow_page_pte(vma, address, pmd, flags, &ctx->pgmap);\n\n\tif ((flags & FOLL_NUMA) && pmd_protnone(pmdval))\n\t\treturn no_page_table(vma, flags);\n\nretry_locked:\n\tptl = pmd_lock(mm, pmd);\n\tif (unlikely(pmd_none(*pmd))) {\n\t\tspin_unlock(ptl);\n\t\treturn no_page_table(vma, flags);\n\t}\n\tif (unlikely(!pmd_present(*pmd))) {\n\t\tspin_unlock(ptl);\n\t\tif (likely(!(flags & FOLL_MIGRATION)))\n\t\t\treturn no_page_table(vma, flags);\n\t\tpmd_migration_entry_wait(mm, pmd);\n\t\tgoto retry_locked;\n\t}\n\tif (unlikely(!pmd_trans_huge(*pmd))) {\n\t\tspin_unlock(ptl);\n\t\treturn follow_page_pte(vma, address, pmd, flags, &ctx->pgmap);\n\t}\n\tif (flags & FOLL_SPLIT_PMD) {\n\t\tint ret;\n\t\tpage = pmd_page(*pmd);\n\t\tif (is_huge_zero_page(page)) {\n\t\t\tspin_unlock(ptl);\n\t\t\tret = 0;\n\t\t\tsplit_huge_pmd(vma, pmd, address);\n\t\t\tif (pmd_trans_unstable(pmd))\n\t\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tspin_unlock(ptl);\n\t\t\tsplit_huge_pmd(vma, pmd, address);\n\t\t\tret = pte_alloc(mm, pmd) ? -ENOMEM : 0;\n\t\t}\n\n\t\treturn ret ? ERR_PTR(ret) :\n\t\t\tfollow_page_pte(vma, address, pmd, flags, &ctx->pgmap);\n\t}\n\tpage = follow_trans_huge_pmd(vma, address, pmd, flags);\n\tspin_unlock(ptl);\n\tctx->page_mask = HPAGE_PMD_NR - 1;\n\treturn page;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3827,
            "cve_id": "CVE-2022-39188",
            "code_snippet": "static inline void\ntlb_update_vma_flags(struct mmu_gather *tlb, struct vm_area_struct *vma)\n{\n\t/*\n\t * flush_tlb_range() implementations that look at VM_HUGETLB (tile,\n\t * mips-4k) flush only large pages.\n\t *\n\t * flush_tlb_range() implementations that flush I-TLB also flush D-TLB\n\t * (tile, xtensa, arm), so it's ok to just add VM_EXEC to an existing\n\t * range.\n\t *\n\t * We rely on tlb_end_vma() to issue a flush, such that when we reset\n\t * these values the batch is empty.\n\t */\n\ttlb->vma_huge = is_vm_hugetlb_page(vma);\n\ttlb->vma_exec = !!(vma->vm_flags & VM_EXEC);\n\ttlb->vma_pfn  = !!(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP));\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3867,
            "cve_id": "CVE-2022-41849",
            "code_snippet": "static int ufx_ops_open(struct fb_info *info, int user)\n{\n\tstruct ufx_data *dev = info->par;\n\n\t/* fbcon aggressively connects to first framebuffer it finds,\n\t * preventing other clients (X) from working properly. Usually\n\t * not what the user wants. Fail by default with option to enable. */\n\tif (user == 0 && !console)\n\t\treturn -EBUSY;\n\n\tmutex_lock(&disconnect_mutex);\n\n\t/* If the USB device is gone, we don't accept new opens */\n\tif (dev->virtualized) {\n\t\tmutex_unlock(&disconnect_mutex);\n\t\treturn -ENODEV;\n\t}\n\n\tdev->fb_count++;\n\n\tkref_get(&dev->kref);\n\n\tif (fb_defio && (info->fbdefio == NULL)) {\n\t\t/* enable defio at last moment if not disabled by client */\n\n\t\tstruct fb_deferred_io *fbdefio;\n\n\t\tfbdefio = kzalloc(sizeof(*fbdefio), GFP_KERNEL);\n\t\tif (fbdefio) {\n\t\t\tfbdefio->delay = UFX_DEFIO_WRITE_DELAY;\n\t\t\tfbdefio->deferred_io = ufx_dpy_deferred_io;\n\t\t}\n\n\t\tinfo->fbdefio = fbdefio;\n\t\tfb_deferred_io_init(info);\n\t}\n\n\tpr_debug(\"open /dev/fb%d user=%d fb_info=%p count=%d\",\n\t\tinfo->node, user, info, dev->fb_count);\n\n\tmutex_unlock(&disconnect_mutex);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3899,
            "cve_id": "CVE-2022-45869",
            "code_snippet": "static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)\n{\n\tbool is_tdp_mmu_fault = is_tdp_mmu(vcpu->arch.mmu);\n\n\tunsigned long mmu_seq;\n\tint r;\n\n\tfault->gfn = fault->addr >> PAGE_SHIFT;\n\tfault->slot = kvm_vcpu_gfn_to_memslot(vcpu, fault->gfn);\n\n\tif (page_fault_handle_page_track(vcpu, fault))\n\t\treturn RET_PF_EMULATE;\n\n\tr = fast_page_fault(vcpu, fault);\n\tif (r != RET_PF_INVALID)\n\t\treturn r;\n\n\tr = mmu_topup_memory_caches(vcpu, false);\n\tif (r)\n\t\treturn r;\n\n\tmmu_seq = vcpu->kvm->mmu_invalidate_seq;\n\tsmp_rmb();\n\n\tr = kvm_faultin_pfn(vcpu, fault);\n\tif (r != RET_PF_CONTINUE)\n\t\treturn r;\n\n\tr = handle_abnormal_pfn(vcpu, fault, ACC_ALL);\n\tif (r != RET_PF_CONTINUE)\n\t\treturn r;\n\n\tr = RET_PF_RETRY;\n\n\tif (is_tdp_mmu_fault)\n\t\tread_lock(&vcpu->kvm->mmu_lock);\n\telse\n\t\twrite_lock(&vcpu->kvm->mmu_lock);\n\n\tif (is_page_fault_stale(vcpu, fault, mmu_seq))\n\t\tgoto out_unlock;\n\n\tif (is_tdp_mmu_fault) {\n\t\tr = kvm_tdp_mmu_map(vcpu, fault);\n\t} else {\n\t\tr = make_mmu_pages_available(vcpu);\n\t\tif (r)\n\t\t\tgoto out_unlock;\n\t\tr = __direct_map(vcpu, fault);\n\t}\n\nout_unlock:\n\tif (is_tdp_mmu_fault)\n\t\tread_unlock(&vcpu->kvm->mmu_lock);\n\telse\n\t\twrite_unlock(&vcpu->kvm->mmu_lock);\n\tkvm_release_pfn_clean(fault->pfn);\n\treturn r;\n}",
            "detect_result": "NO\n\nThe provided code appears to be handling page faults in a KVM (Kernel-based Virtual Machine) environment. It includes several checks and operations to manage memory mappings and faults efficiently. The code also ensures proper locking mechanisms are in place to prevent race conditions when accessing shared resources.\n\nHere are some key points that indicate the code is not obviously vulnerable:\n\n1. **Locking Mechanisms**: The code uses `read_lock` and `write_lock` to ensure that only one thread can modify the MMU (Memory Management Unit) state at a time, which helps prevent race conditions.\n\n2. **Sequence Checking**: The code checks if the page fault is stale by comparing the current `mmu_invalidate_seq` with the sequence number at the time the fault was initiated. This helps ensure that the page fault is still relevant and prevents handling outdated faults.\n\n3. **Error Handling**: The code includes error handling for various operations, such as `fast_page_fault`, `mmu_topup_memory_caches`, and `kvm_faultin_pfn`. If any of these operations fail, the function returns an appropriate error code.\n\n4. **Resource Management**: The code properly releases resources, such as calling `kvm_release_pfn_clean` to release the page frame number (PFN) after it is no longer needed.\n\nWhile this code does not appear to have obvious vulnerabilities, it is always important to consider the broader context and potential interactions with other parts of the system. Additionally, security vulnerabilities can sometimes arise from unexpected inputs or edge cases that are not immediately apparent from the code alone. Therefore, thorough testing and code review are essential to ensure the security of the system.",
            "final_result": 0
        },
        {
            "id": 3902,
            "cve_id": "CVE-2022-45888",
            "code_snippet": "static void xillyusb_disconnect(struct usb_interface *interface)\n{\n\tstruct xillyusb_dev *xdev = usb_get_intfdata(interface);\n\tstruct xillyusb_endpoint *msg_ep = xdev->msg_ep;\n\tstruct xillyfifo *fifo = &msg_ep->fifo;\n\tint rc;\n\tint i;\n\n\txillybus_cleanup_chrdev(xdev, &interface->dev);\n\n\t/*\n\t * Try to send OPCODE_QUIESCE, which will fail silently if the device\n\t * was disconnected, but makes sense on module unload.\n\t */\n\n\tmsg_ep->wake_on_drain = true;\n\txillyusb_send_opcode(xdev, ~0, OPCODE_QUIESCE, 0);\n\n\t/*\n\t * If the device has been disconnected, sending the opcode causes\n\t * a global device error with xdev->error, if such error didn't\n\t * occur earlier. Hence timing out means that the USB link is fine,\n\t * but somehow the message wasn't sent. Should never happen.\n\t */\n\n\trc = wait_event_interruptible_timeout(fifo->waitq,\n\t\t\t\t\t      msg_ep->drained || xdev->error,\n\t\t\t\t\t      XILLY_RESPONSE_TIMEOUT);\n\n\tif (!rc)\n\t\tdev_err(&interface->dev,\n\t\t\t\"Weird timeout condition on sending quiesce request.\\n\");\n\n\treport_io_error(xdev, -ENODEV); /* Discourage further activity */\n\n\t/*\n\t * This device driver is declared with soft_unbind set, or else\n\t * sending OPCODE_QUIESCE above would always fail. The price is\n\t * that the USB framework didn't kill outstanding URBs, so it has\n\t * to be done explicitly before returning from this call.\n\t */\n\n\tfor (i = 0; i < xdev->num_channels; i++) {\n\t\tstruct xillyusb_channel *chan = &xdev->channels[i];\n\n\t\t/*\n\t\t * Lock taken to prevent chan->out_ep from changing. It also\n\t\t * ensures xillyusb_open() and xillyusb_flush() don't access\n\t\t * xdev->dev after being nullified below.\n\t\t */\n\t\tmutex_lock(&chan->lock);\n\t\tif (chan->out_ep)\n\t\t\tendpoint_quiesce(chan->out_ep);\n\t\tmutex_unlock(&chan->lock);\n\t}\n\n\tendpoint_quiesce(xdev->in_ep);\n\tendpoint_quiesce(xdev->msg_ep);\n\n\tusb_set_intfdata(interface, NULL);\n\n\txdev->dev = NULL;\n\n\tmutex_lock(&kref_mutex);\n\tkref_put(&xdev->kref, cleanup_dev);\n\tmutex_unlock(&kref_mutex);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4018,
            "cve_id": "CVE-2023-1582",
            "code_snippet": "static void smaps_pte_entry(pte_t *pte, unsigned long addr,\n\t\tstruct mm_walk *walk)\n{\n\tstruct mem_size_stats *mss = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tbool locked = !!(vma->vm_flags & VM_LOCKED);\n\tstruct page *page = NULL;\n\tbool migration = false;\n\n\tif (pte_present(*pte)) {\n\t\tpage = vm_normal_page(vma, addr, *pte);\n\t} else if (is_swap_pte(*pte)) {\n\t\tswp_entry_t swpent = pte_to_swp_entry(*pte);\n\n\t\tif (!non_swap_entry(swpent)) {\n\t\t\tint mapcount;\n\n\t\t\tmss->swap += PAGE_SIZE;\n\t\t\tmapcount = swp_swapcount(swpent);\n\t\t\tif (mapcount >= 2) {\n\t\t\t\tu64 pss_delta = (u64)PAGE_SIZE << PSS_SHIFT;\n\n\t\t\t\tdo_div(pss_delta, mapcount);\n\t\t\t\tmss->swap_pss += pss_delta;\n\t\t\t} else {\n\t\t\t\tmss->swap_pss += (u64)PAGE_SIZE << PSS_SHIFT;\n\t\t\t}\n\t\t} else if (is_pfn_swap_entry(swpent)) {\n\t\t\tif (is_migration_entry(swpent))\n\t\t\t\tmigration = true;\n\t\t\tpage = pfn_swap_entry_to_page(swpent);\n\t\t}\n\t} else {\n\t\tsmaps_pte_hole_lookup(addr, walk);\n\t\treturn;\n\t}\n\n\tif (!page)\n\t\treturn;\n\n\tsmaps_account(mss, page, false, pte_young(*pte), pte_dirty(*pte),\n\t\t      locked, migration);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4043,
            "cve_id": "CVE-2023-2006",
            "code_snippet": "static struct rxrpc_bundle *rxrpc_alloc_bundle(struct rxrpc_conn_parameters *cp,\n\t\t\t\t\t       gfp_t gfp)\n{\n\tstruct rxrpc_bundle *bundle;\n\n\tbundle = kzalloc(sizeof(*bundle), gfp);\n\tif (bundle) {\n\t\tbundle->params = *cp;\n\t\trxrpc_get_peer(bundle->params.peer);\n\t\trefcount_set(&bundle->ref, 1);\n\t\tatomic_set(&bundle->active, 1);\n\t\tspin_lock_init(&bundle->channel_lock);\n\t\tINIT_LIST_HEAD(&bundle->waiting_calls);\n\t}\n\treturn bundle;\n}",
            "detect_result": "NO\n\nExplanation of the code behavior:\nThe function `rxrpc_alloc_bundle` is designed to allocate and initialize a new `rxrpc_bundle` structure. Here's a step-by-step breakdown of what it does:\n\n1. It declares a pointer `bundle` of type `struct rxrpc_bundle*`.\n2. It allocates memory for a `struct rxrpc_bundle` using `kzalloc`, which initializes the allocated memory to zero. The size of the memory block is determined by `sizeof(*bundle)`, and the allocation flags are specified by the `gfp` parameter.\n3. If the memory allocation is successful (`if (bundle)`), it proceeds to initialize the newly allocated `bundle`:\n   - It copies the contents of the `cp` (connection parameters) into the `params` field of the `bundle`.\n   - It increments the reference count of the peer associated with the connection parameters using `rxrpc_get_peer`.\n   - It sets the reference count of the `bundle` itself to 1 using `refcount_set`.\n   - It sets the active state of the `bundle` to 1 using `atomic_set`.\n   - It initializes a spinlock (`channel_lock`) used for synchronizing access to the `bundle`.\n   - It initializes a list head (`waiting_calls`) that will be used to manage a list of waiting calls associated with the `bundle`.\n4. Finally, it returns the pointer to the newly allocated and initialized `bundle`.\n\nVulnerability assessment:\nThe provided code snippet does not show any obvious vulnerabilities such as buffer overflows, use-after-free, or improper memory management issues. The memory allocation is done safely with `kzalloc`, and the initialization process appears to be correctly setting up the `bundle` structure. However, without seeing the rest of the codebase, it's impossible to rule out vulnerabilities that might arise from how this function is used elsewhere in the program. But based solely on the given code, there are no apparent vulnerabilities present.",
            "final_result": 0
        },
        {
            "id": 4044,
            "cve_id": "CVE-2023-2006",
            "code_snippet": "static void rxrpc_unbundle_conn(struct rxrpc_connection *conn)\n{\n\tstruct rxrpc_bundle *bundle = conn->bundle;\n\tunsigned int bindex;\n\tbool need_drop = false;\n\tint i;\n\n\t_enter(\"C=%x\", conn->debug_id);\n\n\tif (conn->flags & RXRPC_CONN_FINAL_ACK_MASK)\n\t\trxrpc_process_delayed_final_acks(conn, true);\n\n\tspin_lock(&bundle->channel_lock);\n\tbindex = conn->bundle_shift / RXRPC_MAXCALLS;\n\tif (bundle->conns[bindex] == conn) {\n\t\t_debug(\"clear slot %u\", bindex);\n\t\tbundle->conns[bindex] = NULL;\n\t\tfor (i = 0; i < RXRPC_MAXCALLS; i++)\n\t\t\tclear_bit(conn->bundle_shift + i, &bundle->avail_chans);\n\t\tneed_drop = true;\n\t}\n\tspin_unlock(&bundle->channel_lock);\n\n\tif (need_drop) {\n\t\trxrpc_deactivate_bundle(bundle);\n\t\trxrpc_put_connection(conn);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4157,
            "cve_id": "CVE-2023-3108",
            "code_snippet": "static int skcipher_recvmsg(struct kiocb *unused, struct socket *sock,\n\t\t\t    struct msghdr *msg, size_t ignored, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct alg_sock *ask = alg_sk(sk);\n\tstruct skcipher_ctx *ctx = ask->private;\n\tunsigned bs = crypto_ablkcipher_blocksize(crypto_ablkcipher_reqtfm(\n\t\t&ctx->req));\n\tstruct skcipher_sg_list *sgl;\n\tstruct scatterlist *sg;\n\tint err = -EAGAIN;\n\tint used;\n\tlong copied = 0;\n\n\tlock_sock(sk);\n\twhile (iov_iter_count(&msg->msg_iter)) {\n\t\tsgl = list_first_entry(&ctx->tsgl,\n\t\t\t\t       struct skcipher_sg_list, list);\n\t\tsg = sgl->sg;\n\n\t\twhile (!sg->length)\n\t\t\tsg++;\n\n\t\tif (!ctx->used) {\n\t\t\terr = skcipher_wait_for_data(sk, flags);\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\t\t}\n\n\t\tused = min_t(unsigned long, ctx->used, iov_iter_count(&msg->msg_iter));\n\n\t\tused = af_alg_make_sg(&ctx->rsgl, &msg->msg_iter, used);\n\t\terr = used;\n\t\tif (err < 0)\n\t\t\tgoto unlock;\n\n\t\tif (ctx->more || used < ctx->used)\n\t\t\tused -= used % bs;\n\n\t\terr = -EINVAL;\n\t\tif (!used)\n\t\t\tgoto free;\n\n\t\tablkcipher_request_set_crypt(&ctx->req, sg,\n\t\t\t\t\t     ctx->rsgl.sg, used,\n\t\t\t\t\t     ctx->iv);\n\n\t\terr = af_alg_wait_for_completion(\n\t\t\t\tctx->enc ?\n\t\t\t\t\tcrypto_ablkcipher_encrypt(&ctx->req) :\n\t\t\t\t\tcrypto_ablkcipher_decrypt(&ctx->req),\n\t\t\t\t&ctx->completion);\n\nfree:\n\t\taf_alg_free_sg(&ctx->rsgl);\n\n\t\tif (err)\n\t\t\tgoto unlock;\n\n\t\tcopied += used;\n\t\tskcipher_pull_sgl(sk, used);\n\t\tiov_iter_advance(&msg->msg_iter, used);\n\t}\n\n\terr = 0;\n\nunlock:\n\tskcipher_wmem_wakeup(sk);\n\trelease_sock(sk);\n\n\treturn copied ?: err;\n}",
            "detect_result": "NO\n\nThe provided code appears to be handling data reception for a symmetric key cipher (skcipher) in a Linux kernel context. It manages scatter-gather lists for cryptographic operations and ensures that data is processed in blocks of the correct size. The code includes checks and error handling mechanisms that mitigate common vulnerabilities such as buffer overflows and incorrect data handling.\n\nHowever, without additional context about how this function is called and the state of the system when it is called, it's challenging to identify all potential vulnerabilities. The code itself does not exhibit obvious security flaws like buffer overflows, improper memory management, or lack of input validation that could lead to immediate exploitation.\n\nIf there are specific areas of concern or if this function interacts with other parts of the system in a way that could introduce vulnerabilities, further analysis would be necessary. But based on the code snippet alone, the answer is NO, it does not appear to be vulnerable to common issues.",
            "final_result": 0
        },
        {
            "id": 4181,
            "cve_id": "CVE-2023-32250",
            "code_snippet": "void ksmbd_conn_lock(struct ksmbd_conn *conn)\n{\n\tmutex_lock(&conn->srv_mutex);\n}",
            "detect_result": "NO\n\nExplanation: The provided code snippet is a function that locks a mutex associated with a connection structure (`ksmbd_conn`). The function `mutex_lock` is used to acquire the lock, which is generally a safe operation. However, without additional context about how this function is used within the broader codebase, it's not possible to definitively state that there is a vulnerability just from this snippet alone. The function itself does not introduce any obvious vulnerabilities such as buffer overflows, format string issues, or improper memory management. It simply ensures that the mutex is locked before proceeding, which is a common practice to prevent race conditions in multi-threaded applications.",
            "final_result": 0
        },
        {
            "id": 4182,
            "cve_id": "CVE-2023-32250",
            "code_snippet": "bool ksmbd_conn_alive(struct ksmbd_conn *conn)\n{\n\tif (!ksmbd_server_running())\n\t\treturn false;\n\n\tif (ksmbd_conn_exiting(conn))\n\t\treturn false;\n\n\tif (kthread_should_stop())\n\t\treturn false;\n\n\tif (atomic_read(&conn->stats.open_files_count) > 0)\n\t\treturn true;\n\n\t/*\n\t * Stop current session if the time that get last request from client\n\t * is bigger than deadtime user configured and opening file count is\n\t * zero.\n\t */\n\tif (server_conf.deadtime > 0 &&\n\t    time_after(jiffies, conn->last_active + server_conf.deadtime)) {\n\t\tksmbd_debug(CONN, \"No response from client in %lu minutes\\n\",\n\t\t\t    server_conf.deadtime / SMB_ECHO_INTERVAL);\n\t\treturn false;\n\t}\n\treturn true;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4183,
            "cve_id": "CVE-2023-32250",
            "code_snippet": "static inline int check_conn_state(struct ksmbd_work *work)\n{\n\tstruct smb_hdr *rsp_hdr;\n\n\tif (ksmbd_conn_exiting(work->conn) ||\n\t    ksmbd_conn_need_reconnect(work->conn)) {\n\t\trsp_hdr = work->response_buf;\n\t\trsp_hdr->Status.CifsError = STATUS_CONNECTION_DISCONNECTED;\n\t\treturn 1;\n\t}\n\treturn 0;\n}",
            "detect_result": "NO\n\nExplanation: The provided code snippet defines a function `check_conn_state` that checks if a connection associated with a work structure (`struct ksmbd_work`) is exiting or needs to be reconnected. If either condition is true, it sets an error status in the response header and returns 1. Otherwise, it returns 0. This function does not appear to have any obvious vulnerabilities such as buffer overflows, improper memory access, or other common security issues based on the given code alone. However, the safety of this function could depend on the implementation details of `ksmbd_conn_exiting`, `ksmbd_conn_need_reconnect`, and how `work->response_buf` is managed elsewhere in the code.",
            "final_result": 0
        },
        {
            "id": 4242,
            "cve_id": "CVE-2023-33951",
            "code_snippet": "int vmw_user_bo_synccpu_ioctl(struct drm_device *dev, void *data,\n\t\t\t      struct drm_file *file_priv)\n{\n\tstruct drm_vmw_synccpu_arg *arg =\n\t\t(struct drm_vmw_synccpu_arg *) data;\n\tstruct vmw_bo *vbo;\n\tint ret;\n\n\tif ((arg->flags & (drm_vmw_synccpu_read | drm_vmw_synccpu_write)) == 0\n\t    || (arg->flags & ~(drm_vmw_synccpu_read | drm_vmw_synccpu_write |\n\t\t\t       drm_vmw_synccpu_dontblock |\n\t\t\t       drm_vmw_synccpu_allow_cs)) != 0) {\n\t\tDRM_ERROR(\"Illegal synccpu flags.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (arg->op) {\n\tcase drm_vmw_synccpu_grab:\n\t\tret = vmw_user_bo_lookup(file_priv, arg->handle, &vbo);\n\t\tif (unlikely(ret != 0))\n\t\t\treturn ret;\n\n\t\tret = vmw_user_bo_synccpu_grab(vbo, arg->flags);\n\t\tvmw_bo_unreference(&vbo);\n\t\tdrm_gem_object_put(&vbo->tbo.base);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tif (ret == -ERESTARTSYS || ret == -EBUSY)\n\t\t\t\treturn -EBUSY;\n\t\t\tDRM_ERROR(\"Failed synccpu grab on handle 0x%08x.\\n\",\n\t\t\t\t  (unsigned int) arg->handle);\n\t\t\treturn ret;\n\t\t}\n\t\tbreak;\n\tcase drm_vmw_synccpu_release:\n\t\tret = vmw_user_bo_synccpu_release(file_priv,\n\t\t\t\t\t\t  arg->handle,\n\t\t\t\t\t\t  arg->flags);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Failed synccpu release on handle 0x%08x.\\n\",\n\t\t\t\t  (unsigned int) arg->handle);\n\t\t\treturn ret;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Invalid synccpu operation.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4243,
            "cve_id": "CVE-2023-33951",
            "code_snippet": "static int vmw_user_bo_synccpu_release(struct drm_file *filp,\n\t\t\t\t       uint32_t handle,\n\t\t\t\t       uint32_t flags)\n{\n\tstruct vmw_bo *vmw_bo;\n\tint ret = vmw_user_bo_lookup(filp, handle, &vmw_bo);\n\n\tif (!ret) {\n\t\tif (!(flags & drm_vmw_synccpu_allow_cs)) {\n\t\t\tatomic_dec(&vmw_bo->cpu_writers);\n\t\t}\n\t\tttm_bo_put(&vmw_bo->tbo);\n\t}\n\n\tdrm_gem_object_put(&vmw_bo->tbo.base);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4244,
            "cve_id": "CVE-2023-33951",
            "code_snippet": "static int vmw_translate_guest_ptr(struct vmw_private *dev_priv,\n\t\t\t\t   struct vmw_sw_context *sw_context,\n\t\t\t\t   SVGAGuestPtr *ptr,\n\t\t\t\t   struct vmw_bo **vmw_bo_p)\n{\n\tstruct vmw_bo *vmw_bo;\n\tuint32_t handle = ptr->gmrId;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use GMR region.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM,\n\t\t\t     VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM);\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n\tttm_bo_put(&vmw_bo->tbo);\n\tdrm_gem_object_put(&vmw_bo->tbo.base);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->location = ptr;\n\treloc->vbo = vmw_bo;\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
            "detect_result": "NO\n\nExplanation of the code behavior:\n1. The function `vmw_translate_guest_ptr` takes several parameters including a pointer to a `vmw_private` structure, a pointer to a `vmw_sw_context` structure, a pointer to an `SVGAGuestPtr` structure, and a pointer to a `vmw_bo` pointer.\n2. It initializes a `vmw_bo` pointer and extracts a handle from the `SVGAGuestPtr` structure.\n3. It preloads buffer objects for validation using the context provided in `sw_context`.\n4. It attempts to look up a user buffer object (`vmw_bo`) using the handle. If this fails, it logs a debug message and returns an error.\n5. It sets the placement of the buffer object to either GMR (Guest Memory Region) or VRAM (Video RAM).\n6. It adds the buffer object to the validation list in the context. After adding, it releases the reference to the buffer object using `ttm_bo_put` and `drm_gem_object_put`.\n7. If adding the buffer object to the validation list fails, it returns the error.\n8. It allocates memory for a `vmw_relocation` structure. If allocation fails, it returns `-ENOMEM`.\n9. It sets the location and virtual buffer object (`vbo`) fields of the `vmw_relocation` structure and adds it to the list of buffer relocations in the `sw_context`.\n10. Finally, it returns 0 indicating success.\n\nVulnerability assessment:\nThe code does not show obvious signs of common vulnerabilities such as buffer overflows, format string vulnerabilities, or improper input validation. However, there are some considerations:\n- The function relies on the correctness of the `vmw_user_bo_lookup` function to ensure that the buffer object is valid and can be used safely.\n- The use of `ttm_bo_put` and `drm_gem_object_put` suggests proper reference counting, which helps prevent use-after-free issues.\n- The function checks for errors at various points and handles them appropriately.\n\nGiven these points, the code does not appear to have any immediate vulnerabilities based on the provided snippet. However, a more comprehensive analysis would require reviewing the implementation details of the functions called within this function, especially `vmw_user_bo_lookup`.",
            "final_result": 0
        },
        {
            "id": 4245,
            "cve_id": "CVE-2023-33951",
            "code_snippet": "static int vmw_translate_mob_ptr(struct vmw_private *dev_priv,\n\t\t\t\t struct vmw_sw_context *sw_context,\n\t\t\t\t SVGAMobId *id,\n\t\t\t\t struct vmw_bo **vmw_bo_p)\n{\n\tstruct vmw_bo *vmw_bo;\n\tuint32_t handle = *id;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use MOB buffer.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_MOB, VMW_BO_DOMAIN_MOB);\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n\tttm_bo_put(&vmw_bo->tbo);\n\tdrm_gem_object_put(&vmw_bo->tbo.base);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->mob_loc = id;\n\treloc->vbo = vmw_bo;\n\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
            "detect_result": "NO\n\nExplanation of the code behavior:\n1. The function `vmw_translate_mob_ptr` takes several parameters including a pointer to a `vmw_private` structure, a pointer to a `vmw_sw_context` structure, a pointer to an `SVGAMobId`, and a pointer to a `vmw_bo` pointer.\n2. It initializes a `vmw_bo` pointer and sets a `handle` variable to the value pointed to by `id`.\n3. It then attempts to look up a user buffer object (BO) using the `vmw_user_bo_lookup` function with the file pointer from the `sw_context` and the `handle`. If this lookup fails, it logs a debug message and returns an error.\n4. If the lookup succeeds, it sets the placement of the BO to be within the MOB domain and adds the BO to the validation context of the `sw_context`.\n5. After adding the BO to the validation context, it releases the reference to the BO using `ttm_bo_put` and `drm_gem_object_put`.\n6. If adding the BO to the validation context fails, it returns the error.\n7. It allocates memory for a `vmw_relocation` structure and checks if the allocation was successful. If not, it returns `-ENOMEM`.\n8. It sets the `mob_loc` field of the `reloc` structure to point to the `id` and the `vbo` field to point to the `vmw_bo`.\n9. It assigns the `vmw_bo` to the pointer provided by the caller (`vmw_bo_p`) and adds the `reloc` structure to the list of BO relocations in the `sw_context`.\n10. Finally, it returns 0 indicating success.\n\nVulnerability assessment:\nThe code does not appear to have obvious vulnerabilities such as buffer overflows, use-after-free, or improper input validation. However, there are some considerations:\n- The function assumes that `vmw_user_bo_lookup` will return a valid BO if it succeeds, but it does not check if the BO is in a valid state before proceeding.\n- The function releases the reference to the BO immediately after adding it to the validation context, which could lead to issues if the BO is needed elsewhere in the code before the validation process completes.\n- There is no explicit check for the value of `handle` before using it in `vmw_user_bo_lookup`, but this might be handled elsewhere in the codebase.\n\nGiven the information provided, there are no immediate security issues apparent in the snippet itself, hence the answer is NO. However, a more thorough review of the surrounding code would be necessary for a complete security analysis.",
            "final_result": 0
        },
        {
            "id": 4273,
            "cve_id": "CVE-2023-35823",
            "code_snippet": "int saa7134_vbi_fini(struct saa7134_dev *dev)\n{\n\t/* nothing */\n\tdel_timer_sync(&dev->vbi_q.timeout);\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4407,
            "cve_id": "CVE-2023-4732",
            "code_snippet": "void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)\n{\n\tstruct vm_area_struct *vma = pvmw->vma;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long address = pvmw->address;\n\tunsigned long mmun_start = address & HPAGE_PMD_MASK;\n\tpmd_t pmde;\n\tswp_entry_t entry;\n\n\tif (!(pvmw->pmd && !pvmw->pte))\n\t\treturn;\n\n\tentry = pmd_to_swp_entry(*pvmw->pmd);\n\tget_page(new);\n\tpmde = pmd_mkold(mk_huge_pmd(new, vma->vm_page_prot));\n\tif (pmd_swp_soft_dirty(*pvmw->pmd))\n\t\tpmde = pmd_mksoft_dirty(pmde);\n\tif (is_write_migration_entry(entry))\n\t\tpmde = maybe_pmd_mkwrite(pmde, vma);\n\tif (pmd_swp_uffd_wp(*pvmw->pmd))\n\t\tpmde = pmd_wrprotect(pmd_mkuffd_wp(pmde));\n\n\tflush_cache_range(vma, mmun_start, mmun_start + HPAGE_PMD_SIZE);\n\tif (PageAnon(new))\n\t\tpage_add_anon_rmap(new, vma, mmun_start, true);\n\telse\n\t\tpage_add_file_rmap(new, true);\n\tset_pmd_at(mm, mmun_start, pvmw->pmd, pmde);\n\tif ((vma->vm_flags & VM_LOCKED) && !PageDoubleMap(new))\n\t\tmlock_vma_page(new);\n\tupdate_mmu_cache_pmd(vma, address, pvmw->pmd);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4408,
            "cve_id": "CVE-2023-4732",
            "code_snippet": "static inline int\ncopy_present_pte(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,\n\t\t pte_t *dst_pte, pte_t *src_pte, unsigned long addr, int *rss,\n\t\t struct page **prealloc)\n{\n\tstruct mm_struct *src_mm = src_vma->vm_mm;\n\tunsigned long vm_flags = src_vma->vm_flags;\n\tpte_t pte = *src_pte;\n\tstruct page *page;\n\n\tpage = vm_normal_page(src_vma, addr, pte);\n\tif (page) {\n\t\tint retval;\n\n\t\tretval = copy_present_page(dst_vma, src_vma, dst_pte, src_pte,\n\t\t\t\t\t   addr, rss, prealloc, pte, page);\n\t\tif (retval <= 0)\n\t\t\treturn retval;\n\n\t\tget_page(page);\n\t\tpage_dup_rmap(page, false);\n\t\trss[mm_counter(page)]++;\n\t}\n\n\t/*\n\t * If it's a COW mapping, write protect it both\n\t * in the parent and the child\n\t */\n\tif (is_cow_mapping(vm_flags) && pte_write(pte)) {\n\t\tptep_set_wrprotect(src_mm, addr, src_pte);\n\t\tpte = pte_wrprotect(pte);\n\t}\n\n\t/*\n\t * If it's a shared mapping, mark it clean in\n\t * the child\n\t */\n\tif (vm_flags & VM_SHARED)\n\t\tpte = pte_mkclean(pte);\n\tpte = pte_mkold(pte);\n\n\tif (!userfaultfd_wp(dst_vma))\n\t\tpte = pte_clear_uffd_wp(pte);\n\n\tset_pte_at(dst_vma->vm_mm, addr, dst_pte, pte);\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        }
    ]
}