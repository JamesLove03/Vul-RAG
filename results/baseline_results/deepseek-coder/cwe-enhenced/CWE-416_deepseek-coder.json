{
    "vul_data": [
        {
            "id": 528,
            "cve_id": "CVE-2014-0203",
            "code_snippet": "static void *proc_pid_follow_link(struct dentry *dentry, struct nameidata *nd)\n{\n\tstruct inode *inode = dentry->d_inode;\n\tint error = -EACCES;\n\n\t/* We don't need a base pointer in the /proc filesystem */\n\tpath_put(&nd->path);\n\n\t/* Are we allowed to snoop on the tasks file descriptors? */\n\tif (!proc_fd_access_allowed(inode))\n\t\tgoto out;\n\n\terror = PROC_I(inode)->op.proc_get_link(inode, &nd->path);\n\tnd->last_type = LAST_BIND;\nout:\n\treturn ERR_PTR(error);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 550,
            "cve_id": "CVE-2014-2568",
            "code_snippet": "static int queue_userspace_packet(struct datapath *dp, struct sk_buff *skb,\n\t\t\t\t  const struct dp_upcall_info *upcall_info)\n{\n\tstruct ovs_header *upcall;\n\tstruct sk_buff *nskb = NULL;\n\tstruct sk_buff *user_skb; /* to be queued to userspace */\n\tstruct nlattr *nla;\n\tstruct genl_info info = {\n\t\t.dst_sk = ovs_dp_get_net(dp)->genl_sock,\n\t\t.snd_portid = upcall_info->portid,\n\t};\n\tsize_t len;\n\tunsigned int hlen;\n\tint err, dp_ifindex;\n\n\tdp_ifindex = get_dpifindex(dp);\n\tif (!dp_ifindex)\n\t\treturn -ENODEV;\n\n\tif (vlan_tx_tag_present(skb)) {\n\t\tnskb = skb_clone(skb, GFP_ATOMIC);\n\t\tif (!nskb)\n\t\t\treturn -ENOMEM;\n\n\t\tnskb = __vlan_put_tag(nskb, nskb->vlan_proto, vlan_tx_tag_get(nskb));\n\t\tif (!nskb)\n\t\t\treturn -ENOMEM;\n\n\t\tnskb->vlan_tci = 0;\n\t\tskb = nskb;\n\t}\n\n\tif (nla_attr_size(skb->len) > USHRT_MAX) {\n\t\terr = -EFBIG;\n\t\tgoto out;\n\t}\n\n\t/* Complete checksum if needed */\n\tif (skb->ip_summed == CHECKSUM_PARTIAL &&\n\t    (err = skb_checksum_help(skb)))\n\t\tgoto out;\n\n\t/* Older versions of OVS user space enforce alignment of the last\n\t * Netlink attribute to NLA_ALIGNTO which would require extensive\n\t * padding logic. Only perform zerocopy if padding is not required.\n\t */\n\tif (dp->user_features & OVS_DP_F_UNALIGNED)\n\t\thlen = skb_zerocopy_headlen(skb);\n\telse\n\t\thlen = skb->len;\n\n\tlen = upcall_msg_size(upcall_info->userdata, hlen);\n\tuser_skb = genlmsg_new_unicast(len, &info, GFP_ATOMIC);\n\tif (!user_skb) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tupcall = genlmsg_put(user_skb, 0, 0, &dp_packet_genl_family,\n\t\t\t     0, upcall_info->cmd);\n\tupcall->dp_ifindex = dp_ifindex;\n\n\tnla = nla_nest_start(user_skb, OVS_PACKET_ATTR_KEY);\n\tovs_nla_put_flow(upcall_info->key, upcall_info->key, user_skb);\n\tnla_nest_end(user_skb, nla);\n\n\tif (upcall_info->userdata)\n\t\t__nla_put(user_skb, OVS_PACKET_ATTR_USERDATA,\n\t\t\t  nla_len(upcall_info->userdata),\n\t\t\t  nla_data(upcall_info->userdata));\n\n\t/* Only reserve room for attribute header, packet data is added\n\t * in skb_zerocopy() */\n\tif (!(nla = nla_reserve(user_skb, OVS_PACKET_ATTR_PACKET, 0))) {\n\t\terr = -ENOBUFS;\n\t\tgoto out;\n\t}\n\tnla->nla_len = nla_attr_size(skb->len);\n\n\tskb_zerocopy(user_skb, skb, skb->len, hlen);\n\n\t/* Pad OVS_PACKET_ATTR_PACKET if linear copy was performed */\n\tif (!(dp->user_features & OVS_DP_F_UNALIGNED)) {\n\t\tsize_t plen = NLA_ALIGN(user_skb->len) - user_skb->len;\n\n\t\tif (plen > 0)\n\t\t\tmemset(skb_put(user_skb, plen), 0, plen);\n\t}\n\n\t((struct nlmsghdr *) user_skb->data)->nlmsg_len = user_skb->len;\n\n\terr = genlmsg_unicast(ovs_dp_get_net(dp), user_skb, upcall_info->portid);\nout:\n\tkfree_skb(nskb);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 633,
            "cve_id": "CVE-2014-4653",
            "code_snippet": "static int snd_ctl_tlv_ioctl(struct snd_ctl_file *file,\n                             struct snd_ctl_tlv __user *_tlv,\n                             int op_flag)\n{\n\tstruct snd_card *card = file->card;\n\tstruct snd_ctl_tlv tlv;\n\tstruct snd_kcontrol *kctl;\n\tstruct snd_kcontrol_volatile *vd;\n\tunsigned int len;\n\tint err = 0;\n\n\tif (copy_from_user(&tlv, _tlv, sizeof(tlv)))\n\t\treturn -EFAULT;\n\tif (tlv.length < sizeof(unsigned int) * 2)\n\t\treturn -EINVAL;\n\tdown_read(&card->controls_rwsem);\n\tkctl = snd_ctl_find_numid(card, tlv.numid);\n\tif (kctl == NULL) {\n\t\terr = -ENOENT;\n\t\tgoto __kctl_end;\n\t}\n\tif (kctl->tlv.p == NULL) {\n\t\terr = -ENXIO;\n\t\tgoto __kctl_end;\n\t}\n\tvd = &kctl->vd[tlv.numid - kctl->id.numid];\n\tif ((op_flag == 0 && (vd->access & SNDRV_CTL_ELEM_ACCESS_TLV_READ) == 0) ||\n\t    (op_flag > 0 && (vd->access & SNDRV_CTL_ELEM_ACCESS_TLV_WRITE) == 0) ||\n\t    (op_flag < 0 && (vd->access & SNDRV_CTL_ELEM_ACCESS_TLV_COMMAND) == 0)) {\n\t    \terr = -ENXIO;\n\t    \tgoto __kctl_end;\n\t}\n\tif (vd->access & SNDRV_CTL_ELEM_ACCESS_TLV_CALLBACK) {\n\t\tif (vd->owner != NULL && vd->owner != file) {\n\t\t\terr = -EPERM;\n\t\t\tgoto __kctl_end;\n\t\t}\n\t\terr = kctl->tlv.c(kctl, op_flag, tlv.length, _tlv->tlv);\n\t\tif (err > 0) {\n\t\t\tup_read(&card->controls_rwsem);\n\t\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_TLV, &kctl->id);\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\tif (op_flag) {\n\t\t\terr = -ENXIO;\n\t\t\tgoto __kctl_end;\n\t\t}\n\t\tlen = kctl->tlv.p[1] + 2 * sizeof(unsigned int);\n\t\tif (tlv.length < len) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto __kctl_end;\n\t\t}\n\t\tif (copy_to_user(_tlv->tlv, kctl->tlv.p, len))\n\t\t\terr = -EFAULT;\n\t}\n      __kctl_end:\n\tup_read(&card->controls_rwsem);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 939,
            "cve_id": "CVE-2015-8963",
            "code_snippet": "static void perf_event_init_cpu(int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\n\tmutex_lock(&swhash->hlist_mutex);\n\tswhash->online = true;\n\tif (swhash->hlist_refcount > 0) {\n\t\tstruct swevent_hlist *hlist;\n\n\t\thlist = kzalloc_node(sizeof(*hlist), GFP_KERNEL, cpu_to_node(cpu));\n\t\tWARN_ON(!hlist);\n\t\trcu_assign_pointer(swhash->swevent_hlist, hlist);\n\t}\n\tmutex_unlock(&swhash->hlist_mutex);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 961,
            "cve_id": "CVE-2016-10088",
            "code_snippet": "static ssize_t\nsg_write(struct file *filp, const char __user *buf, size_t count, loff_t * ppos)\n{\n\tint mxsize, cmd_size, k;\n\tint input_size, blocking;\n\tunsigned char opcode;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tSg_request *srp;\n\tstruct sg_header old_hdr;\n\tsg_io_hdr_t *hp;\n\tunsigned char cmnd[SG_MAX_CDB_SIZE];\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_write: count=%d\\n\", (int) count));\n\tif (atomic_read(&sdp->detaching))\n\t\treturn -ENODEV;\n\tif (!((filp->f_flags & O_NONBLOCK) ||\n\t      scsi_block_when_processing_errors(sdp->device)))\n\t\treturn -ENXIO;\n\n\tif (!access_ok(VERIFY_READ, buf, count))\n\t\treturn -EFAULT;\t/* protects following copy_from_user()s + get_user()s */\n\tif (count < SZ_SG_HEADER)\n\t\treturn -EIO;\n\tif (__copy_from_user(&old_hdr, buf, SZ_SG_HEADER))\n\t\treturn -EFAULT;\n\tblocking = !(filp->f_flags & O_NONBLOCK);\n\tif (old_hdr.reply_len < 0)\n\t\treturn sg_new_write(sfp, filp, buf, count,\n\t\t\t\t    blocking, 0, 0, NULL);\n\tif (count < (SZ_SG_HEADER + 6))\n\t\treturn -EIO;\t/* The minimum scsi command length is 6 bytes. */\n\n\tif (!(srp = sg_add_request(sfp))) {\n\t\tSCSI_LOG_TIMEOUT(1, sg_printk(KERN_INFO, sdp,\n\t\t\t\t\t      \"sg_write: queue full\\n\"));\n\t\treturn -EDOM;\n\t}\n\tbuf += SZ_SG_HEADER;\n\t__get_user(opcode, buf);\n\tif (sfp->next_cmd_len > 0) {\n\t\tcmd_size = sfp->next_cmd_len;\n\t\tsfp->next_cmd_len = 0;\t/* reset so only this write() effected */\n\t} else {\n\t\tcmd_size = COMMAND_SIZE(opcode);\t/* based on SCSI command group */\n\t\tif ((opcode >= 0xc0) && old_hdr.twelve_byte)\n\t\t\tcmd_size = 12;\n\t}\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sdp,\n\t\t\"sg_write:   scsi opcode=0x%02x, cmd_size=%d\\n\", (int) opcode, cmd_size));\n/* Determine buffer size.  */\n\tinput_size = count - cmd_size;\n\tmxsize = (input_size > old_hdr.reply_len) ? input_size : old_hdr.reply_len;\n\tmxsize -= SZ_SG_HEADER;\n\tinput_size -= SZ_SG_HEADER;\n\tif (input_size < 0) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EIO;\t/* User did not pass enough bytes for this command. */\n\t}\n\thp = &srp->header;\n\thp->interface_id = '\\0';\t/* indicator of old interface tunnelled */\n\thp->cmd_len = (unsigned char) cmd_size;\n\thp->iovec_count = 0;\n\thp->mx_sb_len = 0;\n\tif (input_size > 0)\n\t\thp->dxfer_direction = (old_hdr.reply_len > SZ_SG_HEADER) ?\n\t\t    SG_DXFER_TO_FROM_DEV : SG_DXFER_TO_DEV;\n\telse\n\t\thp->dxfer_direction = (mxsize > 0) ? SG_DXFER_FROM_DEV : SG_DXFER_NONE;\n\thp->dxfer_len = mxsize;\n\tif ((hp->dxfer_direction == SG_DXFER_TO_DEV) ||\n\t    (hp->dxfer_direction == SG_DXFER_TO_FROM_DEV))\n\t\thp->dxferp = (char __user *)buf + cmd_size;\n\telse\n\t\thp->dxferp = NULL;\n\thp->sbp = NULL;\n\thp->timeout = old_hdr.reply_len;\t/* structure abuse ... */\n\thp->flags = input_size;\t/* structure abuse ... */\n\thp->pack_id = old_hdr.pack_id;\n\thp->usr_ptr = NULL;\n\tif (__copy_from_user(cmnd, buf, cmd_size))\n\t\treturn -EFAULT;\n\t/*\n\t * SG_DXFER_TO_FROM_DEV is functionally equivalent to SG_DXFER_FROM_DEV,\n\t * but is is possible that the app intended SG_DXFER_TO_DEV, because there\n\t * is a non-zero input_size, so emit a warning.\n\t */\n\tif (hp->dxfer_direction == SG_DXFER_TO_FROM_DEV) {\n\t\tstatic char cmd[TASK_COMM_LEN];\n\t\tif (strcmp(current->comm, cmd)) {\n\t\t\tprintk_ratelimited(KERN_WARNING\n\t\t\t\t\t   \"sg_write: data in/out %d/%d bytes \"\n\t\t\t\t\t   \"for SCSI command 0x%x-- guessing \"\n\t\t\t\t\t   \"data in;\\n   program %s not setting \"\n\t\t\t\t\t   \"count and/or reply_len properly\\n\",\n\t\t\t\t\t   old_hdr.reply_len - (int)SZ_SG_HEADER,\n\t\t\t\t\t   input_size, (unsigned int) cmnd[0],\n\t\t\t\t\t   current->comm);\n\t\t\tstrcpy(cmd, current->comm);\n\t\t}\n\t}\n\tk = sg_common_write(sfp, srp, cmnd, sfp->timeout, blocking);\n\treturn (k < 0) ? k : count;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 966,
            "cve_id": "CVE-2016-10200",
            "code_snippet": "static int l2tp_ip6_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_l2tpip6 *addr = (struct sockaddr_l2tpip6 *) uaddr;\n\tstruct net *net = sock_net(sk);\n\t__be32 v4addr = 0;\n\tint addr_type;\n\tint err;\n\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\treturn -EINVAL;\n\tif (addr->l2tp_family != AF_INET6)\n\t\treturn -EINVAL;\n\tif (addr_len < sizeof(*addr))\n\t\treturn -EINVAL;\n\n\taddr_type = ipv6_addr_type(&addr->l2tp_addr);\n\n\t/* l2tp_ip6 sockets are IPv6 only */\n\tif (addr_type == IPV6_ADDR_MAPPED)\n\t\treturn -EADDRNOTAVAIL;\n\n\t/* L2TP is point-point, not multicast */\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -EADDRNOTAVAIL;\n\n\terr = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip6_lock);\n\tif (__l2tp_ip6_bind_lookup(net, &addr->l2tp_addr,\n\t\t\t\t   sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\tread_unlock_bh(&l2tp_ip6_lock);\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_CLOSE)\n\t\tgoto out_unlock;\n\n\t/* Check if the address belongs to the host. */\n\trcu_read_lock();\n\tif (addr_type != IPV6_ADDR_ANY) {\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t\t    addr->l2tp_scope_id) {\n\t\t\t\t/* Override any existing binding, if another\n\t\t\t\t * one is supplied by user.\n\t\t\t\t */\n\t\t\t\tsk->sk_bound_dev_if = addr->l2tp_scope_id;\n\t\t\t}\n\n\t\t\t/* Binding to link-local address requires an\n\t\t\t   interface */\n\t\t\tif (!sk->sk_bound_dev_if)\n\t\t\t\tgoto out_unlock_rcu;\n\n\t\t\terr = -ENODEV;\n\t\t\tdev = dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\t\tif (!dev)\n\t\t\t\tgoto out_unlock_rcu;\n\t\t}\n\n\t\t/* ipv4 addr of the socket is invalid.  Only the\n\t\t * unspecified and mapped address have a v4 equivalent.\n\t\t */\n\t\tv4addr = LOOPBACK4_IPV6;\n\t\terr = -EADDRNOTAVAIL;\n\t\tif (!ipv6_chk_addr(sock_net(sk), &addr->l2tp_addr, dev, 0))\n\t\t\tgoto out_unlock_rcu;\n\t}\n\trcu_read_unlock();\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = v4addr;\n\tsk->sk_v6_rcv_saddr = addr->l2tp_addr;\n\tnp->saddr = addr->l2tp_addr;\n\n\tl2tp_ip6_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip6_lock);\n\tsk_add_bind_node(sk, &l2tp_ip6_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip6_lock);\n\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\trelease_sock(sk);\n\treturn 0;\n\nout_unlock_rcu:\n\trcu_read_unlock();\nout_unlock:\n\trelease_sock(sk);\n\treturn err;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip6_lock);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 975,
            "cve_id": "CVE-2016-10905",
            "code_snippet": "static int read_rindex_entry(struct gfs2_inode *ip)\n{\n\tstruct gfs2_sbd *sdp = GFS2_SB(&ip->i_inode);\n\tconst unsigned bsize = sdp->sd_sb.sb_bsize;\n\tloff_t pos = sdp->sd_rgrps * sizeof(struct gfs2_rindex);\n\tstruct gfs2_rindex buf;\n\tint error;\n\tstruct gfs2_rgrpd *rgd;\n\n\tif (pos >= i_size_read(&ip->i_inode))\n\t\treturn 1;\n\n\terror = gfs2_internal_read(ip, (char *)&buf, &pos,\n\t\t\t\t   sizeof(struct gfs2_rindex));\n\n\tif (error != sizeof(struct gfs2_rindex))\n\t\treturn (error == 0) ? 1 : error;\n\n\trgd = kmem_cache_zalloc(gfs2_rgrpd_cachep, GFP_NOFS);\n\terror = -ENOMEM;\n\tif (!rgd)\n\t\treturn error;\n\n\trgd->rd_sbd = sdp;\n\trgd->rd_addr = be64_to_cpu(buf.ri_addr);\n\trgd->rd_length = be32_to_cpu(buf.ri_length);\n\trgd->rd_data0 = be64_to_cpu(buf.ri_data0);\n\trgd->rd_data = be32_to_cpu(buf.ri_data);\n\trgd->rd_bitbytes = be32_to_cpu(buf.ri_bitbytes);\n\tspin_lock_init(&rgd->rd_rsspin);\n\n\terror = compute_bitstructs(rgd);\n\tif (error)\n\t\tgoto fail;\n\n\terror = gfs2_glock_get(sdp, rgd->rd_addr,\n\t\t\t       &gfs2_rgrp_glops, CREATE, &rgd->rd_gl);\n\tif (error)\n\t\tgoto fail;\n\n\trgd->rd_gl->gl_object = rgd;\n\trgd->rd_gl->gl_vm.start = (rgd->rd_addr * bsize) & PAGE_MASK;\n\trgd->rd_gl->gl_vm.end = PAGE_ALIGN((rgd->rd_addr + rgd->rd_length) * bsize) - 1;\n\trgd->rd_rgl = (struct gfs2_rgrp_lvb *)rgd->rd_gl->gl_lksb.sb_lvbptr;\n\trgd->rd_flags &= ~(GFS2_RDF_UPTODATE | GFS2_RDF_PREFERRED);\n\tif (rgd->rd_data > sdp->sd_max_rg_data)\n\t\tsdp->sd_max_rg_data = rgd->rd_data;\n\tspin_lock(&sdp->sd_rindex_spin);\n\terror = rgd_insert(rgd);\n\tspin_unlock(&sdp->sd_rindex_spin);\n\tif (!error)\n\t\treturn 0;\n\n\terror = 0; /* someone else read in the rgrp; free it and ignore it */\n\tgfs2_glock_put(rgd->rd_gl);\n\nfail:\n\tkfree(rgd->rd_bits);\n\tkmem_cache_free(gfs2_rgrpd_cachep, rgd);\n\treturn error;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 977,
            "cve_id": "CVE-2016-10906",
            "code_snippet": "static void arc_emac_tx_clean(struct net_device *ndev)\n{\n\tstruct arc_emac_priv *priv = netdev_priv(ndev);\n\tstruct net_device_stats *stats = &ndev->stats;\n\tunsigned int i;\n\n\tfor (i = 0; i < TX_BD_NUM; i++) {\n\t\tunsigned int *txbd_dirty = &priv->txbd_dirty;\n\t\tstruct arc_emac_bd *txbd = &priv->txbd[*txbd_dirty];\n\t\tstruct buffer_state *tx_buff = &priv->tx_buff[*txbd_dirty];\n\t\tstruct sk_buff *skb = tx_buff->skb;\n\t\tunsigned int info = le32_to_cpu(txbd->info);\n\n\t\tif ((info & FOR_EMAC) || !txbd->data)\n\t\t\tbreak;\n\n\t\tif (unlikely(info & (DROP | DEFR | LTCL | UFLO))) {\n\t\t\tstats->tx_errors++;\n\t\t\tstats->tx_dropped++;\n\n\t\t\tif (info & DEFR)\n\t\t\t\tstats->tx_carrier_errors++;\n\n\t\t\tif (info & LTCL)\n\t\t\t\tstats->collisions++;\n\n\t\t\tif (info & UFLO)\n\t\t\t\tstats->tx_fifo_errors++;\n\t\t} else if (likely(info & FIRST_OR_LAST_MASK)) {\n\t\t\tstats->tx_packets++;\n\t\t\tstats->tx_bytes += skb->len;\n\t\t}\n\n\t\tdma_unmap_single(&ndev->dev, dma_unmap_addr(tx_buff, addr),\n\t\t\t\t dma_unmap_len(tx_buff, len), DMA_TO_DEVICE);\n\n\t\t/* return the sk_buff to system */\n\t\tdev_kfree_skb_irq(skb);\n\n\t\ttxbd->data = 0;\n\t\ttxbd->info = 0;\n\n\t\t*txbd_dirty = (*txbd_dirty + 1) % TX_BD_NUM;\n\t}\n\n\t/* Ensure that txbd_dirty is visible to tx() before checking\n\t * for queue stopped.\n\t */\n\tsmp_mb();\n\n\tif (netif_queue_stopped(ndev) && arc_emac_tx_avail(priv))\n\t\tnetif_wake_queue(ndev);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1057,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "static int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *)uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel & IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\t/*\n\t * connect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t * DCCP over IPv4\n\t */\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\n\t\terr = dccp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &dccp_ipv6_af_ops;\n\t\t\tsk->sk_backlog_rcv = dccp_v6_do_rcv;\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (saddr == NULL) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (np->opt != NULL)\n\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n\t\t\t\t\t  np->opt->opt_nflen);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet6_hash_connect(&dccp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tdp->dccps_iss = secure_dccpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t      sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t      inet->inet_sport,\n\t\t\t\t\t\t      inet->inet_dport);\n\terr = dccp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\tdccp_set_state(sk, DCCP_CLOSED);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1058,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "static int dccp_v6_send_response(const struct sock *sk, struct request_sock *req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct in6_addr *final_p, final;\n\tstruct flowi6 fl6;\n\tint err = -1;\n\tstruct dst_entry *dst;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\tfl6.saddr = ireq->ir_v6_loc_addr;\n\tfl6.flowlabel = 0;\n\tfl6.flowi6_oif = ireq->ir_iif;\n\tfl6.fl6_dport = ireq->ir_rmt_port;\n\tfl6.fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto done;\n\t}\n\n\tskb = dccp_make_response(sk, dst, req);\n\tif (skb != NULL) {\n\t\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\t\tdh->dccph_checksum = dccp_v6_csum_finish(skb,\n\t\t\t\t\t\t\t &ireq->ir_v6_loc_addr,\n\t\t\t\t\t\t\t &ireq->ir_v6_rmt_addr);\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\terr = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);\n\t\terr = net_xmit_eval(err);\n\t}\n\ndone:\n\tdst_release(dst);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1059,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "void inet6_destroy_sock(struct sock *sk)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct ipv6_txoptions *opt;\n\n\t/* Release rx options */\n\n\tskb = xchg(&np->pktoptions, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\tskb = xchg(&np->rxpmtu, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\t/* Free flowlabels */\n\tfl6_free_socklist(sk);\n\n\t/* Free tx options */\n\n\topt = xchg(&np->opt, NULL);\n\tif (opt)\n\t\tsock_kfree_s(sk, opt, opt->tot_len);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1060,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "int inet6_sk_rebuild_header(struct sock *sk)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dst_entry *dst;\n\n\tdst = __sk_dst_check(sk, np->dst_cookie);\n\n\tif (!dst) {\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = sk->sk_protocol;\n\t\tfl6.daddr = sk->sk_v6_daddr;\n\t\tfl6.saddr = np->saddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tfl6.fl6_sport = inet->inet_sport;\n\t\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\t\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\t\tif (IS_ERR(dst)) {\n\t\t\tsk->sk_route_caps = 0;\n\t\t\tsk->sk_err_soft = -PTR_ERR(dst);\n\t\t\treturn PTR_ERR(dst);\n\t\t}\n\n\t\t__ip6_dst_store(sk, dst, NULL, NULL);\n\t}\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1061,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "static int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t*daddr, *final_p, final;\n\tstruct dst_entry\t*dst;\n\tstruct flowi6\t\tfl6;\n\tstruct ip6_flowlabel\t*flowlabel = NULL;\n\tstruct ipv6_txoptions\t*opt;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tusin->sin6_addr.s6_addr[15] = 0x01;\n\t}\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6.flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet->inet_dport;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tif (!fl6.flowi6_oif && (addr_type&IPV6_ADDR_MULTICAST))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\topt = flowlabel ? flowlabel->opt : np->opt;\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\terr = 0;\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\t/* source address lookup done in ip6_dst_lookup */\n\n\tif (ipv6_addr_any(&np->saddr))\n\t\tnp->saddr = fl6.saddr;\n\n\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tsk->sk_v6_rcv_saddr = fl6.saddr;\n\t\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\t\tif (sk->sk_prot->rehash)\n\t\t\tsk->sk_prot->rehash(sk);\n\t}\n\n\tip6_dst_store(sk, dst,\n\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t      &np->saddr :\n#endif\n\t\t      NULL);\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1062,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "struct ipv6_txoptions *\nipv6_dup_options(struct sock *sk, struct ipv6_txoptions *opt)\n{\n\tstruct ipv6_txoptions *opt2;\n\n\topt2 = sock_kmalloc(sk, opt->tot_len, GFP_ATOMIC);\n\tif (opt2) {\n\t\tlong dif = (char *)opt2 - (char *)opt;\n\t\tmemcpy(opt2, opt, opt->tot_len);\n\t\tif (opt2->hopopt)\n\t\t\t*((char **)&opt2->hopopt) += dif;\n\t\tif (opt2->dst0opt)\n\t\t\t*((char **)&opt2->dst0opt) += dif;\n\t\tif (opt2->dst1opt)\n\t\t\t*((char **)&opt2->dst1opt) += dif;\n\t\tif (opt2->srcrt)\n\t\t\t*((char **)&opt2->srcrt) += dif;\n\t}\n\treturn opt2;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1063,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "struct ipv6_txoptions *\nipv6_renew_options(struct sock *sk, struct ipv6_txoptions *opt,\n\t\t   int newtype,\n\t\t   struct ipv6_opt_hdr __user *newopt, int newoptlen)\n{\n\tint tot_len = 0;\n\tchar *p;\n\tstruct ipv6_txoptions *opt2;\n\tint err;\n\n\tif (opt) {\n\t\tif (newtype != IPV6_HOPOPTS && opt->hopopt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->hopopt));\n\t\tif (newtype != IPV6_RTHDRDSTOPTS && opt->dst0opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst0opt));\n\t\tif (newtype != IPV6_RTHDR && opt->srcrt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->srcrt));\n\t\tif (newtype != IPV6_DSTOPTS && opt->dst1opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst1opt));\n\t}\n\n\tif (newopt && newoptlen)\n\t\ttot_len += CMSG_ALIGN(newoptlen);\n\n\tif (!tot_len)\n\t\treturn NULL;\n\n\ttot_len += sizeof(*opt2);\n\topt2 = sock_kmalloc(sk, tot_len, GFP_ATOMIC);\n\tif (!opt2)\n\t\treturn ERR_PTR(-ENOBUFS);\n\n\tmemset(opt2, 0, tot_len);\n\n\topt2->tot_len = tot_len;\n\tp = (char *)(opt2 + 1);\n\n\terr = ipv6_renew_option(opt ? opt->hopopt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_HOPOPTS,\n\t\t\t\t&opt2->hopopt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst0opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDRDSTOPTS,\n\t\t\t\t&opt2->dst0opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->srcrt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDR,\n\t\t\t\t(struct ipv6_opt_hdr **)&opt2->srcrt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst1opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_DSTOPTS,\n\t\t\t\t&opt2->dst1opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\topt2->opt_nflen = (opt2->hopopt ? ipv6_optlen(opt2->hopopt) : 0) +\n\t\t\t  (opt2->dst0opt ? ipv6_optlen(opt2->dst0opt) : 0) +\n\t\t\t  (opt2->srcrt ? ipv6_optlen(opt2->srcrt) : 0);\n\topt2->opt_flen = (opt2->dst1opt ? ipv6_optlen(opt2->dst1opt) : 0);\n\n\treturn opt2;\nout:\n\tsock_kfree_s(sk, opt2, opt2->tot_len);\n\treturn ERR_PTR(err);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1064,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "static struct dst_entry *inet6_csk_route_socket(struct sock *sk,\n\t\t\t\t\t\tstruct flowi6 *fl6)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = sk->sk_protocol;\n\tfl6->daddr = sk->sk_v6_daddr;\n\tfl6->saddr = np->saddr;\n\tfl6->flowlabel = np->flow_label;\n\tIP6_ECN_flow_xmit(sk, fl6->flowlabel);\n\tfl6->flowi6_oif = sk->sk_bound_dev_if;\n\tfl6->flowi6_mark = sk->sk_mark;\n\tfl6->fl6_sport = inet->inet_sport;\n\tfl6->fl6_dport = inet->inet_dport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(fl6));\n\n\tfinal_p = fl6_update_dst(fl6, np->opt, &final);\n\n\tdst = __inet6_csk_dst_check(sk, np->dst_cookie);\n\tif (!dst) {\n\t\tdst = ip6_dst_lookup_flow(sk, fl6, final_p);\n\n\t\tif (!IS_ERR(dst))\n\t\t\t__inet6_csk_dst_store(sk, dst, NULL, NULL);\n\t}\n\treturn dst;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1065,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "struct dst_entry *inet6_csk_route_req(const struct sock *sk,\n\t\t\t\t      struct flowi6 *fl6,\n\t\t\t\t      const struct request_sock *req,\n\t\t\t\t      u8 proto)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = proto;\n\tfl6->daddr = ireq->ir_v6_rmt_addr;\n\tfinal_p = fl6_update_dst(fl6, np->opt, &final);\n\tfl6->saddr = ireq->ir_v6_loc_addr;\n\tfl6->flowi6_oif = ireq->ir_iif;\n\tfl6->flowi6_mark = ireq->ir_mark;\n\tfl6->fl6_dport = ireq->ir_rmt_port;\n\tfl6->fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, fl6, final_p);\n\tif (IS_ERR(dst))\n\t\treturn NULL;\n\n\treturn dst;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1066,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "int inet6_csk_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl_unused)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint res;\n\n\tdst = inet6_csk_route_socket(sk, &fl6);\n\tif (IS_ERR(dst)) {\n\t\tsk->sk_err_soft = -PTR_ERR(dst);\n\t\tsk->sk_route_caps = 0;\n\t\tkfree_skb(skb);\n\t\treturn PTR_ERR(dst);\n\t}\n\n\trcu_read_lock();\n\tskb_dst_set_noref(skb, dst);\n\n\t/* Restore final destination back after routing done */\n\tfl6.daddr = sk->sk_v6_daddr;\n\n\tres = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);\n\trcu_read_unlock();\n\treturn res;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1103,
            "cve_id": "CVE-2016-4805",
            "code_snippet": "int ppp_register_net_channel(struct net *net, struct ppp_channel *chan)\n{\n\tstruct channel *pch;\n\tstruct ppp_net *pn;\n\n\tpch = kzalloc(sizeof(struct channel), GFP_KERNEL);\n\tif (!pch)\n\t\treturn -ENOMEM;\n\n\tpn = ppp_pernet(net);\n\n\tpch->ppp = NULL;\n\tpch->chan = chan;\n\tpch->chan_net = net;\n\tchan->ppp = pch;\n\tinit_ppp_file(&pch->file, CHANNEL);\n\tpch->file.hdrlen = chan->hdrlen;\n#ifdef CONFIG_PPP_MULTILINK\n\tpch->lastseq = -1;\n#endif /* CONFIG_PPP_MULTILINK */\n\tinit_rwsem(&pch->chan_sem);\n\tspin_lock_init(&pch->downl);\n\trwlock_init(&pch->upl);\n\n\tspin_lock_bh(&pn->all_channels_lock);\n\tpch->file.index = ++pn->last_channel_index;\n\tlist_add(&pch->list, &pn->new_channels);\n\tatomic_inc(&channel_count);\n\tspin_unlock_bh(&pn->all_channels_lock);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1212,
            "cve_id": "CVE-2016-8655",
            "code_snippet": "static int\npacket_setsockopt(struct socket *sock, int level, int optname, char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint ret;\n\n\tif (level != SOL_PACKET)\n\t\treturn -ENOPROTOOPT;\n\n\tswitch (optname) {\n\tcase PACKET_ADD_MEMBERSHIP:\n\tcase PACKET_DROP_MEMBERSHIP:\n\t{\n\t\tstruct packet_mreq_max mreq;\n\t\tint len = optlen;\n\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\tif (len < sizeof(struct packet_mreq))\n\t\t\treturn -EINVAL;\n\t\tif (len > sizeof(mreq))\n\t\t\tlen = sizeof(mreq);\n\t\tif (copy_from_user(&mreq, optval, len))\n\t\t\treturn -EFAULT;\n\t\tif (len < (mreq.mr_alen + offsetof(struct packet_mreq, mr_address)))\n\t\t\treturn -EINVAL;\n\t\tif (optname == PACKET_ADD_MEMBERSHIP)\n\t\t\tret = packet_mc_add(sk, &mreq);\n\t\telse\n\t\t\tret = packet_mc_drop(sk, &mreq);\n\t\treturn ret;\n\t}\n\n\tcase PACKET_RX_RING:\n\tcase PACKET_TX_RING:\n\t{\n\t\tunion tpacket_req_u req_u;\n\t\tint len;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\t\tlen = sizeof(req_u.req);\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\tdefault:\n\t\t\tlen = sizeof(req_u.req3);\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&req_u.req, optval, len))\n\t\t\treturn -EFAULT;\n\t\treturn packet_set_ring(sk, &req_u, 0,\n\t\t\toptname == PACKET_TX_RING);\n\t}\n\tcase PACKET_COPY_THRESH:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpkt_sk(sk)->copy_thresh = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VERSION:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tswitch (val) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\tcase TPACKET_V3:\n\t\t\tpo->tp_version = val;\n\t\t\treturn 0;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\tcase PACKET_RESERVE:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_reserve = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_LOSS:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_loss = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_AUXDATA:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->auxdata = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_ORIGDEV:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->origdev = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VNET_HDR:\n\t{\n\t\tint val;\n\n\t\tif (sock->type != SOCK_RAW)\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->has_vnet_hdr = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_TIMESTAMP:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->tp_tstamp = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_FANOUT:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\treturn fanout_add(sk, val & 0xffff, val >> 16);\n\t}\n\tcase PACKET_FANOUT_DATA:\n\t{\n\t\tif (!po->fanout)\n\t\t\treturn -EINVAL;\n\n\t\treturn fanout_set_data(po, optval, optlen);\n\t}\n\tcase PACKET_TX_HAS_OFF:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_tx_has_off = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_QDISC_BYPASS:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->xmit = val ? packet_direct_xmit : dev_queue_xmit;\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1220,
            "cve_id": "CVE-2016-9120",
            "code_snippet": "static long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tion_free(client, handle);\n\t\tion_handle_put(handle);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1324,
            "cve_id": "CVE-2017-10661",
            "code_snippet": "\nSYSCALL_DEFINE2(timerfd_create, int, clockid, int, flags)\n{\n\tint ufd;\n\tstruct timerfd_ctx *ctx;\n\n\t/* Check the TFD_* constants for consistency.  */\n\tBUILD_BUG_ON(TFD_CLOEXEC != O_CLOEXEC);\n\tBUILD_BUG_ON(TFD_NONBLOCK != O_NONBLOCK);\n\n\tif ((flags & ~TFD_CREATE_FLAGS) ||\n\t    (clockid != CLOCK_MONOTONIC &&\n\t     clockid != CLOCK_REALTIME &&\n\t     clockid != CLOCK_REALTIME_ALARM &&\n\t     clockid != CLOCK_BOOTTIME &&\n\t     clockid != CLOCK_BOOTTIME_ALARM))\n\t\treturn -EINVAL;\n\n\tif (!capable(CAP_WAKE_ALARM) &&\n\t    (clockid == CLOCK_REALTIME_ALARM ||\n\t     clockid == CLOCK_BOOTTIME_ALARM))\n\t\treturn -EPERM;\n\n\tctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\tinit_waitqueue_head(&ctx->wqh);\n\tctx->clockid = clockid;\n\n\tif (isalarm(ctx))\n\t\talarm_init(&ctx->t.alarm,\n\t\t\t   ctx->clockid == CLOCK_REALTIME_ALARM ?\n\t\t\t   ALARM_REALTIME : ALARM_BOOTTIME,\n\t\t\t   timerfd_alarmproc);\n\telse\n\t\thrtimer_init(&ctx->t.tmr, clockid, HRTIMER_MODE_ABS);\n\n\tctx->moffs = ktime_mono_to_real(0);\n\n\tufd = anon_inode_getfd(\"[timerfd]\", &timerfd_fops, ctx,\n\t\t\t       O_RDWR | (flags & TFD_SHARED_FCNTL_FLAGS));\n\tif (ufd < 0)\n\t\tkfree(ctx);\n\n\treturn ufd;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1382,
            "cve_id": "CVE-2017-15265",
            "code_snippet": "struct snd_seq_client_port *snd_seq_create_port(struct snd_seq_client *client,\n\t\t\t\t\t\tint port)\n{\n\tunsigned long flags;\n\tstruct snd_seq_client_port *new_port, *p;\n\tint num = -1;\n\t\n\t/* sanity check */\n\tif (snd_BUG_ON(!client))\n\t\treturn NULL;\n\n\tif (client->num_ports >= SNDRV_SEQ_MAX_PORTS) {\n\t\tpr_warn(\"ALSA: seq: too many ports for client %d\\n\", client->number);\n\t\treturn NULL;\n\t}\n\n\t/* create a new port */\n\tnew_port = kzalloc(sizeof(*new_port), GFP_KERNEL);\n\tif (!new_port)\n\t\treturn NULL;\t/* failure, out of memory */\n\t/* init port data */\n\tnew_port->addr.client = client->number;\n\tnew_port->addr.port = -1;\n\tnew_port->owner = THIS_MODULE;\n\tsprintf(new_port->name, \"port-%d\", num);\n\tsnd_use_lock_init(&new_port->use_lock);\n\tport_subs_info_init(&new_port->c_src);\n\tport_subs_info_init(&new_port->c_dest);\n\n\tnum = port >= 0 ? port : 0;\n\tmutex_lock(&client->ports_mutex);\n\twrite_lock_irqsave(&client->ports_lock, flags);\n\tlist_for_each_entry(p, &client->ports_list_head, list) {\n\t\tif (p->addr.port > num)\n\t\t\tbreak;\n\t\tif (port < 0) /* auto-probe mode */\n\t\t\tnum = p->addr.port + 1;\n\t}\n\t/* insert the new port */\n\tlist_add_tail(&new_port->list, &p->list);\n\tclient->num_ports++;\n\tnew_port->addr.port = num;\t/* store the port number in the port */\n\twrite_unlock_irqrestore(&client->ports_lock, flags);\n\tmutex_unlock(&client->ports_mutex);\n\tsprintf(new_port->name, \"port-%d\", num);\n\n\treturn new_port;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1394,
            "cve_id": "CVE-2017-16527",
            "code_snippet": "void snd_usb_mixer_disconnect(struct usb_mixer_interface *mixer)\n{\n\tusb_kill_urb(mixer->urb);\n\tusb_kill_urb(mixer->rc_urb);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1434,
            "cve_id": "CVE-2017-16939",
            "code_snippet": "static int xfrm_dump_policy(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct xfrm_policy_walk *walk = (struct xfrm_policy_walk *) &cb->args[1];\n\tstruct xfrm_dump_info info;\n\n\tBUILD_BUG_ON(sizeof(struct xfrm_policy_walk) >\n\t\t     sizeof(cb->args) - sizeof(cb->args[0]));\n\n\tinfo.in_skb = cb->skb;\n\tinfo.out_skb = skb;\n\tinfo.nlmsg_seq = cb->nlh->nlmsg_seq;\n\tinfo.nlmsg_flags = NLM_F_MULTI;\n\n\tif (!cb->args[0]) {\n\t\tcb->args[0] = 1;\n\t\txfrm_policy_walk_init(walk, XFRM_POLICY_TYPE_ANY);\n\t}\n\n\t(void) xfrm_policy_walk(net, walk, dump_one_policy, &info);\n\n\treturn skb->len;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1488,
            "cve_id": "CVE-2017-18218",
            "code_snippet": "static netdev_tx_t hns_nic_net_xmit(struct sk_buff *skb,\n\t\t\t\t    struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tint ret;\n\n\tassert(skb->queue_mapping < ndev->ae_handle->q_num);\n\tret = hns_nic_net_xmit_hw(ndev, skb,\n\t\t\t\t  &tx_ring_data(priv, skb->queue_mapping));\n\tif (ret == NETDEV_TX_OK) {\n\t\tnetif_trans_update(ndev);\n\t\tndev->stats.tx_bytes += skb->len;\n\t\tndev->stats.tx_packets++;\n\t}\n\treturn (netdev_tx_t)ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1530,
            "cve_id": "CVE-2017-2584",
            "code_snippet": "static int emulate_store_desc_ptr(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t  void (*get)(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t\t      struct desc_ptr *ptr))\n{\n\tstruct desc_ptr desc_ptr;\n\n\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\tctxt->op_bytes = 8;\n\tget(ctxt, &desc_ptr);\n\tif (ctxt->op_bytes == 2) {\n\t\tctxt->op_bytes = 4;\n\t\tdesc_ptr.address &= 0x00ffffff;\n\t}\n\t/* Disable writeback. */\n\tctxt->dst.type = OP_NONE;\n\treturn segmented_write(ctxt, ctxt->dst.addr.mem,\n\t\t\t       &desc_ptr, 2 + ctxt->op_bytes);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1569,
            "cve_id": "CVE-2017-6346",
            "code_snippet": "static void fanout_release(struct sock *sk)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f;\n\n\tf = po->fanout;\n\tif (!f)\n\t\treturn;\n\n\tmutex_lock(&fanout_mutex);\n\tpo->fanout = NULL;\n\n\tif (atomic_dec_and_test(&f->sk_ref)) {\n\t\tlist_del(&f->list);\n\t\tdev_remove_pack(&f->prot_hook);\n\t\tfanout_release_data(f);\n\t\tkfree(f);\n\t}\n\tmutex_unlock(&fanout_mutex);\n\n\tif (po->rollover)\n\t\tkfree_rcu(po->rollover, rcu);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1574,
            "cve_id": "CVE-2017-6874",
            "code_snippet": "static struct ucounts *get_ucounts(struct user_namespace *ns, kuid_t uid)\n{\n\tstruct hlist_head *hashent = ucounts_hashentry(ns, uid);\n\tstruct ucounts *ucounts, *new;\n\n\tspin_lock_irq(&ucounts_lock);\n\tucounts = find_ucounts(ns, uid, hashent);\n\tif (!ucounts) {\n\t\tspin_unlock_irq(&ucounts_lock);\n\n\t\tnew = kzalloc(sizeof(*new), GFP_KERNEL);\n\t\tif (!new)\n\t\t\treturn NULL;\n\n\t\tnew->ns = ns;\n\t\tnew->uid = uid;\n\t\tatomic_set(&new->count, 0);\n\n\t\tspin_lock_irq(&ucounts_lock);\n\t\tucounts = find_ucounts(ns, uid, hashent);\n\t\tif (ucounts) {\n\t\t\tkfree(new);\n\t\t} else {\n\t\t\thlist_add_head(&new->node, hashent);\n\t\t\tucounts = new;\n\t\t}\n\t}\n\tif (!atomic_add_unless(&ucounts->count, 1, INT_MAX))\n\t\tucounts = NULL;\n\tspin_unlock_irq(&ucounts_lock);\n\treturn ucounts;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1584,
            "cve_id": "CVE-2017-7374",
            "code_snippet": "int fscrypt_setup_filename(struct inode *dir, const struct qstr *iname,\n\t\t\t      int lookup, struct fscrypt_name *fname)\n{\n\tint ret = 0, bigname = 0;\n\n\tmemset(fname, 0, sizeof(struct fscrypt_name));\n\tfname->usr_fname = iname;\n\n\tif (!dir->i_sb->s_cop->is_encrypted(dir) ||\n\t\t\t\tfscrypt_is_dot_dotdot(iname)) {\n\t\tfname->disk_name.name = (unsigned char *)iname->name;\n\t\tfname->disk_name.len = iname->len;\n\t\treturn 0;\n\t}\n\tret = fscrypt_get_crypt_info(dir);\n\tif (ret && ret != -EOPNOTSUPP)\n\t\treturn ret;\n\n\tif (dir->i_crypt_info) {\n\t\tret = fscrypt_fname_alloc_buffer(dir, iname->len,\n\t\t\t\t\t\t\t&fname->crypto_buf);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = fname_encrypt(dir, iname, &fname->crypto_buf);\n\t\tif (ret)\n\t\t\tgoto errout;\n\t\tfname->disk_name.name = fname->crypto_buf.name;\n\t\tfname->disk_name.len = fname->crypto_buf.len;\n\t\treturn 0;\n\t}\n\tif (!lookup)\n\t\treturn -ENOKEY;\n\n\t/*\n\t * We don't have the key and we are doing a lookup; decode the\n\t * user-supplied name\n\t */\n\tif (iname->name[0] == '_')\n\t\tbigname = 1;\n\tif ((bigname && (iname->len != 33)) || (!bigname && (iname->len > 43)))\n\t\treturn -ENOENT;\n\n\tfname->crypto_buf.name = kmalloc(32, GFP_KERNEL);\n\tif (fname->crypto_buf.name == NULL)\n\t\treturn -ENOMEM;\n\n\tret = digest_decode(iname->name + bigname, iname->len - bigname,\n\t\t\t\tfname->crypto_buf.name);\n\tif (ret < 0) {\n\t\tret = -ENOENT;\n\t\tgoto errout;\n\t}\n\tfname->crypto_buf.len = ret;\n\tif (bigname) {\n\t\tmemcpy(&fname->hash, fname->crypto_buf.name, 4);\n\t\tmemcpy(&fname->minor_hash, fname->crypto_buf.name + 4, 4);\n\t} else {\n\t\tfname->disk_name.name = fname->crypto_buf.name;\n\t\tfname->disk_name.len = fname->crypto_buf.len;\n\t}\n\treturn 0;\n\nerrout:\n\tfscrypt_fname_free_buffer(&fname->crypto_buf);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1585,
            "cve_id": "CVE-2017-7374",
            "code_snippet": "int fscrypt_get_encryption_info(struct inode *inode)\n{\n\tstruct fscrypt_info *ci = inode->i_crypt_info;\n\n\tif (!ci ||\n\t\t(ci->ci_keyring_key &&\n\t\t (ci->ci_keyring_key->flags & ((1 << KEY_FLAG_INVALIDATED) |\n\t\t\t\t\t       (1 << KEY_FLAG_REVOKED) |\n\t\t\t\t\t       (1 << KEY_FLAG_DEAD)))))\n\t\treturn fscrypt_get_crypt_info(inode);\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1695,
            "cve_id": "CVE-2018-10876",
            "code_snippet": "struct inode *__ext4_new_inode(handle_t *handle, struct inode *dir,\n\t\t\t       umode_t mode, const struct qstr *qstr,\n\t\t\t       __u32 goal, uid_t *owner, __u32 i_flags,\n\t\t\t       int handle_type, unsigned int line_no,\n\t\t\t       int nblocks)\n{\n\tstruct super_block *sb;\n\tstruct buffer_head *inode_bitmap_bh = NULL;\n\tstruct buffer_head *group_desc_bh;\n\text4_group_t ngroups, group = 0;\n\tunsigned long ino = 0;\n\tstruct inode *inode;\n\tstruct ext4_group_desc *gdp = NULL;\n\tstruct ext4_inode_info *ei;\n\tstruct ext4_sb_info *sbi;\n\tint ret2, err;\n\tstruct inode *ret;\n\text4_group_t i;\n\text4_group_t flex_group;\n\tstruct ext4_group_info *grp;\n\tint encrypt = 0;\n\n\t/* Cannot create files in a deleted directory */\n\tif (!dir || !dir->i_nlink)\n\t\treturn ERR_PTR(-EPERM);\n\n\tsb = dir->i_sb;\n\tsbi = EXT4_SB(sb);\n\n\tif (unlikely(ext4_forced_shutdown(sbi)))\n\t\treturn ERR_PTR(-EIO);\n\n\tif ((ext4_encrypted_inode(dir) || DUMMY_ENCRYPTION_ENABLED(sbi)) &&\n\t    (S_ISREG(mode) || S_ISDIR(mode) || S_ISLNK(mode)) &&\n\t    !(i_flags & EXT4_EA_INODE_FL)) {\n\t\terr = fscrypt_get_encryption_info(dir);\n\t\tif (err)\n\t\t\treturn ERR_PTR(err);\n\t\tif (!fscrypt_has_encryption_key(dir))\n\t\t\treturn ERR_PTR(-ENOKEY);\n\t\tencrypt = 1;\n\t}\n\n\tif (!handle && sbi->s_journal && !(i_flags & EXT4_EA_INODE_FL)) {\n#ifdef CONFIG_EXT4_FS_POSIX_ACL\n\t\tstruct posix_acl *p = get_acl(dir, ACL_TYPE_DEFAULT);\n\n\t\tif (IS_ERR(p))\n\t\t\treturn ERR_CAST(p);\n\t\tif (p) {\n\t\t\tint acl_size = p->a_count * sizeof(ext4_acl_entry);\n\n\t\t\tnblocks += (S_ISDIR(mode) ? 2 : 1) *\n\t\t\t\t__ext4_xattr_set_credits(sb, NULL /* inode */,\n\t\t\t\t\tNULL /* block_bh */, acl_size,\n\t\t\t\t\ttrue /* is_create */);\n\t\t\tposix_acl_release(p);\n\t\t}\n#endif\n\n#ifdef CONFIG_SECURITY\n\t\t{\n\t\t\tint num_security_xattrs = 1;\n\n#ifdef CONFIG_INTEGRITY\n\t\t\tnum_security_xattrs++;\n#endif\n\t\t\t/*\n\t\t\t * We assume that security xattrs are never\n\t\t\t * more than 1k.  In practice they are under\n\t\t\t * 128 bytes.\n\t\t\t */\n\t\t\tnblocks += num_security_xattrs *\n\t\t\t\t__ext4_xattr_set_credits(sb, NULL /* inode */,\n\t\t\t\t\tNULL /* block_bh */, 1024,\n\t\t\t\t\ttrue /* is_create */);\n\t\t}\n#endif\n\t\tif (encrypt)\n\t\t\tnblocks += __ext4_xattr_set_credits(sb,\n\t\t\t\t\tNULL /* inode */, NULL /* block_bh */,\n\t\t\t\t\tFSCRYPT_SET_CONTEXT_MAX_SIZE,\n\t\t\t\t\ttrue /* is_create */);\n\t}\n\n\tngroups = ext4_get_groups_count(sb);\n\ttrace_ext4_request_inode(dir, mode);\n\tinode = new_inode(sb);\n\tif (!inode)\n\t\treturn ERR_PTR(-ENOMEM);\n\tei = EXT4_I(inode);\n\n\t/*\n\t * Initialize owners and quota early so that we don't have to account\n\t * for quota initialization worst case in standard inode creating\n\t * transaction\n\t */\n\tif (owner) {\n\t\tinode->i_mode = mode;\n\t\ti_uid_write(inode, owner[0]);\n\t\ti_gid_write(inode, owner[1]);\n\t} else if (test_opt(sb, GRPID)) {\n\t\tinode->i_mode = mode;\n\t\tinode->i_uid = current_fsuid();\n\t\tinode->i_gid = dir->i_gid;\n\t} else\n\t\tinode_init_owner(inode, dir, mode);\n\n\tif (ext4_has_feature_project(sb) &&\n\t    ext4_test_inode_flag(dir, EXT4_INODE_PROJINHERIT))\n\t\tei->i_projid = EXT4_I(dir)->i_projid;\n\telse\n\t\tei->i_projid = make_kprojid(&init_user_ns, EXT4_DEF_PROJID);\n\n\terr = dquot_initialize(inode);\n\tif (err)\n\t\tgoto out;\n\n\tif (!goal)\n\t\tgoal = sbi->s_inode_goal;\n\n\tif (goal && goal <= le32_to_cpu(sbi->s_es->s_inodes_count)) {\n\t\tgroup = (goal - 1) / EXT4_INODES_PER_GROUP(sb);\n\t\tino = (goal - 1) % EXT4_INODES_PER_GROUP(sb);\n\t\tret2 = 0;\n\t\tgoto got_group;\n\t}\n\n\tif (S_ISDIR(mode))\n\t\tret2 = find_group_orlov(sb, dir, &group, mode, qstr);\n\telse\n\t\tret2 = find_group_other(sb, dir, &group, mode);\n\ngot_group:\n\tEXT4_I(dir)->i_last_alloc_group = group;\n\terr = -ENOSPC;\n\tif (ret2 == -1)\n\t\tgoto out;\n\n\t/*\n\t * Normally we will only go through one pass of this loop,\n\t * unless we get unlucky and it turns out the group we selected\n\t * had its last inode grabbed by someone else.\n\t */\n\tfor (i = 0; i < ngroups; i++, ino = 0) {\n\t\terr = -EIO;\n\n\t\tgdp = ext4_get_group_desc(sb, group, &group_desc_bh);\n\t\tif (!gdp)\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * Check free inodes count before loading bitmap.\n\t\t */\n\t\tif (ext4_free_inodes_count(sb, gdp) == 0)\n\t\t\tgoto next_group;\n\n\t\tgrp = ext4_get_group_info(sb, group);\n\t\t/* Skip groups with already-known suspicious inode tables */\n\t\tif (EXT4_MB_GRP_IBITMAP_CORRUPT(grp))\n\t\t\tgoto next_group;\n\n\t\tbrelse(inode_bitmap_bh);\n\t\tinode_bitmap_bh = ext4_read_inode_bitmap(sb, group);\n\t\t/* Skip groups with suspicious inode tables */\n\t\tif (EXT4_MB_GRP_IBITMAP_CORRUPT(grp) ||\n\t\t    IS_ERR(inode_bitmap_bh)) {\n\t\t\tinode_bitmap_bh = NULL;\n\t\t\tgoto next_group;\n\t\t}\n\nrepeat_in_this_group:\n\t\tret2 = find_inode_bit(sb, group, inode_bitmap_bh, &ino);\n\t\tif (!ret2)\n\t\t\tgoto next_group;\n\n\t\tif (group == 0 && (ino + 1) < EXT4_FIRST_INO(sb)) {\n\t\t\text4_error(sb, \"reserved inode found cleared - \"\n\t\t\t\t   \"inode=%lu\", ino + 1);\n\t\t\text4_mark_group_bitmap_corrupted(sb, group,\n\t\t\t\t\tEXT4_GROUP_INFO_IBITMAP_CORRUPT);\n\t\t\tgoto next_group;\n\t\t}\n\n\t\tif (!handle) {\n\t\t\tBUG_ON(nblocks <= 0);\n\t\t\thandle = __ext4_journal_start_sb(dir->i_sb, line_no,\n\t\t\t\t\t\t\t handle_type, nblocks,\n\t\t\t\t\t\t\t 0);\n\t\t\tif (IS_ERR(handle)) {\n\t\t\t\terr = PTR_ERR(handle);\n\t\t\t\text4_std_error(sb, err);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tBUFFER_TRACE(inode_bitmap_bh, \"get_write_access\");\n\t\terr = ext4_journal_get_write_access(handle, inode_bitmap_bh);\n\t\tif (err) {\n\t\t\text4_std_error(sb, err);\n\t\t\tgoto out;\n\t\t}\n\t\text4_lock_group(sb, group);\n\t\tret2 = ext4_test_and_set_bit(ino, inode_bitmap_bh->b_data);\n\t\tif (ret2) {\n\t\t\t/* Someone already took the bit. Repeat the search\n\t\t\t * with lock held.\n\t\t\t */\n\t\t\tret2 = find_inode_bit(sb, group, inode_bitmap_bh, &ino);\n\t\t\tif (ret2) {\n\t\t\t\text4_set_bit(ino, inode_bitmap_bh->b_data);\n\t\t\t\tret2 = 0;\n\t\t\t} else {\n\t\t\t\tret2 = 1; /* we didn't grab the inode */\n\t\t\t}\n\t\t}\n\t\text4_unlock_group(sb, group);\n\t\tino++;\t\t/* the inode bitmap is zero-based */\n\t\tif (!ret2)\n\t\t\tgoto got; /* we grabbed the inode! */\n\n\t\tif (ino < EXT4_INODES_PER_GROUP(sb))\n\t\t\tgoto repeat_in_this_group;\nnext_group:\n\t\tif (++group == ngroups)\n\t\t\tgroup = 0;\n\t}\n\terr = -ENOSPC;\n\tgoto out;\n\ngot:\n\tBUFFER_TRACE(inode_bitmap_bh, \"call ext4_handle_dirty_metadata\");\n\terr = ext4_handle_dirty_metadata(handle, NULL, inode_bitmap_bh);\n\tif (err) {\n\t\text4_std_error(sb, err);\n\t\tgoto out;\n\t}\n\n\tBUFFER_TRACE(group_desc_bh, \"get_write_access\");\n\terr = ext4_journal_get_write_access(handle, group_desc_bh);\n\tif (err) {\n\t\text4_std_error(sb, err);\n\t\tgoto out;\n\t}\n\n\t/* We may have to initialize the block bitmap if it isn't already */\n\tif (ext4_has_group_desc_csum(sb) &&\n\t    gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) {\n\t\tstruct buffer_head *block_bitmap_bh;\n\n\t\tblock_bitmap_bh = ext4_read_block_bitmap(sb, group);\n\t\tif (IS_ERR(block_bitmap_bh)) {\n\t\t\terr = PTR_ERR(block_bitmap_bh);\n\t\t\tgoto out;\n\t\t}\n\t\tBUFFER_TRACE(block_bitmap_bh, \"get block bitmap access\");\n\t\terr = ext4_journal_get_write_access(handle, block_bitmap_bh);\n\t\tif (err) {\n\t\t\tbrelse(block_bitmap_bh);\n\t\t\text4_std_error(sb, err);\n\t\t\tgoto out;\n\t\t}\n\n\t\tBUFFER_TRACE(block_bitmap_bh, \"dirty block bitmap\");\n\t\terr = ext4_handle_dirty_metadata(handle, NULL, block_bitmap_bh);\n\n\t\t/* recheck and clear flag under lock if we still need to */\n\t\text4_lock_group(sb, group);\n\t\tif (gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) {\n\t\t\tgdp->bg_flags &= cpu_to_le16(~EXT4_BG_BLOCK_UNINIT);\n\t\t\text4_free_group_clusters_set(sb, gdp,\n\t\t\t\text4_free_clusters_after_init(sb, group, gdp));\n\t\t\text4_block_bitmap_csum_set(sb, group, gdp,\n\t\t\t\t\t\t   block_bitmap_bh);\n\t\t\text4_group_desc_csum_set(sb, group, gdp);\n\t\t}\n\t\text4_unlock_group(sb, group);\n\t\tbrelse(block_bitmap_bh);\n\n\t\tif (err) {\n\t\t\text4_std_error(sb, err);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* Update the relevant bg descriptor fields */\n\tif (ext4_has_group_desc_csum(sb)) {\n\t\tint free;\n\t\tstruct ext4_group_info *grp = ext4_get_group_info(sb, group);\n\n\t\tdown_read(&grp->alloc_sem); /* protect vs itable lazyinit */\n\t\text4_lock_group(sb, group); /* while we modify the bg desc */\n\t\tfree = EXT4_INODES_PER_GROUP(sb) -\n\t\t\text4_itable_unused_count(sb, gdp);\n\t\tif (gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_UNINIT)) {\n\t\t\tgdp->bg_flags &= cpu_to_le16(~EXT4_BG_INODE_UNINIT);\n\t\t\tfree = 0;\n\t\t}\n\t\t/*\n\t\t * Check the relative inode number against the last used\n\t\t * relative inode number in this group. if it is greater\n\t\t * we need to update the bg_itable_unused count\n\t\t */\n\t\tif (ino > free)\n\t\t\text4_itable_unused_set(sb, gdp,\n\t\t\t\t\t(EXT4_INODES_PER_GROUP(sb) - ino));\n\t\tup_read(&grp->alloc_sem);\n\t} else {\n\t\text4_lock_group(sb, group);\n\t}\n\n\text4_free_inodes_set(sb, gdp, ext4_free_inodes_count(sb, gdp) - 1);\n\tif (S_ISDIR(mode)) {\n\t\text4_used_dirs_set(sb, gdp, ext4_used_dirs_count(sb, gdp) + 1);\n\t\tif (sbi->s_log_groups_per_flex) {\n\t\t\text4_group_t f = ext4_flex_group(sbi, group);\n\n\t\t\tatomic_inc(&sbi->s_flex_groups[f].used_dirs);\n\t\t}\n\t}\n\tif (ext4_has_group_desc_csum(sb)) {\n\t\text4_inode_bitmap_csum_set(sb, group, gdp, inode_bitmap_bh,\n\t\t\t\t\t   EXT4_INODES_PER_GROUP(sb) / 8);\n\t\text4_group_desc_csum_set(sb, group, gdp);\n\t}\n\text4_unlock_group(sb, group);\n\n\tBUFFER_TRACE(group_desc_bh, \"call ext4_handle_dirty_metadata\");\n\terr = ext4_handle_dirty_metadata(handle, NULL, group_desc_bh);\n\tif (err) {\n\t\text4_std_error(sb, err);\n\t\tgoto out;\n\t}\n\n\tpercpu_counter_dec(&sbi->s_freeinodes_counter);\n\tif (S_ISDIR(mode))\n\t\tpercpu_counter_inc(&sbi->s_dirs_counter);\n\n\tif (sbi->s_log_groups_per_flex) {\n\t\tflex_group = ext4_flex_group(sbi, group);\n\t\tatomic_dec(&sbi->s_flex_groups[flex_group].free_inodes);\n\t}\n\n\tinode->i_ino = ino + group * EXT4_INODES_PER_GROUP(sb);\n\t/* This is the optimal IO size (for stat), not the fs block size */\n\tinode->i_blocks = 0;\n\tinode->i_mtime = inode->i_atime = inode->i_ctime = ei->i_crtime =\n\t\t\t\t\t\t       current_time(inode);\n\n\tmemset(ei->i_data, 0, sizeof(ei->i_data));\n\tei->i_dir_start_lookup = 0;\n\tei->i_disksize = 0;\n\n\t/* Don't inherit extent flag from directory, amongst others. */\n\tei->i_flags =\n\t\text4_mask_flags(mode, EXT4_I(dir)->i_flags & EXT4_FL_INHERITED);\n\tei->i_flags |= i_flags;\n\tei->i_file_acl = 0;\n\tei->i_dtime = 0;\n\tei->i_block_group = group;\n\tei->i_last_alloc_group = ~0;\n\n\text4_set_inode_flags(inode);\n\tif (IS_DIRSYNC(inode))\n\t\text4_handle_sync(handle);\n\tif (insert_inode_locked(inode) < 0) {\n\t\t/*\n\t\t * Likely a bitmap corruption causing inode to be allocated\n\t\t * twice.\n\t\t */\n\t\terr = -EIO;\n\t\text4_error(sb, \"failed to insert inode %lu: doubly allocated?\",\n\t\t\t   inode->i_ino);\n\t\text4_mark_group_bitmap_corrupted(sb, group,\n\t\t\t\t\tEXT4_GROUP_INFO_IBITMAP_CORRUPT);\n\t\tgoto out;\n\t}\n\tinode->i_generation = prandom_u32();\n\n\t/* Precompute checksum seed for inode metadata */\n\tif (ext4_has_metadata_csum(sb)) {\n\t\t__u32 csum;\n\t\t__le32 inum = cpu_to_le32(inode->i_ino);\n\t\t__le32 gen = cpu_to_le32(inode->i_generation);\n\t\tcsum = ext4_chksum(sbi, sbi->s_csum_seed, (__u8 *)&inum,\n\t\t\t\t   sizeof(inum));\n\t\tei->i_csum_seed = ext4_chksum(sbi, csum, (__u8 *)&gen,\n\t\t\t\t\t      sizeof(gen));\n\t}\n\n\text4_clear_state_flags(ei); /* Only relevant on 32-bit archs */\n\text4_set_inode_state(inode, EXT4_STATE_NEW);\n\n\tei->i_extra_isize = sbi->s_want_extra_isize;\n\tei->i_inline_off = 0;\n\tif (ext4_has_feature_inline_data(sb))\n\t\text4_set_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA);\n\tret = inode;\n\terr = dquot_alloc_inode(inode);\n\tif (err)\n\t\tgoto fail_drop;\n\n\t/*\n\t * Since the encryption xattr will always be unique, create it first so\n\t * that it's less likely to end up in an external xattr block and\n\t * prevent its deduplication.\n\t */\n\tif (encrypt) {\n\t\terr = fscrypt_inherit_context(dir, inode, handle, true);\n\t\tif (err)\n\t\t\tgoto fail_free_drop;\n\t}\n\n\tif (!(ei->i_flags & EXT4_EA_INODE_FL)) {\n\t\terr = ext4_init_acl(handle, inode, dir);\n\t\tif (err)\n\t\t\tgoto fail_free_drop;\n\n\t\terr = ext4_init_security(handle, inode, dir, qstr);\n\t\tif (err)\n\t\t\tgoto fail_free_drop;\n\t}\n\n\tif (ext4_has_feature_extents(sb)) {\n\t\t/* set extent flag only for directory, file and normal symlink*/\n\t\tif (S_ISDIR(mode) || S_ISREG(mode) || S_ISLNK(mode)) {\n\t\t\text4_set_inode_flag(inode, EXT4_INODE_EXTENTS);\n\t\t\text4_ext_tree_init(handle, inode);\n\t\t}\n\t}\n\n\tif (ext4_handle_valid(handle)) {\n\t\tei->i_sync_tid = handle->h_transaction->t_tid;\n\t\tei->i_datasync_tid = handle->h_transaction->t_tid;\n\t}\n\n\terr = ext4_mark_inode_dirty(handle, inode);\n\tif (err) {\n\t\text4_std_error(sb, err);\n\t\tgoto fail_free_drop;\n\t}\n\n\text4_debug(\"allocating inode %lu\\n\", inode->i_ino);\n\ttrace_ext4_allocate_inode(inode, dir, mode);\n\tbrelse(inode_bitmap_bh);\n\treturn ret;\n\nfail_free_drop:\n\tdquot_free_inode(inode);\nfail_drop:\n\tclear_nlink(inode);\n\tunlock_new_inode(inode);\nout:\n\tdquot_drop(inode);\n\tinode->i_flags |= S_NOQUOTA;\n\tiput(inode);\n\tbrelse(inode_bitmap_bh);\n\treturn ERR_PTR(err);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1696,
            "cve_id": "CVE-2018-10876",
            "code_snippet": "static struct buffer_head *\next4_read_inode_bitmap(struct super_block *sb, ext4_group_t block_group)\n{\n\tstruct ext4_group_desc *desc;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct buffer_head *bh = NULL;\n\text4_fsblk_t bitmap_blk;\n\tint err;\n\n\tdesc = ext4_get_group_desc(sb, block_group, NULL);\n\tif (!desc)\n\t\treturn ERR_PTR(-EFSCORRUPTED);\n\n\tbitmap_blk = ext4_inode_bitmap(sb, desc);\n\tif ((bitmap_blk <= le32_to_cpu(sbi->s_es->s_first_data_block)) ||\n\t    (bitmap_blk >= ext4_blocks_count(sbi->s_es))) {\n\t\text4_error(sb, \"Invalid inode bitmap blk %llu in \"\n\t\t\t   \"block_group %u\", bitmap_blk, block_group);\n\t\text4_mark_group_bitmap_corrupted(sb, block_group,\n\t\t\t\t\tEXT4_GROUP_INFO_IBITMAP_CORRUPT);\n\t\treturn ERR_PTR(-EFSCORRUPTED);\n\t}\n\tbh = sb_getblk(sb, bitmap_blk);\n\tif (unlikely(!bh)) {\n\t\text4_error(sb, \"Cannot read inode bitmap - \"\n\t\t\t    \"block_group = %u, inode_bitmap = %llu\",\n\t\t\t    block_group, bitmap_blk);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tif (bitmap_uptodate(bh))\n\t\tgoto verify;\n\n\tlock_buffer(bh);\n\tif (bitmap_uptodate(bh)) {\n\t\tunlock_buffer(bh);\n\t\tgoto verify;\n\t}\n\n\text4_lock_group(sb, block_group);\n\tif (desc->bg_flags & cpu_to_le16(EXT4_BG_INODE_UNINIT)) {\n\t\tmemset(bh->b_data, 0, (EXT4_INODES_PER_GROUP(sb) + 7) / 8);\n\t\text4_mark_bitmap_end(EXT4_INODES_PER_GROUP(sb),\n\t\t\t\t     sb->s_blocksize * 8, bh->b_data);\n\t\tset_bitmap_uptodate(bh);\n\t\tset_buffer_uptodate(bh);\n\t\tset_buffer_verified(bh);\n\t\text4_unlock_group(sb, block_group);\n\t\tunlock_buffer(bh);\n\t\treturn bh;\n\t}\n\text4_unlock_group(sb, block_group);\n\n\tif (buffer_uptodate(bh)) {\n\t\t/*\n\t\t * if not uninit if bh is uptodate,\n\t\t * bitmap is also uptodate\n\t\t */\n\t\tset_bitmap_uptodate(bh);\n\t\tunlock_buffer(bh);\n\t\tgoto verify;\n\t}\n\t/*\n\t * submit the buffer_head for reading\n\t */\n\ttrace_ext4_load_inode_bitmap(sb, block_group);\n\tbh->b_end_io = ext4_end_bitmap_read;\n\tget_bh(bh);\n\tsubmit_bh(REQ_OP_READ, REQ_META | REQ_PRIO, bh);\n\twait_on_buffer(bh);\n\tif (!buffer_uptodate(bh)) {\n\t\tput_bh(bh);\n\t\text4_error(sb, \"Cannot read inode bitmap - \"\n\t\t\t   \"block_group = %u, inode_bitmap = %llu\",\n\t\t\t   block_group, bitmap_blk);\n\t\text4_mark_group_bitmap_corrupted(sb, block_group,\n\t\t\t\tEXT4_GROUP_INFO_IBITMAP_CORRUPT);\n\t\treturn ERR_PTR(-EIO);\n\t}\n\nverify:\n\terr = ext4_validate_inode_bitmap(sb, desc, block_group, bh);\n\tif (err)\n\t\tgoto out;\n\treturn bh;\nout:\n\tput_bh(bh);\n\treturn ERR_PTR(err);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1712,
            "cve_id": "CVE-2018-10902",
            "code_snippet": "int snd_rawmidi_output_params(struct snd_rawmidi_substream *substream,\n\t\t\t      struct snd_rawmidi_params * params)\n{\n\tchar *newbuf;\n\tstruct snd_rawmidi_runtime *runtime = substream->runtime;\n\t\n\tif (substream->append && substream->use_count > 1)\n\t\treturn -EBUSY;\n\tsnd_rawmidi_drain_output(substream);\n\tif (params->buffer_size < 32 || params->buffer_size > 1024L * 1024L) {\n\t\treturn -EINVAL;\n\t}\n\tif (params->avail_min < 1 || params->avail_min > params->buffer_size) {\n\t\treturn -EINVAL;\n\t}\n\tif (params->buffer_size != runtime->buffer_size) {\n\t\tnewbuf = krealloc(runtime->buffer, params->buffer_size,\n\t\t\t\t  GFP_KERNEL);\n\t\tif (!newbuf)\n\t\t\treturn -ENOMEM;\n\t\truntime->buffer = newbuf;\n\t\truntime->buffer_size = params->buffer_size;\n\t\truntime->avail = runtime->buffer_size;\n\t}\n\truntime->avail_min = params->avail_min;\n\tsubstream->active_sensing = !params->no_active_sensing;\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1786,
            "cve_id": "CVE-2018-14625",
            "code_snippet": "static int\nvhost_transport_cancel_pkt(struct vsock_sock *vsk)\n{\n\tstruct vhost_vsock *vsock;\n\tstruct virtio_vsock_pkt *pkt, *n;\n\tint cnt = 0;\n\tLIST_HEAD(freeme);\n\n\t/* Find the vhost_vsock according to guest context id  */\n\tvsock = vhost_vsock_get(vsk->remote_addr.svm_cid);\n\tif (!vsock)\n\t\treturn -ENODEV;\n\n\tspin_lock_bh(&vsock->send_pkt_list_lock);\n\tlist_for_each_entry_safe(pkt, n, &vsock->send_pkt_list, list) {\n\t\tif (pkt->vsk != vsk)\n\t\t\tcontinue;\n\t\tlist_move(&pkt->list, &freeme);\n\t}\n\tspin_unlock_bh(&vsock->send_pkt_list_lock);\n\n\tlist_for_each_entry_safe(pkt, n, &freeme, list) {\n\t\tif (pkt->reply)\n\t\t\tcnt++;\n\t\tlist_del(&pkt->list);\n\t\tvirtio_transport_free_pkt(pkt);\n\t}\n\n\tif (cnt) {\n\t\tstruct vhost_virtqueue *tx_vq = &vsock->vqs[VSOCK_VQ_TX];\n\t\tint new_cnt;\n\n\t\tnew_cnt = atomic_sub_return(cnt, &vsock->queued_replies);\n\t\tif (new_cnt + cnt >= tx_vq->num && new_cnt < tx_vq->num)\n\t\t\tvhost_poll_queue(&tx_vq->poll);\n\t}\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1787,
            "cve_id": "CVE-2018-14625",
            "code_snippet": "static int\nvhost_transport_send_pkt(struct virtio_vsock_pkt *pkt)\n{\n\tstruct vhost_vsock *vsock;\n\tint len = pkt->len;\n\n\t/* Find the vhost_vsock according to guest context id  */\n\tvsock = vhost_vsock_get(le64_to_cpu(pkt->hdr.dst_cid));\n\tif (!vsock) {\n\t\tvirtio_transport_free_pkt(pkt);\n\t\treturn -ENODEV;\n\t}\n\n\tif (pkt->reply)\n\t\tatomic_inc(&vsock->queued_replies);\n\n\tspin_lock_bh(&vsock->send_pkt_list_lock);\n\tlist_add_tail(&pkt->list, &vsock->send_pkt_list);\n\tspin_unlock_bh(&vsock->send_pkt_list_lock);\n\n\tvhost_work_queue(&vsock->dev, &vsock->send_pkt_work);\n\treturn len;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1798,
            "cve_id": "CVE-2018-14734",
            "code_snippet": "static struct ucma_multicast* ucma_alloc_multicast(struct ucma_context *ctx)\n{\n\tstruct ucma_multicast *mc;\n\n\tmc = kzalloc(sizeof(*mc), GFP_KERNEL);\n\tif (!mc)\n\t\treturn NULL;\n\n\tmutex_lock(&mut);\n\tmc->id = idr_alloc(&multicast_idr, mc, 0, 0, GFP_KERNEL);\n\tmutex_unlock(&mut);\n\tif (mc->id < 0)\n\t\tgoto error;\n\n\tmc->ctx = ctx;\n\tlist_add_tail(&mc->list, &ctx->mc_list);\n\treturn mc;\n\nerror:\n\tkfree(mc);\n\treturn NULL;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1815,
            "cve_id": "CVE-2018-16884",
            "code_snippet": "int\nbc_svc_process(struct svc_serv *serv, struct rpc_rqst *req,\n\t       struct svc_rqst *rqstp)\n{\n\tstruct kvec\t*argv = &rqstp->rq_arg.head[0];\n\tstruct kvec\t*resv = &rqstp->rq_res.head[0];\n\tstruct rpc_task *task;\n\tint proc_error;\n\tint error;\n\n\tdprintk(\"svc: %s(%p)\\n\", __func__, req);\n\n\t/* Build the svc_rqst used by the common processing routine */\n\trqstp->rq_xprt = serv->sv_bc_xprt;\n\trqstp->rq_xid = req->rq_xid;\n\trqstp->rq_prot = req->rq_xprt->prot;\n\trqstp->rq_server = serv;\n\n\trqstp->rq_addrlen = sizeof(req->rq_xprt->addr);\n\tmemcpy(&rqstp->rq_addr, &req->rq_xprt->addr, rqstp->rq_addrlen);\n\tmemcpy(&rqstp->rq_arg, &req->rq_rcv_buf, sizeof(rqstp->rq_arg));\n\tmemcpy(&rqstp->rq_res, &req->rq_snd_buf, sizeof(rqstp->rq_res));\n\n\t/* Adjust the argument buffer length */\n\trqstp->rq_arg.len = req->rq_private_buf.len;\n\tif (rqstp->rq_arg.len <= rqstp->rq_arg.head[0].iov_len) {\n\t\trqstp->rq_arg.head[0].iov_len = rqstp->rq_arg.len;\n\t\trqstp->rq_arg.page_len = 0;\n\t} else if (rqstp->rq_arg.len <= rqstp->rq_arg.head[0].iov_len +\n\t\t\trqstp->rq_arg.page_len)\n\t\trqstp->rq_arg.page_len = rqstp->rq_arg.len -\n\t\t\trqstp->rq_arg.head[0].iov_len;\n\telse\n\t\trqstp->rq_arg.len = rqstp->rq_arg.head[0].iov_len +\n\t\t\trqstp->rq_arg.page_len;\n\n\t/* reset result send buffer \"put\" position */\n\tresv->iov_len = 0;\n\n\t/*\n\t * Skip the next two words because they've already been\n\t * processed in the transport\n\t */\n\tsvc_getu32(argv);\t/* XID */\n\tsvc_getnl(argv);\t/* CALLDIR */\n\n\t/* Parse and execute the bc call */\n\tproc_error = svc_process_common(rqstp, argv, resv);\n\n\tatomic_inc(&req->rq_xprt->bc_free_slots);\n\tif (!proc_error) {\n\t\t/* Processing error: drop the request */\n\t\txprt_free_bc_request(req);\n\t\treturn 0;\n\t}\n\n\t/* Finally, send the reply synchronously */\n\tmemcpy(&req->rq_snd_buf, &rqstp->rq_res, sizeof(req->rq_snd_buf));\n\ttask = rpc_run_bc_task(req);\n\tif (IS_ERR(task)) {\n\t\terror = PTR_ERR(task);\n\t\tgoto out;\n\t}\n\n\tWARN_ON_ONCE(atomic_read(&task->tk_count) != 1);\n\terror = task->tk_status;\n\trpc_put_task(task);\n\nout:\n\tdprintk(\"svc: %s(), error=%d\\n\", __func__, error);\n\treturn error;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1819,
            "cve_id": "CVE-2018-17182",
            "code_snippet": "void dump_mm(const struct mm_struct *mm)\n{\n\tpr_emerg(\"mm %px mmap %px seqnum %d task_size %lu\\n\"\n#ifdef CONFIG_MMU\n\t\t\"get_unmapped_area %px\\n\"\n#endif\n\t\t\"mmap_base %lu mmap_legacy_base %lu highest_vm_end %lu\\n\"\n\t\t\"pgd %px mm_users %d mm_count %d pgtables_bytes %lu map_count %d\\n\"\n\t\t\"hiwater_rss %lx hiwater_vm %lx total_vm %lx locked_vm %lx\\n\"\n\t\t\"pinned_vm %lx data_vm %lx exec_vm %lx stack_vm %lx\\n\"\n\t\t\"start_code %lx end_code %lx start_data %lx end_data %lx\\n\"\n\t\t\"start_brk %lx brk %lx start_stack %lx\\n\"\n\t\t\"arg_start %lx arg_end %lx env_start %lx env_end %lx\\n\"\n\t\t\"binfmt %px flags %lx core_state %px\\n\"\n#ifdef CONFIG_AIO\n\t\t\"ioctx_table %px\\n\"\n#endif\n#ifdef CONFIG_MEMCG\n\t\t\"owner %px \"\n#endif\n\t\t\"exe_file %px\\n\"\n#ifdef CONFIG_MMU_NOTIFIER\n\t\t\"mmu_notifier_mm %px\\n\"\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\t\t\"numa_next_scan %lu numa_scan_offset %lu numa_scan_seq %d\\n\"\n#endif\n\t\t\"tlb_flush_pending %d\\n\"\n\t\t\"def_flags: %#lx(%pGv)\\n\",\n\n\t\tmm, mm->mmap, mm->vmacache_seqnum, mm->task_size,\n#ifdef CONFIG_MMU\n\t\tmm->get_unmapped_area,\n#endif\n\t\tmm->mmap_base, mm->mmap_legacy_base, mm->highest_vm_end,\n\t\tmm->pgd, atomic_read(&mm->mm_users),\n\t\tatomic_read(&mm->mm_count),\n\t\tmm_pgtables_bytes(mm),\n\t\tmm->map_count,\n\t\tmm->hiwater_rss, mm->hiwater_vm, mm->total_vm, mm->locked_vm,\n\t\tmm->pinned_vm, mm->data_vm, mm->exec_vm, mm->stack_vm,\n\t\tmm->start_code, mm->end_code, mm->start_data, mm->end_data,\n\t\tmm->start_brk, mm->brk, mm->start_stack,\n\t\tmm->arg_start, mm->arg_end, mm->env_start, mm->env_end,\n\t\tmm->binfmt, mm->flags, mm->core_state,\n#ifdef CONFIG_AIO\n\t\tmm->ioctx_table,\n#endif\n#ifdef CONFIG_MEMCG\n\t\tmm->owner,\n#endif\n\t\tmm->exe_file,\n#ifdef CONFIG_MMU_NOTIFIER\n\t\tmm->mmu_notifier_mm,\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\t\tmm->numa_next_scan, mm->numa_scan_offset, mm->numa_scan_seq,\n#endif\n\t\tatomic_read(&mm->tlb_flush_pending),\n\t\tmm->def_flags, &mm->def_flags\n\t);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1875,
            "cve_id": "CVE-2018-20836",
            "code_snippet": "static void smp_task_done(struct sas_task *task)\n{\n\tif (!del_timer(&task->slow_task->timer))\n\t\treturn;\n\tcomplete(&task->slow_task->completion);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1883,
            "cve_id": "CVE-2018-20976",
            "code_snippet": "static long\nxfs_fs_nr_cached_objects(\n\tstruct super_block\t*sb,\n\tstruct shrink_control\t*sc)\n{\n\treturn xfs_reclaim_inodes_count(XFS_M(sb));\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1969,
            "cve_id": "CVE-2018-9465",
            "code_snippet": "static void task_fd_install(\n\tstruct binder_proc *proc, unsigned int fd, struct file *file)\n{\n\tif (proc->files)\n\t\t__fd_install(proc->files, fd, file);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1970,
            "cve_id": "CVE-2018-9465",
            "code_snippet": "static int binder_open(struct inode *nodp, struct file *filp)\n{\n\tstruct binder_proc *proc;\n\tstruct binder_device *binder_dev;\n\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE, \"binder_open: %d:%d\\n\",\n\t\t     current->group_leader->pid, current->pid);\n\n\tproc = kzalloc(sizeof(*proc), GFP_KERNEL);\n\tif (proc == NULL)\n\t\treturn -ENOMEM;\n\tspin_lock_init(&proc->inner_lock);\n\tspin_lock_init(&proc->outer_lock);\n\tget_task_struct(current->group_leader);\n\tproc->tsk = current->group_leader;\n\tINIT_LIST_HEAD(&proc->todo);\n\tproc->default_priority = task_nice(current);\n\tbinder_dev = container_of(filp->private_data, struct binder_device,\n\t\t\t\t  miscdev);\n\tproc->context = &binder_dev->context;\n\tbinder_alloc_init(&proc->alloc);\n\n\tbinder_stats_created(BINDER_STAT_PROC);\n\tproc->pid = current->group_leader->pid;\n\tINIT_LIST_HEAD(&proc->delivered_death);\n\tINIT_LIST_HEAD(&proc->waiting_threads);\n\tfilp->private_data = proc;\n\n\tmutex_lock(&binder_procs_lock);\n\thlist_add_head(&proc->proc_node, &binder_procs);\n\tmutex_unlock(&binder_procs_lock);\n\n\tif (binder_debugfs_dir_entry_proc) {\n\t\tchar strbuf[11];\n\n\t\tsnprintf(strbuf, sizeof(strbuf), \"%u\", proc->pid);\n\t\t/*\n\t\t * proc debug entries are shared between contexts, so\n\t\t * this will fail if the process tries to open the driver\n\t\t * again with a different context. The priting code will\n\t\t * anyway print all contexts that a given PID has, so this\n\t\t * is not a problem.\n\t\t */\n\t\tproc->debugfs_entry = debugfs_create_file(strbuf, S_IRUGO,\n\t\t\tbinder_debugfs_dir_entry_proc,\n\t\t\t(void *)(unsigned long)proc->pid,\n\t\t\t&binder_proc_fops);\n\t}\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1986,
            "cve_id": "CVE-2019-10125",
            "code_snippet": "static int aio_fsync(struct fsync_iocb *req, const struct iocb *iocb,\n\t\t     bool datasync)\n{\n\tif (unlikely(iocb->aio_buf || iocb->aio_offset || iocb->aio_nbytes ||\n\t\t\tiocb->aio_rw_flags))\n\t\treturn -EINVAL;\n\n\treq->file = fget(iocb->aio_fildes);\n\tif (unlikely(!req->file))\n\t\treturn -EBADF;\n\tif (unlikely(!req->file->f_op->fsync)) {\n\t\tfput(req->file);\n\t\treturn -EINVAL;\n\t}\n\n\treq->datasync = datasync;\n\tINIT_WORK(&req->work, aio_fsync_work);\n\tschedule_work(&req->work);\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1987,
            "cve_id": "CVE-2019-10125",
            "code_snippet": "static inline void aio_poll_complete(struct aio_kiocb *iocb, __poll_t mask)\n{\n\tstruct file *file = iocb->poll.file;\n\n\taio_complete(iocb, mangle_poll(mask), 0);\n\tfput(file);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1988,
            "cve_id": "CVE-2019-10125",
            "code_snippet": "static ssize_t aio_poll(struct aio_kiocb *aiocb, const struct iocb *iocb)\n{\n\tstruct kioctx *ctx = aiocb->ki_ctx;\n\tstruct poll_iocb *req = &aiocb->poll;\n\tstruct aio_poll_table apt;\n\t__poll_t mask;\n\n\t/* reject any unknown events outside the normal event mask. */\n\tif ((u16)iocb->aio_buf != iocb->aio_buf)\n\t\treturn -EINVAL;\n\t/* reject fields that are not defined for poll */\n\tif (iocb->aio_offset || iocb->aio_nbytes || iocb->aio_rw_flags)\n\t\treturn -EINVAL;\n\n\tINIT_WORK(&req->work, aio_poll_complete_work);\n\treq->events = demangle_poll(iocb->aio_buf) | EPOLLERR | EPOLLHUP;\n\treq->file = fget(iocb->aio_fildes);\n\tif (unlikely(!req->file))\n\t\treturn -EBADF;\n\n\treq->head = NULL;\n\treq->woken = false;\n\treq->cancelled = false;\n\n\tapt.pt._qproc = aio_poll_queue_proc;\n\tapt.pt._key = req->events;\n\tapt.iocb = aiocb;\n\tapt.error = -EINVAL; /* same as no support for IOCB_CMD_POLL */\n\n\t/* initialized the list so that we can do list_empty checks */\n\tINIT_LIST_HEAD(&req->wait.entry);\n\tinit_waitqueue_func_entry(&req->wait, aio_poll_wake);\n\n\t/* one for removal from waitqueue, one for this function */\n\trefcount_set(&aiocb->ki_refcnt, 2);\n\n\tmask = vfs_poll(req->file, &apt.pt) & req->events;\n\tif (unlikely(!req->head)) {\n\t\t/* we did not manage to set up a waitqueue, done */\n\t\tgoto out;\n\t}\n\n\tspin_lock_irq(&ctx->ctx_lock);\n\tspin_lock(&req->head->lock);\n\tif (req->woken) {\n\t\t/* wake_up context handles the rest */\n\t\tmask = 0;\n\t\tapt.error = 0;\n\t} else if (mask || apt.error) {\n\t\t/* if we get an error or a mask we are done */\n\t\tWARN_ON_ONCE(list_empty(&req->wait.entry));\n\t\tlist_del_init(&req->wait.entry);\n\t} else {\n\t\t/* actually waiting for an event */\n\t\tlist_add_tail(&aiocb->ki_list, &ctx->active_reqs);\n\t\taiocb->ki_cancel = aio_poll_cancel;\n\t}\n\tspin_unlock(&req->head->lock);\n\tspin_unlock_irq(&ctx->ctx_lock);\n\nout:\n\tif (unlikely(apt.error)) {\n\t\tfput(req->file);\n\t\treturn apt.error;\n\t}\n\n\tif (mask)\n\t\taio_poll_complete(aiocb, mask);\n\tiocb_put(aiocb);\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1989,
            "cve_id": "CVE-2019-10125",
            "code_snippet": "static void aio_complete_rw(struct kiocb *kiocb, long res, long res2)\n{\n\tstruct aio_kiocb *iocb = container_of(kiocb, struct aio_kiocb, rw);\n\n\tif (!list_empty_careful(&iocb->ki_list))\n\t\taio_remove_iocb(iocb);\n\n\tif (kiocb->ki_flags & IOCB_WRITE) {\n\t\tstruct inode *inode = file_inode(kiocb->ki_filp);\n\n\t\t/*\n\t\t * Tell lockdep we inherited freeze protection from submission\n\t\t * thread.\n\t\t */\n\t\tif (S_ISREG(inode->i_mode))\n\t\t\t__sb_writers_acquired(inode->i_sb, SB_FREEZE_WRITE);\n\t\tfile_end_write(kiocb->ki_filp);\n\t}\n\n\tfput(kiocb->ki_filp);\n\taio_complete(iocb, res, res2);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2019,
            "cve_id": "CVE-2019-11487",
            "code_snippet": "void generic_pipe_buf_get(struct pipe_inode_info *pipe, struct pipe_buffer *buf)\n{\n\tget_page(buf->page);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2020,
            "cve_id": "CVE-2019-11487",
            "code_snippet": "static int splice_pipe_to_pipe(struct pipe_inode_info *ipipe,\n\t\t\t       struct pipe_inode_info *opipe,\n\t\t\t       size_t len, unsigned int flags)\n{\n\tstruct pipe_buffer *ibuf, *obuf;\n\tint ret = 0, nbuf;\n\tbool input_wakeup = false;\n\n\nretry:\n\tret = ipipe_prep(ipipe, flags);\n\tif (ret)\n\t\treturn ret;\n\n\tret = opipe_prep(opipe, flags);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Potential ABBA deadlock, work around it by ordering lock\n\t * grabbing by pipe info address. Otherwise two different processes\n\t * could deadlock (one doing tee from A -> B, the other from B -> A).\n\t */\n\tpipe_double_lock(ipipe, opipe);\n\n\tdo {\n\t\tif (!opipe->readers) {\n\t\t\tsend_sig(SIGPIPE, current, 0);\n\t\t\tif (!ret)\n\t\t\t\tret = -EPIPE;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!ipipe->nrbufs && !ipipe->writers)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Cannot make any progress, because either the input\n\t\t * pipe is empty or the output pipe is full.\n\t\t */\n\t\tif (!ipipe->nrbufs || opipe->nrbufs >= opipe->buffers) {\n\t\t\t/* Already processed some buffers, break */\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\n\t\t\tif (flags & SPLICE_F_NONBLOCK) {\n\t\t\t\tret = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * We raced with another reader/writer and haven't\n\t\t\t * managed to process any buffers.  A zero return\n\t\t\t * value means EOF, so retry instead.\n\t\t\t */\n\t\t\tpipe_unlock(ipipe);\n\t\t\tpipe_unlock(opipe);\n\t\t\tgoto retry;\n\t\t}\n\n\t\tibuf = ipipe->bufs + ipipe->curbuf;\n\t\tnbuf = (opipe->curbuf + opipe->nrbufs) & (opipe->buffers - 1);\n\t\tobuf = opipe->bufs + nbuf;\n\n\t\tif (len >= ibuf->len) {\n\t\t\t/*\n\t\t\t * Simply move the whole buffer from ipipe to opipe\n\t\t\t */\n\t\t\t*obuf = *ibuf;\n\t\t\tibuf->ops = NULL;\n\t\t\topipe->nrbufs++;\n\t\t\tipipe->curbuf = (ipipe->curbuf + 1) & (ipipe->buffers - 1);\n\t\t\tipipe->nrbufs--;\n\t\t\tinput_wakeup = true;\n\t\t} else {\n\t\t\t/*\n\t\t\t * Get a reference to this pipe buffer,\n\t\t\t * so we can copy the contents over.\n\t\t\t */\n\t\t\tpipe_buf_get(ipipe, ibuf);\n\t\t\t*obuf = *ibuf;\n\n\t\t\t/*\n\t\t\t * Don't inherit the gift flag, we need to\n\t\t\t * prevent multiple steals of this page.\n\t\t\t */\n\t\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n\n\t\t\tobuf->len = len;\n\t\t\topipe->nrbufs++;\n\t\t\tibuf->offset += obuf->len;\n\t\t\tibuf->len -= obuf->len;\n\t\t}\n\t\tret += obuf->len;\n\t\tlen -= obuf->len;\n\t} while (len);\n\n\tpipe_unlock(ipipe);\n\tpipe_unlock(opipe);\n\n\t/*\n\t * If we put data in the output pipe, wakeup any potential readers.\n\t */\n\tif (ret > 0)\n\t\twakeup_pipe_readers(opipe);\n\n\tif (input_wakeup)\n\t\twakeup_pipe_writers(ipipe);\n\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2034,
            "cve_id": "CVE-2019-11811",
            "code_snippet": "int ipmi_si_mem_setup(struct si_sm_io *io)\n{\n\tunsigned long addr = io->addr_data;\n\tint           mapsize, idx;\n\n\tif (!addr)\n\t\treturn -ENODEV;\n\n\tio->io_cleanup = mem_cleanup;\n\n\t/*\n\t * Figure out the actual readb/readw/readl/etc routine to use based\n\t * upon the register size.\n\t */\n\tswitch (io->regsize) {\n\tcase 1:\n\t\tio->inputb = intf_mem_inb;\n\t\tio->outputb = intf_mem_outb;\n\t\tbreak;\n\tcase 2:\n\t\tio->inputb = intf_mem_inw;\n\t\tio->outputb = intf_mem_outw;\n\t\tbreak;\n\tcase 4:\n\t\tio->inputb = intf_mem_inl;\n\t\tio->outputb = intf_mem_outl;\n\t\tbreak;\n#ifdef readq\n\tcase 8:\n\t\tio->inputb = mem_inq;\n\t\tio->outputb = mem_outq;\n\t\tbreak;\n#endif\n\tdefault:\n\t\tdev_warn(io->dev, \"Invalid register size: %d\\n\",\n\t\t\t io->regsize);\n\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Some BIOSes reserve disjoint memory regions in their ACPI\n\t * tables.  This causes problems when trying to request the\n\t * entire region.  Therefore we must request each register\n\t * separately.\n\t */\n\tfor (idx = 0; idx < io->io_size; idx++) {\n\t\tif (request_mem_region(addr + idx * io->regspacing,\n\t\t\t\t       io->regsize, DEVICE_NAME) == NULL) {\n\t\t\t/* Undo allocations */\n\t\t\tmem_region_cleanup(io, idx);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\t/*\n\t * Calculate the total amount of memory to claim.  This is an\n\t * unusual looking calculation, but it avoids claiming any\n\t * more memory than it has to.  It will claim everything\n\t * between the first address to the end of the last full\n\t * register.\n\t */\n\tmapsize = ((io->io_size * io->regspacing)\n\t\t   - (io->regspacing - io->regsize));\n\tio->addr = ioremap(addr, mapsize);\n\tif (io->addr == NULL) {\n\t\tmem_region_cleanup(io, io->io_size);\n\t\treturn -EIO;\n\t}\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2059,
            "cve_id": "CVE-2019-13233",
            "code_snippet": "int insn_get_code_seg_params(struct pt_regs *regs)\n{\n\tstruct desc_struct *desc;\n\tshort sel;\n\n\tif (v8086_mode(regs))\n\t\t/* Address and operand size are both 16-bit. */\n\t\treturn INSN_CODE_SEG_PARAMS(2, 2);\n\n\tsel = get_segment_selector(regs, INAT_SEG_REG_CS);\n\tif (sel < 0)\n\t\treturn sel;\n\n\tdesc = get_desc(sel);\n\tif (!desc)\n\t\treturn -EINVAL;\n\n\t/*\n\t * The most significant byte of the Type field of the segment descriptor\n\t * determines whether a segment contains data or code. If this is a data\n\t * segment, return error.\n\t */\n\tif (!(desc->type & BIT(3)))\n\t\treturn -EINVAL;\n\n\tswitch ((desc->l << 1) | desc->d) {\n\tcase 0: /*\n\t\t * Legacy mode. CS.L=0, CS.D=0. Address and operand size are\n\t\t * both 16-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(2, 2);\n\tcase 1: /*\n\t\t * Legacy mode. CS.L=0, CS.D=1. Address and operand size are\n\t\t * both 32-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(4, 4);\n\tcase 2: /*\n\t\t * IA-32e 64-bit mode. CS.L=1, CS.D=0. Address size is 64-bit;\n\t\t * operand size is 32-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(4, 8);\n\tcase 3: /* Invalid setting. CS.L=1, CS.D=1 */\n\t\t/* fall through */\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2110,
            "cve_id": "CVE-2019-15220",
            "code_snippet": "static int p54u_probe(struct usb_interface *intf,\n\t\t\t\tconst struct usb_device_id *id)\n{\n\tstruct usb_device *udev = interface_to_usbdev(intf);\n\tstruct ieee80211_hw *dev;\n\tstruct p54u_priv *priv;\n\tint err;\n\tunsigned int i, recognized_pipes;\n\n\tdev = p54_init_common(sizeof(*priv));\n\n\tif (!dev) {\n\t\tdev_err(&udev->dev, \"(p54usb) ieee80211 alloc failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tpriv = dev->priv;\n\tpriv->hw_type = P54U_INVALID_HW;\n\n\tSET_IEEE80211_DEV(dev, &intf->dev);\n\tusb_set_intfdata(intf, dev);\n\tpriv->udev = udev;\n\tpriv->intf = intf;\n\tskb_queue_head_init(&priv->rx_queue);\n\tinit_usb_anchor(&priv->submitted);\n\n\tusb_get_dev(udev);\n\n\t/* really lazy and simple way of figuring out if we're a 3887 */\n\t/* TODO: should just stick the identification in the device table */\n\ti = intf->altsetting->desc.bNumEndpoints;\n\trecognized_pipes = 0;\n\twhile (i--) {\n\t\tswitch (intf->altsetting->endpoint[i].desc.bEndpointAddress) {\n\t\tcase P54U_PIPE_DATA:\n\t\tcase P54U_PIPE_MGMT:\n\t\tcase P54U_PIPE_BRG:\n\t\tcase P54U_PIPE_DEV:\n\t\tcase P54U_PIPE_DATA | USB_DIR_IN:\n\t\tcase P54U_PIPE_MGMT | USB_DIR_IN:\n\t\tcase P54U_PIPE_BRG | USB_DIR_IN:\n\t\tcase P54U_PIPE_DEV | USB_DIR_IN:\n\t\tcase P54U_PIPE_INT | USB_DIR_IN:\n\t\t\trecognized_pipes++;\n\t\t}\n\t}\n\tpriv->common.open = p54u_open;\n\tpriv->common.stop = p54u_stop;\n\tif (recognized_pipes < P54U_PIPE_NUMBER) {\n#ifdef CONFIG_PM\n\t\t/* ISL3887 needs a full reset on resume */\n\t\tudev->reset_resume = 1;\n#endif /* CONFIG_PM */\n\t\terr = p54u_device_reset(dev);\n\n\t\tpriv->hw_type = P54U_3887;\n\t\tdev->extra_tx_headroom += sizeof(struct lm87_tx_hdr);\n\t\tpriv->common.tx_hdr_len = sizeof(struct lm87_tx_hdr);\n\t\tpriv->common.tx = p54u_tx_lm87;\n\t\tpriv->upload_fw = p54u_upload_firmware_3887;\n\t} else {\n\t\tpriv->hw_type = P54U_NET2280;\n\t\tdev->extra_tx_headroom += sizeof(struct net2280_tx_hdr);\n\t\tpriv->common.tx_hdr_len = sizeof(struct net2280_tx_hdr);\n\t\tpriv->common.tx = p54u_tx_net2280;\n\t\tpriv->upload_fw = p54u_upload_firmware_net2280;\n\t}\n\terr = p54u_load_firmware(dev, intf);\n\tif (err) {\n\t\tusb_put_dev(udev);\n\t\tp54_free_common(dev);\n\t}\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2123,
            "cve_id": "CVE-2019-15292",
            "code_snippet": "void atalk_register_sysctl(void)\n{\n\tatalk_table_header = register_net_sysctl(&init_net, \"net/appletalk\", atalk_table);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2200,
            "cve_id": "CVE-2019-18683",
            "code_snippet": "static int vivid_thread_vid_cap(void *data)\n{\n\tstruct vivid_dev *dev = data;\n\tu64 numerators_since_start;\n\tu64 buffers_since_start;\n\tu64 next_jiffies_since_start;\n\tunsigned long jiffies_since_start;\n\tunsigned long cur_jiffies;\n\tunsigned wait_jiffies;\n\tunsigned numerator;\n\tunsigned denominator;\n\tint dropped_bufs;\n\n\tdprintk(dev, 1, \"Video Capture Thread Start\\n\");\n\n\tset_freezable();\n\n\t/* Resets frame counters */\n\tdev->cap_seq_offset = 0;\n\tdev->cap_seq_count = 0;\n\tdev->cap_seq_resync = false;\n\tdev->jiffies_vid_cap = jiffies;\n\tdev->cap_stream_start = ktime_get_ns();\n\tvivid_cap_update_frame_period(dev);\n\n\tfor (;;) {\n\t\ttry_to_freeze();\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tmutex_lock(&dev->mutex);\n\t\tcur_jiffies = jiffies;\n\t\tif (dev->cap_seq_resync) {\n\t\t\tdev->jiffies_vid_cap = cur_jiffies;\n\t\t\tdev->cap_seq_offset = dev->cap_seq_count + 1;\n\t\t\tdev->cap_seq_count = 0;\n\t\t\tdev->cap_stream_start += dev->cap_frame_period *\n\t\t\t\t\t\t dev->cap_seq_offset;\n\t\t\tvivid_cap_update_frame_period(dev);\n\t\t\tdev->cap_seq_resync = false;\n\t\t}\n\t\tnumerator = dev->timeperframe_vid_cap.numerator;\n\t\tdenominator = dev->timeperframe_vid_cap.denominator;\n\n\t\tif (dev->field_cap == V4L2_FIELD_ALTERNATE)\n\t\t\tdenominator *= 2;\n\n\t\t/* Calculate the number of jiffies since we started streaming */\n\t\tjiffies_since_start = cur_jiffies - dev->jiffies_vid_cap;\n\t\t/* Get the number of buffers streamed since the start */\n\t\tbuffers_since_start = (u64)jiffies_since_start * denominator +\n\t\t\t\t      (HZ * numerator) / 2;\n\t\tdo_div(buffers_since_start, HZ * numerator);\n\n\t\t/*\n\t\t * After more than 0xf0000000 (rounded down to a multiple of\n\t\t * 'jiffies-per-day' to ease jiffies_to_msecs calculation)\n\t\t * jiffies have passed since we started streaming reset the\n\t\t * counters and keep track of the sequence offset.\n\t\t */\n\t\tif (jiffies_since_start > JIFFIES_RESYNC) {\n\t\t\tdev->jiffies_vid_cap = cur_jiffies;\n\t\t\tdev->cap_seq_offset = buffers_since_start;\n\t\t\tbuffers_since_start = 0;\n\t\t}\n\t\tdropped_bufs = buffers_since_start + dev->cap_seq_offset - dev->cap_seq_count;\n\t\tdev->cap_seq_count = buffers_since_start + dev->cap_seq_offset;\n\t\tdev->vid_cap_seq_count = dev->cap_seq_count - dev->vid_cap_seq_start;\n\t\tdev->vbi_cap_seq_count = dev->cap_seq_count - dev->vbi_cap_seq_start;\n\t\tdev->meta_cap_seq_count = dev->cap_seq_count - dev->meta_cap_seq_start;\n\n\t\tvivid_thread_vid_cap_tick(dev, dropped_bufs);\n\n\t\t/*\n\t\t * Calculate the number of 'numerators' streamed since we started,\n\t\t * including the current buffer.\n\t\t */\n\t\tnumerators_since_start = ++buffers_since_start * numerator;\n\n\t\t/* And the number of jiffies since we started */\n\t\tjiffies_since_start = jiffies - dev->jiffies_vid_cap;\n\n\t\tmutex_unlock(&dev->mutex);\n\n\t\t/*\n\t\t * Calculate when that next buffer is supposed to start\n\t\t * in jiffies since we started streaming.\n\t\t */\n\t\tnext_jiffies_since_start = numerators_since_start * HZ +\n\t\t\t\t\t   denominator / 2;\n\t\tdo_div(next_jiffies_since_start, denominator);\n\t\t/* If it is in the past, then just schedule asap */\n\t\tif (next_jiffies_since_start < jiffies_since_start)\n\t\t\tnext_jiffies_since_start = jiffies_since_start;\n\n\t\twait_jiffies = next_jiffies_since_start - jiffies_since_start;\n\t\tschedule_timeout_interruptible(wait_jiffies ? wait_jiffies : 1);\n\t}\n\tdprintk(dev, 1, \"Video Capture Thread End\\n\");\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2201,
            "cve_id": "CVE-2019-18683",
            "code_snippet": "static int vivid_thread_vid_out(void *data)\n{\n\tstruct vivid_dev *dev = data;\n\tu64 numerators_since_start;\n\tu64 buffers_since_start;\n\tu64 next_jiffies_since_start;\n\tunsigned long jiffies_since_start;\n\tunsigned long cur_jiffies;\n\tunsigned wait_jiffies;\n\tunsigned numerator;\n\tunsigned denominator;\n\n\tdprintk(dev, 1, \"Video Output Thread Start\\n\");\n\n\tset_freezable();\n\n\t/* Resets frame counters */\n\tdev->out_seq_offset = 0;\n\tif (dev->seq_wrap)\n\t\tdev->out_seq_count = 0xffffff80U;\n\tdev->jiffies_vid_out = jiffies;\n\tdev->vid_out_seq_start = dev->vbi_out_seq_start = 0;\n\tdev->meta_out_seq_start = 0;\n\tdev->out_seq_resync = false;\n\n\tfor (;;) {\n\t\ttry_to_freeze();\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tmutex_lock(&dev->mutex);\n\t\tcur_jiffies = jiffies;\n\t\tif (dev->out_seq_resync) {\n\t\t\tdev->jiffies_vid_out = cur_jiffies;\n\t\t\tdev->out_seq_offset = dev->out_seq_count + 1;\n\t\t\tdev->out_seq_count = 0;\n\t\t\tdev->out_seq_resync = false;\n\t\t}\n\t\tnumerator = dev->timeperframe_vid_out.numerator;\n\t\tdenominator = dev->timeperframe_vid_out.denominator;\n\n\t\tif (dev->field_out == V4L2_FIELD_ALTERNATE)\n\t\t\tdenominator *= 2;\n\n\t\t/* Calculate the number of jiffies since we started streaming */\n\t\tjiffies_since_start = cur_jiffies - dev->jiffies_vid_out;\n\t\t/* Get the number of buffers streamed since the start */\n\t\tbuffers_since_start = (u64)jiffies_since_start * denominator +\n\t\t\t\t      (HZ * numerator) / 2;\n\t\tdo_div(buffers_since_start, HZ * numerator);\n\n\t\t/*\n\t\t * After more than 0xf0000000 (rounded down to a multiple of\n\t\t * 'jiffies-per-day' to ease jiffies_to_msecs calculation)\n\t\t * jiffies have passed since we started streaming reset the\n\t\t * counters and keep track of the sequence offset.\n\t\t */\n\t\tif (jiffies_since_start > JIFFIES_RESYNC) {\n\t\t\tdev->jiffies_vid_out = cur_jiffies;\n\t\t\tdev->out_seq_offset = buffers_since_start;\n\t\t\tbuffers_since_start = 0;\n\t\t}\n\t\tdev->out_seq_count = buffers_since_start + dev->out_seq_offset;\n\t\tdev->vid_out_seq_count = dev->out_seq_count - dev->vid_out_seq_start;\n\t\tdev->vbi_out_seq_count = dev->out_seq_count - dev->vbi_out_seq_start;\n\t\tdev->meta_out_seq_count = dev->out_seq_count - dev->meta_out_seq_start;\n\n\t\tvivid_thread_vid_out_tick(dev);\n\t\tmutex_unlock(&dev->mutex);\n\n\t\t/*\n\t\t * Calculate the number of 'numerators' streamed since we started,\n\t\t * not including the current buffer.\n\t\t */\n\t\tnumerators_since_start = buffers_since_start * numerator;\n\n\t\t/* And the number of jiffies since we started */\n\t\tjiffies_since_start = jiffies - dev->jiffies_vid_out;\n\n\t\t/* Increase by the 'numerator' of one buffer */\n\t\tnumerators_since_start += numerator;\n\t\t/*\n\t\t * Calculate when that next buffer is supposed to start\n\t\t * in jiffies since we started streaming.\n\t\t */\n\t\tnext_jiffies_since_start = numerators_since_start * HZ +\n\t\t\t\t\t   denominator / 2;\n\t\tdo_div(next_jiffies_since_start, denominator);\n\t\t/* If it is in the past, then just schedule asap */\n\t\tif (next_jiffies_since_start < jiffies_since_start)\n\t\t\tnext_jiffies_since_start = jiffies_since_start;\n\n\t\twait_jiffies = next_jiffies_since_start - jiffies_since_start;\n\t\tschedule_timeout_interruptible(wait_jiffies ? wait_jiffies : 1);\n\t}\n\tdprintk(dev, 1, \"Video Output Thread End\\n\");\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2297,
            "cve_id": "CVE-2019-19319",
            "code_snippet": "static int __check_block_validity(struct inode *inode, const char *func,\n\t\t\t\tunsigned int line,\n\t\t\t\tstruct ext4_map_blocks *map)\n{\n\tif (!ext4_data_block_valid(EXT4_SB(inode->i_sb), map->m_pblk,\n\t\t\t\t   map->m_len)) {\n\t\text4_error_inode(inode, func, line, map->m_pblk,\n\t\t\t\t \"lblock %lu mapped to illegal pblock %llu \"\n\t\t\t\t \"(length %d)\", (unsigned long) map->m_lblk,\n\t\t\t\t map->m_pblk, map->m_len);\n\t\treturn -EFSCORRUPTED;\n\t}\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2332,
            "cve_id": "CVE-2019-19767",
            "code_snippet": "static void ext4_clamp_want_extra_isize(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_super_block *es = sbi->s_es;\n\n\t/* determine the minimum size of new large inodes, if present */\n\tif (sbi->s_inode_size > EXT4_GOOD_OLD_INODE_SIZE &&\n\t    sbi->s_want_extra_isize == 0) {\n\t\tsbi->s_want_extra_isize = sizeof(struct ext4_inode) -\n\t\t\t\t\t\t     EXT4_GOOD_OLD_INODE_SIZE;\n\t\tif (ext4_has_feature_extra_isize(sb)) {\n\t\t\tif (sbi->s_want_extra_isize <\n\t\t\t    le16_to_cpu(es->s_want_extra_isize))\n\t\t\t\tsbi->s_want_extra_isize =\n\t\t\t\t\tle16_to_cpu(es->s_want_extra_isize);\n\t\t\tif (sbi->s_want_extra_isize <\n\t\t\t    le16_to_cpu(es->s_min_extra_isize))\n\t\t\t\tsbi->s_want_extra_isize =\n\t\t\t\t\tle16_to_cpu(es->s_min_extra_isize);\n\t\t}\n\t}\n\t/* Check if enough inode space is available */\n\tif (EXT4_GOOD_OLD_INODE_SIZE + sbi->s_want_extra_isize >\n\t\t\t\t\t\t\tsbi->s_inode_size) {\n\t\tsbi->s_want_extra_isize = sizeof(struct ext4_inode) -\n\t\t\t\t\t\t       EXT4_GOOD_OLD_INODE_SIZE;\n\t\text4_msg(sb, KERN_INFO,\n\t\t\t \"required extra inode space not available\");\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2334,
            "cve_id": "CVE-2019-19768",
            "code_snippet": "static void blk_add_trace_rq(struct request *rq, int error,\n\t\t\t     unsigned int nr_bytes, u32 what, u64 cgid)\n{\n\tstruct blk_trace *bt = rq->q->blk_trace;\n\n\tif (likely(!bt))\n\t\treturn;\n\n\tif (blk_rq_is_passthrough(rq))\n\t\twhat |= BLK_TC_ACT(BLK_TC_PC);\n\telse\n\t\twhat |= BLK_TC_ACT(BLK_TC_FS);\n\n\t__blk_add_trace(bt, blk_rq_trace_sector(rq), nr_bytes, req_op(rq),\n\t\t\trq->cmd_flags, what, error, 0, NULL, cgid);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2335,
            "cve_id": "CVE-2019-19768",
            "code_snippet": "static int blk_trace_remove_queue(struct request_queue *q)\n{\n\tstruct blk_trace *bt;\n\n\tbt = xchg(&q->blk_trace, NULL);\n\tif (bt == NULL)\n\t\treturn -EINVAL;\n\n\tput_probe_ref();\n\tblk_trace_free(bt);\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2336,
            "cve_id": "CVE-2019-19768",
            "code_snippet": "static void blk_add_trace_getrq(void *ignore,\n\t\t\t\tstruct request_queue *q,\n\t\t\t\tstruct bio *bio, int rw)\n{\n\tif (bio)\n\t\tblk_add_trace_bio(q, bio, BLK_TA_GETRQ, 0);\n\telse {\n\t\tstruct blk_trace *bt = q->blk_trace;\n\n\t\tif (bt)\n\t\t\t__blk_add_trace(bt, 0, 0, rw, 0, BLK_TA_GETRQ, 0, 0,\n\t\t\t\t\tNULL, 0);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2337,
            "cve_id": "CVE-2019-19768",
            "code_snippet": "static void blk_add_trace_bio(struct request_queue *q, struct bio *bio,\n\t\t\t      u32 what, int error)\n{\n\tstruct blk_trace *bt = q->blk_trace;\n\n\tif (likely(!bt))\n\t\treturn;\n\n\t__blk_add_trace(bt, bio->bi_iter.bi_sector, bio->bi_iter.bi_size,\n\t\t\tbio_op(bio), bio->bi_opf, what, error, 0, NULL,\n\t\t\tblk_trace_bio_get_cgid(q, bio));\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2338,
            "cve_id": "CVE-2019-19768",
            "code_snippet": "void blk_add_driver_data(struct request_queue *q,\n\t\t\t struct request *rq,\n\t\t\t void *data, size_t len)\n{\n\tstruct blk_trace *bt = q->blk_trace;\n\n\tif (likely(!bt))\n\t\treturn;\n\n\t__blk_add_trace(bt, blk_rq_trace_sector(rq), blk_rq_bytes(rq), 0, 0,\n\t\t\t\tBLK_TA_DRV_DATA, 0, len, data,\n\t\t\t\tblk_trace_request_get_cgid(q, rq));\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2339,
            "cve_id": "CVE-2019-19768",
            "code_snippet": "static ssize_t sysfs_blk_trace_attr_show(struct device *dev,\n\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t char *buf)\n{\n\tstruct hd_struct *p = dev_to_part(dev);\n\tstruct request_queue *q;\n\tstruct block_device *bdev;\n\tssize_t ret = -ENXIO;\n\n\tbdev = bdget(part_devt(p));\n\tif (bdev == NULL)\n\t\tgoto out;\n\n\tq = blk_trace_get_queue(bdev);\n\tif (q == NULL)\n\t\tgoto out_bdput;\n\n\tmutex_lock(&q->blk_trace_mutex);\n\n\tif (attr == &dev_attr_enable) {\n\t\tret = sprintf(buf, \"%u\\n\", !!q->blk_trace);\n\t\tgoto out_unlock_bdev;\n\t}\n\n\tif (q->blk_trace == NULL)\n\t\tret = sprintf(buf, \"disabled\\n\");\n\telse if (attr == &dev_attr_act_mask)\n\t\tret = blk_trace_mask2str(buf, q->blk_trace->act_mask);\n\telse if (attr == &dev_attr_pid)\n\t\tret = sprintf(buf, \"%u\\n\", q->blk_trace->pid);\n\telse if (attr == &dev_attr_start_lba)\n\t\tret = sprintf(buf, \"%llu\\n\", q->blk_trace->start_lba);\n\telse if (attr == &dev_attr_end_lba)\n\t\tret = sprintf(buf, \"%llu\\n\", q->blk_trace->end_lba);\n\nout_unlock_bdev:\n\tmutex_unlock(&q->blk_trace_mutex);\nout_bdput:\n\tbdput(bdev);\nout:\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2340,
            "cve_id": "CVE-2019-19768",
            "code_snippet": "static void blk_trace_cleanup(struct blk_trace *bt)\n{\n\tblk_trace_free(bt);\n\tput_probe_ref();\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2341,
            "cve_id": "CVE-2019-19768",
            "code_snippet": "static void blk_add_trace_split(void *ignore,\n\t\t\t\tstruct request_queue *q, struct bio *bio,\n\t\t\t\tunsigned int pdu)\n{\n\tstruct blk_trace *bt = q->blk_trace;\n\n\tif (bt) {\n\t\t__be64 rpdu = cpu_to_be64(pdu);\n\n\t\t__blk_add_trace(bt, bio->bi_iter.bi_sector,\n\t\t\t\tbio->bi_iter.bi_size, bio_op(bio), bio->bi_opf,\n\t\t\t\tBLK_TA_SPLIT, bio->bi_status, sizeof(rpdu),\n\t\t\t\t&rpdu, blk_trace_bio_get_cgid(q, bio));\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2354,
            "cve_id": "CVE-2019-19813",
            "code_snippet": "struct extent_map *btrfs_get_extent(struct btrfs_inode *inode,\n\t\t\t\t    struct page *page,\n\t\t\t\t    size_t pg_offset, u64 start, u64 len,\n\t\t\t\t    int create)\n{\n\tstruct btrfs_fs_info *fs_info = inode->root->fs_info;\n\tint ret;\n\tint err = 0;\n\tu64 extent_start = 0;\n\tu64 extent_end = 0;\n\tu64 objectid = btrfs_ino(inode);\n\tu8 extent_type;\n\tstruct btrfs_path *path = NULL;\n\tstruct btrfs_root *root = inode->root;\n\tstruct btrfs_file_extent_item *item;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key found_key;\n\tstruct extent_map *em = NULL;\n\tstruct extent_map_tree *em_tree = &inode->extent_tree;\n\tstruct extent_io_tree *io_tree = &inode->io_tree;\n\tconst bool new_inline = !page || create;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, start, len);\n\tif (em)\n\t\tem->bdev = fs_info->fs_devices->latest_bdev;\n\tread_unlock(&em_tree->lock);\n\n\tif (em) {\n\t\tif (em->start > start || em->start + em->len <= start)\n\t\t\tfree_extent_map(em);\n\t\telse if (em->block_start == EXTENT_MAP_INLINE && page)\n\t\t\tfree_extent_map(em);\n\t\telse\n\t\t\tgoto out;\n\t}\n\tem = alloc_extent_map();\n\tif (!em) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\tem->bdev = fs_info->fs_devices->latest_bdev;\n\tem->start = EXTENT_MAP_HOLE;\n\tem->orig_start = EXTENT_MAP_HOLE;\n\tem->len = (u64)-1;\n\tem->block_len = (u64)-1;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t/* Chances are we'll be called again, so go ahead and do readahead */\n\tpath->reada = READA_FORWARD;\n\n\t/*\n\t * Unless we're going to uncompress the inline extent, no sleep would\n\t * happen.\n\t */\n\tpath->leave_spinning = 1;\n\n\tret = btrfs_lookup_file_extent(NULL, root, path, objectid, start, 0);\n\tif (ret < 0) {\n\t\terr = ret;\n\t\tgoto out;\n\t} else if (ret > 0) {\n\t\tif (path->slots[0] == 0)\n\t\t\tgoto not_found;\n\t\tpath->slots[0]--;\n\t}\n\n\tleaf = path->nodes[0];\n\titem = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t      struct btrfs_file_extent_item);\n\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\tif (found_key.objectid != objectid ||\n\t    found_key.type != BTRFS_EXTENT_DATA_KEY) {\n\t\t/*\n\t\t * If we backup past the first extent we want to move forward\n\t\t * and see if there is an extent in front of us, otherwise we'll\n\t\t * say there is a hole for our whole search range which can\n\t\t * cause problems.\n\t\t */\n\t\textent_end = start;\n\t\tgoto next;\n\t}\n\n\textent_type = btrfs_file_extent_type(leaf, item);\n\textent_start = found_key.offset;\n\tif (extent_type == BTRFS_FILE_EXTENT_REG ||\n\t    extent_type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\textent_end = extent_start +\n\t\t       btrfs_file_extent_num_bytes(leaf, item);\n\n\t\ttrace_btrfs_get_extent_show_fi_regular(inode, leaf, item,\n\t\t\t\t\t\t       extent_start);\n\t} else if (extent_type == BTRFS_FILE_EXTENT_INLINE) {\n\t\tsize_t size;\n\n\t\tsize = btrfs_file_extent_ram_bytes(leaf, item);\n\t\textent_end = ALIGN(extent_start + size,\n\t\t\t\t   fs_info->sectorsize);\n\n\t\ttrace_btrfs_get_extent_show_fi_inline(inode, leaf, item,\n\t\t\t\t\t\t      path->slots[0],\n\t\t\t\t\t\t      extent_start);\n\t}\nnext:\n\tif (start >= extent_end) {\n\t\tpath->slots[0]++;\n\t\tif (path->slots[0] >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret < 0) {\n\t\t\t\terr = ret;\n\t\t\t\tgoto out;\n\t\t\t} else if (ret > 0) {\n\t\t\t\tgoto not_found;\n\t\t\t}\n\t\t\tleaf = path->nodes[0];\n\t\t}\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\t\tif (found_key.objectid != objectid ||\n\t\t    found_key.type != BTRFS_EXTENT_DATA_KEY)\n\t\t\tgoto not_found;\n\t\tif (start + len <= found_key.offset)\n\t\t\tgoto not_found;\n\t\tif (start > found_key.offset)\n\t\t\tgoto next;\n\n\t\t/* New extent overlaps with existing one */\n\t\tem->start = start;\n\t\tem->orig_start = start;\n\t\tem->len = found_key.offset - start;\n\t\tem->block_start = EXTENT_MAP_HOLE;\n\t\tgoto insert;\n\t}\n\n\tbtrfs_extent_item_to_extent_map(inode, path, item,\n\t\t\tnew_inline, em);\n\n\tif (extent_type == BTRFS_FILE_EXTENT_REG ||\n\t    extent_type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\tgoto insert;\n\t} else if (extent_type == BTRFS_FILE_EXTENT_INLINE) {\n\t\tunsigned long ptr;\n\t\tchar *map;\n\t\tsize_t size;\n\t\tsize_t extent_offset;\n\t\tsize_t copy_size;\n\n\t\tif (new_inline)\n\t\t\tgoto out;\n\n\t\tsize = btrfs_file_extent_ram_bytes(leaf, item);\n\t\textent_offset = page_offset(page) + pg_offset - extent_start;\n\t\tcopy_size = min_t(u64, PAGE_SIZE - pg_offset,\n\t\t\t\t  size - extent_offset);\n\t\tem->start = extent_start + extent_offset;\n\t\tem->len = ALIGN(copy_size, fs_info->sectorsize);\n\t\tem->orig_block_len = em->len;\n\t\tem->orig_start = em->start;\n\t\tptr = btrfs_file_extent_inline_start(item) + extent_offset;\n\n\t\tbtrfs_set_path_blocking(path);\n\t\tif (!PageUptodate(page)) {\n\t\t\tif (btrfs_file_extent_compression(leaf, item) !=\n\t\t\t    BTRFS_COMPRESS_NONE) {\n\t\t\t\tret = uncompress_inline(path, page, pg_offset,\n\t\t\t\t\t\t\textent_offset, item);\n\t\t\t\tif (ret) {\n\t\t\t\t\terr = ret;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tmap = kmap(page);\n\t\t\t\tread_extent_buffer(leaf, map + pg_offset, ptr,\n\t\t\t\t\t\t   copy_size);\n\t\t\t\tif (pg_offset + copy_size < PAGE_SIZE) {\n\t\t\t\t\tmemset(map + pg_offset + copy_size, 0,\n\t\t\t\t\t       PAGE_SIZE - pg_offset -\n\t\t\t\t\t       copy_size);\n\t\t\t\t}\n\t\t\t\tkunmap(page);\n\t\t\t}\n\t\t\tflush_dcache_page(page);\n\t\t}\n\t\tset_extent_uptodate(io_tree, em->start,\n\t\t\t\t    extent_map_end(em) - 1, NULL, GFP_NOFS);\n\t\tgoto insert;\n\t}\nnot_found:\n\tem->start = start;\n\tem->orig_start = start;\n\tem->len = len;\n\tem->block_start = EXTENT_MAP_HOLE;\ninsert:\n\tbtrfs_release_path(path);\n\tif (em->start > start || extent_map_end(em) <= start) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"bad extent! em: [%llu %llu] passed [%llu %llu]\",\n\t\t\t  em->start, em->len, start, len);\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\terr = 0;\n\twrite_lock(&em_tree->lock);\n\terr = btrfs_add_extent_mapping(fs_info, em_tree, &em, start, len);\n\twrite_unlock(&em_tree->lock);\nout:\n\tbtrfs_free_path(path);\n\n\ttrace_btrfs_get_extent(root, inode, em);\n\n\tif (err) {\n\t\tfree_extent_map(em);\n\t\treturn ERR_PTR(err);\n\t}\n\tBUG_ON(!em); /* Error is always set */\n\treturn em;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2384,
            "cve_id": "CVE-2019-2025",
            "code_snippet": "static int binder_thread_write(struct binder_proc *proc,\n\t\t\tstruct binder_thread *thread,\n\t\t\tbinder_uintptr_t binder_buffer, size_t size,\n\t\t\tbinder_size_t *consumed)\n{\n\tuint32_t cmd;\n\tstruct binder_context *context = proc->context;\n\tvoid __user *buffer = (void __user *)(uintptr_t)binder_buffer;\n\tvoid __user *ptr = buffer + *consumed;\n\tvoid __user *end = buffer + size;\n\n\twhile (ptr < end && thread->return_error.cmd == BR_OK) {\n\t\tint ret;\n\n\t\tif (get_user(cmd, (uint32_t __user *)ptr))\n\t\t\treturn -EFAULT;\n\t\tptr += sizeof(uint32_t);\n\t\ttrace_binder_command(cmd);\n\t\tif (_IOC_NR(cmd) < ARRAY_SIZE(binder_stats.bc)) {\n\t\t\tatomic_inc(&binder_stats.bc[_IOC_NR(cmd)]);\n\t\t\tatomic_inc(&proc->stats.bc[_IOC_NR(cmd)]);\n\t\t\tatomic_inc(&thread->stats.bc[_IOC_NR(cmd)]);\n\t\t}\n\t\tswitch (cmd) {\n\t\tcase BC_INCREFS:\n\t\tcase BC_ACQUIRE:\n\t\tcase BC_RELEASE:\n\t\tcase BC_DECREFS: {\n\t\t\tuint32_t target;\n\t\t\tconst char *debug_string;\n\t\t\tbool strong = cmd == BC_ACQUIRE || cmd == BC_RELEASE;\n\t\t\tbool increment = cmd == BC_INCREFS || cmd == BC_ACQUIRE;\n\t\t\tstruct binder_ref_data rdata;\n\n\t\t\tif (get_user(target, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tret = -1;\n\t\t\tif (increment && !target) {\n\t\t\t\tstruct binder_node *ctx_mgr_node;\n\t\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\t\tctx_mgr_node = context->binder_context_mgr_node;\n\t\t\t\tif (ctx_mgr_node)\n\t\t\t\t\tret = binder_inc_ref_for_node(\n\t\t\t\t\t\t\tproc, ctx_mgr_node,\n\t\t\t\t\t\t\tstrong, NULL, &rdata);\n\t\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\t}\n\t\t\tif (ret)\n\t\t\t\tret = binder_update_ref_for_handle(\n\t\t\t\t\t\tproc, target, increment, strong,\n\t\t\t\t\t\t&rdata);\n\t\t\tif (!ret && rdata.desc != target) {\n\t\t\t\tbinder_user_error(\"%d:%d tried to acquire reference to desc %d, got %d instead\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\ttarget, rdata.desc);\n\t\t\t}\n\t\t\tswitch (cmd) {\n\t\t\tcase BC_INCREFS:\n\t\t\t\tdebug_string = \"IncRefs\";\n\t\t\t\tbreak;\n\t\t\tcase BC_ACQUIRE:\n\t\t\t\tdebug_string = \"Acquire\";\n\t\t\t\tbreak;\n\t\t\tcase BC_RELEASE:\n\t\t\t\tdebug_string = \"Release\";\n\t\t\t\tbreak;\n\t\t\tcase BC_DECREFS:\n\t\t\tdefault:\n\t\t\t\tdebug_string = \"DecRefs\";\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ret) {\n\t\t\t\tbinder_user_error(\"%d:%d %s %d refcount change on invalid ref %d ret %d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, debug_string,\n\t\t\t\t\tstrong, target, ret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n\t\t\t\t     \"%d:%d %s ref %d desc %d s %d w %d\\n\",\n\t\t\t\t     proc->pid, thread->pid, debug_string,\n\t\t\t\t     rdata.debug_id, rdata.desc, rdata.strong,\n\t\t\t\t     rdata.weak);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_INCREFS_DONE:\n\t\tcase BC_ACQUIRE_DONE: {\n\t\t\tbinder_uintptr_t node_ptr;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_node *node;\n\t\t\tbool free_node;\n\n\t\t\tif (get_user(node_ptr, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tnode = binder_get_node(proc, node_ptr);\n\t\t\tif (node == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d %s u%016llx no match\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_INCREFS_DONE ?\n\t\t\t\t\t\"BC_INCREFS_DONE\" :\n\t\t\t\t\t\"BC_ACQUIRE_DONE\",\n\t\t\t\t\t(u64)node_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (cookie != node->cookie) {\n\t\t\t\tbinder_user_error(\"%d:%d %s u%016llx node %d cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_INCREFS_DONE ?\n\t\t\t\t\t\"BC_INCREFS_DONE\" : \"BC_ACQUIRE_DONE\",\n\t\t\t\t\t(u64)node_ptr, node->debug_id,\n\t\t\t\t\t(u64)cookie, (u64)node->cookie);\n\t\t\t\tbinder_put_node(node);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_node_inner_lock(node);\n\t\t\tif (cmd == BC_ACQUIRE_DONE) {\n\t\t\t\tif (node->pending_strong_ref == 0) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_ACQUIRE_DONE node %d has no pending acquire request\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\tnode->debug_id);\n\t\t\t\t\tbinder_node_inner_unlock(node);\n\t\t\t\t\tbinder_put_node(node);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode->pending_strong_ref = 0;\n\t\t\t} else {\n\t\t\t\tif (node->pending_weak_ref == 0) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_INCREFS_DONE node %d has no pending increfs request\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\tnode->debug_id);\n\t\t\t\t\tbinder_node_inner_unlock(node);\n\t\t\t\t\tbinder_put_node(node);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode->pending_weak_ref = 0;\n\t\t\t}\n\t\t\tfree_node = binder_dec_node_nilocked(node,\n\t\t\t\t\tcmd == BC_ACQUIRE_DONE, 0);\n\t\t\tWARN_ON(free_node);\n\t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n\t\t\t\t     \"%d:%d %s node %d ls %d lw %d tr %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     cmd == BC_INCREFS_DONE ? \"BC_INCREFS_DONE\" : \"BC_ACQUIRE_DONE\",\n\t\t\t\t     node->debug_id, node->local_strong_refs,\n\t\t\t\t     node->local_weak_refs, node->tmp_refs);\n\t\t\tbinder_node_inner_unlock(node);\n\t\t\tbinder_put_node(node);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_ATTEMPT_ACQUIRE:\n\t\t\tpr_err(\"BC_ATTEMPT_ACQUIRE not supported\\n\");\n\t\t\treturn -EINVAL;\n\t\tcase BC_ACQUIRE_RESULT:\n\t\t\tpr_err(\"BC_ACQUIRE_RESULT not supported\\n\");\n\t\t\treturn -EINVAL;\n\n\t\tcase BC_FREE_BUFFER: {\n\t\t\tbinder_uintptr_t data_ptr;\n\t\t\tstruct binder_buffer *buffer;\n\n\t\t\tif (get_user(data_ptr, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\n\t\t\tbuffer = binder_alloc_prepare_to_free(&proc->alloc,\n\t\t\t\t\t\t\t      data_ptr);\n\t\t\tif (buffer == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_FREE_BUFFER u%016llx no match\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)data_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!buffer->allow_user_free) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_FREE_BUFFER u%016llx matched unreturned buffer\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)data_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_FREE_BUFFER,\n\t\t\t\t     \"%d:%d BC_FREE_BUFFER u%016llx found buffer %d for %s transaction\\n\",\n\t\t\t\t     proc->pid, thread->pid, (u64)data_ptr,\n\t\t\t\t     buffer->debug_id,\n\t\t\t\t     buffer->transaction ? \"active\" : \"finished\");\n\t\t\tbinder_free_buf(proc, buffer);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BC_TRANSACTION_SG:\n\t\tcase BC_REPLY_SG: {\n\t\t\tstruct binder_transaction_data_sg tr;\n\n\t\t\tif (copy_from_user(&tr, ptr, sizeof(tr)))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(tr);\n\t\t\tbinder_transaction(proc, thread, &tr.transaction_data,\n\t\t\t\t\t   cmd == BC_REPLY_SG, tr.buffers_size);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_TRANSACTION:\n\t\tcase BC_REPLY: {\n\t\t\tstruct binder_transaction_data tr;\n\n\t\t\tif (copy_from_user(&tr, ptr, sizeof(tr)))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(tr);\n\t\t\tbinder_transaction(proc, thread, &tr,\n\t\t\t\t\t   cmd == BC_REPLY, 0);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BC_REGISTER_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_REGISTER_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tif (thread->looper & BINDER_LOOPER_STATE_ENTERED) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_REGISTER_LOOPER called after BC_ENTER_LOOPER\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t} else if (proc->requested_threads == 0) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_REGISTER_LOOPER called without request\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t} else {\n\t\t\t\tproc->requested_threads--;\n\t\t\t\tproc->requested_threads_started++;\n\t\t\t}\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_REGISTERED;\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbreak;\n\t\tcase BC_ENTER_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_ENTER_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tif (thread->looper & BINDER_LOOPER_STATE_REGISTERED) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_ENTER_LOOPER called after BC_REGISTER_LOOPER\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t}\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_ENTERED;\n\t\t\tbreak;\n\t\tcase BC_EXIT_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_EXIT_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_EXITED;\n\t\t\tbreak;\n\n\t\tcase BC_REQUEST_DEATH_NOTIFICATION:\n\t\tcase BC_CLEAR_DEATH_NOTIFICATION: {\n\t\t\tuint32_t target;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_ref *ref;\n\t\t\tstruct binder_ref_death *death = NULL;\n\n\t\t\tif (get_user(target, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tif (cmd == BC_REQUEST_DEATH_NOTIFICATION) {\n\t\t\t\t/*\n\t\t\t\t * Allocate memory for death notification\n\t\t\t\t * before taking lock\n\t\t\t\t */\n\t\t\t\tdeath = kzalloc(sizeof(*death), GFP_KERNEL);\n\t\t\t\tif (death == NULL) {\n\t\t\t\t\tWARN_ON(thread->return_error.cmd !=\n\t\t\t\t\t\tBR_OK);\n\t\t\t\t\tthread->return_error.cmd = BR_ERROR;\n\t\t\t\t\tbinder_enqueue_thread_work(\n\t\t\t\t\t\tthread,\n\t\t\t\t\t\t&thread->return_error.work);\n\t\t\t\t\tbinder_debug(\n\t\t\t\t\t\tBINDER_DEBUG_FAILED_TRANSACTION,\n\t\t\t\t\t\t\"%d:%d BC_REQUEST_DEATH_NOTIFICATION failed\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, target, false);\n\t\t\tif (ref == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d %s invalid ref %d\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_REQUEST_DEATH_NOTIFICATION ?\n\t\t\t\t\t\"BC_REQUEST_DEATH_NOTIFICATION\" :\n\t\t\t\t\t\"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\t\t\t\ttarget);\n\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\tkfree(death);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tbinder_debug(BINDER_DEBUG_DEATH_NOTIFICATION,\n\t\t\t\t     \"%d:%d %s %016llx ref %d desc %d s %d w %d for node %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     cmd == BC_REQUEST_DEATH_NOTIFICATION ?\n\t\t\t\t     \"BC_REQUEST_DEATH_NOTIFICATION\" :\n\t\t\t\t     \"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\t\t\t     (u64)cookie, ref->data.debug_id,\n\t\t\t\t     ref->data.desc, ref->data.strong,\n\t\t\t\t     ref->data.weak, ref->node->debug_id);\n\n\t\t\tbinder_node_lock(ref->node);\n\t\t\tif (cmd == BC_REQUEST_DEATH_NOTIFICATION) {\n\t\t\t\tif (ref->death) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_REQUEST_DEATH_NOTIFICATION death notification already set\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tkfree(death);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbinder_stats_created(BINDER_STAT_DEATH);\n\t\t\t\tINIT_LIST_HEAD(&death->work.entry);\n\t\t\t\tdeath->cookie = cookie;\n\t\t\t\tref->death = death;\n\t\t\t\tif (ref->node->proc == NULL) {\n\t\t\t\t\tref->death->work.type = BINDER_WORK_DEAD_BINDER;\n\n\t\t\t\t\tbinder_inner_proc_lock(proc);\n\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t&ref->death->work, &proc->todo);\n\t\t\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (ref->death == NULL) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_CLEAR_DEATH_NOTIFICATION death notification not active\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tdeath = ref->death;\n\t\t\t\tif (death->cookie != cookie) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_CLEAR_DEATH_NOTIFICATION death notification cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\t(u64)death->cookie,\n\t\t\t\t\t\t(u64)cookie);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tref->death = NULL;\n\t\t\t\tbinder_inner_proc_lock(proc);\n\t\t\t\tif (list_empty(&death->work.entry)) {\n\t\t\t\t\tdeath->work.type = BINDER_WORK_CLEAR_DEATH_NOTIFICATION;\n\t\t\t\t\tif (thread->looper &\n\t\t\t\t\t    (BINDER_LOOPER_STATE_REGISTERED |\n\t\t\t\t\t     BINDER_LOOPER_STATE_ENTERED))\n\t\t\t\t\t\tbinder_enqueue_thread_work_ilocked(\n\t\t\t\t\t\t\t\tthread,\n\t\t\t\t\t\t\t\t&death->work);\n\t\t\t\t\telse {\n\t\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t\t\t&death->work,\n\t\t\t\t\t\t\t\t&proc->todo);\n\t\t\t\t\t\tbinder_wakeup_proc_ilocked(\n\t\t\t\t\t\t\t\tproc);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tBUG_ON(death->work.type != BINDER_WORK_DEAD_BINDER);\n\t\t\t\t\tdeath->work.type = BINDER_WORK_DEAD_BINDER_AND_CLEAR;\n\t\t\t\t}\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t}\n\t\t\tbinder_node_unlock(ref->node);\n\t\t\tbinder_proc_unlock(proc);\n\t\t} break;\n\t\tcase BC_DEAD_BINDER_DONE: {\n\t\t\tstruct binder_work *w;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_ref_death *death = NULL;\n\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tptr += sizeof(cookie);\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tlist_for_each_entry(w, &proc->delivered_death,\n\t\t\t\t\t    entry) {\n\t\t\t\tstruct binder_ref_death *tmp_death =\n\t\t\t\t\tcontainer_of(w,\n\t\t\t\t\t\t     struct binder_ref_death,\n\t\t\t\t\t\t     work);\n\n\t\t\t\tif (tmp_death->cookie == cookie) {\n\t\t\t\t\tdeath = tmp_death;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n\t\t\t\t     \"%d:%d BC_DEAD_BINDER_DONE %016llx found %pK\\n\",\n\t\t\t\t     proc->pid, thread->pid, (u64)cookie,\n\t\t\t\t     death);\n\t\t\tif (death == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_DEAD_BINDER_DONE %016llx not found\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)cookie);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_dequeue_work_ilocked(&death->work);\n\t\t\tif (death->work.type == BINDER_WORK_DEAD_BINDER_AND_CLEAR) {\n\t\t\t\tdeath->work.type = BINDER_WORK_CLEAR_DEATH_NOTIFICATION;\n\t\t\t\tif (thread->looper &\n\t\t\t\t\t(BINDER_LOOPER_STATE_REGISTERED |\n\t\t\t\t\t BINDER_LOOPER_STATE_ENTERED))\n\t\t\t\t\tbinder_enqueue_thread_work_ilocked(\n\t\t\t\t\t\tthread, &death->work);\n\t\t\t\telse {\n\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t\t&death->work,\n\t\t\t\t\t\t\t&proc->todo);\n\t\t\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t} break;\n\n\t\tdefault:\n\t\t\tpr_err(\"%d:%d unknown command %d\\n\",\n\t\t\t       proc->pid, thread->pid, cmd);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t*consumed = ptr - buffer;\n\t}\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2403,
            "cve_id": "CVE-2019-2213",
            "code_snippet": "static void\nbinder_free_buf(struct binder_proc *proc, struct binder_buffer *buffer)\n{\n\tif (buffer->transaction) {\n\t\tbuffer->transaction->buffer = NULL;\n\t\tbuffer->transaction = NULL;\n\t}\n\tif (buffer->async_transaction && buffer->target_node) {\n\t\tstruct binder_node *buf_node;\n\t\tstruct binder_work *w;\n\n\t\tbuf_node = buffer->target_node;\n\t\tbinder_node_inner_lock(buf_node);\n\t\tBUG_ON(!buf_node->has_async_transaction);\n\t\tBUG_ON(buf_node->proc != proc);\n\t\tw = binder_dequeue_work_head_ilocked(\n\t\t\t\t&buf_node->async_todo);\n\t\tif (!w) {\n\t\t\tbuf_node->has_async_transaction = false;\n\t\t} else {\n\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\tw, &proc->todo);\n\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t}\n\t\tbinder_node_inner_unlock(buf_node);\n\t}\n\ttrace_binder_transaction_buffer_release(buffer);\n\tbinder_transaction_buffer_release(proc, buffer, 0, false);\n\tbinder_alloc_free_buf(&proc->alloc, buffer);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2407,
            "cve_id": "CVE-2019-25044",
            "code_snippet": "static void blk_mq_sched_tags_teardown(struct request_queue *q)\n{\n\tstruct blk_mq_tag_set *set = q->tag_set;\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\tblk_mq_sched_free_tags(set, hctx, i);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2411,
            "cve_id": "CVE-2019-25045",
            "code_snippet": "static int\nparse_ipsecrequest(struct xfrm_policy *xp, struct sadb_x_ipsecrequest *rq)\n{\n\tstruct net *net = xp_net(xp);\n\tstruct xfrm_tmpl *t = xp->xfrm_vec + xp->xfrm_nr;\n\tint mode;\n\n\tif (xp->xfrm_nr >= XFRM_MAX_DEPTH)\n\t\treturn -ELOOP;\n\n\tif (rq->sadb_x_ipsecrequest_mode == 0)\n\t\treturn -EINVAL;\n\n\tt->id.proto = rq->sadb_x_ipsecrequest_proto; /* XXX check proto */\n\tif ((mode = pfkey_mode_to_xfrm(rq->sadb_x_ipsecrequest_mode)) < 0)\n\t\treturn -EINVAL;\n\tt->mode = mode;\n\tif (rq->sadb_x_ipsecrequest_level == IPSEC_LEVEL_USE)\n\t\tt->optional = 1;\n\telse if (rq->sadb_x_ipsecrequest_level == IPSEC_LEVEL_UNIQUE) {\n\t\tt->reqid = rq->sadb_x_ipsecrequest_reqid;\n\t\tif (t->reqid > IPSEC_MANUAL_REQID_MAX)\n\t\t\tt->reqid = 0;\n\t\tif (!t->reqid && !(t->reqid = gen_reqid(net)))\n\t\t\treturn -ENOBUFS;\n\t}\n\n\t/* addresses present only in tunnel mode */\n\tif (t->mode == XFRM_MODE_TUNNEL) {\n\t\tint err;\n\n\t\terr = parse_sockaddr_pair(\n\t\t\t(struct sockaddr *)(rq + 1),\n\t\t\trq->sadb_x_ipsecrequest_len - sizeof(*rq),\n\t\t\t&t->saddr, &t->id.daddr, &t->encap_family);\n\t\tif (err)\n\t\t\treturn err;\n\t} else\n\t\tt->encap_family = xp->family;\n\n\t/* No way to set this via kame pfkey */\n\tt->allalgs = 1;\n\txp->xfrm_nr++;\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2459,
            "cve_id": "CVE-2019-9003",
            "code_snippet": "int ipmi_destroy_user(struct ipmi_user *user)\n{\n\t_ipmi_destroy_user(user);\n\n\tcleanup_srcu_struct(&user->release_barrier);\n\tkref_put(&user->refcount, free_user);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2492,
            "cve_id": "CVE-2020-0427",
            "code_snippet": "static int dt_remember_or_free_map(struct pinctrl *p, const char *statename,\n\t\t\t\t   struct pinctrl_dev *pctldev,\n\t\t\t\t   struct pinctrl_map *map, unsigned num_maps)\n{\n\tint i;\n\tstruct pinctrl_dt_map *dt_map;\n\n\t/* Initialize common mapping table entry fields */\n\tfor (i = 0; i < num_maps; i++) {\n\t\tmap[i].dev_name = dev_name(p->dev);\n\t\tmap[i].name = statename;\n\t\tif (pctldev)\n\t\t\tmap[i].ctrl_dev_name = dev_name(pctldev->dev);\n\t}\n\n\t/* Remember the converted mapping table entries */\n\tdt_map = kzalloc(sizeof(*dt_map), GFP_KERNEL);\n\tif (!dt_map) {\n\t\tdt_free_map(pctldev, map, num_maps);\n\t\treturn -ENOMEM;\n\t}\n\n\tdt_map->pctldev = pctldev;\n\tdt_map->map = map;\n\tdt_map->num_maps = num_maps;\n\tlist_add_tail(&dt_map->node, &p->dt_maps);\n\n\treturn pinctrl_register_map(map, num_maps, false);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2494,
            "cve_id": "CVE-2020-0429",
            "code_snippet": "int l2tp_session_delete(struct l2tp_session *session)\n{\n\tif (session->ref)\n\t\t(*session->ref)(session);\n\t__l2tp_session_unhash(session);\n\tl2tp_session_queue_purge(session);\n\tif (session->session_close != NULL)\n\t\t(*session->session_close)(session);\n\tif (session->deref)\n\t\t(*session->deref)(session);\n\tl2tp_session_dec_refcount(session);\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2500,
            "cve_id": "CVE-2020-0433",
            "code_snippet": "static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,\n\t\t\t\t\t\t\tint nr_hw_queues)\n{\n\tstruct request_queue *q;\n\tLIST_HEAD(head);\n\n\tlockdep_assert_held(&set->tag_list_lock);\n\n\tif (nr_hw_queues > nr_cpu_ids)\n\t\tnr_hw_queues = nr_cpu_ids;\n\tif (nr_hw_queues < 1 || nr_hw_queues == set->nr_hw_queues)\n\t\treturn;\n\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n\t\tblk_mq_freeze_queue(q);\n\t/*\n\t * Switch IO scheduler to 'none', cleaning up the data associated\n\t * with the previous scheduler. We will switch back once we are done\n\t * updating the new sw to hw queue mappings.\n\t */\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n\t\tif (!blk_mq_elv_switch_none(&head, q))\n\t\t\tgoto switch_back;\n\n\tset->nr_hw_queues = nr_hw_queues;\n\tblk_mq_update_queue_map(set);\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list) {\n\t\tblk_mq_realloc_hw_ctxs(set, q);\n\t\tblk_mq_queue_reinit(q);\n\t}\n\nswitch_back:\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n\t\tblk_mq_elv_switch_back(&head, q);\n\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n\t\tblk_mq_unfreeze_queue(q);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2511,
            "cve_id": "CVE-2020-10690",
            "code_snippet": "int ptp_clock_unregister(struct ptp_clock *ptp)\n{\n\tptp->defunct = 1;\n\twake_up_interruptible(&ptp->tsev_wq);\n\n\tif (ptp->kworker) {\n\t\tkthread_cancel_delayed_work_sync(&ptp->aux_work);\n\t\tkthread_destroy_worker(ptp->kworker);\n\t}\n\n\t/* Release the clock's resources. */\n\tif (ptp->pps_source)\n\t\tpps_unregister_source(ptp->pps_source);\n\n\tdevice_destroy(ptp_class, ptp->devid);\n\tptp_cleanup_pin_groups(ptp);\n\n\tposix_clock_unregister(&ptp->clock);\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2512,
            "cve_id": "CVE-2020-10690",
            "code_snippet": "static int posix_clock_release(struct inode *inode, struct file *fp)\n{\n\tstruct posix_clock *clk = fp->private_data;\n\tint err = 0;\n\n\tif (clk->ops.release)\n\t\terr = clk->ops.release(clk);\n\n\tkref_put(&clk->kref, delete_clock);\n\n\tfp->private_data = NULL;\n\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2632,
            "cve_id": "CVE-2020-14381",
            "code_snippet": "static void get_futex_key_refs(union futex_key *key)\n{\n\tif (!key->both.ptr)\n\t\treturn;\n\n\t/*\n\t * On MMU less systems futexes are always \"private\" as there is no per\n\t * process address space. We need the smp wmb nevertheless - yes,\n\t * arch/blackfin has MMU less SMP ...\n\t */\n\tif (!IS_ENABLED(CONFIG_MMU)) {\n\t\tsmp_mb(); /* explicit smp_mb(); (B) */\n\t\treturn;\n\t}\n\n\tswitch (key->both.offset & (FUT_OFF_INODE|FUT_OFF_MMSHARED)) {\n\tcase FUT_OFF_INODE:\n\t\tihold(key->shared.inode); /* implies smp_mb(); (B) */\n\t\tbreak;\n\tcase FUT_OFF_MMSHARED:\n\t\tfutex_get_mm(key); /* implies smp_mb(); (B) */\n\t\tbreak;\n\tdefault:\n\t\t/*\n\t\t * Private futexes do not hold reference on an inode or\n\t\t * mm, therefore the only purpose of calling get_futex_key_refs\n\t\t * is because we need the barrier for the lockless waiter check.\n\t\t */\n\t\tsmp_mb(); /* explicit smp_mb(); (B) */\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2653,
            "cve_id": "CVE-2020-14416",
            "code_snippet": "static void slcan_write_wakeup(struct tty_struct *tty)\n{\n\tstruct slcan *sl = tty->disc_data;\n\n\tschedule_work(&sl->tx_work);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2656,
            "cve_id": "CVE-2020-15436",
            "code_snippet": "int blkdev_get(struct block_device *bdev, fmode_t mode, void *holder)\n{\n\tstruct block_device *whole = NULL;\n\tint res;\n\n\tWARN_ON_ONCE((mode & FMODE_EXCL) && !holder);\n\n\tif ((mode & FMODE_EXCL) && holder) {\n\t\twhole = bd_start_claiming(bdev, holder);\n\t\tif (IS_ERR(whole)) {\n\t\t\tbdput(bdev);\n\t\t\treturn PTR_ERR(whole);\n\t\t}\n\t}\n\n\tres = __blkdev_get(bdev, mode, 0);\n\n\tif (whole) {\n\t\tstruct gendisk *disk = whole->bd_disk;\n\n\t\t/* finish claiming */\n\t\tmutex_lock(&bdev->bd_mutex);\n\t\tif (!res)\n\t\t\tbd_finish_claiming(bdev, whole, holder);\n\t\telse\n\t\t\tbd_abort_claiming(bdev, whole, holder);\n\t\t/*\n\t\t * Block event polling for write claims if requested.  Any\n\t\t * write holder makes the write_holder state stick until\n\t\t * all are released.  This is good enough and tracking\n\t\t * individual writeable reference is too fragile given the\n\t\t * way @mode is used in blkdev_get/put().\n\t\t */\n\t\tif (!res && (mode & FMODE_WRITE) && !bdev->bd_write_holder &&\n\t\t    (disk->flags & GENHD_FL_BLOCK_EVENTS_ON_EXCL_WRITE)) {\n\t\t\tbdev->bd_write_holder = true;\n\t\t\tdisk_block_events(disk);\n\t\t}\n\n\t\tmutex_unlock(&bdev->bd_mutex);\n\t\tbdput(whole);\n\t}\n\n\treturn res;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2706,
            "cve_id": "CVE-2020-25656",
            "code_snippet": "static void k_fn(struct vc_data *vc, unsigned char value, char up_flag)\n{\n\tif (up_flag)\n\t\treturn;\n\n\tif ((unsigned)value < ARRAY_SIZE(func_table)) {\n\t\tif (func_table[value])\n\t\t\tputs_queue(vc, func_table[value]);\n\t} else\n\t\tpr_err(\"k_fn called with value=%d\\n\", value);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2709,
            "cve_id": "CVE-2020-25669",
            "code_snippet": "static void sunkbd_enable(struct sunkbd *sunkbd, bool enable)\n{\n\tserio_pause_rx(sunkbd->serio);\n\tsunkbd->enabled = enable;\n\tserio_continue_rx(sunkbd->serio);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2724,
            "cve_id": "CVE-2020-27067",
            "code_snippet": "static int l2tp_eth_create(struct net *net, struct l2tp_tunnel *tunnel,\n\t\t\t   u32 session_id, u32 peer_session_id,\n\t\t\t   struct l2tp_session_cfg *cfg)\n{\n\tunsigned char name_assign_type;\n\tstruct net_device *dev;\n\tchar name[IFNAMSIZ];\n\tstruct l2tp_session *session;\n\tstruct l2tp_eth *priv;\n\tstruct l2tp_eth_sess *spriv;\n\tint rc;\n\tstruct l2tp_eth_net *pn;\n\n\tif (cfg->ifname) {\n\t\tstrlcpy(name, cfg->ifname, IFNAMSIZ);\n\t\tname_assign_type = NET_NAME_USER;\n\t} else {\n\t\tstrcpy(name, L2TP_ETH_DEV_NAME);\n\t\tname_assign_type = NET_NAME_ENUM;\n\t}\n\n\tsession = l2tp_session_create(sizeof(*spriv), tunnel, session_id,\n\t\t\t\t      peer_session_id, cfg);\n\tif (IS_ERR(session)) {\n\t\trc = PTR_ERR(session);\n\t\tgoto out;\n\t}\n\n\tdev = alloc_netdev(sizeof(*priv), name, name_assign_type,\n\t\t\t   l2tp_eth_dev_setup);\n\tif (!dev) {\n\t\trc = -ENOMEM;\n\t\tgoto out_del_session;\n\t}\n\n\tdev_net_set(dev, net);\n\tdev->min_mtu = 0;\n\tdev->max_mtu = ETH_MAX_MTU;\n\tl2tp_eth_adjust_mtu(tunnel, session, dev);\n\n\tpriv = netdev_priv(dev);\n\tpriv->dev = dev;\n\tpriv->session = session;\n\tINIT_LIST_HEAD(&priv->list);\n\n\tpriv->tunnel_sock = tunnel->sock;\n\tsession->recv_skb = l2tp_eth_dev_recv;\n\tsession->session_close = l2tp_eth_delete;\n#if IS_ENABLED(CONFIG_L2TP_DEBUGFS)\n\tsession->show = l2tp_eth_show;\n#endif\n\n\tspriv = l2tp_session_priv(session);\n\tspriv->dev = dev;\n\n\trc = register_netdev(dev);\n\tif (rc < 0)\n\t\tgoto out_del_dev;\n\n\t__module_get(THIS_MODULE);\n\t/* Must be done after register_netdev() */\n\tstrlcpy(session->ifname, dev->name, IFNAMSIZ);\n\n\tdev_hold(dev);\n\tpn = l2tp_eth_pernet(dev_net(dev));\n\tspin_lock(&pn->l2tp_eth_lock);\n\tlist_add(&priv->list, &pn->l2tp_eth_dev_list);\n\tspin_unlock(&pn->l2tp_eth_lock);\n\n\treturn 0;\n\nout_del_dev:\n\tfree_netdev(dev);\n\tspriv->dev = NULL;\nout_del_session:\n\tl2tp_session_delete(session);\nout:\n\treturn rc;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2734,
            "cve_id": "CVE-2020-27675",
            "code_snippet": "static void clear_evtchn_to_irq_row(unsigned row)\n{\n\tunsigned col;\n\n\tfor (col = 0; col < EVTCHN_PER_ROW; col++)\n\t\tevtchn_to_irq[row][col] = -1;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2735,
            "cve_id": "CVE-2020-27675",
            "code_snippet": "static void xen_free_irq(unsigned irq)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\n\tif (WARN_ON(!info))\n\t\treturn;\n\n\tlist_del(&info->list);\n\n\tset_info_for_irq(irq, NULL);\n\n\tWARN_ON(info->refcnt > 0);\n\n\tkfree(info);\n\n\t/* Legacy IRQ descriptors are managed by the arch. */\n\tif (irq < nr_legacy_irqs())\n\t\treturn;\n\n\tirq_free_desc(irq);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2742,
            "cve_id": "CVE-2020-27784",
            "code_snippet": "static int\nprinter_close(struct inode *inode, struct file *fd)\n{\n\tstruct printer_dev\t*dev = fd->private_data;\n\tunsigned long\t\tflags;\n\n\tspin_lock_irqsave(&dev->lock, flags);\n\tdev->printer_cdev_open = 0;\n\tfd->private_data = NULL;\n\t/* Change printer status to show that the printer is off-line. */\n\tdev->printer_status &= ~PRINTER_SELECTED;\n\tspin_unlock_irqrestore(&dev->lock, flags);\n\n\tDBG(dev, \"printer_close\\n\");\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2746,
            "cve_id": "CVE-2020-27786",
            "code_snippet": "static long snd_rawmidi_kernel_write1(struct snd_rawmidi_substream *substream,\n\t\t\t\t      const unsigned char __user *userbuf,\n\t\t\t\t      const unsigned char *kernelbuf,\n\t\t\t\t      long count)\n{\n\tunsigned long flags;\n\tlong count1, result;\n\tstruct snd_rawmidi_runtime *runtime = substream->runtime;\n\tunsigned long appl_ptr;\n\n\tif (!kernelbuf && !userbuf)\n\t\treturn -EINVAL;\n\tif (snd_BUG_ON(!runtime->buffer))\n\t\treturn -EINVAL;\n\n\tresult = 0;\n\tspin_lock_irqsave(&runtime->lock, flags);\n\tif (substream->append) {\n\t\tif ((long)runtime->avail < count) {\n\t\t\tspin_unlock_irqrestore(&runtime->lock, flags);\n\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\twhile (count > 0 && runtime->avail > 0) {\n\t\tcount1 = runtime->buffer_size - runtime->appl_ptr;\n\t\tif (count1 > count)\n\t\t\tcount1 = count;\n\t\tif (count1 > (long)runtime->avail)\n\t\t\tcount1 = runtime->avail;\n\n\t\t/* update runtime->appl_ptr before unlocking for userbuf */\n\t\tappl_ptr = runtime->appl_ptr;\n\t\truntime->appl_ptr += count1;\n\t\truntime->appl_ptr %= runtime->buffer_size;\n\t\truntime->avail -= count1;\n\n\t\tif (kernelbuf)\n\t\t\tmemcpy(runtime->buffer + appl_ptr,\n\t\t\t       kernelbuf + result, count1);\n\t\telse if (userbuf) {\n\t\t\tspin_unlock_irqrestore(&runtime->lock, flags);\n\t\t\tif (copy_from_user(runtime->buffer + appl_ptr,\n\t\t\t\t\t   userbuf + result, count1)) {\n\t\t\t\tspin_lock_irqsave(&runtime->lock, flags);\n\t\t\t\tresult = result > 0 ? result : -EFAULT;\n\t\t\t\tgoto __end;\n\t\t\t}\n\t\t\tspin_lock_irqsave(&runtime->lock, flags);\n\t\t}\n\t\tresult += count1;\n\t\tcount -= count1;\n\t}\n      __end:\n\tcount1 = runtime->avail < runtime->buffer_size;\n\tspin_unlock_irqrestore(&runtime->lock, flags);\n\tif (count1)\n\t\tsnd_rawmidi_output_trigger(substream, 1);\n\treturn result;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2754,
            "cve_id": "CVE-2020-27835",
            "code_snippet": "static int hfi1_file_open(struct inode *inode, struct file *fp)\n{\n\tstruct hfi1_filedata *fd;\n\tstruct hfi1_devdata *dd = container_of(inode->i_cdev,\n\t\t\t\t\t       struct hfi1_devdata,\n\t\t\t\t\t       user_cdev);\n\n\tif (!((dd->flags & HFI1_PRESENT) && dd->kregbase1))\n\t\treturn -EINVAL;\n\n\tif (!atomic_inc_not_zero(&dd->user_refcount))\n\t\treturn -ENXIO;\n\n\t/* The real work is performed later in assign_ctxt() */\n\n\tfd = kzalloc(sizeof(*fd), GFP_KERNEL);\n\n\tif (!fd || init_srcu_struct(&fd->pq_srcu))\n\t\tgoto nomem;\n\tspin_lock_init(&fd->pq_rcu_lock);\n\tspin_lock_init(&fd->tid_lock);\n\tspin_lock_init(&fd->invalid_lock);\n\tfd->rec_cpu_num = -1; /* no cpu affinity by default */\n\tfd->mm = current->mm;\n\tmmgrab(fd->mm);\n\tfd->dd = dd;\n\tfp->private_data = fd;\n\treturn 0;\nnomem:\n\tkfree(fd);\n\tfp->private_data = NULL;\n\tif (atomic_dec_and_test(&dd->user_refcount))\n\t\tcomplete(&dd->user_comp);\n\treturn -ENOMEM;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2755,
            "cve_id": "CVE-2020-27835",
            "code_snippet": "int hfi1_mmu_rb_insert(struct mmu_rb_handler *handler,\n\t\t       struct mmu_rb_node *mnode)\n{\n\tstruct mmu_rb_node *node;\n\tunsigned long flags;\n\tint ret = 0;\n\n\ttrace_hfi1_mmu_rb_insert(mnode->addr, mnode->len);\n\tspin_lock_irqsave(&handler->lock, flags);\n\tnode = __mmu_rb_search(handler, mnode->addr, mnode->len);\n\tif (node) {\n\t\tret = -EINVAL;\n\t\tgoto unlock;\n\t}\n\t__mmu_int_rb_insert(mnode, &handler->root);\n\tlist_add(&mnode->list, &handler->lru_list);\n\n\tret = handler->ops->insert(handler->ops_arg, mnode);\n\tif (ret) {\n\t\t__mmu_int_rb_remove(mnode, &handler->root);\n\t\tlist_del(&mnode->list); /* remove from LRU list */\n\t}\nunlock:\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2756,
            "cve_id": "CVE-2020-27835",
            "code_snippet": "void hfi1_mmu_rb_evict(struct mmu_rb_handler *handler, void *evict_arg)\n{\n\tstruct mmu_rb_node *rbnode, *ptr;\n\tstruct list_head del_list;\n\tunsigned long flags;\n\tbool stop = false;\n\n\tINIT_LIST_HEAD(&del_list);\n\n\tspin_lock_irqsave(&handler->lock, flags);\n\tlist_for_each_entry_safe_reverse(rbnode, ptr, &handler->lru_list,\n\t\t\t\t\t list) {\n\t\tif (handler->ops->evict(handler->ops_arg, rbnode, evict_arg,\n\t\t\t\t\t&stop)) {\n\t\t\t__mmu_int_rb_remove(rbnode, &handler->root);\n\t\t\t/* move from LRU list to delete list */\n\t\t\tlist_move(&rbnode->list, &del_list);\n\t\t}\n\t\tif (stop)\n\t\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\n\twhile (!list_empty(&del_list)) {\n\t\trbnode = list_first_entry(&del_list, struct mmu_rb_node, list);\n\t\tlist_del(&rbnode->list);\n\t\thandler->ops->remove(handler->ops_arg, rbnode);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2757,
            "cve_id": "CVE-2020-27835",
            "code_snippet": "bool hfi1_mmu_rb_remove_unless_exact(struct mmu_rb_handler *handler,\n\t\t\t\t     unsigned long addr, unsigned long len,\n\t\t\t\t     struct mmu_rb_node **rb_node)\n{\n\tstruct mmu_rb_node *node;\n\tunsigned long flags;\n\tbool ret = false;\n\n\tspin_lock_irqsave(&handler->lock, flags);\n\tnode = __mmu_rb_search(handler, addr, len);\n\tif (node) {\n\t\tif (node->addr == addr && node->len == len)\n\t\t\tgoto unlock;\n\t\t__mmu_int_rb_remove(node, &handler->root);\n\t\tlist_del(&node->list); /* remove from LRU list */\n\t\tret = true;\n\t}\nunlock:\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\t*rb_node = node;\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2758,
            "cve_id": "CVE-2020-27835",
            "code_snippet": "void hfi1_mmu_rb_remove(struct mmu_rb_handler *handler,\n\t\t\tstruct mmu_rb_node *node)\n{\n\tunsigned long flags;\n\n\t/* Validity of handler and node pointers has been checked by caller. */\n\ttrace_hfi1_mmu_rb_remove(node->addr, node->len);\n\tspin_lock_irqsave(&handler->lock, flags);\n\t__mmu_int_rb_remove(node, &handler->root);\n\tlist_del(&node->list); /* remove from LRU list */\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\n\thandler->ops->remove(handler->ops_arg, node);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2759,
            "cve_id": "CVE-2020-27835",
            "code_snippet": "void hfi1_mmu_rb_unregister(struct mmu_rb_handler *handler)\n{\n\tstruct mmu_rb_node *rbnode;\n\tstruct rb_node *node;\n\tunsigned long flags;\n\tstruct list_head del_list;\n\n\t/* Unregister first so we don't get any more notifications. */\n\tmmu_notifier_unregister(&handler->mn, handler->mm);\n\n\t/*\n\t * Make sure the wq delete handler is finished running.  It will not\n\t * be triggered once the mmu notifiers are unregistered above.\n\t */\n\tflush_work(&handler->del_work);\n\n\tINIT_LIST_HEAD(&del_list);\n\n\tspin_lock_irqsave(&handler->lock, flags);\n\twhile ((node = rb_first_cached(&handler->root))) {\n\t\trbnode = rb_entry(node, struct mmu_rb_node, node);\n\t\trb_erase_cached(node, &handler->root);\n\t\t/* move from LRU list to delete list */\n\t\tlist_move(&rbnode->list, &del_list);\n\t}\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\n\tdo_remove(handler, &del_list);\n\n\tkfree(handler);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2813,
            "cve_id": "CVE-2020-29660",
            "code_snippet": "void disassociate_ctty(int on_exit)\n{\n\tstruct tty_struct *tty;\n\n\tif (!current->signal->leader)\n\t\treturn;\n\n\ttty = get_current_tty();\n\tif (tty) {\n\t\tif (on_exit && tty->driver->type != TTY_DRIVER_TYPE_PTY) {\n\t\t\ttty_vhangup_session(tty);\n\t\t} else {\n\t\t\tstruct pid *tty_pgrp = tty_get_pgrp(tty);\n\t\t\tif (tty_pgrp) {\n\t\t\t\tkill_pgrp(tty_pgrp, SIGHUP, on_exit);\n\t\t\t\tif (!on_exit)\n\t\t\t\t\tkill_pgrp(tty_pgrp, SIGCONT, on_exit);\n\t\t\t\tput_pid(tty_pgrp);\n\t\t\t}\n\t\t}\n\t\ttty_kref_put(tty);\n\n\t} else if (on_exit) {\n\t\tstruct pid *old_pgrp;\n\t\tspin_lock_irq(&current->sighand->siglock);\n\t\told_pgrp = current->signal->tty_old_pgrp;\n\t\tcurrent->signal->tty_old_pgrp = NULL;\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\t\tif (old_pgrp) {\n\t\t\tkill_pgrp(old_pgrp, SIGHUP, on_exit);\n\t\t\tkill_pgrp(old_pgrp, SIGCONT, on_exit);\n\t\t\tput_pid(old_pgrp);\n\t\t}\n\t\treturn;\n\t}\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tput_pid(current->signal->tty_old_pgrp);\n\tcurrent->signal->tty_old_pgrp = NULL;\n\n\ttty = tty_kref_get(current->signal->tty);\n\tif (tty) {\n\t\tunsigned long flags;\n\t\tspin_lock_irqsave(&tty->ctrl_lock, flags);\n\t\tput_pid(tty->session);\n\t\tput_pid(tty->pgrp);\n\t\ttty->session = NULL;\n\t\ttty->pgrp = NULL;\n\t\tspin_unlock_irqrestore(&tty->ctrl_lock, flags);\n\t\ttty_kref_put(tty);\n\t}\n\n\tspin_unlock_irq(&current->sighand->siglock);\n\t/* Now clear signal->tty under the lock */\n\tread_lock(&tasklist_lock);\n\tsession_clear_tty(task_session(current));\n\tread_unlock(&tasklist_lock);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2814,
            "cve_id": "CVE-2020-29660",
            "code_snippet": "static void __proc_set_tty(struct tty_struct *tty)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tty->ctrl_lock, flags);\n\t/*\n\t * The session and fg pgrp references will be non-NULL if\n\t * tiocsctty() is stealing the controlling tty\n\t */\n\tput_pid(tty->session);\n\tput_pid(tty->pgrp);\n\ttty->pgrp = get_pid(task_pgrp(current));\n\tspin_unlock_irqrestore(&tty->ctrl_lock, flags);\n\ttty->session = get_pid(task_session(current));\n\tif (current->signal->tty) {\n\t\ttty_debug(tty, \"current tty %s not NULL!!\\n\",\n\t\t\t  current->signal->tty->name);\n\t\ttty_kref_put(current->signal->tty);\n\t}\n\tput_pid(current->signal->tty_old_pgrp);\n\tcurrent->signal->tty = tty_kref_get(tty);\n\tcurrent->signal->tty_old_pgrp = NULL;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2830,
            "cve_id": "CVE-2020-36313",
            "code_snippet": "static inline struct kvm_memory_slot *\nsearch_memslots(struct kvm_memslots *slots, gfn_t gfn)\n{\n\tint start = 0, end = slots->used_slots;\n\tint slot = atomic_read(&slots->lru_slot);\n\tstruct kvm_memory_slot *memslots = slots->memslots;\n\n\tif (gfn >= memslots[slot].base_gfn &&\n\t    gfn < memslots[slot].base_gfn + memslots[slot].npages)\n\t\treturn &memslots[slot];\n\n\twhile (start < end) {\n\t\tslot = start + (end - start) / 2;\n\n\t\tif (gfn >= memslots[slot].base_gfn)\n\t\t\tend = slot;\n\t\telse\n\t\t\tstart = slot + 1;\n\t}\n\n\tif (gfn >= memslots[start].base_gfn &&\n\t    gfn < memslots[start].base_gfn + memslots[start].npages) {\n\t\tatomic_set(&slots->lru_slot, start);\n\t\treturn &memslots[start];\n\t}\n\n\treturn NULL;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2867,
            "cve_id": "CVE-2020-36387",
            "code_snippet": "static void io_poll_task_func(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct io_kiocb *nxt = NULL;\n\n\tio_poll_task_handler(req, &nxt);\n\tif (nxt)\n\t\t__io_req_task_submit(nxt);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2868,
            "cve_id": "CVE-2020-36387",
            "code_snippet": "static void io_async_task_func(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct async_poll *apoll = req->apoll;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\ttrace_io_uring_task_run(req->ctx, req->opcode, req->user_data);\n\n\tif (io_poll_rewait(req, &apoll->poll)) {\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\t\treturn;\n\t}\n\n\t/* If req is still hashed, it cannot have been canceled. Don't check. */\n\tif (hash_hashed(&req->hash_node))\n\t\thash_del(&req->hash_node);\n\n\tio_poll_remove_double(req, apoll->double_poll);\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tif (!READ_ONCE(apoll->poll.canceled))\n\t\t__io_req_task_submit(req);\n\telse\n\t\t__io_req_task_cancel(req, -ECANCELED);\n\n\tkfree(apoll->double_poll);\n\tkfree(apoll);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2869,
            "cve_id": "CVE-2020-36387",
            "code_snippet": "static int io_async_buf_func(struct wait_queue_entry *wait, unsigned mode,\n\t\t\t     int sync, void *arg)\n{\n\tstruct wait_page_queue *wpq;\n\tstruct io_kiocb *req = wait->private;\n\tstruct wait_page_key *key = arg;\n\tint ret;\n\n\twpq = container_of(wait, struct wait_page_queue, wait);\n\n\tif (!wake_page_match(wpq, key))\n\t\treturn 0;\n\n\tlist_del_init(&wait->entry);\n\n\tinit_task_work(&req->task_work, io_req_task_submit);\n\t/* submit ref gets dropped, acquire a new one */\n\trefcount_inc(&req->refs);\n\tret = io_req_task_work_add(req, &req->task_work);\n\tif (unlikely(ret)) {\n\t\tstruct task_struct *tsk;\n\n\t\t/* queue just for cancelation */\n\t\tinit_task_work(&req->task_work, io_req_task_cancel);\n\t\ttsk = io_wq_get_task(req->ctx->io_wq);\n\t\ttask_work_add(tsk, &req->task_work, 0);\n\t\twake_up_process(tsk);\n\t}\n\treturn 1;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2876,
            "cve_id": "CVE-2020-36557",
            "code_snippet": "int vc_allocate(unsigned int currcons)\t/* return 0 on success */\n{\n\tstruct vt_notifier_param param;\n\tstruct vc_data *vc;\n\n\tWARN_CONSOLE_UNLOCKED();\n\n\tif (currcons >= MAX_NR_CONSOLES)\n\t\treturn -ENXIO;\n\n\tif (vc_cons[currcons].d)\n\t\treturn 0;\n\n\t/* due to the granularity of kmalloc, we waste some memory here */\n\t/* the alloc is done in two steps, to optimize the common situation\n\t   of a 25x80 console (structsize=216, screenbuf_size=4000) */\n\t/* although the numbers above are not valid since long ago, the\n\t   point is still up-to-date and the comment still has its value\n\t   even if only as a historical artifact.  --mj, July 1998 */\n\tparam.vc = vc = kzalloc(sizeof(struct vc_data), GFP_KERNEL);\n\tif (!vc)\n\t\treturn -ENOMEM;\n\n\tvc_cons[currcons].d = vc;\n\ttty_port_init(&vc->port);\n\tINIT_WORK(&vc_cons[currcons].SAK_work, vc_SAK);\n\n\tvisual_init(vc, currcons, 1);\n\n\tif (!*vc->vc_uni_pagedir_loc)\n\t\tcon_set_default_unimap(vc);\n\n\tvc->vc_screenbuf = kzalloc(vc->vc_screenbuf_size, GFP_KERNEL);\n\tif (!vc->vc_screenbuf)\n\t\tgoto err_free;\n\n\t/* If no drivers have overridden us and the user didn't pass a\n\t   boot option, default to displaying the cursor */\n\tif (global_cursor_default == -1)\n\t\tglobal_cursor_default = 1;\n\n\tvc_init(vc, vc->vc_rows, vc->vc_cols, 1);\n\tvcs_make_sysfs(currcons);\n\tatomic_notifier_call_chain(&vt_notifier_list, VT_ALLOCATE, &param);\n\n\treturn 0;\nerr_free:\n\tvisual_deinit(vc);\n\tkfree(vc);\n\tvc_cons[currcons].d = NULL;\n\treturn -ENOMEM;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2883,
            "cve_id": "CVE-2020-36694",
            "code_snippet": "unsigned int arpt_do_table(struct sk_buff *skb,\n\t\t\t   const struct nf_hook_state *state,\n\t\t\t   struct xt_table *table)\n{\n\tunsigned int hook = state->hook;\n\tstatic const char nulldevname[IFNAMSIZ] __attribute__((aligned(sizeof(long))));\n\tunsigned int verdict = NF_DROP;\n\tconst struct arphdr *arp;\n\tstruct arpt_entry *e, **jumpstack;\n\tconst char *indev, *outdev;\n\tconst void *table_base;\n\tunsigned int cpu, stackidx = 0;\n\tconst struct xt_table_info *private;\n\tstruct xt_action_param acpar;\n\tunsigned int addend;\n\n\tif (!pskb_may_pull(skb, arp_hdr_len(skb->dev)))\n\t\treturn NF_DROP;\n\n\tindev = state->in ? state->in->name : nulldevname;\n\toutdev = state->out ? state->out->name : nulldevname;\n\n\tlocal_bh_disable();\n\taddend = xt_write_recseq_begin();\n\tprivate = READ_ONCE(table->private); /* Address dependency. */\n\tcpu     = smp_processor_id();\n\ttable_base = private->entries;\n\tjumpstack  = (struct arpt_entry **)private->jumpstack[cpu];\n\n\t/* No TEE support for arptables, so no need to switch to alternate\n\t * stack.  All targets that reenter must return absolute verdicts.\n\t */\n\te = get_entry(table_base, private->hook_entry[hook]);\n\n\tacpar.state   = state;\n\tacpar.hotdrop = false;\n\n\tarp = arp_hdr(skb);\n\tdo {\n\t\tconst struct xt_entry_target *t;\n\t\tstruct xt_counters *counter;\n\n\t\tif (!arp_packet_match(arp, skb->dev, indev, outdev, &e->arp)) {\n\t\t\te = arpt_next_entry(e);\n\t\t\tcontinue;\n\t\t}\n\n\t\tcounter = xt_get_this_cpu_counter(&e->counters);\n\t\tADD_COUNTER(*counter, arp_hdr_len(skb->dev), 1);\n\n\t\tt = arpt_get_target_c(e);\n\n\t\t/* Standard target? */\n\t\tif (!t->u.kernel.target->target) {\n\t\t\tint v;\n\n\t\t\tv = ((struct xt_standard_target *)t)->verdict;\n\t\t\tif (v < 0) {\n\t\t\t\t/* Pop from stack? */\n\t\t\t\tif (v != XT_RETURN) {\n\t\t\t\t\tverdict = (unsigned int)(-v) - 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (stackidx == 0) {\n\t\t\t\t\te = get_entry(table_base,\n\t\t\t\t\t\t      private->underflow[hook]);\n\t\t\t\t} else {\n\t\t\t\t\te = jumpstack[--stackidx];\n\t\t\t\t\te = arpt_next_entry(e);\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (table_base + v\n\t\t\t    != arpt_next_entry(e)) {\n\t\t\t\tif (unlikely(stackidx >= private->stacksize)) {\n\t\t\t\t\tverdict = NF_DROP;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tjumpstack[stackidx++] = e;\n\t\t\t}\n\n\t\t\te = get_entry(table_base, v);\n\t\t\tcontinue;\n\t\t}\n\n\t\tacpar.target   = t->u.kernel.target;\n\t\tacpar.targinfo = t->data;\n\t\tverdict = t->u.kernel.target->target(skb, &acpar);\n\n\t\tif (verdict == XT_CONTINUE) {\n\t\t\t/* Target might have changed stuff. */\n\t\t\tarp = arp_hdr(skb);\n\t\t\te = arpt_next_entry(e);\n\t\t} else {\n\t\t\t/* Verdict */\n\t\t\tbreak;\n\t\t}\n\t} while (!acpar.hotdrop);\n\txt_write_recseq_end(addend);\n\tlocal_bh_enable();\n\n\tif (acpar.hotdrop)\n\t\treturn NF_DROP;\n\telse\n\t\treturn verdict;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2884,
            "cve_id": "CVE-2020-36694",
            "code_snippet": "static int get_info(struct net *net, void __user *user, const int *len)\n{\n\tchar name[XT_TABLE_MAXNAMELEN];\n\tstruct xt_table *t;\n\tint ret;\n\n\tif (*len != sizeof(struct arpt_getinfo))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(name, user, sizeof(name)) != 0)\n\t\treturn -EFAULT;\n\n\tname[XT_TABLE_MAXNAMELEN-1] = '\\0';\n#ifdef CONFIG_COMPAT\n\tif (in_compat_syscall())\n\t\txt_compat_lock(NFPROTO_ARP);\n#endif\n\tt = xt_request_find_table_lock(net, NFPROTO_ARP, name);\n\tif (!IS_ERR(t)) {\n\t\tstruct arpt_getinfo info;\n\t\tconst struct xt_table_info *private = t->private;\n#ifdef CONFIG_COMPAT\n\t\tstruct xt_table_info tmp;\n\n\t\tif (in_compat_syscall()) {\n\t\t\tret = compat_table_info(private, &tmp);\n\t\t\txt_compat_flush_offsets(NFPROTO_ARP);\n\t\t\tprivate = &tmp;\n\t\t}\n#endif\n\t\tmemset(&info, 0, sizeof(info));\n\t\tinfo.valid_hooks = t->valid_hooks;\n\t\tmemcpy(info.hook_entry, private->hook_entry,\n\t\t       sizeof(info.hook_entry));\n\t\tmemcpy(info.underflow, private->underflow,\n\t\t       sizeof(info.underflow));\n\t\tinfo.num_entries = private->number;\n\t\tinfo.size = private->size;\n\t\tstrcpy(info.name, name);\n\n\t\tif (copy_to_user(user, &info, *len) != 0)\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = 0;\n\t\txt_table_unlock(t);\n\t\tmodule_put(t->me);\n\t} else\n\t\tret = PTR_ERR(t);\n#ifdef CONFIG_COMPAT\n\tif (in_compat_syscall())\n\t\txt_compat_unlock(NFPROTO_ARP);\n#endif\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2885,
            "cve_id": "CVE-2020-36694",
            "code_snippet": "static int compat_copy_entries_to_user(unsigned int total_size,\n\t\t\t\t       struct xt_table *table,\n\t\t\t\t       void __user *userptr)\n{\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = table->private;\n\tvoid __user *pos;\n\tunsigned int size;\n\tint ret = 0;\n\tunsigned int i = 0;\n\tstruct arpt_entry *iter;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tpos = userptr;\n\tsize = total_size;\n\txt_entry_foreach(iter, private->entries, total_size) {\n\t\tret = compat_copy_entry_to_user(iter, &pos,\n\t\t\t\t\t\t&size, counters, i++);\n\t\tif (ret != 0)\n\t\t\tbreak;\n\t}\n\tvfree(counters);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2886,
            "cve_id": "CVE-2020-36694",
            "code_snippet": "static int do_add_counters(struct net *net, sockptr_t arg, unsigned int len)\n{\n\tunsigned int i;\n\tstruct xt_counters_info tmp;\n\tstruct xt_counters *paddc;\n\tstruct xt_table *t;\n\tconst struct xt_table_info *private;\n\tint ret = 0;\n\tstruct arpt_entry *iter;\n\tunsigned int addend;\n\n\tpaddc = xt_copy_counters(arg, len, &tmp);\n\tif (IS_ERR(paddc))\n\t\treturn PTR_ERR(paddc);\n\n\tt = xt_find_table_lock(net, NFPROTO_ARP, tmp.name);\n\tif (IS_ERR(t)) {\n\t\tret = PTR_ERR(t);\n\t\tgoto free;\n\t}\n\n\tlocal_bh_disable();\n\tprivate = t->private;\n\tif (private->number != tmp.num_counters) {\n\t\tret = -EINVAL;\n\t\tgoto unlock_up_free;\n\t}\n\n\ti = 0;\n\n\taddend = xt_write_recseq_begin();\n\txt_entry_foreach(iter,  private->entries, private->size) {\n\t\tstruct xt_counters *tmp;\n\n\t\ttmp = xt_get_this_cpu_counter(&iter->counters);\n\t\tADD_COUNTER(*tmp, paddc[i].bcnt, paddc[i].pcnt);\n\t\t++i;\n\t}\n\txt_write_recseq_end(addend);\n unlock_up_free:\n\tlocal_bh_enable();\n\txt_table_unlock(t);\n\tmodule_put(t->me);\n free:\n\tvfree(paddc);\n\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2887,
            "cve_id": "CVE-2020-36694",
            "code_snippet": "static int get_entries(struct net *net, struct arpt_get_entries __user *uptr,\n\t\t       const int *len)\n{\n\tint ret;\n\tstruct arpt_get_entries get;\n\tstruct xt_table *t;\n\n\tif (*len < sizeof(get))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&get, uptr, sizeof(get)) != 0)\n\t\treturn -EFAULT;\n\tif (*len != sizeof(struct arpt_get_entries) + get.size)\n\t\treturn -EINVAL;\n\n\tget.name[sizeof(get.name) - 1] = '\\0';\n\n\tt = xt_find_table_lock(net, NFPROTO_ARP, get.name);\n\tif (!IS_ERR(t)) {\n\t\tconst struct xt_table_info *private = t->private;\n\n\t\tif (get.size == private->size)\n\t\t\tret = copy_entries_to_user(private->size,\n\t\t\t\t\t\t   t, uptr->entrytable);\n\t\telse\n\t\t\tret = -EAGAIN;\n\n\t\tmodule_put(t->me);\n\t\txt_table_unlock(t);\n\t} else\n\t\tret = PTR_ERR(t);\n\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2888,
            "cve_id": "CVE-2020-36694",
            "code_snippet": "static int copy_entries_to_user(unsigned int total_size,\n\t\t\t\tconst struct xt_table *table,\n\t\t\t\tvoid __user *userptr)\n{\n\tunsigned int off, num;\n\tconst struct arpt_entry *e;\n\tstruct xt_counters *counters;\n\tstruct xt_table_info *private = table->private;\n\tint ret = 0;\n\tvoid *loc_cpu_entry;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tloc_cpu_entry = private->entries;\n\n\t/* FIXME: use iterator macros --RR */\n\t/* ... then go back and fix counters and names */\n\tfor (off = 0, num = 0; off < total_size; off += e->next_offset, num++){\n\t\tconst struct xt_entry_target *t;\n\n\t\te = loc_cpu_entry + off;\n\t\tif (copy_to_user(userptr + off, e, sizeof(*e))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\t\tif (copy_to_user(userptr + off\n\t\t\t\t + offsetof(struct arpt_entry, counters),\n\t\t\t\t &counters[num],\n\t\t\t\t sizeof(counters[num])) != 0) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\n\t\tt = arpt_get_target_c(e);\n\t\tif (xt_target_to_user(t, userptr + off + e->target_offset)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\t}\n\n free_counters:\n\tvfree(counters);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2889,
            "cve_id": "CVE-2020-36694",
            "code_snippet": "static struct xt_counters *alloc_counters(const struct xt_table *table)\n{\n\tunsigned int countersize;\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = table->private;\n\n\t/* We need atomic snapshot of counters: rest doesn't change\n\t   (other than comefrom, which userspace doesn't care\n\t   about). */\n\tcountersize = sizeof(struct xt_counters) * private->number;\n\tcounters = vzalloc(countersize);\n\n\tif (counters == NULL)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tget_counters(private, counters);\n\n\treturn counters;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2890,
            "cve_id": "CVE-2020-36694",
            "code_snippet": "static int\nget_entries(struct net *net, struct ipt_get_entries __user *uptr,\n\t    const int *len)\n{\n\tint ret;\n\tstruct ipt_get_entries get;\n\tstruct xt_table *t;\n\n\tif (*len < sizeof(get))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&get, uptr, sizeof(get)) != 0)\n\t\treturn -EFAULT;\n\tif (*len != sizeof(struct ipt_get_entries) + get.size)\n\t\treturn -EINVAL;\n\tget.name[sizeof(get.name) - 1] = '\\0';\n\n\tt = xt_find_table_lock(net, AF_INET, get.name);\n\tif (!IS_ERR(t)) {\n\t\tconst struct xt_table_info *private = t->private;\n\t\tif (get.size == private->size)\n\t\t\tret = copy_entries_to_user(private->size,\n\t\t\t\t\t\t   t, uptr->entrytable);\n\t\telse\n\t\t\tret = -EAGAIN;\n\n\t\tmodule_put(t->me);\n\t\txt_table_unlock(t);\n\t} else\n\t\tret = PTR_ERR(t);\n\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2891,
            "cve_id": "CVE-2020-36694",
            "code_snippet": "unsigned int\nipt_do_table(struct sk_buff *skb,\n\t     const struct nf_hook_state *state,\n\t     struct xt_table *table)\n{\n\tunsigned int hook = state->hook;\n\tstatic const char nulldevname[IFNAMSIZ] __attribute__((aligned(sizeof(long))));\n\tconst struct iphdr *ip;\n\t/* Initializing verdict to NF_DROP keeps gcc happy. */\n\tunsigned int verdict = NF_DROP;\n\tconst char *indev, *outdev;\n\tconst void *table_base;\n\tstruct ipt_entry *e, **jumpstack;\n\tunsigned int stackidx, cpu;\n\tconst struct xt_table_info *private;\n\tstruct xt_action_param acpar;\n\tunsigned int addend;\n\n\t/* Initialization */\n\tstackidx = 0;\n\tip = ip_hdr(skb);\n\tindev = state->in ? state->in->name : nulldevname;\n\toutdev = state->out ? state->out->name : nulldevname;\n\t/* We handle fragments by dealing with the first fragment as\n\t * if it was a normal packet.  All other fragments are treated\n\t * normally, except that they will NEVER match rules that ask\n\t * things we don't know, ie. tcp syn flag or ports).  If the\n\t * rule is also a fragment-specific rule, non-fragments won't\n\t * match it. */\n\tacpar.fragoff = ntohs(ip->frag_off) & IP_OFFSET;\n\tacpar.thoff   = ip_hdrlen(skb);\n\tacpar.hotdrop = false;\n\tacpar.state   = state;\n\n\tWARN_ON(!(table->valid_hooks & (1 << hook)));\n\tlocal_bh_disable();\n\taddend = xt_write_recseq_begin();\n\tprivate = READ_ONCE(table->private); /* Address dependency. */\n\tcpu        = smp_processor_id();\n\ttable_base = private->entries;\n\tjumpstack  = (struct ipt_entry **)private->jumpstack[cpu];\n\n\t/* Switch to alternate jumpstack if we're being invoked via TEE.\n\t * TEE issues XT_CONTINUE verdict on original skb so we must not\n\t * clobber the jumpstack.\n\t *\n\t * For recursion via REJECT or SYNPROXY the stack will be clobbered\n\t * but it is no problem since absolute verdict is issued by these.\n\t */\n\tif (static_key_false(&xt_tee_enabled))\n\t\tjumpstack += private->stacksize * __this_cpu_read(nf_skb_duplicated);\n\n\te = get_entry(table_base, private->hook_entry[hook]);\n\n\tdo {\n\t\tconst struct xt_entry_target *t;\n\t\tconst struct xt_entry_match *ematch;\n\t\tstruct xt_counters *counter;\n\n\t\tWARN_ON(!e);\n\t\tif (!ip_packet_match(ip, indev, outdev,\n\t\t    &e->ip, acpar.fragoff)) {\n no_match:\n\t\t\te = ipt_next_entry(e);\n\t\t\tcontinue;\n\t\t}\n\n\t\txt_ematch_foreach(ematch, e) {\n\t\t\tacpar.match     = ematch->u.kernel.match;\n\t\t\tacpar.matchinfo = ematch->data;\n\t\t\tif (!acpar.match->match(skb, &acpar))\n\t\t\t\tgoto no_match;\n\t\t}\n\n\t\tcounter = xt_get_this_cpu_counter(&e->counters);\n\t\tADD_COUNTER(*counter, skb->len, 1);\n\n\t\tt = ipt_get_target_c(e);\n\t\tWARN_ON(!t->u.kernel.target);\n\n#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE)\n\t\t/* The packet is traced: log it */\n\t\tif (unlikely(skb->nf_trace))\n\t\t\ttrace_packet(state->net, skb, hook, state->in,\n\t\t\t\t     state->out, table->name, private, e);\n#endif\n\t\t/* Standard target? */\n\t\tif (!t->u.kernel.target->target) {\n\t\t\tint v;\n\n\t\t\tv = ((struct xt_standard_target *)t)->verdict;\n\t\t\tif (v < 0) {\n\t\t\t\t/* Pop from stack? */\n\t\t\t\tif (v != XT_RETURN) {\n\t\t\t\t\tverdict = (unsigned int)(-v) - 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (stackidx == 0) {\n\t\t\t\t\te = get_entry(table_base,\n\t\t\t\t\t    private->underflow[hook]);\n\t\t\t\t} else {\n\t\t\t\t\te = jumpstack[--stackidx];\n\t\t\t\t\te = ipt_next_entry(e);\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (table_base + v != ipt_next_entry(e) &&\n\t\t\t    !(e->ip.flags & IPT_F_GOTO)) {\n\t\t\t\tif (unlikely(stackidx >= private->stacksize)) {\n\t\t\t\t\tverdict = NF_DROP;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tjumpstack[stackidx++] = e;\n\t\t\t}\n\n\t\t\te = get_entry(table_base, v);\n\t\t\tcontinue;\n\t\t}\n\n\t\tacpar.target   = t->u.kernel.target;\n\t\tacpar.targinfo = t->data;\n\n\t\tverdict = t->u.kernel.target->target(skb, &acpar);\n\t\tif (verdict == XT_CONTINUE) {\n\t\t\t/* Target might have changed stuff. */\n\t\t\tip = ip_hdr(skb);\n\t\t\te = ipt_next_entry(e);\n\t\t} else {\n\t\t\t/* Verdict */\n\t\t\tbreak;\n\t\t}\n\t} while (!acpar.hotdrop);\n\n\txt_write_recseq_end(addend);\n\tlocal_bh_enable();\n\n\tif (acpar.hotdrop)\n\t\treturn NF_DROP;\n\telse return verdict;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2892,
            "cve_id": "CVE-2020-36694",
            "code_snippet": "static int get_info(struct net *net, void __user *user, const int *len)\n{\n\tchar name[XT_TABLE_MAXNAMELEN];\n\tstruct xt_table *t;\n\tint ret;\n\n\tif (*len != sizeof(struct ipt_getinfo))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(name, user, sizeof(name)) != 0)\n\t\treturn -EFAULT;\n\n\tname[XT_TABLE_MAXNAMELEN-1] = '\\0';\n#ifdef CONFIG_COMPAT\n\tif (in_compat_syscall())\n\t\txt_compat_lock(AF_INET);\n#endif\n\tt = xt_request_find_table_lock(net, AF_INET, name);\n\tif (!IS_ERR(t)) {\n\t\tstruct ipt_getinfo info;\n\t\tconst struct xt_table_info *private = t->private;\n#ifdef CONFIG_COMPAT\n\t\tstruct xt_table_info tmp;\n\n\t\tif (in_compat_syscall()) {\n\t\t\tret = compat_table_info(private, &tmp);\n\t\t\txt_compat_flush_offsets(AF_INET);\n\t\t\tprivate = &tmp;\n\t\t}\n#endif\n\t\tmemset(&info, 0, sizeof(info));\n\t\tinfo.valid_hooks = t->valid_hooks;\n\t\tmemcpy(info.hook_entry, private->hook_entry,\n\t\t       sizeof(info.hook_entry));\n\t\tmemcpy(info.underflow, private->underflow,\n\t\t       sizeof(info.underflow));\n\t\tinfo.num_entries = private->number;\n\t\tinfo.size = private->size;\n\t\tstrcpy(info.name, name);\n\n\t\tif (copy_to_user(user, &info, *len) != 0)\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = 0;\n\n\t\txt_table_unlock(t);\n\t\tmodule_put(t->me);\n\t} else\n\t\tret = PTR_ERR(t);\n#ifdef CONFIG_COMPAT\n\tif (in_compat_syscall())\n\t\txt_compat_unlock(AF_INET);\n#endif\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2893,
            "cve_id": "CVE-2020-36694",
            "code_snippet": "static int\ncompat_copy_entries_to_user(unsigned int total_size, struct xt_table *table,\n\t\t\t    void __user *userptr)\n{\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = table->private;\n\tvoid __user *pos;\n\tunsigned int size;\n\tint ret = 0;\n\tunsigned int i = 0;\n\tstruct ipt_entry *iter;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tpos = userptr;\n\tsize = total_size;\n\txt_entry_foreach(iter, private->entries, total_size) {\n\t\tret = compat_copy_entry_to_user(iter, &pos,\n\t\t\t\t\t\t&size, counters, i++);\n\t\tif (ret != 0)\n\t\t\tbreak;\n\t}\n\n\tvfree(counters);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2912,
            "cve_id": "CVE-2020-7053",
            "code_snippet": "static int gem_context_register(struct i915_gem_context *ctx,\n\t\t\t\tstruct drm_i915_file_private *fpriv)\n{\n\tint ret;\n\n\tctx->file_priv = fpriv;\n\tif (ctx->ppgtt)\n\t\tctx->ppgtt->vm.file = fpriv;\n\n\tctx->pid = get_task_pid(current, PIDTYPE_PID);\n\tctx->name = kasprintf(GFP_KERNEL, \"%s[%d]\",\n\t\t\t      current->comm, pid_nr(ctx->pid));\n\tif (!ctx->name) {\n\t\tret = -ENOMEM;\n\t\tgoto err_pid;\n\t}\n\n\t/* And finally expose ourselves to userspace via the idr */\n\tret = idr_alloc(&fpriv->context_idr, ctx,\n\t\t\tDEFAULT_CONTEXT_HANDLE, 0, GFP_KERNEL);\n\tif (ret < 0)\n\t\tgoto err_name;\n\n\tctx->user_handle = ret;\n\n\treturn 0;\n\nerr_name:\n\tkfree(fetch_and_zero(&ctx->name));\nerr_pid:\n\tput_pid(fetch_and_zero(&ctx->pid));\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2913,
            "cve_id": "CVE-2020-7053",
            "code_snippet": "int i915_gem_context_create_ioctl(struct drm_device *dev, void *data,\n\t\t\t\t  struct drm_file *file)\n{\n\tstruct drm_i915_private *i915 = to_i915(dev);\n\tstruct drm_i915_gem_context_create *args = data;\n\tstruct drm_i915_file_private *file_priv = file->driver_priv;\n\tstruct i915_gem_context *ctx;\n\tint ret;\n\n\tif (!DRIVER_CAPS(i915)->has_logical_contexts)\n\t\treturn -ENODEV;\n\n\tif (args->pad != 0)\n\t\treturn -EINVAL;\n\n\tret = i915_terminally_wedged(i915);\n\tif (ret)\n\t\treturn ret;\n\n\tif (client_is_banned(file_priv)) {\n\t\tDRM_DEBUG(\"client %s[%d] banned from creating ctx\\n\",\n\t\t\t  current->comm,\n\t\t\t  pid_nr(get_task_pid(current, PIDTYPE_PID)));\n\n\t\treturn -EIO;\n\t}\n\n\tret = i915_mutex_lock_interruptible(dev);\n\tif (ret)\n\t\treturn ret;\n\n\tctx = i915_gem_create_context(i915);\n\tif (IS_ERR(ctx)) {\n\t\tret = PTR_ERR(ctx);\n\t\tgoto err_unlock;\n\t}\n\n\tret = gem_context_register(ctx, file_priv);\n\tif (ret)\n\t\tgoto err_ctx;\n\n\tmutex_unlock(&dev->struct_mutex);\n\n\targs->ctx_id = ctx->user_handle;\n\tDRM_DEBUG(\"HW context %d created\\n\", args->ctx_id);\n\n\treturn 0;\n\nerr_ctx:\n\tcontext_close(ctx);\nerr_unlock:\n\tmutex_unlock(&dev->struct_mutex);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2920,
            "cve_id": "CVE-2020-8648",
            "code_snippet": "int paste_selection(struct tty_struct *tty)\n{\n\tstruct vc_data *vc = tty->driver_data;\n\tint\tpasted = 0;\n\tunsigned int count;\n\tstruct  tty_ldisc *ld;\n\tDECLARE_WAITQUEUE(wait, current);\n\tint ret = 0;\n\n\tconsole_lock();\n\tpoke_blanked_console();\n\tconsole_unlock();\n\n\tld = tty_ldisc_ref_wait(tty);\n\tif (!ld)\n\t\treturn -EIO;\t/* ldisc was hung up */\n\ttty_buffer_lock_exclusive(&vc->port);\n\n\tadd_wait_queue(&vc->paste_wait, &wait);\n\twhile (sel_buffer && sel_buffer_lth > pasted) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tif (signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\t\tif (tty_throttled(tty)) {\n\t\t\tschedule();\n\t\t\tcontinue;\n\t\t}\n\t\t__set_current_state(TASK_RUNNING);\n\t\tcount = sel_buffer_lth - pasted;\n\t\tcount = tty_ldisc_receive_buf(ld, sel_buffer + pasted, NULL,\n\t\t\t\t\t      count);\n\t\tpasted += count;\n\t}\n\tremove_wait_queue(&vc->paste_wait, &wait);\n\t__set_current_state(TASK_RUNNING);\n\n\ttty_buffer_unlock_exclusive(&vc->port);\n\ttty_ldisc_deref(ld);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2954,
            "cve_id": "CVE-2021-0941",
            "code_snippet": "static inline int __bpf_skb_change_head(struct sk_buff *skb, u32 head_room,\n\t\t\t\t\tu64 flags)\n{\n\tu32 max_len = __bpf_skb_max_len(skb);\n\tu32 new_len = skb->len + head_room;\n\tint ret;\n\n\tif (unlikely(flags || (!skb_is_gso(skb) && new_len > max_len) ||\n\t\t     new_len < skb->len))\n\t\treturn -EINVAL;\n\n\tret = skb_cow(skb, head_room);\n\tif (likely(!ret)) {\n\t\t/* Idea for this helper is that we currently only\n\t\t * allow to expand on mac header. This means that\n\t\t * skb->protocol network header, etc, stay as is.\n\t\t * Compared to bpf_skb_change_tail(), we're more\n\t\t * flexible due to not needing to linearize or\n\t\t * reset GSO. Intention for this helper is to be\n\t\t * used by an L3 skb that needs to push mac header\n\t\t * for redirection into L2 device.\n\t\t */\n\t\t__skb_push(skb, head_room);\n\t\tmemset(skb->data, 0, head_room);\n\t\tskb_reset_mac_header(skb);\n\t}\n\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2984,
            "cve_id": "CVE-2021-20292",
            "code_snippet": "int ttm_dma_tt_init(struct ttm_dma_tt *ttm_dma, struct ttm_buffer_object *bo,\n\t\t    uint32_t page_flags)\n{\n\tstruct ttm_tt *ttm = &ttm_dma->ttm;\n\n\tttm_tt_init_fields(ttm, bo, page_flags);\n\n\tINIT_LIST_HEAD(&ttm_dma->pages_list);\n\tif (ttm_dma_tt_alloc_page_directory(ttm_dma)) {\n\t\tttm_tt_destroy(ttm);\n\t\tpr_err(\"Failed allocating page table\\n\");\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3032,
            "cve_id": "CVE-2021-28691",
            "code_snippet": "static void xenvif_disconnect_queue(struct xenvif_queue *queue)\n{\n\tif (queue->task) {\n\t\tkthread_stop(queue->task);\n\t\tqueue->task = NULL;\n\t}\n\n\tif (queue->dealloc_task) {\n\t\tkthread_stop(queue->dealloc_task);\n\t\tqueue->dealloc_task = NULL;\n\t}\n\n\tif (queue->napi.poll) {\n\t\tnetif_napi_del(&queue->napi);\n\t\tqueue->napi.poll = NULL;\n\t}\n\n\tif (queue->tx_irq) {\n\t\tunbind_from_irqhandler(queue->tx_irq, queue);\n\t\tif (queue->tx_irq == queue->rx_irq)\n\t\t\tqueue->rx_irq = 0;\n\t\tqueue->tx_irq = 0;\n\t}\n\n\tif (queue->rx_irq) {\n\t\tunbind_from_irqhandler(queue->rx_irq, queue);\n\t\tqueue->rx_irq = 0;\n\t}\n\n\txenvif_unmap_frontend_data_rings(queue);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3072,
            "cve_id": "CVE-2021-29657",
            "code_snippet": "int nested_svm_vmrun(struct vcpu_svm *svm)\n{\n\tint ret;\n\tstruct vmcb *vmcb12;\n\tstruct vmcb *hsave = svm->nested.hsave;\n\tstruct vmcb *vmcb = svm->vmcb;\n\tstruct kvm_host_map map;\n\tu64 vmcb12_gpa;\n\n\tif (is_smm(&svm->vcpu)) {\n\t\tkvm_queue_exception(&svm->vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tvmcb12_gpa = svm->vmcb->save.rax;\n\tret = kvm_vcpu_map(&svm->vcpu, gpa_to_gfn(vmcb12_gpa), &map);\n\tif (ret == -EINVAL) {\n\t\tkvm_inject_gp(&svm->vcpu, 0);\n\t\treturn 1;\n\t} else if (ret) {\n\t\treturn kvm_skip_emulated_instruction(&svm->vcpu);\n\t}\n\n\tret = kvm_skip_emulated_instruction(&svm->vcpu);\n\n\tvmcb12 = map.hva;\n\n\tif (WARN_ON_ONCE(!svm->nested.initialized))\n\t\treturn -EINVAL;\n\n\tif (!nested_vmcb_checks(svm, vmcb12)) {\n\t\tvmcb12->control.exit_code    = SVM_EXIT_ERR;\n\t\tvmcb12->control.exit_code_hi = 0;\n\t\tvmcb12->control.exit_info_1  = 0;\n\t\tvmcb12->control.exit_info_2  = 0;\n\t\tgoto out;\n\t}\n\n\ttrace_kvm_nested_vmrun(svm->vmcb->save.rip, vmcb12_gpa,\n\t\t\t       vmcb12->save.rip,\n\t\t\t       vmcb12->control.int_ctl,\n\t\t\t       vmcb12->control.event_inj,\n\t\t\t       vmcb12->control.nested_ctl);\n\n\ttrace_kvm_nested_intercepts(vmcb12->control.intercepts[INTERCEPT_CR] & 0xffff,\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_CR] >> 16,\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_EXCEPTION],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD3],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD4],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD5]);\n\n\t/* Clear internal status */\n\tkvm_clear_exception_queue(&svm->vcpu);\n\tkvm_clear_interrupt_queue(&svm->vcpu);\n\n\t/*\n\t * Save the old vmcb, so we don't need to pick what we save, but can\n\t * restore everything when a VMEXIT occurs\n\t */\n\thsave->save.es     = vmcb->save.es;\n\thsave->save.cs     = vmcb->save.cs;\n\thsave->save.ss     = vmcb->save.ss;\n\thsave->save.ds     = vmcb->save.ds;\n\thsave->save.gdtr   = vmcb->save.gdtr;\n\thsave->save.idtr   = vmcb->save.idtr;\n\thsave->save.efer   = svm->vcpu.arch.efer;\n\thsave->save.cr0    = kvm_read_cr0(&svm->vcpu);\n\thsave->save.cr4    = svm->vcpu.arch.cr4;\n\thsave->save.rflags = kvm_get_rflags(&svm->vcpu);\n\thsave->save.rip    = kvm_rip_read(&svm->vcpu);\n\thsave->save.rsp    = vmcb->save.rsp;\n\thsave->save.rax    = vmcb->save.rax;\n\tif (npt_enabled)\n\t\thsave->save.cr3    = vmcb->save.cr3;\n\telse\n\t\thsave->save.cr3    = kvm_read_cr3(&svm->vcpu);\n\n\tcopy_vmcb_control_area(&hsave->control, &vmcb->control);\n\n\tsvm->nested.nested_run_pending = 1;\n\n\tif (enter_svm_guest_mode(svm, vmcb12_gpa, vmcb12))\n\t\tgoto out_exit_err;\n\n\tif (nested_svm_vmrun_msrpm(svm))\n\t\tgoto out;\n\nout_exit_err:\n\tsvm->nested.nested_run_pending = 0;\n\n\tsvm->vmcb->control.exit_code    = SVM_EXIT_ERR;\n\tsvm->vmcb->control.exit_code_hi = 0;\n\tsvm->vmcb->control.exit_info_1  = 0;\n\tsvm->vmcb->control.exit_info_2  = 0;\n\n\tnested_svm_vmexit(svm);\n\nout:\n\tkvm_vcpu_unmap(&svm->vcpu, &map, true);\n\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3082,
            "cve_id": "CVE-2021-32606",
            "code_snippet": "static int isotp_bind(struct socket *sock, struct sockaddr *uaddr, int len)\n{\n\tstruct sockaddr_can *addr = (struct sockaddr_can *)uaddr;\n\tstruct sock *sk = sock->sk;\n\tstruct isotp_sock *so = isotp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint ifindex;\n\tstruct net_device *dev;\n\tint err = 0;\n\tint notify_enetdown = 0;\n\tint do_rx_reg = 1;\n\n\tif (len < ISOTP_MIN_NAMELEN)\n\t\treturn -EINVAL;\n\n\t/* do not register frame reception for functional addressing */\n\tif (so->opt.flags & CAN_ISOTP_SF_BROADCAST)\n\t\tdo_rx_reg = 0;\n\n\t/* do not validate rx address for functional addressing */\n\tif (do_rx_reg) {\n\t\tif (addr->can_addr.tp.rx_id == addr->can_addr.tp.tx_id)\n\t\t\treturn -EADDRNOTAVAIL;\n\n\t\tif (addr->can_addr.tp.rx_id & (CAN_ERR_FLAG | CAN_RTR_FLAG))\n\t\t\treturn -EADDRNOTAVAIL;\n\t}\n\n\tif (addr->can_addr.tp.tx_id & (CAN_ERR_FLAG | CAN_RTR_FLAG))\n\t\treturn -EADDRNOTAVAIL;\n\n\tif (!addr->can_ifindex)\n\t\treturn -ENODEV;\n\n\tlock_sock(sk);\n\n\tif (so->bound && addr->can_ifindex == so->ifindex &&\n\t    addr->can_addr.tp.rx_id == so->rxid &&\n\t    addr->can_addr.tp.tx_id == so->txid)\n\t\tgoto out;\n\n\tdev = dev_get_by_index(net, addr->can_ifindex);\n\tif (!dev) {\n\t\terr = -ENODEV;\n\t\tgoto out;\n\t}\n\tif (dev->type != ARPHRD_CAN) {\n\t\tdev_put(dev);\n\t\terr = -ENODEV;\n\t\tgoto out;\n\t}\n\tif (dev->mtu < so->ll.mtu) {\n\t\tdev_put(dev);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (!(dev->flags & IFF_UP))\n\t\tnotify_enetdown = 1;\n\n\tifindex = dev->ifindex;\n\n\tif (do_rx_reg)\n\t\tcan_rx_register(net, dev, addr->can_addr.tp.rx_id,\n\t\t\t\tSINGLE_MASK(addr->can_addr.tp.rx_id),\n\t\t\t\tisotp_rcv, sk, \"isotp\", sk);\n\n\tdev_put(dev);\n\n\tif (so->bound && do_rx_reg) {\n\t\t/* unregister old filter */\n\t\tif (so->ifindex) {\n\t\t\tdev = dev_get_by_index(net, so->ifindex);\n\t\t\tif (dev) {\n\t\t\t\tcan_rx_unregister(net, dev, so->rxid,\n\t\t\t\t\t\t  SINGLE_MASK(so->rxid),\n\t\t\t\t\t\t  isotp_rcv, sk);\n\t\t\t\tdev_put(dev);\n\t\t\t}\n\t\t}\n\t}\n\n\t/* switch to new settings */\n\tso->ifindex = ifindex;\n\tso->rxid = addr->can_addr.tp.rx_id;\n\tso->txid = addr->can_addr.tp.tx_id;\n\tso->bound = 1;\n\nout:\n\trelease_sock(sk);\n\n\tif (notify_enetdown) {\n\t\tsk->sk_err = ENETDOWN;\n\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\tsk->sk_error_report(sk);\n\t}\n\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3084,
            "cve_id": "CVE-2021-33034",
            "code_snippet": "static void hci_disconn_loglink_complete_evt(struct hci_dev *hdev,\n\t\t\t\t\t     struct sk_buff *skb)\n{\n\tstruct hci_ev_disconn_logical_link_complete *ev = (void *) skb->data;\n\tstruct hci_chan *hchan;\n\n\tBT_DBG(\"%s log handle 0x%4.4x status 0x%2.2x\", hdev->name,\n\t       le16_to_cpu(ev->handle), ev->status);\n\n\tif (ev->status)\n\t\treturn;\n\n\thci_dev_lock(hdev);\n\n\thchan = hci_chan_lookup_handle(hdev, le16_to_cpu(ev->handle));\n\tif (!hchan)\n\t\tgoto unlock;\n\n\tamp_destroy_logical_link(hchan, ev->reason);\n\nunlock:\n\thci_dev_unlock(hdev);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3093,
            "cve_id": "CVE-2021-3347",
            "code_snippet": "static int fixup_owner(u32 __user *uaddr, struct futex_q *q, int locked)\n{\n\tint ret = 0;\n\n\tif (locked) {\n\t\t/*\n\t\t * Got the lock. We might not be the anticipated owner if we\n\t\t * did a lock-steal - fix up the PI-state in that case:\n\t\t *\n\t\t * Speculative pi_state->owner read (we don't hold wait_lock);\n\t\t * since we own the lock pi_state->owner == current is the\n\t\t * stable state, anything else needs more attention.\n\t\t */\n\t\tif (q->pi_state->owner != current)\n\t\t\tret = fixup_pi_state_owner(uaddr, q, current);\n\t\treturn ret ? ret : locked;\n\t}\n\n\t/*\n\t * If we didn't get the lock; check if anybody stole it from us. In\n\t * that case, we need to fix up the uval to point to them instead of\n\t * us, otherwise bad things happen. [10]\n\t *\n\t * Another speculative read; pi_state->owner == current is unstable\n\t * but needs our attention.\n\t */\n\tif (q->pi_state->owner == current) {\n\t\tret = fixup_pi_state_owner(uaddr, q, NULL);\n\t\treturn ret;\n\t}\n\n\t/*\n\t * Paranoia check. If we did not take the lock, then we should not be\n\t * the owner of the rt_mutex.\n\t */\n\tif (rt_mutex_owner(&q->pi_state->pi_mutex) == current) {\n\t\tprintk(KERN_ERR \"fixup_owner: ret = %d pi-mutex: %p \"\n\t\t\t\t\"pi-state %p\\n\", ret,\n\t\t\t\tq->pi_state->pi_mutex.owner,\n\t\t\t\tq->pi_state->owner);\n\t}\n\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3209,
            "cve_id": "CVE-2021-38204",
            "code_snippet": "static int\nmax3421_urb_done(struct usb_hcd *hcd)\n{\n\tstruct max3421_hcd *max3421_hcd = hcd_to_max3421(hcd);\n\tunsigned long flags;\n\tstruct urb *urb;\n\tint status;\n\n\tstatus = max3421_hcd->urb_done;\n\tmax3421_hcd->urb_done = 0;\n\tif (status > 0)\n\t\tstatus = 0;\n\turb = max3421_hcd->curr_urb;\n\tif (urb) {\n\t\tmax3421_hcd->curr_urb = NULL;\n\t\tspin_lock_irqsave(&max3421_hcd->lock, flags);\n\t\tusb_hcd_unlink_urb_from_ep(hcd, urb);\n\t\tspin_unlock_irqrestore(&max3421_hcd->lock, flags);\n\n\t\t/* must be called without the HCD spinlock: */\n\t\tusb_hcd_giveback_urb(hcd, urb, status);\n\t}\n\treturn 1;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3238,
            "cve_id": "CVE-2021-39800",
            "code_snippet": "struct ion_handle *ion_alloc(struct ion_client *client, size_t len,\n\t\t\t     size_t align, unsigned int heap_id_mask,\n\t\t\t     unsigned int flags)\n{\n\tstruct ion_handle *handle;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_buffer *buffer = NULL;\n\tstruct ion_heap *heap;\n\tint ret;\n\n\tpr_debug(\"%s: len %zu align %zu heap_id_mask %u flags %x\\n\", __func__,\n\t\t len, align, heap_id_mask, flags);\n\t/*\n\t * traverse the list of heaps available in this system in priority\n\t * order.  If the heap type is supported by the client, and matches the\n\t * request of the caller allocate from it.  Repeat until allocate has\n\t * succeeded or all heaps have been tried\n\t */\n\tlen = PAGE_ALIGN(len);\n\n\tif (!len)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tdown_read(&dev->lock);\n\tplist_for_each_entry(heap, &dev->heaps, node) {\n\t\t/* if the caller didn't specify this heap id */\n\t\tif (!((1 << heap->id) & heap_id_mask))\n\t\t\tcontinue;\n\t\tbuffer = ion_buffer_create(heap, dev, len, align, flags);\n\t\tif (!IS_ERR(buffer))\n\t\t\tbreak;\n\t}\n\tup_read(&dev->lock);\n\n\tif (buffer == NULL)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tif (IS_ERR(buffer))\n\t\treturn ERR_CAST(buffer);\n\n\thandle = ion_handle_create(client, buffer);\n\n\t/*\n\t * ion_buffer_create will create a buffer with a ref_cnt of 1,\n\t * and ion_handle_create will take a second reference, drop one here\n\t */\n\tion_buffer_put(buffer);\n\n\tif (IS_ERR(handle))\n\t\treturn handle;\n\n\tmutex_lock(&client->lock);\n\tret = ion_handle_add(client, handle);\n\tmutex_unlock(&client->lock);\n\tif (ret) {\n\t\tion_handle_put(handle);\n\t\thandle = ERR_PTR(ret);\n\t}\n\n\treturn handle;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3240,
            "cve_id": "CVE-2021-39801",
            "code_snippet": "struct ion_handle *ion_alloc(struct ion_client *client, size_t len,\n\t\t\t     size_t align, unsigned int heap_id_mask,\n\t\t\t     unsigned int flags)\n{\n\tstruct ion_handle *handle;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_buffer *buffer = NULL;\n\tstruct ion_heap *heap;\n\tint ret;\n\n\tpr_debug(\"%s: len %zu align %zu heap_id_mask %u flags %x\\n\", __func__,\n\t\t len, align, heap_id_mask, flags);\n\t/*\n\t * traverse the list of heaps available in this system in priority\n\t * order.  If the heap type is supported by the client, and matches the\n\t * request of the caller allocate from it.  Repeat until allocate has\n\t * succeeded or all heaps have been tried\n\t */\n\tlen = PAGE_ALIGN(len);\n\n\tif (!len)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tdown_read(&dev->lock);\n\tplist_for_each_entry(heap, &dev->heaps, node) {\n\t\t/* if the caller didn't specify this heap id */\n\t\tif (!((1 << heap->id) & heap_id_mask))\n\t\t\tcontinue;\n\t\tbuffer = ion_buffer_create(heap, dev, len, align, flags);\n\t\tif (!IS_ERR(buffer))\n\t\t\tbreak;\n\t}\n\tup_read(&dev->lock);\n\n\tif (buffer == NULL)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tif (IS_ERR(buffer))\n\t\treturn ERR_CAST(buffer);\n\n\thandle = ion_handle_create(client, buffer);\n\n\t/*\n\t * ion_buffer_create will create a buffer with a ref_cnt of 1,\n\t * and ion_handle_create will take a second reference, drop one here\n\t */\n\tion_buffer_put(buffer);\n\n\tif (IS_ERR(handle))\n\t\treturn handle;\n\n\tmutex_lock(&client->lock);\n\tret = ion_handle_add(client, handle);\n\tmutex_unlock(&client->lock);\n\tif (ret) {\n\t\tion_handle_put(handle);\n\t\thandle = ERR_PTR(ret);\n\t}\n\n\treturn handle;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3289,
            "cve_id": "CVE-2021-43057",
            "code_snippet": "static int selinux_msg_queue_msgrcv(struct kern_ipc_perm *msq, struct msg_msg *msg,\n\t\t\t\t    struct task_struct *target,\n\t\t\t\t    long type, int mode)\n{\n\tstruct ipc_security_struct *isec;\n\tstruct msg_security_struct *msec;\n\tstruct common_audit_data ad;\n\tu32 sid = task_sid_subj(target);\n\tint rc;\n\n\tisec = selinux_ipc(msq);\n\tmsec = selinux_msg_msg(msg);\n\n\tad.type = LSM_AUDIT_DATA_IPC;\n\tad.u.ipc_id = msq->key;\n\n\trc = avc_has_perm(&selinux_state,\n\t\t\t  sid, isec->sid,\n\t\t\t  SECCLASS_MSGQ, MSGQ__READ, &ad);\n\tif (!rc)\n\t\trc = avc_has_perm(&selinux_state,\n\t\t\t\t  sid, msec->sid,\n\t\t\t\t  SECCLASS_MSG, MSG__RECEIVE, &ad);\n\treturn rc;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3313,
            "cve_id": "CVE-2021-45868",
            "code_snippet": "static loff_t find_tree_dqentry(struct qtree_mem_dqinfo *info,\n\t\t\t\tstruct dquot *dquot, uint blk, int depth)\n{\n\tchar *buf = kmalloc(info->dqi_usable_bs, GFP_NOFS);\n\tloff_t ret = 0;\n\t__le32 *ref = (__le32 *)buf;\n\n\tif (!buf)\n\t\treturn -ENOMEM;\n\tret = read_blk(info, blk, buf);\n\tif (ret < 0) {\n\t\tquota_error(dquot->dq_sb, \"Can't read quota tree block %u\",\n\t\t\t    blk);\n\t\tgoto out_buf;\n\t}\n\tret = 0;\n\tblk = le32_to_cpu(ref[get_index(info, dquot->dq_id, depth)]);\n\tif (!blk)\t/* No reference? */\n\t\tgoto out_buf;\n\tif (depth < info->dqi_qtree_depth - 1)\n\t\tret = find_tree_dqentry(info, dquot, blk, depth+1);\n\telse\n\t\tret = find_block_dqentry(info, dquot, blk);\nout_buf:\n\tkfree(buf);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3364,
            "cve_id": "CVE-2022-1011",
            "code_snippet": "static int fuse_get_user_pages(struct fuse_args_pages *ap, struct iov_iter *ii,\n\t\t\t       size_t *nbytesp, int write,\n\t\t\t       unsigned int max_pages)\n{\n\tsize_t nbytes = 0;  /* # bytes already packed in req */\n\tssize_t ret = 0;\n\n\t/* Special case for kernel I/O: can copy directly into the buffer */\n\tif (iov_iter_is_kvec(ii)) {\n\t\tunsigned long user_addr = fuse_get_user_addr(ii);\n\t\tsize_t frag_size = fuse_get_frag_size(ii, *nbytesp);\n\n\t\tif (write)\n\t\t\tap->args.in_args[1].value = (void *) user_addr;\n\t\telse\n\t\t\tap->args.out_args[0].value = (void *) user_addr;\n\n\t\tiov_iter_advance(ii, frag_size);\n\t\t*nbytesp = frag_size;\n\t\treturn 0;\n\t}\n\n\twhile (nbytes < *nbytesp && ap->num_pages < max_pages) {\n\t\tunsigned npages;\n\t\tsize_t start;\n\t\tret = iov_iter_get_pages(ii, &ap->pages[ap->num_pages],\n\t\t\t\t\t*nbytesp - nbytes,\n\t\t\t\t\tmax_pages - ap->num_pages,\n\t\t\t\t\t&start);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\n\t\tiov_iter_advance(ii, ret);\n\t\tnbytes += ret;\n\n\t\tret += start;\n\t\tnpages = DIV_ROUND_UP(ret, PAGE_SIZE);\n\n\t\tap->descs[ap->num_pages].offset = start;\n\t\tfuse_page_descs_length_init(ap->descs, ap->num_pages, npages);\n\n\t\tap->num_pages += npages;\n\t\tap->descs[ap->num_pages - 1].length -=\n\t\t\t(PAGE_SIZE - ret) & (PAGE_SIZE - 1);\n\t}\n\n\tif (write)\n\t\tap->args.in_pages = true;\n\telse\n\t\tap->args.out_pages = true;\n\n\t*nbytesp = nbytes;\n\n\treturn ret < 0 ? ret : 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3376,
            "cve_id": "CVE-2022-1048",
            "code_snippet": "void snd_pcm_detach_substream(struct snd_pcm_substream *substream)\n{\n\tstruct snd_pcm_runtime *runtime;\n\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn;\n\truntime = substream->runtime;\n\tif (runtime->private_free != NULL)\n\t\truntime->private_free(runtime);\n\tfree_pages_exact(runtime->status,\n\t\t       PAGE_ALIGN(sizeof(struct snd_pcm_mmap_status)));\n\tfree_pages_exact(runtime->control,\n\t\t       PAGE_ALIGN(sizeof(struct snd_pcm_mmap_control)));\n\tkfree(runtime->hw_constraints.rules);\n\t/* Avoid concurrent access to runtime via PCM timer interface */\n\tif (substream->timer) {\n\t\tspin_lock_irq(&substream->timer->lock);\n\t\tsubstream->runtime = NULL;\n\t\tspin_unlock_irq(&substream->timer->lock);\n\t} else {\n\t\tsubstream->runtime = NULL;\n\t}\n\tkfree(runtime);\n\tput_pid(substream->pid);\n\tsubstream->pid = NULL;\n\tsubstream->pstr->substream_opened--;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3380,
            "cve_id": "CVE-2022-1055",
            "code_snippet": "static int tc_new_tfilter(struct sk_buff *skb, struct nlmsghdr *n,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tchar name[IFNAMSIZ];\n\tstruct tcmsg *t;\n\tu32 protocol;\n\tu32 prio;\n\tbool prio_allocate;\n\tu32 parent;\n\tu32 chain_index;\n\tstruct Qdisc *q = NULL;\n\tstruct tcf_chain_info chain_info;\n\tstruct tcf_chain *chain = NULL;\n\tstruct tcf_block *block;\n\tstruct tcf_proto *tp;\n\tunsigned long cl;\n\tvoid *fh;\n\tint err;\n\tint tp_created;\n\tbool rtnl_held = false;\n\tu32 flags;\n\n\tif (!netlink_ns_capable(skb, net->user_ns, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\ttp_created = 0;\n\n\terr = nlmsg_parse_deprecated(n, sizeof(*t), tca, TCA_MAX,\n\t\t\t\t     rtm_tca_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tt = nlmsg_data(n);\n\tprotocol = TC_H_MIN(t->tcm_info);\n\tprio = TC_H_MAJ(t->tcm_info);\n\tprio_allocate = false;\n\tparent = t->tcm_parent;\n\ttp = NULL;\n\tcl = 0;\n\tblock = NULL;\n\tflags = 0;\n\n\tif (prio == 0) {\n\t\t/* If no priority is provided by the user,\n\t\t * we allocate one.\n\t\t */\n\t\tif (n->nlmsg_flags & NLM_F_CREATE) {\n\t\t\tprio = TC_H_MAKE(0x80000000U, 0U);\n\t\t\tprio_allocate = true;\n\t\t} else {\n\t\t\tNL_SET_ERR_MSG(extack, \"Invalid filter command with priority of zero\");\n\t\t\treturn -ENOENT;\n\t\t}\n\t}\n\n\t/* Find head of filter chain. */\n\n\terr = __tcf_qdisc_find(net, &q, &parent, t->tcm_ifindex, false, extack);\n\tif (err)\n\t\treturn err;\n\n\tif (tcf_proto_check_kind(tca[TCA_KIND], name)) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified TC filter name too long\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\t/* Take rtnl mutex if rtnl_held was set to true on previous iteration,\n\t * block is shared (no qdisc found), qdisc is not unlocked, classifier\n\t * type is not specified, classifier is not unlocked.\n\t */\n\tif (rtnl_held ||\n\t    (q && !(q->ops->cl_ops->flags & QDISC_CLASS_OPS_DOIT_UNLOCKED)) ||\n\t    !tcf_proto_is_unlocked(name)) {\n\t\trtnl_held = true;\n\t\trtnl_lock();\n\t}\n\n\terr = __tcf_qdisc_cl_find(q, parent, &cl, t->tcm_ifindex, extack);\n\tif (err)\n\t\tgoto errout;\n\n\tblock = __tcf_block_find(net, q, cl, t->tcm_ifindex, t->tcm_block_index,\n\t\t\t\t extack);\n\tif (IS_ERR(block)) {\n\t\terr = PTR_ERR(block);\n\t\tgoto errout;\n\t}\n\tblock->classid = parent;\n\n\tchain_index = tca[TCA_CHAIN] ? nla_get_u32(tca[TCA_CHAIN]) : 0;\n\tif (chain_index > TC_ACT_EXT_VAL_MASK) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified chain index exceeds upper limit\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\tchain = tcf_chain_get(block, chain_index, true);\n\tif (!chain) {\n\t\tNL_SET_ERR_MSG(extack, \"Cannot create specified filter chain\");\n\t\terr = -ENOMEM;\n\t\tgoto errout;\n\t}\n\n\tmutex_lock(&chain->filter_chain_lock);\n\ttp = tcf_chain_tp_find(chain, &chain_info, protocol,\n\t\t\t       prio, prio_allocate);\n\tif (IS_ERR(tp)) {\n\t\tNL_SET_ERR_MSG(extack, \"Filter with specified priority/protocol not found\");\n\t\terr = PTR_ERR(tp);\n\t\tgoto errout_locked;\n\t}\n\n\tif (tp == NULL) {\n\t\tstruct tcf_proto *tp_new = NULL;\n\n\t\tif (chain->flushing) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto errout_locked;\n\t\t}\n\n\t\t/* Proto-tcf does not exist, create new one */\n\n\t\tif (tca[TCA_KIND] == NULL || !protocol) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Filter kind and protocol must be specified\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout_locked;\n\t\t}\n\n\t\tif (!(n->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Need both RTM_NEWTFILTER and NLM_F_CREATE to create a new filter\");\n\t\t\terr = -ENOENT;\n\t\t\tgoto errout_locked;\n\t\t}\n\n\t\tif (prio_allocate)\n\t\t\tprio = tcf_auto_prio(tcf_chain_tp_prev(chain,\n\t\t\t\t\t\t\t       &chain_info));\n\n\t\tmutex_unlock(&chain->filter_chain_lock);\n\t\ttp_new = tcf_proto_create(name, protocol, prio, chain,\n\t\t\t\t\t  rtnl_held, extack);\n\t\tif (IS_ERR(tp_new)) {\n\t\t\terr = PTR_ERR(tp_new);\n\t\t\tgoto errout_tp;\n\t\t}\n\n\t\ttp_created = 1;\n\t\ttp = tcf_chain_tp_insert_unique(chain, tp_new, protocol, prio,\n\t\t\t\t\t\trtnl_held);\n\t\tif (IS_ERR(tp)) {\n\t\t\terr = PTR_ERR(tp);\n\t\t\tgoto errout_tp;\n\t\t}\n\t} else {\n\t\tmutex_unlock(&chain->filter_chain_lock);\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], tp->ops->kind)) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified filter kind does not match existing one\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\tfh = tp->ops->get(tp, t->tcm_handle);\n\n\tif (!fh) {\n\t\tif (!(n->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Need both RTM_NEWTFILTER and NLM_F_CREATE to create a new filter\");\n\t\t\terr = -ENOENT;\n\t\t\tgoto errout;\n\t\t}\n\t} else if (n->nlmsg_flags & NLM_F_EXCL) {\n\t\ttfilter_put(tp, fh);\n\t\tNL_SET_ERR_MSG(extack, \"Filter already exists\");\n\t\terr = -EEXIST;\n\t\tgoto errout;\n\t}\n\n\tif (chain->tmplt_ops && chain->tmplt_ops != tp->ops) {\n\t\tNL_SET_ERR_MSG(extack, \"Chain template is set to a different filter kind\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\tif (!(n->nlmsg_flags & NLM_F_CREATE))\n\t\tflags |= TCA_ACT_FLAGS_REPLACE;\n\tif (!rtnl_held)\n\t\tflags |= TCA_ACT_FLAGS_NO_RTNL;\n\terr = tp->ops->change(net, skb, tp, cl, t->tcm_handle, tca, &fh,\n\t\t\t      flags, extack);\n\tif (err == 0) {\n\t\ttfilter_notify(net, skb, n, tp, block, q, parent, fh,\n\t\t\t       RTM_NEWTFILTER, false, rtnl_held);\n\t\ttfilter_put(tp, fh);\n\t\t/* q pointer is NULL for shared blocks */\n\t\tif (q)\n\t\t\tq->flags &= ~TCQ_F_CAN_BYPASS;\n\t}\n\nerrout:\n\tif (err && tp_created)\n\t\ttcf_chain_tp_delete_empty(chain, tp, rtnl_held, NULL);\nerrout_tp:\n\tif (chain) {\n\t\tif (tp && !IS_ERR(tp))\n\t\t\ttcf_proto_put(tp, rtnl_held, NULL);\n\t\tif (!tp_created)\n\t\t\ttcf_chain_put(chain);\n\t}\n\ttcf_block_release(q, block, rtnl_held);\n\n\tif (rtnl_held)\n\t\trtnl_unlock();\n\n\tif (err == -EAGAIN) {\n\t\t/* Take rtnl lock in case EAGAIN is caused by concurrent flush\n\t\t * of target chain.\n\t\t */\n\t\trtnl_held = true;\n\t\t/* Replay the request. */\n\t\tgoto replay;\n\t}\n\treturn err;\n\nerrout_locked:\n\tmutex_unlock(&chain->filter_chain_lock);\n\tgoto errout;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3393,
            "cve_id": "CVE-2022-1419",
            "code_snippet": "static int vgem_gem_dumb_create(struct drm_file *file, struct drm_device *dev,\n\t\t\t\tstruct drm_mode_create_dumb *args)\n{\n\tstruct drm_gem_object *gem_object;\n\tu64 pitch, size;\n\n\tpitch = args->width * DIV_ROUND_UP(args->bpp, 8);\n\tsize = args->height * pitch;\n\tif (size == 0)\n\t\treturn -EINVAL;\n\n\tgem_object = vgem_gem_create(dev, file, &args->handle, size);\n\tif (IS_ERR(gem_object))\n\t\treturn PTR_ERR(gem_object);\n\n\targs->size = gem_object->size;\n\targs->pitch = pitch;\n\n\tDRM_DEBUG(\"Created object of size %lld\\n\", size);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3400,
            "cve_id": "CVE-2022-1652",
            "code_snippet": "static int do_format(int drive, struct format_descr *tmp_format_req)\n{\n\tint ret;\n\n\tif (lock_fdc(drive))\n\t\treturn -EINTR;\n\n\tset_floppy(drive);\n\tif (!_floppy ||\n\t    _floppy->track > drive_params[current_drive].tracks ||\n\t    tmp_format_req->track >= _floppy->track ||\n\t    tmp_format_req->head >= _floppy->head ||\n\t    (_floppy->sect << 2) % (1 << FD_SIZECODE(_floppy)) ||\n\t    !_floppy->fmt_gap) {\n\t\tprocess_fd_request();\n\t\treturn -EINVAL;\n\t}\n\tformat_req = *tmp_format_req;\n\tformat_errors = 0;\n\tcont = &format_cont;\n\terrors = &format_errors;\n\tret = wait_til_done(redo_format, true);\n\tif (ret == -EINTR)\n\t\treturn -EINTR;\n\tprocess_fd_request();\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3401,
            "cve_id": "CVE-2022-1652",
            "code_snippet": "static void redo_fd_request(void)\n{\n\tint drive;\n\tint tmp;\n\n\tlastredo = jiffies;\n\tif (current_drive < N_DRIVE)\n\t\tfloppy_off(current_drive);\n\ndo_request:\n\tif (!current_req) {\n\t\tint pending;\n\n\t\tspin_lock_irq(&floppy_lock);\n\t\tpending = set_next_request();\n\t\tspin_unlock_irq(&floppy_lock);\n\t\tif (!pending) {\n\t\t\tdo_floppy = NULL;\n\t\t\tunlock_fdc();\n\t\t\treturn;\n\t\t}\n\t}\n\tdrive = (long)current_req->q->disk->private_data;\n\tset_fdc(drive);\n\treschedule_timeout(current_drive, \"redo fd request\");\n\n\tset_floppy(drive);\n\traw_cmd = &default_raw_cmd;\n\traw_cmd->flags = 0;\n\tif (start_motor(redo_fd_request))\n\t\treturn;\n\n\tdisk_change(current_drive);\n\tif (test_bit(current_drive, &fake_change) ||\n\t    test_bit(FD_DISK_CHANGED_BIT, &drive_state[current_drive].flags)) {\n\t\tDPRINT(\"disk absent or changed during operation\\n\");\n\t\trequest_done(0);\n\t\tgoto do_request;\n\t}\n\tif (!_floppy) {\t/* Autodetection */\n\t\tif (!probing) {\n\t\t\tdrive_state[current_drive].probed_format = 0;\n\t\t\tif (next_valid_format(current_drive)) {\n\t\t\t\tDPRINT(\"no autodetectable formats\\n\");\n\t\t\t\t_floppy = NULL;\n\t\t\t\trequest_done(0);\n\t\t\t\tgoto do_request;\n\t\t\t}\n\t\t}\n\t\tprobing = 1;\n\t\t_floppy = floppy_type + drive_params[current_drive].autodetect[drive_state[current_drive].probed_format];\n\t} else\n\t\tprobing = 0;\n\terrors = &(current_req->error_count);\n\ttmp = make_raw_rw_request();\n\tif (tmp < 2) {\n\t\trequest_done(tmp);\n\t\tgoto do_request;\n\t}\n\n\tif (test_bit(FD_NEED_TWADDLE_BIT, &drive_state[current_drive].flags))\n\t\ttwaddle(current_fdc, current_drive);\n\tschedule_bh(floppy_start);\n\tdebugt(__func__, \"queue fd request\");\n\treturn;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3412,
            "cve_id": "CVE-2022-1786",
            "code_snippet": "static void io_worker_exit(struct io_worker *worker)\n{\n\tstruct io_wqe *wqe = worker->wqe;\n\tstruct io_wqe_acct *acct = io_wqe_get_acct(worker);\n\n\t/*\n\t * If we're not at zero, someone else is holding a brief reference\n\t * to the worker. Wait for that to go away.\n\t */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tif (!refcount_dec_and_test(&worker->ref))\n\t\tschedule();\n\t__set_current_state(TASK_RUNNING);\n\n\tpreempt_disable();\n\tcurrent->flags &= ~PF_IO_WORKER;\n\tif (worker->flags & IO_WORKER_F_RUNNING)\n\t\tatomic_dec(&acct->nr_running);\n\tif (!(worker->flags & IO_WORKER_F_BOUND))\n\t\tatomic_dec(&wqe->wq->user->processes);\n\tworker->flags = 0;\n\tpreempt_enable();\n\n\traw_spin_lock_irq(&wqe->lock);\n\thlist_nulls_del_rcu(&worker->nulls_node);\n\tlist_del_rcu(&worker->all_list);\n\tacct->nr_workers--;\n\traw_spin_unlock_irq(&wqe->lock);\n\n\tkfree_rcu(worker, rcu);\n\tif (refcount_dec_and_test(&wqe->wq->refs))\n\t\tcomplete(&wqe->wq->done);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3413,
            "cve_id": "CVE-2022-1786",
            "code_snippet": "static void io_worker_handle_work(struct io_worker *worker)\n\t__releases(wqe->lock)\n{\n\tstruct io_wqe *wqe = worker->wqe;\n\tstruct io_wq *wq = wqe->wq;\n\n\tdo {\n\t\tstruct io_wq_work *work;\nget_next:\n\t\t/*\n\t\t * If we got some work, mark us as busy. If we didn't, but\n\t\t * the list isn't empty, it means we stalled on hashed work.\n\t\t * Mark us stalled so we don't keep looking for work when we\n\t\t * can't make progress, any work completion or insertion will\n\t\t * clear the stalled flag.\n\t\t */\n\t\twork = io_get_next_work(wqe);\n\t\tif (work)\n\t\t\t__io_worker_busy(wqe, worker, work);\n\t\telse if (!wq_list_empty(&wqe->work_list))\n\t\t\twqe->flags |= IO_WQE_FLAG_STALLED;\n\n\t\traw_spin_unlock_irq(&wqe->lock);\n\t\tif (!work)\n\t\t\tbreak;\n\t\tio_assign_current_work(worker, work);\n\n\t\t/* handle a whole dependent link */\n\t\tdo {\n\t\t\tstruct io_wq_work *next_hashed, *linked;\n\t\t\tunsigned int hash = io_get_work_hash(work);\n\n\t\t\tnext_hashed = wq_next_work(work);\n\t\t\twq->do_work(work);\n\t\t\tio_assign_current_work(worker, NULL);\n\n\t\t\tlinked = wq->free_work(work);\n\t\t\twork = next_hashed;\n\t\t\tif (!work && linked && !io_wq_is_hashed(linked)) {\n\t\t\t\twork = linked;\n\t\t\t\tlinked = NULL;\n\t\t\t}\n\t\t\tio_assign_current_work(worker, work);\n\t\t\tif (linked)\n\t\t\t\tio_wqe_enqueue(wqe, linked);\n\n\t\t\tif (hash != -1U && !next_hashed) {\n\t\t\t\traw_spin_lock_irq(&wqe->lock);\n\t\t\t\twqe->hash_map &= ~BIT_ULL(hash);\n\t\t\t\twqe->flags &= ~IO_WQE_FLAG_STALLED;\n\t\t\t\t/* skip unnecessary unlock-lock wqe->lock */\n\t\t\t\tif (!work)\n\t\t\t\t\tgoto get_next;\n\t\t\t\traw_spin_unlock_irq(&wqe->lock);\n\t\t\t}\n\t\t} while (work);\n\n\t\traw_spin_lock_irq(&wqe->lock);\n\t} while (1);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3414,
            "cve_id": "CVE-2022-1786",
            "code_snippet": "static bool io_match_task(struct io_kiocb *head,\n\t\t\t  struct task_struct *task,\n\t\t\t  struct files_struct *files)\n{\n\tstruct io_kiocb *req;\n\n\tif (task && head->task != task) {\n\t\t/* in terms of cancelation, always match if req task is dead */\n\t\tif (head->task->flags & PF_EXITING)\n\t\t\treturn true;\n\t\treturn false;\n\t}\n\tif (!files)\n\t\treturn true;\n\n\tio_for_each_link(req, head) {\n\t\tif (!(req->flags & REQ_F_WORK_INITIALIZED))\n\t\t\tcontinue;\n\t\tif (req->file && req->file->f_op == &io_uring_fops)\n\t\t\treturn true;\n\t\tif (req->work.identity->files == files)\n\t\t\treturn true;\n\t}\n\treturn false;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3415,
            "cve_id": "CVE-2022-1786",
            "code_snippet": "static void io_prep_async_work(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_req_init_async(req);\n\n\tif (req->flags & REQ_F_FORCE_ASYNC)\n\t\treq->work.flags |= IO_WQ_WORK_CONCURRENT;\n\n\tif (req->flags & REQ_F_ISREG) {\n\t\tif (def->hash_reg_file || (ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tio_wq_hash_work(&req->work, file_inode(req->file));\n\t} else {\n\t\tif (def->unbound_nonreg_file)\n\t\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3416,
            "cve_id": "CVE-2022-1786",
            "code_snippet": "static int io_uring_show_cred(int id, void *p, void *data)\n{\n\tstruct io_identity *iod = p;\n\tconst struct cred *cred = iod->creds;\n\tstruct seq_file *m = data;\n\tstruct user_namespace *uns = seq_user_ns(m);\n\tstruct group_info *gi;\n\tkernel_cap_t cap;\n\tunsigned __capi;\n\tint g;\n\n\tseq_printf(m, \"%5d\\n\", id);\n\tseq_put_decimal_ull(m, \"\\tUid:\\t\", from_kuid_munged(uns, cred->uid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->euid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->suid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->fsuid));\n\tseq_put_decimal_ull(m, \"\\n\\tGid:\\t\", from_kgid_munged(uns, cred->gid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->egid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->sgid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->fsgid));\n\tseq_puts(m, \"\\n\\tGroups:\\t\");\n\tgi = cred->group_info;\n\tfor (g = 0; g < gi->ngroups; g++) {\n\t\tseq_put_decimal_ull(m, g ? \" \" : \"\",\n\t\t\t\t\tfrom_kgid_munged(uns, gi->gid[g]));\n\t}\n\tseq_puts(m, \"\\n\\tCapEff:\\t\");\n\tcap = cred->cap_effective;\n\tCAP_FOR_EACH_U32(__capi)\n\t\tseq_put_hex_ll(m, NULL, cap.cap[CAP_LAST_U32 - __capi], 8);\n\tseq_putc(m, '\\n');\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3417,
            "cve_id": "CVE-2022-1786",
            "code_snippet": "static inline void io_req_init_async(struct io_kiocb *req)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\n\tif (req->flags & REQ_F_WORK_INITIALIZED)\n\t\treturn;\n\n\t__io_req_init_async(req);\n\n\t/* Grab a ref if this isn't our static identity */\n\treq->work.identity = tctx->identity;\n\tif (tctx->identity != &tctx->__identity)\n\t\trefcount_inc(&req->work.identity->count);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3430,
            "cve_id": "CVE-2022-1882",
            "code_snippet": "static bool post_one_notification(struct watch_queue *wqueue,\n\t\t\t\t  struct watch_notification *n)\n{\n\tvoid *p;\n\tstruct pipe_inode_info *pipe = wqueue->pipe;\n\tstruct pipe_buffer *buf;\n\tstruct page *page;\n\tunsigned int head, tail, mask, note, offset, len;\n\tbool done = false;\n\n\tif (!pipe)\n\t\treturn false;\n\n\tspin_lock_irq(&pipe->rd_wait.lock);\n\n\tif (wqueue->defunct)\n\t\tgoto out;\n\n\tmask = pipe->ring_size - 1;\n\thead = pipe->head;\n\ttail = pipe->tail;\n\tif (pipe_full(head, tail, pipe->ring_size))\n\t\tgoto lost;\n\n\tnote = find_first_bit(wqueue->notes_bitmap, wqueue->nr_notes);\n\tif (note >= wqueue->nr_notes)\n\t\tgoto lost;\n\n\tpage = wqueue->notes[note / WATCH_QUEUE_NOTES_PER_PAGE];\n\toffset = note % WATCH_QUEUE_NOTES_PER_PAGE * WATCH_QUEUE_NOTE_SIZE;\n\tget_page(page);\n\tlen = n->info & WATCH_INFO_LENGTH;\n\tp = kmap_atomic(page);\n\tmemcpy(p + offset, n, len);\n\tkunmap_atomic(p);\n\n\tbuf = &pipe->bufs[head & mask];\n\tbuf->page = page;\n\tbuf->private = (unsigned long)wqueue;\n\tbuf->ops = &watch_queue_pipe_buf_ops;\n\tbuf->offset = offset;\n\tbuf->len = len;\n\tbuf->flags = PIPE_BUF_FLAG_WHOLE;\n\tsmp_store_release(&pipe->head, head + 1); /* vs pipe_read() */\n\n\tif (!test_and_clear_bit(note, wqueue->notes_bitmap)) {\n\t\tspin_unlock_irq(&pipe->rd_wait.lock);\n\t\tBUG();\n\t}\n\twake_up_interruptible_sync_poll_locked(&pipe->rd_wait, EPOLLIN | EPOLLRDNORM);\n\tdone = true;\n\nout:\n\tspin_unlock_irq(&pipe->rd_wait.lock);\n\tif (done)\n\t\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n\treturn done;\n\nlost:\n\tbuf = &pipe->bufs[(head - 1) & mask];\n\tbuf->flags |= PIPE_BUF_FLAG_LOSS;\n\tgoto out;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3439,
            "cve_id": "CVE-2022-1973",
            "code_snippet": "int log_replay(struct ntfs_inode *ni, bool *initialized)\n{\n\tint err;\n\tstruct ntfs_sb_info *sbi = ni->mi.sbi;\n\tstruct ntfs_log *log;\n\n\tstruct restart_info rst_info, rst_info2;\n\tu64 rec_lsn, ra_lsn, checkpt_lsn = 0, rlsn = 0;\n\tstruct ATTR_NAME_ENTRY *attr_names = NULL;\n\tstruct ATTR_NAME_ENTRY *ane;\n\tstruct RESTART_TABLE *dptbl = NULL;\n\tstruct RESTART_TABLE *trtbl = NULL;\n\tconst struct RESTART_TABLE *rt;\n\tstruct RESTART_TABLE *oatbl = NULL;\n\tstruct inode *inode;\n\tstruct OpenAttr *oa;\n\tstruct ntfs_inode *ni_oe;\n\tstruct ATTRIB *attr = NULL;\n\tu64 size, vcn, undo_next_lsn;\n\tCLST rno, lcn, lcn0, len0, clen;\n\tvoid *data;\n\tstruct NTFS_RESTART *rst = NULL;\n\tstruct lcb *lcb = NULL;\n\tstruct OPEN_ATTR_ENRTY *oe;\n\tstruct TRANSACTION_ENTRY *tr;\n\tstruct DIR_PAGE_ENTRY *dp;\n\tu32 i, bytes_per_attr_entry;\n\tu32 l_size = ni->vfs_inode.i_size;\n\tu32 orig_file_size = l_size;\n\tu32 page_size, vbo, tail, off, dlen;\n\tu32 saved_len, rec_len, transact_id;\n\tbool use_second_page;\n\tstruct RESTART_AREA *ra2, *ra = NULL;\n\tstruct CLIENT_REC *ca, *cr;\n\t__le16 client;\n\tstruct RESTART_HDR *rh;\n\tconst struct LFS_RECORD_HDR *frh;\n\tconst struct LOG_REC_HDR *lrh;\n\tbool is_mapped;\n\tbool is_ro = sb_rdonly(sbi->sb);\n\tu64 t64;\n\tu16 t16;\n\tu32 t32;\n\n\t/* Get the size of page. NOTE: To replay we can use default page. */\n#if PAGE_SIZE >= DefaultLogPageSize && PAGE_SIZE <= DefaultLogPageSize * 2\n\tpage_size = norm_file_page(PAGE_SIZE, &l_size, true);\n#else\n\tpage_size = norm_file_page(PAGE_SIZE, &l_size, false);\n#endif\n\tif (!page_size)\n\t\treturn -EINVAL;\n\n\tlog = kzalloc(sizeof(struct ntfs_log), GFP_NOFS);\n\tif (!log)\n\t\treturn -ENOMEM;\n\n\tlog->ni = ni;\n\tlog->l_size = l_size;\n\tlog->one_page_buf = kmalloc(page_size, GFP_NOFS);\n\n\tif (!log->one_page_buf) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tlog->page_size = page_size;\n\tlog->page_mask = page_size - 1;\n\tlog->page_bits = blksize_bits(page_size);\n\n\t/* Look for a restart area on the disk. */\n\terr = log_read_rst(log, l_size, true, &rst_info);\n\tif (err)\n\t\tgoto out;\n\n\t/* remember 'initialized' */\n\t*initialized = rst_info.initialized;\n\n\tif (!rst_info.restart) {\n\t\tif (rst_info.initialized) {\n\t\t\t/* No restart area but the file is not initialized. */\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tlog_init_pg_hdr(log, page_size, page_size, 1, 1);\n\t\tlog_create(log, l_size, 0, get_random_int(), false, false);\n\n\t\tlog->ra = ra;\n\n\t\tra = log_create_ra(log);\n\t\tif (!ra) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tlog->ra = ra;\n\t\tlog->init_ra = true;\n\n\t\tgoto process_log;\n\t}\n\n\t/*\n\t * If the restart offset above wasn't zero then we won't\n\t * look for a second restart.\n\t */\n\tif (rst_info.vbo)\n\t\tgoto check_restart_area;\n\n\terr = log_read_rst(log, l_size, false, &rst_info2);\n\n\t/* Determine which restart area to use. */\n\tif (!rst_info2.restart || rst_info2.last_lsn <= rst_info.last_lsn)\n\t\tgoto use_first_page;\n\n\tuse_second_page = true;\n\n\tif (rst_info.chkdsk_was_run && page_size != rst_info.vbo) {\n\t\tstruct RECORD_PAGE_HDR *sp = NULL;\n\t\tbool usa_error;\n\n\t\tif (!read_log_page(log, page_size, &sp, &usa_error) &&\n\t\t    sp->rhdr.sign == NTFS_CHKD_SIGNATURE) {\n\t\t\tuse_second_page = false;\n\t\t}\n\t\tkfree(sp);\n\t}\n\n\tif (use_second_page) {\n\t\tkfree(rst_info.r_page);\n\t\tmemcpy(&rst_info, &rst_info2, sizeof(struct restart_info));\n\t\trst_info2.r_page = NULL;\n\t}\n\nuse_first_page:\n\tkfree(rst_info2.r_page);\n\ncheck_restart_area:\n\t/*\n\t * If the restart area is at offset 0, we want\n\t * to write the second restart area first.\n\t */\n\tlog->init_ra = !!rst_info.vbo;\n\n\t/* If we have a valid page then grab a pointer to the restart area. */\n\tra2 = rst_info.valid_page\n\t\t      ? Add2Ptr(rst_info.r_page,\n\t\t\t\tle16_to_cpu(rst_info.r_page->ra_off))\n\t\t      : NULL;\n\n\tif (rst_info.chkdsk_was_run ||\n\t    (ra2 && ra2->client_idx[1] == LFS_NO_CLIENT_LE)) {\n\t\tbool wrapped = false;\n\t\tbool use_multi_page = false;\n\t\tu32 open_log_count;\n\n\t\t/* Do some checks based on whether we have a valid log page. */\n\t\tif (!rst_info.valid_page) {\n\t\t\topen_log_count = get_random_int();\n\t\t\tgoto init_log_instance;\n\t\t}\n\t\topen_log_count = le32_to_cpu(ra2->open_log_count);\n\n\t\t/*\n\t\t * If the restart page size isn't changing then we want to\n\t\t * check how much work we need to do.\n\t\t */\n\t\tif (page_size != le32_to_cpu(rst_info.r_page->sys_page_size))\n\t\t\tgoto init_log_instance;\n\ninit_log_instance:\n\t\tlog_init_pg_hdr(log, page_size, page_size, 1, 1);\n\n\t\tlog_create(log, l_size, rst_info.last_lsn, open_log_count,\n\t\t\t   wrapped, use_multi_page);\n\n\t\tra = log_create_ra(log);\n\t\tif (!ra) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tlog->ra = ra;\n\n\t\t/* Put the restart areas and initialize\n\t\t * the log file as required.\n\t\t */\n\t\tgoto process_log;\n\t}\n\n\tif (!ra2) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * If the log page or the system page sizes have changed, we can't\n\t * use the log file. We must use the system page size instead of the\n\t * default size if there is not a clean shutdown.\n\t */\n\tt32 = le32_to_cpu(rst_info.r_page->sys_page_size);\n\tif (page_size != t32) {\n\t\tl_size = orig_file_size;\n\t\tpage_size =\n\t\t\tnorm_file_page(t32, &l_size, t32 == DefaultLogPageSize);\n\t}\n\n\tif (page_size != t32 ||\n\t    page_size != le32_to_cpu(rst_info.r_page->page_size)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* If the file size has shrunk then we won't mount it. */\n\tif (l_size < le64_to_cpu(ra2->l_size)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tlog_init_pg_hdr(log, page_size, page_size,\n\t\t\tle16_to_cpu(rst_info.r_page->major_ver),\n\t\t\tle16_to_cpu(rst_info.r_page->minor_ver));\n\n\tlog->l_size = le64_to_cpu(ra2->l_size);\n\tlog->seq_num_bits = le32_to_cpu(ra2->seq_num_bits);\n\tlog->file_data_bits = sizeof(u64) * 8 - log->seq_num_bits;\n\tlog->seq_num_mask = (8 << log->file_data_bits) - 1;\n\tlog->last_lsn = le64_to_cpu(ra2->current_lsn);\n\tlog->seq_num = log->last_lsn >> log->file_data_bits;\n\tlog->ra_off = le16_to_cpu(rst_info.r_page->ra_off);\n\tlog->restart_size = log->sys_page_size - log->ra_off;\n\tlog->record_header_len = le16_to_cpu(ra2->rec_hdr_len);\n\tlog->ra_size = le16_to_cpu(ra2->ra_len);\n\tlog->data_off = le16_to_cpu(ra2->data_off);\n\tlog->data_size = log->page_size - log->data_off;\n\tlog->reserved = log->data_size - log->record_header_len;\n\n\tvbo = lsn_to_vbo(log, log->last_lsn);\n\n\tif (vbo < log->first_page) {\n\t\t/* This is a pseudo lsn. */\n\t\tlog->l_flags |= NTFSLOG_NO_LAST_LSN;\n\t\tlog->next_page = log->first_page;\n\t\tgoto find_oldest;\n\t}\n\n\t/* Find the end of this log record. */\n\toff = final_log_off(log, log->last_lsn,\n\t\t\t    le32_to_cpu(ra2->last_lsn_data_len));\n\n\t/* If we wrapped the file then increment the sequence number. */\n\tif (off <= vbo) {\n\t\tlog->seq_num += 1;\n\t\tlog->l_flags |= NTFSLOG_WRAPPED;\n\t}\n\n\t/* Now compute the next log page to use. */\n\tvbo &= ~log->sys_page_mask;\n\ttail = log->page_size - (off & log->page_mask) - 1;\n\n\t/*\n\t *If we can fit another log record on the page,\n\t * move back a page the log file.\n\t */\n\tif (tail >= log->record_header_len) {\n\t\tlog->l_flags |= NTFSLOG_REUSE_TAIL;\n\t\tlog->next_page = vbo;\n\t} else {\n\t\tlog->next_page = next_page_off(log, vbo);\n\t}\n\nfind_oldest:\n\t/*\n\t * Find the oldest client lsn. Use the last\n\t * flushed lsn as a starting point.\n\t */\n\tlog->oldest_lsn = log->last_lsn;\n\toldest_client_lsn(Add2Ptr(ra2, le16_to_cpu(ra2->client_off)),\n\t\t\t  ra2->client_idx[1], &log->oldest_lsn);\n\tlog->oldest_lsn_off = lsn_to_vbo(log, log->oldest_lsn);\n\n\tif (log->oldest_lsn_off < log->first_page)\n\t\tlog->l_flags |= NTFSLOG_NO_OLDEST_LSN;\n\n\tif (!(ra2->flags & RESTART_SINGLE_PAGE_IO))\n\t\tlog->l_flags |= NTFSLOG_WRAPPED | NTFSLOG_MULTIPLE_PAGE_IO;\n\n\tlog->current_openlog_count = le32_to_cpu(ra2->open_log_count);\n\tlog->total_avail_pages = log->l_size - log->first_page;\n\tlog->total_avail = log->total_avail_pages >> log->page_bits;\n\tlog->max_current_avail = log->total_avail * log->reserved;\n\tlog->total_avail = log->total_avail * log->data_size;\n\n\tlog->current_avail = current_log_avail(log);\n\n\tra = kzalloc(log->restart_size, GFP_NOFS);\n\tif (!ra) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\tlog->ra = ra;\n\n\tt16 = le16_to_cpu(ra2->client_off);\n\tif (t16 == offsetof(struct RESTART_AREA, clients)) {\n\t\tmemcpy(ra, ra2, log->ra_size);\n\t} else {\n\t\tmemcpy(ra, ra2, offsetof(struct RESTART_AREA, clients));\n\t\tmemcpy(ra->clients, Add2Ptr(ra2, t16),\n\t\t       le16_to_cpu(ra2->ra_len) - t16);\n\n\t\tlog->current_openlog_count = get_random_int();\n\t\tra->open_log_count = cpu_to_le32(log->current_openlog_count);\n\t\tlog->ra_size = offsetof(struct RESTART_AREA, clients) +\n\t\t\t       sizeof(struct CLIENT_REC);\n\t\tra->client_off =\n\t\t\tcpu_to_le16(offsetof(struct RESTART_AREA, clients));\n\t\tra->ra_len = cpu_to_le16(log->ra_size);\n\t}\n\n\tle32_add_cpu(&ra->open_log_count, 1);\n\n\t/* Now we need to walk through looking for the last lsn. */\n\terr = last_log_lsn(log);\n\tif (err)\n\t\tgoto out;\n\n\tlog->current_avail = current_log_avail(log);\n\n\t/* Remember which restart area to write first. */\n\tlog->init_ra = rst_info.vbo;\n\nprocess_log:\n\t/* 1.0, 1.1, 2.0 log->major_ver/minor_ver - short values. */\n\tswitch ((log->major_ver << 16) + log->minor_ver) {\n\tcase 0x10000:\n\tcase 0x10001:\n\tcase 0x20000:\n\t\tbreak;\n\tdefault:\n\t\tntfs_warn(sbi->sb, \"\\x24LogFile version %d.%d is not supported\",\n\t\t\t  log->major_ver, log->minor_ver);\n\t\terr = -EOPNOTSUPP;\n\t\tlog->set_dirty = true;\n\t\tgoto out;\n\t}\n\n\t/* One client \"NTFS\" per logfile. */\n\tca = Add2Ptr(ra, le16_to_cpu(ra->client_off));\n\n\tfor (client = ra->client_idx[1];; client = cr->next_client) {\n\t\tif (client == LFS_NO_CLIENT_LE) {\n\t\t\t/* Insert \"NTFS\" client LogFile. */\n\t\t\tclient = ra->client_idx[0];\n\t\t\tif (client == LFS_NO_CLIENT_LE) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tt16 = le16_to_cpu(client);\n\t\t\tcr = ca + t16;\n\n\t\t\tremove_client(ca, cr, &ra->client_idx[0]);\n\n\t\t\tcr->restart_lsn = 0;\n\t\t\tcr->oldest_lsn = cpu_to_le64(log->oldest_lsn);\n\t\t\tcr->name_bytes = cpu_to_le32(8);\n\t\t\tcr->name[0] = cpu_to_le16('N');\n\t\t\tcr->name[1] = cpu_to_le16('T');\n\t\t\tcr->name[2] = cpu_to_le16('F');\n\t\t\tcr->name[3] = cpu_to_le16('S');\n\n\t\t\tadd_client(ca, t16, &ra->client_idx[1]);\n\t\t\tbreak;\n\t\t}\n\n\t\tcr = ca + le16_to_cpu(client);\n\n\t\tif (cpu_to_le32(8) == cr->name_bytes &&\n\t\t    cpu_to_le16('N') == cr->name[0] &&\n\t\t    cpu_to_le16('T') == cr->name[1] &&\n\t\t    cpu_to_le16('F') == cr->name[2] &&\n\t\t    cpu_to_le16('S') == cr->name[3])\n\t\t\tbreak;\n\t}\n\n\t/* Update the client handle with the client block information. */\n\tlog->client_id.seq_num = cr->seq_num;\n\tlog->client_id.client_idx = client;\n\n\terr = read_rst_area(log, &rst, &ra_lsn);\n\tif (err)\n\t\tgoto out;\n\n\tif (!rst)\n\t\tgoto out;\n\n\tbytes_per_attr_entry = !rst->major_ver ? 0x2C : 0x28;\n\n\tcheckpt_lsn = le64_to_cpu(rst->check_point_start);\n\tif (!checkpt_lsn)\n\t\tcheckpt_lsn = ra_lsn;\n\n\t/* Allocate and Read the Transaction Table. */\n\tif (!rst->transact_table_len)\n\t\tgoto check_dirty_page_table;\n\n\tt64 = le64_to_cpu(rst->transact_table_lsn);\n\terr = read_log_rec_lcb(log, t64, lcb_ctx_prev, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\tlrh = lcb->log_rec;\n\tfrh = lcb->lrh;\n\trec_len = le32_to_cpu(frh->client_data_len);\n\n\tif (!check_log_rec(lrh, rec_len, le32_to_cpu(frh->transact_id),\n\t\t\t   bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tt16 = le16_to_cpu(lrh->redo_off);\n\n\trt = Add2Ptr(lrh, t16);\n\tt32 = rec_len - t16;\n\n\t/* Now check that this is a valid restart table. */\n\tif (!check_rstbl(rt, t32)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\ttrtbl = kmemdup(rt, t32, GFP_NOFS);\n\tif (!trtbl) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tlcb_put(lcb);\n\tlcb = NULL;\n\ncheck_dirty_page_table:\n\t/* The next record back should be the Dirty Pages Table. */\n\tif (!rst->dirty_pages_len)\n\t\tgoto check_attribute_names;\n\n\tt64 = le64_to_cpu(rst->dirty_pages_table_lsn);\n\terr = read_log_rec_lcb(log, t64, lcb_ctx_prev, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\tlrh = lcb->log_rec;\n\tfrh = lcb->lrh;\n\trec_len = le32_to_cpu(frh->client_data_len);\n\n\tif (!check_log_rec(lrh, rec_len, le32_to_cpu(frh->transact_id),\n\t\t\t   bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tt16 = le16_to_cpu(lrh->redo_off);\n\n\trt = Add2Ptr(lrh, t16);\n\tt32 = rec_len - t16;\n\n\t/* Now check that this is a valid restart table. */\n\tif (!check_rstbl(rt, t32)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tdptbl = kmemdup(rt, t32, GFP_NOFS);\n\tif (!dptbl) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t/* Convert Ra version '0' into version '1'. */\n\tif (rst->major_ver)\n\t\tgoto end_conv_1;\n\n\tdp = NULL;\n\twhile ((dp = enum_rstbl(dptbl, dp))) {\n\t\tstruct DIR_PAGE_ENTRY_32 *dp0 = (struct DIR_PAGE_ENTRY_32 *)dp;\n\t\t// NOTE: Danger. Check for of boundary.\n\t\tmemmove(&dp->vcn, &dp0->vcn_low,\n\t\t\t2 * sizeof(u64) +\n\t\t\t\tle32_to_cpu(dp->lcns_follow) * sizeof(u64));\n\t}\n\nend_conv_1:\n\tlcb_put(lcb);\n\tlcb = NULL;\n\n\t/*\n\t * Go through the table and remove the duplicates,\n\t * remembering the oldest lsn values.\n\t */\n\tif (sbi->cluster_size <= log->page_size)\n\t\tgoto trace_dp_table;\n\n\tdp = NULL;\n\twhile ((dp = enum_rstbl(dptbl, dp))) {\n\t\tstruct DIR_PAGE_ENTRY *next = dp;\n\n\t\twhile ((next = enum_rstbl(dptbl, next))) {\n\t\t\tif (next->target_attr == dp->target_attr &&\n\t\t\t    next->vcn == dp->vcn) {\n\t\t\t\tif (le64_to_cpu(next->oldest_lsn) <\n\t\t\t\t    le64_to_cpu(dp->oldest_lsn)) {\n\t\t\t\t\tdp->oldest_lsn = next->oldest_lsn;\n\t\t\t\t}\n\n\t\t\t\tfree_rsttbl_idx(dptbl, PtrOffset(dptbl, next));\n\t\t\t}\n\t\t}\n\t}\ntrace_dp_table:\ncheck_attribute_names:\n\t/* The next record should be the Attribute Names. */\n\tif (!rst->attr_names_len)\n\t\tgoto check_attr_table;\n\n\tt64 = le64_to_cpu(rst->attr_names_lsn);\n\terr = read_log_rec_lcb(log, t64, lcb_ctx_prev, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\tlrh = lcb->log_rec;\n\tfrh = lcb->lrh;\n\trec_len = le32_to_cpu(frh->client_data_len);\n\n\tif (!check_log_rec(lrh, rec_len, le32_to_cpu(frh->transact_id),\n\t\t\t   bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tt32 = lrh_length(lrh);\n\trec_len -= t32;\n\n\tattr_names = kmemdup(Add2Ptr(lrh, t32), rec_len, GFP_NOFS);\n\n\tlcb_put(lcb);\n\tlcb = NULL;\n\ncheck_attr_table:\n\t/* The next record should be the attribute Table. */\n\tif (!rst->open_attr_len)\n\t\tgoto check_attribute_names2;\n\n\tt64 = le64_to_cpu(rst->open_attr_table_lsn);\n\terr = read_log_rec_lcb(log, t64, lcb_ctx_prev, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\tlrh = lcb->log_rec;\n\tfrh = lcb->lrh;\n\trec_len = le32_to_cpu(frh->client_data_len);\n\n\tif (!check_log_rec(lrh, rec_len, le32_to_cpu(frh->transact_id),\n\t\t\t   bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tt16 = le16_to_cpu(lrh->redo_off);\n\n\trt = Add2Ptr(lrh, t16);\n\tt32 = rec_len - t16;\n\n\tif (!check_rstbl(rt, t32)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\toatbl = kmemdup(rt, t32, GFP_NOFS);\n\tif (!oatbl) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tlog->open_attr_tbl = oatbl;\n\n\t/* Clear all of the Attr pointers. */\n\toe = NULL;\n\twhile ((oe = enum_rstbl(oatbl, oe))) {\n\t\tif (!rst->major_ver) {\n\t\t\tstruct OPEN_ATTR_ENRTY_32 oe0;\n\n\t\t\t/* Really 'oe' points to OPEN_ATTR_ENRTY_32. */\n\t\t\tmemcpy(&oe0, oe, SIZEOF_OPENATTRIBUTEENTRY0);\n\n\t\t\toe->bytes_per_index = oe0.bytes_per_index;\n\t\t\toe->type = oe0.type;\n\t\t\toe->is_dirty_pages = oe0.is_dirty_pages;\n\t\t\toe->name_len = 0;\n\t\t\toe->ref = oe0.ref;\n\t\t\toe->open_record_lsn = oe0.open_record_lsn;\n\t\t}\n\n\t\toe->is_attr_name = 0;\n\t\toe->ptr = NULL;\n\t}\n\n\tlcb_put(lcb);\n\tlcb = NULL;\n\ncheck_attribute_names2:\n\tif (!rst->attr_names_len)\n\t\tgoto trace_attribute_table;\n\n\tane = attr_names;\n\tif (!oatbl)\n\t\tgoto trace_attribute_table;\n\twhile (ane->off) {\n\t\t/* TODO: Clear table on exit! */\n\t\toe = Add2Ptr(oatbl, le16_to_cpu(ane->off));\n\t\tt16 = le16_to_cpu(ane->name_bytes);\n\t\toe->name_len = t16 / sizeof(short);\n\t\toe->ptr = ane->name;\n\t\toe->is_attr_name = 2;\n\t\tane = Add2Ptr(ane, sizeof(struct ATTR_NAME_ENTRY) + t16);\n\t}\n\ntrace_attribute_table:\n\t/*\n\t * If the checkpt_lsn is zero, then this is a freshly\n\t * formatted disk and we have no work to do.\n\t */\n\tif (!checkpt_lsn) {\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\tif (!oatbl) {\n\t\toatbl = init_rsttbl(bytes_per_attr_entry, 8);\n\t\tif (!oatbl) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tlog->open_attr_tbl = oatbl;\n\n\t/* Start the analysis pass from the Checkpoint lsn. */\n\trec_lsn = checkpt_lsn;\n\n\t/* Read the first lsn. */\n\terr = read_log_rec_lcb(log, checkpt_lsn, lcb_ctx_next, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\t/* Loop to read all subsequent records to the end of the log file. */\nnext_log_record_analyze:\n\terr = read_next_log_rec(log, lcb, &rec_lsn);\n\tif (err)\n\t\tgoto out;\n\n\tif (!rec_lsn)\n\t\tgoto end_log_records_enumerate;\n\n\tfrh = lcb->lrh;\n\ttransact_id = le32_to_cpu(frh->transact_id);\n\trec_len = le32_to_cpu(frh->client_data_len);\n\tlrh = lcb->log_rec;\n\n\tif (!check_log_rec(lrh, rec_len, transact_id, bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * The first lsn after the previous lsn remembered\n\t * the checkpoint is the first candidate for the rlsn.\n\t */\n\tif (!rlsn)\n\t\trlsn = rec_lsn;\n\n\tif (LfsClientRecord != frh->record_type)\n\t\tgoto next_log_record_analyze;\n\n\t/*\n\t * Now update the Transaction Table for this transaction. If there\n\t * is no entry present or it is unallocated we allocate the entry.\n\t */\n\tif (!trtbl) {\n\t\ttrtbl = init_rsttbl(sizeof(struct TRANSACTION_ENTRY),\n\t\t\t\t    INITIAL_NUMBER_TRANSACTIONS);\n\t\tif (!trtbl) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\ttr = Add2Ptr(trtbl, transact_id);\n\n\tif (transact_id >= bytes_per_rt(trtbl) ||\n\t    tr->next != RESTART_ENTRY_ALLOCATED_LE) {\n\t\ttr = alloc_rsttbl_from_idx(&trtbl, transact_id);\n\t\tif (!tr) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\ttr->transact_state = TransactionActive;\n\t\ttr->first_lsn = cpu_to_le64(rec_lsn);\n\t}\n\n\ttr->prev_lsn = tr->undo_next_lsn = cpu_to_le64(rec_lsn);\n\n\t/*\n\t * If this is a compensation log record, then change\n\t * the undo_next_lsn to be the undo_next_lsn of this record.\n\t */\n\tif (lrh->undo_op == cpu_to_le16(CompensationLogRecord))\n\t\ttr->undo_next_lsn = frh->client_undo_next_lsn;\n\n\t/* Dispatch to handle log record depending on type. */\n\tswitch (le16_to_cpu(lrh->redo_op)) {\n\tcase InitializeFileRecordSegment:\n\tcase DeallocateFileRecordSegment:\n\tcase WriteEndOfFileRecordSegment:\n\tcase CreateAttribute:\n\tcase DeleteAttribute:\n\tcase UpdateResidentValue:\n\tcase UpdateNonresidentValue:\n\tcase UpdateMappingPairs:\n\tcase SetNewAttributeSizes:\n\tcase AddIndexEntryRoot:\n\tcase DeleteIndexEntryRoot:\n\tcase AddIndexEntryAllocation:\n\tcase DeleteIndexEntryAllocation:\n\tcase WriteEndOfIndexBuffer:\n\tcase SetIndexEntryVcnRoot:\n\tcase SetIndexEntryVcnAllocation:\n\tcase UpdateFileNameRoot:\n\tcase UpdateFileNameAllocation:\n\tcase SetBitsInNonresidentBitMap:\n\tcase ClearBitsInNonresidentBitMap:\n\tcase UpdateRecordDataRoot:\n\tcase UpdateRecordDataAllocation:\n\tcase ZeroEndOfFileRecord:\n\t\tt16 = le16_to_cpu(lrh->target_attr);\n\t\tt64 = le64_to_cpu(lrh->target_vcn);\n\t\tdp = find_dp(dptbl, t16, t64);\n\n\t\tif (dp)\n\t\t\tgoto copy_lcns;\n\n\t\t/*\n\t\t * Calculate the number of clusters per page the system\n\t\t * which wrote the checkpoint, possibly creating the table.\n\t\t */\n\t\tif (dptbl) {\n\t\t\tt32 = (le16_to_cpu(dptbl->size) -\n\t\t\t       sizeof(struct DIR_PAGE_ENTRY)) /\n\t\t\t      sizeof(u64);\n\t\t} else {\n\t\t\tt32 = log->clst_per_page;\n\t\t\tkfree(dptbl);\n\t\t\tdptbl = init_rsttbl(struct_size(dp, page_lcns, t32),\n\t\t\t\t\t    32);\n\t\t\tif (!dptbl) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tdp = alloc_rsttbl_idx(&dptbl);\n\t\tif (!dp) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tdp->target_attr = cpu_to_le32(t16);\n\t\tdp->transfer_len = cpu_to_le32(t32 << sbi->cluster_bits);\n\t\tdp->lcns_follow = cpu_to_le32(t32);\n\t\tdp->vcn = cpu_to_le64(t64 & ~((u64)t32 - 1));\n\t\tdp->oldest_lsn = cpu_to_le64(rec_lsn);\n\ncopy_lcns:\n\t\t/*\n\t\t * Copy the Lcns from the log record into the Dirty Page Entry.\n\t\t * TODO: For different page size support, must somehow make\n\t\t * whole routine a loop, case Lcns do not fit below.\n\t\t */\n\t\tt16 = le16_to_cpu(lrh->lcns_follow);\n\t\tfor (i = 0; i < t16; i++) {\n\t\t\tsize_t j = (size_t)(le64_to_cpu(lrh->target_vcn) -\n\t\t\t\t\t    le64_to_cpu(dp->vcn));\n\t\t\tdp->page_lcns[j + i] = lrh->page_lcns[i];\n\t\t}\n\n\t\tgoto next_log_record_analyze;\n\n\tcase DeleteDirtyClusters: {\n\t\tu32 range_count =\n\t\t\tle16_to_cpu(lrh->redo_len) / sizeof(struct LCN_RANGE);\n\t\tconst struct LCN_RANGE *r =\n\t\t\tAdd2Ptr(lrh, le16_to_cpu(lrh->redo_off));\n\n\t\t/* Loop through all of the Lcn ranges this log record. */\n\t\tfor (i = 0; i < range_count; i++, r++) {\n\t\t\tu64 lcn0 = le64_to_cpu(r->lcn);\n\t\t\tu64 lcn_e = lcn0 + le64_to_cpu(r->len) - 1;\n\n\t\t\tdp = NULL;\n\t\t\twhile ((dp = enum_rstbl(dptbl, dp))) {\n\t\t\t\tu32 j;\n\n\t\t\t\tt32 = le32_to_cpu(dp->lcns_follow);\n\t\t\t\tfor (j = 0; j < t32; j++) {\n\t\t\t\t\tt64 = le64_to_cpu(dp->page_lcns[j]);\n\t\t\t\t\tif (t64 >= lcn0 && t64 <= lcn_e)\n\t\t\t\t\t\tdp->page_lcns[j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tgoto next_log_record_analyze;\n\t\t;\n\t}\n\n\tcase OpenNonresidentAttribute:\n\t\tt16 = le16_to_cpu(lrh->target_attr);\n\t\tif (t16 >= bytes_per_rt(oatbl)) {\n\t\t\t/*\n\t\t\t * Compute how big the table needs to be.\n\t\t\t * Add 10 extra entries for some cushion.\n\t\t\t */\n\t\t\tu32 new_e = t16 / le16_to_cpu(oatbl->size);\n\n\t\t\tnew_e += 10 - le16_to_cpu(oatbl->used);\n\n\t\t\toatbl = extend_rsttbl(oatbl, new_e, ~0u);\n\t\t\tlog->open_attr_tbl = oatbl;\n\t\t\tif (!oatbl) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\t/* Point to the entry being opened. */\n\t\toe = alloc_rsttbl_from_idx(&oatbl, t16);\n\t\tlog->open_attr_tbl = oatbl;\n\t\tif (!oe) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/* Initialize this entry from the log record. */\n\t\tt16 = le16_to_cpu(lrh->redo_off);\n\t\tif (!rst->major_ver) {\n\t\t\t/* Convert version '0' into version '1'. */\n\t\t\tstruct OPEN_ATTR_ENRTY_32 *oe0 = Add2Ptr(lrh, t16);\n\n\t\t\toe->bytes_per_index = oe0->bytes_per_index;\n\t\t\toe->type = oe0->type;\n\t\t\toe->is_dirty_pages = oe0->is_dirty_pages;\n\t\t\toe->name_len = 0; //oe0.name_len;\n\t\t\toe->ref = oe0->ref;\n\t\t\toe->open_record_lsn = oe0->open_record_lsn;\n\t\t} else {\n\t\t\tmemcpy(oe, Add2Ptr(lrh, t16), bytes_per_attr_entry);\n\t\t}\n\n\t\tt16 = le16_to_cpu(lrh->undo_len);\n\t\tif (t16) {\n\t\t\toe->ptr = kmalloc(t16, GFP_NOFS);\n\t\t\tif (!oe->ptr) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\toe->name_len = t16 / sizeof(short);\n\t\t\tmemcpy(oe->ptr,\n\t\t\t       Add2Ptr(lrh, le16_to_cpu(lrh->undo_off)), t16);\n\t\t\toe->is_attr_name = 1;\n\t\t} else {\n\t\t\toe->ptr = NULL;\n\t\t\toe->is_attr_name = 0;\n\t\t}\n\n\t\tgoto next_log_record_analyze;\n\n\tcase HotFix:\n\t\tt16 = le16_to_cpu(lrh->target_attr);\n\t\tt64 = le64_to_cpu(lrh->target_vcn);\n\t\tdp = find_dp(dptbl, t16, t64);\n\t\tif (dp) {\n\t\t\tsize_t j = le64_to_cpu(lrh->target_vcn) -\n\t\t\t\t   le64_to_cpu(dp->vcn);\n\t\t\tif (dp->page_lcns[j])\n\t\t\t\tdp->page_lcns[j] = lrh->page_lcns[0];\n\t\t}\n\t\tgoto next_log_record_analyze;\n\n\tcase EndTopLevelAction:\n\t\ttr = Add2Ptr(trtbl, transact_id);\n\t\ttr->prev_lsn = cpu_to_le64(rec_lsn);\n\t\ttr->undo_next_lsn = frh->client_undo_next_lsn;\n\t\tgoto next_log_record_analyze;\n\n\tcase PrepareTransaction:\n\t\ttr = Add2Ptr(trtbl, transact_id);\n\t\ttr->transact_state = TransactionPrepared;\n\t\tgoto next_log_record_analyze;\n\n\tcase CommitTransaction:\n\t\ttr = Add2Ptr(trtbl, transact_id);\n\t\ttr->transact_state = TransactionCommitted;\n\t\tgoto next_log_record_analyze;\n\n\tcase ForgetTransaction:\n\t\tfree_rsttbl_idx(trtbl, transact_id);\n\t\tgoto next_log_record_analyze;\n\n\tcase Noop:\n\tcase OpenAttributeTableDump:\n\tcase AttributeNamesDump:\n\tcase DirtyPageTableDump:\n\tcase TransactionTableDump:\n\t\t/* The following cases require no action the Analysis Pass. */\n\t\tgoto next_log_record_analyze;\n\n\tdefault:\n\t\t/*\n\t\t * All codes will be explicitly handled.\n\t\t * If we see a code we do not expect, then we are trouble.\n\t\t */\n\t\tgoto next_log_record_analyze;\n\t}\n\nend_log_records_enumerate:\n\tlcb_put(lcb);\n\tlcb = NULL;\n\n\t/*\n\t * Scan the Dirty Page Table and Transaction Table for\n\t * the lowest lsn, and return it as the Redo lsn.\n\t */\n\tdp = NULL;\n\twhile ((dp = enum_rstbl(dptbl, dp))) {\n\t\tt64 = le64_to_cpu(dp->oldest_lsn);\n\t\tif (t64 && t64 < rlsn)\n\t\t\trlsn = t64;\n\t}\n\n\ttr = NULL;\n\twhile ((tr = enum_rstbl(trtbl, tr))) {\n\t\tt64 = le64_to_cpu(tr->first_lsn);\n\t\tif (t64 && t64 < rlsn)\n\t\t\trlsn = t64;\n\t}\n\n\t/*\n\t * Only proceed if the Dirty Page Table or Transaction\n\t * table are not empty.\n\t */\n\tif ((!dptbl || !dptbl->total) && (!trtbl || !trtbl->total))\n\t\tgoto end_reply;\n\n\tsbi->flags |= NTFS_FLAGS_NEED_REPLAY;\n\tif (is_ro)\n\t\tgoto out;\n\n\t/* Reopen all of the attributes with dirty pages. */\n\toe = NULL;\nnext_open_attribute:\n\n\toe = enum_rstbl(oatbl, oe);\n\tif (!oe) {\n\t\terr = 0;\n\t\tdp = NULL;\n\t\tgoto next_dirty_page;\n\t}\n\n\toa = kzalloc(sizeof(struct OpenAttr), GFP_NOFS);\n\tif (!oa) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tinode = ntfs_iget5(sbi->sb, &oe->ref, NULL);\n\tif (IS_ERR(inode))\n\t\tgoto fake_attr;\n\n\tif (is_bad_inode(inode)) {\n\t\tiput(inode);\nfake_attr:\n\t\tif (oa->ni) {\n\t\t\tiput(&oa->ni->vfs_inode);\n\t\t\toa->ni = NULL;\n\t\t}\n\n\t\tattr = attr_create_nonres_log(sbi, oe->type, 0, oe->ptr,\n\t\t\t\t\t      oe->name_len, 0);\n\t\tif (!attr) {\n\t\t\tkfree(oa);\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\toa->attr = attr;\n\t\toa->run1 = &oa->run0;\n\t\tgoto final_oe;\n\t}\n\n\tni_oe = ntfs_i(inode);\n\toa->ni = ni_oe;\n\n\tattr = ni_find_attr(ni_oe, NULL, NULL, oe->type, oe->ptr, oe->name_len,\n\t\t\t    NULL, NULL);\n\n\tif (!attr)\n\t\tgoto fake_attr;\n\n\tt32 = le32_to_cpu(attr->size);\n\toa->attr = kmemdup(attr, t32, GFP_NOFS);\n\tif (!oa->attr)\n\t\tgoto fake_attr;\n\n\tif (!S_ISDIR(inode->i_mode)) {\n\t\tif (attr->type == ATTR_DATA && !attr->name_len) {\n\t\t\toa->run1 = &ni_oe->file.run;\n\t\t\tgoto final_oe;\n\t\t}\n\t} else {\n\t\tif (attr->type == ATTR_ALLOC &&\n\t\t    attr->name_len == ARRAY_SIZE(I30_NAME) &&\n\t\t    !memcmp(attr_name(attr), I30_NAME, sizeof(I30_NAME))) {\n\t\t\toa->run1 = &ni_oe->dir.alloc_run;\n\t\t\tgoto final_oe;\n\t\t}\n\t}\n\n\tif (attr->non_res) {\n\t\tu16 roff = le16_to_cpu(attr->nres.run_off);\n\t\tCLST svcn = le64_to_cpu(attr->nres.svcn);\n\n\t\terr = run_unpack(&oa->run0, sbi, inode->i_ino, svcn,\n\t\t\t\t le64_to_cpu(attr->nres.evcn), svcn,\n\t\t\t\t Add2Ptr(attr, roff), t32 - roff);\n\t\tif (err < 0) {\n\t\t\tkfree(oa->attr);\n\t\t\toa->attr = NULL;\n\t\t\tgoto fake_attr;\n\t\t}\n\t\terr = 0;\n\t}\n\toa->run1 = &oa->run0;\n\tattr = oa->attr;\n\nfinal_oe:\n\tif (oe->is_attr_name == 1)\n\t\tkfree(oe->ptr);\n\toe->is_attr_name = 0;\n\toe->ptr = oa;\n\toe->name_len = attr->name_len;\n\n\tgoto next_open_attribute;\n\n\t/*\n\t * Now loop through the dirty page table to extract all of the Vcn/Lcn.\n\t * Mapping that we have, and insert it into the appropriate run.\n\t */\nnext_dirty_page:\n\tdp = enum_rstbl(dptbl, dp);\n\tif (!dp)\n\t\tgoto do_redo_1;\n\n\toe = Add2Ptr(oatbl, le32_to_cpu(dp->target_attr));\n\n\tif (oe->next != RESTART_ENTRY_ALLOCATED_LE)\n\t\tgoto next_dirty_page;\n\n\toa = oe->ptr;\n\tif (!oa)\n\t\tgoto next_dirty_page;\n\n\ti = -1;\nnext_dirty_page_vcn:\n\ti += 1;\n\tif (i >= le32_to_cpu(dp->lcns_follow))\n\t\tgoto next_dirty_page;\n\n\tvcn = le64_to_cpu(dp->vcn) + i;\n\tsize = (vcn + 1) << sbi->cluster_bits;\n\n\tif (!dp->page_lcns[i])\n\t\tgoto next_dirty_page_vcn;\n\n\trno = ino_get(&oe->ref);\n\tif (rno <= MFT_REC_MIRR &&\n\t    size < (MFT_REC_VOL + 1) * sbi->record_size &&\n\t    oe->type == ATTR_DATA) {\n\t\tgoto next_dirty_page_vcn;\n\t}\n\n\tlcn = le64_to_cpu(dp->page_lcns[i]);\n\n\tif ((!run_lookup_entry(oa->run1, vcn, &lcn0, &len0, NULL) ||\n\t     lcn0 != lcn) &&\n\t    !run_add_entry(oa->run1, vcn, lcn, 1, false)) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\tattr = oa->attr;\n\tt64 = le64_to_cpu(attr->nres.alloc_size);\n\tif (size > t64) {\n\t\tattr->nres.valid_size = attr->nres.data_size =\n\t\t\tattr->nres.alloc_size = cpu_to_le64(size);\n\t}\n\tgoto next_dirty_page_vcn;\n\ndo_redo_1:\n\t/*\n\t * Perform the Redo Pass, to restore all of the dirty pages to the same\n\t * contents that they had immediately before the crash. If the dirty\n\t * page table is empty, then we can skip the entire Redo Pass.\n\t */\n\tif (!dptbl || !dptbl->total)\n\t\tgoto do_undo_action;\n\n\trec_lsn = rlsn;\n\n\t/*\n\t * Read the record at the Redo lsn, before falling\n\t * into common code to handle each record.\n\t */\n\terr = read_log_rec_lcb(log, rlsn, lcb_ctx_next, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\t/*\n\t * Now loop to read all of our log records forwards, until\n\t * we hit the end of the file, cleaning up at the end.\n\t */\ndo_action_next:\n\tfrh = lcb->lrh;\n\n\tif (LfsClientRecord != frh->record_type)\n\t\tgoto read_next_log_do_action;\n\n\ttransact_id = le32_to_cpu(frh->transact_id);\n\trec_len = le32_to_cpu(frh->client_data_len);\n\tlrh = lcb->log_rec;\n\n\tif (!check_log_rec(lrh, rec_len, transact_id, bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* Ignore log records that do not update pages. */\n\tif (lrh->lcns_follow)\n\t\tgoto find_dirty_page;\n\n\tgoto read_next_log_do_action;\n\nfind_dirty_page:\n\tt16 = le16_to_cpu(lrh->target_attr);\n\tt64 = le64_to_cpu(lrh->target_vcn);\n\tdp = find_dp(dptbl, t16, t64);\n\n\tif (!dp)\n\t\tgoto read_next_log_do_action;\n\n\tif (rec_lsn < le64_to_cpu(dp->oldest_lsn))\n\t\tgoto read_next_log_do_action;\n\n\tt16 = le16_to_cpu(lrh->target_attr);\n\tif (t16 >= bytes_per_rt(oatbl)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\toe = Add2Ptr(oatbl, t16);\n\n\tif (oe->next != RESTART_ENTRY_ALLOCATED_LE) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\toa = oe->ptr;\n\n\tif (!oa) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tattr = oa->attr;\n\n\tvcn = le64_to_cpu(lrh->target_vcn);\n\n\tif (!run_lookup_entry(oa->run1, vcn, &lcn, NULL, NULL) ||\n\t    lcn == SPARSE_LCN) {\n\t\tgoto read_next_log_do_action;\n\t}\n\n\t/* Point to the Redo data and get its length. */\n\tdata = Add2Ptr(lrh, le16_to_cpu(lrh->redo_off));\n\tdlen = le16_to_cpu(lrh->redo_len);\n\n\t/* Shorten length by any Lcns which were deleted. */\n\tsaved_len = dlen;\n\n\tfor (i = le16_to_cpu(lrh->lcns_follow); i; i--) {\n\t\tsize_t j;\n\t\tu32 alen, voff;\n\n\t\tvoff = le16_to_cpu(lrh->record_off) +\n\t\t       le16_to_cpu(lrh->attr_off);\n\t\tvoff += le16_to_cpu(lrh->cluster_off) << SECTOR_SHIFT;\n\n\t\t/* If the Vcn question is allocated, we can just get out. */\n\t\tj = le64_to_cpu(lrh->target_vcn) - le64_to_cpu(dp->vcn);\n\t\tif (dp->page_lcns[j + i - 1])\n\t\t\tbreak;\n\n\t\tif (!saved_len)\n\t\t\tsaved_len = 1;\n\n\t\t/*\n\t\t * Calculate the allocated space left relative to the\n\t\t * log record Vcn, after removing this unallocated Vcn.\n\t\t */\n\t\talen = (i - 1) << sbi->cluster_bits;\n\n\t\t/*\n\t\t * If the update described this log record goes beyond\n\t\t * the allocated space, then we will have to reduce the length.\n\t\t */\n\t\tif (voff >= alen)\n\t\t\tdlen = 0;\n\t\telse if (voff + dlen > alen)\n\t\t\tdlen = alen - voff;\n\t}\n\n\t/*\n\t * If the resulting dlen from above is now zero,\n\t * we can skip this log record.\n\t */\n\tif (!dlen && saved_len)\n\t\tgoto read_next_log_do_action;\n\n\tt16 = le16_to_cpu(lrh->redo_op);\n\tif (can_skip_action(t16))\n\t\tgoto read_next_log_do_action;\n\n\t/* Apply the Redo operation a common routine. */\n\terr = do_action(log, oe, lrh, t16, data, dlen, rec_len, &rec_lsn);\n\tif (err)\n\t\tgoto out;\n\n\t/* Keep reading and looping back until end of file. */\nread_next_log_do_action:\n\terr = read_next_log_rec(log, lcb, &rec_lsn);\n\tif (!err && rec_lsn)\n\t\tgoto do_action_next;\n\n\tlcb_put(lcb);\n\tlcb = NULL;\n\ndo_undo_action:\n\t/* Scan Transaction Table. */\n\ttr = NULL;\ntransaction_table_next:\n\ttr = enum_rstbl(trtbl, tr);\n\tif (!tr)\n\t\tgoto undo_action_done;\n\n\tif (TransactionActive != tr->transact_state || !tr->undo_next_lsn) {\n\t\tfree_rsttbl_idx(trtbl, PtrOffset(trtbl, tr));\n\t\tgoto transaction_table_next;\n\t}\n\n\tlog->transaction_id = PtrOffset(trtbl, tr);\n\tundo_next_lsn = le64_to_cpu(tr->undo_next_lsn);\n\n\t/*\n\t * We only have to do anything if the transaction has\n\t * something its undo_next_lsn field.\n\t */\n\tif (!undo_next_lsn)\n\t\tgoto commit_undo;\n\n\t/* Read the first record to be undone by this transaction. */\n\terr = read_log_rec_lcb(log, undo_next_lsn, lcb_ctx_undo_next, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\t/*\n\t * Now loop to read all of our log records forwards,\n\t * until we hit the end of the file, cleaning up at the end.\n\t */\nundo_action_next:\n\n\tlrh = lcb->log_rec;\n\tfrh = lcb->lrh;\n\ttransact_id = le32_to_cpu(frh->transact_id);\n\trec_len = le32_to_cpu(frh->client_data_len);\n\n\tif (!check_log_rec(lrh, rec_len, transact_id, bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (lrh->undo_op == cpu_to_le16(Noop))\n\t\tgoto read_next_log_undo_action;\n\n\toe = Add2Ptr(oatbl, le16_to_cpu(lrh->target_attr));\n\toa = oe->ptr;\n\n\tt16 = le16_to_cpu(lrh->lcns_follow);\n\tif (!t16)\n\t\tgoto add_allocated_vcns;\n\n\tis_mapped = run_lookup_entry(oa->run1, le64_to_cpu(lrh->target_vcn),\n\t\t\t\t     &lcn, &clen, NULL);\n\n\t/*\n\t * If the mapping isn't already the table or the  mapping\n\t * corresponds to a hole the mapping, we need to make sure\n\t * there is no partial page already memory.\n\t */\n\tif (is_mapped && lcn != SPARSE_LCN && clen >= t16)\n\t\tgoto add_allocated_vcns;\n\n\tvcn = le64_to_cpu(lrh->target_vcn);\n\tvcn &= ~(log->clst_per_page - 1);\n\nadd_allocated_vcns:\n\tfor (i = 0, vcn = le64_to_cpu(lrh->target_vcn),\n\t    size = (vcn + 1) << sbi->cluster_bits;\n\t     i < t16; i++, vcn += 1, size += sbi->cluster_size) {\n\t\tattr = oa->attr;\n\t\tif (!attr->non_res) {\n\t\t\tif (size > le32_to_cpu(attr->res.data_size))\n\t\t\t\tattr->res.data_size = cpu_to_le32(size);\n\t\t} else {\n\t\t\tif (size > le64_to_cpu(attr->nres.data_size))\n\t\t\t\tattr->nres.valid_size = attr->nres.data_size =\n\t\t\t\t\tattr->nres.alloc_size =\n\t\t\t\t\t\tcpu_to_le64(size);\n\t\t}\n\t}\n\n\tt16 = le16_to_cpu(lrh->undo_op);\n\tif (can_skip_action(t16))\n\t\tgoto read_next_log_undo_action;\n\n\t/* Point to the Redo data and get its length. */\n\tdata = Add2Ptr(lrh, le16_to_cpu(lrh->undo_off));\n\tdlen = le16_to_cpu(lrh->undo_len);\n\n\t/* It is time to apply the undo action. */\n\terr = do_action(log, oe, lrh, t16, data, dlen, rec_len, NULL);\n\nread_next_log_undo_action:\n\t/*\n\t * Keep reading and looping back until we have read the\n\t * last record for this transaction.\n\t */\n\terr = read_next_log_rec(log, lcb, &rec_lsn);\n\tif (err)\n\t\tgoto out;\n\n\tif (rec_lsn)\n\t\tgoto undo_action_next;\n\n\tlcb_put(lcb);\n\tlcb = NULL;\n\ncommit_undo:\n\tfree_rsttbl_idx(trtbl, log->transaction_id);\n\n\tlog->transaction_id = 0;\n\n\tgoto transaction_table_next;\n\nundo_action_done:\n\n\tntfs_update_mftmirr(sbi, 0);\n\n\tsbi->flags &= ~NTFS_FLAGS_NEED_REPLAY;\n\nend_reply:\n\n\terr = 0;\n\tif (is_ro)\n\t\tgoto out;\n\n\trh = kzalloc(log->page_size, GFP_NOFS);\n\tif (!rh) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trh->rhdr.sign = NTFS_RSTR_SIGNATURE;\n\trh->rhdr.fix_off = cpu_to_le16(offsetof(struct RESTART_HDR, fixups));\n\tt16 = (log->page_size >> SECTOR_SHIFT) + 1;\n\trh->rhdr.fix_num = cpu_to_le16(t16);\n\trh->sys_page_size = cpu_to_le32(log->page_size);\n\trh->page_size = cpu_to_le32(log->page_size);\n\n\tt16 = ALIGN(offsetof(struct RESTART_HDR, fixups) + sizeof(short) * t16,\n\t\t    8);\n\trh->ra_off = cpu_to_le16(t16);\n\trh->minor_ver = cpu_to_le16(1); // 0x1A:\n\trh->major_ver = cpu_to_le16(1); // 0x1C:\n\n\tra2 = Add2Ptr(rh, t16);\n\tmemcpy(ra2, ra, sizeof(struct RESTART_AREA));\n\n\tra2->client_idx[0] = 0;\n\tra2->client_idx[1] = LFS_NO_CLIENT_LE;\n\tra2->flags = cpu_to_le16(2);\n\n\tle32_add_cpu(&ra2->open_log_count, 1);\n\n\tntfs_fix_pre_write(&rh->rhdr, log->page_size);\n\n\terr = ntfs_sb_write_run(sbi, &ni->file.run, 0, rh, log->page_size, 0);\n\tif (!err)\n\t\terr = ntfs_sb_write_run(sbi, &log->ni->file.run, log->page_size,\n\t\t\t\t\trh, log->page_size, 0);\n\n\tkfree(rh);\n\tif (err)\n\t\tgoto out;\n\nout:\n\tkfree(rst);\n\tif (lcb)\n\t\tlcb_put(lcb);\n\n\t/*\n\t * Scan the Open Attribute Table to close all of\n\t * the open attributes.\n\t */\n\toe = NULL;\n\twhile ((oe = enum_rstbl(oatbl, oe))) {\n\t\trno = ino_get(&oe->ref);\n\n\t\tif (oe->is_attr_name == 1) {\n\t\t\tkfree(oe->ptr);\n\t\t\toe->ptr = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (oe->is_attr_name)\n\t\t\tcontinue;\n\n\t\toa = oe->ptr;\n\t\tif (!oa)\n\t\t\tcontinue;\n\n\t\trun_close(&oa->run0);\n\t\tkfree(oa->attr);\n\t\tif (oa->ni)\n\t\t\tiput(&oa->ni->vfs_inode);\n\t\tkfree(oa);\n\t}\n\n\tkfree(trtbl);\n\tkfree(oatbl);\n\tkfree(dptbl);\n\tkfree(attr_names);\n\tkfree(rst_info.r_page);\n\n\tkfree(ra);\n\tkfree(log->one_page_buf);\n\n\tif (err)\n\t\tsbi->flags |= NTFS_FLAGS_NEED_REPLAY;\n\n\tif (err == -EROFS)\n\t\terr = 0;\n\telse if (log->set_dirty)\n\t\tntfs_set_state(sbi, NTFS_DIRTY_ERROR);\n\n\tkfree(log);\n\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3456,
            "cve_id": "CVE-2022-1976",
            "code_snippet": "static bool io_match_task_safe(struct io_kiocb *head, struct task_struct *task,\n\t\t\t       bool cancel_all)\n{\n\tif (task && head->task != task)\n\t\treturn false;\n\treturn cancel_all;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3457,
            "cve_id": "CVE-2022-1976",
            "code_snippet": "static void __io_req_task_work_add(struct io_kiocb *req,\n\t\t\t\t   struct io_uring_task *tctx,\n\t\t\t\t   struct io_wq_work_list *list)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_wq_work_node *node;\n\tunsigned long flags;\n\tbool running;\n\n\tio_drop_inflight_file(req);\n\n\tspin_lock_irqsave(&tctx->task_lock, flags);\n\twq_list_add_tail(&req->io_task_work.node, list);\n\trunning = tctx->task_running;\n\tif (!running)\n\t\ttctx->task_running = true;\n\tspin_unlock_irqrestore(&tctx->task_lock, flags);\n\n\t/* task_work already pending, we're done */\n\tif (running)\n\t\treturn;\n\n\tif (ctx->flags & IORING_SETUP_TASKRUN_FLAG)\n\t\tatomic_or(IORING_SQ_TASKRUN, &ctx->rings->sq_flags);\n\n\tif (likely(!task_work_add(req->task, &tctx->task_work, ctx->notify_method)))\n\t\treturn;\n\n\tspin_lock_irqsave(&tctx->task_lock, flags);\n\ttctx->task_running = false;\n\tnode = wq_list_merge(&tctx->prio_task_list, &tctx->task_list);\n\tspin_unlock_irqrestore(&tctx->task_lock, flags);\n\n\twhile (node) {\n\t\treq = container_of(node, struct io_kiocb, io_task_work.node);\n\t\tnode = node->next;\n\t\tif (llist_add(&req->io_task_work.fallback_node,\n\t\t\t      &req->ctx->fallback_llist))\n\t\t\tschedule_delayed_work(&req->ctx->fallback_work, 1);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3458,
            "cve_id": "CVE-2022-1976",
            "code_snippet": "static int io_poll_check_events(struct io_kiocb *req, bool *locked)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint v, ret;\n\n\t/* req->task == current here, checking PF_EXITING is safe */\n\tif (unlikely(req->task->flags & PF_EXITING))\n\t\treturn -ECANCELED;\n\n\tdo {\n\t\tv = atomic_read(&req->poll_refs);\n\n\t\t/* tw handler should be the owner, and so have some references */\n\t\tif (WARN_ON_ONCE(!(v & IO_POLL_REF_MASK)))\n\t\t\treturn 0;\n\t\tif (v & IO_POLL_CANCEL_FLAG)\n\t\t\treturn -ECANCELED;\n\n\t\tif (!req->cqe.res) {\n\t\t\tstruct poll_table_struct pt = { ._key = req->apoll_events };\n\t\t\tunsigned flags = locked ? 0 : IO_URING_F_UNLOCKED;\n\n\t\t\tif (unlikely(!io_assign_file(req, flags)))\n\t\t\t\treturn -EBADF;\n\t\t\treq->cqe.res = vfs_poll(req->file, &pt) & req->apoll_events;\n\t\t}\n\n\t\tif ((unlikely(!req->cqe.res)))\n\t\t\tcontinue;\n\t\tif (req->apoll_events & EPOLLONESHOT)\n\t\t\treturn 0;\n\n\t\t/* multishot, just fill a CQE and proceed */\n\t\tif (!(req->flags & REQ_F_APOLL_MULTISHOT)) {\n\t\t\t__poll_t mask = mangle_poll(req->cqe.res &\n\t\t\t\t\t\t    req->apoll_events);\n\t\t\tbool filled;\n\n\t\t\tspin_lock(&ctx->completion_lock);\n\t\t\tfilled = io_fill_cqe_aux(ctx, req->cqe.user_data,\n\t\t\t\t\t\t mask, IORING_CQE_F_MORE);\n\t\t\tio_commit_cqring(ctx);\n\t\t\tspin_unlock(&ctx->completion_lock);\n\t\t\tif (filled) {\n\t\t\t\tio_cqring_ev_posted(ctx);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\treturn -ECANCELED;\n\t\t}\n\n\t\tio_tw_lock(req->ctx, locked);\n\t\tif (unlikely(req->task->flags & PF_EXITING))\n\t\t\treturn -EFAULT;\n\t\tret = io_issue_sqe(req,\n\t\t\t\t   IO_URING_F_NONBLOCK|IO_URING_F_COMPLETE_DEFER);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t/*\n\t\t * Release all references, retry if someone tried to restart\n\t\t * task_work while we were executing it.\n\t\t */\n\t} while (atomic_sub_return(v & IO_POLL_REF_MASK, &req->poll_refs));\n\n\treturn 1;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3477,
            "cve_id": "CVE-2022-20158",
            "code_snippet": "static int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t       struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct packet_sock *po;\n\tstruct sockaddr_ll *sll;\n\tunion tpacket_uhdr h;\n\tu8 *skb_head = skb->data;\n\tint skb_len = skb->len;\n\tunsigned int snaplen, res;\n\tunsigned long status = TP_STATUS_USER;\n\tunsigned short macoff, hdrlen;\n\tunsigned int netoff;\n\tstruct sk_buff *copy_skb = NULL;\n\tstruct timespec64 ts;\n\t__u32 ts_status;\n\tbool is_drop_n_account = false;\n\tunsigned int slot_id = 0;\n\tbool do_vnet = false;\n\n\t/* struct tpacket{2,3}_hdr is aligned to a multiple of TPACKET_ALIGNMENT.\n\t * We may add members to them until current aligned size without forcing\n\t * userspace to call getsockopt(..., PACKET_HDRLEN, ...).\n\t */\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h2)) != 32);\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h3)) != 48);\n\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto drop;\n\n\tsk = pt->af_packet_priv;\n\tpo = pkt_sk(sk);\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto drop;\n\n\tif (dev_has_header(dev)) {\n\t\tif (sk->sk_type != SOCK_DGRAM)\n\t\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\telse if (skb->pkt_type == PACKET_OUTGOING) {\n\t\t\t/* Special case: outgoing packets have ll header at head */\n\t\t\tskb_pull(skb, skb_network_offset(skb));\n\t\t}\n\t}\n\n\tsnaplen = skb->len;\n\n\tres = run_filter(skb, sk, snaplen);\n\tif (!res)\n\t\tgoto drop_n_restore;\n\n\t/* If we are flooded, just give up */\n\tif (__packet_rcv_has_room(po, skb) == ROOM_NONE) {\n\t\tatomic_inc(&po->tp_drops);\n\t\tgoto drop_n_restore;\n\t}\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tstatus |= TP_STATUS_CSUMNOTREADY;\n\telse if (skb->pkt_type != PACKET_OUTGOING &&\n\t\t (skb->ip_summed == CHECKSUM_COMPLETE ||\n\t\t  skb_csum_unnecessary(skb)))\n\t\tstatus |= TP_STATUS_CSUM_VALID;\n\n\tif (snaplen > res)\n\t\tsnaplen = res;\n\n\tif (sk->sk_type == SOCK_DGRAM) {\n\t\tmacoff = netoff = TPACKET_ALIGN(po->tp_hdrlen) + 16 +\n\t\t\t\t  po->tp_reserve;\n\t} else {\n\t\tunsigned int maclen = skb_network_offset(skb);\n\t\tnetoff = TPACKET_ALIGN(po->tp_hdrlen +\n\t\t\t\t       (maclen < 16 ? 16 : maclen)) +\n\t\t\t\t       po->tp_reserve;\n\t\tif (po->has_vnet_hdr) {\n\t\t\tnetoff += sizeof(struct virtio_net_hdr);\n\t\t\tdo_vnet = true;\n\t\t}\n\t\tmacoff = netoff - maclen;\n\t}\n\tif (netoff > USHRT_MAX) {\n\t\tatomic_inc(&po->tp_drops);\n\t\tgoto drop_n_restore;\n\t}\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tif (macoff + snaplen > po->rx_ring.frame_size) {\n\t\t\tif (po->copy_thresh &&\n\t\t\t    atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf) {\n\t\t\t\tif (skb_shared(skb)) {\n\t\t\t\t\tcopy_skb = skb_clone(skb, GFP_ATOMIC);\n\t\t\t\t} else {\n\t\t\t\t\tcopy_skb = skb_get(skb);\n\t\t\t\t\tskb_head = skb->data;\n\t\t\t\t}\n\t\t\t\tif (copy_skb)\n\t\t\t\t\tskb_set_owner_r(copy_skb, sk);\n\t\t\t}\n\t\t\tsnaplen = po->rx_ring.frame_size - macoff;\n\t\t\tif ((int)snaplen < 0) {\n\t\t\t\tsnaplen = 0;\n\t\t\t\tdo_vnet = false;\n\t\t\t}\n\t\t}\n\t} else if (unlikely(macoff + snaplen >\n\t\t\t    GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len)) {\n\t\tu32 nval;\n\n\t\tnval = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len - macoff;\n\t\tpr_err_once(\"tpacket_rcv: packet too big, clamped from %u to %u. macoff=%u\\n\",\n\t\t\t    snaplen, nval, macoff);\n\t\tsnaplen = nval;\n\t\tif (unlikely((int)snaplen < 0)) {\n\t\t\tsnaplen = 0;\n\t\t\tmacoff = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len;\n\t\t\tdo_vnet = false;\n\t\t}\n\t}\n\tspin_lock(&sk->sk_receive_queue.lock);\n\th.raw = packet_current_rx_frame(po, skb,\n\t\t\t\t\tTP_STATUS_KERNEL, (macoff+snaplen));\n\tif (!h.raw)\n\t\tgoto drop_n_account;\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tslot_id = po->rx_ring.head;\n\t\tif (test_bit(slot_id, po->rx_ring.rx_owner_map))\n\t\t\tgoto drop_n_account;\n\t\t__set_bit(slot_id, po->rx_ring.rx_owner_map);\n\t}\n\n\tif (do_vnet &&\n\t    virtio_net_hdr_from_skb(skb, h.raw + macoff -\n\t\t\t\t    sizeof(struct virtio_net_hdr),\n\t\t\t\t    vio_le(), true, 0)) {\n\t\tif (po->tp_version == TPACKET_V3)\n\t\t\tprb_clear_blk_fill_status(&po->rx_ring);\n\t\tgoto drop_n_account;\n\t}\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tpacket_increment_rx_head(po, &po->rx_ring);\n\t/*\n\t * LOSING will be reported till you read the stats,\n\t * because it's COR - Clear On Read.\n\t * Anyways, moving it for V1/V2 only as V3 doesn't need this\n\t * at packet level.\n\t */\n\t\tif (atomic_read(&po->tp_drops))\n\t\t\tstatus |= TP_STATUS_LOSING;\n\t}\n\n\tpo->stats.stats1.tp_packets++;\n\tif (copy_skb) {\n\t\tstatus |= TP_STATUS_COPY;\n\t\t__skb_queue_tail(&sk->sk_receive_queue, copy_skb);\n\t}\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\n\tskb_copy_bits(skb, 0, h.raw + macoff, snaplen);\n\n\t/* Always timestamp; prefer an existing software timestamp taken\n\t * closer to the time of capture.\n\t */\n\tts_status = tpacket_get_timestamp(skb, &ts,\n\t\t\t\t\t  po->tp_tstamp | SOF_TIMESTAMPING_SOFTWARE);\n\tif (!ts_status)\n\t\tktime_get_real_ts64(&ts);\n\n\tstatus |= ts_status;\n\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\th.h1->tp_len = skb->len;\n\t\th.h1->tp_snaplen = snaplen;\n\t\th.h1->tp_mac = macoff;\n\t\th.h1->tp_net = netoff;\n\t\th.h1->tp_sec = ts.tv_sec;\n\t\th.h1->tp_usec = ts.tv_nsec / NSEC_PER_USEC;\n\t\thdrlen = sizeof(*h.h1);\n\t\tbreak;\n\tcase TPACKET_V2:\n\t\th.h2->tp_len = skb->len;\n\t\th.h2->tp_snaplen = snaplen;\n\t\th.h2->tp_mac = macoff;\n\t\th.h2->tp_net = netoff;\n\t\th.h2->tp_sec = ts.tv_sec;\n\t\th.h2->tp_nsec = ts.tv_nsec;\n\t\tif (skb_vlan_tag_present(skb)) {\n\t\t\th.h2->tp_vlan_tci = skb_vlan_tag_get(skb);\n\t\t\th.h2->tp_vlan_tpid = ntohs(skb->vlan_proto);\n\t\t\tstatus |= TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;\n\t\t} else {\n\t\t\th.h2->tp_vlan_tci = 0;\n\t\t\th.h2->tp_vlan_tpid = 0;\n\t\t}\n\t\tmemset(h.h2->tp_padding, 0, sizeof(h.h2->tp_padding));\n\t\thdrlen = sizeof(*h.h2);\n\t\tbreak;\n\tcase TPACKET_V3:\n\t\t/* tp_nxt_offset,vlan are already populated above.\n\t\t * So DONT clear those fields here\n\t\t */\n\t\th.h3->tp_status |= status;\n\t\th.h3->tp_len = skb->len;\n\t\th.h3->tp_snaplen = snaplen;\n\t\th.h3->tp_mac = macoff;\n\t\th.h3->tp_net = netoff;\n\t\th.h3->tp_sec  = ts.tv_sec;\n\t\th.h3->tp_nsec = ts.tv_nsec;\n\t\tmemset(h.h3->tp_padding, 0, sizeof(h.h3->tp_padding));\n\t\thdrlen = sizeof(*h.h3);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tsll = h.raw + TPACKET_ALIGN(hdrlen);\n\tsll->sll_halen = dev_parse_header(skb, sll->sll_addr);\n\tsll->sll_family = AF_PACKET;\n\tsll->sll_hatype = dev->type;\n\tsll->sll_protocol = skb->protocol;\n\tsll->sll_pkttype = skb->pkt_type;\n\tif (unlikely(po->origdev))\n\t\tsll->sll_ifindex = orig_dev->ifindex;\n\telse\n\t\tsll->sll_ifindex = dev->ifindex;\n\n\tsmp_mb();\n\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE == 1\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tu8 *start, *end;\n\n\t\tend = (u8 *) PAGE_ALIGN((unsigned long) h.raw +\n\t\t\t\t\tmacoff + snaplen);\n\n\t\tfor (start = h.raw; start < end; start += PAGE_SIZE)\n\t\t\tflush_dcache_page(pgv_to_page(start));\n\t}\n\tsmp_wmb();\n#endif\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tspin_lock(&sk->sk_receive_queue.lock);\n\t\t__packet_set_status(po, h.raw, status);\n\t\t__clear_bit(slot_id, po->rx_ring.rx_owner_map);\n\t\tspin_unlock(&sk->sk_receive_queue.lock);\n\t\tsk->sk_data_ready(sk);\n\t} else if (po->tp_version == TPACKET_V3) {\n\t\tprb_clear_blk_fill_status(&po->rx_ring);\n\t}\n\ndrop_n_restore:\n\tif (skb_head != skb->data && skb_shared(skb)) {\n\t\tskb->data = skb_head;\n\t\tskb->len = skb_len;\n\t}\ndrop:\n\tif (!is_drop_n_account)\n\t\tconsume_skb(skb);\n\telse\n\t\tkfree_skb(skb);\n\treturn 0;\n\ndrop_n_account:\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\tatomic_inc(&po->tp_drops);\n\tis_drop_n_account = true;\n\n\tsk->sk_data_ready(sk);\n\tkfree_skb(copy_skb);\n\tgoto drop_n_restore;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3485,
            "cve_id": "CVE-2022-20409",
            "code_snippet": "static void io_worker_exit(struct io_worker *worker)\n{\n\tstruct io_wqe *wqe = worker->wqe;\n\tstruct io_wqe_acct *acct = io_wqe_get_acct(worker);\n\n\t/*\n\t * If we're not at zero, someone else is holding a brief reference\n\t * to the worker. Wait for that to go away.\n\t */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tif (!refcount_dec_and_test(&worker->ref))\n\t\tschedule();\n\t__set_current_state(TASK_RUNNING);\n\n\tpreempt_disable();\n\tcurrent->flags &= ~PF_IO_WORKER;\n\tif (worker->flags & IO_WORKER_F_RUNNING)\n\t\tatomic_dec(&acct->nr_running);\n\tif (!(worker->flags & IO_WORKER_F_BOUND))\n\t\tatomic_dec(&wqe->wq->user->processes);\n\tworker->flags = 0;\n\tpreempt_enable();\n\n\traw_spin_lock_irq(&wqe->lock);\n\thlist_nulls_del_rcu(&worker->nulls_node);\n\tlist_del_rcu(&worker->all_list);\n\tacct->nr_workers--;\n\traw_spin_unlock_irq(&wqe->lock);\n\n\tkfree_rcu(worker, rcu);\n\tif (refcount_dec_and_test(&wqe->wq->refs))\n\t\tcomplete(&wqe->wq->done);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3486,
            "cve_id": "CVE-2022-20409",
            "code_snippet": "static void __io_worker_idle(struct io_wqe *wqe, struct io_worker *worker)\n\t__must_hold(wqe->lock)\n{\n\tif (!(worker->flags & IO_WORKER_F_FREE)) {\n\t\tworker->flags |= IO_WORKER_F_FREE;\n\t\thlist_nulls_add_head_rcu(&worker->nulls_node, &wqe->free_list);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3487,
            "cve_id": "CVE-2022-20409",
            "code_snippet": "static bool io_match_task(struct io_kiocb *head,\n\t\t\t  struct task_struct *task,\n\t\t\t  struct files_struct *files)\n{\n\tstruct io_kiocb *req;\n\n\tif (task && head->task != task) {\n\t\t/* in terms of cancelation, always match if req task is dead */\n\t\tif (head->task->flags & PF_EXITING)\n\t\t\treturn true;\n\t\treturn false;\n\t}\n\tif (!files)\n\t\treturn true;\n\n\tio_for_each_link(req, head) {\n\t\tif (!(req->flags & REQ_F_WORK_INITIALIZED))\n\t\t\tcontinue;\n\t\tif (req->file && req->file->f_op == &io_uring_fops)\n\t\t\treturn true;\n\t\tif (req->work.identity->files == files)\n\t\t\treturn true;\n\t}\n\treturn false;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3488,
            "cve_id": "CVE-2022-20409",
            "code_snippet": "static void io_prep_async_work(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_req_init_async(req);\n\n\tif (req->flags & REQ_F_FORCE_ASYNC)\n\t\treq->work.flags |= IO_WQ_WORK_CONCURRENT;\n\n\tif (req->flags & REQ_F_ISREG) {\n\t\tif (def->hash_reg_file || (ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tio_wq_hash_work(&req->work, file_inode(req->file));\n\t} else {\n\t\tif (def->unbound_nonreg_file)\n\t\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3489,
            "cve_id": "CVE-2022-20409",
            "code_snippet": "static int io_uring_show_cred(int id, void *p, void *data)\n{\n\tstruct io_identity *iod = p;\n\tconst struct cred *cred = iod->creds;\n\tstruct seq_file *m = data;\n\tstruct user_namespace *uns = seq_user_ns(m);\n\tstruct group_info *gi;\n\tkernel_cap_t cap;\n\tunsigned __capi;\n\tint g;\n\n\tseq_printf(m, \"%5d\\n\", id);\n\tseq_put_decimal_ull(m, \"\\tUid:\\t\", from_kuid_munged(uns, cred->uid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->euid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->suid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->fsuid));\n\tseq_put_decimal_ull(m, \"\\n\\tGid:\\t\", from_kgid_munged(uns, cred->gid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->egid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->sgid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->fsgid));\n\tseq_puts(m, \"\\n\\tGroups:\\t\");\n\tgi = cred->group_info;\n\tfor (g = 0; g < gi->ngroups; g++) {\n\t\tseq_put_decimal_ull(m, g ? \" \" : \"\",\n\t\t\t\t\tfrom_kgid_munged(uns, gi->gid[g]));\n\t}\n\tseq_puts(m, \"\\n\\tCapEff:\\t\");\n\tcap = cred->cap_effective;\n\tCAP_FOR_EACH_U32(__capi)\n\t\tseq_put_hex_ll(m, NULL, cap.cap[CAP_LAST_U32 - __capi], 8);\n\tseq_putc(m, '\\n');\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3490,
            "cve_id": "CVE-2022-20409",
            "code_snippet": "static inline void io_req_init_async(struct io_kiocb *req)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\n\tif (req->flags & REQ_F_WORK_INITIALIZED)\n\t\treturn;\n\n\t__io_req_init_async(req);\n\n\t/* Grab a ref if this isn't our static identity */\n\treq->work.identity = tctx->identity;\n\tif (tctx->identity != &tctx->__identity)\n\t\trefcount_inc(&req->work.identity->count);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3519,
            "cve_id": "CVE-2022-20566",
            "code_snippet": "static struct l2cap_chan *l2cap_get_chan_by_dcid(struct l2cap_conn *conn,\n\t\t\t\t\t\t u16 cid)\n{\n\tstruct l2cap_chan *c;\n\n\tmutex_lock(&conn->chan_lock);\n\tc = __l2cap_get_chan_by_dcid(conn, cid);\n\tif (c)\n\t\tl2cap_chan_lock(c);\n\tmutex_unlock(&conn->chan_lock);\n\n\treturn c;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3520,
            "cve_id": "CVE-2022-20566",
            "code_snippet": "static void l2cap_move_continue(struct l2cap_conn *conn, u16 icid, u16 result)\n{\n\tstruct l2cap_chan *chan;\n\tstruct hci_chan *hchan = NULL;\n\n\tchan = l2cap_get_chan_by_scid(conn, icid);\n\tif (!chan) {\n\t\tl2cap_send_move_chan_cfm_icid(conn, icid);\n\t\treturn;\n\t}\n\n\t__clear_chan_timer(chan);\n\tif (result == L2CAP_MR_PEND)\n\t\t__set_chan_timer(chan, L2CAP_MOVE_ERTX_TIMEOUT);\n\n\tswitch (chan->move_state) {\n\tcase L2CAP_MOVE_WAIT_LOGICAL_COMP:\n\t\t/* Move confirm will be sent when logical link\n\t\t * is complete.\n\t\t */\n\t\tchan->move_state = L2CAP_MOVE_WAIT_LOGICAL_CFM;\n\t\tbreak;\n\tcase L2CAP_MOVE_WAIT_RSP_SUCCESS:\n\t\tif (result == L2CAP_MR_PEND) {\n\t\t\tbreak;\n\t\t} else if (test_bit(CONN_LOCAL_BUSY,\n\t\t\t\t    &chan->conn_state)) {\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_LOCAL_BUSY;\n\t\t} else {\n\t\t\t/* Logical link is up or moving to BR/EDR,\n\t\t\t * proceed with move\n\t\t\t */\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_CONFIRM_RSP;\n\t\t\tl2cap_send_move_chan_cfm(chan, L2CAP_MC_CONFIRMED);\n\t\t}\n\t\tbreak;\n\tcase L2CAP_MOVE_WAIT_RSP:\n\t\t/* Moving to AMP */\n\t\tif (result == L2CAP_MR_SUCCESS) {\n\t\t\t/* Remote is ready, send confirm immediately\n\t\t\t * after logical link is ready\n\t\t\t */\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_LOGICAL_CFM;\n\t\t} else {\n\t\t\t/* Both logical link and move success\n\t\t\t * are required to confirm\n\t\t\t */\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_LOGICAL_COMP;\n\t\t}\n\n\t\t/* Placeholder - get hci_chan for logical link */\n\t\tif (!hchan) {\n\t\t\t/* Logical link not available */\n\t\t\tl2cap_send_move_chan_cfm(chan, L2CAP_MC_UNCONFIRMED);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* If the logical link is not yet connected, do not\n\t\t * send confirmation.\n\t\t */\n\t\tif (hchan->state != BT_CONNECTED)\n\t\t\tbreak;\n\n\t\t/* Logical link is already ready to go */\n\n\t\tchan->hs_hcon = hchan->conn;\n\t\tchan->hs_hcon->l2cap_data = chan->conn;\n\n\t\tif (result == L2CAP_MR_SUCCESS) {\n\t\t\t/* Can confirm now */\n\t\t\tl2cap_send_move_chan_cfm(chan, L2CAP_MC_CONFIRMED);\n\t\t} else {\n\t\t\t/* Now only need move success\n\t\t\t * to confirm\n\t\t\t */\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_RSP_SUCCESS;\n\t\t}\n\n\t\tl2cap_logical_cfm(chan, hchan, L2CAP_MR_SUCCESS);\n\t\tbreak;\n\tdefault:\n\t\t/* Any other amp move state means the move failed. */\n\t\tchan->move_id = chan->local_amp_id;\n\t\tl2cap_move_done(chan);\n\t\tl2cap_send_move_chan_cfm(chan, L2CAP_MC_UNCONFIRMED);\n\t}\n\n\tl2cap_chan_unlock(chan);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3521,
            "cve_id": "CVE-2022-20566",
            "code_snippet": "static inline int l2cap_move_channel_confirm_rsp(struct l2cap_conn *conn,\n\t\t\t\t\t\t struct l2cap_cmd_hdr *cmd,\n\t\t\t\t\t\t u16 cmd_len, void *data)\n{\n\tstruct l2cap_move_chan_cfm_rsp *rsp = data;\n\tstruct l2cap_chan *chan;\n\tu16 icid;\n\n\tif (cmd_len != sizeof(*rsp))\n\t\treturn -EPROTO;\n\n\ticid = le16_to_cpu(rsp->icid);\n\n\tBT_DBG(\"icid 0x%4.4x\", icid);\n\n\tchan = l2cap_get_chan_by_scid(conn, icid);\n\tif (!chan)\n\t\treturn 0;\n\n\t__clear_chan_timer(chan);\n\n\tif (chan->move_state == L2CAP_MOVE_WAIT_CONFIRM_RSP) {\n\t\tchan->local_amp_id = chan->move_id;\n\n\t\tif (chan->local_amp_id == AMP_ID_BREDR && chan->hs_hchan)\n\t\t\t__release_logical_link(chan);\n\n\t\tl2cap_move_done(chan);\n\t}\n\n\tl2cap_chan_unlock(chan);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3522,
            "cve_id": "CVE-2022-20566",
            "code_snippet": "static struct l2cap_chan *l2cap_get_chan_by_scid(struct l2cap_conn *conn,\n\t\t\t\t\t\t u16 cid)\n{\n\tstruct l2cap_chan *c;\n\n\tmutex_lock(&conn->chan_lock);\n\tc = __l2cap_get_chan_by_scid(conn, cid);\n\tif (c)\n\t\tl2cap_chan_lock(c);\n\tmutex_unlock(&conn->chan_lock);\n\n\treturn c;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3523,
            "cve_id": "CVE-2022-20566",
            "code_snippet": "static inline int l2cap_config_rsp(struct l2cap_conn *conn,\n\t\t\t\t   struct l2cap_cmd_hdr *cmd, u16 cmd_len,\n\t\t\t\t   u8 *data)\n{\n\tstruct l2cap_conf_rsp *rsp = (struct l2cap_conf_rsp *)data;\n\tu16 scid, flags, result;\n\tstruct l2cap_chan *chan;\n\tint len = cmd_len - sizeof(*rsp);\n\tint err = 0;\n\n\tif (cmd_len < sizeof(*rsp))\n\t\treturn -EPROTO;\n\n\tscid   = __le16_to_cpu(rsp->scid);\n\tflags  = __le16_to_cpu(rsp->flags);\n\tresult = __le16_to_cpu(rsp->result);\n\n\tBT_DBG(\"scid 0x%4.4x flags 0x%2.2x result 0x%2.2x len %d\", scid, flags,\n\t       result, len);\n\n\tchan = l2cap_get_chan_by_scid(conn, scid);\n\tif (!chan)\n\t\treturn 0;\n\n\tswitch (result) {\n\tcase L2CAP_CONF_SUCCESS:\n\t\tl2cap_conf_rfc_get(chan, rsp->data, len);\n\t\tclear_bit(CONF_REM_CONF_PEND, &chan->conf_state);\n\t\tbreak;\n\n\tcase L2CAP_CONF_PENDING:\n\t\tset_bit(CONF_REM_CONF_PEND, &chan->conf_state);\n\n\t\tif (test_bit(CONF_LOC_CONF_PEND, &chan->conf_state)) {\n\t\t\tchar buf[64];\n\n\t\t\tlen = l2cap_parse_conf_rsp(chan, rsp->data, len,\n\t\t\t\t\t\t   buf, sizeof(buf), &result);\n\t\t\tif (len < 0) {\n\t\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\tif (!chan->hs_hcon) {\n\t\t\t\tl2cap_send_efs_conf_rsp(chan, buf, cmd->ident,\n\t\t\t\t\t\t\t0);\n\t\t\t} else {\n\t\t\t\tif (l2cap_check_efs(chan)) {\n\t\t\t\t\tamp_create_logical_link(chan);\n\t\t\t\t\tchan->ident = cmd->ident;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tgoto done;\n\n\tcase L2CAP_CONF_UNKNOWN:\n\tcase L2CAP_CONF_UNACCEPT:\n\t\tif (chan->num_conf_rsp <= L2CAP_CONF_MAX_CONF_RSP) {\n\t\t\tchar req[64];\n\n\t\t\tif (len > sizeof(req) - sizeof(struct l2cap_conf_req)) {\n\t\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\t/* throw out any old stored conf requests */\n\t\t\tresult = L2CAP_CONF_SUCCESS;\n\t\t\tlen = l2cap_parse_conf_rsp(chan, rsp->data, len,\n\t\t\t\t\t\t   req, sizeof(req), &result);\n\t\t\tif (len < 0) {\n\t\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\tl2cap_send_cmd(conn, l2cap_get_ident(conn),\n\t\t\t\t       L2CAP_CONF_REQ, len, req);\n\t\t\tchan->num_conf_req++;\n\t\t\tif (result != L2CAP_CONF_SUCCESS)\n\t\t\t\tgoto done;\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\n\n\tdefault:\n\t\tl2cap_chan_set_err(chan, ECONNRESET);\n\n\t\t__set_chan_timer(chan, L2CAP_DISC_REJ_TIMEOUT);\n\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\tgoto done;\n\t}\n\n\tif (flags & L2CAP_CONF_FLAG_CONTINUATION)\n\t\tgoto done;\n\n\tset_bit(CONF_INPUT_DONE, &chan->conf_state);\n\n\tif (test_bit(CONF_OUTPUT_DONE, &chan->conf_state)) {\n\t\tset_default_fcs(chan);\n\n\t\tif (chan->mode == L2CAP_MODE_ERTM ||\n\t\t    chan->mode == L2CAP_MODE_STREAMING)\n\t\t\terr = l2cap_ertm_init(chan);\n\n\t\tif (err < 0)\n\t\t\tl2cap_send_disconn_req(chan, -err);\n\t\telse\n\t\t\tl2cap_chan_ready(chan);\n\t}\n\ndone:\n\tl2cap_chan_unlock(chan);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3550,
            "cve_id": "CVE-2022-22942",
            "code_snippet": "int vmw_fence_event_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *file_priv)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct drm_vmw_fence_event_arg *arg =\n\t\t(struct drm_vmw_fence_event_arg *) data;\n\tstruct vmw_fence_obj *fence = NULL;\n\tstruct vmw_fpriv *vmw_fp = vmw_fpriv(file_priv);\n\tstruct ttm_object_file *tfile = vmw_fp->tfile;\n\tstruct drm_vmw_fence_rep __user *user_fence_rep =\n\t\t(struct drm_vmw_fence_rep __user *)(unsigned long)\n\t\targ->fence_rep;\n\tuint32_t handle;\n\tint ret;\n\n\t/*\n\t * Look up an existing fence object,\n\t * and if user-space wants a new reference,\n\t * add one.\n\t */\n\tif (arg->handle) {\n\t\tstruct ttm_base_object *base =\n\t\t\tvmw_fence_obj_lookup(tfile, arg->handle);\n\n\t\tif (IS_ERR(base))\n\t\t\treturn PTR_ERR(base);\n\n\t\tfence = &(container_of(base, struct vmw_user_fence,\n\t\t\t\t       base)->fence);\n\t\t(void) vmw_fence_obj_reference(fence);\n\n\t\tif (user_fence_rep != NULL) {\n\t\t\tret = ttm_ref_object_add(vmw_fp->tfile, base,\n\t\t\t\t\t\t NULL, false);\n\t\t\tif (unlikely(ret != 0)) {\n\t\t\t\tDRM_ERROR(\"Failed to reference a fence \"\n\t\t\t\t\t  \"object.\\n\");\n\t\t\t\tgoto out_no_ref_obj;\n\t\t\t}\n\t\t\thandle = base->handle;\n\t\t}\n\t\tttm_base_object_unref(&base);\n\t}\n\n\t/*\n\t * Create a new fence object.\n\t */\n\tif (!fence) {\n\t\tret = vmw_execbuf_fence_commands(file_priv, dev_priv,\n\t\t\t\t\t\t &fence,\n\t\t\t\t\t\t (user_fence_rep) ?\n\t\t\t\t\t\t &handle : NULL);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Fence event failed to create fence.\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tBUG_ON(fence == NULL);\n\n\tret = vmw_event_fence_action_create(file_priv, fence,\n\t\t\t\t\t    arg->flags,\n\t\t\t\t\t    arg->user_data,\n\t\t\t\t\t    true);\n\tif (unlikely(ret != 0)) {\n\t\tif (ret != -ERESTARTSYS)\n\t\t\tDRM_ERROR(\"Failed to attach event to fence.\\n\");\n\t\tgoto out_no_create;\n\t}\n\n\tvmw_execbuf_copy_fence_user(dev_priv, vmw_fp, 0, user_fence_rep, fence,\n\t\t\t\t    handle, -1, NULL);\n\tvmw_fence_obj_unreference(&fence);\n\treturn 0;\nout_no_create:\n\tif (user_fence_rep != NULL)\n\t\tttm_ref_object_base_unref(tfile, handle);\nout_no_ref_obj:\n\tvmw_fence_obj_unreference(&fence);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3563,
            "cve_id": "CVE-2022-2318",
            "code_snippet": "void rose_start_idletimer(struct sock *sk)\n{\n\tstruct rose_sock *rose = rose_sk(sk);\n\n\tdel_timer(&rose->idletimer);\n\n\tif (rose->idle > 0) {\n\t\trose->idletimer.function = rose_idletimer_expiry;\n\t\trose->idletimer.expires  = jiffies + rose->idle;\n\n\t\tadd_timer(&rose->idletimer);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3564,
            "cve_id": "CVE-2022-2318",
            "code_snippet": "void rose_start_t1timer(struct sock *sk)\n{\n\tstruct rose_sock *rose = rose_sk(sk);\n\n\tdel_timer(&rose->timer);\n\n\trose->timer.function = rose_timer_expiry;\n\trose->timer.expires  = jiffies + rose->t1;\n\n\tadd_timer(&rose->timer);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3565,
            "cve_id": "CVE-2022-2318",
            "code_snippet": "void rose_start_hbtimer(struct sock *sk)\n{\n\tstruct rose_sock *rose = rose_sk(sk);\n\n\tdel_timer(&rose->timer);\n\n\trose->timer.function = rose_timer_expiry;\n\trose->timer.expires  = jiffies + rose->hb;\n\n\tadd_timer(&rose->timer);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3566,
            "cve_id": "CVE-2022-2318",
            "code_snippet": "static void rose_heartbeat_expiry(struct timer_list *t)\n{\n\tstruct sock *sk = from_timer(sk, t, sk_timer);\n\tstruct rose_sock *rose = rose_sk(sk);\n\n\tbh_lock_sock(sk);\n\tswitch (rose->state) {\n\tcase ROSE_STATE_0:\n\t\t/* Magic here: If we listen() and a new link dies before it\n\t\t   is accepted() it isn't 'dead' so doesn't get removed. */\n\t\tif (sock_flag(sk, SOCK_DESTROY) ||\n\t\t    (sk->sk_state == TCP_LISTEN && sock_flag(sk, SOCK_DEAD))) {\n\t\t\tbh_unlock_sock(sk);\n\t\t\trose_destroy_socket(sk);\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tcase ROSE_STATE_3:\n\t\t/*\n\t\t * Check for the state of the receive buffer.\n\t\t */\n\t\tif (atomic_read(&sk->sk_rmem_alloc) < (sk->sk_rcvbuf / 2) &&\n\t\t    (rose->condition & ROSE_COND_OWN_RX_BUSY)) {\n\t\t\trose->condition &= ~ROSE_COND_OWN_RX_BUSY;\n\t\t\trose->condition &= ~ROSE_COND_ACK_PENDING;\n\t\t\trose->vl         = rose->vr;\n\t\t\trose_write_internal(sk, ROSE_RR);\n\t\t\trose_stop_timer(sk);\t/* HB */\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\n\trose_start_heartbeat(sk);\n\tbh_unlock_sock(sk);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3567,
            "cve_id": "CVE-2022-2318",
            "code_snippet": "void rose_start_heartbeat(struct sock *sk)\n{\n\tdel_timer(&sk->sk_timer);\n\n\tsk->sk_timer.function = rose_heartbeat_expiry;\n\tsk->sk_timer.expires  = jiffies + 5 * HZ;\n\n\tadd_timer(&sk->sk_timer);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3605,
            "cve_id": "CVE-2022-24122",
            "code_snippet": "void put_ucounts(struct ucounts *ucounts)\n{\n\tunsigned long flags;\n\n\tif (atomic_dec_and_lock_irqsave(&ucounts->count, &ucounts_lock, flags)) {\n\t\thlist_del_init(&ucounts->node);\n\t\tspin_unlock_irqrestore(&ucounts_lock, flags);\n\t\tkfree(ucounts);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3618,
            "cve_id": "CVE-2022-2602",
            "code_snippet": "void unix_gc(void)\n{\n\tstruct unix_sock *u;\n\tstruct unix_sock *next;\n\tstruct sk_buff_head hitlist;\n\tstruct list_head cursor;\n\tLIST_HEAD(not_cycle_list);\n\n\tspin_lock(&unix_gc_lock);\n\n\t/* Avoid a recursive GC. */\n\tif (gc_in_progress)\n\t\tgoto out;\n\n\t/* Paired with READ_ONCE() in wait_for_unix_gc(). */\n\tWRITE_ONCE(gc_in_progress, true);\n\n\t/* First, select candidates for garbage collection.  Only\n\t * in-flight sockets are considered, and from those only ones\n\t * which don't have any external reference.\n\t *\n\t * Holding unix_gc_lock will protect these candidates from\n\t * being detached, and hence from gaining an external\n\t * reference.  Since there are no possible receivers, all\n\t * buffers currently on the candidates' queues stay there\n\t * during the garbage collection.\n\t *\n\t * We also know that no new candidate can be added onto the\n\t * receive queues.  Other, non candidate sockets _can_ be\n\t * added to queue, so we must make sure only to touch\n\t * candidates.\n\t */\n\tlist_for_each_entry_safe(u, next, &gc_inflight_list, link) {\n\t\tlong total_refs;\n\t\tlong inflight_refs;\n\n\t\ttotal_refs = file_count(u->sk.sk_socket->file);\n\t\tinflight_refs = atomic_long_read(&u->inflight);\n\n\t\tBUG_ON(inflight_refs < 1);\n\t\tBUG_ON(total_refs < inflight_refs);\n\t\tif (total_refs == inflight_refs) {\n\t\t\tlist_move_tail(&u->link, &gc_candidates);\n\t\t\t__set_bit(UNIX_GC_CANDIDATE, &u->gc_flags);\n\t\t\t__set_bit(UNIX_GC_MAYBE_CYCLE, &u->gc_flags);\n\t\t}\n\t}\n\n\t/* Now remove all internal in-flight reference to children of\n\t * the candidates.\n\t */\n\tlist_for_each_entry(u, &gc_candidates, link)\n\t\tscan_children(&u->sk, dec_inflight, NULL);\n\n\t/* Restore the references for children of all candidates,\n\t * which have remaining references.  Do this recursively, so\n\t * only those remain, which form cyclic references.\n\t *\n\t * Use a \"cursor\" link, to make the list traversal safe, even\n\t * though elements might be moved about.\n\t */\n\tlist_add(&cursor, &gc_candidates);\n\twhile (cursor.next != &gc_candidates) {\n\t\tu = list_entry(cursor.next, struct unix_sock, link);\n\n\t\t/* Move cursor to after the current position. */\n\t\tlist_move(&cursor, &u->link);\n\n\t\tif (atomic_long_read(&u->inflight) > 0) {\n\t\t\tlist_move_tail(&u->link, &not_cycle_list);\n\t\t\t__clear_bit(UNIX_GC_MAYBE_CYCLE, &u->gc_flags);\n\t\t\tscan_children(&u->sk, inc_inflight_move_tail, NULL);\n\t\t}\n\t}\n\tlist_del(&cursor);\n\n\t/* Now gc_candidates contains only garbage.  Restore original\n\t * inflight counters for these as well, and remove the skbuffs\n\t * which are creating the cycle(s).\n\t */\n\tskb_queue_head_init(&hitlist);\n\tlist_for_each_entry(u, &gc_candidates, link)\n\t\tscan_children(&u->sk, inc_inflight, &hitlist);\n\n\t/* not_cycle_list contains those sockets which do not make up a\n\t * cycle.  Restore these to the inflight list.\n\t */\n\twhile (!list_empty(&not_cycle_list)) {\n\t\tu = list_entry(not_cycle_list.next, struct unix_sock, link);\n\t\t__clear_bit(UNIX_GC_CANDIDATE, &u->gc_flags);\n\t\tlist_move_tail(&u->link, &gc_inflight_list);\n\t}\n\n\tspin_unlock(&unix_gc_lock);\n\n\t/* Here we are. Hitlist is filled. Die. */\n\t__skb_queue_purge(&hitlist);\n\n\tspin_lock(&unix_gc_lock);\n\n\t/* All candidates should have been detached by now. */\n\tBUG_ON(!list_empty(&gc_candidates));\n\n\t/* Paired with READ_ONCE() in wait_for_unix_gc(). */\n\tWRITE_ONCE(gc_in_progress, false);\n\n\twake_up(&unix_gc_wait);\n\n out:\n\tspin_unlock(&unix_gc_lock);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3642,
            "cve_id": "CVE-2022-28893",
            "code_snippet": "static int xs_local_send_request(struct rpc_rqst *req)\n{\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\tstruct sock_xprt *transport =\n\t\t\t\tcontainer_of(xprt, struct sock_xprt, xprt);\n\tstruct xdr_buf *xdr = &req->rq_snd_buf;\n\trpc_fraghdr rm = xs_stream_record_marker(xdr);\n\tunsigned int msglen = rm ? req->rq_slen + sizeof(rm) : req->rq_slen;\n\tstruct msghdr msg = {\n\t\t.msg_flags\t= XS_SENDMSG_FLAGS,\n\t};\n\tbool vm_wait;\n\tunsigned int sent;\n\tint status;\n\n\t/* Close the stream if the previous transmission was incomplete */\n\tif (xs_send_request_was_aborted(transport, req)) {\n\t\txs_close(xprt);\n\t\treturn -ENOTCONN;\n\t}\n\n\txs_pktdump(\"packet data:\",\n\t\t\treq->rq_svec->iov_base, req->rq_svec->iov_len);\n\n\tvm_wait = sk_stream_is_writeable(transport->inet) ? true : false;\n\n\treq->rq_xtime = ktime_get();\n\tstatus = xprt_sock_sendmsg(transport->sock, &msg, xdr,\n\t\t\t\t   transport->xmit.offset, rm, &sent);\n\tdprintk(\"RPC:       %s(%u) = %d\\n\",\n\t\t\t__func__, xdr->len - transport->xmit.offset, status);\n\n\tif (likely(sent > 0) || status == 0) {\n\t\ttransport->xmit.offset += sent;\n\t\treq->rq_bytes_sent = transport->xmit.offset;\n\t\tif (likely(req->rq_bytes_sent >= msglen)) {\n\t\t\treq->rq_xmit_bytes_sent += transport->xmit.offset;\n\t\t\ttransport->xmit.offset = 0;\n\t\t\treturn 0;\n\t\t}\n\t\tstatus = -EAGAIN;\n\t\tvm_wait = false;\n\t}\n\n\tswitch (status) {\n\tcase -EAGAIN:\n\t\tstatus = xs_stream_nospace(req, vm_wait);\n\t\tbreak;\n\tdefault:\n\t\tdprintk(\"RPC:       sendmsg returned unrecognized error %d\\n\",\n\t\t\t-status);\n\t\tfallthrough;\n\tcase -EPIPE:\n\t\txs_close(xprt);\n\t\tstatus = -ENOTCONN;\n\t}\n\n\treturn status;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3649,
            "cve_id": "CVE-2022-2938",
            "code_snippet": "static void cgroup_pressure_release(struct kernfs_open_file *of)\n{\n\tstruct cgroup_file_ctx *ctx = of->priv;\n\n\tpsi_trigger_replace(&ctx->psi.trigger, NULL);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3650,
            "cve_id": "CVE-2022-2938",
            "code_snippet": "static int psi_fop_release(struct inode *inode, struct file *file)\n{\n\tstruct seq_file *seq = file->private_data;\n\n\tpsi_trigger_replace(&seq->private, NULL);\n\treturn single_release(inode, file);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3661,
            "cve_id": "CVE-2022-2977",
            "code_snippet": "static int tpm_add_char_device(struct tpm_chip *chip)\n{\n\tint rc;\n\n\trc = cdev_device_add(&chip->cdev, &chip->dev);\n\tif (rc) {\n\t\tdev_err(&chip->dev,\n\t\t\t\"unable to cdev_device_add() %s, major %d, minor %d, err=%d\\n\",\n\t\t\tdev_name(&chip->dev), MAJOR(chip->dev.devt),\n\t\t\tMINOR(chip->dev.devt), rc);\n\t\treturn rc;\n\t}\n\n\tif (chip->flags & TPM_CHIP_FLAG_TPM2 && !tpm_is_firmware_upgrade(chip)) {\n\t\trc = cdev_device_add(&chip->cdevs, &chip->devs);\n\t\tif (rc) {\n\t\t\tdev_err(&chip->devs,\n\t\t\t\t\"unable to cdev_device_add() %s, major %d, minor %d, err=%d\\n\",\n\t\t\t\tdev_name(&chip->devs), MAJOR(chip->devs.devt),\n\t\t\t\tMINOR(chip->devs.devt), rc);\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\t/* Make the chip available. */\n\tmutex_lock(&idr_lock);\n\tidr_replace(&dev_nums_idr, chip, chip->dev_num);\n\tmutex_unlock(&idr_lock);\n\n\treturn rc;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3699,
            "cve_id": "CVE-2022-3176",
            "code_snippet": "static int io_poll_wake(struct wait_queue_entry *wait, unsigned mode, int sync,\n\t\t\tvoid *key)\n{\n\tstruct io_kiocb *req = wait->private;\n\tstruct io_poll_iocb *poll = container_of(wait, struct io_poll_iocb,\n\t\t\t\t\t\t wait);\n\t__poll_t mask = key_to_poll(key);\n\n\t/* for instances that support it check for an event match first */\n\tif (mask && !(mask & poll->events))\n\t\treturn 0;\n\n\tif (io_poll_get_ownership(req)) {\n\t\t/* optional, saves extra locking for removal in tw handler */\n\t\tif (mask && poll->events & EPOLLONESHOT) {\n\t\t\tlist_del_init(&poll->wait.entry);\n\t\t\tpoll->head = NULL;\n\t\t}\n\t\t__io_poll_execute(req, mask);\n\t}\n\treturn 1;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3703,
            "cve_id": "CVE-2022-32250",
            "code_snippet": "static struct nft_expr *nft_expr_init(const struct nft_ctx *ctx,\n\t\t\t\t      const struct nlattr *nla)\n{\n\tstruct nft_expr_info expr_info;\n\tstruct nft_expr *expr;\n\tstruct module *owner;\n\tint err;\n\n\terr = nf_tables_expr_parse(ctx, nla, &expr_info);\n\tif (err < 0)\n\t\tgoto err1;\n\n\terr = -ENOMEM;\n\texpr = kzalloc(expr_info.ops->size, GFP_KERNEL_ACCOUNT);\n\tif (expr == NULL)\n\t\tgoto err2;\n\n\terr = nf_tables_newexpr(ctx, &expr_info, expr);\n\tif (err < 0)\n\t\tgoto err3;\n\n\treturn expr;\nerr3:\n\tkfree(expr);\nerr2:\n\towner = expr_info.ops->type->owner;\n\tif (expr_info.ops->type->release_ops)\n\t\texpr_info.ops->type->release_ops(expr_info.ops);\n\n\tmodule_put(owner);\nerr1:\n\treturn ERR_PTR(err);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3730,
            "cve_id": "CVE-2022-3424",
            "code_snippet": "int gru_set_context_option(unsigned long arg)\n{\n\tstruct gru_thread_state *gts;\n\tstruct gru_set_context_option_req req;\n\tint ret = 0;\n\n\tSTAT(set_context_option);\n\tif (copy_from_user(&req, (void __user *)arg, sizeof(req)))\n\t\treturn -EFAULT;\n\tgru_dbg(grudev, \"op %d, gseg 0x%lx, value1 0x%lx\\n\", req.op, req.gseg, req.val1);\n\n\tgts = gru_find_lock_gts(req.gseg);\n\tif (!gts) {\n\t\tgts = gru_alloc_locked_gts(req.gseg);\n\t\tif (IS_ERR(gts))\n\t\t\treturn PTR_ERR(gts);\n\t}\n\n\tswitch (req.op) {\n\tcase sco_blade_chiplet:\n\t\t/* Select blade/chiplet for GRU context */\n\t\tif (req.val0 < -1 || req.val0 >= GRU_CHIPLETS_PER_HUB ||\n\t\t    req.val1 < -1 || req.val1 >= GRU_MAX_BLADES ||\n\t\t    (req.val1 >= 0 && !gru_base[req.val1])) {\n\t\t\tret = -EINVAL;\n\t\t} else {\n\t\t\tgts->ts_user_blade_id = req.val1;\n\t\t\tgts->ts_user_chiplet_id = req.val0;\n\t\t\tgru_check_context_placement(gts);\n\t\t}\n\t\tbreak;\n\tcase sco_gseg_owner:\n \t\t/* Register the current task as the GSEG owner */\n\t\tgts->ts_tgid_owner = current->tgid;\n\t\tbreak;\n\tcase sco_cch_req_slice:\n \t\t/* Set the CCH slice option */\n\t\tgts->ts_cch_req_slice = req.val1 & 3;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\tgru_unlock_gts(gts);\n\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3740,
            "cve_id": "CVE-2022-3523",
            "code_snippet": "unsigned long\nkvmppc_h_svm_page_out(struct kvm *kvm, unsigned long gpa,\n\t\t      unsigned long flags, unsigned long page_shift)\n{\n\tunsigned long gfn = gpa >> page_shift;\n\tunsigned long start, end;\n\tstruct vm_area_struct *vma;\n\tint srcu_idx;\n\tint ret;\n\n\tif (!(kvm->arch.secure_guest & KVMPPC_SECURE_INIT_START))\n\t\treturn H_UNSUPPORTED;\n\n\tif (page_shift != PAGE_SHIFT)\n\t\treturn H_P3;\n\n\tif (flags)\n\t\treturn H_P2;\n\n\tret = H_PARAMETER;\n\tsrcu_idx = srcu_read_lock(&kvm->srcu);\n\tmmap_read_lock(kvm->mm);\n\tstart = gfn_to_hva(kvm, gfn);\n\tif (kvm_is_error_hva(start))\n\t\tgoto out;\n\n\tend = start + (1UL << page_shift);\n\tvma = find_vma_intersection(kvm->mm, start, end);\n\tif (!vma || vma->vm_start > start || vma->vm_end < end)\n\t\tgoto out;\n\n\tif (!kvmppc_svm_page_out(vma, start, end, page_shift, kvm, gpa))\n\t\tret = H_SUCCESS;\nout:\n\tmmap_read_unlock(kvm->mm);\n\tsrcu_read_unlock(&kvm->srcu, srcu_idx);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3741,
            "cve_id": "CVE-2022-3523",
            "code_snippet": "static vm_fault_t kvmppc_uvmem_migrate_to_ram(struct vm_fault *vmf)\n{\n\tstruct kvmppc_uvmem_page_pvt *pvt = vmf->page->zone_device_data;\n\n\tif (kvmppc_svm_page_out(vmf->vma, vmf->address,\n\t\t\t\tvmf->address + PAGE_SIZE, PAGE_SHIFT,\n\t\t\t\tpvt->kvm, pvt->gpa))\n\t\treturn VM_FAULT_SIGBUS;\n\telse\n\t\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3742,
            "cve_id": "CVE-2022-3523",
            "code_snippet": "static int kvmppc_svm_page_in(struct vm_area_struct *vma,\n\t\tunsigned long start,\n\t\tunsigned long end, unsigned long gpa, struct kvm *kvm,\n\t\tunsigned long page_shift,\n\t\tbool pagein)\n{\n\tunsigned long src_pfn, dst_pfn = 0;\n\tstruct migrate_vma mig;\n\tstruct page *spage;\n\tunsigned long pfn;\n\tstruct page *dpage;\n\tint ret = 0;\n\n\tmemset(&mig, 0, sizeof(mig));\n\tmig.vma = vma;\n\tmig.start = start;\n\tmig.end = end;\n\tmig.src = &src_pfn;\n\tmig.dst = &dst_pfn;\n\tmig.flags = MIGRATE_VMA_SELECT_SYSTEM;\n\n\tret = migrate_vma_setup(&mig);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!(*mig.src & MIGRATE_PFN_MIGRATE)) {\n\t\tret = -1;\n\t\tgoto out_finalize;\n\t}\n\n\tdpage = kvmppc_uvmem_get_page(gpa, kvm);\n\tif (!dpage) {\n\t\tret = -1;\n\t\tgoto out_finalize;\n\t}\n\n\tif (pagein) {\n\t\tpfn = *mig.src >> MIGRATE_PFN_SHIFT;\n\t\tspage = migrate_pfn_to_page(*mig.src);\n\t\tif (spage) {\n\t\t\tret = uv_page_in(kvm->arch.lpid, pfn << page_shift,\n\t\t\t\t\tgpa, 0, page_shift);\n\t\t\tif (ret)\n\t\t\t\tgoto out_finalize;\n\t\t}\n\t}\n\n\t*mig.dst = migrate_pfn(page_to_pfn(dpage));\n\tmigrate_vma_pages(&mig);\nout_finalize:\n\tmigrate_vma_finalize(&mig);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3743,
            "cve_id": "CVE-2022-3523",
            "code_snippet": "static vm_fault_t svm_migrate_to_ram(struct vm_fault *vmf)\n{\n\tunsigned long addr = vmf->address;\n\tstruct vm_area_struct *vma;\n\tenum svm_work_list_ops op;\n\tstruct svm_range *parent;\n\tstruct svm_range *prange;\n\tstruct kfd_process *p;\n\tstruct mm_struct *mm;\n\tint r = 0;\n\n\tvma = vmf->vma;\n\tmm = vma->vm_mm;\n\n\tp = kfd_lookup_process_by_mm(vma->vm_mm);\n\tif (!p) {\n\t\tpr_debug(\"failed find process at fault address 0x%lx\\n\", addr);\n\t\treturn VM_FAULT_SIGBUS;\n\t}\n\tif (READ_ONCE(p->svms.faulting_task) == current) {\n\t\tpr_debug(\"skipping ram migration\\n\");\n\t\tkfd_unref_process(p);\n\t\treturn 0;\n\t}\n\taddr >>= PAGE_SHIFT;\n\tpr_debug(\"CPU page fault svms 0x%p address 0x%lx\\n\", &p->svms, addr);\n\n\tmutex_lock(&p->svms.lock);\n\n\tprange = svm_range_from_addr(&p->svms, addr, &parent);\n\tif (!prange) {\n\t\tpr_debug(\"cannot find svm range at 0x%lx\\n\", addr);\n\t\tr = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tmutex_lock(&parent->migrate_mutex);\n\tif (prange != parent)\n\t\tmutex_lock_nested(&prange->migrate_mutex, 1);\n\n\tif (!prange->actual_loc)\n\t\tgoto out_unlock_prange;\n\n\tsvm_range_lock(parent);\n\tif (prange != parent)\n\t\tmutex_lock_nested(&prange->lock, 1);\n\tr = svm_range_split_by_granularity(p, mm, addr, parent, prange);\n\tif (prange != parent)\n\t\tmutex_unlock(&prange->lock);\n\tsvm_range_unlock(parent);\n\tif (r) {\n\t\tpr_debug(\"failed %d to split range by granularity\\n\", r);\n\t\tgoto out_unlock_prange;\n\t}\n\n\tr = svm_migrate_vram_to_ram(prange, mm, KFD_MIGRATE_TRIGGER_PAGEFAULT_CPU);\n\tif (r)\n\t\tpr_debug(\"failed %d migrate 0x%p [0x%lx 0x%lx] to ram\\n\", r,\n\t\t\t prange, prange->start, prange->last);\n\n\t/* xnack on, update mapping on GPUs with ACCESS_IN_PLACE */\n\tif (p->xnack_enabled && parent == prange)\n\t\top = SVM_OP_UPDATE_RANGE_NOTIFIER_AND_MAP;\n\telse\n\t\top = SVM_OP_UPDATE_RANGE_NOTIFIER;\n\tsvm_range_add_list_work(&p->svms, parent, mm, op);\n\tschedule_deferred_list_work(&p->svms);\n\nout_unlock_prange:\n\tif (prange != parent)\n\t\tmutex_unlock(&prange->migrate_mutex);\n\tmutex_unlock(&parent->migrate_mutex);\nout:\n\tmutex_unlock(&p->svms.lock);\n\tkfd_unref_process(p);\n\n\tpr_debug(\"CPU fault svms 0x%p address 0x%lx done\\n\", &p->svms, addr);\n\n\treturn r ? VM_FAULT_SIGBUS : 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3744,
            "cve_id": "CVE-2022-3523",
            "code_snippet": "static long\nsvm_migrate_vma_to_vram(struct amdgpu_device *adev, struct svm_range *prange,\n\t\t\tstruct vm_area_struct *vma, uint64_t start,\n\t\t\tuint64_t end, uint32_t trigger)\n{\n\tstruct kfd_process *p = container_of(prange->svms, struct kfd_process, svms);\n\tuint64_t npages = (end - start) >> PAGE_SHIFT;\n\tstruct kfd_process_device *pdd;\n\tstruct dma_fence *mfence = NULL;\n\tstruct migrate_vma migrate;\n\tunsigned long cpages = 0;\n\tdma_addr_t *scratch;\n\tvoid *buf;\n\tint r = -ENOMEM;\n\n\tmemset(&migrate, 0, sizeof(migrate));\n\tmigrate.vma = vma;\n\tmigrate.start = start;\n\tmigrate.end = end;\n\tmigrate.flags = MIGRATE_VMA_SELECT_SYSTEM;\n\tmigrate.pgmap_owner = SVM_ADEV_PGMAP_OWNER(adev);\n\n\tbuf = kvcalloc(npages,\n\t\t       2 * sizeof(*migrate.src) + sizeof(uint64_t) + sizeof(dma_addr_t),\n\t\t       GFP_KERNEL);\n\tif (!buf)\n\t\tgoto out;\n\n\tmigrate.src = buf;\n\tmigrate.dst = migrate.src + npages;\n\tscratch = (dma_addr_t *)(migrate.dst + npages);\n\n\tkfd_smi_event_migration_start(adev->kfd.dev, p->lead_thread->pid,\n\t\t\t\t      start >> PAGE_SHIFT, end >> PAGE_SHIFT,\n\t\t\t\t      0, adev->kfd.dev->id, prange->prefetch_loc,\n\t\t\t\t      prange->preferred_loc, trigger);\n\n\tr = migrate_vma_setup(&migrate);\n\tif (r) {\n\t\tdev_err(adev->dev, \"%s: vma setup fail %d range [0x%lx 0x%lx]\\n\",\n\t\t\t__func__, r, prange->start, prange->last);\n\t\tgoto out_free;\n\t}\n\n\tcpages = migrate.cpages;\n\tif (!cpages) {\n\t\tpr_debug(\"failed collect migrate sys pages [0x%lx 0x%lx]\\n\",\n\t\t\t prange->start, prange->last);\n\t\tgoto out_free;\n\t}\n\tif (cpages != npages)\n\t\tpr_debug(\"partial migration, 0x%lx/0x%llx pages migrated\\n\",\n\t\t\t cpages, npages);\n\telse\n\t\tpr_debug(\"0x%lx pages migrated\\n\", cpages);\n\n\tr = svm_migrate_copy_to_vram(adev, prange, &migrate, &mfence, scratch);\n\tmigrate_vma_pages(&migrate);\n\n\tpr_debug(\"successful/cpages/npages 0x%lx/0x%lx/0x%lx\\n\",\n\t\tsvm_migrate_successful_pages(&migrate), cpages, migrate.npages);\n\n\tsvm_migrate_copy_done(adev, mfence);\n\tmigrate_vma_finalize(&migrate);\n\n\tkfd_smi_event_migration_end(adev->kfd.dev, p->lead_thread->pid,\n\t\t\t\t    start >> PAGE_SHIFT, end >> PAGE_SHIFT,\n\t\t\t\t    0, adev->kfd.dev->id, trigger);\n\n\tsvm_range_dma_unmap(adev->dev, scratch, 0, npages);\n\tsvm_range_free_dma_mappings(prange);\n\nout_free:\n\tkvfree(buf);\nout:\n\tif (!r && cpages) {\n\t\tpdd = svm_range_get_pdd_by_adev(prange, adev);\n\t\tif (pdd)\n\t\t\tWRITE_ONCE(pdd->page_in, pdd->page_in + cpages);\n\n\t\treturn cpages;\n\t}\n\treturn r;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3745,
            "cve_id": "CVE-2022-3523",
            "code_snippet": "static int\nsvm_migrate_vram_to_vram(struct svm_range *prange, uint32_t best_loc,\n\t\t\t struct mm_struct *mm, uint32_t trigger)\n{\n\tint r, retries = 3;\n\n\t/*\n\t * TODO: for both devices with PCIe large bar or on same xgmi hive, skip\n\t * system memory as migration bridge\n\t */\n\n\tpr_debug(\"from gpu 0x%x to gpu 0x%x\\n\", prange->actual_loc, best_loc);\n\n\tdo {\n\t\tr = svm_migrate_vram_to_ram(prange, mm, trigger);\n\t\tif (r)\n\t\t\treturn r;\n\t} while (prange->actual_loc && --retries);\n\n\tif (prange->actual_loc)\n\t\treturn -EDEADLK;\n\n\treturn svm_migrate_ram_to_vram(prange, best_loc, mm, trigger);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3746,
            "cve_id": "CVE-2022-3523",
            "code_snippet": "static void svm_range_evict_svm_bo_worker(struct work_struct *work)\n{\n\tstruct svm_range_bo *svm_bo;\n\tstruct mm_struct *mm;\n\tint r = 0;\n\n\tsvm_bo = container_of(work, struct svm_range_bo, eviction_work);\n\tif (!svm_bo_ref_unless_zero(svm_bo))\n\t\treturn; /* svm_bo was freed while eviction was pending */\n\n\tif (mmget_not_zero(svm_bo->eviction_fence->mm)) {\n\t\tmm = svm_bo->eviction_fence->mm;\n\t} else {\n\t\tsvm_range_bo_unref(svm_bo);\n\t\treturn;\n\t}\n\n\tmmap_read_lock(mm);\n\tspin_lock(&svm_bo->list_lock);\n\twhile (!list_empty(&svm_bo->range_list) && !r) {\n\t\tstruct svm_range *prange =\n\t\t\t\tlist_first_entry(&svm_bo->range_list,\n\t\t\t\t\t\tstruct svm_range, svm_bo_list);\n\t\tint retries = 3;\n\n\t\tlist_del_init(&prange->svm_bo_list);\n\t\tspin_unlock(&svm_bo->list_lock);\n\n\t\tpr_debug(\"svms 0x%p [0x%lx 0x%lx]\\n\", prange->svms,\n\t\t\t prange->start, prange->last);\n\n\t\tmutex_lock(&prange->migrate_mutex);\n\t\tdo {\n\t\t\tr = svm_migrate_vram_to_ram(prange, mm,\n\t\t\t\t\t\tKFD_MIGRATE_TRIGGER_TTM_EVICTION);\n\t\t} while (!r && prange->actual_loc && --retries);\n\n\t\tif (!r && prange->actual_loc)\n\t\t\tpr_info_once(\"Migration failed during eviction\");\n\n\t\tif (!prange->actual_loc) {\n\t\t\tmutex_lock(&prange->lock);\n\t\t\tprange->svm_bo = NULL;\n\t\t\tmutex_unlock(&prange->lock);\n\t\t}\n\t\tmutex_unlock(&prange->migrate_mutex);\n\n\t\tspin_lock(&svm_bo->list_lock);\n\t}\n\tspin_unlock(&svm_bo->list_lock);\n\tmmap_read_unlock(mm);\n\tmmput(mm);\n\n\tdma_fence_signal(&svm_bo->eviction_fence->base);\n\n\t/* This is the last reference to svm_bo, after svm_range_vram_node_free\n\t * has been called in svm_migrate_vram_to_ram\n\t */\n\tWARN_ONCE(!r && kref_read(&svm_bo->kref) != 1, \"This was not the last reference\\n\");\n\tsvm_range_bo_unref(svm_bo);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3747,
            "cve_id": "CVE-2022-3523",
            "code_snippet": "static int\nsvm_range_trigger_migration(struct mm_struct *mm, struct svm_range *prange,\n\t\t\t    bool *migrated)\n{\n\tuint32_t best_loc;\n\tint r = 0;\n\n\t*migrated = false;\n\tbest_loc = svm_range_best_prefetch_location(prange);\n\n\tif (best_loc == KFD_IOCTL_SVM_LOCATION_UNDEFINED ||\n\t    best_loc == prange->actual_loc)\n\t\treturn 0;\n\n\tif (!best_loc) {\n\t\tr = svm_migrate_vram_to_ram(prange, mm, KFD_MIGRATE_TRIGGER_PREFETCH);\n\t\t*migrated = !r;\n\t\treturn r;\n\t}\n\n\tr = svm_migrate_to_vram(prange, best_loc, mm, KFD_MIGRATE_TRIGGER_PREFETCH);\n\t*migrated = !r;\n\n\treturn r;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3763,
            "cve_id": "CVE-2022-3534",
            "code_snippet": "void btf_dump__free(struct btf_dump *d)\n{\n\tint i;\n\n\tif (IS_ERR_OR_NULL(d))\n\t\treturn;\n\n\tfree(d->type_states);\n\tif (d->cached_names) {\n\t\t/* any set cached name is owned by us and should be freed */\n\t\tfor (i = 0; i <= d->last_id; i++) {\n\t\t\tif (d->cached_names[i])\n\t\t\t\tfree((void *)d->cached_names[i]);\n\t\t}\n\t}\n\tfree(d->cached_names);\n\tfree(d->emit_queue);\n\tfree(d->decl_stack);\n\thashmap__free(d->type_names);\n\thashmap__free(d->ident_names);\n\n\tfree(d);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3813,
            "cve_id": "CVE-2022-38457",
            "code_snippet": "static int vmw_cmd_dx_bind_streamoutput(struct vmw_private *dev_priv,\n\t\t\t\t\tstruct vmw_sw_context *sw_context,\n\t\t\t\t\tSVGA3dCmdHeader *header)\n{\n\tstruct vmw_ctx_validation_info *ctx_node = sw_context->dx_ctx_node;\n\tstruct vmw_resource *res;\n\tstruct {\n\t\tSVGA3dCmdHeader header;\n\t\tSVGA3dCmdDXBindStreamOutput body;\n\t} *cmd = container_of(header, typeof(*cmd), header);\n\tint ret;\n\n\tif (!has_sm5_context(dev_priv))\n\t\treturn -EINVAL;\n\n\tif (!ctx_node) {\n\t\tDRM_ERROR(\"DX Context not set.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tres = vmw_dx_streamoutput_lookup(vmw_context_res_man(ctx_node->ctx),\n\t\t\t\t\t cmd->body.soid);\n\tif (IS_ERR(res)) {\n\t\tDRM_ERROR(\"Could not find streamoutput to bind.\\n\");\n\t\treturn PTR_ERR(res);\n\t}\n\n\tvmw_dx_streamoutput_set_size(res, cmd->body.sizeInBytes);\n\n\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,\n\t\t\t\t\t    VMW_RES_DIRTY_NONE);\n\tif (ret) {\n\t\tDRM_ERROR(\"Error creating resource validation node.\\n\");\n\t\treturn ret;\n\t}\n\n\treturn vmw_cmd_res_switch_backup(dev_priv, sw_context, res,\n\t\t\t\t\t &cmd->body.mobid,\n\t\t\t\t\t cmd->body.offsetInBytes);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3814,
            "cve_id": "CVE-2022-38457",
            "code_snippet": "static int vmw_translate_guest_ptr(struct vmw_private *dev_priv,\n\t\t\t\t   struct vmw_sw_context *sw_context,\n\t\t\t\t   SVGAGuestPtr *ptr,\n\t\t\t\t   struct vmw_buffer_object **vmw_bo_p)\n{\n\tstruct vmw_buffer_object *vmw_bo;\n\tuint32_t handle = ptr->gmrId;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tvmw_bo = vmw_user_bo_noref_lookup(sw_context->filp, handle);\n\tif (IS_ERR(vmw_bo)) {\n\t\tVMW_DEBUG_USER(\"Could not find or use GMR region.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo, false, false);\n\tttm_bo_put(&vmw_bo->base);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->location = ptr;\n\treloc->vbo = vmw_bo;\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3815,
            "cve_id": "CVE-2022-38457",
            "code_snippet": "static int vmw_execbuf_tie_context(struct vmw_private *dev_priv,\n\t\t\t\t   struct vmw_sw_context *sw_context,\n\t\t\t\t   uint32_t handle)\n{\n\tstruct vmw_resource *res;\n\tint ret;\n\tunsigned int size;\n\n\tif (handle == SVGA3D_INVALID_ID)\n\t\treturn 0;\n\n\tsize = vmw_execbuf_res_size(dev_priv, vmw_res_dx_context);\n\tret = vmw_validation_preload_res(sw_context->ctx, size);\n\tif (ret)\n\t\treturn ret;\n\n\tres = vmw_user_resource_noref_lookup_handle\n\t\t(dev_priv, sw_context->fp->tfile, handle,\n\t\t user_context_converter);\n\tif (IS_ERR(res)) {\n\t\tVMW_DEBUG_USER(\"Could not find or user DX context 0x%08x.\\n\",\n\t\t\t       (unsigned int) handle);\n\t\treturn PTR_ERR(res);\n\t}\n\n\tret = vmw_execbuf_res_noref_val_add(sw_context, res, VMW_RES_DIRTY_SET);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\tsw_context->dx_ctx_node = vmw_execbuf_info_from_res(sw_context, res);\n\tsw_context->man = vmw_context_res_man(res);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3816,
            "cve_id": "CVE-2022-38457",
            "code_snippet": "static int vmw_cmd_dx_bind_shader(struct vmw_private *dev_priv,\n\t\t\t\t  struct vmw_sw_context *sw_context,\n\t\t\t\t  SVGA3dCmdHeader *header)\n{\n\tstruct vmw_resource *ctx;\n\tstruct vmw_resource *res;\n\tVMW_DECLARE_CMD_VAR(*cmd, SVGA3dCmdDXBindShader) =\n\t\tcontainer_of(header, typeof(*cmd), header);\n\tint ret;\n\n\tif (cmd->body.cid != SVGA3D_INVALID_ID) {\n\t\tret = vmw_cmd_res_check(dev_priv, sw_context, vmw_res_context,\n\t\t\t\t\tVMW_RES_DIRTY_SET,\n\t\t\t\t\tuser_context_converter, &cmd->body.cid,\n\t\t\t\t\t&ctx);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} else {\n\t\tstruct vmw_ctx_validation_info *ctx_node =\n\t\t\tVMW_GET_CTX_NODE(sw_context);\n\n\t\tif (!ctx_node)\n\t\t\treturn -EINVAL;\n\n\t\tctx = ctx_node->ctx;\n\t}\n\n\tres = vmw_shader_lookup(vmw_context_res_man(ctx), cmd->body.shid, 0);\n\tif (IS_ERR(res)) {\n\t\tVMW_DEBUG_USER(\"Could not find shader to bind.\\n\");\n\t\treturn PTR_ERR(res);\n\t}\n\n\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,\n\t\t\t\t\t    VMW_RES_DIRTY_NONE);\n\tif (ret) {\n\t\tVMW_DEBUG_USER(\"Error creating resource validation node.\\n\");\n\t\treturn ret;\n\t}\n\n\treturn vmw_cmd_res_switch_backup(dev_priv, sw_context, res,\n\t\t\t\t\t &cmd->body.mobid,\n\t\t\t\t\t cmd->body.offsetInBytes);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3817,
            "cve_id": "CVE-2022-38457",
            "code_snippet": "static int vmw_cmd_dx_set_shader(struct vmw_private *dev_priv,\n\t\t\t\t struct vmw_sw_context *sw_context,\n\t\t\t\t SVGA3dCmdHeader *header)\n{\n\tVMW_DECLARE_CMD_VAR(*cmd, SVGA3dCmdDXSetShader);\n\tSVGA3dShaderType max_allowed = has_sm5_context(dev_priv) ?\n\t\tSVGA3D_SHADERTYPE_MAX : SVGA3D_SHADERTYPE_DX10_MAX;\n\tstruct vmw_resource *res = NULL;\n\tstruct vmw_ctx_validation_info *ctx_node = VMW_GET_CTX_NODE(sw_context);\n\tstruct vmw_ctx_bindinfo_shader binding;\n\tint ret = 0;\n\n\tif (!ctx_node)\n\t\treturn -EINVAL;\n\n\tcmd = container_of(header, typeof(*cmd), header);\n\n\tif (cmd->body.type >= max_allowed ||\n\t    cmd->body.type < SVGA3D_SHADERTYPE_MIN) {\n\t\tVMW_DEBUG_USER(\"Illegal shader type %u.\\n\",\n\t\t\t       (unsigned int) cmd->body.type);\n\t\treturn -EINVAL;\n\t}\n\n\tif (cmd->body.shaderId != SVGA3D_INVALID_ID) {\n\t\tres = vmw_shader_lookup(sw_context->man, cmd->body.shaderId, 0);\n\t\tif (IS_ERR(res)) {\n\t\t\tVMW_DEBUG_USER(\"Could not find shader for binding.\\n\");\n\t\t\treturn PTR_ERR(res);\n\t\t}\n\n\t\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,\n\t\t\t\t\t\t    VMW_RES_DIRTY_NONE);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tbinding.bi.ctx = ctx_node->ctx;\n\tbinding.bi.res = res;\n\tbinding.bi.bt = vmw_ctx_binding_dx_shader;\n\tbinding.shader_slot = cmd->body.type - SVGA3D_SHADERTYPE_MIN;\n\n\tvmw_binding_add(ctx_node->staged, &binding.bi, binding.shader_slot, 0);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3834,
            "cve_id": "CVE-2022-3977",
            "code_snippet": "static void __mctp_key_remove(struct mctp_sk_key *key, struct net *net,\n\t\t\t      unsigned long flags, unsigned long reason)\n__releases(&key->lock)\n__must_hold(&net->mctp.keys_lock)\n{\n\tstruct sk_buff *skb;\n\n\ttrace_mctp_key_release(key, reason);\n\tskb = key->reasm_head;\n\tkey->reasm_head = NULL;\n\tkey->reasm_dead = true;\n\tkey->valid = false;\n\tmctp_dev_release_key(key->dev, key);\n\tspin_unlock_irqrestore(&key->lock, flags);\n\n\thlist_del(&key->hlist);\n\thlist_del(&key->sklist);\n\n\t/* unref for the lists */\n\tmctp_key_unref(key);\n\n\tkfree_skb(skb);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3838,
            "cve_id": "CVE-2022-40133",
            "code_snippet": "static int vmw_cmd_dx_bind_streamoutput(struct vmw_private *dev_priv,\n\t\t\t\t\tstruct vmw_sw_context *sw_context,\n\t\t\t\t\tSVGA3dCmdHeader *header)\n{\n\tstruct vmw_ctx_validation_info *ctx_node = sw_context->dx_ctx_node;\n\tstruct vmw_resource *res;\n\tstruct {\n\t\tSVGA3dCmdHeader header;\n\t\tSVGA3dCmdDXBindStreamOutput body;\n\t} *cmd = container_of(header, typeof(*cmd), header);\n\tint ret;\n\n\tif (!has_sm5_context(dev_priv))\n\t\treturn -EINVAL;\n\n\tif (!ctx_node) {\n\t\tDRM_ERROR(\"DX Context not set.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tres = vmw_dx_streamoutput_lookup(vmw_context_res_man(ctx_node->ctx),\n\t\t\t\t\t cmd->body.soid);\n\tif (IS_ERR(res)) {\n\t\tDRM_ERROR(\"Could not find streamoutput to bind.\\n\");\n\t\treturn PTR_ERR(res);\n\t}\n\n\tvmw_dx_streamoutput_set_size(res, cmd->body.sizeInBytes);\n\n\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,\n\t\t\t\t\t    VMW_RES_DIRTY_NONE);\n\tif (ret) {\n\t\tDRM_ERROR(\"Error creating resource validation node.\\n\");\n\t\treturn ret;\n\t}\n\n\treturn vmw_cmd_res_switch_backup(dev_priv, sw_context, res,\n\t\t\t\t\t &cmd->body.mobid,\n\t\t\t\t\t cmd->body.offsetInBytes);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3839,
            "cve_id": "CVE-2022-40133",
            "code_snippet": "static int vmw_translate_guest_ptr(struct vmw_private *dev_priv,\n\t\t\t\t   struct vmw_sw_context *sw_context,\n\t\t\t\t   SVGAGuestPtr *ptr,\n\t\t\t\t   struct vmw_buffer_object **vmw_bo_p)\n{\n\tstruct vmw_buffer_object *vmw_bo;\n\tuint32_t handle = ptr->gmrId;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tvmw_bo = vmw_user_bo_noref_lookup(sw_context->filp, handle);\n\tif (IS_ERR(vmw_bo)) {\n\t\tVMW_DEBUG_USER(\"Could not find or use GMR region.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo, false, false);\n\tttm_bo_put(&vmw_bo->base);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->location = ptr;\n\treloc->vbo = vmw_bo;\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3840,
            "cve_id": "CVE-2022-40133",
            "code_snippet": "static int vmw_execbuf_tie_context(struct vmw_private *dev_priv,\n\t\t\t\t   struct vmw_sw_context *sw_context,\n\t\t\t\t   uint32_t handle)\n{\n\tstruct vmw_resource *res;\n\tint ret;\n\tunsigned int size;\n\n\tif (handle == SVGA3D_INVALID_ID)\n\t\treturn 0;\n\n\tsize = vmw_execbuf_res_size(dev_priv, vmw_res_dx_context);\n\tret = vmw_validation_preload_res(sw_context->ctx, size);\n\tif (ret)\n\t\treturn ret;\n\n\tres = vmw_user_resource_noref_lookup_handle\n\t\t(dev_priv, sw_context->fp->tfile, handle,\n\t\t user_context_converter);\n\tif (IS_ERR(res)) {\n\t\tVMW_DEBUG_USER(\"Could not find or user DX context 0x%08x.\\n\",\n\t\t\t       (unsigned int) handle);\n\t\treturn PTR_ERR(res);\n\t}\n\n\tret = vmw_execbuf_res_noref_val_add(sw_context, res, VMW_RES_DIRTY_SET);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\tsw_context->dx_ctx_node = vmw_execbuf_info_from_res(sw_context, res);\n\tsw_context->man = vmw_context_res_man(res);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3841,
            "cve_id": "CVE-2022-40133",
            "code_snippet": "static int vmw_cmd_dx_bind_shader(struct vmw_private *dev_priv,\n\t\t\t\t  struct vmw_sw_context *sw_context,\n\t\t\t\t  SVGA3dCmdHeader *header)\n{\n\tstruct vmw_resource *ctx;\n\tstruct vmw_resource *res;\n\tVMW_DECLARE_CMD_VAR(*cmd, SVGA3dCmdDXBindShader) =\n\t\tcontainer_of(header, typeof(*cmd), header);\n\tint ret;\n\n\tif (cmd->body.cid != SVGA3D_INVALID_ID) {\n\t\tret = vmw_cmd_res_check(dev_priv, sw_context, vmw_res_context,\n\t\t\t\t\tVMW_RES_DIRTY_SET,\n\t\t\t\t\tuser_context_converter, &cmd->body.cid,\n\t\t\t\t\t&ctx);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} else {\n\t\tstruct vmw_ctx_validation_info *ctx_node =\n\t\t\tVMW_GET_CTX_NODE(sw_context);\n\n\t\tif (!ctx_node)\n\t\t\treturn -EINVAL;\n\n\t\tctx = ctx_node->ctx;\n\t}\n\n\tres = vmw_shader_lookup(vmw_context_res_man(ctx), cmd->body.shid, 0);\n\tif (IS_ERR(res)) {\n\t\tVMW_DEBUG_USER(\"Could not find shader to bind.\\n\");\n\t\treturn PTR_ERR(res);\n\t}\n\n\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,\n\t\t\t\t\t    VMW_RES_DIRTY_NONE);\n\tif (ret) {\n\t\tVMW_DEBUG_USER(\"Error creating resource validation node.\\n\");\n\t\treturn ret;\n\t}\n\n\treturn vmw_cmd_res_switch_backup(dev_priv, sw_context, res,\n\t\t\t\t\t &cmd->body.mobid,\n\t\t\t\t\t cmd->body.offsetInBytes);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3842,
            "cve_id": "CVE-2022-40133",
            "code_snippet": "static int vmw_cmd_dx_set_shader(struct vmw_private *dev_priv,\n\t\t\t\t struct vmw_sw_context *sw_context,\n\t\t\t\t SVGA3dCmdHeader *header)\n{\n\tVMW_DECLARE_CMD_VAR(*cmd, SVGA3dCmdDXSetShader);\n\tSVGA3dShaderType max_allowed = has_sm5_context(dev_priv) ?\n\t\tSVGA3D_SHADERTYPE_MAX : SVGA3D_SHADERTYPE_DX10_MAX;\n\tstruct vmw_resource *res = NULL;\n\tstruct vmw_ctx_validation_info *ctx_node = VMW_GET_CTX_NODE(sw_context);\n\tstruct vmw_ctx_bindinfo_shader binding;\n\tint ret = 0;\n\n\tif (!ctx_node)\n\t\treturn -EINVAL;\n\n\tcmd = container_of(header, typeof(*cmd), header);\n\n\tif (cmd->body.type >= max_allowed ||\n\t    cmd->body.type < SVGA3D_SHADERTYPE_MIN) {\n\t\tVMW_DEBUG_USER(\"Illegal shader type %u.\\n\",\n\t\t\t       (unsigned int) cmd->body.type);\n\t\treturn -EINVAL;\n\t}\n\n\tif (cmd->body.shaderId != SVGA3D_INVALID_ID) {\n\t\tres = vmw_shader_lookup(sw_context->man, cmd->body.shaderId, 0);\n\t\tif (IS_ERR(res)) {\n\t\t\tVMW_DEBUG_USER(\"Could not find shader for binding.\\n\");\n\t\t\treturn PTR_ERR(res);\n\t\t}\n\n\t\tret = vmw_execbuf_res_noctx_val_add(sw_context, res,\n\t\t\t\t\t\t    VMW_RES_DIRTY_NONE);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tbinding.bi.ctx = ctx_node->ctx;\n\tbinding.bi.res = res;\n\tbinding.bi.bt = vmw_ctx_binding_dx_shader;\n\tbinding.shader_slot = cmd->body.type - SVGA3D_SHADERTYPE_MIN;\n\n\tvmw_binding_add(ctx_node->staged, &binding.bi, binding.shader_slot, 0);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3858,
            "cve_id": "CVE-2022-41218",
            "code_snippet": "static int dvb_demux_open(struct inode *inode, struct file *file)\n{\n\tstruct dvb_device *dvbdev = file->private_data;\n\tstruct dmxdev *dmxdev = dvbdev->priv;\n\tint i;\n\tstruct dmxdev_filter *dmxdevfilter;\n\n\tif (!dmxdev->filter)\n\t\treturn -EINVAL;\n\n\tif (mutex_lock_interruptible(&dmxdev->mutex))\n\t\treturn -ERESTARTSYS;\n\n\tfor (i = 0; i < dmxdev->filternum; i++)\n\t\tif (dmxdev->filter[i].state == DMXDEV_STATE_FREE)\n\t\t\tbreak;\n\n\tif (i == dmxdev->filternum) {\n\t\tmutex_unlock(&dmxdev->mutex);\n\t\treturn -EMFILE;\n\t}\n\n\tdmxdevfilter = &dmxdev->filter[i];\n\tmutex_init(&dmxdevfilter->mutex);\n\tfile->private_data = dmxdevfilter;\n\n#ifdef CONFIG_DVB_MMAP\n\tdmxdev->may_do_mmap = 1;\n#else\n\tdmxdev->may_do_mmap = 0;\n#endif\n\n\tdvb_ringbuffer_init(&dmxdevfilter->buffer, NULL, 8192);\n\tdvb_vb2_init(&dmxdevfilter->vb2_ctx, \"demux_filter\",\n\t\t     file->f_flags & O_NONBLOCK);\n\tdmxdevfilter->type = DMXDEV_TYPE_NONE;\n\tdvb_dmxdev_filter_state_set(dmxdevfilter, DMXDEV_STATE_ALLOCATED);\n\ttimer_setup(&dmxdevfilter->timer, dvb_dmxdev_filter_timeout, 0);\n\n\tdvbdev->users++;\n\n\tmutex_unlock(&dmxdev->mutex);\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3867,
            "cve_id": "CVE-2022-41849",
            "code_snippet": "static int ufx_ops_open(struct fb_info *info, int user)\n{\n\tstruct ufx_data *dev = info->par;\n\n\t/* fbcon aggressively connects to first framebuffer it finds,\n\t * preventing other clients (X) from working properly. Usually\n\t * not what the user wants. Fail by default with option to enable. */\n\tif (user == 0 && !console)\n\t\treturn -EBUSY;\n\n\t/* If the USB device is gone, we don't accept new opens */\n\tif (dev->virtualized)\n\t\treturn -ENODEV;\n\n\tdev->fb_count++;\n\n\tkref_get(&dev->kref);\n\n\tif (fb_defio && (info->fbdefio == NULL)) {\n\t\t/* enable defio at last moment if not disabled by client */\n\n\t\tstruct fb_deferred_io *fbdefio;\n\n\t\tfbdefio = kzalloc(sizeof(*fbdefio), GFP_KERNEL);\n\t\tif (fbdefio) {\n\t\t\tfbdefio->delay = UFX_DEFIO_WRITE_DELAY;\n\t\t\tfbdefio->deferred_io = ufx_dpy_deferred_io;\n\t\t}\n\n\t\tinfo->fbdefio = fbdefio;\n\t\tfb_deferred_io_init(info);\n\t}\n\n\tpr_debug(\"open /dev/fb%d user=%d fb_info=%p count=%d\",\n\t\tinfo->node, user, info, dev->fb_count);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3876,
            "cve_id": "CVE-2022-42703",
            "code_snippet": "int anon_vma_fork(struct vm_area_struct *vma, struct vm_area_struct *pvma)\n{\n\tstruct anon_vma_chain *avc;\n\tstruct anon_vma *anon_vma;\n\tint error;\n\n\t/* Don't bother if the parent process has no anon_vma here. */\n\tif (!pvma->anon_vma)\n\t\treturn 0;\n\n\t/* Drop inherited anon_vma, we'll reuse existing or allocate new. */\n\tvma->anon_vma = NULL;\n\n\t/*\n\t * First, attach the new VMA to the parent VMA's anon_vmas,\n\t * so rmap can find non-COWed pages in child processes.\n\t */\n\terror = anon_vma_clone(vma, pvma);\n\tif (error)\n\t\treturn error;\n\n\t/* An existing anon_vma has been reused, all done then. */\n\tif (vma->anon_vma)\n\t\treturn 0;\n\n\t/* Then add our own anon_vma. */\n\tanon_vma = anon_vma_alloc();\n\tif (!anon_vma)\n\t\tgoto out_error;\n\tavc = anon_vma_chain_alloc(GFP_KERNEL);\n\tif (!avc)\n\t\tgoto out_error_free_anon_vma;\n\n\t/*\n\t * The root anon_vma's rwsem is the lock actually used when we\n\t * lock any of the anon_vmas in this anon_vma tree.\n\t */\n\tanon_vma->root = pvma->anon_vma->root;\n\tanon_vma->parent = pvma->anon_vma;\n\t/*\n\t * With refcounts, an anon_vma can stay around longer than the\n\t * process it belongs to. The root anon_vma needs to be pinned until\n\t * this anon_vma is freed, because the lock lives in the root.\n\t */\n\tget_anon_vma(anon_vma->root);\n\t/* Mark this anon_vma as the one where our new (COWed) pages go. */\n\tvma->anon_vma = anon_vma;\n\tanon_vma_lock_write(anon_vma);\n\tanon_vma_chain_link(vma, avc, anon_vma);\n\tanon_vma->parent->degree++;\n\tanon_vma_unlock_write(anon_vma);\n\n\treturn 0;\n\n out_error_free_anon_vma:\n\tput_anon_vma(anon_vma);\n out_error:\n\tunlink_anon_vmas(vma);\n\treturn -ENOMEM;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3877,
            "cve_id": "CVE-2022-42703",
            "code_snippet": "int __anon_vma_prepare(struct vm_area_struct *vma)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct anon_vma *anon_vma, *allocated;\n\tstruct anon_vma_chain *avc;\n\n\tmight_sleep();\n\n\tavc = anon_vma_chain_alloc(GFP_KERNEL);\n\tif (!avc)\n\t\tgoto out_enomem;\n\n\tanon_vma = find_mergeable_anon_vma(vma);\n\tallocated = NULL;\n\tif (!anon_vma) {\n\t\tanon_vma = anon_vma_alloc();\n\t\tif (unlikely(!anon_vma))\n\t\t\tgoto out_enomem_free_avc;\n\t\tallocated = anon_vma;\n\t}\n\n\tanon_vma_lock_write(anon_vma);\n\t/* page_table_lock to protect against threads */\n\tspin_lock(&mm->page_table_lock);\n\tif (likely(!vma->anon_vma)) {\n\t\tvma->anon_vma = anon_vma;\n\t\tanon_vma_chain_link(vma, avc, anon_vma);\n\t\t/* vma reference or self-parent link for new root */\n\t\tanon_vma->degree++;\n\t\tallocated = NULL;\n\t\tavc = NULL;\n\t}\n\tspin_unlock(&mm->page_table_lock);\n\tanon_vma_unlock_write(anon_vma);\n\n\tif (unlikely(allocated))\n\t\tput_anon_vma(allocated);\n\tif (unlikely(avc))\n\t\tanon_vma_chain_free(avc);\n\n\treturn 0;\n\n out_enomem_free_avc:\n\tanon_vma_chain_free(avc);\n out_enomem:\n\treturn -ENOMEM;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3882,
            "cve_id": "CVE-2022-42720",
            "code_snippet": "static struct cfg80211_bss *\ncfg80211_inform_single_bss_data(struct wiphy *wiphy,\n\t\t\t\tstruct cfg80211_inform_bss *data,\n\t\t\t\tenum cfg80211_bss_frame_type ftype,\n\t\t\t\tconst u8 *bssid, u64 tsf, u16 capability,\n\t\t\t\tu16 beacon_interval, const u8 *ie, size_t ielen,\n\t\t\t\tstruct cfg80211_non_tx_bss *non_tx_data,\n\t\t\t\tgfp_t gfp)\n{\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\tstruct cfg80211_bss_ies *ies;\n\tstruct ieee80211_channel *channel;\n\tstruct cfg80211_internal_bss tmp = {}, *res;\n\tint bss_type;\n\tbool signal_valid;\n\tunsigned long ts;\n\n\tif (WARN_ON(!wiphy))\n\t\treturn NULL;\n\n\tif (WARN_ON(wiphy->signal_type == CFG80211_SIGNAL_TYPE_UNSPEC &&\n\t\t    (data->signal < 0 || data->signal > 100)))\n\t\treturn NULL;\n\n\tchannel = cfg80211_get_bss_channel(wiphy, ie, ielen, data->chan,\n\t\t\t\t\t   data->scan_width, ftype);\n\tif (!channel)\n\t\treturn NULL;\n\n\tmemcpy(tmp.pub.bssid, bssid, ETH_ALEN);\n\ttmp.pub.channel = channel;\n\ttmp.pub.scan_width = data->scan_width;\n\ttmp.pub.signal = data->signal;\n\ttmp.pub.beacon_interval = beacon_interval;\n\ttmp.pub.capability = capability;\n\ttmp.ts_boottime = data->boottime_ns;\n\ttmp.parent_tsf = data->parent_tsf;\n\tether_addr_copy(tmp.parent_bssid, data->parent_bssid);\n\n\tif (non_tx_data) {\n\t\ttmp.pub.transmitted_bss = non_tx_data->tx_bss;\n\t\tts = bss_from_pub(non_tx_data->tx_bss)->ts;\n\t\ttmp.pub.bssid_index = non_tx_data->bssid_index;\n\t\ttmp.pub.max_bssid_indicator = non_tx_data->max_bssid_indicator;\n\t} else {\n\t\tts = jiffies;\n\t}\n\n\t/*\n\t * If we do not know here whether the IEs are from a Beacon or Probe\n\t * Response frame, we need to pick one of the options and only use it\n\t * with the driver that does not provide the full Beacon/Probe Response\n\t * frame. Use Beacon frame pointer to avoid indicating that this should\n\t * override the IEs pointer should we have received an earlier\n\t * indication of Probe Response data.\n\t */\n\ties = kzalloc(sizeof(*ies) + ielen, gfp);\n\tif (!ies)\n\t\treturn NULL;\n\ties->len = ielen;\n\ties->tsf = tsf;\n\ties->from_beacon = false;\n\tmemcpy(ies->data, ie, ielen);\n\n\tswitch (ftype) {\n\tcase CFG80211_BSS_FTYPE_BEACON:\n\t\ties->from_beacon = true;\n\t\tfallthrough;\n\tcase CFG80211_BSS_FTYPE_UNKNOWN:\n\t\trcu_assign_pointer(tmp.pub.beacon_ies, ies);\n\t\tbreak;\n\tcase CFG80211_BSS_FTYPE_PRESP:\n\t\trcu_assign_pointer(tmp.pub.proberesp_ies, ies);\n\t\tbreak;\n\t}\n\trcu_assign_pointer(tmp.pub.ies, ies);\n\n\tsignal_valid = data->chan == channel;\n\tres = cfg80211_bss_update(wiphy_to_rdev(wiphy), &tmp, signal_valid, ts);\n\tif (!res)\n\t\treturn NULL;\n\n\tif (channel->band == NL80211_BAND_60GHZ) {\n\t\tbss_type = res->pub.capability & WLAN_CAPABILITY_DMG_TYPE_MASK;\n\t\tif (bss_type == WLAN_CAPABILITY_DMG_TYPE_AP ||\n\t\t    bss_type == WLAN_CAPABILITY_DMG_TYPE_PBSS)\n\t\t\tregulatory_hint_found_beacon(wiphy, channel, gfp);\n\t} else {\n\t\tif (res->pub.capability & WLAN_CAPABILITY_ESS)\n\t\t\tregulatory_hint_found_beacon(wiphy, channel, gfp);\n\t}\n\n\tif (non_tx_data) {\n\t\t/* this is a nontransmitting bss, we need to add it to\n\t\t * transmitting bss' list if it is not there\n\t\t */\n\t\tspin_lock_bh(&rdev->bss_lock);\n\t\tif (cfg80211_add_nontrans_list(non_tx_data->tx_bss,\n\t\t\t\t\t       &res->pub)) {\n\t\t\tif (__cfg80211_unlink_bss(rdev, res))\n\t\t\t\trdev->bss_generation++;\n\t\t}\n\t\tspin_unlock_bh(&rdev->bss_lock);\n\t}\n\n\ttrace_cfg80211_return_bss(&res->pub);\n\t/* cfg80211_bss_update gives us a referenced result */\n\treturn &res->pub;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3888,
            "cve_id": "CVE-2022-42896",
            "code_snippet": "static inline int l2cap_ecred_conn_req(struct l2cap_conn *conn,\n\t\t\t\t       struct l2cap_cmd_hdr *cmd, u16 cmd_len,\n\t\t\t\t       u8 *data)\n{\n\tstruct l2cap_ecred_conn_req *req = (void *) data;\n\tstruct {\n\t\tstruct l2cap_ecred_conn_rsp rsp;\n\t\t__le16 dcid[L2CAP_ECRED_MAX_CID];\n\t} __packed pdu;\n\tstruct l2cap_chan *chan, *pchan;\n\tu16 mtu, mps;\n\t__le16 psm;\n\tu8 result, len = 0;\n\tint i, num_scid;\n\tbool defer = false;\n\n\tif (!enable_ecred)\n\t\treturn -EINVAL;\n\n\tif (cmd_len < sizeof(*req) || (cmd_len - sizeof(*req)) % sizeof(u16)) {\n\t\tresult = L2CAP_CR_LE_INVALID_PARAMS;\n\t\tgoto response;\n\t}\n\n\tcmd_len -= sizeof(*req);\n\tnum_scid = cmd_len / sizeof(u16);\n\n\tif (num_scid > ARRAY_SIZE(pdu.dcid)) {\n\t\tresult = L2CAP_CR_LE_INVALID_PARAMS;\n\t\tgoto response;\n\t}\n\n\tmtu  = __le16_to_cpu(req->mtu);\n\tmps  = __le16_to_cpu(req->mps);\n\n\tif (mtu < L2CAP_ECRED_MIN_MTU || mps < L2CAP_ECRED_MIN_MPS) {\n\t\tresult = L2CAP_CR_LE_UNACCEPT_PARAMS;\n\t\tgoto response;\n\t}\n\n\tpsm  = req->psm;\n\n\tBT_DBG(\"psm 0x%2.2x mtu %u mps %u\", __le16_to_cpu(psm), mtu, mps);\n\n\tmemset(&pdu, 0, sizeof(pdu));\n\n\t/* Check if we have socket listening on psm */\n\tpchan = l2cap_global_chan_by_psm(BT_LISTEN, psm, &conn->hcon->src,\n\t\t\t\t\t &conn->hcon->dst, LE_LINK);\n\tif (!pchan) {\n\t\tresult = L2CAP_CR_LE_BAD_PSM;\n\t\tgoto response;\n\t}\n\n\tmutex_lock(&conn->chan_lock);\n\tl2cap_chan_lock(pchan);\n\n\tif (!smp_sufficient_security(conn->hcon, pchan->sec_level,\n\t\t\t\t     SMP_ALLOW_STK)) {\n\t\tresult = L2CAP_CR_LE_AUTHENTICATION;\n\t\tgoto unlock;\n\t}\n\n\tresult = L2CAP_CR_LE_SUCCESS;\n\n\tfor (i = 0; i < num_scid; i++) {\n\t\tu16 scid = __le16_to_cpu(req->scid[i]);\n\n\t\tBT_DBG(\"scid[%d] 0x%4.4x\", i, scid);\n\n\t\tpdu.dcid[i] = 0x0000;\n\t\tlen += sizeof(*pdu.dcid);\n\n\t\t/* Check for valid dynamic CID range */\n\t\tif (scid < L2CAP_CID_DYN_START || scid > L2CAP_CID_LE_DYN_END) {\n\t\t\tresult = L2CAP_CR_LE_INVALID_SCID;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Check if we already have channel with that dcid */\n\t\tif (__l2cap_get_chan_by_dcid(conn, scid)) {\n\t\t\tresult = L2CAP_CR_LE_SCID_IN_USE;\n\t\t\tcontinue;\n\t\t}\n\n\t\tchan = pchan->ops->new_connection(pchan);\n\t\tif (!chan) {\n\t\t\tresult = L2CAP_CR_LE_NO_MEM;\n\t\t\tcontinue;\n\t\t}\n\n\t\tbacpy(&chan->src, &conn->hcon->src);\n\t\tbacpy(&chan->dst, &conn->hcon->dst);\n\t\tchan->src_type = bdaddr_src_type(conn->hcon);\n\t\tchan->dst_type = bdaddr_dst_type(conn->hcon);\n\t\tchan->psm  = psm;\n\t\tchan->dcid = scid;\n\t\tchan->omtu = mtu;\n\t\tchan->remote_mps = mps;\n\n\t\t__l2cap_chan_add(conn, chan);\n\n\t\tl2cap_ecred_init(chan, __le16_to_cpu(req->credits));\n\n\t\t/* Init response */\n\t\tif (!pdu.rsp.credits) {\n\t\t\tpdu.rsp.mtu = cpu_to_le16(chan->imtu);\n\t\t\tpdu.rsp.mps = cpu_to_le16(chan->mps);\n\t\t\tpdu.rsp.credits = cpu_to_le16(chan->rx_credits);\n\t\t}\n\n\t\tpdu.dcid[i] = cpu_to_le16(chan->scid);\n\n\t\t__set_chan_timer(chan, chan->ops->get_sndtimeo(chan));\n\n\t\tchan->ident = cmd->ident;\n\n\t\tif (test_bit(FLAG_DEFER_SETUP, &chan->flags)) {\n\t\t\tl2cap_state_change(chan, BT_CONNECT2);\n\t\t\tdefer = true;\n\t\t\tchan->ops->defer(chan);\n\t\t} else {\n\t\t\tl2cap_chan_ready(chan);\n\t\t}\n\t}\n\nunlock:\n\tl2cap_chan_unlock(pchan);\n\tmutex_unlock(&conn->chan_lock);\n\tl2cap_chan_put(pchan);\n\nresponse:\n\tpdu.rsp.result = cpu_to_le16(result);\n\n\tif (defer)\n\t\treturn 0;\n\n\tl2cap_send_cmd(conn, cmd->ident, L2CAP_ECRED_CONN_RSP,\n\t\t       sizeof(pdu.rsp) + len, &pdu);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3894,
            "cve_id": "CVE-2022-4379",
            "code_snippet": "static __be32\nnfsd4_copy(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\tunion nfsd4_op_u *u)\n{\n\tstruct nfsd4_copy *copy = &u->copy;\n\t__be32 status;\n\tstruct nfsd4_copy *async_copy = NULL;\n\n\tif (nfsd4_ssc_is_inter(copy)) {\n\t\tif (!inter_copy_offload_enable || nfsd4_copy_is_sync(copy)) {\n\t\t\tstatus = nfserr_notsupp;\n\t\t\tgoto out;\n\t\t}\n\t\tstatus = nfsd4_setup_inter_ssc(rqstp, cstate, copy,\n\t\t\t\t&copy->ss_mnt);\n\t\tif (status)\n\t\t\treturn nfserr_offload_denied;\n\t} else {\n\t\tstatus = nfsd4_setup_intra_ssc(rqstp, cstate, copy);\n\t\tif (status)\n\t\t\treturn status;\n\t}\n\n\tcopy->cp_clp = cstate->clp;\n\tmemcpy(&copy->fh, &cstate->current_fh.fh_handle,\n\t\tsizeof(struct knfsd_fh));\n\tif (nfsd4_copy_is_async(copy)) {\n\t\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\t\tstatus = nfserrno(-ENOMEM);\n\t\tasync_copy = kzalloc(sizeof(struct nfsd4_copy), GFP_KERNEL);\n\t\tif (!async_copy)\n\t\t\tgoto out_err;\n\t\tasync_copy->cp_src = kmalloc(sizeof(*async_copy->cp_src), GFP_KERNEL);\n\t\tif (!async_copy->cp_src)\n\t\t\tgoto out_err;\n\t\tif (!nfs4_init_copy_state(nn, copy))\n\t\t\tgoto out_err;\n\t\trefcount_set(&async_copy->refcount, 1);\n\t\tmemcpy(&copy->cp_res.cb_stateid, &copy->cp_stateid.cs_stid,\n\t\t\tsizeof(copy->cp_res.cb_stateid));\n\t\tdup_copy_fields(copy, async_copy);\n\t\tasync_copy->copy_task = kthread_create(nfsd4_do_async_copy,\n\t\t\t\tasync_copy, \"%s\", \"copy thread\");\n\t\tif (IS_ERR(async_copy->copy_task))\n\t\t\tgoto out_err;\n\t\tspin_lock(&async_copy->cp_clp->async_lock);\n\t\tlist_add(&async_copy->copies,\n\t\t\t\t&async_copy->cp_clp->async_copies);\n\t\tspin_unlock(&async_copy->cp_clp->async_lock);\n\t\twake_up_process(async_copy->copy_task);\n\t\tstatus = nfs_ok;\n\t} else {\n\t\tstatus = nfsd4_do_copy(copy, copy->nf_src->nf_file,\n\t\t\t\t       copy->nf_dst->nf_file, true);\n\t\tnfsd4_cleanup_intra_ssc(copy->nf_src, copy->nf_dst);\n\t}\nout:\n\treturn status;\nout_err:\n\tif (async_copy)\n\t\tcleanup_async_copy(async_copy);\n\tstatus = nfserrno(-ENOMEM);\n\tif (nfsd4_ssc_is_inter(copy))\n\t\tnfsd4_interssc_disconnect(copy->ss_mnt);\n\tgoto out;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3896,
            "cve_id": "CVE-2022-4382",
            "code_snippet": "static void\ngadgetfs_kill_sb (struct super_block *sb)\n{\n\tkill_litter_super (sb);\n\tif (the_device) {\n\t\tput_dev (the_device);\n\t\tthe_device = NULL;\n\t}\n\tkfree(CHIP);\n\tCHIP = NULL;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3902,
            "cve_id": "CVE-2022-45888",
            "code_snippet": "static void xillyusb_disconnect(struct usb_interface *interface)\n{\n\tstruct xillyusb_dev *xdev = usb_get_intfdata(interface);\n\tstruct xillyusb_endpoint *msg_ep = xdev->msg_ep;\n\tstruct xillyfifo *fifo = &msg_ep->fifo;\n\tint rc;\n\tint i;\n\n\txillybus_cleanup_chrdev(xdev, &interface->dev);\n\n\t/*\n\t * Try to send OPCODE_QUIESCE, which will fail silently if the device\n\t * was disconnected, but makes sense on module unload.\n\t */\n\n\tmsg_ep->wake_on_drain = true;\n\txillyusb_send_opcode(xdev, ~0, OPCODE_QUIESCE, 0);\n\n\t/*\n\t * If the device has been disconnected, sending the opcode causes\n\t * a global device error with xdev->error, if such error didn't\n\t * occur earlier. Hence timing out means that the USB link is fine,\n\t * but somehow the message wasn't sent. Should never happen.\n\t */\n\n\trc = wait_event_interruptible_timeout(fifo->waitq,\n\t\t\t\t\t      msg_ep->drained || xdev->error,\n\t\t\t\t\t      XILLY_RESPONSE_TIMEOUT);\n\n\tif (!rc)\n\t\tdev_err(&interface->dev,\n\t\t\t\"Weird timeout condition on sending quiesce request.\\n\");\n\n\treport_io_error(xdev, -ENODEV); /* Discourage further activity */\n\n\t/*\n\t * This device driver is declared with soft_unbind set, or else\n\t * sending OPCODE_QUIESCE above would always fail. The price is\n\t * that the USB framework didn't kill outstanding URBs, so it has\n\t * to be done explicitly before returning from this call.\n\t */\n\n\tfor (i = 0; i < xdev->num_channels; i++) {\n\t\tstruct xillyusb_channel *chan = &xdev->channels[i];\n\n\t\t/*\n\t\t * Lock taken to prevent chan->out_ep from changing. It also\n\t\t * ensures xillyusb_open() and xillyusb_flush() don't access\n\t\t * xdev->dev after being nullified below.\n\t\t */\n\t\tmutex_lock(&chan->lock);\n\t\tif (chan->out_ep)\n\t\t\tendpoint_quiesce(chan->out_ep);\n\t\tmutex_unlock(&chan->lock);\n\t}\n\n\tendpoint_quiesce(xdev->in_ep);\n\tendpoint_quiesce(xdev->msg_ep);\n\n\tusb_set_intfdata(interface, NULL);\n\n\txdev->dev = NULL;\n\n\tkref_put(&xdev->kref, cleanup_dev);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3904,
            "cve_id": "CVE-2022-45919",
            "code_snippet": "static int dvb_ca_en50221_io_release(struct inode *inode, struct file *file)\n{\n\tstruct dvb_device *dvbdev = file->private_data;\n\tstruct dvb_ca_private *ca = dvbdev->priv;\n\tint err;\n\n\tdprintk(\"%s\\n\", __func__);\n\n\t/* mark the CA device as closed */\n\tca->open = 0;\n\tdvb_ca_en50221_thread_update_delay(ca);\n\n\terr = dvb_generic_release(inode, file);\n\n\tmodule_put(ca->pub->owner);\n\n\tdvb_ca_private_put(ca);\n\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3927,
            "cve_id": "CVE-2022-47946",
            "code_snippet": "static void io_ring_ctx_wait_and_kill(struct io_ring_ctx *ctx)\n{\n\tmutex_lock(&ctx->uring_lock);\n\tpercpu_ref_kill(&ctx->refs);\n\n\tif (WARN_ON_ONCE((ctx->flags & IORING_SETUP_SQPOLL) && !ctx->sqo_dead))\n\t\tctx->sqo_dead = 1;\n\n\t/* if force is set, the ring is going away. always drop after that */\n\tctx->cq_overflow_flushed = 1;\n\tif (ctx->rings)\n\t\t__io_cqring_overflow_flush(ctx, true, NULL, NULL);\n\tidr_for_each(&ctx->personality_idr, io_remove_personalities, ctx);\n\tmutex_unlock(&ctx->uring_lock);\n\n\tio_kill_timeouts(ctx, NULL, NULL);\n\tio_poll_remove_all(ctx, NULL, NULL);\n\n\t/* if we failed setting up the ctx, we might not have any rings */\n\tio_iopoll_try_reap_events(ctx);\n\n\tINIT_WORK(&ctx->exit_work, io_ring_exit_work);\n\t/*\n\t * Use system_unbound_wq to avoid spawning tons of event kworkers\n\t * if we're exiting a ton of rings at the same time. It just adds\n\t * noise and overhead, there's no discernable change in runtime\n\t * over using system_wq.\n\t */\n\tqueue_work(system_unbound_wq, &ctx->exit_work);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3928,
            "cve_id": "CVE-2022-47946",
            "code_snippet": "SYSCALL_DEFINE6(io_uring_enter, unsigned int, fd, u32, to_submit,\n\t\tu32, min_complete, u32, flags, const void __user *, argp,\n\t\tsize_t, argsz)\n{\n\tstruct io_ring_ctx *ctx;\n\tlong ret = -EBADF;\n\tint submitted = 0;\n\tstruct fd f;\n\n\tio_run_task_work();\n\n\tif (flags & ~(IORING_ENTER_GETEVENTS | IORING_ENTER_SQ_WAKEUP |\n\t\t\tIORING_ENTER_SQ_WAIT | IORING_ENTER_EXT_ARG))\n\t\treturn -EINVAL;\n\n\tf = fdget(fd);\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tret = -EOPNOTSUPP;\n\tif (f.file->f_op != &io_uring_fops)\n\t\tgoto out_fput;\n\n\tret = -ENXIO;\n\tctx = f.file->private_data;\n\tif (!percpu_ref_tryget(&ctx->refs))\n\t\tgoto out_fput;\n\n\tret = -EBADFD;\n\tif (ctx->flags & IORING_SETUP_R_DISABLED)\n\t\tgoto out;\n\n\t/*\n\t * For SQ polling, the thread will do all submissions and completions.\n\t * Just return the requested submit count, and wake the thread if\n\t * we were asked to.\n\t */\n\tret = 0;\n\tif (ctx->flags & IORING_SETUP_SQPOLL) {\n\t\tio_cqring_overflow_flush(ctx, false, NULL, NULL);\n\n\t\tif (unlikely(ctx->sqo_exec)) {\n\t\t\tret = io_sq_thread_fork(ctx->sq_data, ctx);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tctx->sqo_exec = 0;\n\t\t}\n\t\tret = -EOWNERDEAD;\n\t\tif (unlikely(ctx->sqo_dead))\n\t\t\tgoto out;\n\t\tif (flags & IORING_ENTER_SQ_WAKEUP)\n\t\t\twake_up(&ctx->sq_data->wait);\n\t\tif (flags & IORING_ENTER_SQ_WAIT) {\n\t\t\tret = io_sqpoll_wait_sq(ctx);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\t\tsubmitted = to_submit;\n\t} else if (to_submit) {\n\t\tret = io_uring_add_task_file(ctx, f.file);\n\t\tif (unlikely(ret))\n\t\t\tgoto out;\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tsubmitted = io_submit_sqes(ctx, to_submit);\n\t\tmutex_unlock(&ctx->uring_lock);\n\n\t\tif (submitted != to_submit)\n\t\t\tgoto out;\n\t}\n\tif (flags & IORING_ENTER_GETEVENTS) {\n\t\tconst sigset_t __user *sig;\n\t\tstruct __kernel_timespec __user *ts;\n\n\t\tret = io_get_ext_arg(flags, argp, &argsz, &ts, &sig);\n\t\tif (unlikely(ret))\n\t\t\tgoto out;\n\n\t\tmin_complete = min(min_complete, ctx->cq_entries);\n\n\t\t/*\n\t\t * When SETUP_IOPOLL and SETUP_SQPOLL are both enabled, user\n\t\t * space applications don't need to do io completion events\n\t\t * polling again, they can rely on io_sq_thread to do polling\n\t\t * work, which can reduce cpu usage and uring_lock contention.\n\t\t */\n\t\tif (ctx->flags & IORING_SETUP_IOPOLL &&\n\t\t    !(ctx->flags & IORING_SETUP_SQPOLL)) {\n\t\t\tret = io_iopoll_check(ctx, min_complete);\n\t\t} else {\n\t\t\tret = io_cqring_wait(ctx, min_complete, sig, argsz, ts);\n\t\t}\n\t}\n\nout:\n\tpercpu_ref_put(&ctx->refs);\nout_fput:\n\tfdput(f);\n\treturn submitted ? submitted : ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3929,
            "cve_id": "CVE-2022-47946",
            "code_snippet": "static int io_sq_thread_fork(struct io_sq_data *sqd, struct io_ring_ctx *ctx)\n{\n\tint ret;\n\n\tclear_bit(IO_SQ_THREAD_SHOULD_STOP, &sqd->state);\n\treinit_completion(&sqd->completion);\n\tctx->sqo_dead = ctx->sqo_exec = 0;\n\tsqd->task_pid = current->pid;\n\tcurrent->flags |= PF_IO_WORKER;\n\tret = io_wq_fork_thread(io_sq_thread, sqd);\n\tcurrent->flags &= ~PF_IO_WORKER;\n\tif (ret < 0) {\n\t\tsqd->thread = NULL;\n\t\treturn ret;\n\t}\n\twait_for_completion(&sqd->completion);\n\treturn io_uring_alloc_task_context(sqd->thread, ctx);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3930,
            "cve_id": "CVE-2022-47946",
            "code_snippet": "static int io_uring_create(unsigned entries, struct io_uring_params *p,\n\t\t\t   struct io_uring_params __user *params)\n{\n\tstruct io_ring_ctx *ctx;\n\tstruct file *file;\n\tint ret;\n\n\tif (!entries)\n\t\treturn -EINVAL;\n\tif (entries > IORING_MAX_ENTRIES) {\n\t\tif (!(p->flags & IORING_SETUP_CLAMP))\n\t\t\treturn -EINVAL;\n\t\tentries = IORING_MAX_ENTRIES;\n\t}\n\n\t/*\n\t * Use twice as many entries for the CQ ring. It's possible for the\n\t * application to drive a higher depth than the size of the SQ ring,\n\t * since the sqes are only used at submission time. This allows for\n\t * some flexibility in overcommitting a bit. If the application has\n\t * set IORING_SETUP_CQSIZE, it will have passed in the desired number\n\t * of CQ ring entries manually.\n\t */\n\tp->sq_entries = roundup_pow_of_two(entries);\n\tif (p->flags & IORING_SETUP_CQSIZE) {\n\t\t/*\n\t\t * If IORING_SETUP_CQSIZE is set, we do the same roundup\n\t\t * to a power-of-two, if it isn't already. We do NOT impose\n\t\t * any cq vs sq ring sizing.\n\t\t */\n\t\tif (!p->cq_entries)\n\t\t\treturn -EINVAL;\n\t\tif (p->cq_entries > IORING_MAX_CQ_ENTRIES) {\n\t\t\tif (!(p->flags & IORING_SETUP_CLAMP))\n\t\t\t\treturn -EINVAL;\n\t\t\tp->cq_entries = IORING_MAX_CQ_ENTRIES;\n\t\t}\n\t\tp->cq_entries = roundup_pow_of_two(p->cq_entries);\n\t\tif (p->cq_entries < p->sq_entries)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tp->cq_entries = 2 * p->sq_entries;\n\t}\n\n\tctx = io_ring_ctx_alloc(p);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\tctx->compat = in_compat_syscall();\n\tif (!capable(CAP_IPC_LOCK))\n\t\tctx->user = get_uid(current_user());\n\tctx->sqo_task = current;\n\n\t/*\n\t * This is just grabbed for accounting purposes. When a process exits,\n\t * the mm is exited and dropped before the files, hence we need to hang\n\t * on to this mm purely for the purposes of being able to unaccount\n\t * memory (locked/pinned vm). It's not used for anything else.\n\t */\n\tmmgrab(current->mm);\n\tctx->mm_account = current->mm;\n\n\tret = io_allocate_scq_urings(ctx, p);\n\tif (ret)\n\t\tgoto err;\n\n\tret = io_sq_offload_create(ctx, p);\n\tif (ret)\n\t\tgoto err;\n\n\tif (!(p->flags & IORING_SETUP_R_DISABLED))\n\t\tio_sq_offload_start(ctx);\n\n\tmemset(&p->sq_off, 0, sizeof(p->sq_off));\n\tp->sq_off.head = offsetof(struct io_rings, sq.head);\n\tp->sq_off.tail = offsetof(struct io_rings, sq.tail);\n\tp->sq_off.ring_mask = offsetof(struct io_rings, sq_ring_mask);\n\tp->sq_off.ring_entries = offsetof(struct io_rings, sq_ring_entries);\n\tp->sq_off.flags = offsetof(struct io_rings, sq_flags);\n\tp->sq_off.dropped = offsetof(struct io_rings, sq_dropped);\n\tp->sq_off.array = (char *)ctx->sq_array - (char *)ctx->rings;\n\n\tmemset(&p->cq_off, 0, sizeof(p->cq_off));\n\tp->cq_off.head = offsetof(struct io_rings, cq.head);\n\tp->cq_off.tail = offsetof(struct io_rings, cq.tail);\n\tp->cq_off.ring_mask = offsetof(struct io_rings, cq_ring_mask);\n\tp->cq_off.ring_entries = offsetof(struct io_rings, cq_ring_entries);\n\tp->cq_off.overflow = offsetof(struct io_rings, cq_overflow);\n\tp->cq_off.cqes = offsetof(struct io_rings, cqes);\n\tp->cq_off.flags = offsetof(struct io_rings, cq_flags);\n\n\tp->features = IORING_FEAT_SINGLE_MMAP | IORING_FEAT_NODROP |\n\t\t\tIORING_FEAT_SUBMIT_STABLE | IORING_FEAT_RW_CUR_POS |\n\t\t\tIORING_FEAT_CUR_PERSONALITY | IORING_FEAT_FAST_POLL |\n\t\t\tIORING_FEAT_POLL_32BITS | IORING_FEAT_SQPOLL_NONFIXED |\n\t\t\tIORING_FEAT_EXT_ARG | IORING_FEAT_NATIVE_WORKERS;\n\n\tif (copy_to_user(params, p, sizeof(*p))) {\n\t\tret = -EFAULT;\n\t\tgoto err;\n\t}\n\n\tfile = io_uring_get_file(ctx);\n\tif (IS_ERR(file)) {\n\t\tret = PTR_ERR(file);\n\t\tgoto err;\n\t}\n\n\t/*\n\t * Install ring fd as the very last thing, so we don't risk someone\n\t * having closed it before we finish setup\n\t */\n\tret = io_uring_install_fd(ctx, file);\n\tif (ret < 0) {\n\t\tio_disable_sqo_submit(ctx);\n\t\t/* fput will clean it up */\n\t\tfput(file);\n\t\treturn ret;\n\t}\n\n\ttrace_io_uring_create(ret, ctx, p->sq_entries, p->cq_entries, p->flags);\n\treturn ret;\nerr:\n\tio_disable_sqo_submit(ctx);\n\tio_ring_ctx_wait_and_kill(ctx);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3952,
            "cve_id": "CVE-2023-0030",
            "code_snippet": "int\nnvkm_vmm_get_locked(struct nvkm_vmm *vmm, bool getref, bool mapref, bool sparse,\n\t\t    u8 shift, u8 align, u64 size, struct nvkm_vma **pvma)\n{\n\tconst struct nvkm_vmm_page *page = &vmm->func->page[NVKM_VMA_PAGE_NONE];\n\tstruct rb_node *node = NULL, *temp;\n\tstruct nvkm_vma *vma = NULL, *tmp;\n\tu64 addr, tail;\n\tint ret;\n\n\tVMM_TRACE(vmm, \"getref %d mapref %d sparse %d \"\n\t\t       \"shift: %d align: %d size: %016llx\",\n\t\t  getref, mapref, sparse, shift, align, size);\n\n\t/* Zero-sized, or lazily-allocated sparse VMAs, make no sense. */\n\tif (unlikely(!size || (!getref && !mapref && sparse))) {\n\t\tVMM_DEBUG(vmm, \"args %016llx %d %d %d\",\n\t\t\t  size, getref, mapref, sparse);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Tesla-class GPUs can only select page size per-PDE, which means\n\t * we're required to know the mapping granularity up-front to find\n\t * a suitable region of address-space.\n\t *\n\t * The same goes if we're requesting up-front allocation of PTES.\n\t */\n\tif (unlikely((getref || vmm->func->page_block) && !shift)) {\n\t\tVMM_DEBUG(vmm, \"page size required: %d %016llx\",\n\t\t\t  getref, vmm->func->page_block);\n\t\treturn -EINVAL;\n\t}\n\n\t/* If a specific page size was requested, determine its index and\n\t * make sure the requested size is a multiple of the page size.\n\t */\n\tif (shift) {\n\t\tfor (page = vmm->func->page; page->shift; page++) {\n\t\t\tif (shift == page->shift)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (!page->shift || !IS_ALIGNED(size, 1ULL << page->shift)) {\n\t\t\tVMM_DEBUG(vmm, \"page %d %016llx\", shift, size);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\talign = max_t(u8, align, shift);\n\t} else {\n\t\talign = max_t(u8, align, 12);\n\t}\n\n\t/* Locate smallest block that can possibly satisfy the allocation. */\n\ttemp = vmm->free.rb_node;\n\twhile (temp) {\n\t\tstruct nvkm_vma *this = rb_entry(temp, typeof(*this), tree);\n\t\tif (this->size < size) {\n\t\t\ttemp = temp->rb_right;\n\t\t} else {\n\t\t\tnode = temp;\n\t\t\ttemp = temp->rb_left;\n\t\t}\n\t}\n\n\tif (unlikely(!node))\n\t\treturn -ENOSPC;\n\n\t/* Take into account alignment restrictions, trying larger blocks\n\t * in turn until we find a suitable free block.\n\t */\n\tdo {\n\t\tstruct nvkm_vma *this = rb_entry(node, typeof(*this), tree);\n\t\tstruct nvkm_vma *prev = node(this, prev);\n\t\tstruct nvkm_vma *next = node(this, next);\n\t\tconst int p = page - vmm->func->page;\n\n\t\taddr = this->addr;\n\t\tif (vmm->func->page_block && prev && prev->page != p)\n\t\t\taddr = ALIGN(addr, vmm->func->page_block);\n\t\taddr = ALIGN(addr, 1ULL << align);\n\n\t\ttail = this->addr + this->size;\n\t\tif (vmm->func->page_block && next && next->page != p)\n\t\t\ttail = ALIGN_DOWN(tail, vmm->func->page_block);\n\n\t\tif (addr <= tail && tail - addr >= size) {\n\t\t\trb_erase(&this->tree, &vmm->free);\n\t\t\tvma = this;\n\t\t\tbreak;\n\t\t}\n\t} while ((node = rb_next(node)));\n\n\tif (unlikely(!vma))\n\t\treturn -ENOSPC;\n\n\t/* If the VMA we found isn't already exactly the requested size,\n\t * it needs to be split, and the remaining free blocks returned.\n\t */\n\tif (addr != vma->addr) {\n\t\tif (!(tmp = nvkm_vma_tail(vma, vma->size + vma->addr - addr))) {\n\t\t\tnvkm_vmm_put_region(vmm, vma);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tnvkm_vmm_free_insert(vmm, vma);\n\t\tvma = tmp;\n\t}\n\n\tif (size != vma->size) {\n\t\tif (!(tmp = nvkm_vma_tail(vma, vma->size - size))) {\n\t\t\tnvkm_vmm_put_region(vmm, vma);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tnvkm_vmm_free_insert(vmm, tmp);\n\t}\n\n\t/* Pre-allocate page tables and/or setup sparse mappings. */\n\tif (sparse && getref)\n\t\tret = nvkm_vmm_ptes_sparse_get(vmm, page, vma->addr, vma->size);\n\telse if (sparse)\n\t\tret = nvkm_vmm_ptes_sparse(vmm, vma->addr, vma->size, true);\n\telse if (getref)\n\t\tret = nvkm_vmm_ptes_get(vmm, page, vma->addr, vma->size);\n\telse\n\t\tret = 0;\n\tif (ret) {\n\t\tnvkm_vmm_put_region(vmm, vma);\n\t\treturn ret;\n\t}\n\n\tvma->mapref = mapref && !getref;\n\tvma->sparse = sparse;\n\tvma->page = page - vmm->func->page;\n\tvma->refd = getref ? vma->page : NVKM_VMA_PAGE_NONE;\n\tvma->used = true;\n\tnvkm_vmm_node_insert(vmm, vma);\n\t*pvma = vma;\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3953,
            "cve_id": "CVE-2023-0030",
            "code_snippet": "void\nnvkm_vmm_unmap_region(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tstruct nvkm_vma *next;\n\n\tnvkm_memory_tags_put(vma->memory, vmm->mmu->subdev.device, &vma->tags);\n\tnvkm_memory_unref(&vma->memory);\n\n\tif (vma->part) {\n\t\tstruct nvkm_vma *prev = node(vma, prev);\n\t\tif (!prev->memory) {\n\t\t\tprev->size += vma->size;\n\t\t\trb_erase(&vma->tree, &vmm->root);\n\t\t\tlist_del(&vma->head);\n\t\t\tkfree(vma);\n\t\t\tvma = prev;\n\t\t}\n\t}\n\n\tnext = node(vma, next);\n\tif (next && next->part) {\n\t\tif (!next->memory) {\n\t\t\tvma->size += next->size;\n\t\t\trb_erase(&next->tree, &vmm->root);\n\t\t\tlist_del(&next->head);\n\t\t\tkfree(next);\n\t\t}\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3964,
            "cve_id": "CVE-2023-0240",
            "code_snippet": "static void io_prep_async_work(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_req_init_async(req);\n\n\tif (req->flags & REQ_F_ISREG) {\n\t\tif (def->hash_reg_file || (ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tio_wq_hash_work(&req->work, file_inode(req->file));\n\t} else {\n\t\tif (def->unbound_nonreg_file)\n\t\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t}\n\tif (!(req->work.flags & IO_WQ_WORK_FILES) &&\n\t    (io_op_defs[req->opcode].work_flags & IO_WQ_WORK_FILES) &&\n\t    !(req->flags & REQ_F_NO_FILE_TABLE)) {\n\t\treq->work.identity->files = get_files_struct(current);\n\t\tget_nsproxy(current->nsproxy);\n\t\treq->work.identity->nsproxy = current->nsproxy;\n\t\treq->flags |= REQ_F_INFLIGHT;\n\n\t\tspin_lock_irq(&ctx->inflight_lock);\n\t\tlist_add(&req->inflight_entry, &ctx->inflight_list);\n\t\tspin_unlock_irq(&ctx->inflight_lock);\n\t\treq->work.flags |= IO_WQ_WORK_FILES;\n\t}\n\tif (!(req->work.flags & IO_WQ_WORK_MM) &&\n\t    (def->work_flags & IO_WQ_WORK_MM)) {\n\t\tmmgrab(current->mm);\n\t\treq->work.identity->mm = current->mm;\n\t\treq->work.flags |= IO_WQ_WORK_MM;\n\t}\n#ifdef CONFIG_BLK_CGROUP\n\tif (!(req->work.flags & IO_WQ_WORK_BLKCG) &&\n\t    (def->work_flags & IO_WQ_WORK_BLKCG)) {\n\t\trcu_read_lock();\n\t\treq->work.identity->blkcg_css = blkcg_css();\n\t\t/*\n\t\t * This should be rare, either the cgroup is dying or the task\n\t\t * is moving cgroups. Just punt to root for the handful of ios.\n\t\t */\n\t\tif (css_tryget_online(req->work.identity->blkcg_css))\n\t\t\treq->work.flags |= IO_WQ_WORK_BLKCG;\n\t\trcu_read_unlock();\n\t}\n#endif\n\tif (!(req->work.flags & IO_WQ_WORK_CREDS)) {\n\t\treq->work.identity->creds = get_current_cred();\n\t\treq->work.flags |= IO_WQ_WORK_CREDS;\n\t}\n\tif (!(req->work.flags & IO_WQ_WORK_FS) &&\n\t    (def->work_flags & IO_WQ_WORK_FS)) {\n\t\tspin_lock(&current->fs->lock);\n\t\tif (!current->fs->in_exec) {\n\t\t\treq->work.identity->fs = current->fs;\n\t\t\treq->work.identity->fs->users++;\n\t\t\treq->work.flags |= IO_WQ_WORK_FS;\n\t\t} else {\n\t\t\treq->work.flags |= IO_WQ_WORK_CANCEL;\n\t\t}\n\t\tspin_unlock(&current->fs->lock);\n\t}\n\tif (def->needs_fsize)\n\t\treq->work.identity->fsize = rlimit(RLIMIT_FSIZE);\n\telse\n\t\treq->work.identity->fsize = RLIM_INFINITY;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3965,
            "cve_id": "CVE-2023-0240",
            "code_snippet": "static inline void io_req_init_async(struct io_kiocb *req)\n{\n\tif (req->flags & REQ_F_WORK_INITIALIZED)\n\t\treturn;\n\n\tmemset(&req->work, 0, sizeof(req->work));\n\treq->flags |= REQ_F_WORK_INITIALIZED;\n\treq->work.identity = &req->identity;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3970,
            "cve_id": "CVE-2023-0266",
            "code_snippet": "static int snd_ctl_elem_read_user(struct snd_card *card,\n\t\t\t\t  struct snd_ctl_elem_value __user *_control)\n{\n\tstruct snd_ctl_elem_value *control;\n\tint result;\n\n\tcontrol = memdup_user(_control, sizeof(*control));\n\tif (IS_ERR(control))\n\t\treturn PTR_ERR(control);\n\n\tdown_read(&card->controls_rwsem);\n\tresult = snd_ctl_elem_read(card, control);\n\tup_read(&card->controls_rwsem);\n\tif (result < 0)\n\t\tgoto error;\n\n\tif (copy_to_user(_control, control, sizeof(*control)))\n\t\tresult = -EFAULT;\n error:\n\tkfree(control);\n\treturn result;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3977,
            "cve_id": "CVE-2023-0461",
            "code_snippet": "static int __tcp_set_ulp(struct sock *sk, const struct tcp_ulp_ops *ulp_ops)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tint err;\n\n\terr = -EEXIST;\n\tif (icsk->icsk_ulp_ops)\n\t\tgoto out_err;\n\n\tif (sk->sk_socket)\n\t\tclear_bit(SOCK_SUPPORT_ZC, &sk->sk_socket->flags);\n\n\terr = ulp_ops->init(sk);\n\tif (err)\n\t\tgoto out_err;\n\n\ticsk->icsk_ulp_ops = ulp_ops;\n\treturn 0;\nout_err:\n\tmodule_put(ulp_ops->owner);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3979,
            "cve_id": "CVE-2023-0468",
            "code_snippet": "static inline bool io_poll_get_ownership(struct io_kiocb *req)\n{\n\treturn !(atomic_fetch_inc(&req->poll_refs) & IO_POLL_REF_MASK);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3995,
            "cve_id": "CVE-2023-1079",
            "code_snippet": "static void asus_kbd_backlight_work(struct work_struct *work)\n{\n\tstruct asus_kbd_leds *led = container_of(work, struct asus_kbd_leds, work);\n\tu8 buf[] = { FEATURE_KBD_REPORT_ID, 0xba, 0xc5, 0xc4, 0x00 };\n\tint ret;\n\tunsigned long flags;\n\n\tif (led->removed)\n\t\treturn;\n\n\tspin_lock_irqsave(&led->lock, flags);\n\tbuf[4] = led->brightness;\n\tspin_unlock_irqrestore(&led->lock, flags);\n\n\tret = asus_kbd_set_report(led->hdev, buf, sizeof(buf));\n\tif (ret < 0)\n\t\thid_err(led->hdev, \"Asus failed to set keyboard backlight: %d\\n\", ret);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4000,
            "cve_id": "CVE-2023-1193",
            "code_snippet": "void ksmbd_conn_enqueue_request(struct ksmbd_work *work)\n{\n\tstruct ksmbd_conn *conn = work->conn;\n\tstruct list_head *requests_queue = NULL;\n\n\tif (conn->ops->get_cmd_val(work) != SMB2_CANCEL_HE) {\n\t\trequests_queue = &conn->requests;\n\t\twork->synchronous = true;\n\t}\n\n\tif (requests_queue) {\n\t\tatomic_inc(&conn->req_running);\n\t\tspin_lock(&conn->request_lock);\n\t\tlist_add_tail(&work->request_entry, requests_queue);\n\t\tspin_unlock(&conn->request_lock);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4007,
            "cve_id": "CVE-2023-1249",
            "code_snippet": "void do_coredump(const kernel_siginfo_t *siginfo)\n{\n\tstruct core_state core_state;\n\tstruct core_name cn;\n\tstruct mm_struct *mm = current->mm;\n\tstruct linux_binfmt * binfmt;\n\tconst struct cred *old_cred;\n\tstruct cred *cred;\n\tint retval = 0;\n\tint ispipe;\n\tsize_t *argv = NULL;\n\tint argc = 0;\n\t/* require nonrelative corefile path and be extra careful */\n\tbool need_suid_safe = false;\n\tbool core_dumped = false;\n\tstatic atomic_t core_dump_count = ATOMIC_INIT(0);\n\tstruct coredump_params cprm = {\n\t\t.siginfo = siginfo,\n\t\t.regs = signal_pt_regs(),\n\t\t.limit = rlimit(RLIMIT_CORE),\n\t\t/*\n\t\t * We must use the same mm->flags while dumping core to avoid\n\t\t * inconsistency of bit flags, since this flag is not protected\n\t\t * by any locks.\n\t\t */\n\t\t.mm_flags = mm->flags,\n\t\t.vma_meta = NULL,\n\t};\n\n\taudit_core_dumps(siginfo->si_signo);\n\n\tbinfmt = mm->binfmt;\n\tif (!binfmt || !binfmt->core_dump)\n\t\tgoto fail;\n\tif (!__get_dumpable(cprm.mm_flags))\n\t\tgoto fail;\n\n\tcred = prepare_creds();\n\tif (!cred)\n\t\tgoto fail;\n\t/*\n\t * We cannot trust fsuid as being the \"true\" uid of the process\n\t * nor do we know its entire history. We only know it was tainted\n\t * so we dump it as root in mode 2, and only into a controlled\n\t * environment (pipe handler or fully qualified path).\n\t */\n\tif (__get_dumpable(cprm.mm_flags) == SUID_DUMP_ROOT) {\n\t\t/* Setuid core dump mode */\n\t\tcred->fsuid = GLOBAL_ROOT_UID;\t/* Dump root private */\n\t\tneed_suid_safe = true;\n\t}\n\n\tretval = coredump_wait(siginfo->si_signo, &core_state);\n\tif (retval < 0)\n\t\tgoto fail_creds;\n\n\told_cred = override_creds(cred);\n\n\tispipe = format_corename(&cn, &cprm, &argv, &argc);\n\n\tif (ispipe) {\n\t\tint argi;\n\t\tint dump_count;\n\t\tchar **helper_argv;\n\t\tstruct subprocess_info *sub_info;\n\n\t\tif (ispipe < 0) {\n\t\t\tprintk(KERN_WARNING \"format_corename failed\\n\");\n\t\t\tprintk(KERN_WARNING \"Aborting core\\n\");\n\t\t\tgoto fail_unlock;\n\t\t}\n\n\t\tif (cprm.limit == 1) {\n\t\t\t/* See umh_pipe_setup() which sets RLIMIT_CORE = 1.\n\t\t\t *\n\t\t\t * Normally core limits are irrelevant to pipes, since\n\t\t\t * we're not writing to the file system, but we use\n\t\t\t * cprm.limit of 1 here as a special value, this is a\n\t\t\t * consistent way to catch recursive crashes.\n\t\t\t * We can still crash if the core_pattern binary sets\n\t\t\t * RLIM_CORE = !1, but it runs as root, and can do\n\t\t\t * lots of stupid things.\n\t\t\t *\n\t\t\t * Note that we use task_tgid_vnr here to grab the pid\n\t\t\t * of the process group leader.  That way we get the\n\t\t\t * right pid if a thread in a multi-threaded\n\t\t\t * core_pattern process dies.\n\t\t\t */\n\t\t\tprintk(KERN_WARNING\n\t\t\t\t\"Process %d(%s) has RLIMIT_CORE set to 1\\n\",\n\t\t\t\ttask_tgid_vnr(current), current->comm);\n\t\t\tprintk(KERN_WARNING \"Aborting core\\n\");\n\t\t\tgoto fail_unlock;\n\t\t}\n\t\tcprm.limit = RLIM_INFINITY;\n\n\t\tdump_count = atomic_inc_return(&core_dump_count);\n\t\tif (core_pipe_limit && (core_pipe_limit < dump_count)) {\n\t\t\tprintk(KERN_WARNING \"Pid %d(%s) over core_pipe_limit\\n\",\n\t\t\t       task_tgid_vnr(current), current->comm);\n\t\t\tprintk(KERN_WARNING \"Skipping core dump\\n\");\n\t\t\tgoto fail_dropcount;\n\t\t}\n\n\t\thelper_argv = kmalloc_array(argc + 1, sizeof(*helper_argv),\n\t\t\t\t\t    GFP_KERNEL);\n\t\tif (!helper_argv) {\n\t\t\tprintk(KERN_WARNING \"%s failed to allocate memory\\n\",\n\t\t\t       __func__);\n\t\t\tgoto fail_dropcount;\n\t\t}\n\t\tfor (argi = 0; argi < argc; argi++)\n\t\t\thelper_argv[argi] = cn.corename + argv[argi];\n\t\thelper_argv[argi] = NULL;\n\n\t\tretval = -ENOMEM;\n\t\tsub_info = call_usermodehelper_setup(helper_argv[0],\n\t\t\t\t\t\thelper_argv, NULL, GFP_KERNEL,\n\t\t\t\t\t\tumh_pipe_setup, NULL, &cprm);\n\t\tif (sub_info)\n\t\t\tretval = call_usermodehelper_exec(sub_info,\n\t\t\t\t\t\t\t  UMH_WAIT_EXEC);\n\n\t\tkfree(helper_argv);\n\t\tif (retval) {\n\t\t\tprintk(KERN_INFO \"Core dump to |%s pipe failed\\n\",\n\t\t\t       cn.corename);\n\t\t\tgoto close_fail;\n\t\t}\n\t} else {\n\t\tstruct user_namespace *mnt_userns;\n\t\tstruct inode *inode;\n\t\tint open_flags = O_CREAT | O_RDWR | O_NOFOLLOW |\n\t\t\t\t O_LARGEFILE | O_EXCL;\n\n\t\tif (cprm.limit < binfmt->min_coredump)\n\t\t\tgoto fail_unlock;\n\n\t\tif (need_suid_safe && cn.corename[0] != '/') {\n\t\t\tprintk(KERN_WARNING \"Pid %d(%s) can only dump core \"\\\n\t\t\t\t\"to fully qualified path!\\n\",\n\t\t\t\ttask_tgid_vnr(current), current->comm);\n\t\t\tprintk(KERN_WARNING \"Skipping core dump\\n\");\n\t\t\tgoto fail_unlock;\n\t\t}\n\n\t\t/*\n\t\t * Unlink the file if it exists unless this is a SUID\n\t\t * binary - in that case, we're running around with root\n\t\t * privs and don't want to unlink another user's coredump.\n\t\t */\n\t\tif (!need_suid_safe) {\n\t\t\t/*\n\t\t\t * If it doesn't exist, that's fine. If there's some\n\t\t\t * other problem, we'll catch it at the filp_open().\n\t\t\t */\n\t\t\tdo_unlinkat(AT_FDCWD, getname_kernel(cn.corename));\n\t\t}\n\n\t\t/*\n\t\t * There is a race between unlinking and creating the\n\t\t * file, but if that causes an EEXIST here, that's\n\t\t * fine - another process raced with us while creating\n\t\t * the corefile, and the other process won. To userspace,\n\t\t * what matters is that at least one of the two processes\n\t\t * writes its coredump successfully, not which one.\n\t\t */\n\t\tif (need_suid_safe) {\n\t\t\t/*\n\t\t\t * Using user namespaces, normal user tasks can change\n\t\t\t * their current->fs->root to point to arbitrary\n\t\t\t * directories. Since the intention of the \"only dump\n\t\t\t * with a fully qualified path\" rule is to control where\n\t\t\t * coredumps may be placed using root privileges,\n\t\t\t * current->fs->root must not be used. Instead, use the\n\t\t\t * root directory of init_task.\n\t\t\t */\n\t\t\tstruct path root;\n\n\t\t\ttask_lock(&init_task);\n\t\t\tget_fs_root(init_task.fs, &root);\n\t\t\ttask_unlock(&init_task);\n\t\t\tcprm.file = file_open_root(&root, cn.corename,\n\t\t\t\t\t\t   open_flags, 0600);\n\t\t\tpath_put(&root);\n\t\t} else {\n\t\t\tcprm.file = filp_open(cn.corename, open_flags, 0600);\n\t\t}\n\t\tif (IS_ERR(cprm.file))\n\t\t\tgoto fail_unlock;\n\n\t\tinode = file_inode(cprm.file);\n\t\tif (inode->i_nlink > 1)\n\t\t\tgoto close_fail;\n\t\tif (d_unhashed(cprm.file->f_path.dentry))\n\t\t\tgoto close_fail;\n\t\t/*\n\t\t * AK: actually i see no reason to not allow this for named\n\t\t * pipes etc, but keep the previous behaviour for now.\n\t\t */\n\t\tif (!S_ISREG(inode->i_mode))\n\t\t\tgoto close_fail;\n\t\t/*\n\t\t * Don't dump core if the filesystem changed owner or mode\n\t\t * of the file during file creation. This is an issue when\n\t\t * a process dumps core while its cwd is e.g. on a vfat\n\t\t * filesystem.\n\t\t */\n\t\tmnt_userns = file_mnt_user_ns(cprm.file);\n\t\tif (!uid_eq(i_uid_into_mnt(mnt_userns, inode),\n\t\t\t    current_fsuid())) {\n\t\t\tpr_info_ratelimited(\"Core dump to %s aborted: cannot preserve file owner\\n\",\n\t\t\t\t\t    cn.corename);\n\t\t\tgoto close_fail;\n\t\t}\n\t\tif ((inode->i_mode & 0677) != 0600) {\n\t\t\tpr_info_ratelimited(\"Core dump to %s aborted: cannot preserve file permissions\\n\",\n\t\t\t\t\t    cn.corename);\n\t\t\tgoto close_fail;\n\t\t}\n\t\tif (!(cprm.file->f_mode & FMODE_CAN_WRITE))\n\t\t\tgoto close_fail;\n\t\tif (do_truncate(mnt_userns, cprm.file->f_path.dentry,\n\t\t\t\t0, 0, cprm.file))\n\t\t\tgoto close_fail;\n\t}\n\n\t/* get us an unshared descriptor table; almost always a no-op */\n\t/* The cell spufs coredump code reads the file descriptor tables */\n\tretval = unshare_files();\n\tif (retval)\n\t\tgoto close_fail;\n\tif (!dump_interrupted()) {\n\t\t/*\n\t\t * umh disabled with CONFIG_STATIC_USERMODEHELPER_PATH=\"\" would\n\t\t * have this set to NULL.\n\t\t */\n\t\tif (!cprm.file) {\n\t\t\tpr_info(\"Core dump to |%s disabled\\n\", cn.corename);\n\t\t\tgoto close_fail;\n\t\t}\n\t\tif (!dump_vma_snapshot(&cprm))\n\t\t\tgoto close_fail;\n\n\t\tfile_start_write(cprm.file);\n\t\tcore_dumped = binfmt->core_dump(&cprm);\n\t\t/*\n\t\t * Ensures that file size is big enough to contain the current\n\t\t * file postion. This prevents gdb from complaining about\n\t\t * a truncated file if the last \"write\" to the file was\n\t\t * dump_skip.\n\t\t */\n\t\tif (cprm.to_skip) {\n\t\t\tcprm.to_skip--;\n\t\t\tdump_emit(&cprm, \"\", 1);\n\t\t}\n\t\tfile_end_write(cprm.file);\n\t\tkvfree(cprm.vma_meta);\n\t}\n\tif (ispipe && core_pipe_limit)\n\t\twait_for_dump_helpers(cprm.file);\nclose_fail:\n\tif (cprm.file)\n\t\tfilp_close(cprm.file, NULL);\nfail_dropcount:\n\tif (ispipe)\n\t\tatomic_dec(&core_dump_count);\nfail_unlock:\n\tkfree(argv);\n\tkfree(cn.corename);\n\tcoredump_finish(core_dumped);\n\trevert_creds(old_cred);\nfail_creds:\n\tput_cred(cred);\nfail:\n\treturn;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4023,
            "cve_id": "CVE-2023-1611",
            "code_snippet": "int btrfs_run_qgroups(struct btrfs_trans_handle *trans)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tint ret = 0;\n\n\tif (!fs_info->quota_root)\n\t\treturn ret;\n\n\tspin_lock(&fs_info->qgroup_lock);\n\twhile (!list_empty(&fs_info->dirty_qgroups)) {\n\t\tstruct btrfs_qgroup *qgroup;\n\t\tqgroup = list_first_entry(&fs_info->dirty_qgroups,\n\t\t\t\t\t  struct btrfs_qgroup, dirty);\n\t\tlist_del_init(&qgroup->dirty);\n\t\tspin_unlock(&fs_info->qgroup_lock);\n\t\tret = update_qgroup_info_item(trans, qgroup);\n\t\tif (ret)\n\t\t\tqgroup_mark_inconsistent(fs_info);\n\t\tret = update_qgroup_limit_item(trans, qgroup);\n\t\tif (ret)\n\t\t\tqgroup_mark_inconsistent(fs_info);\n\t\tspin_lock(&fs_info->qgroup_lock);\n\t}\n\tif (test_bit(BTRFS_FS_QUOTA_ENABLED, &fs_info->flags))\n\t\tfs_info->qgroup_flags |= BTRFS_QGROUP_STATUS_FLAG_ON;\n\telse\n\t\tfs_info->qgroup_flags &= ~BTRFS_QGROUP_STATUS_FLAG_ON;\n\tspin_unlock(&fs_info->qgroup_lock);\n\n\tret = update_qgroup_status_item(trans);\n\tif (ret)\n\t\tqgroup_mark_inconsistent(fs_info);\n\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4031,
            "cve_id": "CVE-2023-1872",
            "code_snippet": "static void io_apoll_task_func(struct io_kiocb *req, bool *locked)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\tret = io_poll_check_events(req);\n\tif (ret > 0)\n\t\treturn;\n\n\tio_poll_remove_entries(req);\n\tspin_lock(&ctx->completion_lock);\n\thash_del(&req->hash_node);\n\tspin_unlock(&ctx->completion_lock);\n\n\tif (!ret)\n\t\tio_req_task_submit(req, locked);\n\telse\n\t\tio_req_complete_failed(req, ret);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4032,
            "cve_id": "CVE-2023-1872",
            "code_snippet": "static void io_poll_task_func(struct io_kiocb *req, bool *locked)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\tret = io_poll_check_events(req);\n\tif (ret > 0)\n\t\treturn;\n\n\tif (!ret) {\n\t\treq->result = mangle_poll(req->result & req->poll.events);\n\t} else {\n\t\treq->result = ret;\n\t\treq_set_fail(req);\n\t}\n\n\tio_poll_remove_entries(req);\n\tspin_lock(&ctx->completion_lock);\n\thash_del(&req->hash_node);\n\t__io_req_complete_post(req, req->result, 0);\n\tio_commit_cqring(ctx);\n\tspin_unlock(&ctx->completion_lock);\n\tio_cqring_ev_posted(ctx);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4056,
            "cve_id": "CVE-2023-20928",
            "code_snippet": "static inline void binder_alloc_set_vma(struct binder_alloc *alloc,\n\t\tstruct vm_area_struct *vma)\n{\n\tif (vma)\n\t\talloc->vma_vm_mm = vma->vm_mm;\n\t/*\n\t * If we see alloc->vma is not NULL, buffer data structures set up\n\t * completely. Look at smp_rmb side binder_alloc_get_vma.\n\t * We also want to guarantee new alloc->vma_vm_mm is always visible\n\t * if alloc->vma is set.\n\t */\n\tsmp_wmb();\n\talloc->vma = vma;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4057,
            "cve_id": "CVE-2023-20928",
            "code_snippet": "static int binder_update_page_range(struct binder_alloc *alloc, int allocate,\n\t\t\t\t    void __user *start, void __user *end)\n{\n\tvoid __user *page_addr;\n\tunsigned long user_page_addr;\n\tstruct binder_lru_page *page;\n\tstruct vm_area_struct *vma = NULL;\n\tstruct mm_struct *mm = NULL;\n\tbool need_mm = false;\n\n\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t     \"%d: %s pages %pK-%pK\\n\", alloc->pid,\n\t\t     allocate ? \"allocate\" : \"free\", start, end);\n\n\tif (end <= start)\n\t\treturn 0;\n\n\ttrace_binder_update_page_range(alloc, allocate, start, end);\n\n\tif (allocate == 0)\n\t\tgoto free_range;\n\n\tfor (page_addr = start; page_addr < end; page_addr += PAGE_SIZE) {\n\t\tpage = &alloc->pages[(page_addr - alloc->buffer) / PAGE_SIZE];\n\t\tif (!page->page_ptr) {\n\t\t\tneed_mm = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (need_mm && mmget_not_zero(alloc->vma_vm_mm))\n\t\tmm = alloc->vma_vm_mm;\n\n\tif (mm) {\n\t\tmmap_read_lock(mm);\n\t\tvma = alloc->vma;\n\t}\n\n\tif (!vma && need_mm) {\n\t\tbinder_alloc_debug(BINDER_DEBUG_USER_ERROR,\n\t\t\t\t   \"%d: binder_alloc_buf failed to map pages in userspace, no vma\\n\",\n\t\t\t\t   alloc->pid);\n\t\tgoto err_no_vma;\n\t}\n\n\tfor (page_addr = start; page_addr < end; page_addr += PAGE_SIZE) {\n\t\tint ret;\n\t\tbool on_lru;\n\t\tsize_t index;\n\n\t\tindex = (page_addr - alloc->buffer) / PAGE_SIZE;\n\t\tpage = &alloc->pages[index];\n\n\t\tif (page->page_ptr) {\n\t\t\ttrace_binder_alloc_lru_start(alloc, index);\n\n\t\t\ton_lru = list_lru_del(&binder_alloc_lru, &page->lru);\n\t\t\tWARN_ON(!on_lru);\n\n\t\t\ttrace_binder_alloc_lru_end(alloc, index);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (WARN_ON(!vma))\n\t\t\tgoto err_page_ptr_cleared;\n\n\t\ttrace_binder_alloc_page_start(alloc, index);\n\t\tpage->page_ptr = alloc_page(GFP_KERNEL |\n\t\t\t\t\t    __GFP_HIGHMEM |\n\t\t\t\t\t    __GFP_ZERO);\n\t\tif (!page->page_ptr) {\n\t\t\tpr_err(\"%d: binder_alloc_buf failed for page at %pK\\n\",\n\t\t\t\talloc->pid, page_addr);\n\t\t\tgoto err_alloc_page_failed;\n\t\t}\n\t\tpage->alloc = alloc;\n\t\tINIT_LIST_HEAD(&page->lru);\n\n\t\tuser_page_addr = (uintptr_t)page_addr;\n\t\tret = vm_insert_page(vma, user_page_addr, page[0].page_ptr);\n\t\tif (ret) {\n\t\t\tpr_err(\"%d: binder_alloc_buf failed to map page at %lx in userspace\\n\",\n\t\t\t       alloc->pid, user_page_addr);\n\t\t\tgoto err_vm_insert_page_failed;\n\t\t}\n\n\t\tif (index + 1 > alloc->pages_high)\n\t\t\talloc->pages_high = index + 1;\n\n\t\ttrace_binder_alloc_page_end(alloc, index);\n\t}\n\tif (mm) {\n\t\tmmap_read_unlock(mm);\n\t\tmmput(mm);\n\t}\n\treturn 0;\n\nfree_range:\n\tfor (page_addr = end - PAGE_SIZE; 1; page_addr -= PAGE_SIZE) {\n\t\tbool ret;\n\t\tsize_t index;\n\n\t\tindex = (page_addr - alloc->buffer) / PAGE_SIZE;\n\t\tpage = &alloc->pages[index];\n\n\t\ttrace_binder_free_lru_start(alloc, index);\n\n\t\tret = list_lru_add(&binder_alloc_lru, &page->lru);\n\t\tWARN_ON(!ret);\n\n\t\ttrace_binder_free_lru_end(alloc, index);\n\t\tif (page_addr == start)\n\t\t\tbreak;\n\t\tcontinue;\n\nerr_vm_insert_page_failed:\n\t\t__free_page(page->page_ptr);\n\t\tpage->page_ptr = NULL;\nerr_alloc_page_failed:\nerr_page_ptr_cleared:\n\t\tif (page_addr == start)\n\t\t\tbreak;\n\t}\nerr_no_vma:\n\tif (mm) {\n\t\tmmap_read_unlock(mm);\n\t\tmmput(mm);\n\t}\n\treturn vma ? -ENOMEM : -ESRCH;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4065,
            "cve_id": "CVE-2023-21255",
            "code_snippet": "static int binder_proc_transaction(struct binder_transaction *t,\n\t\t\t\t    struct binder_proc *proc,\n\t\t\t\t    struct binder_thread *thread)\n{\n\tstruct binder_node *node = t->buffer->target_node;\n\tbool oneway = !!(t->flags & TF_ONE_WAY);\n\tbool pending_async = false;\n\tstruct binder_transaction *t_outdated = NULL;\n\tbool frozen = false;\n\n\tBUG_ON(!node);\n\tbinder_node_lock(node);\n\tif (oneway) {\n\t\tBUG_ON(thread);\n\t\tif (node->has_async_transaction)\n\t\t\tpending_async = true;\n\t\telse\n\t\t\tnode->has_async_transaction = true;\n\t}\n\n\tbinder_inner_proc_lock(proc);\n\tif (proc->is_frozen) {\n\t\tfrozen = true;\n\t\tproc->sync_recv |= !oneway;\n\t\tproc->async_recv |= oneway;\n\t}\n\n\tif ((frozen && !oneway) || proc->is_dead ||\n\t\t\t(thread && thread->is_dead)) {\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_node_unlock(node);\n\t\treturn frozen ? BR_FROZEN_REPLY : BR_DEAD_REPLY;\n\t}\n\n\tif (!thread && !pending_async)\n\t\tthread = binder_select_thread_ilocked(proc);\n\n\tif (thread) {\n\t\tbinder_enqueue_thread_work_ilocked(thread, &t->work);\n\t} else if (!pending_async) {\n\t\tbinder_enqueue_work_ilocked(&t->work, &proc->todo);\n\t} else {\n\t\tif ((t->flags & TF_UPDATE_TXN) && frozen) {\n\t\t\tt_outdated = binder_find_outdated_transaction_ilocked(t,\n\t\t\t\t\t\t\t\t\t      &node->async_todo);\n\t\t\tif (t_outdated) {\n\t\t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t\t\t     \"txn %d supersedes %d\\n\",\n\t\t\t\t\t     t->debug_id, t_outdated->debug_id);\n\t\t\t\tlist_del_init(&t_outdated->work.entry);\n\t\t\t\tproc->outstanding_txns--;\n\t\t\t}\n\t\t}\n\t\tbinder_enqueue_work_ilocked(&t->work, &node->async_todo);\n\t}\n\n\tif (!pending_async)\n\t\tbinder_wakeup_thread_ilocked(proc, thread, !oneway /* sync */);\n\n\tproc->outstanding_txns++;\n\tbinder_inner_proc_unlock(proc);\n\tbinder_node_unlock(node);\n\n\t/*\n\t * To reduce potential contention, free the outdated transaction and\n\t * buffer after releasing the locks.\n\t */\n\tif (t_outdated) {\n\t\tstruct binder_buffer *buffer = t_outdated->buffer;\n\n\t\tt_outdated->buffer = NULL;\n\t\tbuffer->transaction = NULL;\n\t\ttrace_binder_transaction_update_buffer_release(buffer);\n\t\tbinder_transaction_buffer_release(proc, NULL, buffer, 0, 0);\n\t\tbinder_alloc_free_buf(&proc->alloc, buffer);\n\t\tkfree(t_outdated);\n\t\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\n\t}\n\n\tif (oneway && frozen)\n\t\treturn BR_TRANSACTION_PENDING_FROZEN;\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4072,
            "cve_id": "CVE-2023-2162",
            "code_snippet": "static struct iscsi_cls_session *\niscsi_sw_tcp_session_create(struct iscsi_endpoint *ep, uint16_t cmds_max,\n\t\t\t    uint16_t qdepth, uint32_t initial_cmdsn)\n{\n\tstruct iscsi_cls_session *cls_session;\n\tstruct iscsi_session *session;\n\tstruct iscsi_sw_tcp_host *tcp_sw_host;\n\tstruct Scsi_Host *shost;\n\tint rc;\n\n\tif (ep) {\n\t\tprintk(KERN_ERR \"iscsi_tcp: invalid ep %p.\\n\", ep);\n\t\treturn NULL;\n\t}\n\n\tshost = iscsi_host_alloc(&iscsi_sw_tcp_sht,\n\t\t\t\t sizeof(struct iscsi_sw_tcp_host), 1);\n\tif (!shost)\n\t\treturn NULL;\n\tshost->transportt = iscsi_sw_tcp_scsi_transport;\n\tshost->cmd_per_lun = qdepth;\n\tshost->max_lun = iscsi_max_lun;\n\tshost->max_id = 0;\n\tshost->max_channel = 0;\n\tshost->max_cmd_len = SCSI_MAX_VARLEN_CDB_SIZE;\n\n\trc = iscsi_host_get_max_scsi_cmds(shost, cmds_max);\n\tif (rc < 0)\n\t\tgoto free_host;\n\tshost->can_queue = rc;\n\n\tif (iscsi_host_add(shost, NULL))\n\t\tgoto free_host;\n\n\tcls_session = iscsi_session_setup(&iscsi_sw_tcp_transport, shost,\n\t\t\t\t\t  cmds_max, 0,\n\t\t\t\t\t  sizeof(struct iscsi_tcp_task) +\n\t\t\t\t\t  sizeof(struct iscsi_sw_tcp_hdrbuf),\n\t\t\t\t\t  initial_cmdsn, 0);\n\tif (!cls_session)\n\t\tgoto remove_host;\n\tsession = cls_session->dd_data;\n\ttcp_sw_host = iscsi_host_priv(shost);\n\ttcp_sw_host->session = session;\n\n\tif (iscsi_tcp_r2tpool_alloc(session))\n\t\tgoto remove_session;\n\treturn cls_session;\n\nremove_session:\n\tiscsi_session_teardown(cls_session);\nremove_host:\n\tiscsi_host_remove(shost, false);\nfree_host:\n\tiscsi_host_free(shost);\n\treturn NULL;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4103,
            "cve_id": "CVE-2023-23586",
            "code_snippet": "static void io_worker_exit(struct io_worker *worker)\n{\n\tstruct io_wqe *wqe = worker->wqe;\n\tstruct io_wqe_acct *acct = io_wqe_get_acct(worker);\n\n\t/*\n\t * If we're not at zero, someone else is holding a brief reference\n\t * to the worker. Wait for that to go away.\n\t */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tif (!refcount_dec_and_test(&worker->ref))\n\t\tschedule();\n\t__set_current_state(TASK_RUNNING);\n\n\tpreempt_disable();\n\tcurrent->flags &= ~PF_IO_WORKER;\n\tif (worker->flags & IO_WORKER_F_RUNNING)\n\t\tatomic_dec(&acct->nr_running);\n\tif (!(worker->flags & IO_WORKER_F_BOUND))\n\t\tatomic_dec(&wqe->wq->user->processes);\n\tworker->flags = 0;\n\tpreempt_enable();\n\n\traw_spin_lock_irq(&wqe->lock);\n\thlist_nulls_del_rcu(&worker->nulls_node);\n\tlist_del_rcu(&worker->all_list);\n\tacct->nr_workers--;\n\traw_spin_unlock_irq(&wqe->lock);\n\n\tkfree_rcu(worker, rcu);\n\tif (refcount_dec_and_test(&wqe->wq->refs))\n\t\tcomplete(&wqe->wq->done);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4104,
            "cve_id": "CVE-2023-23586",
            "code_snippet": "static void __io_worker_idle(struct io_wqe *wqe, struct io_worker *worker)\n\t__must_hold(wqe->lock)\n{\n\tif (!(worker->flags & IO_WORKER_F_FREE)) {\n\t\tworker->flags |= IO_WORKER_F_FREE;\n\t\thlist_nulls_add_head_rcu(&worker->nulls_node, &wqe->free_list);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4105,
            "cve_id": "CVE-2023-23586",
            "code_snippet": "static bool io_match_task(struct io_kiocb *head,\n\t\t\t  struct task_struct *task,\n\t\t\t  struct files_struct *files)\n{\n\tstruct io_kiocb *req;\n\n\tif (task && head->task != task) {\n\t\t/* in terms of cancelation, always match if req task is dead */\n\t\tif (head->task->flags & PF_EXITING)\n\t\t\treturn true;\n\t\treturn false;\n\t}\n\tif (!files)\n\t\treturn true;\n\n\tio_for_each_link(req, head) {\n\t\tif (!(req->flags & REQ_F_WORK_INITIALIZED))\n\t\t\tcontinue;\n\t\tif (req->file && req->file->f_op == &io_uring_fops)\n\t\t\treturn true;\n\t\tif (req->work.identity->files == files)\n\t\t\treturn true;\n\t}\n\treturn false;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4106,
            "cve_id": "CVE-2023-23586",
            "code_snippet": "static void io_prep_async_work(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_req_init_async(req);\n\n\tif (req->flags & REQ_F_FORCE_ASYNC)\n\t\treq->work.flags |= IO_WQ_WORK_CONCURRENT;\n\n\tif (req->flags & REQ_F_ISREG) {\n\t\tif (def->hash_reg_file || (ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tio_wq_hash_work(&req->work, file_inode(req->file));\n\t} else {\n\t\tif (def->unbound_nonreg_file)\n\t\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4107,
            "cve_id": "CVE-2023-23586",
            "code_snippet": "static int io_uring_show_cred(int id, void *p, void *data)\n{\n\tstruct io_identity *iod = p;\n\tconst struct cred *cred = iod->creds;\n\tstruct seq_file *m = data;\n\tstruct user_namespace *uns = seq_user_ns(m);\n\tstruct group_info *gi;\n\tkernel_cap_t cap;\n\tunsigned __capi;\n\tint g;\n\n\tseq_printf(m, \"%5d\\n\", id);\n\tseq_put_decimal_ull(m, \"\\tUid:\\t\", from_kuid_munged(uns, cred->uid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->euid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->suid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->fsuid));\n\tseq_put_decimal_ull(m, \"\\n\\tGid:\\t\", from_kgid_munged(uns, cred->gid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->egid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->sgid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->fsgid));\n\tseq_puts(m, \"\\n\\tGroups:\\t\");\n\tgi = cred->group_info;\n\tfor (g = 0; g < gi->ngroups; g++) {\n\t\tseq_put_decimal_ull(m, g ? \" \" : \"\",\n\t\t\t\t\tfrom_kgid_munged(uns, gi->gid[g]));\n\t}\n\tseq_puts(m, \"\\n\\tCapEff:\\t\");\n\tcap = cred->cap_effective;\n\tCAP_FOR_EACH_U32(__capi)\n\t\tseq_put_hex_ll(m, NULL, cap.cap[CAP_LAST_U32 - __capi], 8);\n\tseq_putc(m, '\\n');\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4108,
            "cve_id": "CVE-2023-23586",
            "code_snippet": "static inline void io_req_init_async(struct io_kiocb *req)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\n\tif (req->flags & REQ_F_WORK_INITIALIZED)\n\t\treturn;\n\n\t__io_req_init_async(req);\n\n\t/* Grab a ref if this isn't our static identity */\n\treq->work.identity = tctx->identity;\n\tif (tctx->identity != &tctx->__identity)\n\t\trefcount_inc(&req->work.identity->count);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4120,
            "cve_id": "CVE-2023-25012",
            "code_snippet": "static void bigben_set_led(struct led_classdev *led,\n\tenum led_brightness value)\n{\n\tstruct device *dev = led->dev->parent;\n\tstruct hid_device *hid = to_hid_device(dev);\n\tstruct bigben_device *bigben = hid_get_drvdata(hid);\n\tint n;\n\tbool work;\n\tunsigned long flags;\n\n\tif (!bigben) {\n\t\thid_err(hid, \"no device data\\n\");\n\t\treturn;\n\t}\n\n\tfor (n = 0; n < NUM_LEDS; n++) {\n\t\tif (led == bigben->leds[n]) {\n\t\t\tspin_lock_irqsave(&bigben->lock, flags);\n\t\t\tif (value == LED_OFF) {\n\t\t\t\twork = (bigben->led_state & BIT(n));\n\t\t\t\tbigben->led_state &= ~BIT(n);\n\t\t\t} else {\n\t\t\t\twork = !(bigben->led_state & BIT(n));\n\t\t\t\tbigben->led_state |= BIT(n);\n\t\t\t}\n\t\t\tspin_unlock_irqrestore(&bigben->lock, flags);\n\n\t\t\tif (work) {\n\t\t\t\tbigben->work_led = true;\n\t\t\t\tschedule_work(&bigben->worker);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4124,
            "cve_id": "CVE-2023-2513",
            "code_snippet": "int ext4_xattr_ibody_set(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_xattr_info *i,\n\t\t\t\tstruct ext4_xattr_ibody_find *is)\n{\n\tstruct ext4_xattr_ibody_header *header;\n\tstruct ext4_xattr_search *s = &is->s;\n\tint error;\n\n\tif (EXT4_I(inode)->i_extra_isize == 0)\n\t\treturn -ENOSPC;\n\terror = ext4_xattr_set_entry(i, s, handle, inode, false /* is_block */);\n\tif (error)\n\t\treturn error;\n\theader = IHDR(inode, ext4_raw_inode(&is->iloc));\n\tif (!IS_LAST_ENTRY(s->first)) {\n\t\theader->h_magic = cpu_to_le32(EXT4_XATTR_MAGIC);\n\t\text4_set_inode_state(inode, EXT4_STATE_XATTR);\n\t} else {\n\t\theader->h_magic = cpu_to_le32(0);\n\t\text4_clear_inode_state(inode, EXT4_STATE_XATTR);\n\t}\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4170,
            "cve_id": "CVE-2023-32233",
            "code_snippet": "static void nft_dynset_activate(const struct nft_ctx *ctx,\n\t\t\t\tconst struct nft_expr *expr)\n{\n\tstruct nft_dynset *priv = nft_expr_priv(expr);\n\n\tpriv->set->use++;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4229,
            "cve_id": "CVE-2023-3389",
            "code_snippet": "static __cold void io_uring_try_cancel_requests(struct io_ring_ctx *ctx,\n\t\t\t\t\t\tstruct task_struct *task,\n\t\t\t\t\t\tbool cancel_all)\n{\n\tstruct io_task_cancel cancel = { .task = task, .all = cancel_all, };\n\tstruct io_uring_task *tctx = task ? task->io_uring : NULL;\n\n\t/* failed during ring init, it couldn't have issued any requests */\n\tif (!ctx->rings)\n\t\treturn;\n\n\twhile (1) {\n\t\tenum io_wq_cancel cret;\n\t\tbool ret = false;\n\n\t\tif (!task) {\n\t\t\tret |= io_uring_try_cancel_iowq(ctx);\n\t\t} else if (tctx && tctx->io_wq) {\n\t\t\t/*\n\t\t\t * Cancels requests of all rings, not only @ctx, but\n\t\t\t * it's fine as the task is in exit/exec.\n\t\t\t */\n\t\t\tcret = io_wq_cancel_cb(tctx->io_wq, io_cancel_task_cb,\n\t\t\t\t\t       &cancel, true);\n\t\t\tret |= (cret != IO_WQ_CANCEL_NOTFOUND);\n\t\t}\n\n\t\t/* SQPOLL thread does its own polling */\n\t\tif ((!(ctx->flags & IORING_SETUP_SQPOLL) && cancel_all) ||\n\t\t    (ctx->sq_data && ctx->sq_data->thread == current)) {\n\t\t\twhile (!wq_list_empty(&ctx->iopoll_list)) {\n\t\t\t\tio_iopoll_try_reap_events(ctx);\n\t\t\t\tret = true;\n\t\t\t}\n\t\t}\n\n\t\tret |= io_cancel_defer_files(ctx, task, cancel_all);\n\t\tret |= io_poll_remove_all(ctx, task, cancel_all);\n\t\tret |= io_kill_timeouts(ctx, task, cancel_all);\n\t\tif (task)\n\t\t\tret |= io_run_task_work();\n\t\tif (!ret)\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4230,
            "cve_id": "CVE-2023-3389",
            "code_snippet": "static __cold struct io_ring_ctx *io_ring_ctx_alloc(struct io_uring_params *p)\n{\n\tstruct io_ring_ctx *ctx;\n\tint hash_bits;\n\n\tctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn NULL;\n\n\txa_init(&ctx->io_bl_xa);\n\n\t/*\n\t * Use 5 bits less than the max cq entries, that should give us around\n\t * 32 entries per hash list if totally full and uniformly spread, but\n\t * don't keep too many buckets to not overconsume memory.\n\t */\n\thash_bits = ilog2(p->cq_entries) - 5;\n\thash_bits = clamp(hash_bits, 1, 8);\n\tif (io_alloc_hash_table(&ctx->cancel_table, hash_bits))\n\t\tgoto err;\n\n\tctx->dummy_ubuf = kzalloc(sizeof(*ctx->dummy_ubuf), GFP_KERNEL);\n\tif (!ctx->dummy_ubuf)\n\t\tgoto err;\n\t/* set invalid range, so io_import_fixed() fails meeting it */\n\tctx->dummy_ubuf->ubuf = -1UL;\n\n\tif (percpu_ref_init(&ctx->refs, io_ring_ctx_ref_free,\n\t\t\t    PERCPU_REF_ALLOW_REINIT, GFP_KERNEL))\n\t\tgoto err;\n\n\tctx->flags = p->flags;\n\tinit_waitqueue_head(&ctx->sqo_sq_wait);\n\tINIT_LIST_HEAD(&ctx->sqd_list);\n\tINIT_LIST_HEAD(&ctx->cq_overflow_list);\n\tINIT_LIST_HEAD(&ctx->io_buffers_cache);\n\tINIT_LIST_HEAD(&ctx->apoll_cache);\n\tinit_completion(&ctx->ref_comp);\n\txa_init_flags(&ctx->personalities, XA_FLAGS_ALLOC1);\n\tmutex_init(&ctx->uring_lock);\n\tinit_waitqueue_head(&ctx->cq_wait);\n\tspin_lock_init(&ctx->completion_lock);\n\tspin_lock_init(&ctx->timeout_lock);\n\tINIT_WQ_LIST(&ctx->iopoll_list);\n\tINIT_LIST_HEAD(&ctx->io_buffers_pages);\n\tINIT_LIST_HEAD(&ctx->io_buffers_comp);\n\tINIT_LIST_HEAD(&ctx->defer_list);\n\tINIT_LIST_HEAD(&ctx->timeout_list);\n\tINIT_LIST_HEAD(&ctx->ltimeout_list);\n\tspin_lock_init(&ctx->rsrc_ref_lock);\n\tINIT_LIST_HEAD(&ctx->rsrc_ref_list);\n\tINIT_DELAYED_WORK(&ctx->rsrc_put_work, io_rsrc_put_work);\n\tinit_llist_head(&ctx->rsrc_put_llist);\n\tINIT_LIST_HEAD(&ctx->tctx_list);\n\tctx->submit_state.free_list.next = NULL;\n\tINIT_WQ_LIST(&ctx->locked_free_list);\n\tINIT_DELAYED_WORK(&ctx->fallback_work, io_fallback_req_func);\n\tINIT_WQ_LIST(&ctx->submit_state.compl_reqs);\n\treturn ctx;\nerr:\n\tkfree(ctx->dummy_ubuf);\n\tkfree(ctx->cancel_table.hbs);\n\tkfree(ctx->io_bl);\n\txa_destroy(&ctx->io_bl_xa);\n\tkfree(ctx);\n\treturn NULL;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4231,
            "cve_id": "CVE-2023-3389",
            "code_snippet": "static __cold void io_ring_ctx_free(struct io_ring_ctx *ctx)\n{\n\tio_sq_thread_finish(ctx);\n\n\tif (ctx->mm_account) {\n\t\tmmdrop(ctx->mm_account);\n\t\tctx->mm_account = NULL;\n\t}\n\n\tio_rsrc_refs_drop(ctx);\n\t/* __io_rsrc_put_work() may need uring_lock to progress, wait w/o it */\n\tio_wait_rsrc_data(ctx->buf_data);\n\tio_wait_rsrc_data(ctx->file_data);\n\n\tmutex_lock(&ctx->uring_lock);\n\tif (ctx->buf_data)\n\t\t__io_sqe_buffers_unregister(ctx);\n\tif (ctx->file_data)\n\t\t__io_sqe_files_unregister(ctx);\n\tif (ctx->rings)\n\t\t__io_cqring_overflow_flush(ctx, true);\n\tio_eventfd_unregister(ctx);\n\tio_flush_apoll_cache(ctx);\n\tmutex_unlock(&ctx->uring_lock);\n\tio_destroy_buffers(ctx);\n\tif (ctx->sq_creds)\n\t\tput_cred(ctx->sq_creds);\n\tif (ctx->submitter_task)\n\t\tput_task_struct(ctx->submitter_task);\n\n\t/* there are no registered resources left, nobody uses it */\n\tif (ctx->rsrc_node)\n\t\tio_rsrc_node_destroy(ctx->rsrc_node);\n\tif (ctx->rsrc_backup_node)\n\t\tio_rsrc_node_destroy(ctx->rsrc_backup_node);\n\tflush_delayed_work(&ctx->rsrc_put_work);\n\tflush_delayed_work(&ctx->fallback_work);\n\n\tWARN_ON_ONCE(!list_empty(&ctx->rsrc_ref_list));\n\tWARN_ON_ONCE(!llist_empty(&ctx->rsrc_put_llist));\n\n#if defined(CONFIG_UNIX)\n\tif (ctx->ring_sock) {\n\t\tctx->ring_sock->file = NULL; /* so that iput() is called */\n\t\tsock_release(ctx->ring_sock);\n\t}\n#endif\n\tWARN_ON_ONCE(!list_empty(&ctx->ltimeout_list));\n\n\tio_mem_free(ctx->rings);\n\tio_mem_free(ctx->sq_sqes);\n\n\tpercpu_ref_exit(&ctx->refs);\n\tfree_uid(ctx->user);\n\tio_req_caches_free(ctx);\n\tif (ctx->hash_map)\n\t\tio_wq_put_hash(ctx->hash_map);\n\tkfree(ctx->cancel_table.hbs);\n\tkfree(ctx->dummy_ubuf);\n\tkfree(ctx->io_bl);\n\txa_destroy(&ctx->io_bl_xa);\n\tkfree(ctx);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4232,
            "cve_id": "CVE-2023-3389",
            "code_snippet": "int io_poll_remove(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_poll_update *poll_update = io_kiocb_to_cmd(req);\n\tstruct io_cancel_data cd = { .data = poll_update->old_user_data, };\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_hash_bucket *bucket;\n\tstruct io_kiocb *preq;\n\tint ret2, ret = 0;\n\tbool locked;\n\n\tpreq = io_poll_find(ctx, true, &cd, &ctx->cancel_table, &bucket);\n\tif (preq)\n\t\tret2 = io_poll_disarm(preq);\n\tif (bucket)\n\t\tspin_unlock(&bucket->lock);\n\n\tif (!preq) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\tif (!ret2) {\n\t\tret = -EALREADY;\n\t\tgoto out;\n\t}\n\n\tif (poll_update->update_events || poll_update->update_user_data) {\n\t\t/* only mask one event flags, keep behavior flags */\n\t\tif (poll_update->update_events) {\n\t\t\tstruct io_poll *poll = io_kiocb_to_cmd(preq);\n\n\t\t\tpoll->events &= ~0xffff;\n\t\t\tpoll->events |= poll_update->events & 0xffff;\n\t\t\tpoll->events |= IO_POLL_UNMASK;\n\t\t}\n\t\tif (poll_update->update_user_data)\n\t\t\tpreq->cqe.user_data = poll_update->new_user_data;\n\n\t\tret2 = io_poll_add(preq, issue_flags);\n\t\t/* successfully updated, don't complete poll request */\n\t\tif (!ret2 || ret2 == -EIOCBQUEUED)\n\t\t\tgoto out;\n\t}\n\n\treq_set_fail(preq);\n\tio_req_set_res(preq, -ECANCELED, 0);\n\tlocked = !(issue_flags & IO_URING_F_UNLOCKED);\n\tio_req_task_complete(preq, &locked);\nout:\n\tif (ret < 0) {\n\t\treq_set_fail(req);\n\t\treturn ret;\n\t}\n\t/* complete update request, we're done with it */\n\tio_req_set_res(req, ret, 0);\n\treturn IOU_OK;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4233,
            "cve_id": "CVE-2023-3389",
            "code_snippet": "int io_arm_poll_handler(struct io_kiocb *req, unsigned issue_flags)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct async_poll *apoll;\n\tstruct io_poll_table ipt;\n\t__poll_t mask = POLLPRI | POLLERR | EPOLLET;\n\tint ret;\n\n\tif (!def->pollin && !def->pollout)\n\t\treturn IO_APOLL_ABORTED;\n\tif (!file_can_poll(req->file))\n\t\treturn IO_APOLL_ABORTED;\n\tif ((req->flags & (REQ_F_POLLED|REQ_F_PARTIAL_IO)) == REQ_F_POLLED)\n\t\treturn IO_APOLL_ABORTED;\n\tif (!(req->flags & REQ_F_APOLL_MULTISHOT))\n\t\tmask |= EPOLLONESHOT;\n\n\tif (def->pollin) {\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\n\t\t/* If reading from MSG_ERRQUEUE using recvmsg, ignore POLLIN */\n\t\tif (req->flags & REQ_F_CLEAR_POLLIN)\n\t\t\tmask &= ~EPOLLIN;\n\t} else {\n\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\t}\n\tif (def->poll_exclusive)\n\t\tmask |= EPOLLEXCLUSIVE;\n\tif (req->flags & REQ_F_POLLED) {\n\t\tapoll = req->apoll;\n\t\tkfree(apoll->double_poll);\n\t} else if (!(issue_flags & IO_URING_F_UNLOCKED) &&\n\t\t   !list_empty(&ctx->apoll_cache)) {\n\t\tapoll = list_first_entry(&ctx->apoll_cache, struct async_poll,\n\t\t\t\t\t\tpoll.wait.entry);\n\t\tlist_del_init(&apoll->poll.wait.entry);\n\t} else {\n\t\tapoll = kmalloc(sizeof(*apoll), GFP_ATOMIC);\n\t\tif (unlikely(!apoll))\n\t\t\treturn IO_APOLL_ABORTED;\n\t}\n\tapoll->double_poll = NULL;\n\treq->apoll = apoll;\n\treq->flags |= REQ_F_POLLED;\n\tipt.pt._qproc = io_async_queue_proc;\n\n\tio_kbuf_recycle(req, issue_flags);\n\n\tret = __io_arm_poll_handler(req, &apoll->poll, &ipt, mask);\n\tif (ret || ipt.error)\n\t\treturn ret ? IO_APOLL_READY : IO_APOLL_ABORTED;\n\n\ttrace_io_uring_poll_arm(ctx, req, req->cqe.user_data, req->opcode,\n\t\t\t\tmask, apoll->poll.events);\n\treturn IO_APOLL_OK;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4268,
            "cve_id": "CVE-2023-3439",
            "code_snippet": "void mctp_dev_put(struct mctp_dev *mdev)\n{\n\tif (mdev && refcount_dec_and_test(&mdev->refs)) {\n\t\tdev_put(mdev->dev);\n\t\tkfree_rcu(mdev, rcu);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4273,
            "cve_id": "CVE-2023-35823",
            "code_snippet": "int saa7134_vbi_fini(struct saa7134_dev *dev)\n{\n\t/* nothing */\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4282,
            "cve_id": "CVE-2023-3610",
            "code_snippet": "static void nft_rule_expr_activate(const struct nft_ctx *ctx,\n\t\t\t\t   struct nft_rule *rule)\n{\n\tstruct nft_expr *expr;\n\n\texpr = nft_expr_first(rule);\n\twhile (nft_expr_more(rule, expr)) {\n\t\tif (expr->ops->activate)\n\t\t\texpr->ops->activate(ctx, expr);\n\n\t\texpr = nft_expr_next(expr);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4283,
            "cve_id": "CVE-2023-3610",
            "code_snippet": "static struct nft_trans *nft_trans_chain_add(struct nft_ctx *ctx, int msg_type)\n{\n\tstruct nft_trans *trans;\n\n\ttrans = nft_trans_alloc(ctx, msg_type, sizeof(struct nft_trans_chain));\n\tif (trans == NULL)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (msg_type == NFT_MSG_NEWCHAIN) {\n\t\tnft_activate_next(ctx->net, ctx->chain);\n\n\t\tif (ctx->nla[NFTA_CHAIN_ID]) {\n\t\t\tnft_trans_chain_id(trans) =\n\t\t\t\tntohl(nla_get_be32(ctx->nla[NFTA_CHAIN_ID]));\n\t\t}\n\t}\n\n\tnft_trans_commit_list_add_tail(ctx->net, trans);\n\treturn trans;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4284,
            "cve_id": "CVE-2023-3610",
            "code_snippet": "void nft_data_hold(const struct nft_data *data, enum nft_data_types type)\n{\n\tstruct nft_chain *chain;\n\tstruct nft_rule *rule;\n\n\tif (type == NFT_DATA_VERDICT) {\n\t\tswitch (data->verdict.code) {\n\t\tcase NFT_JUMP:\n\t\tcase NFT_GOTO:\n\t\t\tchain = data->verdict.chain;\n\t\t\tchain->use++;\n\n\t\t\tif (!nft_chain_is_bound(chain))\n\t\t\t\tbreak;\n\n\t\t\tchain->table->use++;\n\t\t\tlist_for_each_entry(rule, &chain->rules, list)\n\t\t\t\tchain->use++;\n\n\t\t\tnft_chain_add(chain->table, chain);\n\t\t\tbreak;\n\t\t}\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4285,
            "cve_id": "CVE-2023-3610",
            "code_snippet": "static void nft_rule_expr_deactivate(const struct nft_ctx *ctx,\n\t\t\t\t     struct nft_rule *rule,\n\t\t\t\t     enum nft_trans_phase phase)\n{\n\tstruct nft_expr *expr;\n\n\texpr = nft_expr_first(rule);\n\twhile (nft_expr_more(rule, expr)) {\n\t\tif (expr->ops->deactivate)\n\t\t\texpr->ops->deactivate(ctx, expr, phase);\n\n\t\texpr = nft_expr_next(expr);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4286,
            "cve_id": "CVE-2023-3610",
            "code_snippet": "static void nf_tables_rule_destroy(const struct nft_ctx *ctx,\n\t\t\t\t   struct nft_rule *rule)\n{\n\tstruct nft_expr *expr, *next;\n\n\t/*\n\t * Careful: some expressions might not be initialized in case this\n\t * is called on error from nf_tables_newrule().\n\t */\n\texpr = nft_expr_first(rule);\n\twhile (nft_expr_more(rule, expr)) {\n\t\tnext = nft_expr_next(expr);\n\t\tnf_tables_expr_destroy(ctx, expr);\n\t\texpr = next;\n\t}\n\tkfree(rule);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4287,
            "cve_id": "CVE-2023-3610",
            "code_snippet": "static int __nf_tables_abort(struct net *net, enum nfnl_abort_action action)\n{\n\tstruct nftables_pernet *nft_net = nft_pernet(net);\n\tstruct nft_trans *trans, *next;\n\tLIST_HEAD(set_update_list);\n\tstruct nft_trans_elem *te;\n\n\tif (action == NFNL_ABORT_VALIDATE &&\n\t    nf_tables_validate(net) < 0)\n\t\treturn -EAGAIN;\n\n\tlist_for_each_entry_safe_reverse(trans, next, &nft_net->commit_list,\n\t\t\t\t\t list) {\n\t\tswitch (trans->msg_type) {\n\t\tcase NFT_MSG_NEWTABLE:\n\t\t\tif (nft_trans_table_update(trans)) {\n\t\t\t\tif (!(trans->ctx.table->flags & __NFT_TABLE_F_UPDATE)) {\n\t\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (trans->ctx.table->flags & __NFT_TABLE_F_WAS_DORMANT) {\n\t\t\t\t\tnf_tables_table_disable(net, trans->ctx.table);\n\t\t\t\t\ttrans->ctx.table->flags |= NFT_TABLE_F_DORMANT;\n\t\t\t\t} else if (trans->ctx.table->flags & __NFT_TABLE_F_WAS_AWAKEN) {\n\t\t\t\t\ttrans->ctx.table->flags &= ~NFT_TABLE_F_DORMANT;\n\t\t\t\t}\n\t\t\t\ttrans->ctx.table->flags &= ~__NFT_TABLE_F_UPDATE;\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t} else {\n\t\t\t\tlist_del_rcu(&trans->ctx.table->list);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELTABLE:\n\t\tcase NFT_MSG_DESTROYTABLE:\n\t\t\tnft_clear(trans->ctx.net, trans->ctx.table);\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWCHAIN:\n\t\t\tif (nft_trans_chain_update(trans)) {\n\t\t\t\tnft_netdev_unregister_hooks(net,\n\t\t\t\t\t\t\t    &nft_trans_chain_hooks(trans),\n\t\t\t\t\t\t\t    true);\n\t\t\t\tfree_percpu(nft_trans_chain_stats(trans));\n\t\t\t\tkfree(nft_trans_chain_name(trans));\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t} else {\n\t\t\t\tif (nft_chain_is_bound(trans->ctx.chain)) {\n\t\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\ttrans->ctx.table->use--;\n\t\t\t\tnft_chain_del(trans->ctx.chain);\n\t\t\t\tnf_tables_unregister_hook(trans->ctx.net,\n\t\t\t\t\t\t\t  trans->ctx.table,\n\t\t\t\t\t\t\t  trans->ctx.chain);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELCHAIN:\n\t\tcase NFT_MSG_DESTROYCHAIN:\n\t\t\tif (nft_trans_chain_update(trans)) {\n\t\t\t\tlist_splice(&nft_trans_chain_hooks(trans),\n\t\t\t\t\t    &nft_trans_basechain(trans)->hook_list);\n\t\t\t} else {\n\t\t\t\ttrans->ctx.table->use++;\n\t\t\t\tnft_clear(trans->ctx.net, trans->ctx.chain);\n\t\t\t}\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWRULE:\n\t\t\ttrans->ctx.chain->use--;\n\t\t\tlist_del_rcu(&nft_trans_rule(trans)->list);\n\t\t\tnft_rule_expr_deactivate(&trans->ctx,\n\t\t\t\t\t\t nft_trans_rule(trans),\n\t\t\t\t\t\t NFT_TRANS_ABORT);\n\t\t\tif (trans->ctx.chain->flags & NFT_CHAIN_HW_OFFLOAD)\n\t\t\t\tnft_flow_rule_destroy(nft_trans_flow_rule(trans));\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELRULE:\n\t\tcase NFT_MSG_DESTROYRULE:\n\t\t\ttrans->ctx.chain->use++;\n\t\t\tnft_clear(trans->ctx.net, nft_trans_rule(trans));\n\t\t\tnft_rule_expr_activate(&trans->ctx, nft_trans_rule(trans));\n\t\t\tif (trans->ctx.chain->flags & NFT_CHAIN_HW_OFFLOAD)\n\t\t\t\tnft_flow_rule_destroy(nft_trans_flow_rule(trans));\n\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWSET:\n\t\t\tif (nft_trans_set_update(trans)) {\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttrans->ctx.table->use--;\n\t\t\tif (nft_trans_set_bound(trans)) {\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tlist_del_rcu(&nft_trans_set(trans)->list);\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELSET:\n\t\tcase NFT_MSG_DESTROYSET:\n\t\t\ttrans->ctx.table->use++;\n\t\t\tnft_clear(trans->ctx.net, nft_trans_set(trans));\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWSETELEM:\n\t\t\tif (nft_trans_elem_set_bound(trans)) {\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tte = (struct nft_trans_elem *)trans->data;\n\t\t\tnft_setelem_remove(net, te->set, &te->elem);\n\t\t\tif (!nft_setelem_is_catchall(te->set, &te->elem))\n\t\t\t\tatomic_dec(&te->set->nelems);\n\n\t\t\tif (te->set->ops->abort &&\n\t\t\t    list_empty(&te->set->pending_update)) {\n\t\t\t\tlist_add_tail(&te->set->pending_update,\n\t\t\t\t\t      &set_update_list);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELSETELEM:\n\t\tcase NFT_MSG_DESTROYSETELEM:\n\t\t\tte = (struct nft_trans_elem *)trans->data;\n\n\t\t\tnft_setelem_data_activate(net, te->set, &te->elem);\n\t\t\tnft_setelem_activate(net, te->set, &te->elem);\n\t\t\tif (!nft_setelem_is_catchall(te->set, &te->elem))\n\t\t\t\tte->set->ndeact--;\n\n\t\t\tif (te->set->ops->abort &&\n\t\t\t    list_empty(&te->set->pending_update)) {\n\t\t\t\tlist_add_tail(&te->set->pending_update,\n\t\t\t\t\t      &set_update_list);\n\t\t\t}\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWOBJ:\n\t\t\tif (nft_trans_obj_update(trans)) {\n\t\t\t\tnft_obj_destroy(&trans->ctx, nft_trans_obj_newobj(trans));\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t} else {\n\t\t\t\ttrans->ctx.table->use--;\n\t\t\t\tnft_obj_del(nft_trans_obj(trans));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELOBJ:\n\t\tcase NFT_MSG_DESTROYOBJ:\n\t\t\ttrans->ctx.table->use++;\n\t\t\tnft_clear(trans->ctx.net, nft_trans_obj(trans));\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWFLOWTABLE:\n\t\t\tif (nft_trans_flowtable_update(trans)) {\n\t\t\t\tnft_unregister_flowtable_net_hooks(net,\n\t\t\t\t\t\t&nft_trans_flowtable_hooks(trans));\n\t\t\t} else {\n\t\t\t\ttrans->ctx.table->use--;\n\t\t\t\tlist_del_rcu(&nft_trans_flowtable(trans)->list);\n\t\t\t\tnft_unregister_flowtable_net_hooks(net,\n\t\t\t\t\t\t&nft_trans_flowtable(trans)->hook_list);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELFLOWTABLE:\n\t\tcase NFT_MSG_DESTROYFLOWTABLE:\n\t\t\tif (nft_trans_flowtable_update(trans)) {\n\t\t\t\tlist_splice(&nft_trans_flowtable_hooks(trans),\n\t\t\t\t\t    &nft_trans_flowtable(trans)->hook_list);\n\t\t\t} else {\n\t\t\t\ttrans->ctx.table->use++;\n\t\t\t\tnft_clear(trans->ctx.net, nft_trans_flowtable(trans));\n\t\t\t}\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tnft_set_abort_update(&set_update_list);\n\n\tsynchronize_rcu();\n\n\tlist_for_each_entry_safe_reverse(trans, next,\n\t\t\t\t\t &nft_net->commit_list, list) {\n\t\tlist_del(&trans->list);\n\t\tnf_tables_abort_release(trans);\n\t}\n\n\tif (action == NFNL_ABORT_AUTOLOAD)\n\t\tnf_tables_module_autoload(net);\n\telse\n\t\tnf_tables_module_autoload_cleanup(net);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4307,
            "cve_id": "CVE-2023-3863",
            "code_snippet": "void nfc_llcp_mac_is_up(struct nfc_dev *dev, u32 target_idx,\n\t\t\tu8 comm_mode, u8 rf_mode)\n{\n\tstruct nfc_llcp_local *local;\n\n\tpr_debug(\"rf mode %d\\n\", rf_mode);\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL)\n\t\treturn;\n\n\tlocal->target_idx = target_idx;\n\tlocal->comm_mode = comm_mode;\n\tlocal->rf_mode = rf_mode;\n\n\tif (rf_mode == NFC_RF_INITIATOR) {\n\t\tpr_debug(\"Queueing Tx work\\n\");\n\n\t\tschedule_work(&local->tx_work);\n\t} else {\n\t\tmod_timer(&local->link_timer,\n\t\t\t  jiffies + msecs_to_jiffies(local->remote_lto));\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4308,
            "cve_id": "CVE-2023-3863",
            "code_snippet": "struct nfc_llcp_local *nfc_llcp_find_local(struct nfc_dev *dev)\n{\n\tstruct nfc_llcp_local *local;\n\n\tlist_for_each_entry(local, &llcp_devices, list)\n\t\tif (local->dev == dev)\n\t\t\treturn local;\n\n\tpr_debug(\"No device found\\n\");\n\n\treturn NULL;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4309,
            "cve_id": "CVE-2023-3863",
            "code_snippet": "void nfc_llcp_unregister_device(struct nfc_dev *dev)\n{\n\tstruct nfc_llcp_local *local = nfc_llcp_find_local(dev);\n\n\tif (local == NULL) {\n\t\tpr_debug(\"No such device\\n\");\n\t\treturn;\n\t}\n\n\tlocal_cleanup(local);\n\n\tnfc_llcp_local_put(local);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4310,
            "cve_id": "CVE-2023-3863",
            "code_snippet": "struct nfc_llcp_local *nfc_llcp_local_get(struct nfc_llcp_local *local)\n{\n\tkref_get(&local->ref);\n\n\treturn local;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4311,
            "cve_id": "CVE-2023-3863",
            "code_snippet": "u8 *nfc_llcp_general_bytes(struct nfc_dev *dev, size_t *general_bytes_len)\n{\n\tstruct nfc_llcp_local *local;\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL) {\n\t\t*general_bytes_len = 0;\n\t\treturn NULL;\n\t}\n\n\tnfc_llcp_build_gb(local);\n\n\t*general_bytes_len = local->gb_len;\n\n\treturn local->gb;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4312,
            "cve_id": "CVE-2023-3863",
            "code_snippet": "static void local_release(struct kref *ref)\n{\n\tstruct nfc_llcp_local *local;\n\n\tlocal = container_of(ref, struct nfc_llcp_local, ref);\n\n\tlist_del(&local->list);\n\tlocal_cleanup(local);\n\tkfree(local);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4313,
            "cve_id": "CVE-2023-3863",
            "code_snippet": "int nfc_llcp_data_received(struct nfc_dev *dev, struct sk_buff *skb)\n{\n\tstruct nfc_llcp_local *local;\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL) {\n\t\tkfree_skb(skb);\n\t\treturn -ENODEV;\n\t}\n\n\t__nfc_llcp_recv(local, skb);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4349,
            "cve_id": "CVE-2023-39198",
            "code_snippet": "int qxl_alloc_surf_ioctl(struct drm_device *dev, void *data, struct drm_file *file)\n{\n\tstruct qxl_device *qdev = to_qxl(dev);\n\tstruct drm_qxl_alloc_surf *param = data;\n\tstruct qxl_bo *qobj;\n\tint handle;\n\tint ret;\n\tint size, actual_stride;\n\tstruct qxl_surface surf;\n\n\t/* work out size allocate bo with handle */\n\tactual_stride = param->stride < 0 ? -param->stride : param->stride;\n\tsize = actual_stride * param->height + actual_stride;\n\n\tsurf.format = param->format;\n\tsurf.width = param->width;\n\tsurf.height = param->height;\n\tsurf.stride = param->stride;\n\tsurf.data = 0;\n\n\tret = qxl_gem_object_create_with_handle(qdev, file,\n\t\t\t\t\t\tQXL_GEM_DOMAIN_SURFACE,\n\t\t\t\t\t\tsize,\n\t\t\t\t\t\t&surf,\n\t\t\t\t\t\t&qobj, &handle);\n\tif (ret) {\n\t\tDRM_ERROR(\"%s: failed to create gem ret=%d\\n\",\n\t\t\t  __func__, ret);\n\t\treturn -ENOMEM;\n\t} else\n\t\tparam->handle = handle;\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4365,
            "cve_id": "CVE-2023-4244",
            "code_snippet": "static int nft_rcv_nl_event(struct notifier_block *this, unsigned long event,\n\t\t\t    void *ptr)\n{\n\tstruct nft_table *table, *to_delete[8];\n\tstruct nftables_pernet *nft_net;\n\tstruct netlink_notify *n = ptr;\n\tstruct net *net = n->net;\n\tunsigned int deleted;\n\tbool restart = false;\n\n\tif (event != NETLINK_URELEASE || n->protocol != NETLINK_NETFILTER)\n\t\treturn NOTIFY_DONE;\n\n\tnft_net = nft_pernet(net);\n\tdeleted = 0;\n\tmutex_lock(&nft_net->commit_mutex);\n\tif (!list_empty(&nf_tables_destroy_list))\n\t\trcu_barrier();\nagain:\n\tlist_for_each_entry(table, &nft_net->tables, list) {\n\t\tif (nft_table_has_owner(table) &&\n\t\t    n->portid == table->nlpid) {\n\t\t\t__nft_release_hook(net, table);\n\t\t\tlist_del_rcu(&table->list);\n\t\t\tto_delete[deleted++] = table;\n\t\t\tif (deleted >= ARRAY_SIZE(to_delete))\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tif (deleted) {\n\t\trestart = deleted >= ARRAY_SIZE(to_delete);\n\t\tsynchronize_rcu();\n\t\twhile (deleted)\n\t\t\t__nft_release_table(net, to_delete[--deleted]);\n\n\t\tif (restart)\n\t\t\tgoto again;\n\t}\n\tmutex_unlock(&nft_net->commit_mutex);\n\n\treturn NOTIFY_DONE;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4391,
            "cve_id": "CVE-2023-45898",
            "code_snippet": "void ext4_es_remove_extent(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t   ext4_lblk_t len)\n{\n\text4_lblk_t end;\n\tint err = 0;\n\tint reserved = 0;\n\tstruct extent_status *es = NULL;\n\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn;\n\n\ttrace_ext4_es_remove_extent(inode, lblk, len);\n\tes_debug(\"remove [%u/%u) from extent status tree of inode %lu\\n\",\n\t\t lblk, len, inode->i_ino);\n\n\tif (!len)\n\t\treturn;\n\n\tend = lblk + len - 1;\n\tBUG_ON(end < lblk);\n\nretry:\n\tif (err && !es)\n\t\tes = __es_alloc_extent(true);\n\t/*\n\t * ext4_clear_inode() depends on us taking i_es_lock unconditionally\n\t * so that we are sure __es_shrink() is done with the inode before it\n\t * is reclaimed.\n\t */\n\twrite_lock(&EXT4_I(inode)->i_es_lock);\n\terr = __es_remove_extent(inode, lblk, end, &reserved, es);\n\tif (es && !es->es_len)\n\t\t__es_free_extent(es);\n\twrite_unlock(&EXT4_I(inode)->i_es_lock);\n\tif (err)\n\t\tgoto retry;\n\n\text4_es_print_tree(inode);\n\text4_da_release_space(inode, reserved);\n\treturn;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4398,
            "cve_id": "CVE-2023-4611",
            "code_snippet": "\nSYSCALL_DEFINE4(set_mempolicy_home_node, unsigned long, start, unsigned long, len,\n\t\tunsigned long, home_node, unsigned long, flags)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma, *prev;\n\tstruct mempolicy *new, *old;\n\tunsigned long end;\n\tint err = -ENOENT;\n\tVMA_ITERATOR(vmi, mm, start);\n\n\tstart = untagged_addr(start);\n\tif (start & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\t/*\n\t * flags is used for future extension if any.\n\t */\n\tif (flags != 0)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Check home_node is online to avoid accessing uninitialized\n\t * NODE_DATA.\n\t */\n\tif (home_node >= MAX_NUMNODES || !node_online(home_node))\n\t\treturn -EINVAL;\n\n\tlen = PAGE_ALIGN(len);\n\tend = start + len;\n\n\tif (end < start)\n\t\treturn -EINVAL;\n\tif (end == start)\n\t\treturn 0;\n\tmmap_write_lock(mm);\n\tprev = vma_prev(&vmi);\n\tfor_each_vma_range(vmi, vma, end) {\n\t\t/*\n\t\t * If any vma in the range got policy other than MPOL_BIND\n\t\t * or MPOL_PREFERRED_MANY we return error. We don't reset\n\t\t * the home node for vmas we already updated before.\n\t\t */\n\t\told = vma_policy(vma);\n\t\tif (!old)\n\t\t\tcontinue;\n\t\tif (old->mode != MPOL_BIND && old->mode != MPOL_PREFERRED_MANY) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tbreak;\n\t\t}\n\t\tnew = mpol_dup(old);\n\t\tif (IS_ERR(new)) {\n\t\t\terr = PTR_ERR(new);\n\t\t\tbreak;\n\t\t}\n\n\t\tnew->home_node = home_node;\n\t\terr = mbind_range(&vmi, vma, &prev, start, end, new);\n\t\tmpol_put(new);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\tmmap_write_unlock(mm);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4417,
            "cve_id": "CVE-2023-4921",
            "code_snippet": "static struct sk_buff *qfq_dequeue(struct Qdisc *sch)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_aggregate *in_serv_agg = q->in_serv_agg;\n\tstruct qfq_class *cl;\n\tstruct sk_buff *skb = NULL;\n\t/* next-packet len, 0 means no more active classes in in-service agg */\n\tunsigned int len = 0;\n\n\tif (in_serv_agg == NULL)\n\t\treturn NULL;\n\n\tif (!list_empty(&in_serv_agg->active))\n\t\tskb = qfq_peek_skb(in_serv_agg, &cl, &len);\n\n\t/*\n\t * If there are no active classes in the in-service aggregate,\n\t * or if the aggregate has not enough budget to serve its next\n\t * class, then choose the next aggregate to serve.\n\t */\n\tif (len == 0 || in_serv_agg->budget < len) {\n\t\tcharge_actual_service(in_serv_agg);\n\n\t\t/* recharge the budget of the aggregate */\n\t\tin_serv_agg->initial_budget = in_serv_agg->budget =\n\t\t\tin_serv_agg->budgetmax;\n\n\t\tif (!list_empty(&in_serv_agg->active)) {\n\t\t\t/*\n\t\t\t * Still active: reschedule for\n\t\t\t * service. Possible optimization: if no other\n\t\t\t * aggregate is active, then there is no point\n\t\t\t * in rescheduling this aggregate, and we can\n\t\t\t * just keep it as the in-service one. This\n\t\t\t * should be however a corner case, and to\n\t\t\t * handle it, we would need to maintain an\n\t\t\t * extra num_active_aggs field.\n\t\t\t*/\n\t\t\tqfq_update_agg_ts(q, in_serv_agg, requeue);\n\t\t\tqfq_schedule_agg(q, in_serv_agg);\n\t\t} else if (sch->q.qlen == 0) { /* no aggregate to serve */\n\t\t\tq->in_serv_agg = NULL;\n\t\t\treturn NULL;\n\t\t}\n\n\t\t/*\n\t\t * If we get here, there are other aggregates queued:\n\t\t * choose the new aggregate to serve.\n\t\t */\n\t\tin_serv_agg = q->in_serv_agg = qfq_choose_next_agg(q);\n\t\tskb = qfq_peek_skb(in_serv_agg, &cl, &len);\n\t}\n\tif (!skb)\n\t\treturn NULL;\n\n\tqdisc_qstats_backlog_dec(sch, skb);\n\tsch->q.qlen--;\n\tqdisc_bstats_update(sch, skb);\n\n\tagg_dequeue(in_serv_agg, cl, len);\n\t/* If lmax is lowered, through qfq_change_class, for a class\n\t * owning pending packets with larger size than the new value\n\t * of lmax, then the following condition may hold.\n\t */\n\tif (unlikely(in_serv_agg->budget < len))\n\t\tin_serv_agg->budget = 0;\n\telse\n\t\tin_serv_agg->budget -= len;\n\n\tq->V += (u64)len * q->iwsum;\n\tpr_debug(\"qfq dequeue: len %u F %lld now %lld\\n\",\n\t\t len, (unsigned long long) in_serv_agg->F,\n\t\t (unsigned long long) q->V);\n\n\treturn skb;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4425,
            "cve_id": "CVE-2023-5197",
            "code_snippet": "static int nft_flush_table(struct nft_ctx *ctx)\n{\n\tstruct nft_flowtable *flowtable, *nft;\n\tstruct nft_chain *chain, *nc;\n\tstruct nft_object *obj, *ne;\n\tstruct nft_set *set, *ns;\n\tint err;\n\n\tlist_for_each_entry(chain, &ctx->table->chains, list) {\n\t\tif (!nft_is_active_next(ctx->net, chain))\n\t\t\tcontinue;\n\n\t\tif (nft_chain_is_bound(chain))\n\t\t\tcontinue;\n\n\t\tctx->chain = chain;\n\n\t\terr = nft_delrule_by_chain(ctx);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tlist_for_each_entry_safe(set, ns, &ctx->table->sets, list) {\n\t\tif (!nft_is_active_next(ctx->net, set))\n\t\t\tcontinue;\n\n\t\tif (nft_set_is_anonymous(set) &&\n\t\t    !list_empty(&set->bindings))\n\t\t\tcontinue;\n\n\t\terr = nft_delset(ctx, set);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tlist_for_each_entry_safe(flowtable, nft, &ctx->table->flowtables, list) {\n\t\tif (!nft_is_active_next(ctx->net, flowtable))\n\t\t\tcontinue;\n\n\t\terr = nft_delflowtable(ctx, flowtable);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tlist_for_each_entry_safe(obj, ne, &ctx->table->objects, list) {\n\t\tif (!nft_is_active_next(ctx->net, obj))\n\t\t\tcontinue;\n\n\t\terr = nft_delobj(ctx, obj);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tlist_for_each_entry_safe(chain, nc, &ctx->table->chains, list) {\n\t\tif (!nft_is_active_next(ctx->net, chain))\n\t\t\tcontinue;\n\n\t\tif (nft_chain_is_bound(chain))\n\t\t\tcontinue;\n\n\t\tctx->chain = chain;\n\n\t\terr = nft_delchain(ctx);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\terr = nft_deltable(ctx);\nout:\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4426,
            "cve_id": "CVE-2023-5197",
            "code_snippet": "static int nf_tables_delrule(struct sk_buff *skb, const struct nfnl_info *info,\n\t\t\t     const struct nlattr * const nla[])\n{\n\tstruct netlink_ext_ack *extack = info->extack;\n\tu8 genmask = nft_genmask_next(info->net);\n\tu8 family = info->nfmsg->nfgen_family;\n\tstruct nft_chain *chain = NULL;\n\tstruct net *net = info->net;\n\tstruct nft_table *table;\n\tstruct nft_rule *rule;\n\tstruct nft_ctx ctx;\n\tint err = 0;\n\n\ttable = nft_table_lookup(net, nla[NFTA_RULE_TABLE], family, genmask,\n\t\t\t\t NETLINK_CB(skb).portid);\n\tif (IS_ERR(table)) {\n\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_TABLE]);\n\t\treturn PTR_ERR(table);\n\t}\n\n\tif (nla[NFTA_RULE_CHAIN]) {\n\t\tchain = nft_chain_lookup(net, table, nla[NFTA_RULE_CHAIN],\n\t\t\t\t\t genmask);\n\t\tif (IS_ERR(chain)) {\n\t\t\tif (PTR_ERR(chain) == -ENOENT &&\n\t\t\t    NFNL_MSG_TYPE(info->nlh->nlmsg_type) == NFT_MSG_DESTROYRULE)\n\t\t\t\treturn 0;\n\n\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_CHAIN]);\n\t\t\treturn PTR_ERR(chain);\n\t\t}\n\t\tif (nft_chain_is_bound(chain))\n\t\t\treturn -EOPNOTSUPP;\n\t}\n\n\tnft_ctx_init(&ctx, net, skb, info->nlh, family, table, chain, nla);\n\n\tif (chain) {\n\t\tif (nla[NFTA_RULE_HANDLE]) {\n\t\t\trule = nft_rule_lookup(chain, nla[NFTA_RULE_HANDLE]);\n\t\t\tif (IS_ERR(rule)) {\n\t\t\t\tif (PTR_ERR(rule) == -ENOENT &&\n\t\t\t\t    NFNL_MSG_TYPE(info->nlh->nlmsg_type) == NFT_MSG_DESTROYRULE)\n\t\t\t\t\treturn 0;\n\n\t\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_HANDLE]);\n\t\t\t\treturn PTR_ERR(rule);\n\t\t\t}\n\n\t\t\terr = nft_delrule(&ctx, rule);\n\t\t} else if (nla[NFTA_RULE_ID]) {\n\t\t\trule = nft_rule_lookup_byid(net, chain, nla[NFTA_RULE_ID]);\n\t\t\tif (IS_ERR(rule)) {\n\t\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_ID]);\n\t\t\t\treturn PTR_ERR(rule);\n\t\t\t}\n\n\t\t\terr = nft_delrule(&ctx, rule);\n\t\t} else {\n\t\t\terr = nft_delrule_by_chain(&ctx);\n\t\t}\n\t} else {\n\t\tlist_for_each_entry(chain, &table->chains, list) {\n\t\t\tif (!nft_is_active_next(net, chain))\n\t\t\t\tcontinue;\n\t\t\tif (nft_chain_is_bound(chain))\n\t\t\t\tcontinue;\n\n\t\t\tctx.chain = chain;\n\t\t\terr = nft_delrule_by_chain(&ctx);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4431,
            "cve_id": "CVE-2023-5633",
            "code_snippet": "int vmw_user_bo_synccpu_ioctl(struct drm_device *dev, void *data,\n\t\t\t      struct drm_file *file_priv)\n{\n\tstruct drm_vmw_synccpu_arg *arg =\n\t\t(struct drm_vmw_synccpu_arg *) data;\n\tstruct vmw_bo *vbo;\n\tint ret;\n\n\tif ((arg->flags & (drm_vmw_synccpu_read | drm_vmw_synccpu_write)) == 0\n\t    || (arg->flags & ~(drm_vmw_synccpu_read | drm_vmw_synccpu_write |\n\t\t\t       drm_vmw_synccpu_dontblock |\n\t\t\t       drm_vmw_synccpu_allow_cs)) != 0) {\n\t\tDRM_ERROR(\"Illegal synccpu flags.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (arg->op) {\n\tcase drm_vmw_synccpu_grab:\n\t\tret = vmw_user_bo_lookup(file_priv, arg->handle, &vbo);\n\t\tif (unlikely(ret != 0))\n\t\t\treturn ret;\n\n\t\tret = vmw_user_bo_synccpu_grab(vbo, arg->flags);\n\t\tvmw_user_bo_unref(vbo);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tif (ret == -ERESTARTSYS || ret == -EBUSY)\n\t\t\t\treturn -EBUSY;\n\t\t\tDRM_ERROR(\"Failed synccpu grab on handle 0x%08x.\\n\",\n\t\t\t\t  (unsigned int) arg->handle);\n\t\t\treturn ret;\n\t\t}\n\t\tbreak;\n\tcase drm_vmw_synccpu_release:\n\t\tret = vmw_user_bo_synccpu_release(file_priv,\n\t\t\t\t\t\t  arg->handle,\n\t\t\t\t\t\t  arg->flags);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Failed synccpu release on handle 0x%08x.\\n\",\n\t\t\t\t  (unsigned int) arg->handle);\n\t\t\treturn ret;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Invalid synccpu operation.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4432,
            "cve_id": "CVE-2023-5633",
            "code_snippet": "int vmw_user_bo_lookup(struct drm_file *filp,\n\t\t       u32 handle,\n\t\t       struct vmw_bo **out)\n{\n\tstruct drm_gem_object *gobj;\n\n\tgobj = drm_gem_object_lookup(filp, handle);\n\tif (!gobj) {\n\t\tDRM_ERROR(\"Invalid buffer object handle 0x%08lx.\\n\",\n\t\t\t  (unsigned long)handle);\n\t\treturn -ESRCH;\n\t}\n\n\t*out = to_vmw_bo(gobj);\n\tttm_bo_get(&(*out)->tbo);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4433,
            "cve_id": "CVE-2023-5633",
            "code_snippet": "static int vmw_user_bo_synccpu_release(struct drm_file *filp,\n\t\t\t\t       uint32_t handle,\n\t\t\t\t       uint32_t flags)\n{\n\tstruct vmw_bo *vmw_bo;\n\tint ret = vmw_user_bo_lookup(filp, handle, &vmw_bo);\n\n\tif (!ret) {\n\t\tif (!(flags & drm_vmw_synccpu_allow_cs)) {\n\t\t\tatomic_dec(&vmw_bo->cpu_writers);\n\t\t}\n\t\tvmw_user_bo_unref(vmw_bo);\n\t}\n\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4434,
            "cve_id": "CVE-2023-5633",
            "code_snippet": "static int vmw_cotable_resize(struct vmw_resource *res, size_t new_size)\n{\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct vmw_private *dev_priv = res->dev_priv;\n\tstruct vmw_cotable *vcotbl = vmw_cotable(res);\n\tstruct vmw_bo *buf, *old_buf = res->guest_memory_bo;\n\tstruct ttm_buffer_object *bo, *old_bo = &res->guest_memory_bo->tbo;\n\tsize_t old_size = res->guest_memory_size;\n\tsize_t old_size_read_back = vcotbl->size_read_back;\n\tsize_t cur_size_read_back;\n\tstruct ttm_bo_kmap_obj old_map, new_map;\n\tint ret;\n\tsize_t i;\n\tstruct vmw_bo_params bo_params = {\n\t\t.domain = VMW_BO_DOMAIN_MOB,\n\t\t.busy_domain = VMW_BO_DOMAIN_MOB,\n\t\t.bo_type = ttm_bo_type_device,\n\t\t.size = new_size,\n\t\t.pin = true\n\t};\n\n\tMKS_STAT_TIME_DECL(MKSSTAT_KERN_COTABLE_RESIZE);\n\tMKS_STAT_TIME_PUSH(MKSSTAT_KERN_COTABLE_RESIZE);\n\n\tret = vmw_cotable_readback(res);\n\tif (ret)\n\t\tgoto out_done;\n\n\tcur_size_read_back = vcotbl->size_read_back;\n\tvcotbl->size_read_back = old_size_read_back;\n\n\t/*\n\t * While device is processing, Allocate and reserve a buffer object\n\t * for the new COTable. Initially pin the buffer object to make sure\n\t * we can use tryreserve without failure.\n\t */\n\tret = vmw_bo_create(dev_priv, &bo_params, &buf);\n\tif (ret) {\n\t\tDRM_ERROR(\"Failed initializing new cotable MOB.\\n\");\n\t\tgoto out_done;\n\t}\n\n\tbo = &buf->tbo;\n\tWARN_ON_ONCE(ttm_bo_reserve(bo, false, true, NULL));\n\n\tret = ttm_bo_wait(old_bo, false, false);\n\tif (unlikely(ret != 0)) {\n\t\tDRM_ERROR(\"Failed waiting for cotable unbind.\\n\");\n\t\tgoto out_wait;\n\t}\n\n\t/*\n\t * Do a page by page copy of COTables. This eliminates slow vmap()s.\n\t * This should really be a TTM utility.\n\t */\n\tfor (i = 0; i < PFN_UP(old_bo->resource->size); ++i) {\n\t\tbool dummy;\n\n\t\tret = ttm_bo_kmap(old_bo, i, 1, &old_map);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Failed mapping old COTable on resize.\\n\");\n\t\t\tgoto out_wait;\n\t\t}\n\t\tret = ttm_bo_kmap(bo, i, 1, &new_map);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Failed mapping new COTable on resize.\\n\");\n\t\t\tgoto out_map_new;\n\t\t}\n\t\tmemcpy(ttm_kmap_obj_virtual(&new_map, &dummy),\n\t\t       ttm_kmap_obj_virtual(&old_map, &dummy),\n\t\t       PAGE_SIZE);\n\t\tttm_bo_kunmap(&new_map);\n\t\tttm_bo_kunmap(&old_map);\n\t}\n\n\t/* Unpin new buffer, and switch backup buffers. */\n\tvmw_bo_placement_set(buf,\n\t\t\t     VMW_BO_DOMAIN_MOB,\n\t\t\t     VMW_BO_DOMAIN_MOB);\n\tret = ttm_bo_validate(bo, &buf->placement, &ctx);\n\tif (unlikely(ret != 0)) {\n\t\tDRM_ERROR(\"Failed validating new COTable backup buffer.\\n\");\n\t\tgoto out_wait;\n\t}\n\n\tvmw_resource_mob_detach(res);\n\tres->guest_memory_bo = buf;\n\tres->guest_memory_size = new_size;\n\tvcotbl->size_read_back = cur_size_read_back;\n\n\t/*\n\t * Now tell the device to switch. If this fails, then we need to\n\t * revert the full resize.\n\t */\n\tret = vmw_cotable_unscrub(res);\n\tif (ret) {\n\t\tDRM_ERROR(\"Failed switching COTable backup buffer.\\n\");\n\t\tres->guest_memory_bo = old_buf;\n\t\tres->guest_memory_size = old_size;\n\t\tvcotbl->size_read_back = old_size_read_back;\n\t\tvmw_resource_mob_attach(res);\n\t\tgoto out_wait;\n\t}\n\n\tvmw_resource_mob_attach(res);\n\t/* Let go of the old mob. */\n\tvmw_bo_unreference(&old_buf);\n\tres->id = vcotbl->type;\n\n\tret = dma_resv_reserve_fences(bo->base.resv, 1);\n\tif (unlikely(ret))\n\t\tgoto out_wait;\n\n\t/* Release the pin acquired in vmw_bo_create */\n\tttm_bo_unpin(bo);\n\n\tMKS_STAT_TIME_POP(MKSSTAT_KERN_COTABLE_RESIZE);\n\n\treturn 0;\n\nout_map_new:\n\tttm_bo_kunmap(&old_map);\nout_wait:\n\tttm_bo_unpin(bo);\n\tttm_bo_unreserve(bo);\n\tvmw_bo_unreference(&buf);\n\nout_done:\n\tMKS_STAT_TIME_POP(MKSSTAT_KERN_COTABLE_RESIZE);\n\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4435,
            "cve_id": "CVE-2023-5633",
            "code_snippet": "static int vmw_translate_guest_ptr(struct vmw_private *dev_priv,\n\t\t\t\t   struct vmw_sw_context *sw_context,\n\t\t\t\t   SVGAGuestPtr *ptr,\n\t\t\t\t   struct vmw_bo **vmw_bo_p)\n{\n\tstruct vmw_bo *vmw_bo;\n\tuint32_t handle = ptr->gmrId;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use GMR region.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM,\n\t\t\t     VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM);\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n\tvmw_user_bo_unref(vmw_bo);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->location = ptr;\n\treloc->vbo = vmw_bo;\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4436,
            "cve_id": "CVE-2023-5633",
            "code_snippet": "static int vmw_translate_mob_ptr(struct vmw_private *dev_priv,\n\t\t\t\t struct vmw_sw_context *sw_context,\n\t\t\t\t SVGAMobId *id,\n\t\t\t\t struct vmw_bo **vmw_bo_p)\n{\n\tstruct vmw_bo *vmw_bo;\n\tuint32_t handle = *id;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use MOB buffer.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_MOB, VMW_BO_DOMAIN_MOB);\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n\tvmw_user_bo_unref(vmw_bo);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->mob_loc = id;\n\treloc->vbo = vmw_bo;\n\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4437,
            "cve_id": "CVE-2023-5633",
            "code_snippet": "int vmw_gem_object_create_with_handle(struct vmw_private *dev_priv,\n\t\t\t\t      struct drm_file *filp,\n\t\t\t\t      uint32_t size,\n\t\t\t\t      uint32_t *handle,\n\t\t\t\t      struct vmw_bo **p_vbo)\n{\n\tint ret;\n\tstruct vmw_bo_params params = {\n\t\t.domain = (dev_priv->has_mob) ? VMW_BO_DOMAIN_SYS : VMW_BO_DOMAIN_VRAM,\n\t\t.busy_domain = VMW_BO_DOMAIN_SYS,\n\t\t.bo_type = ttm_bo_type_device,\n\t\t.size = size,\n\t\t.pin = false\n\t};\n\n\tret = vmw_bo_create(dev_priv, &params, p_vbo);\n\tif (ret != 0)\n\t\tgoto out_no_bo;\n\n\t(*p_vbo)->tbo.base.funcs = &vmw_gem_object_funcs;\n\n\tret = drm_gem_handle_create(filp, &(*p_vbo)->tbo.base, handle);\nout_no_bo:\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4438,
            "cve_id": "CVE-2023-5633",
            "code_snippet": "static int vmw_create_bo_proxy(struct drm_device *dev,\n\t\t\t       const struct drm_mode_fb_cmd2 *mode_cmd,\n\t\t\t       struct vmw_bo *bo_mob,\n\t\t\t       struct vmw_surface **srf_out)\n{\n\tstruct vmw_surface_metadata metadata = {0};\n\tuint32_t format;\n\tstruct vmw_resource *res;\n\tunsigned int bytes_pp;\n\tint ret;\n\n\tswitch (mode_cmd->pixel_format) {\n\tcase DRM_FORMAT_ARGB8888:\n\tcase DRM_FORMAT_XRGB8888:\n\t\tformat = SVGA3D_X8R8G8B8;\n\t\tbytes_pp = 4;\n\t\tbreak;\n\n\tcase DRM_FORMAT_RGB565:\n\tcase DRM_FORMAT_XRGB1555:\n\t\tformat = SVGA3D_R5G6B5;\n\t\tbytes_pp = 2;\n\t\tbreak;\n\n\tcase 8:\n\t\tformat = SVGA3D_P8;\n\t\tbytes_pp = 1;\n\t\tbreak;\n\n\tdefault:\n\t\tDRM_ERROR(\"Invalid framebuffer format %p4cc\\n\",\n\t\t\t  &mode_cmd->pixel_format);\n\t\treturn -EINVAL;\n\t}\n\n\tmetadata.format = format;\n\tmetadata.mip_levels[0] = 1;\n\tmetadata.num_sizes = 1;\n\tmetadata.base_size.width = mode_cmd->pitches[0] / bytes_pp;\n\tmetadata.base_size.height =  mode_cmd->height;\n\tmetadata.base_size.depth = 1;\n\tmetadata.scanout = true;\n\n\tret = vmw_gb_surface_define(vmw_priv(dev), &metadata, srf_out);\n\tif (ret) {\n\t\tDRM_ERROR(\"Failed to allocate proxy content buffer\\n\");\n\t\treturn ret;\n\t}\n\n\tres = &(*srf_out)->res;\n\n\t/* Reserve and switch the backing mob. */\n\tmutex_lock(&res->dev_priv->cmdbuf_mutex);\n\t(void) vmw_resource_reserve(res, false, true);\n\tvmw_bo_unreference(&res->guest_memory_bo);\n\tres->guest_memory_bo = vmw_bo_reference(bo_mob);\n\tres->guest_memory_offset = 0;\n\tvmw_resource_unreserve(res, false, false, false, NULL, 0);\n\tmutex_unlock(&res->dev_priv->cmdbuf_mutex);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4439,
            "cve_id": "CVE-2023-5633",
            "code_snippet": "static struct drm_framebuffer *vmw_kms_fb_create(struct drm_device *dev,\n\t\t\t\t\t\t struct drm_file *file_priv,\n\t\t\t\t\t\t const struct drm_mode_fb_cmd2 *mode_cmd)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct vmw_framebuffer *vfb = NULL;\n\tstruct vmw_surface *surface = NULL;\n\tstruct vmw_bo *bo = NULL;\n\tint ret;\n\n\t/* returns either a bo or surface */\n\tret = vmw_user_lookup_handle(dev_priv, file_priv,\n\t\t\t\t     mode_cmd->handles[0],\n\t\t\t\t     &surface, &bo);\n\tif (ret) {\n\t\tDRM_ERROR(\"Invalid buffer object handle %u (0x%x).\\n\",\n\t\t\t  mode_cmd->handles[0], mode_cmd->handles[0]);\n\t\tgoto err_out;\n\t}\n\n\n\tif (!bo &&\n\t    !vmw_kms_srf_ok(dev_priv, mode_cmd->width, mode_cmd->height)) {\n\t\tDRM_ERROR(\"Surface size cannot exceed %dx%d\\n\",\n\t\t\tdev_priv->texture_max_width,\n\t\t\tdev_priv->texture_max_height);\n\t\tgoto err_out;\n\t}\n\n\n\tvfb = vmw_kms_new_framebuffer(dev_priv, bo, surface,\n\t\t\t\t      !(dev_priv->capabilities & SVGA_CAP_3D),\n\t\t\t\t      mode_cmd);\n\tif (IS_ERR(vfb)) {\n\t\tret = PTR_ERR(vfb);\n\t\tgoto err_out;\n\t}\n\nerr_out:\n\t/* vmw_user_lookup_handle takes one ref so does new_fb */\n\tif (bo)\n\t\tvmw_user_bo_unref(bo);\n\tif (surface)\n\t\tvmw_surface_unreference(&surface);\n\n\tif (ret) {\n\t\tDRM_ERROR(\"failed to create vmw_framebuffer: %i\\n\", ret);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\treturn &vfb->base;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4440,
            "cve_id": "CVE-2023-5633",
            "code_snippet": "int vmw_overlay_ioctl(struct drm_device *dev, void *data,\n\t\t      struct drm_file *file_priv)\n{\n\tstruct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct vmw_overlay *overlay = dev_priv->overlay_priv;\n\tstruct drm_vmw_control_stream_arg *arg =\n\t    (struct drm_vmw_control_stream_arg *)data;\n\tstruct vmw_bo *buf;\n\tstruct vmw_resource *res;\n\tint ret;\n\n\tif (!vmw_overlay_available(dev_priv))\n\t\treturn -ENOSYS;\n\n\tret = vmw_user_stream_lookup(dev_priv, tfile, &arg->stream_id, &res);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&overlay->mutex);\n\n\tif (!arg->enabled) {\n\t\tret = vmw_overlay_stop(dev_priv, arg->stream_id, false, true);\n\t\tgoto out_unlock;\n\t}\n\n\tret = vmw_user_bo_lookup(file_priv, arg->handle, &buf);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tret = vmw_overlay_update_stream(dev_priv, buf, arg, true);\n\n\tvmw_user_bo_unref(buf);\n\nout_unlock:\n\tmutex_unlock(&overlay->mutex);\n\tvmw_resource_unreference(&res);\n\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4460,
            "cve_id": "CVE-2023-6111",
            "code_snippet": "static void nft_setelem_catchall_remove(const struct net *net,\n\t\t\t\t\tconst struct nft_set *set,\n\t\t\t\t\tstruct nft_elem_priv *elem_priv)\n{\n\tstruct nft_set_elem_catchall *catchall, *next;\n\n\tlist_for_each_entry_safe(catchall, next, &set->catchall_list, list) {\n\t\tif (catchall->elem == elem_priv) {\n\t\t\tlist_del_rcu(&catchall->list);\n\t\t\tkfree_rcu(catchall, rcu);\n\t\t\tbreak;\n\t\t}\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        }
    ],
    "non_vul_data": [
        {
            "id": 528,
            "cve_id": "CVE-2014-0203",
            "code_snippet": "static void *proc_pid_follow_link(struct dentry *dentry, struct nameidata *nd)\n{\n\tstruct inode *inode = dentry->d_inode;\n\tint error = -EACCES;\n\n\t/* We don't need a base pointer in the /proc filesystem */\n\tpath_put(&nd->path);\n\n\t/* Are we allowed to snoop on the tasks file descriptors? */\n\tif (!proc_fd_access_allowed(inode))\n\t\tgoto out;\n\n\terror = PROC_I(inode)->op.proc_get_link(inode, &nd->path);\nout:\n\treturn ERR_PTR(error);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 550,
            "cve_id": "CVE-2014-2568",
            "code_snippet": "static int queue_userspace_packet(struct datapath *dp, struct sk_buff *skb,\n\t\t\t\t  const struct dp_upcall_info *upcall_info)\n{\n\tstruct ovs_header *upcall;\n\tstruct sk_buff *nskb = NULL;\n\tstruct sk_buff *user_skb; /* to be queued to userspace */\n\tstruct nlattr *nla;\n\tstruct genl_info info = {\n\t\t.dst_sk = ovs_dp_get_net(dp)->genl_sock,\n\t\t.snd_portid = upcall_info->portid,\n\t};\n\tsize_t len;\n\tunsigned int hlen;\n\tint err, dp_ifindex;\n\n\tdp_ifindex = get_dpifindex(dp);\n\tif (!dp_ifindex)\n\t\treturn -ENODEV;\n\n\tif (vlan_tx_tag_present(skb)) {\n\t\tnskb = skb_clone(skb, GFP_ATOMIC);\n\t\tif (!nskb)\n\t\t\treturn -ENOMEM;\n\n\t\tnskb = __vlan_put_tag(nskb, nskb->vlan_proto, vlan_tx_tag_get(nskb));\n\t\tif (!nskb)\n\t\t\treturn -ENOMEM;\n\n\t\tnskb->vlan_tci = 0;\n\t\tskb = nskb;\n\t}\n\n\tif (nla_attr_size(skb->len) > USHRT_MAX) {\n\t\terr = -EFBIG;\n\t\tgoto out;\n\t}\n\n\t/* Complete checksum if needed */\n\tif (skb->ip_summed == CHECKSUM_PARTIAL &&\n\t    (err = skb_checksum_help(skb)))\n\t\tgoto out;\n\n\t/* Older versions of OVS user space enforce alignment of the last\n\t * Netlink attribute to NLA_ALIGNTO which would require extensive\n\t * padding logic. Only perform zerocopy if padding is not required.\n\t */\n\tif (dp->user_features & OVS_DP_F_UNALIGNED)\n\t\thlen = skb_zerocopy_headlen(skb);\n\telse\n\t\thlen = skb->len;\n\n\tlen = upcall_msg_size(upcall_info->userdata, hlen);\n\tuser_skb = genlmsg_new_unicast(len, &info, GFP_ATOMIC);\n\tif (!user_skb) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tupcall = genlmsg_put(user_skb, 0, 0, &dp_packet_genl_family,\n\t\t\t     0, upcall_info->cmd);\n\tupcall->dp_ifindex = dp_ifindex;\n\n\tnla = nla_nest_start(user_skb, OVS_PACKET_ATTR_KEY);\n\tovs_nla_put_flow(upcall_info->key, upcall_info->key, user_skb);\n\tnla_nest_end(user_skb, nla);\n\n\tif (upcall_info->userdata)\n\t\t__nla_put(user_skb, OVS_PACKET_ATTR_USERDATA,\n\t\t\t  nla_len(upcall_info->userdata),\n\t\t\t  nla_data(upcall_info->userdata));\n\n\t/* Only reserve room for attribute header, packet data is added\n\t * in skb_zerocopy() */\n\tif (!(nla = nla_reserve(user_skb, OVS_PACKET_ATTR_PACKET, 0))) {\n\t\terr = -ENOBUFS;\n\t\tgoto out;\n\t}\n\tnla->nla_len = nla_attr_size(skb->len);\n\n\terr = skb_zerocopy(user_skb, skb, skb->len, hlen);\n\tif (err)\n\t\tgoto out;\n\n\t/* Pad OVS_PACKET_ATTR_PACKET if linear copy was performed */\n\tif (!(dp->user_features & OVS_DP_F_UNALIGNED)) {\n\t\tsize_t plen = NLA_ALIGN(user_skb->len) - user_skb->len;\n\n\t\tif (plen > 0)\n\t\t\tmemset(skb_put(user_skb, plen), 0, plen);\n\t}\n\n\t((struct nlmsghdr *) user_skb->data)->nlmsg_len = user_skb->len;\n\n\terr = genlmsg_unicast(ovs_dp_get_net(dp), user_skb, upcall_info->portid);\nout:\n\tif (err)\n\t\tskb_tx_error(skb);\n\tkfree_skb(nskb);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 633,
            "cve_id": "CVE-2014-4653",
            "code_snippet": "static int snd_ctl_tlv_ioctl(struct snd_ctl_file *file,\n                             struct snd_ctl_tlv __user *_tlv,\n                             int op_flag)\n{\n\tstruct snd_card *card = file->card;\n\tstruct snd_ctl_tlv tlv;\n\tstruct snd_kcontrol *kctl;\n\tstruct snd_kcontrol_volatile *vd;\n\tunsigned int len;\n\tint err = 0;\n\n\tif (copy_from_user(&tlv, _tlv, sizeof(tlv)))\n\t\treturn -EFAULT;\n\tif (tlv.length < sizeof(unsigned int) * 2)\n\t\treturn -EINVAL;\n\tdown_read(&card->controls_rwsem);\n\tkctl = snd_ctl_find_numid(card, tlv.numid);\n\tif (kctl == NULL) {\n\t\terr = -ENOENT;\n\t\tgoto __kctl_end;\n\t}\n\tif (kctl->tlv.p == NULL) {\n\t\terr = -ENXIO;\n\t\tgoto __kctl_end;\n\t}\n\tvd = &kctl->vd[tlv.numid - kctl->id.numid];\n\tif ((op_flag == 0 && (vd->access & SNDRV_CTL_ELEM_ACCESS_TLV_READ) == 0) ||\n\t    (op_flag > 0 && (vd->access & SNDRV_CTL_ELEM_ACCESS_TLV_WRITE) == 0) ||\n\t    (op_flag < 0 && (vd->access & SNDRV_CTL_ELEM_ACCESS_TLV_COMMAND) == 0)) {\n\t    \terr = -ENXIO;\n\t    \tgoto __kctl_end;\n\t}\n\tif (vd->access & SNDRV_CTL_ELEM_ACCESS_TLV_CALLBACK) {\n\t\tif (vd->owner != NULL && vd->owner != file) {\n\t\t\terr = -EPERM;\n\t\t\tgoto __kctl_end;\n\t\t}\n\t\terr = kctl->tlv.c(kctl, op_flag, tlv.length, _tlv->tlv);\n\t\tif (err > 0) {\n\t\t\tstruct snd_ctl_elem_id id = kctl->id;\n\t\t\tup_read(&card->controls_rwsem);\n\t\t\tsnd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_TLV, &id);\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\tif (op_flag) {\n\t\t\terr = -ENXIO;\n\t\t\tgoto __kctl_end;\n\t\t}\n\t\tlen = kctl->tlv.p[1] + 2 * sizeof(unsigned int);\n\t\tif (tlv.length < len) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto __kctl_end;\n\t\t}\n\t\tif (copy_to_user(_tlv->tlv, kctl->tlv.p, len))\n\t\t\terr = -EFAULT;\n\t}\n      __kctl_end:\n\tup_read(&card->controls_rwsem);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 939,
            "cve_id": "CVE-2015-8963",
            "code_snippet": "static void perf_event_init_cpu(int cpu)\n{\n\tstruct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);\n\n\tmutex_lock(&swhash->hlist_mutex);\n\tif (swhash->hlist_refcount > 0) {\n\t\tstruct swevent_hlist *hlist;\n\n\t\thlist = kzalloc_node(sizeof(*hlist), GFP_KERNEL, cpu_to_node(cpu));\n\t\tWARN_ON(!hlist);\n\t\trcu_assign_pointer(swhash->swevent_hlist, hlist);\n\t}\n\tmutex_unlock(&swhash->hlist_mutex);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 961,
            "cve_id": "CVE-2016-10088",
            "code_snippet": "static ssize_t\nsg_write(struct file *filp, const char __user *buf, size_t count, loff_t * ppos)\n{\n\tint mxsize, cmd_size, k;\n\tint input_size, blocking;\n\tunsigned char opcode;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tSg_request *srp;\n\tstruct sg_header old_hdr;\n\tsg_io_hdr_t *hp;\n\tunsigned char cmnd[SG_MAX_CDB_SIZE];\n\n\tif (unlikely(segment_eq(get_fs(), KERNEL_DS)))\n\t\treturn -EINVAL;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_write: count=%d\\n\", (int) count));\n\tif (atomic_read(&sdp->detaching))\n\t\treturn -ENODEV;\n\tif (!((filp->f_flags & O_NONBLOCK) ||\n\t      scsi_block_when_processing_errors(sdp->device)))\n\t\treturn -ENXIO;\n\n\tif (!access_ok(VERIFY_READ, buf, count))\n\t\treturn -EFAULT;\t/* protects following copy_from_user()s + get_user()s */\n\tif (count < SZ_SG_HEADER)\n\t\treturn -EIO;\n\tif (__copy_from_user(&old_hdr, buf, SZ_SG_HEADER))\n\t\treturn -EFAULT;\n\tblocking = !(filp->f_flags & O_NONBLOCK);\n\tif (old_hdr.reply_len < 0)\n\t\treturn sg_new_write(sfp, filp, buf, count,\n\t\t\t\t    blocking, 0, 0, NULL);\n\tif (count < (SZ_SG_HEADER + 6))\n\t\treturn -EIO;\t/* The minimum scsi command length is 6 bytes. */\n\n\tif (!(srp = sg_add_request(sfp))) {\n\t\tSCSI_LOG_TIMEOUT(1, sg_printk(KERN_INFO, sdp,\n\t\t\t\t\t      \"sg_write: queue full\\n\"));\n\t\treturn -EDOM;\n\t}\n\tbuf += SZ_SG_HEADER;\n\t__get_user(opcode, buf);\n\tif (sfp->next_cmd_len > 0) {\n\t\tcmd_size = sfp->next_cmd_len;\n\t\tsfp->next_cmd_len = 0;\t/* reset so only this write() effected */\n\t} else {\n\t\tcmd_size = COMMAND_SIZE(opcode);\t/* based on SCSI command group */\n\t\tif ((opcode >= 0xc0) && old_hdr.twelve_byte)\n\t\t\tcmd_size = 12;\n\t}\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sdp,\n\t\t\"sg_write:   scsi opcode=0x%02x, cmd_size=%d\\n\", (int) opcode, cmd_size));\n/* Determine buffer size.  */\n\tinput_size = count - cmd_size;\n\tmxsize = (input_size > old_hdr.reply_len) ? input_size : old_hdr.reply_len;\n\tmxsize -= SZ_SG_HEADER;\n\tinput_size -= SZ_SG_HEADER;\n\tif (input_size < 0) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EIO;\t/* User did not pass enough bytes for this command. */\n\t}\n\thp = &srp->header;\n\thp->interface_id = '\\0';\t/* indicator of old interface tunnelled */\n\thp->cmd_len = (unsigned char) cmd_size;\n\thp->iovec_count = 0;\n\thp->mx_sb_len = 0;\n\tif (input_size > 0)\n\t\thp->dxfer_direction = (old_hdr.reply_len > SZ_SG_HEADER) ?\n\t\t    SG_DXFER_TO_FROM_DEV : SG_DXFER_TO_DEV;\n\telse\n\t\thp->dxfer_direction = (mxsize > 0) ? SG_DXFER_FROM_DEV : SG_DXFER_NONE;\n\thp->dxfer_len = mxsize;\n\tif ((hp->dxfer_direction == SG_DXFER_TO_DEV) ||\n\t    (hp->dxfer_direction == SG_DXFER_TO_FROM_DEV))\n\t\thp->dxferp = (char __user *)buf + cmd_size;\n\telse\n\t\thp->dxferp = NULL;\n\thp->sbp = NULL;\n\thp->timeout = old_hdr.reply_len;\t/* structure abuse ... */\n\thp->flags = input_size;\t/* structure abuse ... */\n\thp->pack_id = old_hdr.pack_id;\n\thp->usr_ptr = NULL;\n\tif (__copy_from_user(cmnd, buf, cmd_size))\n\t\treturn -EFAULT;\n\t/*\n\t * SG_DXFER_TO_FROM_DEV is functionally equivalent to SG_DXFER_FROM_DEV,\n\t * but is is possible that the app intended SG_DXFER_TO_DEV, because there\n\t * is a non-zero input_size, so emit a warning.\n\t */\n\tif (hp->dxfer_direction == SG_DXFER_TO_FROM_DEV) {\n\t\tstatic char cmd[TASK_COMM_LEN];\n\t\tif (strcmp(current->comm, cmd)) {\n\t\t\tprintk_ratelimited(KERN_WARNING\n\t\t\t\t\t   \"sg_write: data in/out %d/%d bytes \"\n\t\t\t\t\t   \"for SCSI command 0x%x-- guessing \"\n\t\t\t\t\t   \"data in;\\n   program %s not setting \"\n\t\t\t\t\t   \"count and/or reply_len properly\\n\",\n\t\t\t\t\t   old_hdr.reply_len - (int)SZ_SG_HEADER,\n\t\t\t\t\t   input_size, (unsigned int) cmnd[0],\n\t\t\t\t\t   current->comm);\n\t\t\tstrcpy(cmd, current->comm);\n\t\t}\n\t}\n\tk = sg_common_write(sfp, srp, cmnd, sfp->timeout, blocking);\n\treturn (k < 0) ? k : count;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 966,
            "cve_id": "CVE-2016-10200",
            "code_snippet": "static int l2tp_ip6_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_l2tpip6 *addr = (struct sockaddr_l2tpip6 *) uaddr;\n\tstruct net *net = sock_net(sk);\n\t__be32 v4addr = 0;\n\tint addr_type;\n\tint err;\n\n\tif (addr->l2tp_family != AF_INET6)\n\t\treturn -EINVAL;\n\tif (addr_len < sizeof(*addr))\n\t\treturn -EINVAL;\n\n\taddr_type = ipv6_addr_type(&addr->l2tp_addr);\n\n\t/* l2tp_ip6 sockets are IPv6 only */\n\tif (addr_type == IPV6_ADDR_MAPPED)\n\t\treturn -EADDRNOTAVAIL;\n\n\t/* L2TP is point-point, not multicast */\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -EADDRNOTAVAIL;\n\n\terr = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip6_lock);\n\tif (__l2tp_ip6_bind_lookup(net, &addr->l2tp_addr,\n\t\t\t\t   sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\tread_unlock_bh(&l2tp_ip6_lock);\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\tgoto out_unlock;\n\n\tif (sk->sk_state != TCP_CLOSE)\n\t\tgoto out_unlock;\n\n\t/* Check if the address belongs to the host. */\n\trcu_read_lock();\n\tif (addr_type != IPV6_ADDR_ANY) {\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t\t    addr->l2tp_scope_id) {\n\t\t\t\t/* Override any existing binding, if another\n\t\t\t\t * one is supplied by user.\n\t\t\t\t */\n\t\t\t\tsk->sk_bound_dev_if = addr->l2tp_scope_id;\n\t\t\t}\n\n\t\t\t/* Binding to link-local address requires an\n\t\t\t   interface */\n\t\t\tif (!sk->sk_bound_dev_if)\n\t\t\t\tgoto out_unlock_rcu;\n\n\t\t\terr = -ENODEV;\n\t\t\tdev = dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\t\tif (!dev)\n\t\t\t\tgoto out_unlock_rcu;\n\t\t}\n\n\t\t/* ipv4 addr of the socket is invalid.  Only the\n\t\t * unspecified and mapped address have a v4 equivalent.\n\t\t */\n\t\tv4addr = LOOPBACK4_IPV6;\n\t\terr = -EADDRNOTAVAIL;\n\t\tif (!ipv6_chk_addr(sock_net(sk), &addr->l2tp_addr, dev, 0))\n\t\t\tgoto out_unlock_rcu;\n\t}\n\trcu_read_unlock();\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = v4addr;\n\tsk->sk_v6_rcv_saddr = addr->l2tp_addr;\n\tnp->saddr = addr->l2tp_addr;\n\n\tl2tp_ip6_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip6_lock);\n\tsk_add_bind_node(sk, &l2tp_ip6_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip6_lock);\n\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\trelease_sock(sk);\n\treturn 0;\n\nout_unlock_rcu:\n\trcu_read_unlock();\nout_unlock:\n\trelease_sock(sk);\n\treturn err;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip6_lock);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 975,
            "cve_id": "CVE-2016-10905",
            "code_snippet": "static int read_rindex_entry(struct gfs2_inode *ip)\n{\n\tstruct gfs2_sbd *sdp = GFS2_SB(&ip->i_inode);\n\tconst unsigned bsize = sdp->sd_sb.sb_bsize;\n\tloff_t pos = sdp->sd_rgrps * sizeof(struct gfs2_rindex);\n\tstruct gfs2_rindex buf;\n\tint error;\n\tstruct gfs2_rgrpd *rgd;\n\n\tif (pos >= i_size_read(&ip->i_inode))\n\t\treturn 1;\n\n\terror = gfs2_internal_read(ip, (char *)&buf, &pos,\n\t\t\t\t   sizeof(struct gfs2_rindex));\n\n\tif (error != sizeof(struct gfs2_rindex))\n\t\treturn (error == 0) ? 1 : error;\n\n\trgd = kmem_cache_zalloc(gfs2_rgrpd_cachep, GFP_NOFS);\n\terror = -ENOMEM;\n\tif (!rgd)\n\t\treturn error;\n\n\trgd->rd_sbd = sdp;\n\trgd->rd_addr = be64_to_cpu(buf.ri_addr);\n\trgd->rd_length = be32_to_cpu(buf.ri_length);\n\trgd->rd_data0 = be64_to_cpu(buf.ri_data0);\n\trgd->rd_data = be32_to_cpu(buf.ri_data);\n\trgd->rd_bitbytes = be32_to_cpu(buf.ri_bitbytes);\n\tspin_lock_init(&rgd->rd_rsspin);\n\n\terror = compute_bitstructs(rgd);\n\tif (error)\n\t\tgoto fail;\n\n\terror = gfs2_glock_get(sdp, rgd->rd_addr,\n\t\t\t       &gfs2_rgrp_glops, CREATE, &rgd->rd_gl);\n\tif (error)\n\t\tgoto fail;\n\n\trgd->rd_rgl = (struct gfs2_rgrp_lvb *)rgd->rd_gl->gl_lksb.sb_lvbptr;\n\trgd->rd_flags &= ~(GFS2_RDF_UPTODATE | GFS2_RDF_PREFERRED);\n\tif (rgd->rd_data > sdp->sd_max_rg_data)\n\t\tsdp->sd_max_rg_data = rgd->rd_data;\n\tspin_lock(&sdp->sd_rindex_spin);\n\terror = rgd_insert(rgd);\n\tspin_unlock(&sdp->sd_rindex_spin);\n\tif (!error) {\n\t\trgd->rd_gl->gl_object = rgd;\n\t\trgd->rd_gl->gl_vm.start = (rgd->rd_addr * bsize) & PAGE_MASK;\n\t\trgd->rd_gl->gl_vm.end = PAGE_ALIGN((rgd->rd_addr +\n\t\t\t\t\t\t    rgd->rd_length) * bsize) - 1;\n\t\treturn 0;\n\t}\n\n\terror = 0; /* someone else read in the rgrp; free it and ignore it */\n\tgfs2_glock_put(rgd->rd_gl);\n\nfail:\n\tkfree(rgd->rd_bits);\n\trgd->rd_bits = NULL;\n\tkmem_cache_free(gfs2_rgrpd_cachep, rgd);\n\treturn error;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 977,
            "cve_id": "CVE-2016-10906",
            "code_snippet": "static void arc_emac_tx_clean(struct net_device *ndev)\n{\n\tstruct arc_emac_priv *priv = netdev_priv(ndev);\n\tstruct net_device_stats *stats = &ndev->stats;\n\tunsigned int i;\n\n\tfor (i = 0; i < TX_BD_NUM; i++) {\n\t\tunsigned int *txbd_dirty = &priv->txbd_dirty;\n\t\tstruct arc_emac_bd *txbd = &priv->txbd[*txbd_dirty];\n\t\tstruct buffer_state *tx_buff = &priv->tx_buff[*txbd_dirty];\n\t\tstruct sk_buff *skb = tx_buff->skb;\n\t\tunsigned int info = le32_to_cpu(txbd->info);\n\n\t\tif ((info & FOR_EMAC) || !txbd->data || !skb)\n\t\t\tbreak;\n\n\t\tif (unlikely(info & (DROP | DEFR | LTCL | UFLO))) {\n\t\t\tstats->tx_errors++;\n\t\t\tstats->tx_dropped++;\n\n\t\t\tif (info & DEFR)\n\t\t\t\tstats->tx_carrier_errors++;\n\n\t\t\tif (info & LTCL)\n\t\t\t\tstats->collisions++;\n\n\t\t\tif (info & UFLO)\n\t\t\t\tstats->tx_fifo_errors++;\n\t\t} else if (likely(info & FIRST_OR_LAST_MASK)) {\n\t\t\tstats->tx_packets++;\n\t\t\tstats->tx_bytes += skb->len;\n\t\t}\n\n\t\tdma_unmap_single(&ndev->dev, dma_unmap_addr(tx_buff, addr),\n\t\t\t\t dma_unmap_len(tx_buff, len), DMA_TO_DEVICE);\n\n\t\t/* return the sk_buff to system */\n\t\tdev_kfree_skb_irq(skb);\n\n\t\ttxbd->data = 0;\n\t\ttxbd->info = 0;\n\t\ttx_buff->skb = NULL;\n\n\t\t*txbd_dirty = (*txbd_dirty + 1) % TX_BD_NUM;\n\t}\n\n\t/* Ensure that txbd_dirty is visible to tx() before checking\n\t * for queue stopped.\n\t */\n\tsmp_mb();\n\n\tif (netif_queue_stopped(ndev) && arc_emac_tx_avail(priv))\n\t\tnetif_wake_queue(ndev);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1057,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "static int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *)uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct ipv6_txoptions *opt;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel & IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\t/*\n\t * connect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t * DCCP over IPv4\n\t */\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\n\t\terr = dccp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &dccp_ipv6_af_ops;\n\t\t\tsk->sk_backlog_rcv = dccp_v6_do_rcv;\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (saddr == NULL) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet6_hash_connect(&dccp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tdp->dccps_iss = secure_dccpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t      sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t      inet->inet_sport,\n\t\t\t\t\t\t      inet->inet_dport);\n\terr = dccp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\tdccp_set_state(sk, DCCP_CLOSED);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1058,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "static int dccp_v6_send_response(const struct sock *sk, struct request_sock *req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct in6_addr *final_p, final;\n\tstruct flowi6 fl6;\n\tint err = -1;\n\tstruct dst_entry *dst;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\tfl6.saddr = ireq->ir_v6_loc_addr;\n\tfl6.flowlabel = 0;\n\tfl6.flowi6_oif = ireq->ir_iif;\n\tfl6.fl6_dport = ireq->ir_rmt_port;\n\tfl6.fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\n\trcu_read_lock();\n\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);\n\trcu_read_unlock();\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto done;\n\t}\n\n\tskb = dccp_make_response(sk, dst, req);\n\tif (skb != NULL) {\n\t\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\t\tdh->dccph_checksum = dccp_v6_csum_finish(skb,\n\t\t\t\t\t\t\t &ireq->ir_v6_loc_addr,\n\t\t\t\t\t\t\t &ireq->ir_v6_rmt_addr);\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\trcu_read_lock();\n\t\terr = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),\n\t\t\t       np->tclass);\n\t\trcu_read_unlock();\n\t\terr = net_xmit_eval(err);\n\t}\n\ndone:\n\tdst_release(dst);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1059,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "void inet6_destroy_sock(struct sock *sk)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct ipv6_txoptions *opt;\n\n\t/* Release rx options */\n\n\tskb = xchg(&np->pktoptions, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\tskb = xchg(&np->rxpmtu, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\t/* Free flowlabels */\n\tfl6_free_socklist(sk);\n\n\t/* Free tx options */\n\n\topt = xchg((__force struct ipv6_txoptions **)&np->opt, NULL);\n\tif (opt) {\n\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\ttxopt_put(opt);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1060,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "int inet6_sk_rebuild_header(struct sock *sk)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dst_entry *dst;\n\n\tdst = __sk_dst_check(sk, np->dst_cookie);\n\n\tif (!dst) {\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = sk->sk_protocol;\n\t\tfl6.daddr = sk->sk_v6_daddr;\n\t\tfl6.saddr = np->saddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tfl6.fl6_sport = inet->inet_sport;\n\t\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\t\trcu_read_lock();\n\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt),\n\t\t\t\t\t &final);\n\t\trcu_read_unlock();\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\t\tif (IS_ERR(dst)) {\n\t\t\tsk->sk_route_caps = 0;\n\t\t\tsk->sk_err_soft = -PTR_ERR(dst);\n\t\t\treturn PTR_ERR(dst);\n\t\t}\n\n\t\t__ip6_dst_store(sk, dst, NULL, NULL);\n\t}\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1061,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "static int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t*daddr, *final_p, final;\n\tstruct dst_entry\t*dst;\n\tstruct flowi6\t\tfl6;\n\tstruct ip6_flowlabel\t*flowlabel = NULL;\n\tstruct ipv6_txoptions\t*opt;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tusin->sin6_addr.s6_addr[15] = 0x01;\n\t}\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6.flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet->inet_dport;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tif (!fl6.flowi6_oif && (addr_type&IPV6_ADDR_MULTICAST))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\trcu_read_lock();\n\topt = flowlabel ? flowlabel->opt : rcu_dereference(np->opt);\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\trcu_read_unlock();\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\terr = 0;\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\t/* source address lookup done in ip6_dst_lookup */\n\n\tif (ipv6_addr_any(&np->saddr))\n\t\tnp->saddr = fl6.saddr;\n\n\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tsk->sk_v6_rcv_saddr = fl6.saddr;\n\t\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\t\tif (sk->sk_prot->rehash)\n\t\t\tsk->sk_prot->rehash(sk);\n\t}\n\n\tip6_dst_store(sk, dst,\n\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t      &np->saddr :\n#endif\n\t\t      NULL);\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1062,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "struct ipv6_txoptions *\nipv6_dup_options(struct sock *sk, struct ipv6_txoptions *opt)\n{\n\tstruct ipv6_txoptions *opt2;\n\n\topt2 = sock_kmalloc(sk, opt->tot_len, GFP_ATOMIC);\n\tif (opt2) {\n\t\tlong dif = (char *)opt2 - (char *)opt;\n\t\tmemcpy(opt2, opt, opt->tot_len);\n\t\tif (opt2->hopopt)\n\t\t\t*((char **)&opt2->hopopt) += dif;\n\t\tif (opt2->dst0opt)\n\t\t\t*((char **)&opt2->dst0opt) += dif;\n\t\tif (opt2->dst1opt)\n\t\t\t*((char **)&opt2->dst1opt) += dif;\n\t\tif (opt2->srcrt)\n\t\t\t*((char **)&opt2->srcrt) += dif;\n\t\tatomic_set(&opt2->refcnt, 1);\n\t}\n\treturn opt2;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1063,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "struct ipv6_txoptions *\nipv6_renew_options(struct sock *sk, struct ipv6_txoptions *opt,\n\t\t   int newtype,\n\t\t   struct ipv6_opt_hdr __user *newopt, int newoptlen)\n{\n\tint tot_len = 0;\n\tchar *p;\n\tstruct ipv6_txoptions *opt2;\n\tint err;\n\n\tif (opt) {\n\t\tif (newtype != IPV6_HOPOPTS && opt->hopopt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->hopopt));\n\t\tif (newtype != IPV6_RTHDRDSTOPTS && opt->dst0opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst0opt));\n\t\tif (newtype != IPV6_RTHDR && opt->srcrt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->srcrt));\n\t\tif (newtype != IPV6_DSTOPTS && opt->dst1opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst1opt));\n\t}\n\n\tif (newopt && newoptlen)\n\t\ttot_len += CMSG_ALIGN(newoptlen);\n\n\tif (!tot_len)\n\t\treturn NULL;\n\n\ttot_len += sizeof(*opt2);\n\topt2 = sock_kmalloc(sk, tot_len, GFP_ATOMIC);\n\tif (!opt2)\n\t\treturn ERR_PTR(-ENOBUFS);\n\n\tmemset(opt2, 0, tot_len);\n\tatomic_set(&opt2->refcnt, 1);\n\topt2->tot_len = tot_len;\n\tp = (char *)(opt2 + 1);\n\n\terr = ipv6_renew_option(opt ? opt->hopopt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_HOPOPTS,\n\t\t\t\t&opt2->hopopt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst0opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDRDSTOPTS,\n\t\t\t\t&opt2->dst0opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->srcrt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDR,\n\t\t\t\t(struct ipv6_opt_hdr **)&opt2->srcrt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst1opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_DSTOPTS,\n\t\t\t\t&opt2->dst1opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\topt2->opt_nflen = (opt2->hopopt ? ipv6_optlen(opt2->hopopt) : 0) +\n\t\t\t  (opt2->dst0opt ? ipv6_optlen(opt2->dst0opt) : 0) +\n\t\t\t  (opt2->srcrt ? ipv6_optlen(opt2->srcrt) : 0);\n\topt2->opt_flen = (opt2->dst1opt ? ipv6_optlen(opt2->dst1opt) : 0);\n\n\treturn opt2;\nout:\n\tsock_kfree_s(sk, opt2, opt2->tot_len);\n\treturn ERR_PTR(err);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1064,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "static struct dst_entry *inet6_csk_route_socket(struct sock *sk,\n\t\t\t\t\t\tstruct flowi6 *fl6)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = sk->sk_protocol;\n\tfl6->daddr = sk->sk_v6_daddr;\n\tfl6->saddr = np->saddr;\n\tfl6->flowlabel = np->flow_label;\n\tIP6_ECN_flow_xmit(sk, fl6->flowlabel);\n\tfl6->flowi6_oif = sk->sk_bound_dev_if;\n\tfl6->flowi6_mark = sk->sk_mark;\n\tfl6->fl6_sport = inet->inet_sport;\n\tfl6->fl6_dport = inet->inet_dport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(fl6));\n\n\trcu_read_lock();\n\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);\n\trcu_read_unlock();\n\n\tdst = __inet6_csk_dst_check(sk, np->dst_cookie);\n\tif (!dst) {\n\t\tdst = ip6_dst_lookup_flow(sk, fl6, final_p);\n\n\t\tif (!IS_ERR(dst))\n\t\t\t__inet6_csk_dst_store(sk, dst, NULL, NULL);\n\t}\n\treturn dst;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1065,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "struct dst_entry *inet6_csk_route_req(const struct sock *sk,\n\t\t\t\t      struct flowi6 *fl6,\n\t\t\t\t      const struct request_sock *req,\n\t\t\t\t      u8 proto)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = proto;\n\tfl6->daddr = ireq->ir_v6_rmt_addr;\n\trcu_read_lock();\n\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);\n\trcu_read_unlock();\n\tfl6->saddr = ireq->ir_v6_loc_addr;\n\tfl6->flowi6_oif = ireq->ir_iif;\n\tfl6->flowi6_mark = ireq->ir_mark;\n\tfl6->fl6_dport = ireq->ir_rmt_port;\n\tfl6->fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, fl6, final_p);\n\tif (IS_ERR(dst))\n\t\treturn NULL;\n\n\treturn dst;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1066,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "int inet6_csk_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl_unused)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint res;\n\n\tdst = inet6_csk_route_socket(sk, &fl6);\n\tif (IS_ERR(dst)) {\n\t\tsk->sk_err_soft = -PTR_ERR(dst);\n\t\tsk->sk_route_caps = 0;\n\t\tkfree_skb(skb);\n\t\treturn PTR_ERR(dst);\n\t}\n\n\trcu_read_lock();\n\tskb_dst_set_noref(skb, dst);\n\n\t/* Restore final destination back after routing done */\n\tfl6.daddr = sk->sk_v6_daddr;\n\n\tres = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),\n\t\t       np->tclass);\n\trcu_read_unlock();\n\treturn res;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1103,
            "cve_id": "CVE-2016-4805",
            "code_snippet": "int ppp_register_net_channel(struct net *net, struct ppp_channel *chan)\n{\n\tstruct channel *pch;\n\tstruct ppp_net *pn;\n\n\tpch = kzalloc(sizeof(struct channel), GFP_KERNEL);\n\tif (!pch)\n\t\treturn -ENOMEM;\n\n\tpn = ppp_pernet(net);\n\n\tpch->ppp = NULL;\n\tpch->chan = chan;\n\tpch->chan_net = get_net(net);\n\tchan->ppp = pch;\n\tinit_ppp_file(&pch->file, CHANNEL);\n\tpch->file.hdrlen = chan->hdrlen;\n#ifdef CONFIG_PPP_MULTILINK\n\tpch->lastseq = -1;\n#endif /* CONFIG_PPP_MULTILINK */\n\tinit_rwsem(&pch->chan_sem);\n\tspin_lock_init(&pch->downl);\n\trwlock_init(&pch->upl);\n\n\tspin_lock_bh(&pn->all_channels_lock);\n\tpch->file.index = ++pn->last_channel_index;\n\tlist_add(&pch->list, &pn->new_channels);\n\tatomic_inc(&channel_count);\n\tspin_unlock_bh(&pn->all_channels_lock);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1212,
            "cve_id": "CVE-2016-8655",
            "code_snippet": "static int\npacket_setsockopt(struct socket *sock, int level, int optname, char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint ret;\n\n\tif (level != SOL_PACKET)\n\t\treturn -ENOPROTOOPT;\n\n\tswitch (optname) {\n\tcase PACKET_ADD_MEMBERSHIP:\n\tcase PACKET_DROP_MEMBERSHIP:\n\t{\n\t\tstruct packet_mreq_max mreq;\n\t\tint len = optlen;\n\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\tif (len < sizeof(struct packet_mreq))\n\t\t\treturn -EINVAL;\n\t\tif (len > sizeof(mreq))\n\t\t\tlen = sizeof(mreq);\n\t\tif (copy_from_user(&mreq, optval, len))\n\t\t\treturn -EFAULT;\n\t\tif (len < (mreq.mr_alen + offsetof(struct packet_mreq, mr_address)))\n\t\t\treturn -EINVAL;\n\t\tif (optname == PACKET_ADD_MEMBERSHIP)\n\t\t\tret = packet_mc_add(sk, &mreq);\n\t\telse\n\t\t\tret = packet_mc_drop(sk, &mreq);\n\t\treturn ret;\n\t}\n\n\tcase PACKET_RX_RING:\n\tcase PACKET_TX_RING:\n\t{\n\t\tunion tpacket_req_u req_u;\n\t\tint len;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\t\tlen = sizeof(req_u.req);\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\tdefault:\n\t\t\tlen = sizeof(req_u.req3);\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&req_u.req, optval, len))\n\t\t\treturn -EFAULT;\n\t\treturn packet_set_ring(sk, &req_u, 0,\n\t\t\toptname == PACKET_TX_RING);\n\t}\n\tcase PACKET_COPY_THRESH:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpkt_sk(sk)->copy_thresh = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VERSION:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tswitch (val) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\tcase TPACKET_V3:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tlock_sock(sk);\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {\n\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tpo->tp_version = val;\n\t\t\tret = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t\treturn ret;\n\t}\n\tcase PACKET_RESERVE:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_reserve = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_LOSS:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_loss = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_AUXDATA:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->auxdata = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_ORIGDEV:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->origdev = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VNET_HDR:\n\t{\n\t\tint val;\n\n\t\tif (sock->type != SOCK_RAW)\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->has_vnet_hdr = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_TIMESTAMP:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->tp_tstamp = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_FANOUT:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\treturn fanout_add(sk, val & 0xffff, val >> 16);\n\t}\n\tcase PACKET_FANOUT_DATA:\n\t{\n\t\tif (!po->fanout)\n\t\t\treturn -EINVAL;\n\n\t\treturn fanout_set_data(po, optval, optlen);\n\t}\n\tcase PACKET_TX_HAS_OFF:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_tx_has_off = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_QDISC_BYPASS:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->xmit = val ? packet_direct_xmit : dev_queue_xmit;\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1220,
            "cve_id": "CVE-2016-9120",
            "code_snippet": "static long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\tmutex_lock(&client->lock);\n\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n\t\tif (IS_ERR(handle)) {\n\t\t\tmutex_unlock(&client->lock);\n\t\t\treturn PTR_ERR(handle);\n\t\t}\n\t\tion_free_nolock(client, handle);\n\t\tion_handle_put_nolock(handle);\n\t\tmutex_unlock(&client->lock);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1324,
            "cve_id": "CVE-2017-10661",
            "code_snippet": "\nSYSCALL_DEFINE2(timerfd_create, int, clockid, int, flags)\n{\n\tint ufd;\n\tstruct timerfd_ctx *ctx;\n\n\t/* Check the TFD_* constants for consistency.  */\n\tBUILD_BUG_ON(TFD_CLOEXEC != O_CLOEXEC);\n\tBUILD_BUG_ON(TFD_NONBLOCK != O_NONBLOCK);\n\n\tif ((flags & ~TFD_CREATE_FLAGS) ||\n\t    (clockid != CLOCK_MONOTONIC &&\n\t     clockid != CLOCK_REALTIME &&\n\t     clockid != CLOCK_REALTIME_ALARM &&\n\t     clockid != CLOCK_BOOTTIME &&\n\t     clockid != CLOCK_BOOTTIME_ALARM))\n\t\treturn -EINVAL;\n\n\tif (!capable(CAP_WAKE_ALARM) &&\n\t    (clockid == CLOCK_REALTIME_ALARM ||\n\t     clockid == CLOCK_BOOTTIME_ALARM))\n\t\treturn -EPERM;\n\n\tctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\tinit_waitqueue_head(&ctx->wqh);\n\tspin_lock_init(&ctx->cancel_lock);\n\tctx->clockid = clockid;\n\n\tif (isalarm(ctx))\n\t\talarm_init(&ctx->t.alarm,\n\t\t\t   ctx->clockid == CLOCK_REALTIME_ALARM ?\n\t\t\t   ALARM_REALTIME : ALARM_BOOTTIME,\n\t\t\t   timerfd_alarmproc);\n\telse\n\t\thrtimer_init(&ctx->t.tmr, clockid, HRTIMER_MODE_ABS);\n\n\tctx->moffs = ktime_mono_to_real(0);\n\n\tufd = anon_inode_getfd(\"[timerfd]\", &timerfd_fops, ctx,\n\t\t\t       O_RDWR | (flags & TFD_SHARED_FCNTL_FLAGS));\n\tif (ufd < 0)\n\t\tkfree(ctx);\n\n\treturn ufd;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1382,
            "cve_id": "CVE-2017-15265",
            "code_snippet": "struct snd_seq_client_port *snd_seq_create_port(struct snd_seq_client *client,\n\t\t\t\t\t\tint port)\n{\n\tunsigned long flags;\n\tstruct snd_seq_client_port *new_port, *p;\n\tint num = -1;\n\t\n\t/* sanity check */\n\tif (snd_BUG_ON(!client))\n\t\treturn NULL;\n\n\tif (client->num_ports >= SNDRV_SEQ_MAX_PORTS) {\n\t\tpr_warn(\"ALSA: seq: too many ports for client %d\\n\", client->number);\n\t\treturn NULL;\n\t}\n\n\t/* create a new port */\n\tnew_port = kzalloc(sizeof(*new_port), GFP_KERNEL);\n\tif (!new_port)\n\t\treturn NULL;\t/* failure, out of memory */\n\t/* init port data */\n\tnew_port->addr.client = client->number;\n\tnew_port->addr.port = -1;\n\tnew_port->owner = THIS_MODULE;\n\tsprintf(new_port->name, \"port-%d\", num);\n\tsnd_use_lock_init(&new_port->use_lock);\n\tport_subs_info_init(&new_port->c_src);\n\tport_subs_info_init(&new_port->c_dest);\n\tsnd_use_lock_use(&new_port->use_lock);\n\n\tnum = port >= 0 ? port : 0;\n\tmutex_lock(&client->ports_mutex);\n\twrite_lock_irqsave(&client->ports_lock, flags);\n\tlist_for_each_entry(p, &client->ports_list_head, list) {\n\t\tif (p->addr.port > num)\n\t\t\tbreak;\n\t\tif (port < 0) /* auto-probe mode */\n\t\t\tnum = p->addr.port + 1;\n\t}\n\t/* insert the new port */\n\tlist_add_tail(&new_port->list, &p->list);\n\tclient->num_ports++;\n\tnew_port->addr.port = num;\t/* store the port number in the port */\n\tsprintf(new_port->name, \"port-%d\", num);\n\twrite_unlock_irqrestore(&client->ports_lock, flags);\n\tmutex_unlock(&client->ports_mutex);\n\n\treturn new_port;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1394,
            "cve_id": "CVE-2017-16527",
            "code_snippet": "void snd_usb_mixer_disconnect(struct usb_mixer_interface *mixer)\n{\n\tif (mixer->disconnected)\n\t\treturn;\n\tif (mixer->urb)\n\t\tusb_kill_urb(mixer->urb);\n\tif (mixer->rc_urb)\n\t\tusb_kill_urb(mixer->rc_urb);\n\tmixer->disconnected = true;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1434,
            "cve_id": "CVE-2017-16939",
            "code_snippet": "static int xfrm_dump_policy(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct xfrm_policy_walk *walk = (struct xfrm_policy_walk *)cb->args;\n\tstruct xfrm_dump_info info;\n\n\tinfo.in_skb = cb->skb;\n\tinfo.out_skb = skb;\n\tinfo.nlmsg_seq = cb->nlh->nlmsg_seq;\n\tinfo.nlmsg_flags = NLM_F_MULTI;\n\n\t(void) xfrm_policy_walk(net, walk, dump_one_policy, &info);\n\n\treturn skb->len;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1488,
            "cve_id": "CVE-2017-18218",
            "code_snippet": "static netdev_tx_t hns_nic_net_xmit(struct sk_buff *skb,\n\t\t\t\t    struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\n\tassert(skb->queue_mapping < ndev->ae_handle->q_num);\n\n\treturn hns_nic_net_xmit_hw(ndev, skb,\n\t\t\t\t   &tx_ring_data(priv, skb->queue_mapping));\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1530,
            "cve_id": "CVE-2017-2584",
            "code_snippet": "static int emulate_store_desc_ptr(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t  void (*get)(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t\t      struct desc_ptr *ptr))\n{\n\tstruct desc_ptr desc_ptr;\n\n\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\tctxt->op_bytes = 8;\n\tget(ctxt, &desc_ptr);\n\tif (ctxt->op_bytes == 2) {\n\t\tctxt->op_bytes = 4;\n\t\tdesc_ptr.address &= 0x00ffffff;\n\t}\n\t/* Disable writeback. */\n\tctxt->dst.type = OP_NONE;\n\treturn segmented_write_std(ctxt, ctxt->dst.addr.mem,\n\t\t\t\t   &desc_ptr, 2 + ctxt->op_bytes);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1569,
            "cve_id": "CVE-2017-6346",
            "code_snippet": "static void fanout_release(struct sock *sk)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_fanout *f;\n\n\tmutex_lock(&fanout_mutex);\n\tf = po->fanout;\n\tif (f) {\n\t\tpo->fanout = NULL;\n\n\t\tif (atomic_dec_and_test(&f->sk_ref)) {\n\t\t\tlist_del(&f->list);\n\t\t\tdev_remove_pack(&f->prot_hook);\n\t\t\tfanout_release_data(f);\n\t\t\tkfree(f);\n\t\t}\n\n\t\tif (po->rollover)\n\t\t\tkfree_rcu(po->rollover, rcu);\n\t}\n\tmutex_unlock(&fanout_mutex);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1574,
            "cve_id": "CVE-2017-6874",
            "code_snippet": "static struct ucounts *get_ucounts(struct user_namespace *ns, kuid_t uid)\n{\n\tstruct hlist_head *hashent = ucounts_hashentry(ns, uid);\n\tstruct ucounts *ucounts, *new;\n\n\tspin_lock_irq(&ucounts_lock);\n\tucounts = find_ucounts(ns, uid, hashent);\n\tif (!ucounts) {\n\t\tspin_unlock_irq(&ucounts_lock);\n\n\t\tnew = kzalloc(sizeof(*new), GFP_KERNEL);\n\t\tif (!new)\n\t\t\treturn NULL;\n\n\t\tnew->ns = ns;\n\t\tnew->uid = uid;\n\t\tnew->count = 0;\n\n\t\tspin_lock_irq(&ucounts_lock);\n\t\tucounts = find_ucounts(ns, uid, hashent);\n\t\tif (ucounts) {\n\t\t\tkfree(new);\n\t\t} else {\n\t\t\thlist_add_head(&new->node, hashent);\n\t\t\tucounts = new;\n\t\t}\n\t}\n\tif (ucounts->count == INT_MAX)\n\t\tucounts = NULL;\n\telse\n\t\tucounts->count += 1;\n\tspin_unlock_irq(&ucounts_lock);\n\treturn ucounts;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1584,
            "cve_id": "CVE-2017-7374",
            "code_snippet": "int fscrypt_setup_filename(struct inode *dir, const struct qstr *iname,\n\t\t\t      int lookup, struct fscrypt_name *fname)\n{\n\tint ret = 0, bigname = 0;\n\n\tmemset(fname, 0, sizeof(struct fscrypt_name));\n\tfname->usr_fname = iname;\n\n\tif (!dir->i_sb->s_cop->is_encrypted(dir) ||\n\t\t\t\tfscrypt_is_dot_dotdot(iname)) {\n\t\tfname->disk_name.name = (unsigned char *)iname->name;\n\t\tfname->disk_name.len = iname->len;\n\t\treturn 0;\n\t}\n\tret = fscrypt_get_encryption_info(dir);\n\tif (ret && ret != -EOPNOTSUPP)\n\t\treturn ret;\n\n\tif (dir->i_crypt_info) {\n\t\tret = fscrypt_fname_alloc_buffer(dir, iname->len,\n\t\t\t\t\t\t\t&fname->crypto_buf);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = fname_encrypt(dir, iname, &fname->crypto_buf);\n\t\tif (ret)\n\t\t\tgoto errout;\n\t\tfname->disk_name.name = fname->crypto_buf.name;\n\t\tfname->disk_name.len = fname->crypto_buf.len;\n\t\treturn 0;\n\t}\n\tif (!lookup)\n\t\treturn -ENOKEY;\n\n\t/*\n\t * We don't have the key and we are doing a lookup; decode the\n\t * user-supplied name\n\t */\n\tif (iname->name[0] == '_')\n\t\tbigname = 1;\n\tif ((bigname && (iname->len != 33)) || (!bigname && (iname->len > 43)))\n\t\treturn -ENOENT;\n\n\tfname->crypto_buf.name = kmalloc(32, GFP_KERNEL);\n\tif (fname->crypto_buf.name == NULL)\n\t\treturn -ENOMEM;\n\n\tret = digest_decode(iname->name + bigname, iname->len - bigname,\n\t\t\t\tfname->crypto_buf.name);\n\tif (ret < 0) {\n\t\tret = -ENOENT;\n\t\tgoto errout;\n\t}\n\tfname->crypto_buf.len = ret;\n\tif (bigname) {\n\t\tmemcpy(&fname->hash, fname->crypto_buf.name, 4);\n\t\tmemcpy(&fname->minor_hash, fname->crypto_buf.name + 4, 4);\n\t} else {\n\t\tfname->disk_name.name = fname->crypto_buf.name;\n\t\tfname->disk_name.len = fname->crypto_buf.len;\n\t}\n\treturn 0;\n\nerrout:\n\tfscrypt_fname_free_buffer(&fname->crypto_buf);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1585,
            "cve_id": "CVE-2017-7374",
            "code_snippet": "int fscrypt_get_encryption_info(struct inode *inode)\n{\n\tstruct fscrypt_info *crypt_info;\n\tstruct fscrypt_context ctx;\n\tstruct crypto_skcipher *ctfm;\n\tconst char *cipher_str;\n\tint keysize;\n\tu8 *raw_key = NULL;\n\tint res;\n\n\tif (inode->i_crypt_info)\n\t\treturn 0;\n\n\tres = fscrypt_initialize(inode->i_sb->s_cop->flags);\n\tif (res)\n\t\treturn res;\n\n\tif (!inode->i_sb->s_cop->get_context)\n\t\treturn -EOPNOTSUPP;\n\n\tres = inode->i_sb->s_cop->get_context(inode, &ctx, sizeof(ctx));\n\tif (res < 0) {\n\t\tif (!fscrypt_dummy_context_enabled(inode) ||\n\t\t    inode->i_sb->s_cop->is_encrypted(inode))\n\t\t\treturn res;\n\t\t/* Fake up a context for an unencrypted directory */\n\t\tmemset(&ctx, 0, sizeof(ctx));\n\t\tctx.format = FS_ENCRYPTION_CONTEXT_FORMAT_V1;\n\t\tctx.contents_encryption_mode = FS_ENCRYPTION_MODE_AES_256_XTS;\n\t\tctx.filenames_encryption_mode = FS_ENCRYPTION_MODE_AES_256_CTS;\n\t\tmemset(ctx.master_key_descriptor, 0x42, FS_KEY_DESCRIPTOR_SIZE);\n\t} else if (res != sizeof(ctx)) {\n\t\treturn -EINVAL;\n\t}\n\n\tif (ctx.format != FS_ENCRYPTION_CONTEXT_FORMAT_V1)\n\t\treturn -EINVAL;\n\n\tif (ctx.flags & ~FS_POLICY_FLAGS_VALID)\n\t\treturn -EINVAL;\n\n\tcrypt_info = kmem_cache_alloc(fscrypt_info_cachep, GFP_NOFS);\n\tif (!crypt_info)\n\t\treturn -ENOMEM;\n\n\tcrypt_info->ci_flags = ctx.flags;\n\tcrypt_info->ci_data_mode = ctx.contents_encryption_mode;\n\tcrypt_info->ci_filename_mode = ctx.filenames_encryption_mode;\n\tcrypt_info->ci_ctfm = NULL;\n\tmemcpy(crypt_info->ci_master_key, ctx.master_key_descriptor,\n\t\t\t\tsizeof(crypt_info->ci_master_key));\n\n\tres = determine_cipher_type(crypt_info, inode, &cipher_str, &keysize);\n\tif (res)\n\t\tgoto out;\n\n\t/*\n\t * This cannot be a stack buffer because it is passed to the scatterlist\n\t * crypto API as part of key derivation.\n\t */\n\tres = -ENOMEM;\n\traw_key = kmalloc(FS_MAX_KEY_SIZE, GFP_NOFS);\n\tif (!raw_key)\n\t\tgoto out;\n\n\tres = validate_user_key(crypt_info, &ctx, raw_key, FS_KEY_DESC_PREFIX);\n\tif (res && inode->i_sb->s_cop->key_prefix) {\n\t\tint res2 = validate_user_key(crypt_info, &ctx, raw_key,\n\t\t\t\t\t     inode->i_sb->s_cop->key_prefix);\n\t\tif (res2) {\n\t\t\tif (res2 == -ENOKEY)\n\t\t\t\tres = -ENOKEY;\n\t\t\tgoto out;\n\t\t}\n\t} else if (res) {\n\t\tgoto out;\n\t}\n\tctfm = crypto_alloc_skcipher(cipher_str, 0, 0);\n\tif (!ctfm || IS_ERR(ctfm)) {\n\t\tres = ctfm ? PTR_ERR(ctfm) : -ENOMEM;\n\t\tprintk(KERN_DEBUG\n\t\t       \"%s: error %d (inode %u) allocating crypto tfm\\n\",\n\t\t       __func__, res, (unsigned) inode->i_ino);\n\t\tgoto out;\n\t}\n\tcrypt_info->ci_ctfm = ctfm;\n\tcrypto_skcipher_clear_flags(ctfm, ~0);\n\tcrypto_skcipher_set_flags(ctfm, CRYPTO_TFM_REQ_WEAK_KEY);\n\tres = crypto_skcipher_setkey(ctfm, raw_key, keysize);\n\tif (res)\n\t\tgoto out;\n\n\tif (cmpxchg(&inode->i_crypt_info, NULL, crypt_info) == NULL)\n\t\tcrypt_info = NULL;\nout:\n\tif (res == -ENOKEY)\n\t\tres = 0;\n\tput_crypt_info(crypt_info);\n\tkzfree(raw_key);\n\treturn res;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1695,
            "cve_id": "CVE-2018-10876",
            "code_snippet": "struct inode *__ext4_new_inode(handle_t *handle, struct inode *dir,\n\t\t\t       umode_t mode, const struct qstr *qstr,\n\t\t\t       __u32 goal, uid_t *owner, __u32 i_flags,\n\t\t\t       int handle_type, unsigned int line_no,\n\t\t\t       int nblocks)\n{\n\tstruct super_block *sb;\n\tstruct buffer_head *inode_bitmap_bh = NULL;\n\tstruct buffer_head *group_desc_bh;\n\text4_group_t ngroups, group = 0;\n\tunsigned long ino = 0;\n\tstruct inode *inode;\n\tstruct ext4_group_desc *gdp = NULL;\n\tstruct ext4_inode_info *ei;\n\tstruct ext4_sb_info *sbi;\n\tint ret2, err;\n\tstruct inode *ret;\n\text4_group_t i;\n\text4_group_t flex_group;\n\tstruct ext4_group_info *grp;\n\tint encrypt = 0;\n\n\t/* Cannot create files in a deleted directory */\n\tif (!dir || !dir->i_nlink)\n\t\treturn ERR_PTR(-EPERM);\n\n\tsb = dir->i_sb;\n\tsbi = EXT4_SB(sb);\n\n\tif (unlikely(ext4_forced_shutdown(sbi)))\n\t\treturn ERR_PTR(-EIO);\n\n\tif ((ext4_encrypted_inode(dir) || DUMMY_ENCRYPTION_ENABLED(sbi)) &&\n\t    (S_ISREG(mode) || S_ISDIR(mode) || S_ISLNK(mode)) &&\n\t    !(i_flags & EXT4_EA_INODE_FL)) {\n\t\terr = fscrypt_get_encryption_info(dir);\n\t\tif (err)\n\t\t\treturn ERR_PTR(err);\n\t\tif (!fscrypt_has_encryption_key(dir))\n\t\t\treturn ERR_PTR(-ENOKEY);\n\t\tencrypt = 1;\n\t}\n\n\tif (!handle && sbi->s_journal && !(i_flags & EXT4_EA_INODE_FL)) {\n#ifdef CONFIG_EXT4_FS_POSIX_ACL\n\t\tstruct posix_acl *p = get_acl(dir, ACL_TYPE_DEFAULT);\n\n\t\tif (IS_ERR(p))\n\t\t\treturn ERR_CAST(p);\n\t\tif (p) {\n\t\t\tint acl_size = p->a_count * sizeof(ext4_acl_entry);\n\n\t\t\tnblocks += (S_ISDIR(mode) ? 2 : 1) *\n\t\t\t\t__ext4_xattr_set_credits(sb, NULL /* inode */,\n\t\t\t\t\tNULL /* block_bh */, acl_size,\n\t\t\t\t\ttrue /* is_create */);\n\t\t\tposix_acl_release(p);\n\t\t}\n#endif\n\n#ifdef CONFIG_SECURITY\n\t\t{\n\t\t\tint num_security_xattrs = 1;\n\n#ifdef CONFIG_INTEGRITY\n\t\t\tnum_security_xattrs++;\n#endif\n\t\t\t/*\n\t\t\t * We assume that security xattrs are never\n\t\t\t * more than 1k.  In practice they are under\n\t\t\t * 128 bytes.\n\t\t\t */\n\t\t\tnblocks += num_security_xattrs *\n\t\t\t\t__ext4_xattr_set_credits(sb, NULL /* inode */,\n\t\t\t\t\tNULL /* block_bh */, 1024,\n\t\t\t\t\ttrue /* is_create */);\n\t\t}\n#endif\n\t\tif (encrypt)\n\t\t\tnblocks += __ext4_xattr_set_credits(sb,\n\t\t\t\t\tNULL /* inode */, NULL /* block_bh */,\n\t\t\t\t\tFSCRYPT_SET_CONTEXT_MAX_SIZE,\n\t\t\t\t\ttrue /* is_create */);\n\t}\n\n\tngroups = ext4_get_groups_count(sb);\n\ttrace_ext4_request_inode(dir, mode);\n\tinode = new_inode(sb);\n\tif (!inode)\n\t\treturn ERR_PTR(-ENOMEM);\n\tei = EXT4_I(inode);\n\n\t/*\n\t * Initialize owners and quota early so that we don't have to account\n\t * for quota initialization worst case in standard inode creating\n\t * transaction\n\t */\n\tif (owner) {\n\t\tinode->i_mode = mode;\n\t\ti_uid_write(inode, owner[0]);\n\t\ti_gid_write(inode, owner[1]);\n\t} else if (test_opt(sb, GRPID)) {\n\t\tinode->i_mode = mode;\n\t\tinode->i_uid = current_fsuid();\n\t\tinode->i_gid = dir->i_gid;\n\t} else\n\t\tinode_init_owner(inode, dir, mode);\n\n\tif (ext4_has_feature_project(sb) &&\n\t    ext4_test_inode_flag(dir, EXT4_INODE_PROJINHERIT))\n\t\tei->i_projid = EXT4_I(dir)->i_projid;\n\telse\n\t\tei->i_projid = make_kprojid(&init_user_ns, EXT4_DEF_PROJID);\n\n\terr = dquot_initialize(inode);\n\tif (err)\n\t\tgoto out;\n\n\tif (!goal)\n\t\tgoal = sbi->s_inode_goal;\n\n\tif (goal && goal <= le32_to_cpu(sbi->s_es->s_inodes_count)) {\n\t\tgroup = (goal - 1) / EXT4_INODES_PER_GROUP(sb);\n\t\tino = (goal - 1) % EXT4_INODES_PER_GROUP(sb);\n\t\tret2 = 0;\n\t\tgoto got_group;\n\t}\n\n\tif (S_ISDIR(mode))\n\t\tret2 = find_group_orlov(sb, dir, &group, mode, qstr);\n\telse\n\t\tret2 = find_group_other(sb, dir, &group, mode);\n\ngot_group:\n\tEXT4_I(dir)->i_last_alloc_group = group;\n\terr = -ENOSPC;\n\tif (ret2 == -1)\n\t\tgoto out;\n\n\t/*\n\t * Normally we will only go through one pass of this loop,\n\t * unless we get unlucky and it turns out the group we selected\n\t * had its last inode grabbed by someone else.\n\t */\n\tfor (i = 0; i < ngroups; i++, ino = 0) {\n\t\terr = -EIO;\n\n\t\tgdp = ext4_get_group_desc(sb, group, &group_desc_bh);\n\t\tif (!gdp)\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * Check free inodes count before loading bitmap.\n\t\t */\n\t\tif (ext4_free_inodes_count(sb, gdp) == 0)\n\t\t\tgoto next_group;\n\n\t\tgrp = ext4_get_group_info(sb, group);\n\t\t/* Skip groups with already-known suspicious inode tables */\n\t\tif (EXT4_MB_GRP_IBITMAP_CORRUPT(grp))\n\t\t\tgoto next_group;\n\n\t\tbrelse(inode_bitmap_bh);\n\t\tinode_bitmap_bh = ext4_read_inode_bitmap(sb, group);\n\t\t/* Skip groups with suspicious inode tables */\n\t\tif (EXT4_MB_GRP_IBITMAP_CORRUPT(grp) ||\n\t\t    IS_ERR(inode_bitmap_bh)) {\n\t\t\tinode_bitmap_bh = NULL;\n\t\t\tgoto next_group;\n\t\t}\n\nrepeat_in_this_group:\n\t\tret2 = find_inode_bit(sb, group, inode_bitmap_bh, &ino);\n\t\tif (!ret2)\n\t\t\tgoto next_group;\n\n\t\tif (group == 0 && (ino + 1) < EXT4_FIRST_INO(sb)) {\n\t\t\text4_error(sb, \"reserved inode found cleared - \"\n\t\t\t\t   \"inode=%lu\", ino + 1);\n\t\t\text4_mark_group_bitmap_corrupted(sb, group,\n\t\t\t\t\tEXT4_GROUP_INFO_IBITMAP_CORRUPT);\n\t\t\tgoto next_group;\n\t\t}\n\n\t\tif (!handle) {\n\t\t\tBUG_ON(nblocks <= 0);\n\t\t\thandle = __ext4_journal_start_sb(dir->i_sb, line_no,\n\t\t\t\t\t\t\t handle_type, nblocks,\n\t\t\t\t\t\t\t 0);\n\t\t\tif (IS_ERR(handle)) {\n\t\t\t\terr = PTR_ERR(handle);\n\t\t\t\text4_std_error(sb, err);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tBUFFER_TRACE(inode_bitmap_bh, \"get_write_access\");\n\t\terr = ext4_journal_get_write_access(handle, inode_bitmap_bh);\n\t\tif (err) {\n\t\t\text4_std_error(sb, err);\n\t\t\tgoto out;\n\t\t}\n\t\text4_lock_group(sb, group);\n\t\tret2 = ext4_test_and_set_bit(ino, inode_bitmap_bh->b_data);\n\t\tif (ret2) {\n\t\t\t/* Someone already took the bit. Repeat the search\n\t\t\t * with lock held.\n\t\t\t */\n\t\t\tret2 = find_inode_bit(sb, group, inode_bitmap_bh, &ino);\n\t\t\tif (ret2) {\n\t\t\t\text4_set_bit(ino, inode_bitmap_bh->b_data);\n\t\t\t\tret2 = 0;\n\t\t\t} else {\n\t\t\t\tret2 = 1; /* we didn't grab the inode */\n\t\t\t}\n\t\t}\n\t\text4_unlock_group(sb, group);\n\t\tino++;\t\t/* the inode bitmap is zero-based */\n\t\tif (!ret2)\n\t\t\tgoto got; /* we grabbed the inode! */\n\n\t\tif (ino < EXT4_INODES_PER_GROUP(sb))\n\t\t\tgoto repeat_in_this_group;\nnext_group:\n\t\tif (++group == ngroups)\n\t\t\tgroup = 0;\n\t}\n\terr = -ENOSPC;\n\tgoto out;\n\ngot:\n\tBUFFER_TRACE(inode_bitmap_bh, \"call ext4_handle_dirty_metadata\");\n\terr = ext4_handle_dirty_metadata(handle, NULL, inode_bitmap_bh);\n\tif (err) {\n\t\text4_std_error(sb, err);\n\t\tgoto out;\n\t}\n\n\tBUFFER_TRACE(group_desc_bh, \"get_write_access\");\n\terr = ext4_journal_get_write_access(handle, group_desc_bh);\n\tif (err) {\n\t\text4_std_error(sb, err);\n\t\tgoto out;\n\t}\n\n\t/* We may have to initialize the block bitmap if it isn't already */\n\tif (ext4_has_group_desc_csum(sb) &&\n\t    gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) {\n\t\tstruct buffer_head *block_bitmap_bh;\n\n\t\tblock_bitmap_bh = ext4_read_block_bitmap(sb, group);\n\t\tif (IS_ERR(block_bitmap_bh)) {\n\t\t\terr = PTR_ERR(block_bitmap_bh);\n\t\t\tgoto out;\n\t\t}\n\t\tBUFFER_TRACE(block_bitmap_bh, \"get block bitmap access\");\n\t\terr = ext4_journal_get_write_access(handle, block_bitmap_bh);\n\t\tif (err) {\n\t\t\tbrelse(block_bitmap_bh);\n\t\t\text4_std_error(sb, err);\n\t\t\tgoto out;\n\t\t}\n\n\t\tBUFFER_TRACE(block_bitmap_bh, \"dirty block bitmap\");\n\t\terr = ext4_handle_dirty_metadata(handle, NULL, block_bitmap_bh);\n\n\t\t/* recheck and clear flag under lock if we still need to */\n\t\text4_lock_group(sb, group);\n\t\tif (ext4_has_group_desc_csum(sb) &&\n\t\t    (gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT))) {\n\t\t\tgdp->bg_flags &= cpu_to_le16(~EXT4_BG_BLOCK_UNINIT);\n\t\t\text4_free_group_clusters_set(sb, gdp,\n\t\t\t\text4_free_clusters_after_init(sb, group, gdp));\n\t\t\text4_block_bitmap_csum_set(sb, group, gdp,\n\t\t\t\t\t\t   block_bitmap_bh);\n\t\t\text4_group_desc_csum_set(sb, group, gdp);\n\t\t}\n\t\text4_unlock_group(sb, group);\n\t\tbrelse(block_bitmap_bh);\n\n\t\tif (err) {\n\t\t\text4_std_error(sb, err);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* Update the relevant bg descriptor fields */\n\tif (ext4_has_group_desc_csum(sb)) {\n\t\tint free;\n\t\tstruct ext4_group_info *grp = ext4_get_group_info(sb, group);\n\n\t\tdown_read(&grp->alloc_sem); /* protect vs itable lazyinit */\n\t\text4_lock_group(sb, group); /* while we modify the bg desc */\n\t\tfree = EXT4_INODES_PER_GROUP(sb) -\n\t\t\text4_itable_unused_count(sb, gdp);\n\t\tif (gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_UNINIT)) {\n\t\t\tgdp->bg_flags &= cpu_to_le16(~EXT4_BG_INODE_UNINIT);\n\t\t\tfree = 0;\n\t\t}\n\t\t/*\n\t\t * Check the relative inode number against the last used\n\t\t * relative inode number in this group. if it is greater\n\t\t * we need to update the bg_itable_unused count\n\t\t */\n\t\tif (ino > free)\n\t\t\text4_itable_unused_set(sb, gdp,\n\t\t\t\t\t(EXT4_INODES_PER_GROUP(sb) - ino));\n\t\tup_read(&grp->alloc_sem);\n\t} else {\n\t\text4_lock_group(sb, group);\n\t}\n\n\text4_free_inodes_set(sb, gdp, ext4_free_inodes_count(sb, gdp) - 1);\n\tif (S_ISDIR(mode)) {\n\t\text4_used_dirs_set(sb, gdp, ext4_used_dirs_count(sb, gdp) + 1);\n\t\tif (sbi->s_log_groups_per_flex) {\n\t\t\text4_group_t f = ext4_flex_group(sbi, group);\n\n\t\t\tatomic_inc(&sbi->s_flex_groups[f].used_dirs);\n\t\t}\n\t}\n\tif (ext4_has_group_desc_csum(sb)) {\n\t\text4_inode_bitmap_csum_set(sb, group, gdp, inode_bitmap_bh,\n\t\t\t\t\t   EXT4_INODES_PER_GROUP(sb) / 8);\n\t\text4_group_desc_csum_set(sb, group, gdp);\n\t}\n\text4_unlock_group(sb, group);\n\n\tBUFFER_TRACE(group_desc_bh, \"call ext4_handle_dirty_metadata\");\n\terr = ext4_handle_dirty_metadata(handle, NULL, group_desc_bh);\n\tif (err) {\n\t\text4_std_error(sb, err);\n\t\tgoto out;\n\t}\n\n\tpercpu_counter_dec(&sbi->s_freeinodes_counter);\n\tif (S_ISDIR(mode))\n\t\tpercpu_counter_inc(&sbi->s_dirs_counter);\n\n\tif (sbi->s_log_groups_per_flex) {\n\t\tflex_group = ext4_flex_group(sbi, group);\n\t\tatomic_dec(&sbi->s_flex_groups[flex_group].free_inodes);\n\t}\n\n\tinode->i_ino = ino + group * EXT4_INODES_PER_GROUP(sb);\n\t/* This is the optimal IO size (for stat), not the fs block size */\n\tinode->i_blocks = 0;\n\tinode->i_mtime = inode->i_atime = inode->i_ctime = ei->i_crtime =\n\t\t\t\t\t\t       current_time(inode);\n\n\tmemset(ei->i_data, 0, sizeof(ei->i_data));\n\tei->i_dir_start_lookup = 0;\n\tei->i_disksize = 0;\n\n\t/* Don't inherit extent flag from directory, amongst others. */\n\tei->i_flags =\n\t\text4_mask_flags(mode, EXT4_I(dir)->i_flags & EXT4_FL_INHERITED);\n\tei->i_flags |= i_flags;\n\tei->i_file_acl = 0;\n\tei->i_dtime = 0;\n\tei->i_block_group = group;\n\tei->i_last_alloc_group = ~0;\n\n\text4_set_inode_flags(inode);\n\tif (IS_DIRSYNC(inode))\n\t\text4_handle_sync(handle);\n\tif (insert_inode_locked(inode) < 0) {\n\t\t/*\n\t\t * Likely a bitmap corruption causing inode to be allocated\n\t\t * twice.\n\t\t */\n\t\terr = -EIO;\n\t\text4_error(sb, \"failed to insert inode %lu: doubly allocated?\",\n\t\t\t   inode->i_ino);\n\t\text4_mark_group_bitmap_corrupted(sb, group,\n\t\t\t\t\tEXT4_GROUP_INFO_IBITMAP_CORRUPT);\n\t\tgoto out;\n\t}\n\tinode->i_generation = prandom_u32();\n\n\t/* Precompute checksum seed for inode metadata */\n\tif (ext4_has_metadata_csum(sb)) {\n\t\t__u32 csum;\n\t\t__le32 inum = cpu_to_le32(inode->i_ino);\n\t\t__le32 gen = cpu_to_le32(inode->i_generation);\n\t\tcsum = ext4_chksum(sbi, sbi->s_csum_seed, (__u8 *)&inum,\n\t\t\t\t   sizeof(inum));\n\t\tei->i_csum_seed = ext4_chksum(sbi, csum, (__u8 *)&gen,\n\t\t\t\t\t      sizeof(gen));\n\t}\n\n\text4_clear_state_flags(ei); /* Only relevant on 32-bit archs */\n\text4_set_inode_state(inode, EXT4_STATE_NEW);\n\n\tei->i_extra_isize = sbi->s_want_extra_isize;\n\tei->i_inline_off = 0;\n\tif (ext4_has_feature_inline_data(sb))\n\t\text4_set_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA);\n\tret = inode;\n\terr = dquot_alloc_inode(inode);\n\tif (err)\n\t\tgoto fail_drop;\n\n\t/*\n\t * Since the encryption xattr will always be unique, create it first so\n\t * that it's less likely to end up in an external xattr block and\n\t * prevent its deduplication.\n\t */\n\tif (encrypt) {\n\t\terr = fscrypt_inherit_context(dir, inode, handle, true);\n\t\tif (err)\n\t\t\tgoto fail_free_drop;\n\t}\n\n\tif (!(ei->i_flags & EXT4_EA_INODE_FL)) {\n\t\terr = ext4_init_acl(handle, inode, dir);\n\t\tif (err)\n\t\t\tgoto fail_free_drop;\n\n\t\terr = ext4_init_security(handle, inode, dir, qstr);\n\t\tif (err)\n\t\t\tgoto fail_free_drop;\n\t}\n\n\tif (ext4_has_feature_extents(sb)) {\n\t\t/* set extent flag only for directory, file and normal symlink*/\n\t\tif (S_ISDIR(mode) || S_ISREG(mode) || S_ISLNK(mode)) {\n\t\t\text4_set_inode_flag(inode, EXT4_INODE_EXTENTS);\n\t\t\text4_ext_tree_init(handle, inode);\n\t\t}\n\t}\n\n\tif (ext4_handle_valid(handle)) {\n\t\tei->i_sync_tid = handle->h_transaction->t_tid;\n\t\tei->i_datasync_tid = handle->h_transaction->t_tid;\n\t}\n\n\terr = ext4_mark_inode_dirty(handle, inode);\n\tif (err) {\n\t\text4_std_error(sb, err);\n\t\tgoto fail_free_drop;\n\t}\n\n\text4_debug(\"allocating inode %lu\\n\", inode->i_ino);\n\ttrace_ext4_allocate_inode(inode, dir, mode);\n\tbrelse(inode_bitmap_bh);\n\treturn ret;\n\nfail_free_drop:\n\tdquot_free_inode(inode);\nfail_drop:\n\tclear_nlink(inode);\n\tunlock_new_inode(inode);\nout:\n\tdquot_drop(inode);\n\tinode->i_flags |= S_NOQUOTA;\n\tiput(inode);\n\tbrelse(inode_bitmap_bh);\n\treturn ERR_PTR(err);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1696,
            "cve_id": "CVE-2018-10876",
            "code_snippet": "static struct buffer_head *\next4_read_inode_bitmap(struct super_block *sb, ext4_group_t block_group)\n{\n\tstruct ext4_group_desc *desc;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct buffer_head *bh = NULL;\n\text4_fsblk_t bitmap_blk;\n\tint err;\n\n\tdesc = ext4_get_group_desc(sb, block_group, NULL);\n\tif (!desc)\n\t\treturn ERR_PTR(-EFSCORRUPTED);\n\n\tbitmap_blk = ext4_inode_bitmap(sb, desc);\n\tif ((bitmap_blk <= le32_to_cpu(sbi->s_es->s_first_data_block)) ||\n\t    (bitmap_blk >= ext4_blocks_count(sbi->s_es))) {\n\t\text4_error(sb, \"Invalid inode bitmap blk %llu in \"\n\t\t\t   \"block_group %u\", bitmap_blk, block_group);\n\t\text4_mark_group_bitmap_corrupted(sb, block_group,\n\t\t\t\t\tEXT4_GROUP_INFO_IBITMAP_CORRUPT);\n\t\treturn ERR_PTR(-EFSCORRUPTED);\n\t}\n\tbh = sb_getblk(sb, bitmap_blk);\n\tif (unlikely(!bh)) {\n\t\text4_error(sb, \"Cannot read inode bitmap - \"\n\t\t\t    \"block_group = %u, inode_bitmap = %llu\",\n\t\t\t    block_group, bitmap_blk);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tif (bitmap_uptodate(bh))\n\t\tgoto verify;\n\n\tlock_buffer(bh);\n\tif (bitmap_uptodate(bh)) {\n\t\tunlock_buffer(bh);\n\t\tgoto verify;\n\t}\n\n\text4_lock_group(sb, block_group);\n\tif (ext4_has_group_desc_csum(sb) &&\n\t    (desc->bg_flags & cpu_to_le16(EXT4_BG_INODE_UNINIT))) {\n\t\tif (block_group == 0) {\n\t\t\text4_unlock_group(sb, block_group);\n\t\t\tunlock_buffer(bh);\n\t\t\text4_error(sb, \"Inode bitmap for bg 0 marked \"\n\t\t\t\t   \"uninitialized\");\n\t\t\terr = -EFSCORRUPTED;\n\t\t\tgoto out;\n\t\t}\n\t\tmemset(bh->b_data, 0, (EXT4_INODES_PER_GROUP(sb) + 7) / 8);\n\t\text4_mark_bitmap_end(EXT4_INODES_PER_GROUP(sb),\n\t\t\t\t     sb->s_blocksize * 8, bh->b_data);\n\t\tset_bitmap_uptodate(bh);\n\t\tset_buffer_uptodate(bh);\n\t\tset_buffer_verified(bh);\n\t\text4_unlock_group(sb, block_group);\n\t\tunlock_buffer(bh);\n\t\treturn bh;\n\t}\n\text4_unlock_group(sb, block_group);\n\n\tif (buffer_uptodate(bh)) {\n\t\t/*\n\t\t * if not uninit if bh is uptodate,\n\t\t * bitmap is also uptodate\n\t\t */\n\t\tset_bitmap_uptodate(bh);\n\t\tunlock_buffer(bh);\n\t\tgoto verify;\n\t}\n\t/*\n\t * submit the buffer_head for reading\n\t */\n\ttrace_ext4_load_inode_bitmap(sb, block_group);\n\tbh->b_end_io = ext4_end_bitmap_read;\n\tget_bh(bh);\n\tsubmit_bh(REQ_OP_READ, REQ_META | REQ_PRIO, bh);\n\twait_on_buffer(bh);\n\tif (!buffer_uptodate(bh)) {\n\t\tput_bh(bh);\n\t\text4_error(sb, \"Cannot read inode bitmap - \"\n\t\t\t   \"block_group = %u, inode_bitmap = %llu\",\n\t\t\t   block_group, bitmap_blk);\n\t\text4_mark_group_bitmap_corrupted(sb, block_group,\n\t\t\t\tEXT4_GROUP_INFO_IBITMAP_CORRUPT);\n\t\treturn ERR_PTR(-EIO);\n\t}\n\nverify:\n\terr = ext4_validate_inode_bitmap(sb, desc, block_group, bh);\n\tif (err)\n\t\tgoto out;\n\treturn bh;\nout:\n\tput_bh(bh);\n\treturn ERR_PTR(err);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1712,
            "cve_id": "CVE-2018-10902",
            "code_snippet": "int snd_rawmidi_output_params(struct snd_rawmidi_substream *substream,\n\t\t\t      struct snd_rawmidi_params * params)\n{\n\tchar *newbuf, *oldbuf;\n\tstruct snd_rawmidi_runtime *runtime = substream->runtime;\n\t\n\tif (substream->append && substream->use_count > 1)\n\t\treturn -EBUSY;\n\tsnd_rawmidi_drain_output(substream);\n\tif (params->buffer_size < 32 || params->buffer_size > 1024L * 1024L) {\n\t\treturn -EINVAL;\n\t}\n\tif (params->avail_min < 1 || params->avail_min > params->buffer_size) {\n\t\treturn -EINVAL;\n\t}\n\tif (params->buffer_size != runtime->buffer_size) {\n\t\tnewbuf = kmalloc(params->buffer_size, GFP_KERNEL);\n\t\tif (!newbuf)\n\t\t\treturn -ENOMEM;\n\t\tspin_lock_irq(&runtime->lock);\n\t\toldbuf = runtime->buffer;\n\t\truntime->buffer = newbuf;\n\t\truntime->buffer_size = params->buffer_size;\n\t\truntime->avail = runtime->buffer_size;\n\t\truntime->appl_ptr = runtime->hw_ptr = 0;\n\t\tspin_unlock_irq(&runtime->lock);\n\t\tkfree(oldbuf);\n\t}\n\truntime->avail_min = params->avail_min;\n\tsubstream->active_sensing = !params->no_active_sensing;\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1786,
            "cve_id": "CVE-2018-14625",
            "code_snippet": "static int\nvhost_transport_cancel_pkt(struct vsock_sock *vsk)\n{\n\tstruct vhost_vsock *vsock;\n\tstruct virtio_vsock_pkt *pkt, *n;\n\tint cnt = 0;\n\tint ret = -ENODEV;\n\tLIST_HEAD(freeme);\n\n\trcu_read_lock();\n\n\t/* Find the vhost_vsock according to guest context id  */\n\tvsock = vhost_vsock_get(vsk->remote_addr.svm_cid);\n\tif (!vsock)\n\t\tgoto out;\n\n\tspin_lock_bh(&vsock->send_pkt_list_lock);\n\tlist_for_each_entry_safe(pkt, n, &vsock->send_pkt_list, list) {\n\t\tif (pkt->vsk != vsk)\n\t\t\tcontinue;\n\t\tlist_move(&pkt->list, &freeme);\n\t}\n\tspin_unlock_bh(&vsock->send_pkt_list_lock);\n\n\tlist_for_each_entry_safe(pkt, n, &freeme, list) {\n\t\tif (pkt->reply)\n\t\t\tcnt++;\n\t\tlist_del(&pkt->list);\n\t\tvirtio_transport_free_pkt(pkt);\n\t}\n\n\tif (cnt) {\n\t\tstruct vhost_virtqueue *tx_vq = &vsock->vqs[VSOCK_VQ_TX];\n\t\tint new_cnt;\n\n\t\tnew_cnt = atomic_sub_return(cnt, &vsock->queued_replies);\n\t\tif (new_cnt + cnt >= tx_vq->num && new_cnt < tx_vq->num)\n\t\t\tvhost_poll_queue(&tx_vq->poll);\n\t}\n\n\tret = 0;\nout:\n\trcu_read_unlock();\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1787,
            "cve_id": "CVE-2018-14625",
            "code_snippet": "static int\nvhost_transport_send_pkt(struct virtio_vsock_pkt *pkt)\n{\n\tstruct vhost_vsock *vsock;\n\tint len = pkt->len;\n\n\trcu_read_lock();\n\n\t/* Find the vhost_vsock according to guest context id  */\n\tvsock = vhost_vsock_get(le64_to_cpu(pkt->hdr.dst_cid));\n\tif (!vsock) {\n\t\trcu_read_unlock();\n\t\tvirtio_transport_free_pkt(pkt);\n\t\treturn -ENODEV;\n\t}\n\n\tif (pkt->reply)\n\t\tatomic_inc(&vsock->queued_replies);\n\n\tspin_lock_bh(&vsock->send_pkt_list_lock);\n\tlist_add_tail(&pkt->list, &vsock->send_pkt_list);\n\tspin_unlock_bh(&vsock->send_pkt_list_lock);\n\n\tvhost_work_queue(&vsock->dev, &vsock->send_pkt_work);\n\n\trcu_read_unlock();\n\treturn len;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1798,
            "cve_id": "CVE-2018-14734",
            "code_snippet": "static struct ucma_multicast* ucma_alloc_multicast(struct ucma_context *ctx)\n{\n\tstruct ucma_multicast *mc;\n\n\tmc = kzalloc(sizeof(*mc), GFP_KERNEL);\n\tif (!mc)\n\t\treturn NULL;\n\n\tmutex_lock(&mut);\n\tmc->id = idr_alloc(&multicast_idr, NULL, 0, 0, GFP_KERNEL);\n\tmutex_unlock(&mut);\n\tif (mc->id < 0)\n\t\tgoto error;\n\n\tmc->ctx = ctx;\n\tlist_add_tail(&mc->list, &ctx->mc_list);\n\treturn mc;\n\nerror:\n\tkfree(mc);\n\treturn NULL;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1815,
            "cve_id": "CVE-2018-16884",
            "code_snippet": "int\nbc_svc_process(struct svc_serv *serv, struct rpc_rqst *req,\n\t       struct svc_rqst *rqstp)\n{\n\tstruct kvec\t*argv = &rqstp->rq_arg.head[0];\n\tstruct kvec\t*resv = &rqstp->rq_res.head[0];\n\tstruct rpc_task *task;\n\tint proc_error;\n\tint error;\n\n\tdprintk(\"svc: %s(%p)\\n\", __func__, req);\n\n\t/* Build the svc_rqst used by the common processing routine */\n\trqstp->rq_xid = req->rq_xid;\n\trqstp->rq_prot = req->rq_xprt->prot;\n\trqstp->rq_server = serv;\n\trqstp->rq_bc_net = req->rq_xprt->xprt_net;\n\n\trqstp->rq_addrlen = sizeof(req->rq_xprt->addr);\n\tmemcpy(&rqstp->rq_addr, &req->rq_xprt->addr, rqstp->rq_addrlen);\n\tmemcpy(&rqstp->rq_arg, &req->rq_rcv_buf, sizeof(rqstp->rq_arg));\n\tmemcpy(&rqstp->rq_res, &req->rq_snd_buf, sizeof(rqstp->rq_res));\n\n\t/* Adjust the argument buffer length */\n\trqstp->rq_arg.len = req->rq_private_buf.len;\n\tif (rqstp->rq_arg.len <= rqstp->rq_arg.head[0].iov_len) {\n\t\trqstp->rq_arg.head[0].iov_len = rqstp->rq_arg.len;\n\t\trqstp->rq_arg.page_len = 0;\n\t} else if (rqstp->rq_arg.len <= rqstp->rq_arg.head[0].iov_len +\n\t\t\trqstp->rq_arg.page_len)\n\t\trqstp->rq_arg.page_len = rqstp->rq_arg.len -\n\t\t\trqstp->rq_arg.head[0].iov_len;\n\telse\n\t\trqstp->rq_arg.len = rqstp->rq_arg.head[0].iov_len +\n\t\t\trqstp->rq_arg.page_len;\n\n\t/* reset result send buffer \"put\" position */\n\tresv->iov_len = 0;\n\n\t/*\n\t * Skip the next two words because they've already been\n\t * processed in the transport\n\t */\n\tsvc_getu32(argv);\t/* XID */\n\tsvc_getnl(argv);\t/* CALLDIR */\n\n\t/* Parse and execute the bc call */\n\tproc_error = svc_process_common(rqstp, argv, resv);\n\n\tatomic_inc(&req->rq_xprt->bc_free_slots);\n\tif (!proc_error) {\n\t\t/* Processing error: drop the request */\n\t\txprt_free_bc_request(req);\n\t\treturn 0;\n\t}\n\n\t/* Finally, send the reply synchronously */\n\tmemcpy(&req->rq_snd_buf, &rqstp->rq_res, sizeof(req->rq_snd_buf));\n\ttask = rpc_run_bc_task(req);\n\tif (IS_ERR(task)) {\n\t\terror = PTR_ERR(task);\n\t\tgoto out;\n\t}\n\n\tWARN_ON_ONCE(atomic_read(&task->tk_count) != 1);\n\terror = task->tk_status;\n\trpc_put_task(task);\n\nout:\n\tdprintk(\"svc: %s(), error=%d\\n\", __func__, error);\n\treturn error;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1819,
            "cve_id": "CVE-2018-17182",
            "code_snippet": "void dump_mm(const struct mm_struct *mm)\n{\n\tpr_emerg(\"mm %px mmap %px seqnum %llu task_size %lu\\n\"\n#ifdef CONFIG_MMU\n\t\t\"get_unmapped_area %px\\n\"\n#endif\n\t\t\"mmap_base %lu mmap_legacy_base %lu highest_vm_end %lu\\n\"\n\t\t\"pgd %px mm_users %d mm_count %d pgtables_bytes %lu map_count %d\\n\"\n\t\t\"hiwater_rss %lx hiwater_vm %lx total_vm %lx locked_vm %lx\\n\"\n\t\t\"pinned_vm %lx data_vm %lx exec_vm %lx stack_vm %lx\\n\"\n\t\t\"start_code %lx end_code %lx start_data %lx end_data %lx\\n\"\n\t\t\"start_brk %lx brk %lx start_stack %lx\\n\"\n\t\t\"arg_start %lx arg_end %lx env_start %lx env_end %lx\\n\"\n\t\t\"binfmt %px flags %lx core_state %px\\n\"\n#ifdef CONFIG_AIO\n\t\t\"ioctx_table %px\\n\"\n#endif\n#ifdef CONFIG_MEMCG\n\t\t\"owner %px \"\n#endif\n\t\t\"exe_file %px\\n\"\n#ifdef CONFIG_MMU_NOTIFIER\n\t\t\"mmu_notifier_mm %px\\n\"\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\t\t\"numa_next_scan %lu numa_scan_offset %lu numa_scan_seq %d\\n\"\n#endif\n\t\t\"tlb_flush_pending %d\\n\"\n\t\t\"def_flags: %#lx(%pGv)\\n\",\n\n\t\tmm, mm->mmap, (long long) mm->vmacache_seqnum, mm->task_size,\n#ifdef CONFIG_MMU\n\t\tmm->get_unmapped_area,\n#endif\n\t\tmm->mmap_base, mm->mmap_legacy_base, mm->highest_vm_end,\n\t\tmm->pgd, atomic_read(&mm->mm_users),\n\t\tatomic_read(&mm->mm_count),\n\t\tmm_pgtables_bytes(mm),\n\t\tmm->map_count,\n\t\tmm->hiwater_rss, mm->hiwater_vm, mm->total_vm, mm->locked_vm,\n\t\tmm->pinned_vm, mm->data_vm, mm->exec_vm, mm->stack_vm,\n\t\tmm->start_code, mm->end_code, mm->start_data, mm->end_data,\n\t\tmm->start_brk, mm->brk, mm->start_stack,\n\t\tmm->arg_start, mm->arg_end, mm->env_start, mm->env_end,\n\t\tmm->binfmt, mm->flags, mm->core_state,\n#ifdef CONFIG_AIO\n\t\tmm->ioctx_table,\n#endif\n#ifdef CONFIG_MEMCG\n\t\tmm->owner,\n#endif\n\t\tmm->exe_file,\n#ifdef CONFIG_MMU_NOTIFIER\n\t\tmm->mmu_notifier_mm,\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\t\tmm->numa_next_scan, mm->numa_scan_offset, mm->numa_scan_seq,\n#endif\n\t\tatomic_read(&mm->tlb_flush_pending),\n\t\tmm->def_flags, &mm->def_flags\n\t);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1875,
            "cve_id": "CVE-2018-20836",
            "code_snippet": "static void smp_task_done(struct sas_task *task)\n{\n\tdel_timer(&task->slow_task->timer);\n\tcomplete(&task->slow_task->completion);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1883,
            "cve_id": "CVE-2018-20976",
            "code_snippet": "static long\nxfs_fs_nr_cached_objects(\n\tstruct super_block\t*sb,\n\tstruct shrink_control\t*sc)\n{\n\t/* Paranoia: catch incorrect calls during mount setup or teardown */\n\tif (WARN_ON_ONCE(!sb->s_fs_info))\n\t\treturn 0;\n\treturn xfs_reclaim_inodes_count(XFS_M(sb));\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1969,
            "cve_id": "CVE-2018-9465",
            "code_snippet": "static void task_fd_install(\n\tstruct binder_proc *proc, unsigned int fd, struct file *file)\n{\n\tmutex_lock(&proc->files_lock);\n\tif (proc->files)\n\t\t__fd_install(proc->files, fd, file);\n\tmutex_unlock(&proc->files_lock);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1970,
            "cve_id": "CVE-2018-9465",
            "code_snippet": "static int binder_open(struct inode *nodp, struct file *filp)\n{\n\tstruct binder_proc *proc;\n\tstruct binder_device *binder_dev;\n\n\tbinder_debug(BINDER_DEBUG_OPEN_CLOSE, \"binder_open: %d:%d\\n\",\n\t\t     current->group_leader->pid, current->pid);\n\n\tproc = kzalloc(sizeof(*proc), GFP_KERNEL);\n\tif (proc == NULL)\n\t\treturn -ENOMEM;\n\tspin_lock_init(&proc->inner_lock);\n\tspin_lock_init(&proc->outer_lock);\n\tget_task_struct(current->group_leader);\n\tproc->tsk = current->group_leader;\n\tmutex_init(&proc->files_lock);\n\tINIT_LIST_HEAD(&proc->todo);\n\tproc->default_priority = task_nice(current);\n\tbinder_dev = container_of(filp->private_data, struct binder_device,\n\t\t\t\t  miscdev);\n\tproc->context = &binder_dev->context;\n\tbinder_alloc_init(&proc->alloc);\n\n\tbinder_stats_created(BINDER_STAT_PROC);\n\tproc->pid = current->group_leader->pid;\n\tINIT_LIST_HEAD(&proc->delivered_death);\n\tINIT_LIST_HEAD(&proc->waiting_threads);\n\tfilp->private_data = proc;\n\n\tmutex_lock(&binder_procs_lock);\n\thlist_add_head(&proc->proc_node, &binder_procs);\n\tmutex_unlock(&binder_procs_lock);\n\n\tif (binder_debugfs_dir_entry_proc) {\n\t\tchar strbuf[11];\n\n\t\tsnprintf(strbuf, sizeof(strbuf), \"%u\", proc->pid);\n\t\t/*\n\t\t * proc debug entries are shared between contexts, so\n\t\t * this will fail if the process tries to open the driver\n\t\t * again with a different context. The priting code will\n\t\t * anyway print all contexts that a given PID has, so this\n\t\t * is not a problem.\n\t\t */\n\t\tproc->debugfs_entry = debugfs_create_file(strbuf, S_IRUGO,\n\t\t\tbinder_debugfs_dir_entry_proc,\n\t\t\t(void *)(unsigned long)proc->pid,\n\t\t\t&binder_proc_fops);\n\t}\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1986,
            "cve_id": "CVE-2019-10125",
            "code_snippet": "static int aio_fsync(struct fsync_iocb *req, const struct iocb *iocb,\n\t\t     bool datasync)\n{\n\tif (unlikely(iocb->aio_buf || iocb->aio_offset || iocb->aio_nbytes ||\n\t\t\tiocb->aio_rw_flags))\n\t\treturn -EINVAL;\n\n\tif (unlikely(!req->file->f_op->fsync))\n\t\treturn -EINVAL;\n\n\treq->datasync = datasync;\n\tINIT_WORK(&req->work, aio_fsync_work);\n\tschedule_work(&req->work);\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1987,
            "cve_id": "CVE-2019-10125",
            "code_snippet": "static inline void aio_poll_complete(struct aio_kiocb *iocb, __poll_t mask)\n{\n\taio_complete(iocb, mangle_poll(mask), 0);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1988,
            "cve_id": "CVE-2019-10125",
            "code_snippet": "static ssize_t aio_poll(struct aio_kiocb *aiocb, const struct iocb *iocb)\n{\n\tstruct kioctx *ctx = aiocb->ki_ctx;\n\tstruct poll_iocb *req = &aiocb->poll;\n\tstruct aio_poll_table apt;\n\t__poll_t mask;\n\n\t/* reject any unknown events outside the normal event mask. */\n\tif ((u16)iocb->aio_buf != iocb->aio_buf)\n\t\treturn -EINVAL;\n\t/* reject fields that are not defined for poll */\n\tif (iocb->aio_offset || iocb->aio_nbytes || iocb->aio_rw_flags)\n\t\treturn -EINVAL;\n\n\tINIT_WORK(&req->work, aio_poll_complete_work);\n\treq->events = demangle_poll(iocb->aio_buf) | EPOLLERR | EPOLLHUP;\n\n\treq->head = NULL;\n\treq->woken = false;\n\treq->cancelled = false;\n\n\tapt.pt._qproc = aio_poll_queue_proc;\n\tapt.pt._key = req->events;\n\tapt.iocb = aiocb;\n\tapt.error = -EINVAL; /* same as no support for IOCB_CMD_POLL */\n\n\t/* initialized the list so that we can do list_empty checks */\n\tINIT_LIST_HEAD(&req->wait.entry);\n\tinit_waitqueue_func_entry(&req->wait, aio_poll_wake);\n\n\t/* one for removal from waitqueue, one for this function */\n\trefcount_set(&aiocb->ki_refcnt, 2);\n\n\tmask = vfs_poll(req->file, &apt.pt) & req->events;\n\tif (unlikely(!req->head)) {\n\t\t/* we did not manage to set up a waitqueue, done */\n\t\tgoto out;\n\t}\n\n\tspin_lock_irq(&ctx->ctx_lock);\n\tspin_lock(&req->head->lock);\n\tif (req->woken) {\n\t\t/* wake_up context handles the rest */\n\t\tmask = 0;\n\t\tapt.error = 0;\n\t} else if (mask || apt.error) {\n\t\t/* if we get an error or a mask we are done */\n\t\tWARN_ON_ONCE(list_empty(&req->wait.entry));\n\t\tlist_del_init(&req->wait.entry);\n\t} else {\n\t\t/* actually waiting for an event */\n\t\tlist_add_tail(&aiocb->ki_list, &ctx->active_reqs);\n\t\taiocb->ki_cancel = aio_poll_cancel;\n\t}\n\tspin_unlock(&req->head->lock);\n\tspin_unlock_irq(&ctx->ctx_lock);\n\nout:\n\tif (unlikely(apt.error))\n\t\treturn apt.error;\n\n\tif (mask)\n\t\taio_poll_complete(aiocb, mask);\n\tiocb_put(aiocb);\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 1989,
            "cve_id": "CVE-2019-10125",
            "code_snippet": "static void aio_complete_rw(struct kiocb *kiocb, long res, long res2)\n{\n\tstruct aio_kiocb *iocb = container_of(kiocb, struct aio_kiocb, rw);\n\n\tif (!list_empty_careful(&iocb->ki_list))\n\t\taio_remove_iocb(iocb);\n\n\tif (kiocb->ki_flags & IOCB_WRITE) {\n\t\tstruct inode *inode = file_inode(kiocb->ki_filp);\n\n\t\t/*\n\t\t * Tell lockdep we inherited freeze protection from submission\n\t\t * thread.\n\t\t */\n\t\tif (S_ISREG(inode->i_mode))\n\t\t\t__sb_writers_acquired(inode->i_sb, SB_FREEZE_WRITE);\n\t\tfile_end_write(kiocb->ki_filp);\n\t}\n\n\taio_complete(iocb, res, res2);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2019,
            "cve_id": "CVE-2019-11487",
            "code_snippet": "bool generic_pipe_buf_get(struct pipe_inode_info *pipe, struct pipe_buffer *buf)\n{\n\treturn try_get_page(buf->page);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2020,
            "cve_id": "CVE-2019-11487",
            "code_snippet": "static int splice_pipe_to_pipe(struct pipe_inode_info *ipipe,\n\t\t\t       struct pipe_inode_info *opipe,\n\t\t\t       size_t len, unsigned int flags)\n{\n\tstruct pipe_buffer *ibuf, *obuf;\n\tint ret = 0, nbuf;\n\tbool input_wakeup = false;\n\n\nretry:\n\tret = ipipe_prep(ipipe, flags);\n\tif (ret)\n\t\treturn ret;\n\n\tret = opipe_prep(opipe, flags);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Potential ABBA deadlock, work around it by ordering lock\n\t * grabbing by pipe info address. Otherwise two different processes\n\t * could deadlock (one doing tee from A -> B, the other from B -> A).\n\t */\n\tpipe_double_lock(ipipe, opipe);\n\n\tdo {\n\t\tif (!opipe->readers) {\n\t\t\tsend_sig(SIGPIPE, current, 0);\n\t\t\tif (!ret)\n\t\t\t\tret = -EPIPE;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!ipipe->nrbufs && !ipipe->writers)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Cannot make any progress, because either the input\n\t\t * pipe is empty or the output pipe is full.\n\t\t */\n\t\tif (!ipipe->nrbufs || opipe->nrbufs >= opipe->buffers) {\n\t\t\t/* Already processed some buffers, break */\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\n\t\t\tif (flags & SPLICE_F_NONBLOCK) {\n\t\t\t\tret = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * We raced with another reader/writer and haven't\n\t\t\t * managed to process any buffers.  A zero return\n\t\t\t * value means EOF, so retry instead.\n\t\t\t */\n\t\t\tpipe_unlock(ipipe);\n\t\t\tpipe_unlock(opipe);\n\t\t\tgoto retry;\n\t\t}\n\n\t\tibuf = ipipe->bufs + ipipe->curbuf;\n\t\tnbuf = (opipe->curbuf + opipe->nrbufs) & (opipe->buffers - 1);\n\t\tobuf = opipe->bufs + nbuf;\n\n\t\tif (len >= ibuf->len) {\n\t\t\t/*\n\t\t\t * Simply move the whole buffer from ipipe to opipe\n\t\t\t */\n\t\t\t*obuf = *ibuf;\n\t\t\tibuf->ops = NULL;\n\t\t\topipe->nrbufs++;\n\t\t\tipipe->curbuf = (ipipe->curbuf + 1) & (ipipe->buffers - 1);\n\t\t\tipipe->nrbufs--;\n\t\t\tinput_wakeup = true;\n\t\t} else {\n\t\t\t/*\n\t\t\t * Get a reference to this pipe buffer,\n\t\t\t * so we can copy the contents over.\n\t\t\t */\n\t\t\tif (!pipe_buf_get(ipipe, ibuf)) {\n\t\t\t\tif (ret == 0)\n\t\t\t\t\tret = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t*obuf = *ibuf;\n\n\t\t\t/*\n\t\t\t * Don't inherit the gift flag, we need to\n\t\t\t * prevent multiple steals of this page.\n\t\t\t */\n\t\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n\n\t\t\tobuf->len = len;\n\t\t\topipe->nrbufs++;\n\t\t\tibuf->offset += obuf->len;\n\t\t\tibuf->len -= obuf->len;\n\t\t}\n\t\tret += obuf->len;\n\t\tlen -= obuf->len;\n\t} while (len);\n\n\tpipe_unlock(ipipe);\n\tpipe_unlock(opipe);\n\n\t/*\n\t * If we put data in the output pipe, wakeup any potential readers.\n\t */\n\tif (ret > 0)\n\t\twakeup_pipe_readers(opipe);\n\n\tif (input_wakeup)\n\t\twakeup_pipe_writers(ipipe);\n\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2034,
            "cve_id": "CVE-2019-11811",
            "code_snippet": "int ipmi_si_mem_setup(struct si_sm_io *io)\n{\n\tunsigned long addr = io->addr_data;\n\tint           mapsize, idx;\n\n\tif (!addr)\n\t\treturn -ENODEV;\n\n\t/*\n\t * Figure out the actual readb/readw/readl/etc routine to use based\n\t * upon the register size.\n\t */\n\tswitch (io->regsize) {\n\tcase 1:\n\t\tio->inputb = intf_mem_inb;\n\t\tio->outputb = intf_mem_outb;\n\t\tbreak;\n\tcase 2:\n\t\tio->inputb = intf_mem_inw;\n\t\tio->outputb = intf_mem_outw;\n\t\tbreak;\n\tcase 4:\n\t\tio->inputb = intf_mem_inl;\n\t\tio->outputb = intf_mem_outl;\n\t\tbreak;\n#ifdef readq\n\tcase 8:\n\t\tio->inputb = mem_inq;\n\t\tio->outputb = mem_outq;\n\t\tbreak;\n#endif\n\tdefault:\n\t\tdev_warn(io->dev, \"Invalid register size: %d\\n\",\n\t\t\t io->regsize);\n\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Some BIOSes reserve disjoint memory regions in their ACPI\n\t * tables.  This causes problems when trying to request the\n\t * entire region.  Therefore we must request each register\n\t * separately.\n\t */\n\tfor (idx = 0; idx < io->io_size; idx++) {\n\t\tif (request_mem_region(addr + idx * io->regspacing,\n\t\t\t\t       io->regsize, DEVICE_NAME) == NULL) {\n\t\t\t/* Undo allocations */\n\t\t\tmem_region_cleanup(io, idx);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\t/*\n\t * Calculate the total amount of memory to claim.  This is an\n\t * unusual looking calculation, but it avoids claiming any\n\t * more memory than it has to.  It will claim everything\n\t * between the first address to the end of the last full\n\t * register.\n\t */\n\tmapsize = ((io->io_size * io->regspacing)\n\t\t   - (io->regspacing - io->regsize));\n\tio->addr = ioremap(addr, mapsize);\n\tif (io->addr == NULL) {\n\t\tmem_region_cleanup(io, io->io_size);\n\t\treturn -EIO;\n\t}\n\n\tio->io_cleanup = mem_cleanup;\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2059,
            "cve_id": "CVE-2019-13233",
            "code_snippet": "int insn_get_code_seg_params(struct pt_regs *regs)\n{\n\tstruct desc_struct desc;\n\tshort sel;\n\n\tif (v8086_mode(regs))\n\t\t/* Address and operand size are both 16-bit. */\n\t\treturn INSN_CODE_SEG_PARAMS(2, 2);\n\n\tsel = get_segment_selector(regs, INAT_SEG_REG_CS);\n\tif (sel < 0)\n\t\treturn sel;\n\n\tif (!get_desc(&desc, sel))\n\t\treturn -EINVAL;\n\n\t/*\n\t * The most significant byte of the Type field of the segment descriptor\n\t * determines whether a segment contains data or code. If this is a data\n\t * segment, return error.\n\t */\n\tif (!(desc.type & BIT(3)))\n\t\treturn -EINVAL;\n\n\tswitch ((desc.l << 1) | desc.d) {\n\tcase 0: /*\n\t\t * Legacy mode. CS.L=0, CS.D=0. Address and operand size are\n\t\t * both 16-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(2, 2);\n\tcase 1: /*\n\t\t * Legacy mode. CS.L=0, CS.D=1. Address and operand size are\n\t\t * both 32-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(4, 4);\n\tcase 2: /*\n\t\t * IA-32e 64-bit mode. CS.L=1, CS.D=0. Address size is 64-bit;\n\t\t * operand size is 32-bit.\n\t\t */\n\t\treturn INSN_CODE_SEG_PARAMS(4, 8);\n\tcase 3: /* Invalid setting. CS.L=1, CS.D=1 */\n\t\t/* fall through */\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2110,
            "cve_id": "CVE-2019-15220",
            "code_snippet": "static int p54u_probe(struct usb_interface *intf,\n\t\t\t\tconst struct usb_device_id *id)\n{\n\tstruct usb_device *udev = interface_to_usbdev(intf);\n\tstruct ieee80211_hw *dev;\n\tstruct p54u_priv *priv;\n\tint err;\n\tunsigned int i, recognized_pipes;\n\n\tdev = p54_init_common(sizeof(*priv));\n\n\tif (!dev) {\n\t\tdev_err(&udev->dev, \"(p54usb) ieee80211 alloc failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tpriv = dev->priv;\n\tpriv->hw_type = P54U_INVALID_HW;\n\n\tSET_IEEE80211_DEV(dev, &intf->dev);\n\tusb_set_intfdata(intf, dev);\n\tpriv->udev = udev;\n\tpriv->intf = intf;\n\tskb_queue_head_init(&priv->rx_queue);\n\tinit_usb_anchor(&priv->submitted);\n\n\t/* really lazy and simple way of figuring out if we're a 3887 */\n\t/* TODO: should just stick the identification in the device table */\n\ti = intf->altsetting->desc.bNumEndpoints;\n\trecognized_pipes = 0;\n\twhile (i--) {\n\t\tswitch (intf->altsetting->endpoint[i].desc.bEndpointAddress) {\n\t\tcase P54U_PIPE_DATA:\n\t\tcase P54U_PIPE_MGMT:\n\t\tcase P54U_PIPE_BRG:\n\t\tcase P54U_PIPE_DEV:\n\t\tcase P54U_PIPE_DATA | USB_DIR_IN:\n\t\tcase P54U_PIPE_MGMT | USB_DIR_IN:\n\t\tcase P54U_PIPE_BRG | USB_DIR_IN:\n\t\tcase P54U_PIPE_DEV | USB_DIR_IN:\n\t\tcase P54U_PIPE_INT | USB_DIR_IN:\n\t\t\trecognized_pipes++;\n\t\t}\n\t}\n\tpriv->common.open = p54u_open;\n\tpriv->common.stop = p54u_stop;\n\tif (recognized_pipes < P54U_PIPE_NUMBER) {\n#ifdef CONFIG_PM\n\t\t/* ISL3887 needs a full reset on resume */\n\t\tudev->reset_resume = 1;\n#endif /* CONFIG_PM */\n\t\terr = p54u_device_reset(dev);\n\n\t\tpriv->hw_type = P54U_3887;\n\t\tdev->extra_tx_headroom += sizeof(struct lm87_tx_hdr);\n\t\tpriv->common.tx_hdr_len = sizeof(struct lm87_tx_hdr);\n\t\tpriv->common.tx = p54u_tx_lm87;\n\t\tpriv->upload_fw = p54u_upload_firmware_3887;\n\t} else {\n\t\tpriv->hw_type = P54U_NET2280;\n\t\tdev->extra_tx_headroom += sizeof(struct net2280_tx_hdr);\n\t\tpriv->common.tx_hdr_len = sizeof(struct net2280_tx_hdr);\n\t\tpriv->common.tx = p54u_tx_net2280;\n\t\tpriv->upload_fw = p54u_upload_firmware_net2280;\n\t}\n\terr = p54u_load_firmware(dev, intf);\n\tif (err)\n\t\tp54_free_common(dev);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2123,
            "cve_id": "CVE-2019-15292",
            "code_snippet": "int __init atalk_register_sysctl(void)\n{\n\tatalk_table_header = register_net_sysctl(&init_net, \"net/appletalk\", atalk_table);\n\tif (!atalk_table_header)\n\t\treturn -ENOMEM;\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2200,
            "cve_id": "CVE-2019-18683",
            "code_snippet": "static int vivid_thread_vid_cap(void *data)\n{\n\tstruct vivid_dev *dev = data;\n\tu64 numerators_since_start;\n\tu64 buffers_since_start;\n\tu64 next_jiffies_since_start;\n\tunsigned long jiffies_since_start;\n\tunsigned long cur_jiffies;\n\tunsigned wait_jiffies;\n\tunsigned numerator;\n\tunsigned denominator;\n\tint dropped_bufs;\n\n\tdprintk(dev, 1, \"Video Capture Thread Start\\n\");\n\n\tset_freezable();\n\n\t/* Resets frame counters */\n\tdev->cap_seq_offset = 0;\n\tdev->cap_seq_count = 0;\n\tdev->cap_seq_resync = false;\n\tdev->jiffies_vid_cap = jiffies;\n\tdev->cap_stream_start = ktime_get_ns();\n\tvivid_cap_update_frame_period(dev);\n\n\tfor (;;) {\n\t\ttry_to_freeze();\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tif (!mutex_trylock(&dev->mutex)) {\n\t\t\tschedule_timeout_uninterruptible(1);\n\t\t\tcontinue;\n\t\t}\n\n\t\tcur_jiffies = jiffies;\n\t\tif (dev->cap_seq_resync) {\n\t\t\tdev->jiffies_vid_cap = cur_jiffies;\n\t\t\tdev->cap_seq_offset = dev->cap_seq_count + 1;\n\t\t\tdev->cap_seq_count = 0;\n\t\t\tdev->cap_stream_start += dev->cap_frame_period *\n\t\t\t\t\t\t dev->cap_seq_offset;\n\t\t\tvivid_cap_update_frame_period(dev);\n\t\t\tdev->cap_seq_resync = false;\n\t\t}\n\t\tnumerator = dev->timeperframe_vid_cap.numerator;\n\t\tdenominator = dev->timeperframe_vid_cap.denominator;\n\n\t\tif (dev->field_cap == V4L2_FIELD_ALTERNATE)\n\t\t\tdenominator *= 2;\n\n\t\t/* Calculate the number of jiffies since we started streaming */\n\t\tjiffies_since_start = cur_jiffies - dev->jiffies_vid_cap;\n\t\t/* Get the number of buffers streamed since the start */\n\t\tbuffers_since_start = (u64)jiffies_since_start * denominator +\n\t\t\t\t      (HZ * numerator) / 2;\n\t\tdo_div(buffers_since_start, HZ * numerator);\n\n\t\t/*\n\t\t * After more than 0xf0000000 (rounded down to a multiple of\n\t\t * 'jiffies-per-day' to ease jiffies_to_msecs calculation)\n\t\t * jiffies have passed since we started streaming reset the\n\t\t * counters and keep track of the sequence offset.\n\t\t */\n\t\tif (jiffies_since_start > JIFFIES_RESYNC) {\n\t\t\tdev->jiffies_vid_cap = cur_jiffies;\n\t\t\tdev->cap_seq_offset = buffers_since_start;\n\t\t\tbuffers_since_start = 0;\n\t\t}\n\t\tdropped_bufs = buffers_since_start + dev->cap_seq_offset - dev->cap_seq_count;\n\t\tdev->cap_seq_count = buffers_since_start + dev->cap_seq_offset;\n\t\tdev->vid_cap_seq_count = dev->cap_seq_count - dev->vid_cap_seq_start;\n\t\tdev->vbi_cap_seq_count = dev->cap_seq_count - dev->vbi_cap_seq_start;\n\t\tdev->meta_cap_seq_count = dev->cap_seq_count - dev->meta_cap_seq_start;\n\n\t\tvivid_thread_vid_cap_tick(dev, dropped_bufs);\n\n\t\t/*\n\t\t * Calculate the number of 'numerators' streamed since we started,\n\t\t * including the current buffer.\n\t\t */\n\t\tnumerators_since_start = ++buffers_since_start * numerator;\n\n\t\t/* And the number of jiffies since we started */\n\t\tjiffies_since_start = jiffies - dev->jiffies_vid_cap;\n\n\t\tmutex_unlock(&dev->mutex);\n\n\t\t/*\n\t\t * Calculate when that next buffer is supposed to start\n\t\t * in jiffies since we started streaming.\n\t\t */\n\t\tnext_jiffies_since_start = numerators_since_start * HZ +\n\t\t\t\t\t   denominator / 2;\n\t\tdo_div(next_jiffies_since_start, denominator);\n\t\t/* If it is in the past, then just schedule asap */\n\t\tif (next_jiffies_since_start < jiffies_since_start)\n\t\t\tnext_jiffies_since_start = jiffies_since_start;\n\n\t\twait_jiffies = next_jiffies_since_start - jiffies_since_start;\n\t\tschedule_timeout_interruptible(wait_jiffies ? wait_jiffies : 1);\n\t}\n\tdprintk(dev, 1, \"Video Capture Thread End\\n\");\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2201,
            "cve_id": "CVE-2019-18683",
            "code_snippet": "static int vivid_thread_vid_out(void *data)\n{\n\tstruct vivid_dev *dev = data;\n\tu64 numerators_since_start;\n\tu64 buffers_since_start;\n\tu64 next_jiffies_since_start;\n\tunsigned long jiffies_since_start;\n\tunsigned long cur_jiffies;\n\tunsigned wait_jiffies;\n\tunsigned numerator;\n\tunsigned denominator;\n\n\tdprintk(dev, 1, \"Video Output Thread Start\\n\");\n\n\tset_freezable();\n\n\t/* Resets frame counters */\n\tdev->out_seq_offset = 0;\n\tif (dev->seq_wrap)\n\t\tdev->out_seq_count = 0xffffff80U;\n\tdev->jiffies_vid_out = jiffies;\n\tdev->vid_out_seq_start = dev->vbi_out_seq_start = 0;\n\tdev->meta_out_seq_start = 0;\n\tdev->out_seq_resync = false;\n\n\tfor (;;) {\n\t\ttry_to_freeze();\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tif (!mutex_trylock(&dev->mutex)) {\n\t\t\tschedule_timeout_uninterruptible(1);\n\t\t\tcontinue;\n\t\t}\n\n\t\tcur_jiffies = jiffies;\n\t\tif (dev->out_seq_resync) {\n\t\t\tdev->jiffies_vid_out = cur_jiffies;\n\t\t\tdev->out_seq_offset = dev->out_seq_count + 1;\n\t\t\tdev->out_seq_count = 0;\n\t\t\tdev->out_seq_resync = false;\n\t\t}\n\t\tnumerator = dev->timeperframe_vid_out.numerator;\n\t\tdenominator = dev->timeperframe_vid_out.denominator;\n\n\t\tif (dev->field_out == V4L2_FIELD_ALTERNATE)\n\t\t\tdenominator *= 2;\n\n\t\t/* Calculate the number of jiffies since we started streaming */\n\t\tjiffies_since_start = cur_jiffies - dev->jiffies_vid_out;\n\t\t/* Get the number of buffers streamed since the start */\n\t\tbuffers_since_start = (u64)jiffies_since_start * denominator +\n\t\t\t\t      (HZ * numerator) / 2;\n\t\tdo_div(buffers_since_start, HZ * numerator);\n\n\t\t/*\n\t\t * After more than 0xf0000000 (rounded down to a multiple of\n\t\t * 'jiffies-per-day' to ease jiffies_to_msecs calculation)\n\t\t * jiffies have passed since we started streaming reset the\n\t\t * counters and keep track of the sequence offset.\n\t\t */\n\t\tif (jiffies_since_start > JIFFIES_RESYNC) {\n\t\t\tdev->jiffies_vid_out = cur_jiffies;\n\t\t\tdev->out_seq_offset = buffers_since_start;\n\t\t\tbuffers_since_start = 0;\n\t\t}\n\t\tdev->out_seq_count = buffers_since_start + dev->out_seq_offset;\n\t\tdev->vid_out_seq_count = dev->out_seq_count - dev->vid_out_seq_start;\n\t\tdev->vbi_out_seq_count = dev->out_seq_count - dev->vbi_out_seq_start;\n\t\tdev->meta_out_seq_count = dev->out_seq_count - dev->meta_out_seq_start;\n\n\t\tvivid_thread_vid_out_tick(dev);\n\t\tmutex_unlock(&dev->mutex);\n\n\t\t/*\n\t\t * Calculate the number of 'numerators' streamed since we started,\n\t\t * not including the current buffer.\n\t\t */\n\t\tnumerators_since_start = buffers_since_start * numerator;\n\n\t\t/* And the number of jiffies since we started */\n\t\tjiffies_since_start = jiffies - dev->jiffies_vid_out;\n\n\t\t/* Increase by the 'numerator' of one buffer */\n\t\tnumerators_since_start += numerator;\n\t\t/*\n\t\t * Calculate when that next buffer is supposed to start\n\t\t * in jiffies since we started streaming.\n\t\t */\n\t\tnext_jiffies_since_start = numerators_since_start * HZ +\n\t\t\t\t\t   denominator / 2;\n\t\tdo_div(next_jiffies_since_start, denominator);\n\t\t/* If it is in the past, then just schedule asap */\n\t\tif (next_jiffies_since_start < jiffies_since_start)\n\t\t\tnext_jiffies_since_start = jiffies_since_start;\n\n\t\twait_jiffies = next_jiffies_since_start - jiffies_since_start;\n\t\tschedule_timeout_interruptible(wait_jiffies ? wait_jiffies : 1);\n\t}\n\tdprintk(dev, 1, \"Video Output Thread End\\n\");\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2297,
            "cve_id": "CVE-2019-19319",
            "code_snippet": "static int __check_block_validity(struct inode *inode, const char *func,\n\t\t\t\tunsigned int line,\n\t\t\t\tstruct ext4_map_blocks *map)\n{\n\tif (ext4_has_feature_journal(inode->i_sb) &&\n\t    (inode->i_ino ==\n\t     le32_to_cpu(EXT4_SB(inode->i_sb)->s_es->s_journal_inum)))\n\t\treturn 0;\n\tif (!ext4_data_block_valid(EXT4_SB(inode->i_sb), map->m_pblk,\n\t\t\t\t   map->m_len)) {\n\t\text4_error_inode(inode, func, line, map->m_pblk,\n\t\t\t\t \"lblock %lu mapped to illegal pblock %llu \"\n\t\t\t\t \"(length %d)\", (unsigned long) map->m_lblk,\n\t\t\t\t map->m_pblk, map->m_len);\n\t\treturn -EFSCORRUPTED;\n\t}\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2332,
            "cve_id": "CVE-2019-19767",
            "code_snippet": "static void ext4_clamp_want_extra_isize(struct super_block *sb)\n{\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_super_block *es = sbi->s_es;\n\tunsigned def_extra_isize = sizeof(struct ext4_inode) -\n\t\t\t\t\t\tEXT4_GOOD_OLD_INODE_SIZE;\n\n\tif (sbi->s_inode_size == EXT4_GOOD_OLD_INODE_SIZE) {\n\t\tsbi->s_want_extra_isize = 0;\n\t\treturn;\n\t}\n\tif (sbi->s_want_extra_isize < 4) {\n\t\tsbi->s_want_extra_isize = def_extra_isize;\n\t\tif (ext4_has_feature_extra_isize(sb)) {\n\t\t\tif (sbi->s_want_extra_isize <\n\t\t\t    le16_to_cpu(es->s_want_extra_isize))\n\t\t\t\tsbi->s_want_extra_isize =\n\t\t\t\t\tle16_to_cpu(es->s_want_extra_isize);\n\t\t\tif (sbi->s_want_extra_isize <\n\t\t\t    le16_to_cpu(es->s_min_extra_isize))\n\t\t\t\tsbi->s_want_extra_isize =\n\t\t\t\t\tle16_to_cpu(es->s_min_extra_isize);\n\t\t}\n\t}\n\t/* Check if enough inode space is available */\n\tif ((sbi->s_want_extra_isize > sbi->s_inode_size) ||\n\t    (EXT4_GOOD_OLD_INODE_SIZE + sbi->s_want_extra_isize >\n\t\t\t\t\t\t\tsbi->s_inode_size)) {\n\t\tsbi->s_want_extra_isize = def_extra_isize;\n\t\text4_msg(sb, KERN_INFO,\n\t\t\t \"required extra inode space not available\");\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2334,
            "cve_id": "CVE-2019-19768",
            "code_snippet": "static void blk_add_trace_rq(struct request *rq, int error,\n\t\t\t     unsigned int nr_bytes, u32 what, u64 cgid)\n{\n\tstruct blk_trace *bt;\n\n\trcu_read_lock();\n\tbt = rcu_dereference(rq->q->blk_trace);\n\tif (likely(!bt)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\tif (blk_rq_is_passthrough(rq))\n\t\twhat |= BLK_TC_ACT(BLK_TC_PC);\n\telse\n\t\twhat |= BLK_TC_ACT(BLK_TC_FS);\n\n\t__blk_add_trace(bt, blk_rq_trace_sector(rq), nr_bytes, req_op(rq),\n\t\t\trq->cmd_flags, what, error, 0, NULL, cgid);\n\trcu_read_unlock();\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2335,
            "cve_id": "CVE-2019-19768",
            "code_snippet": "static int blk_trace_remove_queue(struct request_queue *q)\n{\n\tstruct blk_trace *bt;\n\n\tbt = xchg(&q->blk_trace, NULL);\n\tif (bt == NULL)\n\t\treturn -EINVAL;\n\n\tput_probe_ref();\n\tsynchronize_rcu();\n\tblk_trace_free(bt);\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2336,
            "cve_id": "CVE-2019-19768",
            "code_snippet": "static void blk_add_trace_getrq(void *ignore,\n\t\t\t\tstruct request_queue *q,\n\t\t\t\tstruct bio *bio, int rw)\n{\n\tif (bio)\n\t\tblk_add_trace_bio(q, bio, BLK_TA_GETRQ, 0);\n\telse {\n\t\tstruct blk_trace *bt;\n\n\t\trcu_read_lock();\n\t\tbt = rcu_dereference(q->blk_trace);\n\t\tif (bt)\n\t\t\t__blk_add_trace(bt, 0, 0, rw, 0, BLK_TA_GETRQ, 0, 0,\n\t\t\t\t\tNULL, 0);\n\t\trcu_read_unlock();\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2337,
            "cve_id": "CVE-2019-19768",
            "code_snippet": "static void blk_add_trace_bio(struct request_queue *q, struct bio *bio,\n\t\t\t      u32 what, int error)\n{\n\tstruct blk_trace *bt;\n\n\trcu_read_lock();\n\tbt = rcu_dereference(q->blk_trace);\n\tif (likely(!bt)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\t__blk_add_trace(bt, bio->bi_iter.bi_sector, bio->bi_iter.bi_size,\n\t\t\tbio_op(bio), bio->bi_opf, what, error, 0, NULL,\n\t\t\tblk_trace_bio_get_cgid(q, bio));\n\trcu_read_unlock();\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2338,
            "cve_id": "CVE-2019-19768",
            "code_snippet": "void blk_add_driver_data(struct request_queue *q,\n\t\t\t struct request *rq,\n\t\t\t void *data, size_t len)\n{\n\tstruct blk_trace *bt;\n\n\trcu_read_lock();\n\tbt = rcu_dereference(q->blk_trace);\n\tif (likely(!bt)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\t__blk_add_trace(bt, blk_rq_trace_sector(rq), blk_rq_bytes(rq), 0, 0,\n\t\t\t\tBLK_TA_DRV_DATA, 0, len, data,\n\t\t\t\tblk_trace_request_get_cgid(q, rq));\n\trcu_read_unlock();\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2339,
            "cve_id": "CVE-2019-19768",
            "code_snippet": "static ssize_t sysfs_blk_trace_attr_show(struct device *dev,\n\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t char *buf)\n{\n\tstruct hd_struct *p = dev_to_part(dev);\n\tstruct request_queue *q;\n\tstruct block_device *bdev;\n\tstruct blk_trace *bt;\n\tssize_t ret = -ENXIO;\n\n\tbdev = bdget(part_devt(p));\n\tif (bdev == NULL)\n\t\tgoto out;\n\n\tq = blk_trace_get_queue(bdev);\n\tif (q == NULL)\n\t\tgoto out_bdput;\n\n\tmutex_lock(&q->blk_trace_mutex);\n\n\tbt = rcu_dereference_protected(q->blk_trace,\n\t\t\t\t       lockdep_is_held(&q->blk_trace_mutex));\n\tif (attr == &dev_attr_enable) {\n\t\tret = sprintf(buf, \"%u\\n\", !!bt);\n\t\tgoto out_unlock_bdev;\n\t}\n\n\tif (bt == NULL)\n\t\tret = sprintf(buf, \"disabled\\n\");\n\telse if (attr == &dev_attr_act_mask)\n\t\tret = blk_trace_mask2str(buf, bt->act_mask);\n\telse if (attr == &dev_attr_pid)\n\t\tret = sprintf(buf, \"%u\\n\", bt->pid);\n\telse if (attr == &dev_attr_start_lba)\n\t\tret = sprintf(buf, \"%llu\\n\", bt->start_lba);\n\telse if (attr == &dev_attr_end_lba)\n\t\tret = sprintf(buf, \"%llu\\n\", bt->end_lba);\n\nout_unlock_bdev:\n\tmutex_unlock(&q->blk_trace_mutex);\nout_bdput:\n\tbdput(bdev);\nout:\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2340,
            "cve_id": "CVE-2019-19768",
            "code_snippet": "static void blk_trace_cleanup(struct blk_trace *bt)\n{\n\tsynchronize_rcu();\n\tblk_trace_free(bt);\n\tput_probe_ref();\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2341,
            "cve_id": "CVE-2019-19768",
            "code_snippet": "static void blk_add_trace_split(void *ignore,\n\t\t\t\tstruct request_queue *q, struct bio *bio,\n\t\t\t\tunsigned int pdu)\n{\n\tstruct blk_trace *bt;\n\n\trcu_read_lock();\n\tbt = rcu_dereference(q->blk_trace);\n\tif (bt) {\n\t\t__be64 rpdu = cpu_to_be64(pdu);\n\n\t\t__blk_add_trace(bt, bio->bi_iter.bi_sector,\n\t\t\t\tbio->bi_iter.bi_size, bio_op(bio), bio->bi_opf,\n\t\t\t\tBLK_TA_SPLIT, bio->bi_status, sizeof(rpdu),\n\t\t\t\t&rpdu, blk_trace_bio_get_cgid(q, bio));\n\t}\n\trcu_read_unlock();\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2354,
            "cve_id": "CVE-2019-19813",
            "code_snippet": "struct extent_map *btrfs_get_extent(struct btrfs_inode *inode,\n\t\t\t\t    struct page *page,\n\t\t\t\t    size_t pg_offset, u64 start, u64 len,\n\t\t\t\t    int create)\n{\n\tstruct btrfs_fs_info *fs_info = inode->root->fs_info;\n\tint ret;\n\tint err = 0;\n\tu64 extent_start = 0;\n\tu64 extent_end = 0;\n\tu64 objectid = btrfs_ino(inode);\n\tu8 extent_type;\n\tstruct btrfs_path *path = NULL;\n\tstruct btrfs_root *root = inode->root;\n\tstruct btrfs_file_extent_item *item;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_key found_key;\n\tstruct extent_map *em = NULL;\n\tstruct extent_map_tree *em_tree = &inode->extent_tree;\n\tstruct extent_io_tree *io_tree = &inode->io_tree;\n\tconst bool new_inline = !page || create;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, start, len);\n\tif (em)\n\t\tem->bdev = fs_info->fs_devices->latest_bdev;\n\tread_unlock(&em_tree->lock);\n\n\tif (em) {\n\t\tif (em->start > start || em->start + em->len <= start)\n\t\t\tfree_extent_map(em);\n\t\telse if (em->block_start == EXTENT_MAP_INLINE && page)\n\t\t\tfree_extent_map(em);\n\t\telse\n\t\t\tgoto out;\n\t}\n\tem = alloc_extent_map();\n\tif (!em) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\tem->bdev = fs_info->fs_devices->latest_bdev;\n\tem->start = EXTENT_MAP_HOLE;\n\tem->orig_start = EXTENT_MAP_HOLE;\n\tem->len = (u64)-1;\n\tem->block_len = (u64)-1;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t/* Chances are we'll be called again, so go ahead and do readahead */\n\tpath->reada = READA_FORWARD;\n\n\t/*\n\t * Unless we're going to uncompress the inline extent, no sleep would\n\t * happen.\n\t */\n\tpath->leave_spinning = 1;\n\n\tret = btrfs_lookup_file_extent(NULL, root, path, objectid, start, 0);\n\tif (ret < 0) {\n\t\terr = ret;\n\t\tgoto out;\n\t} else if (ret > 0) {\n\t\tif (path->slots[0] == 0)\n\t\t\tgoto not_found;\n\t\tpath->slots[0]--;\n\t}\n\n\tleaf = path->nodes[0];\n\titem = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t      struct btrfs_file_extent_item);\n\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\tif (found_key.objectid != objectid ||\n\t    found_key.type != BTRFS_EXTENT_DATA_KEY) {\n\t\t/*\n\t\t * If we backup past the first extent we want to move forward\n\t\t * and see if there is an extent in front of us, otherwise we'll\n\t\t * say there is a hole for our whole search range which can\n\t\t * cause problems.\n\t\t */\n\t\textent_end = start;\n\t\tgoto next;\n\t}\n\n\textent_type = btrfs_file_extent_type(leaf, item);\n\textent_start = found_key.offset;\n\tif (extent_type == BTRFS_FILE_EXTENT_REG ||\n\t    extent_type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\t/* Only regular file could have regular/prealloc extent */\n\t\tif (!S_ISREG(inode->vfs_inode.i_mode)) {\n\t\t\tret = -EUCLEAN;\n\t\t\tbtrfs_crit(fs_info,\n\t\t\"regular/prealloc extent found for non-regular inode %llu\",\n\t\t\t\t   btrfs_ino(inode));\n\t\t\tgoto out;\n\t\t}\n\t\textent_end = extent_start +\n\t\t       btrfs_file_extent_num_bytes(leaf, item);\n\n\t\ttrace_btrfs_get_extent_show_fi_regular(inode, leaf, item,\n\t\t\t\t\t\t       extent_start);\n\t} else if (extent_type == BTRFS_FILE_EXTENT_INLINE) {\n\t\tsize_t size;\n\n\t\tsize = btrfs_file_extent_ram_bytes(leaf, item);\n\t\textent_end = ALIGN(extent_start + size,\n\t\t\t\t   fs_info->sectorsize);\n\n\t\ttrace_btrfs_get_extent_show_fi_inline(inode, leaf, item,\n\t\t\t\t\t\t      path->slots[0],\n\t\t\t\t\t\t      extent_start);\n\t}\nnext:\n\tif (start >= extent_end) {\n\t\tpath->slots[0]++;\n\t\tif (path->slots[0] >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret < 0) {\n\t\t\t\terr = ret;\n\t\t\t\tgoto out;\n\t\t\t} else if (ret > 0) {\n\t\t\t\tgoto not_found;\n\t\t\t}\n\t\t\tleaf = path->nodes[0];\n\t\t}\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\t\tif (found_key.objectid != objectid ||\n\t\t    found_key.type != BTRFS_EXTENT_DATA_KEY)\n\t\t\tgoto not_found;\n\t\tif (start + len <= found_key.offset)\n\t\t\tgoto not_found;\n\t\tif (start > found_key.offset)\n\t\t\tgoto next;\n\n\t\t/* New extent overlaps with existing one */\n\t\tem->start = start;\n\t\tem->orig_start = start;\n\t\tem->len = found_key.offset - start;\n\t\tem->block_start = EXTENT_MAP_HOLE;\n\t\tgoto insert;\n\t}\n\n\tbtrfs_extent_item_to_extent_map(inode, path, item,\n\t\t\tnew_inline, em);\n\n\tif (extent_type == BTRFS_FILE_EXTENT_REG ||\n\t    extent_type == BTRFS_FILE_EXTENT_PREALLOC) {\n\t\tgoto insert;\n\t} else if (extent_type == BTRFS_FILE_EXTENT_INLINE) {\n\t\tunsigned long ptr;\n\t\tchar *map;\n\t\tsize_t size;\n\t\tsize_t extent_offset;\n\t\tsize_t copy_size;\n\n\t\tif (new_inline)\n\t\t\tgoto out;\n\n\t\tsize = btrfs_file_extent_ram_bytes(leaf, item);\n\t\textent_offset = page_offset(page) + pg_offset - extent_start;\n\t\tcopy_size = min_t(u64, PAGE_SIZE - pg_offset,\n\t\t\t\t  size - extent_offset);\n\t\tem->start = extent_start + extent_offset;\n\t\tem->len = ALIGN(copy_size, fs_info->sectorsize);\n\t\tem->orig_block_len = em->len;\n\t\tem->orig_start = em->start;\n\t\tptr = btrfs_file_extent_inline_start(item) + extent_offset;\n\n\t\tbtrfs_set_path_blocking(path);\n\t\tif (!PageUptodate(page)) {\n\t\t\tif (btrfs_file_extent_compression(leaf, item) !=\n\t\t\t    BTRFS_COMPRESS_NONE) {\n\t\t\t\tret = uncompress_inline(path, page, pg_offset,\n\t\t\t\t\t\t\textent_offset, item);\n\t\t\t\tif (ret) {\n\t\t\t\t\terr = ret;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tmap = kmap(page);\n\t\t\t\tread_extent_buffer(leaf, map + pg_offset, ptr,\n\t\t\t\t\t\t   copy_size);\n\t\t\t\tif (pg_offset + copy_size < PAGE_SIZE) {\n\t\t\t\t\tmemset(map + pg_offset + copy_size, 0,\n\t\t\t\t\t       PAGE_SIZE - pg_offset -\n\t\t\t\t\t       copy_size);\n\t\t\t\t}\n\t\t\t\tkunmap(page);\n\t\t\t}\n\t\t\tflush_dcache_page(page);\n\t\t}\n\t\tset_extent_uptodate(io_tree, em->start,\n\t\t\t\t    extent_map_end(em) - 1, NULL, GFP_NOFS);\n\t\tgoto insert;\n\t}\nnot_found:\n\tem->start = start;\n\tem->orig_start = start;\n\tem->len = len;\n\tem->block_start = EXTENT_MAP_HOLE;\ninsert:\n\tbtrfs_release_path(path);\n\tif (em->start > start || extent_map_end(em) <= start) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"bad extent! em: [%llu %llu] passed [%llu %llu]\",\n\t\t\t  em->start, em->len, start, len);\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\terr = 0;\n\twrite_lock(&em_tree->lock);\n\terr = btrfs_add_extent_mapping(fs_info, em_tree, &em, start, len);\n\twrite_unlock(&em_tree->lock);\nout:\n\tbtrfs_free_path(path);\n\n\ttrace_btrfs_get_extent(root, inode, em);\n\n\tif (err) {\n\t\tfree_extent_map(em);\n\t\treturn ERR_PTR(err);\n\t}\n\tBUG_ON(!em); /* Error is always set */\n\treturn em;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2384,
            "cve_id": "CVE-2019-2025",
            "code_snippet": "static int binder_thread_write(struct binder_proc *proc,\n\t\t\tstruct binder_thread *thread,\n\t\t\tbinder_uintptr_t binder_buffer, size_t size,\n\t\t\tbinder_size_t *consumed)\n{\n\tuint32_t cmd;\n\tstruct binder_context *context = proc->context;\n\tvoid __user *buffer = (void __user *)(uintptr_t)binder_buffer;\n\tvoid __user *ptr = buffer + *consumed;\n\tvoid __user *end = buffer + size;\n\n\twhile (ptr < end && thread->return_error.cmd == BR_OK) {\n\t\tint ret;\n\n\t\tif (get_user(cmd, (uint32_t __user *)ptr))\n\t\t\treturn -EFAULT;\n\t\tptr += sizeof(uint32_t);\n\t\ttrace_binder_command(cmd);\n\t\tif (_IOC_NR(cmd) < ARRAY_SIZE(binder_stats.bc)) {\n\t\t\tatomic_inc(&binder_stats.bc[_IOC_NR(cmd)]);\n\t\t\tatomic_inc(&proc->stats.bc[_IOC_NR(cmd)]);\n\t\t\tatomic_inc(&thread->stats.bc[_IOC_NR(cmd)]);\n\t\t}\n\t\tswitch (cmd) {\n\t\tcase BC_INCREFS:\n\t\tcase BC_ACQUIRE:\n\t\tcase BC_RELEASE:\n\t\tcase BC_DECREFS: {\n\t\t\tuint32_t target;\n\t\t\tconst char *debug_string;\n\t\t\tbool strong = cmd == BC_ACQUIRE || cmd == BC_RELEASE;\n\t\t\tbool increment = cmd == BC_INCREFS || cmd == BC_ACQUIRE;\n\t\t\tstruct binder_ref_data rdata;\n\n\t\t\tif (get_user(target, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tret = -1;\n\t\t\tif (increment && !target) {\n\t\t\t\tstruct binder_node *ctx_mgr_node;\n\t\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\t\tctx_mgr_node = context->binder_context_mgr_node;\n\t\t\t\tif (ctx_mgr_node)\n\t\t\t\t\tret = binder_inc_ref_for_node(\n\t\t\t\t\t\t\tproc, ctx_mgr_node,\n\t\t\t\t\t\t\tstrong, NULL, &rdata);\n\t\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\t}\n\t\t\tif (ret)\n\t\t\t\tret = binder_update_ref_for_handle(\n\t\t\t\t\t\tproc, target, increment, strong,\n\t\t\t\t\t\t&rdata);\n\t\t\tif (!ret && rdata.desc != target) {\n\t\t\t\tbinder_user_error(\"%d:%d tried to acquire reference to desc %d, got %d instead\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\ttarget, rdata.desc);\n\t\t\t}\n\t\t\tswitch (cmd) {\n\t\t\tcase BC_INCREFS:\n\t\t\t\tdebug_string = \"IncRefs\";\n\t\t\t\tbreak;\n\t\t\tcase BC_ACQUIRE:\n\t\t\t\tdebug_string = \"Acquire\";\n\t\t\t\tbreak;\n\t\t\tcase BC_RELEASE:\n\t\t\t\tdebug_string = \"Release\";\n\t\t\t\tbreak;\n\t\t\tcase BC_DECREFS:\n\t\t\tdefault:\n\t\t\t\tdebug_string = \"DecRefs\";\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ret) {\n\t\t\t\tbinder_user_error(\"%d:%d %s %d refcount change on invalid ref %d ret %d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, debug_string,\n\t\t\t\t\tstrong, target, ret);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n\t\t\t\t     \"%d:%d %s ref %d desc %d s %d w %d\\n\",\n\t\t\t\t     proc->pid, thread->pid, debug_string,\n\t\t\t\t     rdata.debug_id, rdata.desc, rdata.strong,\n\t\t\t\t     rdata.weak);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_INCREFS_DONE:\n\t\tcase BC_ACQUIRE_DONE: {\n\t\t\tbinder_uintptr_t node_ptr;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_node *node;\n\t\t\tbool free_node;\n\n\t\t\tif (get_user(node_ptr, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tnode = binder_get_node(proc, node_ptr);\n\t\t\tif (node == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d %s u%016llx no match\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_INCREFS_DONE ?\n\t\t\t\t\t\"BC_INCREFS_DONE\" :\n\t\t\t\t\t\"BC_ACQUIRE_DONE\",\n\t\t\t\t\t(u64)node_ptr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (cookie != node->cookie) {\n\t\t\t\tbinder_user_error(\"%d:%d %s u%016llx node %d cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_INCREFS_DONE ?\n\t\t\t\t\t\"BC_INCREFS_DONE\" : \"BC_ACQUIRE_DONE\",\n\t\t\t\t\t(u64)node_ptr, node->debug_id,\n\t\t\t\t\t(u64)cookie, (u64)node->cookie);\n\t\t\t\tbinder_put_node(node);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_node_inner_lock(node);\n\t\t\tif (cmd == BC_ACQUIRE_DONE) {\n\t\t\t\tif (node->pending_strong_ref == 0) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_ACQUIRE_DONE node %d has no pending acquire request\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\tnode->debug_id);\n\t\t\t\t\tbinder_node_inner_unlock(node);\n\t\t\t\t\tbinder_put_node(node);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode->pending_strong_ref = 0;\n\t\t\t} else {\n\t\t\t\tif (node->pending_weak_ref == 0) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_INCREFS_DONE node %d has no pending increfs request\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\tnode->debug_id);\n\t\t\t\t\tbinder_node_inner_unlock(node);\n\t\t\t\t\tbinder_put_node(node);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnode->pending_weak_ref = 0;\n\t\t\t}\n\t\t\tfree_node = binder_dec_node_nilocked(node,\n\t\t\t\t\tcmd == BC_ACQUIRE_DONE, 0);\n\t\t\tWARN_ON(free_node);\n\t\t\tbinder_debug(BINDER_DEBUG_USER_REFS,\n\t\t\t\t     \"%d:%d %s node %d ls %d lw %d tr %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     cmd == BC_INCREFS_DONE ? \"BC_INCREFS_DONE\" : \"BC_ACQUIRE_DONE\",\n\t\t\t\t     node->debug_id, node->local_strong_refs,\n\t\t\t\t     node->local_weak_refs, node->tmp_refs);\n\t\t\tbinder_node_inner_unlock(node);\n\t\t\tbinder_put_node(node);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_ATTEMPT_ACQUIRE:\n\t\t\tpr_err(\"BC_ATTEMPT_ACQUIRE not supported\\n\");\n\t\t\treturn -EINVAL;\n\t\tcase BC_ACQUIRE_RESULT:\n\t\t\tpr_err(\"BC_ACQUIRE_RESULT not supported\\n\");\n\t\t\treturn -EINVAL;\n\n\t\tcase BC_FREE_BUFFER: {\n\t\t\tbinder_uintptr_t data_ptr;\n\t\t\tstruct binder_buffer *buffer;\n\n\t\t\tif (get_user(data_ptr, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\n\t\t\tbuffer = binder_alloc_prepare_to_free(&proc->alloc,\n\t\t\t\t\t\t\t      data_ptr);\n\t\t\tif (IS_ERR_OR_NULL(buffer)) {\n\t\t\t\tif (PTR_ERR(buffer) == -EPERM) {\n\t\t\t\t\tbinder_user_error(\n\t\t\t\t\t\t\"%d:%d BC_FREE_BUFFER u%016llx matched unreturned or currently freeing buffer\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\t(u64)data_ptr);\n\t\t\t\t} else {\n\t\t\t\t\tbinder_user_error(\n\t\t\t\t\t\t\"%d:%d BC_FREE_BUFFER u%016llx no match\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\t(u64)data_ptr);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_FREE_BUFFER,\n\t\t\t\t     \"%d:%d BC_FREE_BUFFER u%016llx found buffer %d for %s transaction\\n\",\n\t\t\t\t     proc->pid, thread->pid, (u64)data_ptr,\n\t\t\t\t     buffer->debug_id,\n\t\t\t\t     buffer->transaction ? \"active\" : \"finished\");\n\t\t\tbinder_free_buf(proc, buffer);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BC_TRANSACTION_SG:\n\t\tcase BC_REPLY_SG: {\n\t\t\tstruct binder_transaction_data_sg tr;\n\n\t\t\tif (copy_from_user(&tr, ptr, sizeof(tr)))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(tr);\n\t\t\tbinder_transaction(proc, thread, &tr.transaction_data,\n\t\t\t\t\t   cmd == BC_REPLY_SG, tr.buffers_size);\n\t\t\tbreak;\n\t\t}\n\t\tcase BC_TRANSACTION:\n\t\tcase BC_REPLY: {\n\t\t\tstruct binder_transaction_data tr;\n\n\t\t\tif (copy_from_user(&tr, ptr, sizeof(tr)))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(tr);\n\t\t\tbinder_transaction(proc, thread, &tr,\n\t\t\t\t\t   cmd == BC_REPLY, 0);\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BC_REGISTER_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_REGISTER_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tif (thread->looper & BINDER_LOOPER_STATE_ENTERED) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_REGISTER_LOOPER called after BC_ENTER_LOOPER\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t} else if (proc->requested_threads == 0) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_REGISTER_LOOPER called without request\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t} else {\n\t\t\t\tproc->requested_threads--;\n\t\t\t\tproc->requested_threads_started++;\n\t\t\t}\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_REGISTERED;\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbreak;\n\t\tcase BC_ENTER_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_ENTER_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tif (thread->looper & BINDER_LOOPER_STATE_REGISTERED) {\n\t\t\t\tthread->looper |= BINDER_LOOPER_STATE_INVALID;\n\t\t\t\tbinder_user_error(\"%d:%d ERROR: BC_ENTER_LOOPER called after BC_REGISTER_LOOPER\\n\",\n\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t}\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_ENTERED;\n\t\t\tbreak;\n\t\tcase BC_EXIT_LOOPER:\n\t\t\tbinder_debug(BINDER_DEBUG_THREADS,\n\t\t\t\t     \"%d:%d BC_EXIT_LOOPER\\n\",\n\t\t\t\t     proc->pid, thread->pid);\n\t\t\tthread->looper |= BINDER_LOOPER_STATE_EXITED;\n\t\t\tbreak;\n\n\t\tcase BC_REQUEST_DEATH_NOTIFICATION:\n\t\tcase BC_CLEAR_DEATH_NOTIFICATION: {\n\t\t\tuint32_t target;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_ref *ref;\n\t\t\tstruct binder_ref_death *death = NULL;\n\n\t\t\tif (get_user(target, (uint32_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(uint32_t);\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\t\t\tptr += sizeof(binder_uintptr_t);\n\t\t\tif (cmd == BC_REQUEST_DEATH_NOTIFICATION) {\n\t\t\t\t/*\n\t\t\t\t * Allocate memory for death notification\n\t\t\t\t * before taking lock\n\t\t\t\t */\n\t\t\t\tdeath = kzalloc(sizeof(*death), GFP_KERNEL);\n\t\t\t\tif (death == NULL) {\n\t\t\t\t\tWARN_ON(thread->return_error.cmd !=\n\t\t\t\t\t\tBR_OK);\n\t\t\t\t\tthread->return_error.cmd = BR_ERROR;\n\t\t\t\t\tbinder_enqueue_thread_work(\n\t\t\t\t\t\tthread,\n\t\t\t\t\t\t&thread->return_error.work);\n\t\t\t\t\tbinder_debug(\n\t\t\t\t\t\tBINDER_DEBUG_FAILED_TRANSACTION,\n\t\t\t\t\t\t\"%d:%d BC_REQUEST_DEATH_NOTIFICATION failed\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, target, false);\n\t\t\tif (ref == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d %s invalid ref %d\\n\",\n\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\tcmd == BC_REQUEST_DEATH_NOTIFICATION ?\n\t\t\t\t\t\"BC_REQUEST_DEATH_NOTIFICATION\" :\n\t\t\t\t\t\"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\t\t\t\ttarget);\n\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\tkfree(death);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tbinder_debug(BINDER_DEBUG_DEATH_NOTIFICATION,\n\t\t\t\t     \"%d:%d %s %016llx ref %d desc %d s %d w %d for node %d\\n\",\n\t\t\t\t     proc->pid, thread->pid,\n\t\t\t\t     cmd == BC_REQUEST_DEATH_NOTIFICATION ?\n\t\t\t\t     \"BC_REQUEST_DEATH_NOTIFICATION\" :\n\t\t\t\t     \"BC_CLEAR_DEATH_NOTIFICATION\",\n\t\t\t\t     (u64)cookie, ref->data.debug_id,\n\t\t\t\t     ref->data.desc, ref->data.strong,\n\t\t\t\t     ref->data.weak, ref->node->debug_id);\n\n\t\t\tbinder_node_lock(ref->node);\n\t\t\tif (cmd == BC_REQUEST_DEATH_NOTIFICATION) {\n\t\t\t\tif (ref->death) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_REQUEST_DEATH_NOTIFICATION death notification already set\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tkfree(death);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbinder_stats_created(BINDER_STAT_DEATH);\n\t\t\t\tINIT_LIST_HEAD(&death->work.entry);\n\t\t\t\tdeath->cookie = cookie;\n\t\t\t\tref->death = death;\n\t\t\t\tif (ref->node->proc == NULL) {\n\t\t\t\t\tref->death->work.type = BINDER_WORK_DEAD_BINDER;\n\n\t\t\t\t\tbinder_inner_proc_lock(proc);\n\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t&ref->death->work, &proc->todo);\n\t\t\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (ref->death == NULL) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_CLEAR_DEATH_NOTIFICATION death notification not active\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tdeath = ref->death;\n\t\t\t\tif (death->cookie != cookie) {\n\t\t\t\t\tbinder_user_error(\"%d:%d BC_CLEAR_DEATH_NOTIFICATION death notification cookie mismatch %016llx != %016llx\\n\",\n\t\t\t\t\t\tproc->pid, thread->pid,\n\t\t\t\t\t\t(u64)death->cookie,\n\t\t\t\t\t\t(u64)cookie);\n\t\t\t\t\tbinder_node_unlock(ref->node);\n\t\t\t\t\tbinder_proc_unlock(proc);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tref->death = NULL;\n\t\t\t\tbinder_inner_proc_lock(proc);\n\t\t\t\tif (list_empty(&death->work.entry)) {\n\t\t\t\t\tdeath->work.type = BINDER_WORK_CLEAR_DEATH_NOTIFICATION;\n\t\t\t\t\tif (thread->looper &\n\t\t\t\t\t    (BINDER_LOOPER_STATE_REGISTERED |\n\t\t\t\t\t     BINDER_LOOPER_STATE_ENTERED))\n\t\t\t\t\t\tbinder_enqueue_thread_work_ilocked(\n\t\t\t\t\t\t\t\tthread,\n\t\t\t\t\t\t\t\t&death->work);\n\t\t\t\t\telse {\n\t\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t\t\t&death->work,\n\t\t\t\t\t\t\t\t&proc->todo);\n\t\t\t\t\t\tbinder_wakeup_proc_ilocked(\n\t\t\t\t\t\t\t\tproc);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tBUG_ON(death->work.type != BINDER_WORK_DEAD_BINDER);\n\t\t\t\t\tdeath->work.type = BINDER_WORK_DEAD_BINDER_AND_CLEAR;\n\t\t\t\t}\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t}\n\t\t\tbinder_node_unlock(ref->node);\n\t\t\tbinder_proc_unlock(proc);\n\t\t} break;\n\t\tcase BC_DEAD_BINDER_DONE: {\n\t\t\tstruct binder_work *w;\n\t\t\tbinder_uintptr_t cookie;\n\t\t\tstruct binder_ref_death *death = NULL;\n\n\t\t\tif (get_user(cookie, (binder_uintptr_t __user *)ptr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tptr += sizeof(cookie);\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tlist_for_each_entry(w, &proc->delivered_death,\n\t\t\t\t\t    entry) {\n\t\t\t\tstruct binder_ref_death *tmp_death =\n\t\t\t\t\tcontainer_of(w,\n\t\t\t\t\t\t     struct binder_ref_death,\n\t\t\t\t\t\t     work);\n\n\t\t\t\tif (tmp_death->cookie == cookie) {\n\t\t\t\t\tdeath = tmp_death;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_debug(BINDER_DEBUG_DEAD_BINDER,\n\t\t\t\t     \"%d:%d BC_DEAD_BINDER_DONE %016llx found %pK\\n\",\n\t\t\t\t     proc->pid, thread->pid, (u64)cookie,\n\t\t\t\t     death);\n\t\t\tif (death == NULL) {\n\t\t\t\tbinder_user_error(\"%d:%d BC_DEAD_BINDER_DONE %016llx not found\\n\",\n\t\t\t\t\tproc->pid, thread->pid, (u64)cookie);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbinder_dequeue_work_ilocked(&death->work);\n\t\t\tif (death->work.type == BINDER_WORK_DEAD_BINDER_AND_CLEAR) {\n\t\t\t\tdeath->work.type = BINDER_WORK_CLEAR_DEATH_NOTIFICATION;\n\t\t\t\tif (thread->looper &\n\t\t\t\t\t(BINDER_LOOPER_STATE_REGISTERED |\n\t\t\t\t\t BINDER_LOOPER_STATE_ENTERED))\n\t\t\t\t\tbinder_enqueue_thread_work_ilocked(\n\t\t\t\t\t\tthread, &death->work);\n\t\t\t\telse {\n\t\t\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\t\t\t&death->work,\n\t\t\t\t\t\t\t&proc->todo);\n\t\t\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t\t\t}\n\t\t\t}\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t} break;\n\n\t\tdefault:\n\t\t\tpr_err(\"%d:%d unknown command %d\\n\",\n\t\t\t       proc->pid, thread->pid, cmd);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t*consumed = ptr - buffer;\n\t}\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2403,
            "cve_id": "CVE-2019-2213",
            "code_snippet": "static void\nbinder_free_buf(struct binder_proc *proc, struct binder_buffer *buffer)\n{\n\tbinder_inner_proc_lock(proc);\n\tif (buffer->transaction) {\n\t\tbuffer->transaction->buffer = NULL;\n\t\tbuffer->transaction = NULL;\n\t}\n\tbinder_inner_proc_unlock(proc);\n\tif (buffer->async_transaction && buffer->target_node) {\n\t\tstruct binder_node *buf_node;\n\t\tstruct binder_work *w;\n\n\t\tbuf_node = buffer->target_node;\n\t\tbinder_node_inner_lock(buf_node);\n\t\tBUG_ON(!buf_node->has_async_transaction);\n\t\tBUG_ON(buf_node->proc != proc);\n\t\tw = binder_dequeue_work_head_ilocked(\n\t\t\t\t&buf_node->async_todo);\n\t\tif (!w) {\n\t\t\tbuf_node->has_async_transaction = false;\n\t\t} else {\n\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\tw, &proc->todo);\n\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t}\n\t\tbinder_node_inner_unlock(buf_node);\n\t}\n\ttrace_binder_transaction_buffer_release(buffer);\n\tbinder_transaction_buffer_release(proc, buffer, 0, false);\n\tbinder_alloc_free_buf(&proc->alloc, buffer);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2407,
            "cve_id": "CVE-2019-25044",
            "code_snippet": "static void blk_mq_sched_tags_teardown(struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tint i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (hctx->sched_tags) {\n\t\t\tblk_mq_free_rq_map(hctx->sched_tags);\n\t\t\thctx->sched_tags = NULL;\n\t\t}\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2411,
            "cve_id": "CVE-2019-25045",
            "code_snippet": "static int\nparse_ipsecrequest(struct xfrm_policy *xp, struct sadb_x_ipsecrequest *rq)\n{\n\tstruct net *net = xp_net(xp);\n\tstruct xfrm_tmpl *t = xp->xfrm_vec + xp->xfrm_nr;\n\tint mode;\n\n\tif (xp->xfrm_nr >= XFRM_MAX_DEPTH)\n\t\treturn -ELOOP;\n\n\tif (rq->sadb_x_ipsecrequest_mode == 0)\n\t\treturn -EINVAL;\n\tif (!xfrm_id_proto_valid(rq->sadb_x_ipsecrequest_proto))\n\t\treturn -EINVAL;\n\n\tt->id.proto = rq->sadb_x_ipsecrequest_proto;\n\tif ((mode = pfkey_mode_to_xfrm(rq->sadb_x_ipsecrequest_mode)) < 0)\n\t\treturn -EINVAL;\n\tt->mode = mode;\n\tif (rq->sadb_x_ipsecrequest_level == IPSEC_LEVEL_USE)\n\t\tt->optional = 1;\n\telse if (rq->sadb_x_ipsecrequest_level == IPSEC_LEVEL_UNIQUE) {\n\t\tt->reqid = rq->sadb_x_ipsecrequest_reqid;\n\t\tif (t->reqid > IPSEC_MANUAL_REQID_MAX)\n\t\t\tt->reqid = 0;\n\t\tif (!t->reqid && !(t->reqid = gen_reqid(net)))\n\t\t\treturn -ENOBUFS;\n\t}\n\n\t/* addresses present only in tunnel mode */\n\tif (t->mode == XFRM_MODE_TUNNEL) {\n\t\tint err;\n\n\t\terr = parse_sockaddr_pair(\n\t\t\t(struct sockaddr *)(rq + 1),\n\t\t\trq->sadb_x_ipsecrequest_len - sizeof(*rq),\n\t\t\t&t->saddr, &t->id.daddr, &t->encap_family);\n\t\tif (err)\n\t\t\treturn err;\n\t} else\n\t\tt->encap_family = xp->family;\n\n\t/* No way to set this via kame pfkey */\n\tt->allalgs = 1;\n\txp->xfrm_nr++;\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2459,
            "cve_id": "CVE-2019-9003",
            "code_snippet": "int ipmi_destroy_user(struct ipmi_user *user)\n{\n\t_ipmi_destroy_user(user);\n\n\tkref_put(&user->refcount, free_user);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2492,
            "cve_id": "CVE-2020-0427",
            "code_snippet": "static int dt_remember_or_free_map(struct pinctrl *p, const char *statename,\n\t\t\t\t   struct pinctrl_dev *pctldev,\n\t\t\t\t   struct pinctrl_map *map, unsigned num_maps)\n{\n\tint i;\n\tstruct pinctrl_dt_map *dt_map;\n\n\t/* Initialize common mapping table entry fields */\n\tfor (i = 0; i < num_maps; i++) {\n\t\tconst char *devname;\n\n\t\tdevname = kstrdup_const(dev_name(p->dev), GFP_KERNEL);\n\t\tif (!devname)\n\t\t\tgoto err_free_map;\n\n\t\tmap[i].dev_name = devname;\n\t\tmap[i].name = statename;\n\t\tif (pctldev)\n\t\t\tmap[i].ctrl_dev_name = dev_name(pctldev->dev);\n\t}\n\n\t/* Remember the converted mapping table entries */\n\tdt_map = kzalloc(sizeof(*dt_map), GFP_KERNEL);\n\tif (!dt_map)\n\t\tgoto err_free_map;\n\n\tdt_map->pctldev = pctldev;\n\tdt_map->map = map;\n\tdt_map->num_maps = num_maps;\n\tlist_add_tail(&dt_map->node, &p->dt_maps);\n\n\treturn pinctrl_register_map(map, num_maps, false);\n\nerr_free_map:\n\tdt_free_map(pctldev, map, num_maps);\n\treturn -ENOMEM;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2494,
            "cve_id": "CVE-2020-0429",
            "code_snippet": "int l2tp_session_delete(struct l2tp_session *session)\n{\n\tif (test_and_set_bit(0, &session->dead))\n\t\treturn 0;\n\n\tif (session->ref)\n\t\t(*session->ref)(session);\n\t__l2tp_session_unhash(session);\n\tl2tp_session_queue_purge(session);\n\tif (session->session_close != NULL)\n\t\t(*session->session_close)(session);\n\tif (session->deref)\n\t\t(*session->deref)(session);\n\tl2tp_session_dec_refcount(session);\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2500,
            "cve_id": "CVE-2020-0433",
            "code_snippet": "static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,\n\t\t\t\t\t\t\tint nr_hw_queues)\n{\n\tstruct request_queue *q;\n\tLIST_HEAD(head);\n\n\tlockdep_assert_held(&set->tag_list_lock);\n\n\tif (nr_hw_queues > nr_cpu_ids)\n\t\tnr_hw_queues = nr_cpu_ids;\n\tif (nr_hw_queues < 1 || nr_hw_queues == set->nr_hw_queues)\n\t\treturn;\n\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n\t\tblk_mq_freeze_queue(q);\n\t/*\n\t * Sync with blk_mq_queue_tag_busy_iter.\n\t */\n\tsynchronize_rcu();\n\t/*\n\t * Switch IO scheduler to 'none', cleaning up the data associated\n\t * with the previous scheduler. We will switch back once we are done\n\t * updating the new sw to hw queue mappings.\n\t */\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n\t\tif (!blk_mq_elv_switch_none(&head, q))\n\t\t\tgoto switch_back;\n\n\tset->nr_hw_queues = nr_hw_queues;\n\tblk_mq_update_queue_map(set);\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list) {\n\t\tblk_mq_realloc_hw_ctxs(set, q);\n\t\tblk_mq_queue_reinit(q);\n\t}\n\nswitch_back:\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n\t\tblk_mq_elv_switch_back(&head, q);\n\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n\t\tblk_mq_unfreeze_queue(q);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2511,
            "cve_id": "CVE-2020-10690",
            "code_snippet": "int ptp_clock_unregister(struct ptp_clock *ptp)\n{\n\tptp->defunct = 1;\n\twake_up_interruptible(&ptp->tsev_wq);\n\n\tif (ptp->kworker) {\n\t\tkthread_cancel_delayed_work_sync(&ptp->aux_work);\n\t\tkthread_destroy_worker(ptp->kworker);\n\t}\n\n\t/* Release the clock's resources. */\n\tif (ptp->pps_source)\n\t\tpps_unregister_source(ptp->pps_source);\n\n\tptp_cleanup_pin_groups(ptp);\n\n\tposix_clock_unregister(&ptp->clock);\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2512,
            "cve_id": "CVE-2020-10690",
            "code_snippet": "static int posix_clock_release(struct inode *inode, struct file *fp)\n{\n\tstruct posix_clock *clk = fp->private_data;\n\tint err = 0;\n\n\tif (clk->ops.release)\n\t\terr = clk->ops.release(clk);\n\n\tput_device(clk->dev);\n\n\tfp->private_data = NULL;\n\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2632,
            "cve_id": "CVE-2020-14381",
            "code_snippet": "static void get_futex_key_refs(union futex_key *key)\n{\n\tif (!key->both.ptr)\n\t\treturn;\n\n\t/*\n\t * On MMU less systems futexes are always \"private\" as there is no per\n\t * process address space. We need the smp wmb nevertheless - yes,\n\t * arch/blackfin has MMU less SMP ...\n\t */\n\tif (!IS_ENABLED(CONFIG_MMU)) {\n\t\tsmp_mb(); /* explicit smp_mb(); (B) */\n\t\treturn;\n\t}\n\n\tswitch (key->both.offset & (FUT_OFF_INODE|FUT_OFF_MMSHARED)) {\n\tcase FUT_OFF_INODE:\n\t\tsmp_mb();\t\t/* explicit smp_mb(); (B) */\n\t\tbreak;\n\tcase FUT_OFF_MMSHARED:\n\t\tfutex_get_mm(key); /* implies smp_mb(); (B) */\n\t\tbreak;\n\tdefault:\n\t\t/*\n\t\t * Private futexes do not hold reference on an inode or\n\t\t * mm, therefore the only purpose of calling get_futex_key_refs\n\t\t * is because we need the barrier for the lockless waiter check.\n\t\t */\n\t\tsmp_mb(); /* explicit smp_mb(); (B) */\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2653,
            "cve_id": "CVE-2020-14416",
            "code_snippet": "static void slcan_write_wakeup(struct tty_struct *tty)\n{\n\tstruct slcan *sl;\n\n\trcu_read_lock();\n\tsl = rcu_dereference(tty->disc_data);\n\tif (!sl)\n\t\tgoto out;\n\n\tschedule_work(&sl->tx_work);\nout:\n\trcu_read_unlock();\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2656,
            "cve_id": "CVE-2020-15436",
            "code_snippet": "int blkdev_get(struct block_device *bdev, fmode_t mode, void *holder)\n{\n\tstruct block_device *whole = NULL;\n\tint res;\n\n\tWARN_ON_ONCE((mode & FMODE_EXCL) && !holder);\n\n\tif ((mode & FMODE_EXCL) && holder) {\n\t\twhole = bd_start_claiming(bdev, holder);\n\t\tif (IS_ERR(whole)) {\n\t\t\tbdput(bdev);\n\t\t\treturn PTR_ERR(whole);\n\t\t}\n\t}\n\n\tres = __blkdev_get(bdev, mode, 0);\n\n\tif (whole) {\n\t\tstruct gendisk *disk = whole->bd_disk;\n\n\t\t/* finish claiming */\n\t\tmutex_lock(&bdev->bd_mutex);\n\t\tif (!res)\n\t\t\tbd_finish_claiming(bdev, whole, holder);\n\t\telse\n\t\t\tbd_abort_claiming(bdev, whole, holder);\n\t\t/*\n\t\t * Block event polling for write claims if requested.  Any\n\t\t * write holder makes the write_holder state stick until\n\t\t * all are released.  This is good enough and tracking\n\t\t * individual writeable reference is too fragile given the\n\t\t * way @mode is used in blkdev_get/put().\n\t\t */\n\t\tif (!res && (mode & FMODE_WRITE) && !bdev->bd_write_holder &&\n\t\t    (disk->flags & GENHD_FL_BLOCK_EVENTS_ON_EXCL_WRITE)) {\n\t\t\tbdev->bd_write_holder = true;\n\t\t\tdisk_block_events(disk);\n\t\t}\n\n\t\tmutex_unlock(&bdev->bd_mutex);\n\t\tbdput(whole);\n\t}\n\n\tif (res)\n\t\tbdput(bdev);\n\n\treturn res;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2706,
            "cve_id": "CVE-2020-25656",
            "code_snippet": "static void k_fn(struct vc_data *vc, unsigned char value, char up_flag)\n{\n\tif (up_flag)\n\t\treturn;\n\n\tif ((unsigned)value < ARRAY_SIZE(func_table)) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&func_buf_lock, flags);\n\t\tif (func_table[value])\n\t\t\tputs_queue(vc, func_table[value]);\n\t\tspin_unlock_irqrestore(&func_buf_lock, flags);\n\n\t} else\n\t\tpr_err(\"k_fn called with value=%d\\n\", value);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2709,
            "cve_id": "CVE-2020-25669",
            "code_snippet": "static void sunkbd_enable(struct sunkbd *sunkbd, bool enable)\n{\n\tserio_pause_rx(sunkbd->serio);\n\tsunkbd->enabled = enable;\n\tserio_continue_rx(sunkbd->serio);\n\n\tif (!enable) {\n\t\twake_up_interruptible(&sunkbd->wait);\n\t\tcancel_work_sync(&sunkbd->tq);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2724,
            "cve_id": "CVE-2020-27067",
            "code_snippet": "static int l2tp_eth_create(struct net *net, struct l2tp_tunnel *tunnel,\n\t\t\t   u32 session_id, u32 peer_session_id,\n\t\t\t   struct l2tp_session_cfg *cfg)\n{\n\tunsigned char name_assign_type;\n\tstruct net_device *dev;\n\tchar name[IFNAMSIZ];\n\tstruct l2tp_session *session;\n\tstruct l2tp_eth *priv;\n\tstruct l2tp_eth_sess *spriv;\n\tint rc;\n\n\tif (cfg->ifname) {\n\t\tstrlcpy(name, cfg->ifname, IFNAMSIZ);\n\t\tname_assign_type = NET_NAME_USER;\n\t} else {\n\t\tstrcpy(name, L2TP_ETH_DEV_NAME);\n\t\tname_assign_type = NET_NAME_ENUM;\n\t}\n\n\tsession = l2tp_session_create(sizeof(*spriv), tunnel, session_id,\n\t\t\t\t      peer_session_id, cfg);\n\tif (IS_ERR(session)) {\n\t\trc = PTR_ERR(session);\n\t\tgoto out;\n\t}\n\n\tdev = alloc_netdev(sizeof(*priv), name, name_assign_type,\n\t\t\t   l2tp_eth_dev_setup);\n\tif (!dev) {\n\t\trc = -ENOMEM;\n\t\tgoto out_del_session;\n\t}\n\n\tdev_net_set(dev, net);\n\tdev->min_mtu = 0;\n\tdev->max_mtu = ETH_MAX_MTU;\n\tl2tp_eth_adjust_mtu(tunnel, session, dev);\n\n\tpriv = netdev_priv(dev);\n\tpriv->dev = dev;\n\tpriv->session = session;\n\n\tpriv->tunnel_sock = tunnel->sock;\n\tsession->recv_skb = l2tp_eth_dev_recv;\n\tsession->session_close = l2tp_eth_delete;\n#if IS_ENABLED(CONFIG_L2TP_DEBUGFS)\n\tsession->show = l2tp_eth_show;\n#endif\n\n\tspriv = l2tp_session_priv(session);\n\tspriv->dev = dev;\n\n\trc = register_netdev(dev);\n\tif (rc < 0)\n\t\tgoto out_del_dev;\n\n\t__module_get(THIS_MODULE);\n\t/* Must be done after register_netdev() */\n\tstrlcpy(session->ifname, dev->name, IFNAMSIZ);\n\n\tdev_hold(dev);\n\n\treturn 0;\n\nout_del_dev:\n\tfree_netdev(dev);\n\tspriv->dev = NULL;\nout_del_session:\n\tl2tp_session_delete(session);\nout:\n\treturn rc;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2734,
            "cve_id": "CVE-2020-27675",
            "code_snippet": "static void clear_evtchn_to_irq_row(unsigned row)\n{\n\tunsigned col;\n\n\tfor (col = 0; col < EVTCHN_PER_ROW; col++)\n\t\tWRITE_ONCE(evtchn_to_irq[row][col], -1);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2735,
            "cve_id": "CVE-2020-27675",
            "code_snippet": "static void xen_free_irq(unsigned irq)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\tunsigned long flags;\n\n\tif (WARN_ON(!info))\n\t\treturn;\n\n\twrite_lock_irqsave(&evtchn_rwlock, flags);\n\n\tlist_del(&info->list);\n\n\tset_info_for_irq(irq, NULL);\n\n\tWARN_ON(info->refcnt > 0);\n\n\twrite_unlock_irqrestore(&evtchn_rwlock, flags);\n\n\tkfree(info);\n\n\t/* Legacy IRQ descriptors are managed by the arch. */\n\tif (irq < nr_legacy_irqs())\n\t\treturn;\n\n\tirq_free_desc(irq);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2742,
            "cve_id": "CVE-2020-27784",
            "code_snippet": "static int\nprinter_close(struct inode *inode, struct file *fd)\n{\n\tstruct printer_dev\t*dev = fd->private_data;\n\tunsigned long\t\tflags;\n\n\tspin_lock_irqsave(&dev->lock, flags);\n\tdev->printer_cdev_open = 0;\n\tfd->private_data = NULL;\n\t/* Change printer status to show that the printer is off-line. */\n\tdev->printer_status &= ~PRINTER_SELECTED;\n\tspin_unlock_irqrestore(&dev->lock, flags);\n\n\tkref_put(&dev->kref, printer_dev_free);\n\tDBG(dev, \"printer_close\\n\");\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2746,
            "cve_id": "CVE-2020-27786",
            "code_snippet": "static long snd_rawmidi_kernel_write1(struct snd_rawmidi_substream *substream,\n\t\t\t\t      const unsigned char __user *userbuf,\n\t\t\t\t      const unsigned char *kernelbuf,\n\t\t\t\t      long count)\n{\n\tunsigned long flags;\n\tlong count1, result;\n\tstruct snd_rawmidi_runtime *runtime = substream->runtime;\n\tunsigned long appl_ptr;\n\n\tif (!kernelbuf && !userbuf)\n\t\treturn -EINVAL;\n\tif (snd_BUG_ON(!runtime->buffer))\n\t\treturn -EINVAL;\n\n\tresult = 0;\n\tspin_lock_irqsave(&runtime->lock, flags);\n\tif (substream->append) {\n\t\tif ((long)runtime->avail < count) {\n\t\t\tspin_unlock_irqrestore(&runtime->lock, flags);\n\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\tsnd_rawmidi_buffer_ref(runtime);\n\twhile (count > 0 && runtime->avail > 0) {\n\t\tcount1 = runtime->buffer_size - runtime->appl_ptr;\n\t\tif (count1 > count)\n\t\t\tcount1 = count;\n\t\tif (count1 > (long)runtime->avail)\n\t\t\tcount1 = runtime->avail;\n\n\t\t/* update runtime->appl_ptr before unlocking for userbuf */\n\t\tappl_ptr = runtime->appl_ptr;\n\t\truntime->appl_ptr += count1;\n\t\truntime->appl_ptr %= runtime->buffer_size;\n\t\truntime->avail -= count1;\n\n\t\tif (kernelbuf)\n\t\t\tmemcpy(runtime->buffer + appl_ptr,\n\t\t\t       kernelbuf + result, count1);\n\t\telse if (userbuf) {\n\t\t\tspin_unlock_irqrestore(&runtime->lock, flags);\n\t\t\tif (copy_from_user(runtime->buffer + appl_ptr,\n\t\t\t\t\t   userbuf + result, count1)) {\n\t\t\t\tspin_lock_irqsave(&runtime->lock, flags);\n\t\t\t\tresult = result > 0 ? result : -EFAULT;\n\t\t\t\tgoto __end;\n\t\t\t}\n\t\t\tspin_lock_irqsave(&runtime->lock, flags);\n\t\t}\n\t\tresult += count1;\n\t\tcount -= count1;\n\t}\n      __end:\n\tcount1 = runtime->avail < runtime->buffer_size;\n\tsnd_rawmidi_buffer_unref(runtime);\n\tspin_unlock_irqrestore(&runtime->lock, flags);\n\tif (count1)\n\t\tsnd_rawmidi_output_trigger(substream, 1);\n\treturn result;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2754,
            "cve_id": "CVE-2020-27835",
            "code_snippet": "static int hfi1_file_open(struct inode *inode, struct file *fp)\n{\n\tstruct hfi1_filedata *fd;\n\tstruct hfi1_devdata *dd = container_of(inode->i_cdev,\n\t\t\t\t\t       struct hfi1_devdata,\n\t\t\t\t\t       user_cdev);\n\n\tif (!((dd->flags & HFI1_PRESENT) && dd->kregbase1))\n\t\treturn -EINVAL;\n\n\tif (!atomic_inc_not_zero(&dd->user_refcount))\n\t\treturn -ENXIO;\n\n\t/* The real work is performed later in assign_ctxt() */\n\n\tfd = kzalloc(sizeof(*fd), GFP_KERNEL);\n\n\tif (!fd || init_srcu_struct(&fd->pq_srcu))\n\t\tgoto nomem;\n\tspin_lock_init(&fd->pq_rcu_lock);\n\tspin_lock_init(&fd->tid_lock);\n\tspin_lock_init(&fd->invalid_lock);\n\tfd->rec_cpu_num = -1; /* no cpu affinity by default */\n\tfd->dd = dd;\n\tfp->private_data = fd;\n\treturn 0;\nnomem:\n\tkfree(fd);\n\tfp->private_data = NULL;\n\tif (atomic_dec_and_test(&dd->user_refcount))\n\t\tcomplete(&dd->user_comp);\n\treturn -ENOMEM;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2755,
            "cve_id": "CVE-2020-27835",
            "code_snippet": "int hfi1_mmu_rb_insert(struct mmu_rb_handler *handler,\n\t\t       struct mmu_rb_node *mnode)\n{\n\tstruct mmu_rb_node *node;\n\tunsigned long flags;\n\tint ret = 0;\n\n\ttrace_hfi1_mmu_rb_insert(mnode->addr, mnode->len);\n\n\tif (current->mm != handler->mn.mm)\n\t\treturn -EPERM;\n\n\tspin_lock_irqsave(&handler->lock, flags);\n\tnode = __mmu_rb_search(handler, mnode->addr, mnode->len);\n\tif (node) {\n\t\tret = -EINVAL;\n\t\tgoto unlock;\n\t}\n\t__mmu_int_rb_insert(mnode, &handler->root);\n\tlist_add(&mnode->list, &handler->lru_list);\n\n\tret = handler->ops->insert(handler->ops_arg, mnode);\n\tif (ret) {\n\t\t__mmu_int_rb_remove(mnode, &handler->root);\n\t\tlist_del(&mnode->list); /* remove from LRU list */\n\t}\n\tmnode->handler = handler;\nunlock:\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2756,
            "cve_id": "CVE-2020-27835",
            "code_snippet": "void hfi1_mmu_rb_evict(struct mmu_rb_handler *handler, void *evict_arg)\n{\n\tstruct mmu_rb_node *rbnode, *ptr;\n\tstruct list_head del_list;\n\tunsigned long flags;\n\tbool stop = false;\n\n\tif (current->mm != handler->mn.mm)\n\t\treturn;\n\n\tINIT_LIST_HEAD(&del_list);\n\n\tspin_lock_irqsave(&handler->lock, flags);\n\tlist_for_each_entry_safe_reverse(rbnode, ptr, &handler->lru_list,\n\t\t\t\t\t list) {\n\t\tif (handler->ops->evict(handler->ops_arg, rbnode, evict_arg,\n\t\t\t\t\t&stop)) {\n\t\t\t__mmu_int_rb_remove(rbnode, &handler->root);\n\t\t\t/* move from LRU list to delete list */\n\t\t\tlist_move(&rbnode->list, &del_list);\n\t\t}\n\t\tif (stop)\n\t\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\n\twhile (!list_empty(&del_list)) {\n\t\trbnode = list_first_entry(&del_list, struct mmu_rb_node, list);\n\t\tlist_del(&rbnode->list);\n\t\thandler->ops->remove(handler->ops_arg, rbnode);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2757,
            "cve_id": "CVE-2020-27835",
            "code_snippet": "bool hfi1_mmu_rb_remove_unless_exact(struct mmu_rb_handler *handler,\n\t\t\t\t     unsigned long addr, unsigned long len,\n\t\t\t\t     struct mmu_rb_node **rb_node)\n{\n\tstruct mmu_rb_node *node;\n\tunsigned long flags;\n\tbool ret = false;\n\n\tif (current->mm != handler->mn.mm)\n\t\treturn ret;\n\n\tspin_lock_irqsave(&handler->lock, flags);\n\tnode = __mmu_rb_search(handler, addr, len);\n\tif (node) {\n\t\tif (node->addr == addr && node->len == len)\n\t\t\tgoto unlock;\n\t\t__mmu_int_rb_remove(node, &handler->root);\n\t\tlist_del(&node->list); /* remove from LRU list */\n\t\tret = true;\n\t}\nunlock:\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\t*rb_node = node;\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2758,
            "cve_id": "CVE-2020-27835",
            "code_snippet": "void hfi1_mmu_rb_remove(struct mmu_rb_handler *handler,\n\t\t\tstruct mmu_rb_node *node)\n{\n\tunsigned long flags;\n\n\tif (current->mm != handler->mn.mm)\n\t\treturn;\n\n\t/* Validity of handler and node pointers has been checked by caller. */\n\ttrace_hfi1_mmu_rb_remove(node->addr, node->len);\n\tspin_lock_irqsave(&handler->lock, flags);\n\t__mmu_int_rb_remove(node, &handler->root);\n\tlist_del(&node->list); /* remove from LRU list */\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\n\thandler->ops->remove(handler->ops_arg, node);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2759,
            "cve_id": "CVE-2020-27835",
            "code_snippet": "void hfi1_mmu_rb_unregister(struct mmu_rb_handler *handler)\n{\n\tstruct mmu_rb_node *rbnode;\n\tstruct rb_node *node;\n\tunsigned long flags;\n\tstruct list_head del_list;\n\n\t/* Unregister first so we don't get any more notifications. */\n\tmmu_notifier_unregister(&handler->mn, handler->mn.mm);\n\n\t/*\n\t * Make sure the wq delete handler is finished running.  It will not\n\t * be triggered once the mmu notifiers are unregistered above.\n\t */\n\tflush_work(&handler->del_work);\n\n\tINIT_LIST_HEAD(&del_list);\n\n\tspin_lock_irqsave(&handler->lock, flags);\n\twhile ((node = rb_first_cached(&handler->root))) {\n\t\trbnode = rb_entry(node, struct mmu_rb_node, node);\n\t\trb_erase_cached(node, &handler->root);\n\t\t/* move from LRU list to delete list */\n\t\tlist_move(&rbnode->list, &del_list);\n\t}\n\tspin_unlock_irqrestore(&handler->lock, flags);\n\n\tdo_remove(handler, &del_list);\n\n\tkfree(handler);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2813,
            "cve_id": "CVE-2020-29660",
            "code_snippet": "void disassociate_ctty(int on_exit)\n{\n\tstruct tty_struct *tty;\n\n\tif (!current->signal->leader)\n\t\treturn;\n\n\ttty = get_current_tty();\n\tif (tty) {\n\t\tif (on_exit && tty->driver->type != TTY_DRIVER_TYPE_PTY) {\n\t\t\ttty_vhangup_session(tty);\n\t\t} else {\n\t\t\tstruct pid *tty_pgrp = tty_get_pgrp(tty);\n\t\t\tif (tty_pgrp) {\n\t\t\t\tkill_pgrp(tty_pgrp, SIGHUP, on_exit);\n\t\t\t\tif (!on_exit)\n\t\t\t\t\tkill_pgrp(tty_pgrp, SIGCONT, on_exit);\n\t\t\t\tput_pid(tty_pgrp);\n\t\t\t}\n\t\t}\n\t\ttty_kref_put(tty);\n\n\t} else if (on_exit) {\n\t\tstruct pid *old_pgrp;\n\t\tspin_lock_irq(&current->sighand->siglock);\n\t\told_pgrp = current->signal->tty_old_pgrp;\n\t\tcurrent->signal->tty_old_pgrp = NULL;\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\t\tif (old_pgrp) {\n\t\t\tkill_pgrp(old_pgrp, SIGHUP, on_exit);\n\t\t\tkill_pgrp(old_pgrp, SIGCONT, on_exit);\n\t\t\tput_pid(old_pgrp);\n\t\t}\n\t\treturn;\n\t}\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tput_pid(current->signal->tty_old_pgrp);\n\tcurrent->signal->tty_old_pgrp = NULL;\n\ttty = tty_kref_get(current->signal->tty);\n\tspin_unlock_irq(&current->sighand->siglock);\n\n\tif (tty) {\n\t\tunsigned long flags;\n\n\t\ttty_lock(tty);\n\t\tspin_lock_irqsave(&tty->ctrl_lock, flags);\n\t\tput_pid(tty->session);\n\t\tput_pid(tty->pgrp);\n\t\ttty->session = NULL;\n\t\ttty->pgrp = NULL;\n\t\tspin_unlock_irqrestore(&tty->ctrl_lock, flags);\n\t\ttty_unlock(tty);\n\t\ttty_kref_put(tty);\n\t}\n\n\t/* Now clear signal->tty under the lock */\n\tread_lock(&tasklist_lock);\n\tsession_clear_tty(task_session(current));\n\tread_unlock(&tasklist_lock);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2814,
            "cve_id": "CVE-2020-29660",
            "code_snippet": "static void __proc_set_tty(struct tty_struct *tty)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tty->ctrl_lock, flags);\n\t/*\n\t * The session and fg pgrp references will be non-NULL if\n\t * tiocsctty() is stealing the controlling tty\n\t */\n\tput_pid(tty->session);\n\tput_pid(tty->pgrp);\n\ttty->pgrp = get_pid(task_pgrp(current));\n\ttty->session = get_pid(task_session(current));\n\tspin_unlock_irqrestore(&tty->ctrl_lock, flags);\n\tif (current->signal->tty) {\n\t\ttty_debug(tty, \"current tty %s not NULL!!\\n\",\n\t\t\t  current->signal->tty->name);\n\t\ttty_kref_put(current->signal->tty);\n\t}\n\tput_pid(current->signal->tty_old_pgrp);\n\tcurrent->signal->tty = tty_kref_get(tty);\n\tcurrent->signal->tty_old_pgrp = NULL;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2830,
            "cve_id": "CVE-2020-36313",
            "code_snippet": "static inline struct kvm_memory_slot *\nsearch_memslots(struct kvm_memslots *slots, gfn_t gfn)\n{\n\tint start = 0, end = slots->used_slots;\n\tint slot = atomic_read(&slots->lru_slot);\n\tstruct kvm_memory_slot *memslots = slots->memslots;\n\n\tif (unlikely(!slots->used_slots))\n\t\treturn NULL;\n\n\tif (gfn >= memslots[slot].base_gfn &&\n\t    gfn < memslots[slot].base_gfn + memslots[slot].npages)\n\t\treturn &memslots[slot];\n\n\twhile (start < end) {\n\t\tslot = start + (end - start) / 2;\n\n\t\tif (gfn >= memslots[slot].base_gfn)\n\t\t\tend = slot;\n\t\telse\n\t\t\tstart = slot + 1;\n\t}\n\n\tif (gfn >= memslots[start].base_gfn &&\n\t    gfn < memslots[start].base_gfn + memslots[start].npages) {\n\t\tatomic_set(&slots->lru_slot, start);\n\t\treturn &memslots[start];\n\t}\n\n\treturn NULL;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2867,
            "cve_id": "CVE-2020-36387",
            "code_snippet": "static void io_poll_task_func(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_kiocb *nxt = NULL;\n\n\tio_poll_task_handler(req, &nxt);\n\tif (nxt)\n\t\t__io_req_task_submit(nxt);\n\tpercpu_ref_put(&ctx->refs);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2868,
            "cve_id": "CVE-2020-36387",
            "code_snippet": "static void io_async_task_func(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct async_poll *apoll = req->apoll;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\ttrace_io_uring_task_run(req->ctx, req->opcode, req->user_data);\n\n\tif (io_poll_rewait(req, &apoll->poll)) {\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\t\tpercpu_ref_put(&ctx->refs);\n\t\treturn;\n\t}\n\n\t/* If req is still hashed, it cannot have been canceled. Don't check. */\n\tif (hash_hashed(&req->hash_node))\n\t\thash_del(&req->hash_node);\n\n\tio_poll_remove_double(req, apoll->double_poll);\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tif (!READ_ONCE(apoll->poll.canceled))\n\t\t__io_req_task_submit(req);\n\telse\n\t\t__io_req_task_cancel(req, -ECANCELED);\n\n\tpercpu_ref_put(&ctx->refs);\n\tkfree(apoll->double_poll);\n\tkfree(apoll);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2869,
            "cve_id": "CVE-2020-36387",
            "code_snippet": "static int io_async_buf_func(struct wait_queue_entry *wait, unsigned mode,\n\t\t\t     int sync, void *arg)\n{\n\tstruct wait_page_queue *wpq;\n\tstruct io_kiocb *req = wait->private;\n\tstruct wait_page_key *key = arg;\n\tint ret;\n\n\twpq = container_of(wait, struct wait_page_queue, wait);\n\n\tif (!wake_page_match(wpq, key))\n\t\treturn 0;\n\n\tlist_del_init(&wait->entry);\n\n\tinit_task_work(&req->task_work, io_req_task_submit);\n\tpercpu_ref_get(&req->ctx->refs);\n\n\t/* submit ref gets dropped, acquire a new one */\n\trefcount_inc(&req->refs);\n\tret = io_req_task_work_add(req, &req->task_work);\n\tif (unlikely(ret)) {\n\t\tstruct task_struct *tsk;\n\n\t\t/* queue just for cancelation */\n\t\tinit_task_work(&req->task_work, io_req_task_cancel);\n\t\ttsk = io_wq_get_task(req->ctx->io_wq);\n\t\ttask_work_add(tsk, &req->task_work, 0);\n\t\twake_up_process(tsk);\n\t}\n\treturn 1;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2876,
            "cve_id": "CVE-2020-36557",
            "code_snippet": "int vc_allocate(unsigned int currcons)\t/* return 0 on success */\n{\n\tstruct vt_notifier_param param;\n\tstruct vc_data *vc;\n\n\tWARN_CONSOLE_UNLOCKED();\n\n\tif (currcons >= MAX_NR_CONSOLES)\n\t\treturn -ENXIO;\n\n\tif (vc_cons[currcons].d)\n\t\treturn 0;\n\n\t/* due to the granularity of kmalloc, we waste some memory here */\n\t/* the alloc is done in two steps, to optimize the common situation\n\t   of a 25x80 console (structsize=216, screenbuf_size=4000) */\n\t/* although the numbers above are not valid since long ago, the\n\t   point is still up-to-date and the comment still has its value\n\t   even if only as a historical artifact.  --mj, July 1998 */\n\tparam.vc = vc = kzalloc(sizeof(struct vc_data), GFP_KERNEL);\n\tif (!vc)\n\t\treturn -ENOMEM;\n\n\tvc_cons[currcons].d = vc;\n\ttty_port_init(&vc->port);\n\tvc->port.ops = &vc_port_ops;\n\tINIT_WORK(&vc_cons[currcons].SAK_work, vc_SAK);\n\n\tvisual_init(vc, currcons, 1);\n\n\tif (!*vc->vc_uni_pagedir_loc)\n\t\tcon_set_default_unimap(vc);\n\n\tvc->vc_screenbuf = kzalloc(vc->vc_screenbuf_size, GFP_KERNEL);\n\tif (!vc->vc_screenbuf)\n\t\tgoto err_free;\n\n\t/* If no drivers have overridden us and the user didn't pass a\n\t   boot option, default to displaying the cursor */\n\tif (global_cursor_default == -1)\n\t\tglobal_cursor_default = 1;\n\n\tvc_init(vc, vc->vc_rows, vc->vc_cols, 1);\n\tvcs_make_sysfs(currcons);\n\tatomic_notifier_call_chain(&vt_notifier_list, VT_ALLOCATE, &param);\n\n\treturn 0;\nerr_free:\n\tvisual_deinit(vc);\n\tkfree(vc);\n\tvc_cons[currcons].d = NULL;\n\treturn -ENOMEM;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2883,
            "cve_id": "CVE-2020-36694",
            "code_snippet": "unsigned int arpt_do_table(struct sk_buff *skb,\n\t\t\t   const struct nf_hook_state *state,\n\t\t\t   struct xt_table *table)\n{\n\tunsigned int hook = state->hook;\n\tstatic const char nulldevname[IFNAMSIZ] __attribute__((aligned(sizeof(long))));\n\tunsigned int verdict = NF_DROP;\n\tconst struct arphdr *arp;\n\tstruct arpt_entry *e, **jumpstack;\n\tconst char *indev, *outdev;\n\tconst void *table_base;\n\tunsigned int cpu, stackidx = 0;\n\tconst struct xt_table_info *private;\n\tstruct xt_action_param acpar;\n\tunsigned int addend;\n\n\tif (!pskb_may_pull(skb, arp_hdr_len(skb->dev)))\n\t\treturn NF_DROP;\n\n\tindev = state->in ? state->in->name : nulldevname;\n\toutdev = state->out ? state->out->name : nulldevname;\n\n\tlocal_bh_disable();\n\taddend = xt_write_recseq_begin();\n\tprivate = rcu_access_pointer(table->private);\n\tcpu     = smp_processor_id();\n\ttable_base = private->entries;\n\tjumpstack  = (struct arpt_entry **)private->jumpstack[cpu];\n\n\t/* No TEE support for arptables, so no need to switch to alternate\n\t * stack.  All targets that reenter must return absolute verdicts.\n\t */\n\te = get_entry(table_base, private->hook_entry[hook]);\n\n\tacpar.state   = state;\n\tacpar.hotdrop = false;\n\n\tarp = arp_hdr(skb);\n\tdo {\n\t\tconst struct xt_entry_target *t;\n\t\tstruct xt_counters *counter;\n\n\t\tif (!arp_packet_match(arp, skb->dev, indev, outdev, &e->arp)) {\n\t\t\te = arpt_next_entry(e);\n\t\t\tcontinue;\n\t\t}\n\n\t\tcounter = xt_get_this_cpu_counter(&e->counters);\n\t\tADD_COUNTER(*counter, arp_hdr_len(skb->dev), 1);\n\n\t\tt = arpt_get_target_c(e);\n\n\t\t/* Standard target? */\n\t\tif (!t->u.kernel.target->target) {\n\t\t\tint v;\n\n\t\t\tv = ((struct xt_standard_target *)t)->verdict;\n\t\t\tif (v < 0) {\n\t\t\t\t/* Pop from stack? */\n\t\t\t\tif (v != XT_RETURN) {\n\t\t\t\t\tverdict = (unsigned int)(-v) - 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (stackidx == 0) {\n\t\t\t\t\te = get_entry(table_base,\n\t\t\t\t\t\t      private->underflow[hook]);\n\t\t\t\t} else {\n\t\t\t\t\te = jumpstack[--stackidx];\n\t\t\t\t\te = arpt_next_entry(e);\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (table_base + v\n\t\t\t    != arpt_next_entry(e)) {\n\t\t\t\tif (unlikely(stackidx >= private->stacksize)) {\n\t\t\t\t\tverdict = NF_DROP;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tjumpstack[stackidx++] = e;\n\t\t\t}\n\n\t\t\te = get_entry(table_base, v);\n\t\t\tcontinue;\n\t\t}\n\n\t\tacpar.target   = t->u.kernel.target;\n\t\tacpar.targinfo = t->data;\n\t\tverdict = t->u.kernel.target->target(skb, &acpar);\n\n\t\tif (verdict == XT_CONTINUE) {\n\t\t\t/* Target might have changed stuff. */\n\t\t\tarp = arp_hdr(skb);\n\t\t\te = arpt_next_entry(e);\n\t\t} else {\n\t\t\t/* Verdict */\n\t\t\tbreak;\n\t\t}\n\t} while (!acpar.hotdrop);\n\txt_write_recseq_end(addend);\n\tlocal_bh_enable();\n\n\tif (acpar.hotdrop)\n\t\treturn NF_DROP;\n\telse\n\t\treturn verdict;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2884,
            "cve_id": "CVE-2020-36694",
            "code_snippet": "static int get_info(struct net *net, void __user *user, const int *len)\n{\n\tchar name[XT_TABLE_MAXNAMELEN];\n\tstruct xt_table *t;\n\tint ret;\n\n\tif (*len != sizeof(struct arpt_getinfo))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(name, user, sizeof(name)) != 0)\n\t\treturn -EFAULT;\n\n\tname[XT_TABLE_MAXNAMELEN-1] = '\\0';\n#ifdef CONFIG_COMPAT\n\tif (in_compat_syscall())\n\t\txt_compat_lock(NFPROTO_ARP);\n#endif\n\tt = xt_request_find_table_lock(net, NFPROTO_ARP, name);\n\tif (!IS_ERR(t)) {\n\t\tstruct arpt_getinfo info;\n\t\tconst struct xt_table_info *private = xt_table_get_private_protected(t);\n#ifdef CONFIG_COMPAT\n\t\tstruct xt_table_info tmp;\n\n\t\tif (in_compat_syscall()) {\n\t\t\tret = compat_table_info(private, &tmp);\n\t\t\txt_compat_flush_offsets(NFPROTO_ARP);\n\t\t\tprivate = &tmp;\n\t\t}\n#endif\n\t\tmemset(&info, 0, sizeof(info));\n\t\tinfo.valid_hooks = t->valid_hooks;\n\t\tmemcpy(info.hook_entry, private->hook_entry,\n\t\t       sizeof(info.hook_entry));\n\t\tmemcpy(info.underflow, private->underflow,\n\t\t       sizeof(info.underflow));\n\t\tinfo.num_entries = private->number;\n\t\tinfo.size = private->size;\n\t\tstrcpy(info.name, name);\n\n\t\tif (copy_to_user(user, &info, *len) != 0)\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = 0;\n\t\txt_table_unlock(t);\n\t\tmodule_put(t->me);\n\t} else\n\t\tret = PTR_ERR(t);\n#ifdef CONFIG_COMPAT\n\tif (in_compat_syscall())\n\t\txt_compat_unlock(NFPROTO_ARP);\n#endif\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2885,
            "cve_id": "CVE-2020-36694",
            "code_snippet": "static int compat_copy_entries_to_user(unsigned int total_size,\n\t\t\t\t       struct xt_table *table,\n\t\t\t\t       void __user *userptr)\n{\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = xt_table_get_private_protected(table);\n\tvoid __user *pos;\n\tunsigned int size;\n\tint ret = 0;\n\tunsigned int i = 0;\n\tstruct arpt_entry *iter;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tpos = userptr;\n\tsize = total_size;\n\txt_entry_foreach(iter, private->entries, total_size) {\n\t\tret = compat_copy_entry_to_user(iter, &pos,\n\t\t\t\t\t\t&size, counters, i++);\n\t\tif (ret != 0)\n\t\t\tbreak;\n\t}\n\tvfree(counters);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2886,
            "cve_id": "CVE-2020-36694",
            "code_snippet": "static int do_add_counters(struct net *net, sockptr_t arg, unsigned int len)\n{\n\tunsigned int i;\n\tstruct xt_counters_info tmp;\n\tstruct xt_counters *paddc;\n\tstruct xt_table *t;\n\tconst struct xt_table_info *private;\n\tint ret = 0;\n\tstruct arpt_entry *iter;\n\tunsigned int addend;\n\n\tpaddc = xt_copy_counters(arg, len, &tmp);\n\tif (IS_ERR(paddc))\n\t\treturn PTR_ERR(paddc);\n\n\tt = xt_find_table_lock(net, NFPROTO_ARP, tmp.name);\n\tif (IS_ERR(t)) {\n\t\tret = PTR_ERR(t);\n\t\tgoto free;\n\t}\n\n\tlocal_bh_disable();\n\tprivate = xt_table_get_private_protected(t);\n\tif (private->number != tmp.num_counters) {\n\t\tret = -EINVAL;\n\t\tgoto unlock_up_free;\n\t}\n\n\ti = 0;\n\n\taddend = xt_write_recseq_begin();\n\txt_entry_foreach(iter,  private->entries, private->size) {\n\t\tstruct xt_counters *tmp;\n\n\t\ttmp = xt_get_this_cpu_counter(&iter->counters);\n\t\tADD_COUNTER(*tmp, paddc[i].bcnt, paddc[i].pcnt);\n\t\t++i;\n\t}\n\txt_write_recseq_end(addend);\n unlock_up_free:\n\tlocal_bh_enable();\n\txt_table_unlock(t);\n\tmodule_put(t->me);\n free:\n\tvfree(paddc);\n\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2887,
            "cve_id": "CVE-2020-36694",
            "code_snippet": "static int get_entries(struct net *net, struct arpt_get_entries __user *uptr,\n\t\t       const int *len)\n{\n\tint ret;\n\tstruct arpt_get_entries get;\n\tstruct xt_table *t;\n\n\tif (*len < sizeof(get))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&get, uptr, sizeof(get)) != 0)\n\t\treturn -EFAULT;\n\tif (*len != sizeof(struct arpt_get_entries) + get.size)\n\t\treturn -EINVAL;\n\n\tget.name[sizeof(get.name) - 1] = '\\0';\n\n\tt = xt_find_table_lock(net, NFPROTO_ARP, get.name);\n\tif (!IS_ERR(t)) {\n\t\tconst struct xt_table_info *private = xt_table_get_private_protected(t);\n\n\t\tif (get.size == private->size)\n\t\t\tret = copy_entries_to_user(private->size,\n\t\t\t\t\t\t   t, uptr->entrytable);\n\t\telse\n\t\t\tret = -EAGAIN;\n\n\t\tmodule_put(t->me);\n\t\txt_table_unlock(t);\n\t} else\n\t\tret = PTR_ERR(t);\n\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2888,
            "cve_id": "CVE-2020-36694",
            "code_snippet": "static int copy_entries_to_user(unsigned int total_size,\n\t\t\t\tconst struct xt_table *table,\n\t\t\t\tvoid __user *userptr)\n{\n\tunsigned int off, num;\n\tconst struct arpt_entry *e;\n\tstruct xt_counters *counters;\n\tstruct xt_table_info *private = xt_table_get_private_protected(table);\n\tint ret = 0;\n\tvoid *loc_cpu_entry;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tloc_cpu_entry = private->entries;\n\n\t/* FIXME: use iterator macros --RR */\n\t/* ... then go back and fix counters and names */\n\tfor (off = 0, num = 0; off < total_size; off += e->next_offset, num++){\n\t\tconst struct xt_entry_target *t;\n\n\t\te = loc_cpu_entry + off;\n\t\tif (copy_to_user(userptr + off, e, sizeof(*e))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\t\tif (copy_to_user(userptr + off\n\t\t\t\t + offsetof(struct arpt_entry, counters),\n\t\t\t\t &counters[num],\n\t\t\t\t sizeof(counters[num])) != 0) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\n\t\tt = arpt_get_target_c(e);\n\t\tif (xt_target_to_user(t, userptr + off + e->target_offset)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto free_counters;\n\t\t}\n\t}\n\n free_counters:\n\tvfree(counters);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2889,
            "cve_id": "CVE-2020-36694",
            "code_snippet": "static struct xt_counters *alloc_counters(const struct xt_table *table)\n{\n\tunsigned int countersize;\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = xt_table_get_private_protected(table);\n\n\t/* We need atomic snapshot of counters: rest doesn't change\n\t   (other than comefrom, which userspace doesn't care\n\t   about). */\n\tcountersize = sizeof(struct xt_counters) * private->number;\n\tcounters = vzalloc(countersize);\n\n\tif (counters == NULL)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tget_counters(private, counters);\n\n\treturn counters;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2890,
            "cve_id": "CVE-2020-36694",
            "code_snippet": "static int\nget_entries(struct net *net, struct ipt_get_entries __user *uptr,\n\t    const int *len)\n{\n\tint ret;\n\tstruct ipt_get_entries get;\n\tstruct xt_table *t;\n\n\tif (*len < sizeof(get))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&get, uptr, sizeof(get)) != 0)\n\t\treturn -EFAULT;\n\tif (*len != sizeof(struct ipt_get_entries) + get.size)\n\t\treturn -EINVAL;\n\tget.name[sizeof(get.name) - 1] = '\\0';\n\n\tt = xt_find_table_lock(net, AF_INET, get.name);\n\tif (!IS_ERR(t)) {\n\t\tconst struct xt_table_info *private = xt_table_get_private_protected(t);\n\t\tif (get.size == private->size)\n\t\t\tret = copy_entries_to_user(private->size,\n\t\t\t\t\t\t   t, uptr->entrytable);\n\t\telse\n\t\t\tret = -EAGAIN;\n\n\t\tmodule_put(t->me);\n\t\txt_table_unlock(t);\n\t} else\n\t\tret = PTR_ERR(t);\n\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2891,
            "cve_id": "CVE-2020-36694",
            "code_snippet": "unsigned int\nipt_do_table(struct sk_buff *skb,\n\t     const struct nf_hook_state *state,\n\t     struct xt_table *table)\n{\n\tunsigned int hook = state->hook;\n\tstatic const char nulldevname[IFNAMSIZ] __attribute__((aligned(sizeof(long))));\n\tconst struct iphdr *ip;\n\t/* Initializing verdict to NF_DROP keeps gcc happy. */\n\tunsigned int verdict = NF_DROP;\n\tconst char *indev, *outdev;\n\tconst void *table_base;\n\tstruct ipt_entry *e, **jumpstack;\n\tunsigned int stackidx, cpu;\n\tconst struct xt_table_info *private;\n\tstruct xt_action_param acpar;\n\tunsigned int addend;\n\n\t/* Initialization */\n\tstackidx = 0;\n\tip = ip_hdr(skb);\n\tindev = state->in ? state->in->name : nulldevname;\n\toutdev = state->out ? state->out->name : nulldevname;\n\t/* We handle fragments by dealing with the first fragment as\n\t * if it was a normal packet.  All other fragments are treated\n\t * normally, except that they will NEVER match rules that ask\n\t * things we don't know, ie. tcp syn flag or ports).  If the\n\t * rule is also a fragment-specific rule, non-fragments won't\n\t * match it. */\n\tacpar.fragoff = ntohs(ip->frag_off) & IP_OFFSET;\n\tacpar.thoff   = ip_hdrlen(skb);\n\tacpar.hotdrop = false;\n\tacpar.state   = state;\n\n\tWARN_ON(!(table->valid_hooks & (1 << hook)));\n\tlocal_bh_disable();\n\taddend = xt_write_recseq_begin();\n\tprivate = rcu_access_pointer(table->private);\n\tcpu        = smp_processor_id();\n\ttable_base = private->entries;\n\tjumpstack  = (struct ipt_entry **)private->jumpstack[cpu];\n\n\t/* Switch to alternate jumpstack if we're being invoked via TEE.\n\t * TEE issues XT_CONTINUE verdict on original skb so we must not\n\t * clobber the jumpstack.\n\t *\n\t * For recursion via REJECT or SYNPROXY the stack will be clobbered\n\t * but it is no problem since absolute verdict is issued by these.\n\t */\n\tif (static_key_false(&xt_tee_enabled))\n\t\tjumpstack += private->stacksize * __this_cpu_read(nf_skb_duplicated);\n\n\te = get_entry(table_base, private->hook_entry[hook]);\n\n\tdo {\n\t\tconst struct xt_entry_target *t;\n\t\tconst struct xt_entry_match *ematch;\n\t\tstruct xt_counters *counter;\n\n\t\tWARN_ON(!e);\n\t\tif (!ip_packet_match(ip, indev, outdev,\n\t\t    &e->ip, acpar.fragoff)) {\n no_match:\n\t\t\te = ipt_next_entry(e);\n\t\t\tcontinue;\n\t\t}\n\n\t\txt_ematch_foreach(ematch, e) {\n\t\t\tacpar.match     = ematch->u.kernel.match;\n\t\t\tacpar.matchinfo = ematch->data;\n\t\t\tif (!acpar.match->match(skb, &acpar))\n\t\t\t\tgoto no_match;\n\t\t}\n\n\t\tcounter = xt_get_this_cpu_counter(&e->counters);\n\t\tADD_COUNTER(*counter, skb->len, 1);\n\n\t\tt = ipt_get_target_c(e);\n\t\tWARN_ON(!t->u.kernel.target);\n\n#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE)\n\t\t/* The packet is traced: log it */\n\t\tif (unlikely(skb->nf_trace))\n\t\t\ttrace_packet(state->net, skb, hook, state->in,\n\t\t\t\t     state->out, table->name, private, e);\n#endif\n\t\t/* Standard target? */\n\t\tif (!t->u.kernel.target->target) {\n\t\t\tint v;\n\n\t\t\tv = ((struct xt_standard_target *)t)->verdict;\n\t\t\tif (v < 0) {\n\t\t\t\t/* Pop from stack? */\n\t\t\t\tif (v != XT_RETURN) {\n\t\t\t\t\tverdict = (unsigned int)(-v) - 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (stackidx == 0) {\n\t\t\t\t\te = get_entry(table_base,\n\t\t\t\t\t    private->underflow[hook]);\n\t\t\t\t} else {\n\t\t\t\t\te = jumpstack[--stackidx];\n\t\t\t\t\te = ipt_next_entry(e);\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (table_base + v != ipt_next_entry(e) &&\n\t\t\t    !(e->ip.flags & IPT_F_GOTO)) {\n\t\t\t\tif (unlikely(stackidx >= private->stacksize)) {\n\t\t\t\t\tverdict = NF_DROP;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tjumpstack[stackidx++] = e;\n\t\t\t}\n\n\t\t\te = get_entry(table_base, v);\n\t\t\tcontinue;\n\t\t}\n\n\t\tacpar.target   = t->u.kernel.target;\n\t\tacpar.targinfo = t->data;\n\n\t\tverdict = t->u.kernel.target->target(skb, &acpar);\n\t\tif (verdict == XT_CONTINUE) {\n\t\t\t/* Target might have changed stuff. */\n\t\t\tip = ip_hdr(skb);\n\t\t\te = ipt_next_entry(e);\n\t\t} else {\n\t\t\t/* Verdict */\n\t\t\tbreak;\n\t\t}\n\t} while (!acpar.hotdrop);\n\n\txt_write_recseq_end(addend);\n\tlocal_bh_enable();\n\n\tif (acpar.hotdrop)\n\t\treturn NF_DROP;\n\telse return verdict;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2892,
            "cve_id": "CVE-2020-36694",
            "code_snippet": "static int get_info(struct net *net, void __user *user, const int *len)\n{\n\tchar name[XT_TABLE_MAXNAMELEN];\n\tstruct xt_table *t;\n\tint ret;\n\n\tif (*len != sizeof(struct ipt_getinfo))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(name, user, sizeof(name)) != 0)\n\t\treturn -EFAULT;\n\n\tname[XT_TABLE_MAXNAMELEN-1] = '\\0';\n#ifdef CONFIG_COMPAT\n\tif (in_compat_syscall())\n\t\txt_compat_lock(AF_INET);\n#endif\n\tt = xt_request_find_table_lock(net, AF_INET, name);\n\tif (!IS_ERR(t)) {\n\t\tstruct ipt_getinfo info;\n\t\tconst struct xt_table_info *private = xt_table_get_private_protected(t);\n#ifdef CONFIG_COMPAT\n\t\tstruct xt_table_info tmp;\n\n\t\tif (in_compat_syscall()) {\n\t\t\tret = compat_table_info(private, &tmp);\n\t\t\txt_compat_flush_offsets(AF_INET);\n\t\t\tprivate = &tmp;\n\t\t}\n#endif\n\t\tmemset(&info, 0, sizeof(info));\n\t\tinfo.valid_hooks = t->valid_hooks;\n\t\tmemcpy(info.hook_entry, private->hook_entry,\n\t\t       sizeof(info.hook_entry));\n\t\tmemcpy(info.underflow, private->underflow,\n\t\t       sizeof(info.underflow));\n\t\tinfo.num_entries = private->number;\n\t\tinfo.size = private->size;\n\t\tstrcpy(info.name, name);\n\n\t\tif (copy_to_user(user, &info, *len) != 0)\n\t\t\tret = -EFAULT;\n\t\telse\n\t\t\tret = 0;\n\n\t\txt_table_unlock(t);\n\t\tmodule_put(t->me);\n\t} else\n\t\tret = PTR_ERR(t);\n#ifdef CONFIG_COMPAT\n\tif (in_compat_syscall())\n\t\txt_compat_unlock(AF_INET);\n#endif\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2893,
            "cve_id": "CVE-2020-36694",
            "code_snippet": "static int\ncompat_copy_entries_to_user(unsigned int total_size, struct xt_table *table,\n\t\t\t    void __user *userptr)\n{\n\tstruct xt_counters *counters;\n\tconst struct xt_table_info *private = xt_table_get_private_protected(table);\n\tvoid __user *pos;\n\tunsigned int size;\n\tint ret = 0;\n\tunsigned int i = 0;\n\tstruct ipt_entry *iter;\n\n\tcounters = alloc_counters(table);\n\tif (IS_ERR(counters))\n\t\treturn PTR_ERR(counters);\n\n\tpos = userptr;\n\tsize = total_size;\n\txt_entry_foreach(iter, private->entries, total_size) {\n\t\tret = compat_copy_entry_to_user(iter, &pos,\n\t\t\t\t\t\t&size, counters, i++);\n\t\tif (ret != 0)\n\t\t\tbreak;\n\t}\n\n\tvfree(counters);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2912,
            "cve_id": "CVE-2020-7053",
            "code_snippet": "static int gem_context_register(struct i915_gem_context *ctx,\n\t\t\t\tstruct drm_i915_file_private *fpriv)\n{\n\tint ret;\n\n\tctx->file_priv = fpriv;\n\tif (ctx->ppgtt)\n\t\tctx->ppgtt->vm.file = fpriv;\n\n\tctx->pid = get_task_pid(current, PIDTYPE_PID);\n\tctx->name = kasprintf(GFP_KERNEL, \"%s[%d]\",\n\t\t\t      current->comm, pid_nr(ctx->pid));\n\tif (!ctx->name) {\n\t\tret = -ENOMEM;\n\t\tgoto err_pid;\n\t}\n\n\t/* And finally expose ourselves to userspace via the idr */\n\tmutex_lock(&fpriv->context_idr_lock);\n\tret = idr_alloc(&fpriv->context_idr, ctx,\n\t\t\tDEFAULT_CONTEXT_HANDLE, 0, GFP_KERNEL);\n\tif (ret >= 0)\n\t\tctx->user_handle = ret;\n\tmutex_unlock(&fpriv->context_idr_lock);\n\tif (ret < 0)\n\t\tgoto err_name;\n\n\treturn 0;\n\nerr_name:\n\tkfree(fetch_and_zero(&ctx->name));\nerr_pid:\n\tput_pid(fetch_and_zero(&ctx->pid));\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2913,
            "cve_id": "CVE-2020-7053",
            "code_snippet": "int i915_gem_context_create_ioctl(struct drm_device *dev, void *data,\n\t\t\t\t  struct drm_file *file)\n{\n\tstruct drm_i915_private *i915 = to_i915(dev);\n\tstruct drm_i915_gem_context_create *args = data;\n\tstruct drm_i915_file_private *file_priv = file->driver_priv;\n\tstruct i915_gem_context *ctx;\n\tint ret;\n\n\tif (!DRIVER_CAPS(i915)->has_logical_contexts)\n\t\treturn -ENODEV;\n\n\tif (args->pad != 0)\n\t\treturn -EINVAL;\n\n\tret = i915_terminally_wedged(i915);\n\tif (ret)\n\t\treturn ret;\n\n\tif (client_is_banned(file_priv)) {\n\t\tDRM_DEBUG(\"client %s[%d] banned from creating ctx\\n\",\n\t\t\t  current->comm,\n\t\t\t  pid_nr(get_task_pid(current, PIDTYPE_PID)));\n\n\t\treturn -EIO;\n\t}\n\n\tret = i915_mutex_lock_interruptible(dev);\n\tif (ret)\n\t\treturn ret;\n\n\tctx = i915_gem_create_context(i915);\n\tmutex_unlock(&dev->struct_mutex);\n\tif (IS_ERR(ctx))\n\t\treturn PTR_ERR(ctx);\n\n\tret = gem_context_register(ctx, file_priv);\n\tif (ret)\n\t\tgoto err_ctx;\n\n\targs->ctx_id = ctx->user_handle;\n\tDRM_DEBUG(\"HW context %d created\\n\", args->ctx_id);\n\n\treturn 0;\n\nerr_ctx:\n\tmutex_lock(&dev->struct_mutex);\n\tcontext_close(ctx);\n\tmutex_unlock(&dev->struct_mutex);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2920,
            "cve_id": "CVE-2020-8648",
            "code_snippet": "int paste_selection(struct tty_struct *tty)\n{\n\tstruct vc_data *vc = tty->driver_data;\n\tint\tpasted = 0;\n\tunsigned int count;\n\tstruct  tty_ldisc *ld;\n\tDECLARE_WAITQUEUE(wait, current);\n\tint ret = 0;\n\n\tconsole_lock();\n\tpoke_blanked_console();\n\tconsole_unlock();\n\n\tld = tty_ldisc_ref_wait(tty);\n\tif (!ld)\n\t\treturn -EIO;\t/* ldisc was hung up */\n\ttty_buffer_lock_exclusive(&vc->port);\n\n\tadd_wait_queue(&vc->paste_wait, &wait);\n\tmutex_lock(&sel_lock);\n\twhile (sel_buffer && sel_buffer_lth > pasted) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tif (signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\t\tif (tty_throttled(tty)) {\n\t\t\tmutex_unlock(&sel_lock);\n\t\t\tschedule();\n\t\t\tmutex_lock(&sel_lock);\n\t\t\tcontinue;\n\t\t}\n\t\t__set_current_state(TASK_RUNNING);\n\t\tcount = sel_buffer_lth - pasted;\n\t\tcount = tty_ldisc_receive_buf(ld, sel_buffer + pasted, NULL,\n\t\t\t\t\t      count);\n\t\tpasted += count;\n\t}\n\tmutex_unlock(&sel_lock);\n\tremove_wait_queue(&vc->paste_wait, &wait);\n\t__set_current_state(TASK_RUNNING);\n\n\ttty_buffer_unlock_exclusive(&vc->port);\n\ttty_ldisc_deref(ld);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2954,
            "cve_id": "CVE-2021-0941",
            "code_snippet": "static inline int __bpf_skb_change_head(struct sk_buff *skb, u32 head_room,\n\t\t\t\t\tu64 flags)\n{\n\tu32 max_len = BPF_SKB_MAX_LEN;\n\tu32 new_len = skb->len + head_room;\n\tint ret;\n\n\tif (unlikely(flags || (!skb_is_gso(skb) && new_len > max_len) ||\n\t\t     new_len < skb->len))\n\t\treturn -EINVAL;\n\n\tret = skb_cow(skb, head_room);\n\tif (likely(!ret)) {\n\t\t/* Idea for this helper is that we currently only\n\t\t * allow to expand on mac header. This means that\n\t\t * skb->protocol network header, etc, stay as is.\n\t\t * Compared to bpf_skb_change_tail(), we're more\n\t\t * flexible due to not needing to linearize or\n\t\t * reset GSO. Intention for this helper is to be\n\t\t * used by an L3 skb that needs to push mac header\n\t\t * for redirection into L2 device.\n\t\t */\n\t\t__skb_push(skb, head_room);\n\t\tmemset(skb->data, 0, head_room);\n\t\tskb_reset_mac_header(skb);\n\t}\n\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 2984,
            "cve_id": "CVE-2021-20292",
            "code_snippet": "int ttm_dma_tt_init(struct ttm_dma_tt *ttm_dma, struct ttm_buffer_object *bo,\n\t\t    uint32_t page_flags)\n{\n\tstruct ttm_tt *ttm = &ttm_dma->ttm;\n\n\tttm_tt_init_fields(ttm, bo, page_flags);\n\n\tINIT_LIST_HEAD(&ttm_dma->pages_list);\n\tif (ttm_dma_tt_alloc_page_directory(ttm_dma)) {\n\t\tpr_err(\"Failed allocating page table\\n\");\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3032,
            "cve_id": "CVE-2021-28691",
            "code_snippet": "static void xenvif_disconnect_queue(struct xenvif_queue *queue)\n{\n\tif (queue->task) {\n\t\tkthread_stop(queue->task);\n\t\tput_task_struct(queue->task);\n\t\tqueue->task = NULL;\n\t}\n\n\tif (queue->dealloc_task) {\n\t\tkthread_stop(queue->dealloc_task);\n\t\tqueue->dealloc_task = NULL;\n\t}\n\n\tif (queue->napi.poll) {\n\t\tnetif_napi_del(&queue->napi);\n\t\tqueue->napi.poll = NULL;\n\t}\n\n\tif (queue->tx_irq) {\n\t\tunbind_from_irqhandler(queue->tx_irq, queue);\n\t\tif (queue->tx_irq == queue->rx_irq)\n\t\t\tqueue->rx_irq = 0;\n\t\tqueue->tx_irq = 0;\n\t}\n\n\tif (queue->rx_irq) {\n\t\tunbind_from_irqhandler(queue->rx_irq, queue);\n\t\tqueue->rx_irq = 0;\n\t}\n\n\txenvif_unmap_frontend_data_rings(queue);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3072,
            "cve_id": "CVE-2021-29657",
            "code_snippet": "int nested_svm_vmrun(struct vcpu_svm *svm)\n{\n\tint ret;\n\tstruct vmcb *vmcb12;\n\tstruct vmcb *hsave = svm->nested.hsave;\n\tstruct vmcb *vmcb = svm->vmcb;\n\tstruct kvm_host_map map;\n\tu64 vmcb12_gpa;\n\n\tif (is_smm(&svm->vcpu)) {\n\t\tkvm_queue_exception(&svm->vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tvmcb12_gpa = svm->vmcb->save.rax;\n\tret = kvm_vcpu_map(&svm->vcpu, gpa_to_gfn(vmcb12_gpa), &map);\n\tif (ret == -EINVAL) {\n\t\tkvm_inject_gp(&svm->vcpu, 0);\n\t\treturn 1;\n\t} else if (ret) {\n\t\treturn kvm_skip_emulated_instruction(&svm->vcpu);\n\t}\n\n\tret = kvm_skip_emulated_instruction(&svm->vcpu);\n\n\tvmcb12 = map.hva;\n\n\tif (WARN_ON_ONCE(!svm->nested.initialized))\n\t\treturn -EINVAL;\n\n\tload_nested_vmcb_control(svm, &vmcb12->control);\n\n\tif (!nested_vmcb_check_save(svm, vmcb12) ||\n\t    !nested_vmcb_check_controls(&svm->nested.ctl)) {\n\t\tvmcb12->control.exit_code    = SVM_EXIT_ERR;\n\t\tvmcb12->control.exit_code_hi = 0;\n\t\tvmcb12->control.exit_info_1  = 0;\n\t\tvmcb12->control.exit_info_2  = 0;\n\t\tgoto out;\n\t}\n\n\ttrace_kvm_nested_vmrun(svm->vmcb->save.rip, vmcb12_gpa,\n\t\t\t       vmcb12->save.rip,\n\t\t\t       vmcb12->control.int_ctl,\n\t\t\t       vmcb12->control.event_inj,\n\t\t\t       vmcb12->control.nested_ctl);\n\n\ttrace_kvm_nested_intercepts(vmcb12->control.intercepts[INTERCEPT_CR] & 0xffff,\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_CR] >> 16,\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_EXCEPTION],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD3],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD4],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD5]);\n\n\t/* Clear internal status */\n\tkvm_clear_exception_queue(&svm->vcpu);\n\tkvm_clear_interrupt_queue(&svm->vcpu);\n\n\t/*\n\t * Save the old vmcb, so we don't need to pick what we save, but can\n\t * restore everything when a VMEXIT occurs\n\t */\n\thsave->save.es     = vmcb->save.es;\n\thsave->save.cs     = vmcb->save.cs;\n\thsave->save.ss     = vmcb->save.ss;\n\thsave->save.ds     = vmcb->save.ds;\n\thsave->save.gdtr   = vmcb->save.gdtr;\n\thsave->save.idtr   = vmcb->save.idtr;\n\thsave->save.efer   = svm->vcpu.arch.efer;\n\thsave->save.cr0    = kvm_read_cr0(&svm->vcpu);\n\thsave->save.cr4    = svm->vcpu.arch.cr4;\n\thsave->save.rflags = kvm_get_rflags(&svm->vcpu);\n\thsave->save.rip    = kvm_rip_read(&svm->vcpu);\n\thsave->save.rsp    = vmcb->save.rsp;\n\thsave->save.rax    = vmcb->save.rax;\n\tif (npt_enabled)\n\t\thsave->save.cr3    = vmcb->save.cr3;\n\telse\n\t\thsave->save.cr3    = kvm_read_cr3(&svm->vcpu);\n\n\tcopy_vmcb_control_area(&hsave->control, &vmcb->control);\n\n\tsvm->nested.nested_run_pending = 1;\n\n\tif (enter_svm_guest_mode(svm, vmcb12_gpa, vmcb12))\n\t\tgoto out_exit_err;\n\n\tif (nested_svm_vmrun_msrpm(svm))\n\t\tgoto out;\n\nout_exit_err:\n\tsvm->nested.nested_run_pending = 0;\n\n\tsvm->vmcb->control.exit_code    = SVM_EXIT_ERR;\n\tsvm->vmcb->control.exit_code_hi = 0;\n\tsvm->vmcb->control.exit_info_1  = 0;\n\tsvm->vmcb->control.exit_info_2  = 0;\n\n\tnested_svm_vmexit(svm);\n\nout:\n\tkvm_vcpu_unmap(&svm->vcpu, &map, true);\n\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3082,
            "cve_id": "CVE-2021-32606",
            "code_snippet": "static int isotp_bind(struct socket *sock, struct sockaddr *uaddr, int len)\n{\n\tstruct sockaddr_can *addr = (struct sockaddr_can *)uaddr;\n\tstruct sock *sk = sock->sk;\n\tstruct isotp_sock *so = isotp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint ifindex;\n\tstruct net_device *dev;\n\tint err = 0;\n\tint notify_enetdown = 0;\n\tint do_rx_reg = 1;\n\n\tif (len < ISOTP_MIN_NAMELEN)\n\t\treturn -EINVAL;\n\n\tif (addr->can_addr.tp.tx_id & (CAN_ERR_FLAG | CAN_RTR_FLAG))\n\t\treturn -EADDRNOTAVAIL;\n\n\tif (!addr->can_ifindex)\n\t\treturn -ENODEV;\n\n\tlock_sock(sk);\n\n\t/* do not register frame reception for functional addressing */\n\tif (so->opt.flags & CAN_ISOTP_SF_BROADCAST)\n\t\tdo_rx_reg = 0;\n\n\t/* do not validate rx address for functional addressing */\n\tif (do_rx_reg) {\n\t\tif (addr->can_addr.tp.rx_id == addr->can_addr.tp.tx_id) {\n\t\t\terr = -EADDRNOTAVAIL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (addr->can_addr.tp.rx_id & (CAN_ERR_FLAG | CAN_RTR_FLAG)) {\n\t\t\terr = -EADDRNOTAVAIL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (so->bound && addr->can_ifindex == so->ifindex &&\n\t    addr->can_addr.tp.rx_id == so->rxid &&\n\t    addr->can_addr.tp.tx_id == so->txid)\n\t\tgoto out;\n\n\tdev = dev_get_by_index(net, addr->can_ifindex);\n\tif (!dev) {\n\t\terr = -ENODEV;\n\t\tgoto out;\n\t}\n\tif (dev->type != ARPHRD_CAN) {\n\t\tdev_put(dev);\n\t\terr = -ENODEV;\n\t\tgoto out;\n\t}\n\tif (dev->mtu < so->ll.mtu) {\n\t\tdev_put(dev);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (!(dev->flags & IFF_UP))\n\t\tnotify_enetdown = 1;\n\n\tifindex = dev->ifindex;\n\n\tif (do_rx_reg)\n\t\tcan_rx_register(net, dev, addr->can_addr.tp.rx_id,\n\t\t\t\tSINGLE_MASK(addr->can_addr.tp.rx_id),\n\t\t\t\tisotp_rcv, sk, \"isotp\", sk);\n\n\tdev_put(dev);\n\n\tif (so->bound && do_rx_reg) {\n\t\t/* unregister old filter */\n\t\tif (so->ifindex) {\n\t\t\tdev = dev_get_by_index(net, so->ifindex);\n\t\t\tif (dev) {\n\t\t\t\tcan_rx_unregister(net, dev, so->rxid,\n\t\t\t\t\t\t  SINGLE_MASK(so->rxid),\n\t\t\t\t\t\t  isotp_rcv, sk);\n\t\t\t\tdev_put(dev);\n\t\t\t}\n\t\t}\n\t}\n\n\t/* switch to new settings */\n\tso->ifindex = ifindex;\n\tso->rxid = addr->can_addr.tp.rx_id;\n\tso->txid = addr->can_addr.tp.tx_id;\n\tso->bound = 1;\n\nout:\n\trelease_sock(sk);\n\n\tif (notify_enetdown) {\n\t\tsk->sk_err = ENETDOWN;\n\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\tsk->sk_error_report(sk);\n\t}\n\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3084,
            "cve_id": "CVE-2021-33034",
            "code_snippet": "static void hci_disconn_loglink_complete_evt(struct hci_dev *hdev,\n\t\t\t\t\t     struct sk_buff *skb)\n{\n\tstruct hci_ev_disconn_logical_link_complete *ev = (void *) skb->data;\n\tstruct hci_chan *hchan;\n\n\tBT_DBG(\"%s log handle 0x%4.4x status 0x%2.2x\", hdev->name,\n\t       le16_to_cpu(ev->handle), ev->status);\n\n\tif (ev->status)\n\t\treturn;\n\n\thci_dev_lock(hdev);\n\n\thchan = hci_chan_lookup_handle(hdev, le16_to_cpu(ev->handle));\n\tif (!hchan || !hchan->amp)\n\t\tgoto unlock;\n\n\tamp_destroy_logical_link(hchan, ev->reason);\n\nunlock:\n\thci_dev_unlock(hdev);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3093,
            "cve_id": "CVE-2021-3347",
            "code_snippet": "static int fixup_owner(u32 __user *uaddr, struct futex_q *q, int locked)\n{\n\tif (locked) {\n\t\t/*\n\t\t * Got the lock. We might not be the anticipated owner if we\n\t\t * did a lock-steal - fix up the PI-state in that case:\n\t\t *\n\t\t * Speculative pi_state->owner read (we don't hold wait_lock);\n\t\t * since we own the lock pi_state->owner == current is the\n\t\t * stable state, anything else needs more attention.\n\t\t */\n\t\tif (q->pi_state->owner != current)\n\t\t\treturn fixup_pi_state_owner(uaddr, q, current);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * If we didn't get the lock; check if anybody stole it from us. In\n\t * that case, we need to fix up the uval to point to them instead of\n\t * us, otherwise bad things happen. [10]\n\t *\n\t * Another speculative read; pi_state->owner == current is unstable\n\t * but needs our attention.\n\t */\n\tif (q->pi_state->owner == current)\n\t\treturn fixup_pi_state_owner(uaddr, q, NULL);\n\n\t/*\n\t * Paranoia check. If we did not take the lock, then we should not be\n\t * the owner of the rt_mutex.\n\t */\n\tif (rt_mutex_owner(&q->pi_state->pi_mutex) == current) {\n\t\tprintk(KERN_ERR \"fixup_owner: ret = %d pi-mutex: %p \"\n\t\t\t\t\"pi-state %p\\n\", ret,\n\t\t\t\tq->pi_state->pi_mutex.owner,\n\t\t\t\tq->pi_state->owner);\n\t}\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3209,
            "cve_id": "CVE-2021-38204",
            "code_snippet": "static int\nmax3421_urb_done(struct usb_hcd *hcd)\n{\n\tstruct max3421_hcd *max3421_hcd = hcd_to_max3421(hcd);\n\tunsigned long flags;\n\tstruct urb *urb;\n\tint status;\n\n\tstatus = max3421_hcd->urb_done;\n\tmax3421_hcd->urb_done = 0;\n\tif (status > 0)\n\t\tstatus = 0;\n\turb = max3421_hcd->curr_urb;\n\tif (urb) {\n\t\t/* save the old end-points toggles: */\n\t\tu8 hrsl = spi_rd8(hcd, MAX3421_REG_HRSL);\n\t\tint rcvtog = (hrsl >> MAX3421_HRSL_RCVTOGRD_BIT) & 1;\n\t\tint sndtog = (hrsl >> MAX3421_HRSL_SNDTOGRD_BIT) & 1;\n\t\tint epnum = usb_endpoint_num(&urb->ep->desc);\n\n\t\t/* no locking: HCD (i.e., we) own toggles, don't we? */\n\t\tusb_settoggle(urb->dev, epnum, 0, rcvtog);\n\t\tusb_settoggle(urb->dev, epnum, 1, sndtog);\n\n\t\tmax3421_hcd->curr_urb = NULL;\n\t\tspin_lock_irqsave(&max3421_hcd->lock, flags);\n\t\tusb_hcd_unlink_urb_from_ep(hcd, urb);\n\t\tspin_unlock_irqrestore(&max3421_hcd->lock, flags);\n\n\t\t/* must be called without the HCD spinlock: */\n\t\tusb_hcd_giveback_urb(hcd, urb, status);\n\t}\n\treturn 1;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3238,
            "cve_id": "CVE-2021-39800",
            "code_snippet": "struct ion_handle *ion_alloc(struct ion_client *client, size_t len,\n\t\t\t     size_t align, unsigned int heap_id_mask,\n\t\t\t     unsigned int flags)\n{\n\treturn __ion_alloc(client, len, align, heap_id_mask, flags, false);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3240,
            "cve_id": "CVE-2021-39801",
            "code_snippet": "struct ion_handle *ion_alloc(struct ion_client *client, size_t len,\n\t\t\t     size_t align, unsigned int heap_id_mask,\n\t\t\t     unsigned int flags)\n{\n\treturn __ion_alloc(client, len, align, heap_id_mask, flags, false);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3289,
            "cve_id": "CVE-2021-43057",
            "code_snippet": "static int selinux_msg_queue_msgrcv(struct kern_ipc_perm *msq, struct msg_msg *msg,\n\t\t\t\t    struct task_struct *target,\n\t\t\t\t    long type, int mode)\n{\n\tstruct ipc_security_struct *isec;\n\tstruct msg_security_struct *msec;\n\tstruct common_audit_data ad;\n\tu32 sid = task_sid_obj(target);\n\tint rc;\n\n\tisec = selinux_ipc(msq);\n\tmsec = selinux_msg_msg(msg);\n\n\tad.type = LSM_AUDIT_DATA_IPC;\n\tad.u.ipc_id = msq->key;\n\n\trc = avc_has_perm(&selinux_state,\n\t\t\t  sid, isec->sid,\n\t\t\t  SECCLASS_MSGQ, MSGQ__READ, &ad);\n\tif (!rc)\n\t\trc = avc_has_perm(&selinux_state,\n\t\t\t\t  sid, msec->sid,\n\t\t\t\t  SECCLASS_MSG, MSG__RECEIVE, &ad);\n\treturn rc;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3313,
            "cve_id": "CVE-2021-45868",
            "code_snippet": "static loff_t find_tree_dqentry(struct qtree_mem_dqinfo *info,\n\t\t\t\tstruct dquot *dquot, uint blk, int depth)\n{\n\tchar *buf = kmalloc(info->dqi_usable_bs, GFP_NOFS);\n\tloff_t ret = 0;\n\t__le32 *ref = (__le32 *)buf;\n\n\tif (!buf)\n\t\treturn -ENOMEM;\n\tret = read_blk(info, blk, buf);\n\tif (ret < 0) {\n\t\tquota_error(dquot->dq_sb, \"Can't read quota tree block %u\",\n\t\t\t    blk);\n\t\tgoto out_buf;\n\t}\n\tret = 0;\n\tblk = le32_to_cpu(ref[get_index(info, dquot->dq_id, depth)]);\n\tif (!blk)\t/* No reference? */\n\t\tgoto out_buf;\n\tif (blk < QT_TREEOFF || blk >= info->dqi_blocks) {\n\t\tquota_error(dquot->dq_sb, \"Getting block too big (%u >= %u)\",\n\t\t\t    blk, info->dqi_blocks);\n\t\tret = -EUCLEAN;\n\t\tgoto out_buf;\n\t}\n\n\tif (depth < info->dqi_qtree_depth - 1)\n\t\tret = find_tree_dqentry(info, dquot, blk, depth+1);\n\telse\n\t\tret = find_block_dqentry(info, dquot, blk);\nout_buf:\n\tkfree(buf);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3364,
            "cve_id": "CVE-2022-1011",
            "code_snippet": "static int fuse_get_user_pages(struct fuse_args_pages *ap, struct iov_iter *ii,\n\t\t\t       size_t *nbytesp, int write,\n\t\t\t       unsigned int max_pages)\n{\n\tsize_t nbytes = 0;  /* # bytes already packed in req */\n\tssize_t ret = 0;\n\n\t/* Special case for kernel I/O: can copy directly into the buffer */\n\tif (iov_iter_is_kvec(ii)) {\n\t\tunsigned long user_addr = fuse_get_user_addr(ii);\n\t\tsize_t frag_size = fuse_get_frag_size(ii, *nbytesp);\n\n\t\tif (write)\n\t\t\tap->args.in_args[1].value = (void *) user_addr;\n\t\telse\n\t\t\tap->args.out_args[0].value = (void *) user_addr;\n\n\t\tiov_iter_advance(ii, frag_size);\n\t\t*nbytesp = frag_size;\n\t\treturn 0;\n\t}\n\n\twhile (nbytes < *nbytesp && ap->num_pages < max_pages) {\n\t\tunsigned npages;\n\t\tsize_t start;\n\t\tret = iov_iter_get_pages(ii, &ap->pages[ap->num_pages],\n\t\t\t\t\t*nbytesp - nbytes,\n\t\t\t\t\tmax_pages - ap->num_pages,\n\t\t\t\t\t&start);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\n\t\tiov_iter_advance(ii, ret);\n\t\tnbytes += ret;\n\n\t\tret += start;\n\t\tnpages = DIV_ROUND_UP(ret, PAGE_SIZE);\n\n\t\tap->descs[ap->num_pages].offset = start;\n\t\tfuse_page_descs_length_init(ap->descs, ap->num_pages, npages);\n\n\t\tap->num_pages += npages;\n\t\tap->descs[ap->num_pages - 1].length -=\n\t\t\t(PAGE_SIZE - ret) & (PAGE_SIZE - 1);\n\t}\n\n\tap->args.user_pages = true;\n\tif (write)\n\t\tap->args.in_pages = true;\n\telse\n\t\tap->args.out_pages = true;\n\n\t*nbytesp = nbytes;\n\n\treturn ret < 0 ? ret : 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3376,
            "cve_id": "CVE-2022-1048",
            "code_snippet": "void snd_pcm_detach_substream(struct snd_pcm_substream *substream)\n{\n\tstruct snd_pcm_runtime *runtime;\n\n\tif (PCM_RUNTIME_CHECK(substream))\n\t\treturn;\n\truntime = substream->runtime;\n\tif (runtime->private_free != NULL)\n\t\truntime->private_free(runtime);\n\tfree_pages_exact(runtime->status,\n\t\t       PAGE_ALIGN(sizeof(struct snd_pcm_mmap_status)));\n\tfree_pages_exact(runtime->control,\n\t\t       PAGE_ALIGN(sizeof(struct snd_pcm_mmap_control)));\n\tkfree(runtime->hw_constraints.rules);\n\t/* Avoid concurrent access to runtime via PCM timer interface */\n\tif (substream->timer) {\n\t\tspin_lock_irq(&substream->timer->lock);\n\t\tsubstream->runtime = NULL;\n\t\tspin_unlock_irq(&substream->timer->lock);\n\t} else {\n\t\tsubstream->runtime = NULL;\n\t}\n\tmutex_destroy(&runtime->buffer_mutex);\n\tkfree(runtime);\n\tput_pid(substream->pid);\n\tsubstream->pid = NULL;\n\tsubstream->pstr->substream_opened--;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3380,
            "cve_id": "CVE-2022-1055",
            "code_snippet": "static int tc_new_tfilter(struct sk_buff *skb, struct nlmsghdr *n,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tchar name[IFNAMSIZ];\n\tstruct tcmsg *t;\n\tu32 protocol;\n\tu32 prio;\n\tbool prio_allocate;\n\tu32 parent;\n\tu32 chain_index;\n\tstruct Qdisc *q;\n\tstruct tcf_chain_info chain_info;\n\tstruct tcf_chain *chain;\n\tstruct tcf_block *block;\n\tstruct tcf_proto *tp;\n\tunsigned long cl;\n\tvoid *fh;\n\tint err;\n\tint tp_created;\n\tbool rtnl_held = false;\n\tu32 flags;\n\n\tif (!netlink_ns_capable(skb, net->user_ns, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\ttp_created = 0;\n\n\terr = nlmsg_parse_deprecated(n, sizeof(*t), tca, TCA_MAX,\n\t\t\t\t     rtm_tca_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tt = nlmsg_data(n);\n\tprotocol = TC_H_MIN(t->tcm_info);\n\tprio = TC_H_MAJ(t->tcm_info);\n\tprio_allocate = false;\n\tparent = t->tcm_parent;\n\ttp = NULL;\n\tcl = 0;\n\tblock = NULL;\n\tq = NULL;\n\tchain = NULL;\n\tflags = 0;\n\n\tif (prio == 0) {\n\t\t/* If no priority is provided by the user,\n\t\t * we allocate one.\n\t\t */\n\t\tif (n->nlmsg_flags & NLM_F_CREATE) {\n\t\t\tprio = TC_H_MAKE(0x80000000U, 0U);\n\t\t\tprio_allocate = true;\n\t\t} else {\n\t\t\tNL_SET_ERR_MSG(extack, \"Invalid filter command with priority of zero\");\n\t\t\treturn -ENOENT;\n\t\t}\n\t}\n\n\t/* Find head of filter chain. */\n\n\terr = __tcf_qdisc_find(net, &q, &parent, t->tcm_ifindex, false, extack);\n\tif (err)\n\t\treturn err;\n\n\tif (tcf_proto_check_kind(tca[TCA_KIND], name)) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified TC filter name too long\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\t/* Take rtnl mutex if rtnl_held was set to true on previous iteration,\n\t * block is shared (no qdisc found), qdisc is not unlocked, classifier\n\t * type is not specified, classifier is not unlocked.\n\t */\n\tif (rtnl_held ||\n\t    (q && !(q->ops->cl_ops->flags & QDISC_CLASS_OPS_DOIT_UNLOCKED)) ||\n\t    !tcf_proto_is_unlocked(name)) {\n\t\trtnl_held = true;\n\t\trtnl_lock();\n\t}\n\n\terr = __tcf_qdisc_cl_find(q, parent, &cl, t->tcm_ifindex, extack);\n\tif (err)\n\t\tgoto errout;\n\n\tblock = __tcf_block_find(net, q, cl, t->tcm_ifindex, t->tcm_block_index,\n\t\t\t\t extack);\n\tif (IS_ERR(block)) {\n\t\terr = PTR_ERR(block);\n\t\tgoto errout;\n\t}\n\tblock->classid = parent;\n\n\tchain_index = tca[TCA_CHAIN] ? nla_get_u32(tca[TCA_CHAIN]) : 0;\n\tif (chain_index > TC_ACT_EXT_VAL_MASK) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified chain index exceeds upper limit\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\tchain = tcf_chain_get(block, chain_index, true);\n\tif (!chain) {\n\t\tNL_SET_ERR_MSG(extack, \"Cannot create specified filter chain\");\n\t\terr = -ENOMEM;\n\t\tgoto errout;\n\t}\n\n\tmutex_lock(&chain->filter_chain_lock);\n\ttp = tcf_chain_tp_find(chain, &chain_info, protocol,\n\t\t\t       prio, prio_allocate);\n\tif (IS_ERR(tp)) {\n\t\tNL_SET_ERR_MSG(extack, \"Filter with specified priority/protocol not found\");\n\t\terr = PTR_ERR(tp);\n\t\tgoto errout_locked;\n\t}\n\n\tif (tp == NULL) {\n\t\tstruct tcf_proto *tp_new = NULL;\n\n\t\tif (chain->flushing) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto errout_locked;\n\t\t}\n\n\t\t/* Proto-tcf does not exist, create new one */\n\n\t\tif (tca[TCA_KIND] == NULL || !protocol) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Filter kind and protocol must be specified\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout_locked;\n\t\t}\n\n\t\tif (!(n->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Need both RTM_NEWTFILTER and NLM_F_CREATE to create a new filter\");\n\t\t\terr = -ENOENT;\n\t\t\tgoto errout_locked;\n\t\t}\n\n\t\tif (prio_allocate)\n\t\t\tprio = tcf_auto_prio(tcf_chain_tp_prev(chain,\n\t\t\t\t\t\t\t       &chain_info));\n\n\t\tmutex_unlock(&chain->filter_chain_lock);\n\t\ttp_new = tcf_proto_create(name, protocol, prio, chain,\n\t\t\t\t\t  rtnl_held, extack);\n\t\tif (IS_ERR(tp_new)) {\n\t\t\terr = PTR_ERR(tp_new);\n\t\t\tgoto errout_tp;\n\t\t}\n\n\t\ttp_created = 1;\n\t\ttp = tcf_chain_tp_insert_unique(chain, tp_new, protocol, prio,\n\t\t\t\t\t\trtnl_held);\n\t\tif (IS_ERR(tp)) {\n\t\t\terr = PTR_ERR(tp);\n\t\t\tgoto errout_tp;\n\t\t}\n\t} else {\n\t\tmutex_unlock(&chain->filter_chain_lock);\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], tp->ops->kind)) {\n\t\tNL_SET_ERR_MSG(extack, \"Specified filter kind does not match existing one\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\tfh = tp->ops->get(tp, t->tcm_handle);\n\n\tif (!fh) {\n\t\tif (!(n->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Need both RTM_NEWTFILTER and NLM_F_CREATE to create a new filter\");\n\t\t\terr = -ENOENT;\n\t\t\tgoto errout;\n\t\t}\n\t} else if (n->nlmsg_flags & NLM_F_EXCL) {\n\t\ttfilter_put(tp, fh);\n\t\tNL_SET_ERR_MSG(extack, \"Filter already exists\");\n\t\terr = -EEXIST;\n\t\tgoto errout;\n\t}\n\n\tif (chain->tmplt_ops && chain->tmplt_ops != tp->ops) {\n\t\tNL_SET_ERR_MSG(extack, \"Chain template is set to a different filter kind\");\n\t\terr = -EINVAL;\n\t\tgoto errout;\n\t}\n\n\tif (!(n->nlmsg_flags & NLM_F_CREATE))\n\t\tflags |= TCA_ACT_FLAGS_REPLACE;\n\tif (!rtnl_held)\n\t\tflags |= TCA_ACT_FLAGS_NO_RTNL;\n\terr = tp->ops->change(net, skb, tp, cl, t->tcm_handle, tca, &fh,\n\t\t\t      flags, extack);\n\tif (err == 0) {\n\t\ttfilter_notify(net, skb, n, tp, block, q, parent, fh,\n\t\t\t       RTM_NEWTFILTER, false, rtnl_held);\n\t\ttfilter_put(tp, fh);\n\t\t/* q pointer is NULL for shared blocks */\n\t\tif (q)\n\t\t\tq->flags &= ~TCQ_F_CAN_BYPASS;\n\t}\n\nerrout:\n\tif (err && tp_created)\n\t\ttcf_chain_tp_delete_empty(chain, tp, rtnl_held, NULL);\nerrout_tp:\n\tif (chain) {\n\t\tif (tp && !IS_ERR(tp))\n\t\t\ttcf_proto_put(tp, rtnl_held, NULL);\n\t\tif (!tp_created)\n\t\t\ttcf_chain_put(chain);\n\t}\n\ttcf_block_release(q, block, rtnl_held);\n\n\tif (rtnl_held)\n\t\trtnl_unlock();\n\n\tif (err == -EAGAIN) {\n\t\t/* Take rtnl lock in case EAGAIN is caused by concurrent flush\n\t\t * of target chain.\n\t\t */\n\t\trtnl_held = true;\n\t\t/* Replay the request. */\n\t\tgoto replay;\n\t}\n\treturn err;\n\nerrout_locked:\n\tmutex_unlock(&chain->filter_chain_lock);\n\tgoto errout;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3393,
            "cve_id": "CVE-2022-1419",
            "code_snippet": "static int vgem_gem_dumb_create(struct drm_file *file, struct drm_device *dev,\n\t\t\t\tstruct drm_mode_create_dumb *args)\n{\n\tstruct drm_gem_object *gem_object;\n\tu64 pitch, size;\n\n\tpitch = args->width * DIV_ROUND_UP(args->bpp, 8);\n\tsize = args->height * pitch;\n\tif (size == 0)\n\t\treturn -EINVAL;\n\n\tgem_object = vgem_gem_create(dev, file, &args->handle, size);\n\tif (IS_ERR(gem_object))\n\t\treturn PTR_ERR(gem_object);\n\n\targs->size = gem_object->size;\n\targs->pitch = pitch;\n\n\tdrm_gem_object_put_unlocked(gem_object);\n\n\tDRM_DEBUG(\"Created object of size %llu\\n\", args->size);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3400,
            "cve_id": "CVE-2022-1652",
            "code_snippet": "static int do_format(int drive, struct format_descr *tmp_format_req)\n{\n\tint ret;\n\n\tif (lock_fdc(drive))\n\t\treturn -EINTR;\n\n\tset_floppy(drive);\n\tif (!_floppy ||\n\t    _floppy->track > drive_params[current_drive].tracks ||\n\t    tmp_format_req->track >= _floppy->track ||\n\t    tmp_format_req->head >= _floppy->head ||\n\t    (_floppy->sect << 2) % (1 << FD_SIZECODE(_floppy)) ||\n\t    !_floppy->fmt_gap) {\n\t\tprocess_fd_request();\n\t\treturn -EINVAL;\n\t}\n\tformat_req = *tmp_format_req;\n\tcont = &format_cont;\n\tfloppy_errors = 0;\n\tret = wait_til_done(redo_format, true);\n\tif (ret == -EINTR)\n\t\treturn -EINTR;\n\tprocess_fd_request();\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3401,
            "cve_id": "CVE-2022-1652",
            "code_snippet": "static void redo_fd_request(void)\n{\n\tint drive;\n\tint tmp;\n\n\tlastredo = jiffies;\n\tif (current_drive < N_DRIVE)\n\t\tfloppy_off(current_drive);\n\ndo_request:\n\tif (!current_req) {\n\t\tint pending;\n\n\t\tspin_lock_irq(&floppy_lock);\n\t\tpending = set_next_request();\n\t\tspin_unlock_irq(&floppy_lock);\n\t\tif (!pending) {\n\t\t\tdo_floppy = NULL;\n\t\t\tunlock_fdc();\n\t\t\treturn;\n\t\t}\n\t}\n\tdrive = (long)current_req->q->disk->private_data;\n\tset_fdc(drive);\n\treschedule_timeout(current_drive, \"redo fd request\");\n\n\tset_floppy(drive);\n\traw_cmd = &default_raw_cmd;\n\traw_cmd->flags = 0;\n\tif (start_motor(redo_fd_request))\n\t\treturn;\n\n\tdisk_change(current_drive);\n\tif (test_bit(current_drive, &fake_change) ||\n\t    test_bit(FD_DISK_CHANGED_BIT, &drive_state[current_drive].flags)) {\n\t\tDPRINT(\"disk absent or changed during operation\\n\");\n\t\trequest_done(0);\n\t\tgoto do_request;\n\t}\n\tif (!_floppy) {\t/* Autodetection */\n\t\tif (!probing) {\n\t\t\tdrive_state[current_drive].probed_format = 0;\n\t\t\tif (next_valid_format(current_drive)) {\n\t\t\t\tDPRINT(\"no autodetectable formats\\n\");\n\t\t\t\t_floppy = NULL;\n\t\t\t\trequest_done(0);\n\t\t\t\tgoto do_request;\n\t\t\t}\n\t\t}\n\t\tprobing = 1;\n\t\t_floppy = floppy_type + drive_params[current_drive].autodetect[drive_state[current_drive].probed_format];\n\t} else\n\t\tprobing = 0;\n\ttmp = make_raw_rw_request();\n\tif (tmp < 2) {\n\t\trequest_done(tmp);\n\t\tgoto do_request;\n\t}\n\n\tif (test_bit(FD_NEED_TWADDLE_BIT, &drive_state[current_drive].flags))\n\t\ttwaddle(current_fdc, current_drive);\n\tschedule_bh(floppy_start);\n\tdebugt(__func__, \"queue fd request\");\n\treturn;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3412,
            "cve_id": "CVE-2022-1786",
            "code_snippet": "static void io_worker_exit(struct io_worker *worker)\n{\n\tstruct io_wqe *wqe = worker->wqe;\n\tstruct io_wqe_acct *acct = io_wqe_get_acct(worker);\n\n\t/*\n\t * If we're not at zero, someone else is holding a brief reference\n\t * to the worker. Wait for that to go away.\n\t */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tif (!refcount_dec_and_test(&worker->ref))\n\t\tschedule();\n\t__set_current_state(TASK_RUNNING);\n\n\tpreempt_disable();\n\tcurrent->flags &= ~PF_IO_WORKER;\n\tif (worker->flags & IO_WORKER_F_RUNNING)\n\t\tatomic_dec(&acct->nr_running);\n\tif (!(worker->flags & IO_WORKER_F_BOUND))\n\t\tatomic_dec(&wqe->wq->user->processes);\n\tworker->flags = 0;\n\tpreempt_enable();\n\n\tif (worker->saved_creds) {\n\t\trevert_creds(worker->saved_creds);\n\t\tworker->cur_creds = worker->saved_creds = NULL;\n\t}\n\n\traw_spin_lock_irq(&wqe->lock);\n\thlist_nulls_del_rcu(&worker->nulls_node);\n\tlist_del_rcu(&worker->all_list);\n\tacct->nr_workers--;\n\traw_spin_unlock_irq(&wqe->lock);\n\n\tkfree_rcu(worker, rcu);\n\tif (refcount_dec_and_test(&wqe->wq->refs))\n\t\tcomplete(&wqe->wq->done);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3413,
            "cve_id": "CVE-2022-1786",
            "code_snippet": "static void io_worker_handle_work(struct io_worker *worker)\n\t__releases(wqe->lock)\n{\n\tstruct io_wqe *wqe = worker->wqe;\n\tstruct io_wq *wq = wqe->wq;\n\n\tdo {\n\t\tstruct io_wq_work *work;\nget_next:\n\t\t/*\n\t\t * If we got some work, mark us as busy. If we didn't, but\n\t\t * the list isn't empty, it means we stalled on hashed work.\n\t\t * Mark us stalled so we don't keep looking for work when we\n\t\t * can't make progress, any work completion or insertion will\n\t\t * clear the stalled flag.\n\t\t */\n\t\twork = io_get_next_work(wqe);\n\t\tif (work)\n\t\t\t__io_worker_busy(wqe, worker, work);\n\t\telse if (!wq_list_empty(&wqe->work_list))\n\t\t\twqe->flags |= IO_WQE_FLAG_STALLED;\n\n\t\traw_spin_unlock_irq(&wqe->lock);\n\t\tif (!work)\n\t\t\tbreak;\n\t\tio_assign_current_work(worker, work);\n\n\t\t/* handle a whole dependent link */\n\t\tdo {\n\t\t\tstruct io_wq_work *next_hashed, *linked;\n\t\t\tunsigned int hash = io_get_work_hash(work);\n\n\t\t\tnext_hashed = wq_next_work(work);\n\t\t\tif (work->creds && worker->cur_creds != work->creds)\n\t\t\t\tio_wq_switch_creds(worker, work);\n\t\t\twq->do_work(work);\n\t\t\tio_assign_current_work(worker, NULL);\n\n\t\t\tlinked = wq->free_work(work);\n\t\t\twork = next_hashed;\n\t\t\tif (!work && linked && !io_wq_is_hashed(linked)) {\n\t\t\t\twork = linked;\n\t\t\t\tlinked = NULL;\n\t\t\t}\n\t\t\tio_assign_current_work(worker, work);\n\t\t\tif (linked)\n\t\t\t\tio_wqe_enqueue(wqe, linked);\n\n\t\t\tif (hash != -1U && !next_hashed) {\n\t\t\t\traw_spin_lock_irq(&wqe->lock);\n\t\t\t\twqe->hash_map &= ~BIT_ULL(hash);\n\t\t\t\twqe->flags &= ~IO_WQE_FLAG_STALLED;\n\t\t\t\t/* skip unnecessary unlock-lock wqe->lock */\n\t\t\t\tif (!work)\n\t\t\t\t\tgoto get_next;\n\t\t\t\traw_spin_unlock_irq(&wqe->lock);\n\t\t\t}\n\t\t} while (work);\n\n\t\traw_spin_lock_irq(&wqe->lock);\n\t} while (1);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3414,
            "cve_id": "CVE-2022-1786",
            "code_snippet": "static bool io_match_task(struct io_kiocb *head,\n\t\t\t  struct task_struct *task,\n\t\t\t  struct files_struct *files)\n{\n\tstruct io_kiocb *req;\n\n\tif (task && head->task != task) {\n\t\t/* in terms of cancelation, always match if req task is dead */\n\t\tif (head->task->flags & PF_EXITING)\n\t\t\treturn true;\n\t\treturn false;\n\t}\n\tif (!files)\n\t\treturn true;\n\n\tio_for_each_link(req, head) {\n\t\tif (!(req->flags & REQ_F_WORK_INITIALIZED))\n\t\t\tcontinue;\n\t\tif (req->file && req->file->f_op == &io_uring_fops)\n\t\t\treturn true;\n\t\tif (req->task->files == files)\n\t\t\treturn true;\n\t}\n\treturn false;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3415,
            "cve_id": "CVE-2022-1786",
            "code_snippet": "static void io_prep_async_work(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_req_init_async(req);\n\n\tif (req->flags & REQ_F_FORCE_ASYNC)\n\t\treq->work.flags |= IO_WQ_WORK_CONCURRENT;\n\n\tif (req->flags & REQ_F_ISREG) {\n\t\tif (def->hash_reg_file || (ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tio_wq_hash_work(&req->work, file_inode(req->file));\n\t} else {\n\t\tif (def->unbound_nonreg_file)\n\t\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t}\n\tif (!req->work.creds)\n\t\treq->work.creds = get_current_cred();\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3416,
            "cve_id": "CVE-2022-1786",
            "code_snippet": "static int io_uring_show_cred(int id, void *p, void *data)\n{\n\tconst struct cred *cred = p;\n\tstruct seq_file *m = data;\n\tstruct user_namespace *uns = seq_user_ns(m);\n\tstruct group_info *gi;\n\tkernel_cap_t cap;\n\tunsigned __capi;\n\tint g;\n\n\tseq_printf(m, \"%5d\\n\", id);\n\tseq_put_decimal_ull(m, \"\\tUid:\\t\", from_kuid_munged(uns, cred->uid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->euid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->suid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->fsuid));\n\tseq_put_decimal_ull(m, \"\\n\\tGid:\\t\", from_kgid_munged(uns, cred->gid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->egid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->sgid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->fsgid));\n\tseq_puts(m, \"\\n\\tGroups:\\t\");\n\tgi = cred->group_info;\n\tfor (g = 0; g < gi->ngroups; g++) {\n\t\tseq_put_decimal_ull(m, g ? \" \" : \"\",\n\t\t\t\t\tfrom_kgid_munged(uns, gi->gid[g]));\n\t}\n\tseq_puts(m, \"\\n\\tCapEff:\\t\");\n\tcap = cred->cap_effective;\n\tCAP_FOR_EACH_U32(__capi)\n\t\tseq_put_hex_ll(m, NULL, cap.cap[CAP_LAST_U32 - __capi], 8);\n\tseq_putc(m, '\\n');\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3417,
            "cve_id": "CVE-2022-1786",
            "code_snippet": "static inline void io_req_init_async(struct io_kiocb *req)\n{\n\tif (req->flags & REQ_F_WORK_INITIALIZED)\n\t\treturn;\n\n\t__io_req_init_async(req);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3430,
            "cve_id": "CVE-2022-1882",
            "code_snippet": "static bool post_one_notification(struct watch_queue *wqueue,\n\t\t\t\t  struct watch_notification *n)\n{\n\tvoid *p;\n\tstruct pipe_inode_info *pipe = wqueue->pipe;\n\tstruct pipe_buffer *buf;\n\tstruct page *page;\n\tunsigned int head, tail, mask, note, offset, len;\n\tbool done = false;\n\n\tif (!pipe)\n\t\treturn false;\n\n\tspin_lock_irq(&pipe->rd_wait.lock);\n\n\tmask = pipe->ring_size - 1;\n\thead = pipe->head;\n\ttail = pipe->tail;\n\tif (pipe_full(head, tail, pipe->ring_size))\n\t\tgoto lost;\n\n\tnote = find_first_bit(wqueue->notes_bitmap, wqueue->nr_notes);\n\tif (note >= wqueue->nr_notes)\n\t\tgoto lost;\n\n\tpage = wqueue->notes[note / WATCH_QUEUE_NOTES_PER_PAGE];\n\toffset = note % WATCH_QUEUE_NOTES_PER_PAGE * WATCH_QUEUE_NOTE_SIZE;\n\tget_page(page);\n\tlen = n->info & WATCH_INFO_LENGTH;\n\tp = kmap_atomic(page);\n\tmemcpy(p + offset, n, len);\n\tkunmap_atomic(p);\n\n\tbuf = &pipe->bufs[head & mask];\n\tbuf->page = page;\n\tbuf->private = (unsigned long)wqueue;\n\tbuf->ops = &watch_queue_pipe_buf_ops;\n\tbuf->offset = offset;\n\tbuf->len = len;\n\tbuf->flags = PIPE_BUF_FLAG_WHOLE;\n\tsmp_store_release(&pipe->head, head + 1); /* vs pipe_read() */\n\n\tif (!test_and_clear_bit(note, wqueue->notes_bitmap)) {\n\t\tspin_unlock_irq(&pipe->rd_wait.lock);\n\t\tBUG();\n\t}\n\twake_up_interruptible_sync_poll_locked(&pipe->rd_wait, EPOLLIN | EPOLLRDNORM);\n\tdone = true;\n\nout:\n\tspin_unlock_irq(&pipe->rd_wait.lock);\n\tif (done)\n\t\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n\treturn done;\n\nlost:\n\tbuf = &pipe->bufs[(head - 1) & mask];\n\tbuf->flags |= PIPE_BUF_FLAG_LOSS;\n\tgoto out;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3439,
            "cve_id": "CVE-2022-1973",
            "code_snippet": "int log_replay(struct ntfs_inode *ni, bool *initialized)\n{\n\tint err;\n\tstruct ntfs_sb_info *sbi = ni->mi.sbi;\n\tstruct ntfs_log *log;\n\n\tstruct restart_info rst_info, rst_info2;\n\tu64 rec_lsn, ra_lsn, checkpt_lsn = 0, rlsn = 0;\n\tstruct ATTR_NAME_ENTRY *attr_names = NULL;\n\tstruct ATTR_NAME_ENTRY *ane;\n\tstruct RESTART_TABLE *dptbl = NULL;\n\tstruct RESTART_TABLE *trtbl = NULL;\n\tconst struct RESTART_TABLE *rt;\n\tstruct RESTART_TABLE *oatbl = NULL;\n\tstruct inode *inode;\n\tstruct OpenAttr *oa;\n\tstruct ntfs_inode *ni_oe;\n\tstruct ATTRIB *attr = NULL;\n\tu64 size, vcn, undo_next_lsn;\n\tCLST rno, lcn, lcn0, len0, clen;\n\tvoid *data;\n\tstruct NTFS_RESTART *rst = NULL;\n\tstruct lcb *lcb = NULL;\n\tstruct OPEN_ATTR_ENRTY *oe;\n\tstruct TRANSACTION_ENTRY *tr;\n\tstruct DIR_PAGE_ENTRY *dp;\n\tu32 i, bytes_per_attr_entry;\n\tu32 l_size = ni->vfs_inode.i_size;\n\tu32 orig_file_size = l_size;\n\tu32 page_size, vbo, tail, off, dlen;\n\tu32 saved_len, rec_len, transact_id;\n\tbool use_second_page;\n\tstruct RESTART_AREA *ra2, *ra = NULL;\n\tstruct CLIENT_REC *ca, *cr;\n\t__le16 client;\n\tstruct RESTART_HDR *rh;\n\tconst struct LFS_RECORD_HDR *frh;\n\tconst struct LOG_REC_HDR *lrh;\n\tbool is_mapped;\n\tbool is_ro = sb_rdonly(sbi->sb);\n\tu64 t64;\n\tu16 t16;\n\tu32 t32;\n\n\t/* Get the size of page. NOTE: To replay we can use default page. */\n#if PAGE_SIZE >= DefaultLogPageSize && PAGE_SIZE <= DefaultLogPageSize * 2\n\tpage_size = norm_file_page(PAGE_SIZE, &l_size, true);\n#else\n\tpage_size = norm_file_page(PAGE_SIZE, &l_size, false);\n#endif\n\tif (!page_size)\n\t\treturn -EINVAL;\n\n\tlog = kzalloc(sizeof(struct ntfs_log), GFP_NOFS);\n\tif (!log)\n\t\treturn -ENOMEM;\n\n\tmemset(&rst_info, 0, sizeof(struct restart_info));\n\n\tlog->ni = ni;\n\tlog->l_size = l_size;\n\tlog->one_page_buf = kmalloc(page_size, GFP_NOFS);\n\tif (!log->one_page_buf) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tlog->page_size = page_size;\n\tlog->page_mask = page_size - 1;\n\tlog->page_bits = blksize_bits(page_size);\n\n\t/* Look for a restart area on the disk. */\n\terr = log_read_rst(log, l_size, true, &rst_info);\n\tif (err)\n\t\tgoto out;\n\n\t/* remember 'initialized' */\n\t*initialized = rst_info.initialized;\n\n\tif (!rst_info.restart) {\n\t\tif (rst_info.initialized) {\n\t\t\t/* No restart area but the file is not initialized. */\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tlog_init_pg_hdr(log, page_size, page_size, 1, 1);\n\t\tlog_create(log, l_size, 0, get_random_int(), false, false);\n\n\t\tlog->ra = ra;\n\n\t\tra = log_create_ra(log);\n\t\tif (!ra) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tlog->ra = ra;\n\t\tlog->init_ra = true;\n\n\t\tgoto process_log;\n\t}\n\n\t/*\n\t * If the restart offset above wasn't zero then we won't\n\t * look for a second restart.\n\t */\n\tif (rst_info.vbo)\n\t\tgoto check_restart_area;\n\n\tmemset(&rst_info2, 0, sizeof(struct restart_info));\n\terr = log_read_rst(log, l_size, false, &rst_info2);\n\n\t/* Determine which restart area to use. */\n\tif (!rst_info2.restart || rst_info2.last_lsn <= rst_info.last_lsn)\n\t\tgoto use_first_page;\n\n\tuse_second_page = true;\n\n\tif (rst_info.chkdsk_was_run && page_size != rst_info.vbo) {\n\t\tstruct RECORD_PAGE_HDR *sp = NULL;\n\t\tbool usa_error;\n\n\t\tif (!read_log_page(log, page_size, &sp, &usa_error) &&\n\t\t    sp->rhdr.sign == NTFS_CHKD_SIGNATURE) {\n\t\t\tuse_second_page = false;\n\t\t}\n\t\tkfree(sp);\n\t}\n\n\tif (use_second_page) {\n\t\tkfree(rst_info.r_page);\n\t\tmemcpy(&rst_info, &rst_info2, sizeof(struct restart_info));\n\t\trst_info2.r_page = NULL;\n\t}\n\nuse_first_page:\n\tkfree(rst_info2.r_page);\n\ncheck_restart_area:\n\t/*\n\t * If the restart area is at offset 0, we want\n\t * to write the second restart area first.\n\t */\n\tlog->init_ra = !!rst_info.vbo;\n\n\t/* If we have a valid page then grab a pointer to the restart area. */\n\tra2 = rst_info.valid_page\n\t\t      ? Add2Ptr(rst_info.r_page,\n\t\t\t\tle16_to_cpu(rst_info.r_page->ra_off))\n\t\t      : NULL;\n\n\tif (rst_info.chkdsk_was_run ||\n\t    (ra2 && ra2->client_idx[1] == LFS_NO_CLIENT_LE)) {\n\t\tbool wrapped = false;\n\t\tbool use_multi_page = false;\n\t\tu32 open_log_count;\n\n\t\t/* Do some checks based on whether we have a valid log page. */\n\t\tif (!rst_info.valid_page) {\n\t\t\topen_log_count = get_random_int();\n\t\t\tgoto init_log_instance;\n\t\t}\n\t\topen_log_count = le32_to_cpu(ra2->open_log_count);\n\n\t\t/*\n\t\t * If the restart page size isn't changing then we want to\n\t\t * check how much work we need to do.\n\t\t */\n\t\tif (page_size != le32_to_cpu(rst_info.r_page->sys_page_size))\n\t\t\tgoto init_log_instance;\n\ninit_log_instance:\n\t\tlog_init_pg_hdr(log, page_size, page_size, 1, 1);\n\n\t\tlog_create(log, l_size, rst_info.last_lsn, open_log_count,\n\t\t\t   wrapped, use_multi_page);\n\n\t\tra = log_create_ra(log);\n\t\tif (!ra) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tlog->ra = ra;\n\n\t\t/* Put the restart areas and initialize\n\t\t * the log file as required.\n\t\t */\n\t\tgoto process_log;\n\t}\n\n\tif (!ra2) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * If the log page or the system page sizes have changed, we can't\n\t * use the log file. We must use the system page size instead of the\n\t * default size if there is not a clean shutdown.\n\t */\n\tt32 = le32_to_cpu(rst_info.r_page->sys_page_size);\n\tif (page_size != t32) {\n\t\tl_size = orig_file_size;\n\t\tpage_size =\n\t\t\tnorm_file_page(t32, &l_size, t32 == DefaultLogPageSize);\n\t}\n\n\tif (page_size != t32 ||\n\t    page_size != le32_to_cpu(rst_info.r_page->page_size)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* If the file size has shrunk then we won't mount it. */\n\tif (l_size < le64_to_cpu(ra2->l_size)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tlog_init_pg_hdr(log, page_size, page_size,\n\t\t\tle16_to_cpu(rst_info.r_page->major_ver),\n\t\t\tle16_to_cpu(rst_info.r_page->minor_ver));\n\n\tlog->l_size = le64_to_cpu(ra2->l_size);\n\tlog->seq_num_bits = le32_to_cpu(ra2->seq_num_bits);\n\tlog->file_data_bits = sizeof(u64) * 8 - log->seq_num_bits;\n\tlog->seq_num_mask = (8 << log->file_data_bits) - 1;\n\tlog->last_lsn = le64_to_cpu(ra2->current_lsn);\n\tlog->seq_num = log->last_lsn >> log->file_data_bits;\n\tlog->ra_off = le16_to_cpu(rst_info.r_page->ra_off);\n\tlog->restart_size = log->sys_page_size - log->ra_off;\n\tlog->record_header_len = le16_to_cpu(ra2->rec_hdr_len);\n\tlog->ra_size = le16_to_cpu(ra2->ra_len);\n\tlog->data_off = le16_to_cpu(ra2->data_off);\n\tlog->data_size = log->page_size - log->data_off;\n\tlog->reserved = log->data_size - log->record_header_len;\n\n\tvbo = lsn_to_vbo(log, log->last_lsn);\n\n\tif (vbo < log->first_page) {\n\t\t/* This is a pseudo lsn. */\n\t\tlog->l_flags |= NTFSLOG_NO_LAST_LSN;\n\t\tlog->next_page = log->first_page;\n\t\tgoto find_oldest;\n\t}\n\n\t/* Find the end of this log record. */\n\toff = final_log_off(log, log->last_lsn,\n\t\t\t    le32_to_cpu(ra2->last_lsn_data_len));\n\n\t/* If we wrapped the file then increment the sequence number. */\n\tif (off <= vbo) {\n\t\tlog->seq_num += 1;\n\t\tlog->l_flags |= NTFSLOG_WRAPPED;\n\t}\n\n\t/* Now compute the next log page to use. */\n\tvbo &= ~log->sys_page_mask;\n\ttail = log->page_size - (off & log->page_mask) - 1;\n\n\t/*\n\t *If we can fit another log record on the page,\n\t * move back a page the log file.\n\t */\n\tif (tail >= log->record_header_len) {\n\t\tlog->l_flags |= NTFSLOG_REUSE_TAIL;\n\t\tlog->next_page = vbo;\n\t} else {\n\t\tlog->next_page = next_page_off(log, vbo);\n\t}\n\nfind_oldest:\n\t/*\n\t * Find the oldest client lsn. Use the last\n\t * flushed lsn as a starting point.\n\t */\n\tlog->oldest_lsn = log->last_lsn;\n\toldest_client_lsn(Add2Ptr(ra2, le16_to_cpu(ra2->client_off)),\n\t\t\t  ra2->client_idx[1], &log->oldest_lsn);\n\tlog->oldest_lsn_off = lsn_to_vbo(log, log->oldest_lsn);\n\n\tif (log->oldest_lsn_off < log->first_page)\n\t\tlog->l_flags |= NTFSLOG_NO_OLDEST_LSN;\n\n\tif (!(ra2->flags & RESTART_SINGLE_PAGE_IO))\n\t\tlog->l_flags |= NTFSLOG_WRAPPED | NTFSLOG_MULTIPLE_PAGE_IO;\n\n\tlog->current_openlog_count = le32_to_cpu(ra2->open_log_count);\n\tlog->total_avail_pages = log->l_size - log->first_page;\n\tlog->total_avail = log->total_avail_pages >> log->page_bits;\n\tlog->max_current_avail = log->total_avail * log->reserved;\n\tlog->total_avail = log->total_avail * log->data_size;\n\n\tlog->current_avail = current_log_avail(log);\n\n\tra = kzalloc(log->restart_size, GFP_NOFS);\n\tif (!ra) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\tlog->ra = ra;\n\n\tt16 = le16_to_cpu(ra2->client_off);\n\tif (t16 == offsetof(struct RESTART_AREA, clients)) {\n\t\tmemcpy(ra, ra2, log->ra_size);\n\t} else {\n\t\tmemcpy(ra, ra2, offsetof(struct RESTART_AREA, clients));\n\t\tmemcpy(ra->clients, Add2Ptr(ra2, t16),\n\t\t       le16_to_cpu(ra2->ra_len) - t16);\n\n\t\tlog->current_openlog_count = get_random_int();\n\t\tra->open_log_count = cpu_to_le32(log->current_openlog_count);\n\t\tlog->ra_size = offsetof(struct RESTART_AREA, clients) +\n\t\t\t       sizeof(struct CLIENT_REC);\n\t\tra->client_off =\n\t\t\tcpu_to_le16(offsetof(struct RESTART_AREA, clients));\n\t\tra->ra_len = cpu_to_le16(log->ra_size);\n\t}\n\n\tle32_add_cpu(&ra->open_log_count, 1);\n\n\t/* Now we need to walk through looking for the last lsn. */\n\terr = last_log_lsn(log);\n\tif (err)\n\t\tgoto out;\n\n\tlog->current_avail = current_log_avail(log);\n\n\t/* Remember which restart area to write first. */\n\tlog->init_ra = rst_info.vbo;\n\nprocess_log:\n\t/* 1.0, 1.1, 2.0 log->major_ver/minor_ver - short values. */\n\tswitch ((log->major_ver << 16) + log->minor_ver) {\n\tcase 0x10000:\n\tcase 0x10001:\n\tcase 0x20000:\n\t\tbreak;\n\tdefault:\n\t\tntfs_warn(sbi->sb, \"\\x24LogFile version %d.%d is not supported\",\n\t\t\t  log->major_ver, log->minor_ver);\n\t\terr = -EOPNOTSUPP;\n\t\tlog->set_dirty = true;\n\t\tgoto out;\n\t}\n\n\t/* One client \"NTFS\" per logfile. */\n\tca = Add2Ptr(ra, le16_to_cpu(ra->client_off));\n\n\tfor (client = ra->client_idx[1];; client = cr->next_client) {\n\t\tif (client == LFS_NO_CLIENT_LE) {\n\t\t\t/* Insert \"NTFS\" client LogFile. */\n\t\t\tclient = ra->client_idx[0];\n\t\t\tif (client == LFS_NO_CLIENT_LE) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tt16 = le16_to_cpu(client);\n\t\t\tcr = ca + t16;\n\n\t\t\tremove_client(ca, cr, &ra->client_idx[0]);\n\n\t\t\tcr->restart_lsn = 0;\n\t\t\tcr->oldest_lsn = cpu_to_le64(log->oldest_lsn);\n\t\t\tcr->name_bytes = cpu_to_le32(8);\n\t\t\tcr->name[0] = cpu_to_le16('N');\n\t\t\tcr->name[1] = cpu_to_le16('T');\n\t\t\tcr->name[2] = cpu_to_le16('F');\n\t\t\tcr->name[3] = cpu_to_le16('S');\n\n\t\t\tadd_client(ca, t16, &ra->client_idx[1]);\n\t\t\tbreak;\n\t\t}\n\n\t\tcr = ca + le16_to_cpu(client);\n\n\t\tif (cpu_to_le32(8) == cr->name_bytes &&\n\t\t    cpu_to_le16('N') == cr->name[0] &&\n\t\t    cpu_to_le16('T') == cr->name[1] &&\n\t\t    cpu_to_le16('F') == cr->name[2] &&\n\t\t    cpu_to_le16('S') == cr->name[3])\n\t\t\tbreak;\n\t}\n\n\t/* Update the client handle with the client block information. */\n\tlog->client_id.seq_num = cr->seq_num;\n\tlog->client_id.client_idx = client;\n\n\terr = read_rst_area(log, &rst, &ra_lsn);\n\tif (err)\n\t\tgoto out;\n\n\tif (!rst)\n\t\tgoto out;\n\n\tbytes_per_attr_entry = !rst->major_ver ? 0x2C : 0x28;\n\n\tcheckpt_lsn = le64_to_cpu(rst->check_point_start);\n\tif (!checkpt_lsn)\n\t\tcheckpt_lsn = ra_lsn;\n\n\t/* Allocate and Read the Transaction Table. */\n\tif (!rst->transact_table_len)\n\t\tgoto check_dirty_page_table;\n\n\tt64 = le64_to_cpu(rst->transact_table_lsn);\n\terr = read_log_rec_lcb(log, t64, lcb_ctx_prev, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\tlrh = lcb->log_rec;\n\tfrh = lcb->lrh;\n\trec_len = le32_to_cpu(frh->client_data_len);\n\n\tif (!check_log_rec(lrh, rec_len, le32_to_cpu(frh->transact_id),\n\t\t\t   bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tt16 = le16_to_cpu(lrh->redo_off);\n\n\trt = Add2Ptr(lrh, t16);\n\tt32 = rec_len - t16;\n\n\t/* Now check that this is a valid restart table. */\n\tif (!check_rstbl(rt, t32)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\ttrtbl = kmemdup(rt, t32, GFP_NOFS);\n\tif (!trtbl) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tlcb_put(lcb);\n\tlcb = NULL;\n\ncheck_dirty_page_table:\n\t/* The next record back should be the Dirty Pages Table. */\n\tif (!rst->dirty_pages_len)\n\t\tgoto check_attribute_names;\n\n\tt64 = le64_to_cpu(rst->dirty_pages_table_lsn);\n\terr = read_log_rec_lcb(log, t64, lcb_ctx_prev, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\tlrh = lcb->log_rec;\n\tfrh = lcb->lrh;\n\trec_len = le32_to_cpu(frh->client_data_len);\n\n\tif (!check_log_rec(lrh, rec_len, le32_to_cpu(frh->transact_id),\n\t\t\t   bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tt16 = le16_to_cpu(lrh->redo_off);\n\n\trt = Add2Ptr(lrh, t16);\n\tt32 = rec_len - t16;\n\n\t/* Now check that this is a valid restart table. */\n\tif (!check_rstbl(rt, t32)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tdptbl = kmemdup(rt, t32, GFP_NOFS);\n\tif (!dptbl) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t/* Convert Ra version '0' into version '1'. */\n\tif (rst->major_ver)\n\t\tgoto end_conv_1;\n\n\tdp = NULL;\n\twhile ((dp = enum_rstbl(dptbl, dp))) {\n\t\tstruct DIR_PAGE_ENTRY_32 *dp0 = (struct DIR_PAGE_ENTRY_32 *)dp;\n\t\t// NOTE: Danger. Check for of boundary.\n\t\tmemmove(&dp->vcn, &dp0->vcn_low,\n\t\t\t2 * sizeof(u64) +\n\t\t\t\tle32_to_cpu(dp->lcns_follow) * sizeof(u64));\n\t}\n\nend_conv_1:\n\tlcb_put(lcb);\n\tlcb = NULL;\n\n\t/*\n\t * Go through the table and remove the duplicates,\n\t * remembering the oldest lsn values.\n\t */\n\tif (sbi->cluster_size <= log->page_size)\n\t\tgoto trace_dp_table;\n\n\tdp = NULL;\n\twhile ((dp = enum_rstbl(dptbl, dp))) {\n\t\tstruct DIR_PAGE_ENTRY *next = dp;\n\n\t\twhile ((next = enum_rstbl(dptbl, next))) {\n\t\t\tif (next->target_attr == dp->target_attr &&\n\t\t\t    next->vcn == dp->vcn) {\n\t\t\t\tif (le64_to_cpu(next->oldest_lsn) <\n\t\t\t\t    le64_to_cpu(dp->oldest_lsn)) {\n\t\t\t\t\tdp->oldest_lsn = next->oldest_lsn;\n\t\t\t\t}\n\n\t\t\t\tfree_rsttbl_idx(dptbl, PtrOffset(dptbl, next));\n\t\t\t}\n\t\t}\n\t}\ntrace_dp_table:\ncheck_attribute_names:\n\t/* The next record should be the Attribute Names. */\n\tif (!rst->attr_names_len)\n\t\tgoto check_attr_table;\n\n\tt64 = le64_to_cpu(rst->attr_names_lsn);\n\terr = read_log_rec_lcb(log, t64, lcb_ctx_prev, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\tlrh = lcb->log_rec;\n\tfrh = lcb->lrh;\n\trec_len = le32_to_cpu(frh->client_data_len);\n\n\tif (!check_log_rec(lrh, rec_len, le32_to_cpu(frh->transact_id),\n\t\t\t   bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tt32 = lrh_length(lrh);\n\trec_len -= t32;\n\n\tattr_names = kmemdup(Add2Ptr(lrh, t32), rec_len, GFP_NOFS);\n\n\tlcb_put(lcb);\n\tlcb = NULL;\n\ncheck_attr_table:\n\t/* The next record should be the attribute Table. */\n\tif (!rst->open_attr_len)\n\t\tgoto check_attribute_names2;\n\n\tt64 = le64_to_cpu(rst->open_attr_table_lsn);\n\terr = read_log_rec_lcb(log, t64, lcb_ctx_prev, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\tlrh = lcb->log_rec;\n\tfrh = lcb->lrh;\n\trec_len = le32_to_cpu(frh->client_data_len);\n\n\tif (!check_log_rec(lrh, rec_len, le32_to_cpu(frh->transact_id),\n\t\t\t   bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tt16 = le16_to_cpu(lrh->redo_off);\n\n\trt = Add2Ptr(lrh, t16);\n\tt32 = rec_len - t16;\n\n\tif (!check_rstbl(rt, t32)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\toatbl = kmemdup(rt, t32, GFP_NOFS);\n\tif (!oatbl) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tlog->open_attr_tbl = oatbl;\n\n\t/* Clear all of the Attr pointers. */\n\toe = NULL;\n\twhile ((oe = enum_rstbl(oatbl, oe))) {\n\t\tif (!rst->major_ver) {\n\t\t\tstruct OPEN_ATTR_ENRTY_32 oe0;\n\n\t\t\t/* Really 'oe' points to OPEN_ATTR_ENRTY_32. */\n\t\t\tmemcpy(&oe0, oe, SIZEOF_OPENATTRIBUTEENTRY0);\n\n\t\t\toe->bytes_per_index = oe0.bytes_per_index;\n\t\t\toe->type = oe0.type;\n\t\t\toe->is_dirty_pages = oe0.is_dirty_pages;\n\t\t\toe->name_len = 0;\n\t\t\toe->ref = oe0.ref;\n\t\t\toe->open_record_lsn = oe0.open_record_lsn;\n\t\t}\n\n\t\toe->is_attr_name = 0;\n\t\toe->ptr = NULL;\n\t}\n\n\tlcb_put(lcb);\n\tlcb = NULL;\n\ncheck_attribute_names2:\n\tif (!rst->attr_names_len)\n\t\tgoto trace_attribute_table;\n\n\tane = attr_names;\n\tif (!oatbl)\n\t\tgoto trace_attribute_table;\n\twhile (ane->off) {\n\t\t/* TODO: Clear table on exit! */\n\t\toe = Add2Ptr(oatbl, le16_to_cpu(ane->off));\n\t\tt16 = le16_to_cpu(ane->name_bytes);\n\t\toe->name_len = t16 / sizeof(short);\n\t\toe->ptr = ane->name;\n\t\toe->is_attr_name = 2;\n\t\tane = Add2Ptr(ane, sizeof(struct ATTR_NAME_ENTRY) + t16);\n\t}\n\ntrace_attribute_table:\n\t/*\n\t * If the checkpt_lsn is zero, then this is a freshly\n\t * formatted disk and we have no work to do.\n\t */\n\tif (!checkpt_lsn) {\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\tif (!oatbl) {\n\t\toatbl = init_rsttbl(bytes_per_attr_entry, 8);\n\t\tif (!oatbl) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tlog->open_attr_tbl = oatbl;\n\n\t/* Start the analysis pass from the Checkpoint lsn. */\n\trec_lsn = checkpt_lsn;\n\n\t/* Read the first lsn. */\n\terr = read_log_rec_lcb(log, checkpt_lsn, lcb_ctx_next, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\t/* Loop to read all subsequent records to the end of the log file. */\nnext_log_record_analyze:\n\terr = read_next_log_rec(log, lcb, &rec_lsn);\n\tif (err)\n\t\tgoto out;\n\n\tif (!rec_lsn)\n\t\tgoto end_log_records_enumerate;\n\n\tfrh = lcb->lrh;\n\ttransact_id = le32_to_cpu(frh->transact_id);\n\trec_len = le32_to_cpu(frh->client_data_len);\n\tlrh = lcb->log_rec;\n\n\tif (!check_log_rec(lrh, rec_len, transact_id, bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * The first lsn after the previous lsn remembered\n\t * the checkpoint is the first candidate for the rlsn.\n\t */\n\tif (!rlsn)\n\t\trlsn = rec_lsn;\n\n\tif (LfsClientRecord != frh->record_type)\n\t\tgoto next_log_record_analyze;\n\n\t/*\n\t * Now update the Transaction Table for this transaction. If there\n\t * is no entry present or it is unallocated we allocate the entry.\n\t */\n\tif (!trtbl) {\n\t\ttrtbl = init_rsttbl(sizeof(struct TRANSACTION_ENTRY),\n\t\t\t\t    INITIAL_NUMBER_TRANSACTIONS);\n\t\tif (!trtbl) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\ttr = Add2Ptr(trtbl, transact_id);\n\n\tif (transact_id >= bytes_per_rt(trtbl) ||\n\t    tr->next != RESTART_ENTRY_ALLOCATED_LE) {\n\t\ttr = alloc_rsttbl_from_idx(&trtbl, transact_id);\n\t\tif (!tr) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\ttr->transact_state = TransactionActive;\n\t\ttr->first_lsn = cpu_to_le64(rec_lsn);\n\t}\n\n\ttr->prev_lsn = tr->undo_next_lsn = cpu_to_le64(rec_lsn);\n\n\t/*\n\t * If this is a compensation log record, then change\n\t * the undo_next_lsn to be the undo_next_lsn of this record.\n\t */\n\tif (lrh->undo_op == cpu_to_le16(CompensationLogRecord))\n\t\ttr->undo_next_lsn = frh->client_undo_next_lsn;\n\n\t/* Dispatch to handle log record depending on type. */\n\tswitch (le16_to_cpu(lrh->redo_op)) {\n\tcase InitializeFileRecordSegment:\n\tcase DeallocateFileRecordSegment:\n\tcase WriteEndOfFileRecordSegment:\n\tcase CreateAttribute:\n\tcase DeleteAttribute:\n\tcase UpdateResidentValue:\n\tcase UpdateNonresidentValue:\n\tcase UpdateMappingPairs:\n\tcase SetNewAttributeSizes:\n\tcase AddIndexEntryRoot:\n\tcase DeleteIndexEntryRoot:\n\tcase AddIndexEntryAllocation:\n\tcase DeleteIndexEntryAllocation:\n\tcase WriteEndOfIndexBuffer:\n\tcase SetIndexEntryVcnRoot:\n\tcase SetIndexEntryVcnAllocation:\n\tcase UpdateFileNameRoot:\n\tcase UpdateFileNameAllocation:\n\tcase SetBitsInNonresidentBitMap:\n\tcase ClearBitsInNonresidentBitMap:\n\tcase UpdateRecordDataRoot:\n\tcase UpdateRecordDataAllocation:\n\tcase ZeroEndOfFileRecord:\n\t\tt16 = le16_to_cpu(lrh->target_attr);\n\t\tt64 = le64_to_cpu(lrh->target_vcn);\n\t\tdp = find_dp(dptbl, t16, t64);\n\n\t\tif (dp)\n\t\t\tgoto copy_lcns;\n\n\t\t/*\n\t\t * Calculate the number of clusters per page the system\n\t\t * which wrote the checkpoint, possibly creating the table.\n\t\t */\n\t\tif (dptbl) {\n\t\t\tt32 = (le16_to_cpu(dptbl->size) -\n\t\t\t       sizeof(struct DIR_PAGE_ENTRY)) /\n\t\t\t      sizeof(u64);\n\t\t} else {\n\t\t\tt32 = log->clst_per_page;\n\t\t\tkfree(dptbl);\n\t\t\tdptbl = init_rsttbl(struct_size(dp, page_lcns, t32),\n\t\t\t\t\t    32);\n\t\t\tif (!dptbl) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tdp = alloc_rsttbl_idx(&dptbl);\n\t\tif (!dp) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tdp->target_attr = cpu_to_le32(t16);\n\t\tdp->transfer_len = cpu_to_le32(t32 << sbi->cluster_bits);\n\t\tdp->lcns_follow = cpu_to_le32(t32);\n\t\tdp->vcn = cpu_to_le64(t64 & ~((u64)t32 - 1));\n\t\tdp->oldest_lsn = cpu_to_le64(rec_lsn);\n\ncopy_lcns:\n\t\t/*\n\t\t * Copy the Lcns from the log record into the Dirty Page Entry.\n\t\t * TODO: For different page size support, must somehow make\n\t\t * whole routine a loop, case Lcns do not fit below.\n\t\t */\n\t\tt16 = le16_to_cpu(lrh->lcns_follow);\n\t\tfor (i = 0; i < t16; i++) {\n\t\t\tsize_t j = (size_t)(le64_to_cpu(lrh->target_vcn) -\n\t\t\t\t\t    le64_to_cpu(dp->vcn));\n\t\t\tdp->page_lcns[j + i] = lrh->page_lcns[i];\n\t\t}\n\n\t\tgoto next_log_record_analyze;\n\n\tcase DeleteDirtyClusters: {\n\t\tu32 range_count =\n\t\t\tle16_to_cpu(lrh->redo_len) / sizeof(struct LCN_RANGE);\n\t\tconst struct LCN_RANGE *r =\n\t\t\tAdd2Ptr(lrh, le16_to_cpu(lrh->redo_off));\n\n\t\t/* Loop through all of the Lcn ranges this log record. */\n\t\tfor (i = 0; i < range_count; i++, r++) {\n\t\t\tu64 lcn0 = le64_to_cpu(r->lcn);\n\t\t\tu64 lcn_e = lcn0 + le64_to_cpu(r->len) - 1;\n\n\t\t\tdp = NULL;\n\t\t\twhile ((dp = enum_rstbl(dptbl, dp))) {\n\t\t\t\tu32 j;\n\n\t\t\t\tt32 = le32_to_cpu(dp->lcns_follow);\n\t\t\t\tfor (j = 0; j < t32; j++) {\n\t\t\t\t\tt64 = le64_to_cpu(dp->page_lcns[j]);\n\t\t\t\t\tif (t64 >= lcn0 && t64 <= lcn_e)\n\t\t\t\t\t\tdp->page_lcns[j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tgoto next_log_record_analyze;\n\t\t;\n\t}\n\n\tcase OpenNonresidentAttribute:\n\t\tt16 = le16_to_cpu(lrh->target_attr);\n\t\tif (t16 >= bytes_per_rt(oatbl)) {\n\t\t\t/*\n\t\t\t * Compute how big the table needs to be.\n\t\t\t * Add 10 extra entries for some cushion.\n\t\t\t */\n\t\t\tu32 new_e = t16 / le16_to_cpu(oatbl->size);\n\n\t\t\tnew_e += 10 - le16_to_cpu(oatbl->used);\n\n\t\t\toatbl = extend_rsttbl(oatbl, new_e, ~0u);\n\t\t\tlog->open_attr_tbl = oatbl;\n\t\t\tif (!oatbl) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\t/* Point to the entry being opened. */\n\t\toe = alloc_rsttbl_from_idx(&oatbl, t16);\n\t\tlog->open_attr_tbl = oatbl;\n\t\tif (!oe) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/* Initialize this entry from the log record. */\n\t\tt16 = le16_to_cpu(lrh->redo_off);\n\t\tif (!rst->major_ver) {\n\t\t\t/* Convert version '0' into version '1'. */\n\t\t\tstruct OPEN_ATTR_ENRTY_32 *oe0 = Add2Ptr(lrh, t16);\n\n\t\t\toe->bytes_per_index = oe0->bytes_per_index;\n\t\t\toe->type = oe0->type;\n\t\t\toe->is_dirty_pages = oe0->is_dirty_pages;\n\t\t\toe->name_len = 0; //oe0.name_len;\n\t\t\toe->ref = oe0->ref;\n\t\t\toe->open_record_lsn = oe0->open_record_lsn;\n\t\t} else {\n\t\t\tmemcpy(oe, Add2Ptr(lrh, t16), bytes_per_attr_entry);\n\t\t}\n\n\t\tt16 = le16_to_cpu(lrh->undo_len);\n\t\tif (t16) {\n\t\t\toe->ptr = kmalloc(t16, GFP_NOFS);\n\t\t\tif (!oe->ptr) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\toe->name_len = t16 / sizeof(short);\n\t\t\tmemcpy(oe->ptr,\n\t\t\t       Add2Ptr(lrh, le16_to_cpu(lrh->undo_off)), t16);\n\t\t\toe->is_attr_name = 1;\n\t\t} else {\n\t\t\toe->ptr = NULL;\n\t\t\toe->is_attr_name = 0;\n\t\t}\n\n\t\tgoto next_log_record_analyze;\n\n\tcase HotFix:\n\t\tt16 = le16_to_cpu(lrh->target_attr);\n\t\tt64 = le64_to_cpu(lrh->target_vcn);\n\t\tdp = find_dp(dptbl, t16, t64);\n\t\tif (dp) {\n\t\t\tsize_t j = le64_to_cpu(lrh->target_vcn) -\n\t\t\t\t   le64_to_cpu(dp->vcn);\n\t\t\tif (dp->page_lcns[j])\n\t\t\t\tdp->page_lcns[j] = lrh->page_lcns[0];\n\t\t}\n\t\tgoto next_log_record_analyze;\n\n\tcase EndTopLevelAction:\n\t\ttr = Add2Ptr(trtbl, transact_id);\n\t\ttr->prev_lsn = cpu_to_le64(rec_lsn);\n\t\ttr->undo_next_lsn = frh->client_undo_next_lsn;\n\t\tgoto next_log_record_analyze;\n\n\tcase PrepareTransaction:\n\t\ttr = Add2Ptr(trtbl, transact_id);\n\t\ttr->transact_state = TransactionPrepared;\n\t\tgoto next_log_record_analyze;\n\n\tcase CommitTransaction:\n\t\ttr = Add2Ptr(trtbl, transact_id);\n\t\ttr->transact_state = TransactionCommitted;\n\t\tgoto next_log_record_analyze;\n\n\tcase ForgetTransaction:\n\t\tfree_rsttbl_idx(trtbl, transact_id);\n\t\tgoto next_log_record_analyze;\n\n\tcase Noop:\n\tcase OpenAttributeTableDump:\n\tcase AttributeNamesDump:\n\tcase DirtyPageTableDump:\n\tcase TransactionTableDump:\n\t\t/* The following cases require no action the Analysis Pass. */\n\t\tgoto next_log_record_analyze;\n\n\tdefault:\n\t\t/*\n\t\t * All codes will be explicitly handled.\n\t\t * If we see a code we do not expect, then we are trouble.\n\t\t */\n\t\tgoto next_log_record_analyze;\n\t}\n\nend_log_records_enumerate:\n\tlcb_put(lcb);\n\tlcb = NULL;\n\n\t/*\n\t * Scan the Dirty Page Table and Transaction Table for\n\t * the lowest lsn, and return it as the Redo lsn.\n\t */\n\tdp = NULL;\n\twhile ((dp = enum_rstbl(dptbl, dp))) {\n\t\tt64 = le64_to_cpu(dp->oldest_lsn);\n\t\tif (t64 && t64 < rlsn)\n\t\t\trlsn = t64;\n\t}\n\n\ttr = NULL;\n\twhile ((tr = enum_rstbl(trtbl, tr))) {\n\t\tt64 = le64_to_cpu(tr->first_lsn);\n\t\tif (t64 && t64 < rlsn)\n\t\t\trlsn = t64;\n\t}\n\n\t/*\n\t * Only proceed if the Dirty Page Table or Transaction\n\t * table are not empty.\n\t */\n\tif ((!dptbl || !dptbl->total) && (!trtbl || !trtbl->total))\n\t\tgoto end_reply;\n\n\tsbi->flags |= NTFS_FLAGS_NEED_REPLAY;\n\tif (is_ro)\n\t\tgoto out;\n\n\t/* Reopen all of the attributes with dirty pages. */\n\toe = NULL;\nnext_open_attribute:\n\n\toe = enum_rstbl(oatbl, oe);\n\tif (!oe) {\n\t\terr = 0;\n\t\tdp = NULL;\n\t\tgoto next_dirty_page;\n\t}\n\n\toa = kzalloc(sizeof(struct OpenAttr), GFP_NOFS);\n\tif (!oa) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tinode = ntfs_iget5(sbi->sb, &oe->ref, NULL);\n\tif (IS_ERR(inode))\n\t\tgoto fake_attr;\n\n\tif (is_bad_inode(inode)) {\n\t\tiput(inode);\nfake_attr:\n\t\tif (oa->ni) {\n\t\t\tiput(&oa->ni->vfs_inode);\n\t\t\toa->ni = NULL;\n\t\t}\n\n\t\tattr = attr_create_nonres_log(sbi, oe->type, 0, oe->ptr,\n\t\t\t\t\t      oe->name_len, 0);\n\t\tif (!attr) {\n\t\t\tkfree(oa);\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\toa->attr = attr;\n\t\toa->run1 = &oa->run0;\n\t\tgoto final_oe;\n\t}\n\n\tni_oe = ntfs_i(inode);\n\toa->ni = ni_oe;\n\n\tattr = ni_find_attr(ni_oe, NULL, NULL, oe->type, oe->ptr, oe->name_len,\n\t\t\t    NULL, NULL);\n\n\tif (!attr)\n\t\tgoto fake_attr;\n\n\tt32 = le32_to_cpu(attr->size);\n\toa->attr = kmemdup(attr, t32, GFP_NOFS);\n\tif (!oa->attr)\n\t\tgoto fake_attr;\n\n\tif (!S_ISDIR(inode->i_mode)) {\n\t\tif (attr->type == ATTR_DATA && !attr->name_len) {\n\t\t\toa->run1 = &ni_oe->file.run;\n\t\t\tgoto final_oe;\n\t\t}\n\t} else {\n\t\tif (attr->type == ATTR_ALLOC &&\n\t\t    attr->name_len == ARRAY_SIZE(I30_NAME) &&\n\t\t    !memcmp(attr_name(attr), I30_NAME, sizeof(I30_NAME))) {\n\t\t\toa->run1 = &ni_oe->dir.alloc_run;\n\t\t\tgoto final_oe;\n\t\t}\n\t}\n\n\tif (attr->non_res) {\n\t\tu16 roff = le16_to_cpu(attr->nres.run_off);\n\t\tCLST svcn = le64_to_cpu(attr->nres.svcn);\n\n\t\terr = run_unpack(&oa->run0, sbi, inode->i_ino, svcn,\n\t\t\t\t le64_to_cpu(attr->nres.evcn), svcn,\n\t\t\t\t Add2Ptr(attr, roff), t32 - roff);\n\t\tif (err < 0) {\n\t\t\tkfree(oa->attr);\n\t\t\toa->attr = NULL;\n\t\t\tgoto fake_attr;\n\t\t}\n\t\terr = 0;\n\t}\n\toa->run1 = &oa->run0;\n\tattr = oa->attr;\n\nfinal_oe:\n\tif (oe->is_attr_name == 1)\n\t\tkfree(oe->ptr);\n\toe->is_attr_name = 0;\n\toe->ptr = oa;\n\toe->name_len = attr->name_len;\n\n\tgoto next_open_attribute;\n\n\t/*\n\t * Now loop through the dirty page table to extract all of the Vcn/Lcn.\n\t * Mapping that we have, and insert it into the appropriate run.\n\t */\nnext_dirty_page:\n\tdp = enum_rstbl(dptbl, dp);\n\tif (!dp)\n\t\tgoto do_redo_1;\n\n\toe = Add2Ptr(oatbl, le32_to_cpu(dp->target_attr));\n\n\tif (oe->next != RESTART_ENTRY_ALLOCATED_LE)\n\t\tgoto next_dirty_page;\n\n\toa = oe->ptr;\n\tif (!oa)\n\t\tgoto next_dirty_page;\n\n\ti = -1;\nnext_dirty_page_vcn:\n\ti += 1;\n\tif (i >= le32_to_cpu(dp->lcns_follow))\n\t\tgoto next_dirty_page;\n\n\tvcn = le64_to_cpu(dp->vcn) + i;\n\tsize = (vcn + 1) << sbi->cluster_bits;\n\n\tif (!dp->page_lcns[i])\n\t\tgoto next_dirty_page_vcn;\n\n\trno = ino_get(&oe->ref);\n\tif (rno <= MFT_REC_MIRR &&\n\t    size < (MFT_REC_VOL + 1) * sbi->record_size &&\n\t    oe->type == ATTR_DATA) {\n\t\tgoto next_dirty_page_vcn;\n\t}\n\n\tlcn = le64_to_cpu(dp->page_lcns[i]);\n\n\tif ((!run_lookup_entry(oa->run1, vcn, &lcn0, &len0, NULL) ||\n\t     lcn0 != lcn) &&\n\t    !run_add_entry(oa->run1, vcn, lcn, 1, false)) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\tattr = oa->attr;\n\tt64 = le64_to_cpu(attr->nres.alloc_size);\n\tif (size > t64) {\n\t\tattr->nres.valid_size = attr->nres.data_size =\n\t\t\tattr->nres.alloc_size = cpu_to_le64(size);\n\t}\n\tgoto next_dirty_page_vcn;\n\ndo_redo_1:\n\t/*\n\t * Perform the Redo Pass, to restore all of the dirty pages to the same\n\t * contents that they had immediately before the crash. If the dirty\n\t * page table is empty, then we can skip the entire Redo Pass.\n\t */\n\tif (!dptbl || !dptbl->total)\n\t\tgoto do_undo_action;\n\n\trec_lsn = rlsn;\n\n\t/*\n\t * Read the record at the Redo lsn, before falling\n\t * into common code to handle each record.\n\t */\n\terr = read_log_rec_lcb(log, rlsn, lcb_ctx_next, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\t/*\n\t * Now loop to read all of our log records forwards, until\n\t * we hit the end of the file, cleaning up at the end.\n\t */\ndo_action_next:\n\tfrh = lcb->lrh;\n\n\tif (LfsClientRecord != frh->record_type)\n\t\tgoto read_next_log_do_action;\n\n\ttransact_id = le32_to_cpu(frh->transact_id);\n\trec_len = le32_to_cpu(frh->client_data_len);\n\tlrh = lcb->log_rec;\n\n\tif (!check_log_rec(lrh, rec_len, transact_id, bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* Ignore log records that do not update pages. */\n\tif (lrh->lcns_follow)\n\t\tgoto find_dirty_page;\n\n\tgoto read_next_log_do_action;\n\nfind_dirty_page:\n\tt16 = le16_to_cpu(lrh->target_attr);\n\tt64 = le64_to_cpu(lrh->target_vcn);\n\tdp = find_dp(dptbl, t16, t64);\n\n\tif (!dp)\n\t\tgoto read_next_log_do_action;\n\n\tif (rec_lsn < le64_to_cpu(dp->oldest_lsn))\n\t\tgoto read_next_log_do_action;\n\n\tt16 = le16_to_cpu(lrh->target_attr);\n\tif (t16 >= bytes_per_rt(oatbl)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\toe = Add2Ptr(oatbl, t16);\n\n\tif (oe->next != RESTART_ENTRY_ALLOCATED_LE) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\toa = oe->ptr;\n\n\tif (!oa) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\tattr = oa->attr;\n\n\tvcn = le64_to_cpu(lrh->target_vcn);\n\n\tif (!run_lookup_entry(oa->run1, vcn, &lcn, NULL, NULL) ||\n\t    lcn == SPARSE_LCN) {\n\t\tgoto read_next_log_do_action;\n\t}\n\n\t/* Point to the Redo data and get its length. */\n\tdata = Add2Ptr(lrh, le16_to_cpu(lrh->redo_off));\n\tdlen = le16_to_cpu(lrh->redo_len);\n\n\t/* Shorten length by any Lcns which were deleted. */\n\tsaved_len = dlen;\n\n\tfor (i = le16_to_cpu(lrh->lcns_follow); i; i--) {\n\t\tsize_t j;\n\t\tu32 alen, voff;\n\n\t\tvoff = le16_to_cpu(lrh->record_off) +\n\t\t       le16_to_cpu(lrh->attr_off);\n\t\tvoff += le16_to_cpu(lrh->cluster_off) << SECTOR_SHIFT;\n\n\t\t/* If the Vcn question is allocated, we can just get out. */\n\t\tj = le64_to_cpu(lrh->target_vcn) - le64_to_cpu(dp->vcn);\n\t\tif (dp->page_lcns[j + i - 1])\n\t\t\tbreak;\n\n\t\tif (!saved_len)\n\t\t\tsaved_len = 1;\n\n\t\t/*\n\t\t * Calculate the allocated space left relative to the\n\t\t * log record Vcn, after removing this unallocated Vcn.\n\t\t */\n\t\talen = (i - 1) << sbi->cluster_bits;\n\n\t\t/*\n\t\t * If the update described this log record goes beyond\n\t\t * the allocated space, then we will have to reduce the length.\n\t\t */\n\t\tif (voff >= alen)\n\t\t\tdlen = 0;\n\t\telse if (voff + dlen > alen)\n\t\t\tdlen = alen - voff;\n\t}\n\n\t/*\n\t * If the resulting dlen from above is now zero,\n\t * we can skip this log record.\n\t */\n\tif (!dlen && saved_len)\n\t\tgoto read_next_log_do_action;\n\n\tt16 = le16_to_cpu(lrh->redo_op);\n\tif (can_skip_action(t16))\n\t\tgoto read_next_log_do_action;\n\n\t/* Apply the Redo operation a common routine. */\n\terr = do_action(log, oe, lrh, t16, data, dlen, rec_len, &rec_lsn);\n\tif (err)\n\t\tgoto out;\n\n\t/* Keep reading and looping back until end of file. */\nread_next_log_do_action:\n\terr = read_next_log_rec(log, lcb, &rec_lsn);\n\tif (!err && rec_lsn)\n\t\tgoto do_action_next;\n\n\tlcb_put(lcb);\n\tlcb = NULL;\n\ndo_undo_action:\n\t/* Scan Transaction Table. */\n\ttr = NULL;\ntransaction_table_next:\n\ttr = enum_rstbl(trtbl, tr);\n\tif (!tr)\n\t\tgoto undo_action_done;\n\n\tif (TransactionActive != tr->transact_state || !tr->undo_next_lsn) {\n\t\tfree_rsttbl_idx(trtbl, PtrOffset(trtbl, tr));\n\t\tgoto transaction_table_next;\n\t}\n\n\tlog->transaction_id = PtrOffset(trtbl, tr);\n\tundo_next_lsn = le64_to_cpu(tr->undo_next_lsn);\n\n\t/*\n\t * We only have to do anything if the transaction has\n\t * something its undo_next_lsn field.\n\t */\n\tif (!undo_next_lsn)\n\t\tgoto commit_undo;\n\n\t/* Read the first record to be undone by this transaction. */\n\terr = read_log_rec_lcb(log, undo_next_lsn, lcb_ctx_undo_next, &lcb);\n\tif (err)\n\t\tgoto out;\n\n\t/*\n\t * Now loop to read all of our log records forwards,\n\t * until we hit the end of the file, cleaning up at the end.\n\t */\nundo_action_next:\n\n\tlrh = lcb->log_rec;\n\tfrh = lcb->lrh;\n\ttransact_id = le32_to_cpu(frh->transact_id);\n\trec_len = le32_to_cpu(frh->client_data_len);\n\n\tif (!check_log_rec(lrh, rec_len, transact_id, bytes_per_attr_entry)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (lrh->undo_op == cpu_to_le16(Noop))\n\t\tgoto read_next_log_undo_action;\n\n\toe = Add2Ptr(oatbl, le16_to_cpu(lrh->target_attr));\n\toa = oe->ptr;\n\n\tt16 = le16_to_cpu(lrh->lcns_follow);\n\tif (!t16)\n\t\tgoto add_allocated_vcns;\n\n\tis_mapped = run_lookup_entry(oa->run1, le64_to_cpu(lrh->target_vcn),\n\t\t\t\t     &lcn, &clen, NULL);\n\n\t/*\n\t * If the mapping isn't already the table or the  mapping\n\t * corresponds to a hole the mapping, we need to make sure\n\t * there is no partial page already memory.\n\t */\n\tif (is_mapped && lcn != SPARSE_LCN && clen >= t16)\n\t\tgoto add_allocated_vcns;\n\n\tvcn = le64_to_cpu(lrh->target_vcn);\n\tvcn &= ~(log->clst_per_page - 1);\n\nadd_allocated_vcns:\n\tfor (i = 0, vcn = le64_to_cpu(lrh->target_vcn),\n\t    size = (vcn + 1) << sbi->cluster_bits;\n\t     i < t16; i++, vcn += 1, size += sbi->cluster_size) {\n\t\tattr = oa->attr;\n\t\tif (!attr->non_res) {\n\t\t\tif (size > le32_to_cpu(attr->res.data_size))\n\t\t\t\tattr->res.data_size = cpu_to_le32(size);\n\t\t} else {\n\t\t\tif (size > le64_to_cpu(attr->nres.data_size))\n\t\t\t\tattr->nres.valid_size = attr->nres.data_size =\n\t\t\t\t\tattr->nres.alloc_size =\n\t\t\t\t\t\tcpu_to_le64(size);\n\t\t}\n\t}\n\n\tt16 = le16_to_cpu(lrh->undo_op);\n\tif (can_skip_action(t16))\n\t\tgoto read_next_log_undo_action;\n\n\t/* Point to the Redo data and get its length. */\n\tdata = Add2Ptr(lrh, le16_to_cpu(lrh->undo_off));\n\tdlen = le16_to_cpu(lrh->undo_len);\n\n\t/* It is time to apply the undo action. */\n\terr = do_action(log, oe, lrh, t16, data, dlen, rec_len, NULL);\n\nread_next_log_undo_action:\n\t/*\n\t * Keep reading and looping back until we have read the\n\t * last record for this transaction.\n\t */\n\terr = read_next_log_rec(log, lcb, &rec_lsn);\n\tif (err)\n\t\tgoto out;\n\n\tif (rec_lsn)\n\t\tgoto undo_action_next;\n\n\tlcb_put(lcb);\n\tlcb = NULL;\n\ncommit_undo:\n\tfree_rsttbl_idx(trtbl, log->transaction_id);\n\n\tlog->transaction_id = 0;\n\n\tgoto transaction_table_next;\n\nundo_action_done:\n\n\tntfs_update_mftmirr(sbi, 0);\n\n\tsbi->flags &= ~NTFS_FLAGS_NEED_REPLAY;\n\nend_reply:\n\n\terr = 0;\n\tif (is_ro)\n\t\tgoto out;\n\n\trh = kzalloc(log->page_size, GFP_NOFS);\n\tif (!rh) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trh->rhdr.sign = NTFS_RSTR_SIGNATURE;\n\trh->rhdr.fix_off = cpu_to_le16(offsetof(struct RESTART_HDR, fixups));\n\tt16 = (log->page_size >> SECTOR_SHIFT) + 1;\n\trh->rhdr.fix_num = cpu_to_le16(t16);\n\trh->sys_page_size = cpu_to_le32(log->page_size);\n\trh->page_size = cpu_to_le32(log->page_size);\n\n\tt16 = ALIGN(offsetof(struct RESTART_HDR, fixups) + sizeof(short) * t16,\n\t\t    8);\n\trh->ra_off = cpu_to_le16(t16);\n\trh->minor_ver = cpu_to_le16(1); // 0x1A:\n\trh->major_ver = cpu_to_le16(1); // 0x1C:\n\n\tra2 = Add2Ptr(rh, t16);\n\tmemcpy(ra2, ra, sizeof(struct RESTART_AREA));\n\n\tra2->client_idx[0] = 0;\n\tra2->client_idx[1] = LFS_NO_CLIENT_LE;\n\tra2->flags = cpu_to_le16(2);\n\n\tle32_add_cpu(&ra2->open_log_count, 1);\n\n\tntfs_fix_pre_write(&rh->rhdr, log->page_size);\n\n\terr = ntfs_sb_write_run(sbi, &ni->file.run, 0, rh, log->page_size, 0);\n\tif (!err)\n\t\terr = ntfs_sb_write_run(sbi, &log->ni->file.run, log->page_size,\n\t\t\t\t\trh, log->page_size, 0);\n\n\tkfree(rh);\n\tif (err)\n\t\tgoto out;\n\nout:\n\tkfree(rst);\n\tif (lcb)\n\t\tlcb_put(lcb);\n\n\t/*\n\t * Scan the Open Attribute Table to close all of\n\t * the open attributes.\n\t */\n\toe = NULL;\n\twhile ((oe = enum_rstbl(oatbl, oe))) {\n\t\trno = ino_get(&oe->ref);\n\n\t\tif (oe->is_attr_name == 1) {\n\t\t\tkfree(oe->ptr);\n\t\t\toe->ptr = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (oe->is_attr_name)\n\t\t\tcontinue;\n\n\t\toa = oe->ptr;\n\t\tif (!oa)\n\t\t\tcontinue;\n\n\t\trun_close(&oa->run0);\n\t\tkfree(oa->attr);\n\t\tif (oa->ni)\n\t\t\tiput(&oa->ni->vfs_inode);\n\t\tkfree(oa);\n\t}\n\n\tkfree(trtbl);\n\tkfree(oatbl);\n\tkfree(dptbl);\n\tkfree(attr_names);\n\tkfree(rst_info.r_page);\n\n\tkfree(ra);\n\tkfree(log->one_page_buf);\n\n\tif (err)\n\t\tsbi->flags |= NTFS_FLAGS_NEED_REPLAY;\n\n\tif (err == -EROFS)\n\t\terr = 0;\n\telse if (log->set_dirty)\n\t\tntfs_set_state(sbi, NTFS_DIRTY_ERROR);\n\n\tkfree(log);\n\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3456,
            "cve_id": "CVE-2022-1976",
            "code_snippet": "static bool io_match_task_safe(struct io_kiocb *head, struct task_struct *task,\n\t\t\t       bool cancel_all)\n{\n\tbool matched;\n\n\tif (task && head->task != task)\n\t\treturn false;\n\tif (cancel_all)\n\t\treturn true;\n\n\tif (head->flags & REQ_F_LINK_TIMEOUT) {\n\t\tstruct io_ring_ctx *ctx = head->ctx;\n\n\t\t/* protect against races with linked timeouts */\n\t\tspin_lock_irq(&ctx->timeout_lock);\n\t\tmatched = io_match_linked(head);\n\t\tspin_unlock_irq(&ctx->timeout_lock);\n\t} else {\n\t\tmatched = io_match_linked(head);\n\t}\n\treturn matched;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3457,
            "cve_id": "CVE-2022-1976",
            "code_snippet": "static void __io_req_task_work_add(struct io_kiocb *req,\n\t\t\t\t   struct io_uring_task *tctx,\n\t\t\t\t   struct io_wq_work_list *list)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_wq_work_node *node;\n\tunsigned long flags;\n\tbool running;\n\n\tspin_lock_irqsave(&tctx->task_lock, flags);\n\twq_list_add_tail(&req->io_task_work.node, list);\n\trunning = tctx->task_running;\n\tif (!running)\n\t\ttctx->task_running = true;\n\tspin_unlock_irqrestore(&tctx->task_lock, flags);\n\n\t/* task_work already pending, we're done */\n\tif (running)\n\t\treturn;\n\n\tif (ctx->flags & IORING_SETUP_TASKRUN_FLAG)\n\t\tatomic_or(IORING_SQ_TASKRUN, &ctx->rings->sq_flags);\n\n\tif (likely(!task_work_add(req->task, &tctx->task_work, ctx->notify_method)))\n\t\treturn;\n\n\tspin_lock_irqsave(&tctx->task_lock, flags);\n\ttctx->task_running = false;\n\tnode = wq_list_merge(&tctx->prio_task_list, &tctx->task_list);\n\tspin_unlock_irqrestore(&tctx->task_lock, flags);\n\n\twhile (node) {\n\t\treq = container_of(node, struct io_kiocb, io_task_work.node);\n\t\tnode = node->next;\n\t\tif (llist_add(&req->io_task_work.fallback_node,\n\t\t\t      &req->ctx->fallback_llist))\n\t\t\tschedule_delayed_work(&req->ctx->fallback_work, 1);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3458,
            "cve_id": "CVE-2022-1976",
            "code_snippet": "static int io_poll_check_events(struct io_kiocb *req, bool *locked)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint v, ret;\n\n\t/* req->task == current here, checking PF_EXITING is safe */\n\tif (unlikely(req->task->flags & PF_EXITING))\n\t\treturn -ECANCELED;\n\n\tdo {\n\t\tv = atomic_read(&req->poll_refs);\n\n\t\t/* tw handler should be the owner, and so have some references */\n\t\tif (WARN_ON_ONCE(!(v & IO_POLL_REF_MASK)))\n\t\t\treturn 0;\n\t\tif (v & IO_POLL_CANCEL_FLAG)\n\t\t\treturn -ECANCELED;\n\n\t\tif (!req->cqe.res) {\n\t\t\tstruct poll_table_struct pt = { ._key = req->apoll_events };\n\t\t\treq->cqe.res = vfs_poll(req->file, &pt) & req->apoll_events;\n\t\t}\n\n\t\tif ((unlikely(!req->cqe.res)))\n\t\t\tcontinue;\n\t\tif (req->apoll_events & EPOLLONESHOT)\n\t\t\treturn 0;\n\n\t\t/* multishot, just fill a CQE and proceed */\n\t\tif (!(req->flags & REQ_F_APOLL_MULTISHOT)) {\n\t\t\t__poll_t mask = mangle_poll(req->cqe.res &\n\t\t\t\t\t\t    req->apoll_events);\n\t\t\tbool filled;\n\n\t\t\tspin_lock(&ctx->completion_lock);\n\t\t\tfilled = io_fill_cqe_aux(ctx, req->cqe.user_data,\n\t\t\t\t\t\t mask, IORING_CQE_F_MORE);\n\t\t\tio_commit_cqring(ctx);\n\t\t\tspin_unlock(&ctx->completion_lock);\n\t\t\tif (filled) {\n\t\t\t\tio_cqring_ev_posted(ctx);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\treturn -ECANCELED;\n\t\t}\n\n\t\tio_tw_lock(req->ctx, locked);\n\t\tif (unlikely(req->task->flags & PF_EXITING))\n\t\t\treturn -EFAULT;\n\t\tret = io_issue_sqe(req,\n\t\t\t\t   IO_URING_F_NONBLOCK|IO_URING_F_COMPLETE_DEFER);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t/*\n\t\t * Release all references, retry if someone tried to restart\n\t\t * task_work while we were executing it.\n\t\t */\n\t} while (atomic_sub_return(v & IO_POLL_REF_MASK, &req->poll_refs));\n\n\treturn 1;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3477,
            "cve_id": "CVE-2022-20158",
            "code_snippet": "static int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t       struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct packet_sock *po;\n\tstruct sockaddr_ll *sll;\n\tunion tpacket_uhdr h;\n\tu8 *skb_head = skb->data;\n\tint skb_len = skb->len;\n\tunsigned int snaplen, res;\n\tunsigned long status = TP_STATUS_USER;\n\tunsigned short macoff, hdrlen;\n\tunsigned int netoff;\n\tstruct sk_buff *copy_skb = NULL;\n\tstruct timespec64 ts;\n\t__u32 ts_status;\n\tbool is_drop_n_account = false;\n\tunsigned int slot_id = 0;\n\tbool do_vnet = false;\n\n\t/* struct tpacket{2,3}_hdr is aligned to a multiple of TPACKET_ALIGNMENT.\n\t * We may add members to them until current aligned size without forcing\n\t * userspace to call getsockopt(..., PACKET_HDRLEN, ...).\n\t */\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h2)) != 32);\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h3)) != 48);\n\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto drop;\n\n\tsk = pt->af_packet_priv;\n\tpo = pkt_sk(sk);\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto drop;\n\n\tif (dev_has_header(dev)) {\n\t\tif (sk->sk_type != SOCK_DGRAM)\n\t\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\telse if (skb->pkt_type == PACKET_OUTGOING) {\n\t\t\t/* Special case: outgoing packets have ll header at head */\n\t\t\tskb_pull(skb, skb_network_offset(skb));\n\t\t}\n\t}\n\n\tsnaplen = skb->len;\n\n\tres = run_filter(skb, sk, snaplen);\n\tif (!res)\n\t\tgoto drop_n_restore;\n\n\t/* If we are flooded, just give up */\n\tif (__packet_rcv_has_room(po, skb) == ROOM_NONE) {\n\t\tatomic_inc(&po->tp_drops);\n\t\tgoto drop_n_restore;\n\t}\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tstatus |= TP_STATUS_CSUMNOTREADY;\n\telse if (skb->pkt_type != PACKET_OUTGOING &&\n\t\t (skb->ip_summed == CHECKSUM_COMPLETE ||\n\t\t  skb_csum_unnecessary(skb)))\n\t\tstatus |= TP_STATUS_CSUM_VALID;\n\n\tif (snaplen > res)\n\t\tsnaplen = res;\n\n\tif (sk->sk_type == SOCK_DGRAM) {\n\t\tmacoff = netoff = TPACKET_ALIGN(po->tp_hdrlen) + 16 +\n\t\t\t\t  po->tp_reserve;\n\t} else {\n\t\tunsigned int maclen = skb_network_offset(skb);\n\t\tnetoff = TPACKET_ALIGN(po->tp_hdrlen +\n\t\t\t\t       (maclen < 16 ? 16 : maclen)) +\n\t\t\t\t       po->tp_reserve;\n\t\tif (po->has_vnet_hdr) {\n\t\t\tnetoff += sizeof(struct virtio_net_hdr);\n\t\t\tdo_vnet = true;\n\t\t}\n\t\tmacoff = netoff - maclen;\n\t}\n\tif (netoff > USHRT_MAX) {\n\t\tatomic_inc(&po->tp_drops);\n\t\tgoto drop_n_restore;\n\t}\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tif (macoff + snaplen > po->rx_ring.frame_size) {\n\t\t\tif (po->copy_thresh &&\n\t\t\t    atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf) {\n\t\t\t\tif (skb_shared(skb)) {\n\t\t\t\t\tcopy_skb = skb_clone(skb, GFP_ATOMIC);\n\t\t\t\t} else {\n\t\t\t\t\tcopy_skb = skb_get(skb);\n\t\t\t\t\tskb_head = skb->data;\n\t\t\t\t}\n\t\t\t\tif (copy_skb) {\n\t\t\t\t\tmemset(&PACKET_SKB_CB(copy_skb)->sa.ll, 0,\n\t\t\t\t\t       sizeof(PACKET_SKB_CB(copy_skb)->sa.ll));\n\t\t\t\t\tskb_set_owner_r(copy_skb, sk);\n\t\t\t\t}\n\t\t\t}\n\t\t\tsnaplen = po->rx_ring.frame_size - macoff;\n\t\t\tif ((int)snaplen < 0) {\n\t\t\t\tsnaplen = 0;\n\t\t\t\tdo_vnet = false;\n\t\t\t}\n\t\t}\n\t} else if (unlikely(macoff + snaplen >\n\t\t\t    GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len)) {\n\t\tu32 nval;\n\n\t\tnval = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len - macoff;\n\t\tpr_err_once(\"tpacket_rcv: packet too big, clamped from %u to %u. macoff=%u\\n\",\n\t\t\t    snaplen, nval, macoff);\n\t\tsnaplen = nval;\n\t\tif (unlikely((int)snaplen < 0)) {\n\t\t\tsnaplen = 0;\n\t\t\tmacoff = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len;\n\t\t\tdo_vnet = false;\n\t\t}\n\t}\n\tspin_lock(&sk->sk_receive_queue.lock);\n\th.raw = packet_current_rx_frame(po, skb,\n\t\t\t\t\tTP_STATUS_KERNEL, (macoff+snaplen));\n\tif (!h.raw)\n\t\tgoto drop_n_account;\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tslot_id = po->rx_ring.head;\n\t\tif (test_bit(slot_id, po->rx_ring.rx_owner_map))\n\t\t\tgoto drop_n_account;\n\t\t__set_bit(slot_id, po->rx_ring.rx_owner_map);\n\t}\n\n\tif (do_vnet &&\n\t    virtio_net_hdr_from_skb(skb, h.raw + macoff -\n\t\t\t\t    sizeof(struct virtio_net_hdr),\n\t\t\t\t    vio_le(), true, 0)) {\n\t\tif (po->tp_version == TPACKET_V3)\n\t\t\tprb_clear_blk_fill_status(&po->rx_ring);\n\t\tgoto drop_n_account;\n\t}\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tpacket_increment_rx_head(po, &po->rx_ring);\n\t/*\n\t * LOSING will be reported till you read the stats,\n\t * because it's COR - Clear On Read.\n\t * Anyways, moving it for V1/V2 only as V3 doesn't need this\n\t * at packet level.\n\t */\n\t\tif (atomic_read(&po->tp_drops))\n\t\t\tstatus |= TP_STATUS_LOSING;\n\t}\n\n\tpo->stats.stats1.tp_packets++;\n\tif (copy_skb) {\n\t\tstatus |= TP_STATUS_COPY;\n\t\t__skb_queue_tail(&sk->sk_receive_queue, copy_skb);\n\t}\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\n\tskb_copy_bits(skb, 0, h.raw + macoff, snaplen);\n\n\t/* Always timestamp; prefer an existing software timestamp taken\n\t * closer to the time of capture.\n\t */\n\tts_status = tpacket_get_timestamp(skb, &ts,\n\t\t\t\t\t  po->tp_tstamp | SOF_TIMESTAMPING_SOFTWARE);\n\tif (!ts_status)\n\t\tktime_get_real_ts64(&ts);\n\n\tstatus |= ts_status;\n\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\th.h1->tp_len = skb->len;\n\t\th.h1->tp_snaplen = snaplen;\n\t\th.h1->tp_mac = macoff;\n\t\th.h1->tp_net = netoff;\n\t\th.h1->tp_sec = ts.tv_sec;\n\t\th.h1->tp_usec = ts.tv_nsec / NSEC_PER_USEC;\n\t\thdrlen = sizeof(*h.h1);\n\t\tbreak;\n\tcase TPACKET_V2:\n\t\th.h2->tp_len = skb->len;\n\t\th.h2->tp_snaplen = snaplen;\n\t\th.h2->tp_mac = macoff;\n\t\th.h2->tp_net = netoff;\n\t\th.h2->tp_sec = ts.tv_sec;\n\t\th.h2->tp_nsec = ts.tv_nsec;\n\t\tif (skb_vlan_tag_present(skb)) {\n\t\t\th.h2->tp_vlan_tci = skb_vlan_tag_get(skb);\n\t\t\th.h2->tp_vlan_tpid = ntohs(skb->vlan_proto);\n\t\t\tstatus |= TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;\n\t\t} else {\n\t\t\th.h2->tp_vlan_tci = 0;\n\t\t\th.h2->tp_vlan_tpid = 0;\n\t\t}\n\t\tmemset(h.h2->tp_padding, 0, sizeof(h.h2->tp_padding));\n\t\thdrlen = sizeof(*h.h2);\n\t\tbreak;\n\tcase TPACKET_V3:\n\t\t/* tp_nxt_offset,vlan are already populated above.\n\t\t * So DONT clear those fields here\n\t\t */\n\t\th.h3->tp_status |= status;\n\t\th.h3->tp_len = skb->len;\n\t\th.h3->tp_snaplen = snaplen;\n\t\th.h3->tp_mac = macoff;\n\t\th.h3->tp_net = netoff;\n\t\th.h3->tp_sec  = ts.tv_sec;\n\t\th.h3->tp_nsec = ts.tv_nsec;\n\t\tmemset(h.h3->tp_padding, 0, sizeof(h.h3->tp_padding));\n\t\thdrlen = sizeof(*h.h3);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tsll = h.raw + TPACKET_ALIGN(hdrlen);\n\tsll->sll_halen = dev_parse_header(skb, sll->sll_addr);\n\tsll->sll_family = AF_PACKET;\n\tsll->sll_hatype = dev->type;\n\tsll->sll_protocol = skb->protocol;\n\tsll->sll_pkttype = skb->pkt_type;\n\tif (unlikely(po->origdev))\n\t\tsll->sll_ifindex = orig_dev->ifindex;\n\telse\n\t\tsll->sll_ifindex = dev->ifindex;\n\n\tsmp_mb();\n\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE == 1\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tu8 *start, *end;\n\n\t\tend = (u8 *) PAGE_ALIGN((unsigned long) h.raw +\n\t\t\t\t\tmacoff + snaplen);\n\n\t\tfor (start = h.raw; start < end; start += PAGE_SIZE)\n\t\t\tflush_dcache_page(pgv_to_page(start));\n\t}\n\tsmp_wmb();\n#endif\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tspin_lock(&sk->sk_receive_queue.lock);\n\t\t__packet_set_status(po, h.raw, status);\n\t\t__clear_bit(slot_id, po->rx_ring.rx_owner_map);\n\t\tspin_unlock(&sk->sk_receive_queue.lock);\n\t\tsk->sk_data_ready(sk);\n\t} else if (po->tp_version == TPACKET_V3) {\n\t\tprb_clear_blk_fill_status(&po->rx_ring);\n\t}\n\ndrop_n_restore:\n\tif (skb_head != skb->data && skb_shared(skb)) {\n\t\tskb->data = skb_head;\n\t\tskb->len = skb_len;\n\t}\ndrop:\n\tif (!is_drop_n_account)\n\t\tconsume_skb(skb);\n\telse\n\t\tkfree_skb(skb);\n\treturn 0;\n\ndrop_n_account:\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\tatomic_inc(&po->tp_drops);\n\tis_drop_n_account = true;\n\n\tsk->sk_data_ready(sk);\n\tkfree_skb(copy_skb);\n\tgoto drop_n_restore;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3485,
            "cve_id": "CVE-2022-20409",
            "code_snippet": "static void io_worker_exit(struct io_worker *worker)\n{\n\tstruct io_wqe *wqe = worker->wqe;\n\tstruct io_wqe_acct *acct = io_wqe_get_acct(worker);\n\n\t/*\n\t * If we're not at zero, someone else is holding a brief reference\n\t * to the worker. Wait for that to go away.\n\t */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tif (!refcount_dec_and_test(&worker->ref))\n\t\tschedule();\n\t__set_current_state(TASK_RUNNING);\n\n\tpreempt_disable();\n\tcurrent->flags &= ~PF_IO_WORKER;\n\tif (worker->flags & IO_WORKER_F_RUNNING)\n\t\tatomic_dec(&acct->nr_running);\n\tif (!(worker->flags & IO_WORKER_F_BOUND))\n\t\tatomic_dec(&wqe->wq->user->processes);\n\tworker->flags = 0;\n\tpreempt_enable();\n\n\tif (worker->saved_creds) {\n\t\trevert_creds(worker->saved_creds);\n\t\tworker->cur_creds = worker->saved_creds = NULL;\n\t}\n\n\traw_spin_lock_irq(&wqe->lock);\n\thlist_nulls_del_rcu(&worker->nulls_node);\n\tlist_del_rcu(&worker->all_list);\n\tacct->nr_workers--;\n\traw_spin_unlock_irq(&wqe->lock);\n\n\tkfree_rcu(worker, rcu);\n\tif (refcount_dec_and_test(&wqe->wq->refs))\n\t\tcomplete(&wqe->wq->done);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3486,
            "cve_id": "CVE-2022-20409",
            "code_snippet": "static void __io_worker_idle(struct io_wqe *wqe, struct io_worker *worker)\n\t__must_hold(wqe->lock)\n{\n\tif (!(worker->flags & IO_WORKER_F_FREE)) {\n\t\tworker->flags |= IO_WORKER_F_FREE;\n\t\thlist_nulls_add_head_rcu(&worker->nulls_node, &wqe->free_list);\n\t}\n\tif (worker->saved_creds) {\n\t\trevert_creds(worker->saved_creds);\n\t\tworker->cur_creds = worker->saved_creds = NULL;\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3487,
            "cve_id": "CVE-2022-20409",
            "code_snippet": "static bool io_match_task(struct io_kiocb *head,\n\t\t\t  struct task_struct *task,\n\t\t\t  struct files_struct *files)\n{\n\tstruct io_kiocb *req;\n\n\tif (task && head->task != task) {\n\t\t/* in terms of cancelation, always match if req task is dead */\n\t\tif (head->task->flags & PF_EXITING)\n\t\t\treturn true;\n\t\treturn false;\n\t}\n\tif (!files)\n\t\treturn true;\n\n\tio_for_each_link(req, head) {\n\t\tif (!(req->flags & REQ_F_WORK_INITIALIZED))\n\t\t\tcontinue;\n\t\tif (req->file && req->file->f_op == &io_uring_fops)\n\t\t\treturn true;\n\t\tif (req->task->files == files)\n\t\t\treturn true;\n\t}\n\treturn false;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3488,
            "cve_id": "CVE-2022-20409",
            "code_snippet": "static void io_prep_async_work(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_req_init_async(req);\n\n\tif (req->flags & REQ_F_FORCE_ASYNC)\n\t\treq->work.flags |= IO_WQ_WORK_CONCURRENT;\n\n\tif (req->flags & REQ_F_ISREG) {\n\t\tif (def->hash_reg_file || (ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tio_wq_hash_work(&req->work, file_inode(req->file));\n\t} else {\n\t\tif (def->unbound_nonreg_file)\n\t\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t}\n\tif (!req->work.creds)\n\t\treq->work.creds = get_current_cred();\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3489,
            "cve_id": "CVE-2022-20409",
            "code_snippet": "static int io_uring_show_cred(int id, void *p, void *data)\n{\n\tconst struct cred *cred = p;\n\tstruct seq_file *m = data;\n\tstruct user_namespace *uns = seq_user_ns(m);\n\tstruct group_info *gi;\n\tkernel_cap_t cap;\n\tunsigned __capi;\n\tint g;\n\n\tseq_printf(m, \"%5d\\n\", id);\n\tseq_put_decimal_ull(m, \"\\tUid:\\t\", from_kuid_munged(uns, cred->uid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->euid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->suid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->fsuid));\n\tseq_put_decimal_ull(m, \"\\n\\tGid:\\t\", from_kgid_munged(uns, cred->gid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->egid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->sgid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->fsgid));\n\tseq_puts(m, \"\\n\\tGroups:\\t\");\n\tgi = cred->group_info;\n\tfor (g = 0; g < gi->ngroups; g++) {\n\t\tseq_put_decimal_ull(m, g ? \" \" : \"\",\n\t\t\t\t\tfrom_kgid_munged(uns, gi->gid[g]));\n\t}\n\tseq_puts(m, \"\\n\\tCapEff:\\t\");\n\tcap = cred->cap_effective;\n\tCAP_FOR_EACH_U32(__capi)\n\t\tseq_put_hex_ll(m, NULL, cap.cap[CAP_LAST_U32 - __capi], 8);\n\tseq_putc(m, '\\n');\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3490,
            "cve_id": "CVE-2022-20409",
            "code_snippet": "static inline void io_req_init_async(struct io_kiocb *req)\n{\n\tif (req->flags & REQ_F_WORK_INITIALIZED)\n\t\treturn;\n\n\t__io_req_init_async(req);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3519,
            "cve_id": "CVE-2022-20566",
            "code_snippet": "static struct l2cap_chan *l2cap_get_chan_by_dcid(struct l2cap_conn *conn,\n\t\t\t\t\t\t u16 cid)\n{\n\tstruct l2cap_chan *c;\n\n\tmutex_lock(&conn->chan_lock);\n\tc = __l2cap_get_chan_by_dcid(conn, cid);\n\tif (c) {\n\t\t/* Only lock if chan reference is not 0 */\n\t\tc = l2cap_chan_hold_unless_zero(c);\n\t\tif (c)\n\t\t\tl2cap_chan_lock(c);\n\t}\n\tmutex_unlock(&conn->chan_lock);\n\n\treturn c;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3520,
            "cve_id": "CVE-2022-20566",
            "code_snippet": "static void l2cap_move_continue(struct l2cap_conn *conn, u16 icid, u16 result)\n{\n\tstruct l2cap_chan *chan;\n\tstruct hci_chan *hchan = NULL;\n\n\tchan = l2cap_get_chan_by_scid(conn, icid);\n\tif (!chan) {\n\t\tl2cap_send_move_chan_cfm_icid(conn, icid);\n\t\treturn;\n\t}\n\n\t__clear_chan_timer(chan);\n\tif (result == L2CAP_MR_PEND)\n\t\t__set_chan_timer(chan, L2CAP_MOVE_ERTX_TIMEOUT);\n\n\tswitch (chan->move_state) {\n\tcase L2CAP_MOVE_WAIT_LOGICAL_COMP:\n\t\t/* Move confirm will be sent when logical link\n\t\t * is complete.\n\t\t */\n\t\tchan->move_state = L2CAP_MOVE_WAIT_LOGICAL_CFM;\n\t\tbreak;\n\tcase L2CAP_MOVE_WAIT_RSP_SUCCESS:\n\t\tif (result == L2CAP_MR_PEND) {\n\t\t\tbreak;\n\t\t} else if (test_bit(CONN_LOCAL_BUSY,\n\t\t\t\t    &chan->conn_state)) {\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_LOCAL_BUSY;\n\t\t} else {\n\t\t\t/* Logical link is up or moving to BR/EDR,\n\t\t\t * proceed with move\n\t\t\t */\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_CONFIRM_RSP;\n\t\t\tl2cap_send_move_chan_cfm(chan, L2CAP_MC_CONFIRMED);\n\t\t}\n\t\tbreak;\n\tcase L2CAP_MOVE_WAIT_RSP:\n\t\t/* Moving to AMP */\n\t\tif (result == L2CAP_MR_SUCCESS) {\n\t\t\t/* Remote is ready, send confirm immediately\n\t\t\t * after logical link is ready\n\t\t\t */\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_LOGICAL_CFM;\n\t\t} else {\n\t\t\t/* Both logical link and move success\n\t\t\t * are required to confirm\n\t\t\t */\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_LOGICAL_COMP;\n\t\t}\n\n\t\t/* Placeholder - get hci_chan for logical link */\n\t\tif (!hchan) {\n\t\t\t/* Logical link not available */\n\t\t\tl2cap_send_move_chan_cfm(chan, L2CAP_MC_UNCONFIRMED);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* If the logical link is not yet connected, do not\n\t\t * send confirmation.\n\t\t */\n\t\tif (hchan->state != BT_CONNECTED)\n\t\t\tbreak;\n\n\t\t/* Logical link is already ready to go */\n\n\t\tchan->hs_hcon = hchan->conn;\n\t\tchan->hs_hcon->l2cap_data = chan->conn;\n\n\t\tif (result == L2CAP_MR_SUCCESS) {\n\t\t\t/* Can confirm now */\n\t\t\tl2cap_send_move_chan_cfm(chan, L2CAP_MC_CONFIRMED);\n\t\t} else {\n\t\t\t/* Now only need move success\n\t\t\t * to confirm\n\t\t\t */\n\t\t\tchan->move_state = L2CAP_MOVE_WAIT_RSP_SUCCESS;\n\t\t}\n\n\t\tl2cap_logical_cfm(chan, hchan, L2CAP_MR_SUCCESS);\n\t\tbreak;\n\tdefault:\n\t\t/* Any other amp move state means the move failed. */\n\t\tchan->move_id = chan->local_amp_id;\n\t\tl2cap_move_done(chan);\n\t\tl2cap_send_move_chan_cfm(chan, L2CAP_MC_UNCONFIRMED);\n\t}\n\n\tl2cap_chan_unlock(chan);\n\tl2cap_chan_put(chan);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3521,
            "cve_id": "CVE-2022-20566",
            "code_snippet": "static inline int l2cap_move_channel_confirm_rsp(struct l2cap_conn *conn,\n\t\t\t\t\t\t struct l2cap_cmd_hdr *cmd,\n\t\t\t\t\t\t u16 cmd_len, void *data)\n{\n\tstruct l2cap_move_chan_cfm_rsp *rsp = data;\n\tstruct l2cap_chan *chan;\n\tu16 icid;\n\n\tif (cmd_len != sizeof(*rsp))\n\t\treturn -EPROTO;\n\n\ticid = le16_to_cpu(rsp->icid);\n\n\tBT_DBG(\"icid 0x%4.4x\", icid);\n\n\tchan = l2cap_get_chan_by_scid(conn, icid);\n\tif (!chan)\n\t\treturn 0;\n\n\t__clear_chan_timer(chan);\n\n\tif (chan->move_state == L2CAP_MOVE_WAIT_CONFIRM_RSP) {\n\t\tchan->local_amp_id = chan->move_id;\n\n\t\tif (chan->local_amp_id == AMP_ID_BREDR && chan->hs_hchan)\n\t\t\t__release_logical_link(chan);\n\n\t\tl2cap_move_done(chan);\n\t}\n\n\tl2cap_chan_unlock(chan);\n\tl2cap_chan_put(chan);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3522,
            "cve_id": "CVE-2022-20566",
            "code_snippet": "static struct l2cap_chan *l2cap_get_chan_by_scid(struct l2cap_conn *conn,\n\t\t\t\t\t\t u16 cid)\n{\n\tstruct l2cap_chan *c;\n\n\tmutex_lock(&conn->chan_lock);\n\tc = __l2cap_get_chan_by_scid(conn, cid);\n\tif (c) {\n\t\t/* Only lock if chan reference is not 0 */\n\t\tc = l2cap_chan_hold_unless_zero(c);\n\t\tif (c)\n\t\t\tl2cap_chan_lock(c);\n\t}\n\tmutex_unlock(&conn->chan_lock);\n\n\treturn c;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3523,
            "cve_id": "CVE-2022-20566",
            "code_snippet": "static inline int l2cap_config_rsp(struct l2cap_conn *conn,\n\t\t\t\t   struct l2cap_cmd_hdr *cmd, u16 cmd_len,\n\t\t\t\t   u8 *data)\n{\n\tstruct l2cap_conf_rsp *rsp = (struct l2cap_conf_rsp *)data;\n\tu16 scid, flags, result;\n\tstruct l2cap_chan *chan;\n\tint len = cmd_len - sizeof(*rsp);\n\tint err = 0;\n\n\tif (cmd_len < sizeof(*rsp))\n\t\treturn -EPROTO;\n\n\tscid   = __le16_to_cpu(rsp->scid);\n\tflags  = __le16_to_cpu(rsp->flags);\n\tresult = __le16_to_cpu(rsp->result);\n\n\tBT_DBG(\"scid 0x%4.4x flags 0x%2.2x result 0x%2.2x len %d\", scid, flags,\n\t       result, len);\n\n\tchan = l2cap_get_chan_by_scid(conn, scid);\n\tif (!chan)\n\t\treturn 0;\n\n\tswitch (result) {\n\tcase L2CAP_CONF_SUCCESS:\n\t\tl2cap_conf_rfc_get(chan, rsp->data, len);\n\t\tclear_bit(CONF_REM_CONF_PEND, &chan->conf_state);\n\t\tbreak;\n\n\tcase L2CAP_CONF_PENDING:\n\t\tset_bit(CONF_REM_CONF_PEND, &chan->conf_state);\n\n\t\tif (test_bit(CONF_LOC_CONF_PEND, &chan->conf_state)) {\n\t\t\tchar buf[64];\n\n\t\t\tlen = l2cap_parse_conf_rsp(chan, rsp->data, len,\n\t\t\t\t\t\t   buf, sizeof(buf), &result);\n\t\t\tif (len < 0) {\n\t\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\tif (!chan->hs_hcon) {\n\t\t\t\tl2cap_send_efs_conf_rsp(chan, buf, cmd->ident,\n\t\t\t\t\t\t\t0);\n\t\t\t} else {\n\t\t\t\tif (l2cap_check_efs(chan)) {\n\t\t\t\t\tamp_create_logical_link(chan);\n\t\t\t\t\tchan->ident = cmd->ident;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tgoto done;\n\n\tcase L2CAP_CONF_UNKNOWN:\n\tcase L2CAP_CONF_UNACCEPT:\n\t\tif (chan->num_conf_rsp <= L2CAP_CONF_MAX_CONF_RSP) {\n\t\t\tchar req[64];\n\n\t\t\tif (len > sizeof(req) - sizeof(struct l2cap_conf_req)) {\n\t\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\t/* throw out any old stored conf requests */\n\t\t\tresult = L2CAP_CONF_SUCCESS;\n\t\t\tlen = l2cap_parse_conf_rsp(chan, rsp->data, len,\n\t\t\t\t\t\t   req, sizeof(req), &result);\n\t\t\tif (len < 0) {\n\t\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\tl2cap_send_cmd(conn, l2cap_get_ident(conn),\n\t\t\t\t       L2CAP_CONF_REQ, len, req);\n\t\t\tchan->num_conf_req++;\n\t\t\tif (result != L2CAP_CONF_SUCCESS)\n\t\t\t\tgoto done;\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\n\n\tdefault:\n\t\tl2cap_chan_set_err(chan, ECONNRESET);\n\n\t\t__set_chan_timer(chan, L2CAP_DISC_REJ_TIMEOUT);\n\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\tgoto done;\n\t}\n\n\tif (flags & L2CAP_CONF_FLAG_CONTINUATION)\n\t\tgoto done;\n\n\tset_bit(CONF_INPUT_DONE, &chan->conf_state);\n\n\tif (test_bit(CONF_OUTPUT_DONE, &chan->conf_state)) {\n\t\tset_default_fcs(chan);\n\n\t\tif (chan->mode == L2CAP_MODE_ERTM ||\n\t\t    chan->mode == L2CAP_MODE_STREAMING)\n\t\t\terr = l2cap_ertm_init(chan);\n\n\t\tif (err < 0)\n\t\t\tl2cap_send_disconn_req(chan, -err);\n\t\telse\n\t\t\tl2cap_chan_ready(chan);\n\t}\n\ndone:\n\tl2cap_chan_unlock(chan);\n\tl2cap_chan_put(chan);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3550,
            "cve_id": "CVE-2022-22942",
            "code_snippet": "int vmw_fence_event_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *file_priv)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct drm_vmw_fence_event_arg *arg =\n\t\t(struct drm_vmw_fence_event_arg *) data;\n\tstruct vmw_fence_obj *fence = NULL;\n\tstruct vmw_fpriv *vmw_fp = vmw_fpriv(file_priv);\n\tstruct ttm_object_file *tfile = vmw_fp->tfile;\n\tstruct drm_vmw_fence_rep __user *user_fence_rep =\n\t\t(struct drm_vmw_fence_rep __user *)(unsigned long)\n\t\targ->fence_rep;\n\tuint32_t handle;\n\tint ret;\n\n\t/*\n\t * Look up an existing fence object,\n\t * and if user-space wants a new reference,\n\t * add one.\n\t */\n\tif (arg->handle) {\n\t\tstruct ttm_base_object *base =\n\t\t\tvmw_fence_obj_lookup(tfile, arg->handle);\n\n\t\tif (IS_ERR(base))\n\t\t\treturn PTR_ERR(base);\n\n\t\tfence = &(container_of(base, struct vmw_user_fence,\n\t\t\t\t       base)->fence);\n\t\t(void) vmw_fence_obj_reference(fence);\n\n\t\tif (user_fence_rep != NULL) {\n\t\t\tret = ttm_ref_object_add(vmw_fp->tfile, base,\n\t\t\t\t\t\t NULL, false);\n\t\t\tif (unlikely(ret != 0)) {\n\t\t\t\tDRM_ERROR(\"Failed to reference a fence \"\n\t\t\t\t\t  \"object.\\n\");\n\t\t\t\tgoto out_no_ref_obj;\n\t\t\t}\n\t\t\thandle = base->handle;\n\t\t}\n\t\tttm_base_object_unref(&base);\n\t}\n\n\t/*\n\t * Create a new fence object.\n\t */\n\tif (!fence) {\n\t\tret = vmw_execbuf_fence_commands(file_priv, dev_priv,\n\t\t\t\t\t\t &fence,\n\t\t\t\t\t\t (user_fence_rep) ?\n\t\t\t\t\t\t &handle : NULL);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Fence event failed to create fence.\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tBUG_ON(fence == NULL);\n\n\tret = vmw_event_fence_action_create(file_priv, fence,\n\t\t\t\t\t    arg->flags,\n\t\t\t\t\t    arg->user_data,\n\t\t\t\t\t    true);\n\tif (unlikely(ret != 0)) {\n\t\tif (ret != -ERESTARTSYS)\n\t\t\tDRM_ERROR(\"Failed to attach event to fence.\\n\");\n\t\tgoto out_no_create;\n\t}\n\n\tvmw_execbuf_copy_fence_user(dev_priv, vmw_fp, 0, user_fence_rep, fence,\n\t\t\t\t    handle, -1);\n\tvmw_fence_obj_unreference(&fence);\n\treturn 0;\nout_no_create:\n\tif (user_fence_rep != NULL)\n\t\tttm_ref_object_base_unref(tfile, handle);\nout_no_ref_obj:\n\tvmw_fence_obj_unreference(&fence);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3563,
            "cve_id": "CVE-2022-2318",
            "code_snippet": "void rose_start_idletimer(struct sock *sk)\n{\n\tstruct rose_sock *rose = rose_sk(sk);\n\n\tsk_stop_timer(sk, &rose->idletimer);\n\n\tif (rose->idle > 0) {\n\t\trose->idletimer.function = rose_idletimer_expiry;\n\t\trose->idletimer.expires  = jiffies + rose->idle;\n\n\t\tsk_reset_timer(sk, &rose->idletimer, rose->idletimer.expires);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3564,
            "cve_id": "CVE-2022-2318",
            "code_snippet": "void rose_start_t1timer(struct sock *sk)\n{\n\tstruct rose_sock *rose = rose_sk(sk);\n\n\tsk_stop_timer(sk, &rose->timer);\n\n\trose->timer.function = rose_timer_expiry;\n\trose->timer.expires  = jiffies + rose->t1;\n\n\tsk_reset_timer(sk, &rose->timer, rose->timer.expires);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3565,
            "cve_id": "CVE-2022-2318",
            "code_snippet": "void rose_start_hbtimer(struct sock *sk)\n{\n\tstruct rose_sock *rose = rose_sk(sk);\n\n\tsk_stop_timer(sk, &rose->timer);\n\n\trose->timer.function = rose_timer_expiry;\n\trose->timer.expires  = jiffies + rose->hb;\n\n\tsk_reset_timer(sk, &rose->timer, rose->timer.expires);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3566,
            "cve_id": "CVE-2022-2318",
            "code_snippet": "static void rose_heartbeat_expiry(struct timer_list *t)\n{\n\tstruct sock *sk = from_timer(sk, t, sk_timer);\n\tstruct rose_sock *rose = rose_sk(sk);\n\n\tbh_lock_sock(sk);\n\tswitch (rose->state) {\n\tcase ROSE_STATE_0:\n\t\t/* Magic here: If we listen() and a new link dies before it\n\t\t   is accepted() it isn't 'dead' so doesn't get removed. */\n\t\tif (sock_flag(sk, SOCK_DESTROY) ||\n\t\t    (sk->sk_state == TCP_LISTEN && sock_flag(sk, SOCK_DEAD))) {\n\t\t\tbh_unlock_sock(sk);\n\t\t\trose_destroy_socket(sk);\n\t\t\tsock_put(sk);\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tcase ROSE_STATE_3:\n\t\t/*\n\t\t * Check for the state of the receive buffer.\n\t\t */\n\t\tif (atomic_read(&sk->sk_rmem_alloc) < (sk->sk_rcvbuf / 2) &&\n\t\t    (rose->condition & ROSE_COND_OWN_RX_BUSY)) {\n\t\t\trose->condition &= ~ROSE_COND_OWN_RX_BUSY;\n\t\t\trose->condition &= ~ROSE_COND_ACK_PENDING;\n\t\t\trose->vl         = rose->vr;\n\t\t\trose_write_internal(sk, ROSE_RR);\n\t\t\trose_stop_timer(sk);\t/* HB */\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\n\trose_start_heartbeat(sk);\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3567,
            "cve_id": "CVE-2022-2318",
            "code_snippet": "void rose_start_heartbeat(struct sock *sk)\n{\n\tsk_stop_timer(sk, &sk->sk_timer);\n\n\tsk->sk_timer.function = rose_heartbeat_expiry;\n\tsk->sk_timer.expires  = jiffies + 5 * HZ;\n\n\tsk_reset_timer(sk, &sk->sk_timer, sk->sk_timer.expires);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3605,
            "cve_id": "CVE-2022-24122",
            "code_snippet": "void put_ucounts(struct ucounts *ucounts)\n{\n\tunsigned long flags;\n\n\tif (atomic_dec_and_lock_irqsave(&ucounts->count, &ucounts_lock, flags)) {\n\t\thlist_del_init(&ucounts->node);\n\t\tspin_unlock_irqrestore(&ucounts_lock, flags);\n\t\tput_user_ns(ucounts->ns);\n\t\tkfree(ucounts);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3618,
            "cve_id": "CVE-2022-2602",
            "code_snippet": "void unix_gc(void)\n{\n\tstruct sk_buff *next_skb, *skb;\n\tstruct unix_sock *u;\n\tstruct unix_sock *next;\n\tstruct sk_buff_head hitlist;\n\tstruct list_head cursor;\n\tLIST_HEAD(not_cycle_list);\n\n\tspin_lock(&unix_gc_lock);\n\n\t/* Avoid a recursive GC. */\n\tif (gc_in_progress)\n\t\tgoto out;\n\n\t/* Paired with READ_ONCE() in wait_for_unix_gc(). */\n\tWRITE_ONCE(gc_in_progress, true);\n\n\t/* First, select candidates for garbage collection.  Only\n\t * in-flight sockets are considered, and from those only ones\n\t * which don't have any external reference.\n\t *\n\t * Holding unix_gc_lock will protect these candidates from\n\t * being detached, and hence from gaining an external\n\t * reference.  Since there are no possible receivers, all\n\t * buffers currently on the candidates' queues stay there\n\t * during the garbage collection.\n\t *\n\t * We also know that no new candidate can be added onto the\n\t * receive queues.  Other, non candidate sockets _can_ be\n\t * added to queue, so we must make sure only to touch\n\t * candidates.\n\t */\n\tlist_for_each_entry_safe(u, next, &gc_inflight_list, link) {\n\t\tlong total_refs;\n\t\tlong inflight_refs;\n\n\t\ttotal_refs = file_count(u->sk.sk_socket->file);\n\t\tinflight_refs = atomic_long_read(&u->inflight);\n\n\t\tBUG_ON(inflight_refs < 1);\n\t\tBUG_ON(total_refs < inflight_refs);\n\t\tif (total_refs == inflight_refs) {\n\t\t\tlist_move_tail(&u->link, &gc_candidates);\n\t\t\t__set_bit(UNIX_GC_CANDIDATE, &u->gc_flags);\n\t\t\t__set_bit(UNIX_GC_MAYBE_CYCLE, &u->gc_flags);\n\t\t}\n\t}\n\n\t/* Now remove all internal in-flight reference to children of\n\t * the candidates.\n\t */\n\tlist_for_each_entry(u, &gc_candidates, link)\n\t\tscan_children(&u->sk, dec_inflight, NULL);\n\n\t/* Restore the references for children of all candidates,\n\t * which have remaining references.  Do this recursively, so\n\t * only those remain, which form cyclic references.\n\t *\n\t * Use a \"cursor\" link, to make the list traversal safe, even\n\t * though elements might be moved about.\n\t */\n\tlist_add(&cursor, &gc_candidates);\n\twhile (cursor.next != &gc_candidates) {\n\t\tu = list_entry(cursor.next, struct unix_sock, link);\n\n\t\t/* Move cursor to after the current position. */\n\t\tlist_move(&cursor, &u->link);\n\n\t\tif (atomic_long_read(&u->inflight) > 0) {\n\t\t\tlist_move_tail(&u->link, &not_cycle_list);\n\t\t\t__clear_bit(UNIX_GC_MAYBE_CYCLE, &u->gc_flags);\n\t\t\tscan_children(&u->sk, inc_inflight_move_tail, NULL);\n\t\t}\n\t}\n\tlist_del(&cursor);\n\n\t/* Now gc_candidates contains only garbage.  Restore original\n\t * inflight counters for these as well, and remove the skbuffs\n\t * which are creating the cycle(s).\n\t */\n\tskb_queue_head_init(&hitlist);\n\tlist_for_each_entry(u, &gc_candidates, link)\n\t\tscan_children(&u->sk, inc_inflight, &hitlist);\n\n\t/* not_cycle_list contains those sockets which do not make up a\n\t * cycle.  Restore these to the inflight list.\n\t */\n\twhile (!list_empty(&not_cycle_list)) {\n\t\tu = list_entry(not_cycle_list.next, struct unix_sock, link);\n\t\t__clear_bit(UNIX_GC_CANDIDATE, &u->gc_flags);\n\t\tlist_move_tail(&u->link, &gc_inflight_list);\n\t}\n\n\tspin_unlock(&unix_gc_lock);\n\n\t/* We need io_uring to clean its registered files, ignore all io_uring\n\t * originated skbs. It's fine as io_uring doesn't keep references to\n\t * other io_uring instances and so killing all other files in the cycle\n\t * will put all io_uring references forcing it to go through normal\n\t * release.path eventually putting registered files.\n\t */\n\tskb_queue_walk_safe(&hitlist, skb, next_skb) {\n\t\tif (skb->scm_io_uring) {\n\t\t\t__skb_unlink(skb, &hitlist);\n\t\t\tskb_queue_tail(&skb->sk->sk_receive_queue, skb);\n\t\t}\n\t}\n\n\t/* Here we are. Hitlist is filled. Die. */\n\t__skb_queue_purge(&hitlist);\n\n\tspin_lock(&unix_gc_lock);\n\n\t/* There could be io_uring registered files, just push them back to\n\t * the inflight list\n\t */\n\tlist_for_each_entry_safe(u, next, &gc_candidates, link)\n\t\tlist_move_tail(&u->link, &gc_inflight_list);\n\n\t/* All candidates should have been detached by now. */\n\tBUG_ON(!list_empty(&gc_candidates));\n\n\t/* Paired with READ_ONCE() in wait_for_unix_gc(). */\n\tWRITE_ONCE(gc_in_progress, false);\n\n\twake_up(&unix_gc_wait);\n\n out:\n\tspin_unlock(&unix_gc_lock);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3642,
            "cve_id": "CVE-2022-28893",
            "code_snippet": "static int xs_local_send_request(struct rpc_rqst *req)\n{\n\tstruct rpc_xprt *xprt = req->rq_xprt;\n\tstruct sock_xprt *transport =\n\t\t\t\tcontainer_of(xprt, struct sock_xprt, xprt);\n\tstruct xdr_buf *xdr = &req->rq_snd_buf;\n\trpc_fraghdr rm = xs_stream_record_marker(xdr);\n\tunsigned int msglen = rm ? req->rq_slen + sizeof(rm) : req->rq_slen;\n\tstruct msghdr msg = {\n\t\t.msg_flags\t= XS_SENDMSG_FLAGS,\n\t};\n\tbool vm_wait;\n\tunsigned int sent;\n\tint status;\n\n\t/* Close the stream if the previous transmission was incomplete */\n\tif (xs_send_request_was_aborted(transport, req)) {\n\t\txprt_force_disconnect(xprt);\n\t\treturn -ENOTCONN;\n\t}\n\n\txs_pktdump(\"packet data:\",\n\t\t\treq->rq_svec->iov_base, req->rq_svec->iov_len);\n\n\tvm_wait = sk_stream_is_writeable(transport->inet) ? true : false;\n\n\treq->rq_xtime = ktime_get();\n\tstatus = xprt_sock_sendmsg(transport->sock, &msg, xdr,\n\t\t\t\t   transport->xmit.offset, rm, &sent);\n\tdprintk(\"RPC:       %s(%u) = %d\\n\",\n\t\t\t__func__, xdr->len - transport->xmit.offset, status);\n\n\tif (likely(sent > 0) || status == 0) {\n\t\ttransport->xmit.offset += sent;\n\t\treq->rq_bytes_sent = transport->xmit.offset;\n\t\tif (likely(req->rq_bytes_sent >= msglen)) {\n\t\t\treq->rq_xmit_bytes_sent += transport->xmit.offset;\n\t\t\ttransport->xmit.offset = 0;\n\t\t\treturn 0;\n\t\t}\n\t\tstatus = -EAGAIN;\n\t\tvm_wait = false;\n\t}\n\n\tswitch (status) {\n\tcase -EAGAIN:\n\t\tstatus = xs_stream_nospace(req, vm_wait);\n\t\tbreak;\n\tdefault:\n\t\tdprintk(\"RPC:       sendmsg returned unrecognized error %d\\n\",\n\t\t\t-status);\n\t\tfallthrough;\n\tcase -EPIPE:\n\t\txprt_force_disconnect(xprt);\n\t\tstatus = -ENOTCONN;\n\t}\n\n\treturn status;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3649,
            "cve_id": "CVE-2022-2938",
            "code_snippet": "static void cgroup_pressure_release(struct kernfs_open_file *of)\n{\n\tstruct cgroup_file_ctx *ctx = of->priv;\n\n\tpsi_trigger_destroy(ctx->psi.trigger);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3650,
            "cve_id": "CVE-2022-2938",
            "code_snippet": "static int psi_fop_release(struct inode *inode, struct file *file)\n{\n\tstruct seq_file *seq = file->private_data;\n\n\tpsi_trigger_destroy(seq->private);\n\treturn single_release(inode, file);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3661,
            "cve_id": "CVE-2022-2977",
            "code_snippet": "static int tpm_add_char_device(struct tpm_chip *chip)\n{\n\tint rc;\n\n\trc = cdev_device_add(&chip->cdev, &chip->dev);\n\tif (rc) {\n\t\tdev_err(&chip->dev,\n\t\t\t\"unable to cdev_device_add() %s, major %d, minor %d, err=%d\\n\",\n\t\t\tdev_name(&chip->dev), MAJOR(chip->dev.devt),\n\t\t\tMINOR(chip->dev.devt), rc);\n\t\treturn rc;\n\t}\n\n\tif (chip->flags & TPM_CHIP_FLAG_TPM2 && !tpm_is_firmware_upgrade(chip)) {\n\t\trc = tpm_devs_add(chip);\n\t\tif (rc)\n\t\t\tgoto err_del_cdev;\n\t}\n\n\t/* Make the chip available. */\n\tmutex_lock(&idr_lock);\n\tidr_replace(&dev_nums_idr, chip, chip->dev_num);\n\tmutex_unlock(&idr_lock);\n\n\treturn 0;\n\nerr_del_cdev:\n\tcdev_device_del(&chip->cdev, &chip->dev);\n\treturn rc;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3699,
            "cve_id": "CVE-2022-3176",
            "code_snippet": "static int io_poll_wake(struct wait_queue_entry *wait, unsigned mode, int sync,\n\t\t\tvoid *key)\n{\n\tstruct io_kiocb *req = wait->private;\n\tstruct io_poll_iocb *poll = container_of(wait, struct io_poll_iocb,\n\t\t\t\t\t\t wait);\n\t__poll_t mask = key_to_poll(key);\n\n\tif (unlikely(mask & POLLFREE)) {\n\t\tio_poll_mark_cancelled(req);\n\t\t/* we have to kick tw in case it's not already */\n\t\tio_poll_execute(req, 0);\n\n\t\t/*\n\t\t * If the waitqueue is being freed early but someone is already\n\t\t * holds ownership over it, we have to tear down the request as\n\t\t * best we can. That means immediately removing the request from\n\t\t * its waitqueue and preventing all further accesses to the\n\t\t * waitqueue via the request.\n\t\t */\n\t\tlist_del_init(&poll->wait.entry);\n\n\t\t/*\n\t\t * Careful: this *must* be the last step, since as soon\n\t\t * as req->head is NULL'ed out, the request can be\n\t\t * completed and freed, since aio_poll_complete_work()\n\t\t * will no longer need to take the waitqueue lock.\n\t\t */\n\t\tsmp_store_release(&poll->head, NULL);\n\t\treturn 1;\n\t}\n\n\t/* for instances that support it check for an event match first */\n\tif (mask && !(mask & poll->events))\n\t\treturn 0;\n\n\tif (io_poll_get_ownership(req)) {\n\t\t/* optional, saves extra locking for removal in tw handler */\n\t\tif (mask && poll->events & EPOLLONESHOT) {\n\t\t\tlist_del_init(&poll->wait.entry);\n\t\t\tpoll->head = NULL;\n\t\t}\n\t\t__io_poll_execute(req, mask);\n\t}\n\treturn 1;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3703,
            "cve_id": "CVE-2022-32250",
            "code_snippet": "static struct nft_expr *nft_expr_init(const struct nft_ctx *ctx,\n\t\t\t\t      const struct nlattr *nla)\n{\n\tstruct nft_expr_info expr_info;\n\tstruct nft_expr *expr;\n\tstruct module *owner;\n\tint err;\n\n\terr = nf_tables_expr_parse(ctx, nla, &expr_info);\n\tif (err < 0)\n\t\tgoto err_expr_parse;\n\n\terr = -EOPNOTSUPP;\n\tif (!(expr_info.ops->type->flags & NFT_EXPR_STATEFUL))\n\t\tgoto err_expr_stateful;\n\n\terr = -ENOMEM;\n\texpr = kzalloc(expr_info.ops->size, GFP_KERNEL_ACCOUNT);\n\tif (expr == NULL)\n\t\tgoto err_expr_stateful;\n\n\terr = nf_tables_newexpr(ctx, &expr_info, expr);\n\tif (err < 0)\n\t\tgoto err_expr_new;\n\n\treturn expr;\nerr_expr_new:\n\tkfree(expr);\nerr_expr_stateful:\n\towner = expr_info.ops->type->owner;\n\tif (expr_info.ops->type->release_ops)\n\t\texpr_info.ops->type->release_ops(expr_info.ops);\n\n\tmodule_put(owner);\nerr_expr_parse:\n\treturn ERR_PTR(err);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3730,
            "cve_id": "CVE-2022-3424",
            "code_snippet": "int gru_set_context_option(unsigned long arg)\n{\n\tstruct gru_thread_state *gts;\n\tstruct gru_set_context_option_req req;\n\tint ret = 0;\n\n\tSTAT(set_context_option);\n\tif (copy_from_user(&req, (void __user *)arg, sizeof(req)))\n\t\treturn -EFAULT;\n\tgru_dbg(grudev, \"op %d, gseg 0x%lx, value1 0x%lx\\n\", req.op, req.gseg, req.val1);\n\n\tgts = gru_find_lock_gts(req.gseg);\n\tif (!gts) {\n\t\tgts = gru_alloc_locked_gts(req.gseg);\n\t\tif (IS_ERR(gts))\n\t\t\treturn PTR_ERR(gts);\n\t}\n\n\tswitch (req.op) {\n\tcase sco_blade_chiplet:\n\t\t/* Select blade/chiplet for GRU context */\n\t\tif (req.val0 < -1 || req.val0 >= GRU_CHIPLETS_PER_HUB ||\n\t\t    req.val1 < -1 || req.val1 >= GRU_MAX_BLADES ||\n\t\t    (req.val1 >= 0 && !gru_base[req.val1])) {\n\t\t\tret = -EINVAL;\n\t\t} else {\n\t\t\tgts->ts_user_blade_id = req.val1;\n\t\t\tgts->ts_user_chiplet_id = req.val0;\n\t\t\tif (gru_check_context_placement(gts)) {\n\t\t\t\tgru_unlock_gts(gts);\n\t\t\t\tgru_unload_context(gts, 1);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase sco_gseg_owner:\n \t\t/* Register the current task as the GSEG owner */\n\t\tgts->ts_tgid_owner = current->tgid;\n\t\tbreak;\n\tcase sco_cch_req_slice:\n \t\t/* Set the CCH slice option */\n\t\tgts->ts_cch_req_slice = req.val1 & 3;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\tgru_unlock_gts(gts);\n\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3740,
            "cve_id": "CVE-2022-3523",
            "code_snippet": "unsigned long\nkvmppc_h_svm_page_out(struct kvm *kvm, unsigned long gpa,\n\t\t      unsigned long flags, unsigned long page_shift)\n{\n\tunsigned long gfn = gpa >> page_shift;\n\tunsigned long start, end;\n\tstruct vm_area_struct *vma;\n\tint srcu_idx;\n\tint ret;\n\n\tif (!(kvm->arch.secure_guest & KVMPPC_SECURE_INIT_START))\n\t\treturn H_UNSUPPORTED;\n\n\tif (page_shift != PAGE_SHIFT)\n\t\treturn H_P3;\n\n\tif (flags)\n\t\treturn H_P2;\n\n\tret = H_PARAMETER;\n\tsrcu_idx = srcu_read_lock(&kvm->srcu);\n\tmmap_read_lock(kvm->mm);\n\tstart = gfn_to_hva(kvm, gfn);\n\tif (kvm_is_error_hva(start))\n\t\tgoto out;\n\n\tend = start + (1UL << page_shift);\n\tvma = find_vma_intersection(kvm->mm, start, end);\n\tif (!vma || vma->vm_start > start || vma->vm_end < end)\n\t\tgoto out;\n\n\tif (!kvmppc_svm_page_out(vma, start, end, page_shift, kvm, gpa, NULL))\n\t\tret = H_SUCCESS;\nout:\n\tmmap_read_unlock(kvm->mm);\n\tsrcu_read_unlock(&kvm->srcu, srcu_idx);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3741,
            "cve_id": "CVE-2022-3523",
            "code_snippet": "static vm_fault_t kvmppc_uvmem_migrate_to_ram(struct vm_fault *vmf)\n{\n\tstruct kvmppc_uvmem_page_pvt *pvt = vmf->page->zone_device_data;\n\n\tif (kvmppc_svm_page_out(vmf->vma, vmf->address,\n\t\t\t\tvmf->address + PAGE_SIZE, PAGE_SHIFT,\n\t\t\t\tpvt->kvm, pvt->gpa, vmf->page))\n\t\treturn VM_FAULT_SIGBUS;\n\telse\n\t\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3742,
            "cve_id": "CVE-2022-3523",
            "code_snippet": "static int kvmppc_svm_page_in(struct vm_area_struct *vma,\n\t\tunsigned long start,\n\t\tunsigned long end, unsigned long gpa, struct kvm *kvm,\n\t\tunsigned long page_shift,\n\t\tbool pagein)\n{\n\tunsigned long src_pfn, dst_pfn = 0;\n\tstruct migrate_vma mig = { 0 };\n\tstruct page *spage;\n\tunsigned long pfn;\n\tstruct page *dpage;\n\tint ret = 0;\n\n\tmemset(&mig, 0, sizeof(mig));\n\tmig.vma = vma;\n\tmig.start = start;\n\tmig.end = end;\n\tmig.src = &src_pfn;\n\tmig.dst = &dst_pfn;\n\tmig.flags = MIGRATE_VMA_SELECT_SYSTEM;\n\n\tret = migrate_vma_setup(&mig);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!(*mig.src & MIGRATE_PFN_MIGRATE)) {\n\t\tret = -1;\n\t\tgoto out_finalize;\n\t}\n\n\tdpage = kvmppc_uvmem_get_page(gpa, kvm);\n\tif (!dpage) {\n\t\tret = -1;\n\t\tgoto out_finalize;\n\t}\n\n\tif (pagein) {\n\t\tpfn = *mig.src >> MIGRATE_PFN_SHIFT;\n\t\tspage = migrate_pfn_to_page(*mig.src);\n\t\tif (spage) {\n\t\t\tret = uv_page_in(kvm->arch.lpid, pfn << page_shift,\n\t\t\t\t\tgpa, 0, page_shift);\n\t\t\tif (ret)\n\t\t\t\tgoto out_finalize;\n\t\t}\n\t}\n\n\t*mig.dst = migrate_pfn(page_to_pfn(dpage));\n\tmigrate_vma_pages(&mig);\nout_finalize:\n\tmigrate_vma_finalize(&mig);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3743,
            "cve_id": "CVE-2022-3523",
            "code_snippet": "static vm_fault_t svm_migrate_to_ram(struct vm_fault *vmf)\n{\n\tunsigned long addr = vmf->address;\n\tstruct vm_area_struct *vma;\n\tenum svm_work_list_ops op;\n\tstruct svm_range *parent;\n\tstruct svm_range *prange;\n\tstruct kfd_process *p;\n\tstruct mm_struct *mm;\n\tint r = 0;\n\n\tvma = vmf->vma;\n\tmm = vma->vm_mm;\n\n\tp = kfd_lookup_process_by_mm(vma->vm_mm);\n\tif (!p) {\n\t\tpr_debug(\"failed find process at fault address 0x%lx\\n\", addr);\n\t\treturn VM_FAULT_SIGBUS;\n\t}\n\tif (READ_ONCE(p->svms.faulting_task) == current) {\n\t\tpr_debug(\"skipping ram migration\\n\");\n\t\tkfd_unref_process(p);\n\t\treturn 0;\n\t}\n\taddr >>= PAGE_SHIFT;\n\tpr_debug(\"CPU page fault svms 0x%p address 0x%lx\\n\", &p->svms, addr);\n\n\tmutex_lock(&p->svms.lock);\n\n\tprange = svm_range_from_addr(&p->svms, addr, &parent);\n\tif (!prange) {\n\t\tpr_debug(\"cannot find svm range at 0x%lx\\n\", addr);\n\t\tr = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tmutex_lock(&parent->migrate_mutex);\n\tif (prange != parent)\n\t\tmutex_lock_nested(&prange->migrate_mutex, 1);\n\n\tif (!prange->actual_loc)\n\t\tgoto out_unlock_prange;\n\n\tsvm_range_lock(parent);\n\tif (prange != parent)\n\t\tmutex_lock_nested(&prange->lock, 1);\n\tr = svm_range_split_by_granularity(p, mm, addr, parent, prange);\n\tif (prange != parent)\n\t\tmutex_unlock(&prange->lock);\n\tsvm_range_unlock(parent);\n\tif (r) {\n\t\tpr_debug(\"failed %d to split range by granularity\\n\", r);\n\t\tgoto out_unlock_prange;\n\t}\n\n\tr = svm_migrate_vram_to_ram(prange, mm, KFD_MIGRATE_TRIGGER_PAGEFAULT_CPU,\n\t\t\t\tvmf->page);\n\tif (r)\n\t\tpr_debug(\"failed %d migrate 0x%p [0x%lx 0x%lx] to ram\\n\", r,\n\t\t\t prange, prange->start, prange->last);\n\n\t/* xnack on, update mapping on GPUs with ACCESS_IN_PLACE */\n\tif (p->xnack_enabled && parent == prange)\n\t\top = SVM_OP_UPDATE_RANGE_NOTIFIER_AND_MAP;\n\telse\n\t\top = SVM_OP_UPDATE_RANGE_NOTIFIER;\n\tsvm_range_add_list_work(&p->svms, parent, mm, op);\n\tschedule_deferred_list_work(&p->svms);\n\nout_unlock_prange:\n\tif (prange != parent)\n\t\tmutex_unlock(&prange->migrate_mutex);\n\tmutex_unlock(&parent->migrate_mutex);\nout:\n\tmutex_unlock(&p->svms.lock);\n\tkfd_unref_process(p);\n\n\tpr_debug(\"CPU fault svms 0x%p address 0x%lx done\\n\", &p->svms, addr);\n\n\treturn r ? VM_FAULT_SIGBUS : 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3744,
            "cve_id": "CVE-2022-3523",
            "code_snippet": "static long\nsvm_migrate_vma_to_vram(struct amdgpu_device *adev, struct svm_range *prange,\n\t\t\tstruct vm_area_struct *vma, uint64_t start,\n\t\t\tuint64_t end, uint32_t trigger)\n{\n\tstruct kfd_process *p = container_of(prange->svms, struct kfd_process, svms);\n\tuint64_t npages = (end - start) >> PAGE_SHIFT;\n\tstruct kfd_process_device *pdd;\n\tstruct dma_fence *mfence = NULL;\n\tstruct migrate_vma migrate = { 0 };\n\tunsigned long cpages = 0;\n\tdma_addr_t *scratch;\n\tvoid *buf;\n\tint r = -ENOMEM;\n\n\tmemset(&migrate, 0, sizeof(migrate));\n\tmigrate.vma = vma;\n\tmigrate.start = start;\n\tmigrate.end = end;\n\tmigrate.flags = MIGRATE_VMA_SELECT_SYSTEM;\n\tmigrate.pgmap_owner = SVM_ADEV_PGMAP_OWNER(adev);\n\n\tbuf = kvcalloc(npages,\n\t\t       2 * sizeof(*migrate.src) + sizeof(uint64_t) + sizeof(dma_addr_t),\n\t\t       GFP_KERNEL);\n\tif (!buf)\n\t\tgoto out;\n\n\tmigrate.src = buf;\n\tmigrate.dst = migrate.src + npages;\n\tscratch = (dma_addr_t *)(migrate.dst + npages);\n\n\tkfd_smi_event_migration_start(adev->kfd.dev, p->lead_thread->pid,\n\t\t\t\t      start >> PAGE_SHIFT, end >> PAGE_SHIFT,\n\t\t\t\t      0, adev->kfd.dev->id, prange->prefetch_loc,\n\t\t\t\t      prange->preferred_loc, trigger);\n\n\tr = migrate_vma_setup(&migrate);\n\tif (r) {\n\t\tdev_err(adev->dev, \"%s: vma setup fail %d range [0x%lx 0x%lx]\\n\",\n\t\t\t__func__, r, prange->start, prange->last);\n\t\tgoto out_free;\n\t}\n\n\tcpages = migrate.cpages;\n\tif (!cpages) {\n\t\tpr_debug(\"failed collect migrate sys pages [0x%lx 0x%lx]\\n\",\n\t\t\t prange->start, prange->last);\n\t\tgoto out_free;\n\t}\n\tif (cpages != npages)\n\t\tpr_debug(\"partial migration, 0x%lx/0x%llx pages migrated\\n\",\n\t\t\t cpages, npages);\n\telse\n\t\tpr_debug(\"0x%lx pages migrated\\n\", cpages);\n\n\tr = svm_migrate_copy_to_vram(adev, prange, &migrate, &mfence, scratch);\n\tmigrate_vma_pages(&migrate);\n\n\tpr_debug(\"successful/cpages/npages 0x%lx/0x%lx/0x%lx\\n\",\n\t\tsvm_migrate_successful_pages(&migrate), cpages, migrate.npages);\n\n\tsvm_migrate_copy_done(adev, mfence);\n\tmigrate_vma_finalize(&migrate);\n\n\tkfd_smi_event_migration_end(adev->kfd.dev, p->lead_thread->pid,\n\t\t\t\t    start >> PAGE_SHIFT, end >> PAGE_SHIFT,\n\t\t\t\t    0, adev->kfd.dev->id, trigger);\n\n\tsvm_range_dma_unmap(adev->dev, scratch, 0, npages);\n\tsvm_range_free_dma_mappings(prange);\n\nout_free:\n\tkvfree(buf);\nout:\n\tif (!r && cpages) {\n\t\tpdd = svm_range_get_pdd_by_adev(prange, adev);\n\t\tif (pdd)\n\t\t\tWRITE_ONCE(pdd->page_in, pdd->page_in + cpages);\n\n\t\treturn cpages;\n\t}\n\treturn r;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3745,
            "cve_id": "CVE-2022-3523",
            "code_snippet": "static int\nsvm_migrate_vram_to_vram(struct svm_range *prange, uint32_t best_loc,\n\t\t\t struct mm_struct *mm, uint32_t trigger)\n{\n\tint r, retries = 3;\n\n\t/*\n\t * TODO: for both devices with PCIe large bar or on same xgmi hive, skip\n\t * system memory as migration bridge\n\t */\n\n\tpr_debug(\"from gpu 0x%x to gpu 0x%x\\n\", prange->actual_loc, best_loc);\n\n\tdo {\n\t\tr = svm_migrate_vram_to_ram(prange, mm, trigger, NULL);\n\t\tif (r)\n\t\t\treturn r;\n\t} while (prange->actual_loc && --retries);\n\n\tif (prange->actual_loc)\n\t\treturn -EDEADLK;\n\n\treturn svm_migrate_ram_to_vram(prange, best_loc, mm, trigger);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3746,
            "cve_id": "CVE-2022-3523",
            "code_snippet": "static void svm_range_evict_svm_bo_worker(struct work_struct *work)\n{\n\tstruct svm_range_bo *svm_bo;\n\tstruct mm_struct *mm;\n\tint r = 0;\n\n\tsvm_bo = container_of(work, struct svm_range_bo, eviction_work);\n\tif (!svm_bo_ref_unless_zero(svm_bo))\n\t\treturn; /* svm_bo was freed while eviction was pending */\n\n\tif (mmget_not_zero(svm_bo->eviction_fence->mm)) {\n\t\tmm = svm_bo->eviction_fence->mm;\n\t} else {\n\t\tsvm_range_bo_unref(svm_bo);\n\t\treturn;\n\t}\n\n\tmmap_read_lock(mm);\n\tspin_lock(&svm_bo->list_lock);\n\twhile (!list_empty(&svm_bo->range_list) && !r) {\n\t\tstruct svm_range *prange =\n\t\t\t\tlist_first_entry(&svm_bo->range_list,\n\t\t\t\t\t\tstruct svm_range, svm_bo_list);\n\t\tint retries = 3;\n\n\t\tlist_del_init(&prange->svm_bo_list);\n\t\tspin_unlock(&svm_bo->list_lock);\n\n\t\tpr_debug(\"svms 0x%p [0x%lx 0x%lx]\\n\", prange->svms,\n\t\t\t prange->start, prange->last);\n\n\t\tmutex_lock(&prange->migrate_mutex);\n\t\tdo {\n\t\t\tr = svm_migrate_vram_to_ram(prange, mm,\n\t\t\t\t\tKFD_MIGRATE_TRIGGER_TTM_EVICTION, NULL);\n\t\t} while (!r && prange->actual_loc && --retries);\n\n\t\tif (!r && prange->actual_loc)\n\t\t\tpr_info_once(\"Migration failed during eviction\");\n\n\t\tif (!prange->actual_loc) {\n\t\t\tmutex_lock(&prange->lock);\n\t\t\tprange->svm_bo = NULL;\n\t\t\tmutex_unlock(&prange->lock);\n\t\t}\n\t\tmutex_unlock(&prange->migrate_mutex);\n\n\t\tspin_lock(&svm_bo->list_lock);\n\t}\n\tspin_unlock(&svm_bo->list_lock);\n\tmmap_read_unlock(mm);\n\tmmput(mm);\n\n\tdma_fence_signal(&svm_bo->eviction_fence->base);\n\n\t/* This is the last reference to svm_bo, after svm_range_vram_node_free\n\t * has been called in svm_migrate_vram_to_ram\n\t */\n\tWARN_ONCE(!r && kref_read(&svm_bo->kref) != 1, \"This was not the last reference\\n\");\n\tsvm_range_bo_unref(svm_bo);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3747,
            "cve_id": "CVE-2022-3523",
            "code_snippet": "static int\nsvm_range_trigger_migration(struct mm_struct *mm, struct svm_range *prange,\n\t\t\t    bool *migrated)\n{\n\tuint32_t best_loc;\n\tint r = 0;\n\n\t*migrated = false;\n\tbest_loc = svm_range_best_prefetch_location(prange);\n\n\tif (best_loc == KFD_IOCTL_SVM_LOCATION_UNDEFINED ||\n\t    best_loc == prange->actual_loc)\n\t\treturn 0;\n\n\tif (!best_loc) {\n\t\tr = svm_migrate_vram_to_ram(prange, mm,\n\t\t\t\t\tKFD_MIGRATE_TRIGGER_PREFETCH, NULL);\n\t\t*migrated = !r;\n\t\treturn r;\n\t}\n\n\tr = svm_migrate_to_vram(prange, best_loc, mm, KFD_MIGRATE_TRIGGER_PREFETCH);\n\t*migrated = !r;\n\n\treturn r;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3763,
            "cve_id": "CVE-2022-3534",
            "code_snippet": "void btf_dump__free(struct btf_dump *d)\n{\n\tint i;\n\n\tif (IS_ERR_OR_NULL(d))\n\t\treturn;\n\n\tfree(d->type_states);\n\tif (d->cached_names) {\n\t\t/* any set cached name is owned by us and should be freed */\n\t\tfor (i = 0; i <= d->last_id; i++) {\n\t\t\tif (d->cached_names[i])\n\t\t\t\tfree((void *)d->cached_names[i]);\n\t\t}\n\t}\n\tfree(d->cached_names);\n\tfree(d->emit_queue);\n\tfree(d->decl_stack);\n\tbtf_dump_free_names(d->type_names);\n\tbtf_dump_free_names(d->ident_names);\n\n\tfree(d);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3813,
            "cve_id": "CVE-2022-38457",
            "code_snippet": "static int vmw_cmd_dx_bind_streamoutput(struct vmw_private *dev_priv,\n\t\t\t\t\tstruct vmw_sw_context *sw_context,\n\t\t\t\t\tSVGA3dCmdHeader *header)\n{\n\tstruct vmw_ctx_validation_info *ctx_node = sw_context->dx_ctx_node;\n\tstruct vmw_resource *res;\n\tstruct {\n\t\tSVGA3dCmdHeader header;\n\t\tSVGA3dCmdDXBindStreamOutput body;\n\t} *cmd = container_of(header, typeof(*cmd), header);\n\tint ret;\n\n\tif (!has_sm5_context(dev_priv))\n\t\treturn -EINVAL;\n\n\tif (!ctx_node) {\n\t\tDRM_ERROR(\"DX Context not set.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tres = vmw_dx_streamoutput_lookup(vmw_context_res_man(ctx_node->ctx),\n\t\t\t\t\t cmd->body.soid);\n\tif (IS_ERR(res)) {\n\t\tDRM_ERROR(\"Could not find streamoutput to bind.\\n\");\n\t\treturn PTR_ERR(res);\n\t}\n\n\tvmw_dx_streamoutput_set_size(res, cmd->body.sizeInBytes);\n\n\tret = vmw_execbuf_res_val_add(sw_context, res, VMW_RES_DIRTY_NONE,\n\t\t\t\t      vmw_val_add_flag_noctx);\n\tif (ret) {\n\t\tDRM_ERROR(\"Error creating resource validation node.\\n\");\n\t\treturn ret;\n\t}\n\n\treturn vmw_cmd_res_switch_backup(dev_priv, sw_context, res,\n\t\t\t\t\t &cmd->body.mobid,\n\t\t\t\t\t cmd->body.offsetInBytes);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3814,
            "cve_id": "CVE-2022-38457",
            "code_snippet": "static int vmw_translate_guest_ptr(struct vmw_private *dev_priv,\n\t\t\t\t   struct vmw_sw_context *sw_context,\n\t\t\t\t   SVGAGuestPtr *ptr,\n\t\t\t\t   struct vmw_buffer_object **vmw_bo_p)\n{\n\tstruct vmw_buffer_object *vmw_bo;\n\tuint32_t handle = ptr->gmrId;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use GMR region.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo, false, false);\n\tttm_bo_put(&vmw_bo->base);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->location = ptr;\n\treloc->vbo = vmw_bo;\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3815,
            "cve_id": "CVE-2022-38457",
            "code_snippet": "static int vmw_execbuf_tie_context(struct vmw_private *dev_priv,\n\t\t\t\t   struct vmw_sw_context *sw_context,\n\t\t\t\t   uint32_t handle)\n{\n\tstruct vmw_resource *res;\n\tint ret;\n\tunsigned int size;\n\n\tif (handle == SVGA3D_INVALID_ID)\n\t\treturn 0;\n\n\tsize = vmw_execbuf_res_size(dev_priv, vmw_res_dx_context);\n\tret = vmw_validation_preload_res(sw_context->ctx, size);\n\tif (ret)\n\t\treturn ret;\n\n\tret = vmw_user_resource_lookup_handle\n\t\t(dev_priv, sw_context->fp->tfile, handle,\n\t\t user_context_converter, &res);\n\tif (ret != 0) {\n\t\tVMW_DEBUG_USER(\"Could not find or user DX context 0x%08x.\\n\",\n\t\t\t       (unsigned int) handle);\n\t\treturn ret;\n\t}\n\n\tret = vmw_execbuf_res_val_add(sw_context, res, VMW_RES_DIRTY_SET,\n\t\t\t\t      vmw_val_add_flag_none);\n\tif (unlikely(ret != 0)) {\n\t\tvmw_resource_unreference(&res);\n\t\treturn ret;\n\t}\n\n\tsw_context->dx_ctx_node = vmw_execbuf_info_from_res(sw_context, res);\n\tsw_context->man = vmw_context_res_man(res);\n\n\tvmw_resource_unreference(&res);\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3816,
            "cve_id": "CVE-2022-38457",
            "code_snippet": "static int vmw_cmd_dx_bind_shader(struct vmw_private *dev_priv,\n\t\t\t\t  struct vmw_sw_context *sw_context,\n\t\t\t\t  SVGA3dCmdHeader *header)\n{\n\tstruct vmw_resource *ctx;\n\tstruct vmw_resource *res;\n\tVMW_DECLARE_CMD_VAR(*cmd, SVGA3dCmdDXBindShader) =\n\t\tcontainer_of(header, typeof(*cmd), header);\n\tint ret;\n\n\tif (cmd->body.cid != SVGA3D_INVALID_ID) {\n\t\tret = vmw_cmd_res_check(dev_priv, sw_context, vmw_res_context,\n\t\t\t\t\tVMW_RES_DIRTY_SET,\n\t\t\t\t\tuser_context_converter, &cmd->body.cid,\n\t\t\t\t\t&ctx);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} else {\n\t\tstruct vmw_ctx_validation_info *ctx_node =\n\t\t\tVMW_GET_CTX_NODE(sw_context);\n\n\t\tif (!ctx_node)\n\t\t\treturn -EINVAL;\n\n\t\tctx = ctx_node->ctx;\n\t}\n\n\tres = vmw_shader_lookup(vmw_context_res_man(ctx), cmd->body.shid, 0);\n\tif (IS_ERR(res)) {\n\t\tVMW_DEBUG_USER(\"Could not find shader to bind.\\n\");\n\t\treturn PTR_ERR(res);\n\t}\n\n\tret = vmw_execbuf_res_val_add(sw_context, res, VMW_RES_DIRTY_NONE,\n\t\t\t\t      vmw_val_add_flag_noctx);\n\tif (ret) {\n\t\tVMW_DEBUG_USER(\"Error creating resource validation node.\\n\");\n\t\treturn ret;\n\t}\n\n\treturn vmw_cmd_res_switch_backup(dev_priv, sw_context, res,\n\t\t\t\t\t &cmd->body.mobid,\n\t\t\t\t\t cmd->body.offsetInBytes);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3817,
            "cve_id": "CVE-2022-38457",
            "code_snippet": "static int vmw_cmd_dx_set_shader(struct vmw_private *dev_priv,\n\t\t\t\t struct vmw_sw_context *sw_context,\n\t\t\t\t SVGA3dCmdHeader *header)\n{\n\tVMW_DECLARE_CMD_VAR(*cmd, SVGA3dCmdDXSetShader);\n\tSVGA3dShaderType max_allowed = has_sm5_context(dev_priv) ?\n\t\tSVGA3D_SHADERTYPE_MAX : SVGA3D_SHADERTYPE_DX10_MAX;\n\tstruct vmw_resource *res = NULL;\n\tstruct vmw_ctx_validation_info *ctx_node = VMW_GET_CTX_NODE(sw_context);\n\tstruct vmw_ctx_bindinfo_shader binding;\n\tint ret = 0;\n\n\tif (!ctx_node)\n\t\treturn -EINVAL;\n\n\tcmd = container_of(header, typeof(*cmd), header);\n\n\tif (cmd->body.type >= max_allowed ||\n\t    cmd->body.type < SVGA3D_SHADERTYPE_MIN) {\n\t\tVMW_DEBUG_USER(\"Illegal shader type %u.\\n\",\n\t\t\t       (unsigned int) cmd->body.type);\n\t\treturn -EINVAL;\n\t}\n\n\tif (cmd->body.shaderId != SVGA3D_INVALID_ID) {\n\t\tres = vmw_shader_lookup(sw_context->man, cmd->body.shaderId, 0);\n\t\tif (IS_ERR(res)) {\n\t\t\tVMW_DEBUG_USER(\"Could not find shader for binding.\\n\");\n\t\t\treturn PTR_ERR(res);\n\t\t}\n\n\t\tret = vmw_execbuf_res_val_add(sw_context, res,\n\t\t\t\t\t      VMW_RES_DIRTY_NONE,\n\t\t\t\t\t      vmw_val_add_flag_noctx);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tbinding.bi.ctx = ctx_node->ctx;\n\tbinding.bi.res = res;\n\tbinding.bi.bt = vmw_ctx_binding_dx_shader;\n\tbinding.shader_slot = cmd->body.type - SVGA3D_SHADERTYPE_MIN;\n\n\tvmw_binding_add(ctx_node->staged, &binding.bi, binding.shader_slot, 0);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3834,
            "cve_id": "CVE-2022-3977",
            "code_snippet": "static void __mctp_key_remove(struct mctp_sk_key *key, struct net *net,\n\t\t\t      unsigned long flags, unsigned long reason)\n__releases(&key->lock)\n__must_hold(&net->mctp.keys_lock)\n{\n\tstruct sk_buff *skb;\n\n\ttrace_mctp_key_release(key, reason);\n\tskb = key->reasm_head;\n\tkey->reasm_head = NULL;\n\tkey->reasm_dead = true;\n\tkey->valid = false;\n\tmctp_dev_release_key(key->dev, key);\n\tspin_unlock_irqrestore(&key->lock, flags);\n\n\tif (!hlist_unhashed(&key->hlist)) {\n\t\thlist_del_init(&key->hlist);\n\t\thlist_del_init(&key->sklist);\n\t\t/* unref for the lists */\n\t\tmctp_key_unref(key);\n\t}\n\n\tkfree_skb(skb);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3838,
            "cve_id": "CVE-2022-40133",
            "code_snippet": "static int vmw_cmd_dx_bind_streamoutput(struct vmw_private *dev_priv,\n\t\t\t\t\tstruct vmw_sw_context *sw_context,\n\t\t\t\t\tSVGA3dCmdHeader *header)\n{\n\tstruct vmw_ctx_validation_info *ctx_node = sw_context->dx_ctx_node;\n\tstruct vmw_resource *res;\n\tstruct {\n\t\tSVGA3dCmdHeader header;\n\t\tSVGA3dCmdDXBindStreamOutput body;\n\t} *cmd = container_of(header, typeof(*cmd), header);\n\tint ret;\n\n\tif (!has_sm5_context(dev_priv))\n\t\treturn -EINVAL;\n\n\tif (!ctx_node) {\n\t\tDRM_ERROR(\"DX Context not set.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tres = vmw_dx_streamoutput_lookup(vmw_context_res_man(ctx_node->ctx),\n\t\t\t\t\t cmd->body.soid);\n\tif (IS_ERR(res)) {\n\t\tDRM_ERROR(\"Could not find streamoutput to bind.\\n\");\n\t\treturn PTR_ERR(res);\n\t}\n\n\tvmw_dx_streamoutput_set_size(res, cmd->body.sizeInBytes);\n\n\tret = vmw_execbuf_res_val_add(sw_context, res, VMW_RES_DIRTY_NONE,\n\t\t\t\t      vmw_val_add_flag_noctx);\n\tif (ret) {\n\t\tDRM_ERROR(\"Error creating resource validation node.\\n\");\n\t\treturn ret;\n\t}\n\n\treturn vmw_cmd_res_switch_backup(dev_priv, sw_context, res,\n\t\t\t\t\t &cmd->body.mobid,\n\t\t\t\t\t cmd->body.offsetInBytes);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3839,
            "cve_id": "CVE-2022-40133",
            "code_snippet": "static int vmw_translate_guest_ptr(struct vmw_private *dev_priv,\n\t\t\t\t   struct vmw_sw_context *sw_context,\n\t\t\t\t   SVGAGuestPtr *ptr,\n\t\t\t\t   struct vmw_buffer_object **vmw_bo_p)\n{\n\tstruct vmw_buffer_object *vmw_bo;\n\tuint32_t handle = ptr->gmrId;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use GMR region.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo, false, false);\n\tttm_bo_put(&vmw_bo->base);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->location = ptr;\n\treloc->vbo = vmw_bo;\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3840,
            "cve_id": "CVE-2022-40133",
            "code_snippet": "static int vmw_execbuf_tie_context(struct vmw_private *dev_priv,\n\t\t\t\t   struct vmw_sw_context *sw_context,\n\t\t\t\t   uint32_t handle)\n{\n\tstruct vmw_resource *res;\n\tint ret;\n\tunsigned int size;\n\n\tif (handle == SVGA3D_INVALID_ID)\n\t\treturn 0;\n\n\tsize = vmw_execbuf_res_size(dev_priv, vmw_res_dx_context);\n\tret = vmw_validation_preload_res(sw_context->ctx, size);\n\tif (ret)\n\t\treturn ret;\n\n\tret = vmw_user_resource_lookup_handle\n\t\t(dev_priv, sw_context->fp->tfile, handle,\n\t\t user_context_converter, &res);\n\tif (ret != 0) {\n\t\tVMW_DEBUG_USER(\"Could not find or user DX context 0x%08x.\\n\",\n\t\t\t       (unsigned int) handle);\n\t\treturn ret;\n\t}\n\n\tret = vmw_execbuf_res_val_add(sw_context, res, VMW_RES_DIRTY_SET,\n\t\t\t\t      vmw_val_add_flag_none);\n\tif (unlikely(ret != 0)) {\n\t\tvmw_resource_unreference(&res);\n\t\treturn ret;\n\t}\n\n\tsw_context->dx_ctx_node = vmw_execbuf_info_from_res(sw_context, res);\n\tsw_context->man = vmw_context_res_man(res);\n\n\tvmw_resource_unreference(&res);\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3841,
            "cve_id": "CVE-2022-40133",
            "code_snippet": "static int vmw_cmd_dx_bind_shader(struct vmw_private *dev_priv,\n\t\t\t\t  struct vmw_sw_context *sw_context,\n\t\t\t\t  SVGA3dCmdHeader *header)\n{\n\tstruct vmw_resource *ctx;\n\tstruct vmw_resource *res;\n\tVMW_DECLARE_CMD_VAR(*cmd, SVGA3dCmdDXBindShader) =\n\t\tcontainer_of(header, typeof(*cmd), header);\n\tint ret;\n\n\tif (cmd->body.cid != SVGA3D_INVALID_ID) {\n\t\tret = vmw_cmd_res_check(dev_priv, sw_context, vmw_res_context,\n\t\t\t\t\tVMW_RES_DIRTY_SET,\n\t\t\t\t\tuser_context_converter, &cmd->body.cid,\n\t\t\t\t\t&ctx);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} else {\n\t\tstruct vmw_ctx_validation_info *ctx_node =\n\t\t\tVMW_GET_CTX_NODE(sw_context);\n\n\t\tif (!ctx_node)\n\t\t\treturn -EINVAL;\n\n\t\tctx = ctx_node->ctx;\n\t}\n\n\tres = vmw_shader_lookup(vmw_context_res_man(ctx), cmd->body.shid, 0);\n\tif (IS_ERR(res)) {\n\t\tVMW_DEBUG_USER(\"Could not find shader to bind.\\n\");\n\t\treturn PTR_ERR(res);\n\t}\n\n\tret = vmw_execbuf_res_val_add(sw_context, res, VMW_RES_DIRTY_NONE,\n\t\t\t\t      vmw_val_add_flag_noctx);\n\tif (ret) {\n\t\tVMW_DEBUG_USER(\"Error creating resource validation node.\\n\");\n\t\treturn ret;\n\t}\n\n\treturn vmw_cmd_res_switch_backup(dev_priv, sw_context, res,\n\t\t\t\t\t &cmd->body.mobid,\n\t\t\t\t\t cmd->body.offsetInBytes);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3842,
            "cve_id": "CVE-2022-40133",
            "code_snippet": "static int vmw_cmd_dx_set_shader(struct vmw_private *dev_priv,\n\t\t\t\t struct vmw_sw_context *sw_context,\n\t\t\t\t SVGA3dCmdHeader *header)\n{\n\tVMW_DECLARE_CMD_VAR(*cmd, SVGA3dCmdDXSetShader);\n\tSVGA3dShaderType max_allowed = has_sm5_context(dev_priv) ?\n\t\tSVGA3D_SHADERTYPE_MAX : SVGA3D_SHADERTYPE_DX10_MAX;\n\tstruct vmw_resource *res = NULL;\n\tstruct vmw_ctx_validation_info *ctx_node = VMW_GET_CTX_NODE(sw_context);\n\tstruct vmw_ctx_bindinfo_shader binding;\n\tint ret = 0;\n\n\tif (!ctx_node)\n\t\treturn -EINVAL;\n\n\tcmd = container_of(header, typeof(*cmd), header);\n\n\tif (cmd->body.type >= max_allowed ||\n\t    cmd->body.type < SVGA3D_SHADERTYPE_MIN) {\n\t\tVMW_DEBUG_USER(\"Illegal shader type %u.\\n\",\n\t\t\t       (unsigned int) cmd->body.type);\n\t\treturn -EINVAL;\n\t}\n\n\tif (cmd->body.shaderId != SVGA3D_INVALID_ID) {\n\t\tres = vmw_shader_lookup(sw_context->man, cmd->body.shaderId, 0);\n\t\tif (IS_ERR(res)) {\n\t\t\tVMW_DEBUG_USER(\"Could not find shader for binding.\\n\");\n\t\t\treturn PTR_ERR(res);\n\t\t}\n\n\t\tret = vmw_execbuf_res_val_add(sw_context, res,\n\t\t\t\t\t      VMW_RES_DIRTY_NONE,\n\t\t\t\t\t      vmw_val_add_flag_noctx);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tbinding.bi.ctx = ctx_node->ctx;\n\tbinding.bi.res = res;\n\tbinding.bi.bt = vmw_ctx_binding_dx_shader;\n\tbinding.shader_slot = cmd->body.type - SVGA3D_SHADERTYPE_MIN;\n\n\tvmw_binding_add(ctx_node->staged, &binding.bi, binding.shader_slot, 0);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3858,
            "cve_id": "CVE-2022-41218",
            "code_snippet": "static int dvb_demux_open(struct inode *inode, struct file *file)\n{\n\tstruct dvb_device *dvbdev = file->private_data;\n\tstruct dmxdev *dmxdev = dvbdev->priv;\n\tint i;\n\tstruct dmxdev_filter *dmxdevfilter;\n\n\tif (!dmxdev->filter)\n\t\treturn -EINVAL;\n\n\tif (mutex_lock_interruptible(&dmxdev->mutex))\n\t\treturn -ERESTARTSYS;\n\n\tif (dmxdev->exit) {\n\t\tmutex_unlock(&dmxdev->mutex);\n\t\treturn -ENODEV;\n\t}\n\n\tfor (i = 0; i < dmxdev->filternum; i++)\n\t\tif (dmxdev->filter[i].state == DMXDEV_STATE_FREE)\n\t\t\tbreak;\n\n\tif (i == dmxdev->filternum) {\n\t\tmutex_unlock(&dmxdev->mutex);\n\t\treturn -EMFILE;\n\t}\n\n\tdmxdevfilter = &dmxdev->filter[i];\n\tmutex_init(&dmxdevfilter->mutex);\n\tfile->private_data = dmxdevfilter;\n\n#ifdef CONFIG_DVB_MMAP\n\tdmxdev->may_do_mmap = 1;\n#else\n\tdmxdev->may_do_mmap = 0;\n#endif\n\n\tdvb_ringbuffer_init(&dmxdevfilter->buffer, NULL, 8192);\n\tdvb_vb2_init(&dmxdevfilter->vb2_ctx, \"demux_filter\",\n\t\t     file->f_flags & O_NONBLOCK);\n\tdmxdevfilter->type = DMXDEV_TYPE_NONE;\n\tdvb_dmxdev_filter_state_set(dmxdevfilter, DMXDEV_STATE_ALLOCATED);\n\ttimer_setup(&dmxdevfilter->timer, dvb_dmxdev_filter_timeout, 0);\n\n\tdvbdev->users++;\n\n\tmutex_unlock(&dmxdev->mutex);\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3867,
            "cve_id": "CVE-2022-41849",
            "code_snippet": "static int ufx_ops_open(struct fb_info *info, int user)\n{\n\tstruct ufx_data *dev = info->par;\n\n\t/* fbcon aggressively connects to first framebuffer it finds,\n\t * preventing other clients (X) from working properly. Usually\n\t * not what the user wants. Fail by default with option to enable. */\n\tif (user == 0 && !console)\n\t\treturn -EBUSY;\n\n\tmutex_lock(&disconnect_mutex);\n\n\t/* If the USB device is gone, we don't accept new opens */\n\tif (dev->virtualized) {\n\t\tmutex_unlock(&disconnect_mutex);\n\t\treturn -ENODEV;\n\t}\n\n\tdev->fb_count++;\n\n\tkref_get(&dev->kref);\n\n\tif (fb_defio && (info->fbdefio == NULL)) {\n\t\t/* enable defio at last moment if not disabled by client */\n\n\t\tstruct fb_deferred_io *fbdefio;\n\n\t\tfbdefio = kzalloc(sizeof(*fbdefio), GFP_KERNEL);\n\t\tif (fbdefio) {\n\t\t\tfbdefio->delay = UFX_DEFIO_WRITE_DELAY;\n\t\t\tfbdefio->deferred_io = ufx_dpy_deferred_io;\n\t\t}\n\n\t\tinfo->fbdefio = fbdefio;\n\t\tfb_deferred_io_init(info);\n\t}\n\n\tpr_debug(\"open /dev/fb%d user=%d fb_info=%p count=%d\",\n\t\tinfo->node, user, info, dev->fb_count);\n\n\tmutex_unlock(&disconnect_mutex);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3876,
            "cve_id": "CVE-2022-42703",
            "code_snippet": "int anon_vma_fork(struct vm_area_struct *vma, struct vm_area_struct *pvma)\n{\n\tstruct anon_vma_chain *avc;\n\tstruct anon_vma *anon_vma;\n\tint error;\n\n\t/* Don't bother if the parent process has no anon_vma here. */\n\tif (!pvma->anon_vma)\n\t\treturn 0;\n\n\t/* Drop inherited anon_vma, we'll reuse existing or allocate new. */\n\tvma->anon_vma = NULL;\n\n\t/*\n\t * First, attach the new VMA to the parent VMA's anon_vmas,\n\t * so rmap can find non-COWed pages in child processes.\n\t */\n\terror = anon_vma_clone(vma, pvma);\n\tif (error)\n\t\treturn error;\n\n\t/* An existing anon_vma has been reused, all done then. */\n\tif (vma->anon_vma)\n\t\treturn 0;\n\n\t/* Then add our own anon_vma. */\n\tanon_vma = anon_vma_alloc();\n\tif (!anon_vma)\n\t\tgoto out_error;\n\tanon_vma->num_active_vmas++;\n\tavc = anon_vma_chain_alloc(GFP_KERNEL);\n\tif (!avc)\n\t\tgoto out_error_free_anon_vma;\n\n\t/*\n\t * The root anon_vma's rwsem is the lock actually used when we\n\t * lock any of the anon_vmas in this anon_vma tree.\n\t */\n\tanon_vma->root = pvma->anon_vma->root;\n\tanon_vma->parent = pvma->anon_vma;\n\t/*\n\t * With refcounts, an anon_vma can stay around longer than the\n\t * process it belongs to. The root anon_vma needs to be pinned until\n\t * this anon_vma is freed, because the lock lives in the root.\n\t */\n\tget_anon_vma(anon_vma->root);\n\t/* Mark this anon_vma as the one where our new (COWed) pages go. */\n\tvma->anon_vma = anon_vma;\n\tanon_vma_lock_write(anon_vma);\n\tanon_vma_chain_link(vma, avc, anon_vma);\n\tanon_vma->parent->num_children++;\n\tanon_vma_unlock_write(anon_vma);\n\n\treturn 0;\n\n out_error_free_anon_vma:\n\tput_anon_vma(anon_vma);\n out_error:\n\tunlink_anon_vmas(vma);\n\treturn -ENOMEM;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3877,
            "cve_id": "CVE-2022-42703",
            "code_snippet": "int __anon_vma_prepare(struct vm_area_struct *vma)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct anon_vma *anon_vma, *allocated;\n\tstruct anon_vma_chain *avc;\n\n\tmight_sleep();\n\n\tavc = anon_vma_chain_alloc(GFP_KERNEL);\n\tif (!avc)\n\t\tgoto out_enomem;\n\n\tanon_vma = find_mergeable_anon_vma(vma);\n\tallocated = NULL;\n\tif (!anon_vma) {\n\t\tanon_vma = anon_vma_alloc();\n\t\tif (unlikely(!anon_vma))\n\t\t\tgoto out_enomem_free_avc;\n\t\tanon_vma->num_children++; /* self-parent link for new root */\n\t\tallocated = anon_vma;\n\t}\n\n\tanon_vma_lock_write(anon_vma);\n\t/* page_table_lock to protect against threads */\n\tspin_lock(&mm->page_table_lock);\n\tif (likely(!vma->anon_vma)) {\n\t\tvma->anon_vma = anon_vma;\n\t\tanon_vma_chain_link(vma, avc, anon_vma);\n\t\tanon_vma->num_active_vmas++;\n\t\tallocated = NULL;\n\t\tavc = NULL;\n\t}\n\tspin_unlock(&mm->page_table_lock);\n\tanon_vma_unlock_write(anon_vma);\n\n\tif (unlikely(allocated))\n\t\tput_anon_vma(allocated);\n\tif (unlikely(avc))\n\t\tanon_vma_chain_free(avc);\n\n\treturn 0;\n\n out_enomem_free_avc:\n\tanon_vma_chain_free(avc);\n out_enomem:\n\treturn -ENOMEM;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3882,
            "cve_id": "CVE-2022-42720",
            "code_snippet": "static struct cfg80211_bss *\ncfg80211_inform_single_bss_data(struct wiphy *wiphy,\n\t\t\t\tstruct cfg80211_inform_bss *data,\n\t\t\t\tenum cfg80211_bss_frame_type ftype,\n\t\t\t\tconst u8 *bssid, u64 tsf, u16 capability,\n\t\t\t\tu16 beacon_interval, const u8 *ie, size_t ielen,\n\t\t\t\tstruct cfg80211_non_tx_bss *non_tx_data,\n\t\t\t\tgfp_t gfp)\n{\n\tstruct cfg80211_registered_device *rdev = wiphy_to_rdev(wiphy);\n\tstruct cfg80211_bss_ies *ies;\n\tstruct ieee80211_channel *channel;\n\tstruct cfg80211_internal_bss tmp = {}, *res;\n\tint bss_type;\n\tbool signal_valid;\n\tunsigned long ts;\n\n\tif (WARN_ON(!wiphy))\n\t\treturn NULL;\n\n\tif (WARN_ON(wiphy->signal_type == CFG80211_SIGNAL_TYPE_UNSPEC &&\n\t\t    (data->signal < 0 || data->signal > 100)))\n\t\treturn NULL;\n\n\tchannel = cfg80211_get_bss_channel(wiphy, ie, ielen, data->chan,\n\t\t\t\t\t   data->scan_width, ftype);\n\tif (!channel)\n\t\treturn NULL;\n\n\tmemcpy(tmp.pub.bssid, bssid, ETH_ALEN);\n\ttmp.pub.channel = channel;\n\ttmp.pub.scan_width = data->scan_width;\n\ttmp.pub.signal = data->signal;\n\ttmp.pub.beacon_interval = beacon_interval;\n\ttmp.pub.capability = capability;\n\ttmp.ts_boottime = data->boottime_ns;\n\ttmp.parent_tsf = data->parent_tsf;\n\tether_addr_copy(tmp.parent_bssid, data->parent_bssid);\n\n\tif (non_tx_data) {\n\t\ttmp.pub.transmitted_bss = non_tx_data->tx_bss;\n\t\tts = bss_from_pub(non_tx_data->tx_bss)->ts;\n\t\ttmp.pub.bssid_index = non_tx_data->bssid_index;\n\t\ttmp.pub.max_bssid_indicator = non_tx_data->max_bssid_indicator;\n\t} else {\n\t\tts = jiffies;\n\t}\n\n\t/*\n\t * If we do not know here whether the IEs are from a Beacon or Probe\n\t * Response frame, we need to pick one of the options and only use it\n\t * with the driver that does not provide the full Beacon/Probe Response\n\t * frame. Use Beacon frame pointer to avoid indicating that this should\n\t * override the IEs pointer should we have received an earlier\n\t * indication of Probe Response data.\n\t */\n\ties = kzalloc(sizeof(*ies) + ielen, gfp);\n\tif (!ies)\n\t\treturn NULL;\n\ties->len = ielen;\n\ties->tsf = tsf;\n\ties->from_beacon = false;\n\tmemcpy(ies->data, ie, ielen);\n\n\tswitch (ftype) {\n\tcase CFG80211_BSS_FTYPE_BEACON:\n\t\ties->from_beacon = true;\n\t\tfallthrough;\n\tcase CFG80211_BSS_FTYPE_UNKNOWN:\n\t\trcu_assign_pointer(tmp.pub.beacon_ies, ies);\n\t\tbreak;\n\tcase CFG80211_BSS_FTYPE_PRESP:\n\t\trcu_assign_pointer(tmp.pub.proberesp_ies, ies);\n\t\tbreak;\n\t}\n\trcu_assign_pointer(tmp.pub.ies, ies);\n\n\tsignal_valid = data->chan == channel;\n\tres = cfg80211_bss_update(wiphy_to_rdev(wiphy), &tmp, signal_valid, ts);\n\tif (!res)\n\t\treturn NULL;\n\n\tif (channel->band == NL80211_BAND_60GHZ) {\n\t\tbss_type = res->pub.capability & WLAN_CAPABILITY_DMG_TYPE_MASK;\n\t\tif (bss_type == WLAN_CAPABILITY_DMG_TYPE_AP ||\n\t\t    bss_type == WLAN_CAPABILITY_DMG_TYPE_PBSS)\n\t\t\tregulatory_hint_found_beacon(wiphy, channel, gfp);\n\t} else {\n\t\tif (res->pub.capability & WLAN_CAPABILITY_ESS)\n\t\t\tregulatory_hint_found_beacon(wiphy, channel, gfp);\n\t}\n\n\tif (non_tx_data) {\n\t\t/* this is a nontransmitting bss, we need to add it to\n\t\t * transmitting bss' list if it is not there\n\t\t */\n\t\tspin_lock_bh(&rdev->bss_lock);\n\t\tif (cfg80211_add_nontrans_list(non_tx_data->tx_bss,\n\t\t\t\t\t       &res->pub)) {\n\t\t\tif (__cfg80211_unlink_bss(rdev, res)) {\n\t\t\t\trdev->bss_generation++;\n\t\t\t\tres = NULL;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_bh(&rdev->bss_lock);\n\n\t\tif (!res)\n\t\t\treturn NULL;\n\t}\n\n\ttrace_cfg80211_return_bss(&res->pub);\n\t/* cfg80211_bss_update gives us a referenced result */\n\treturn &res->pub;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3888,
            "cve_id": "CVE-2022-42896",
            "code_snippet": "static inline int l2cap_ecred_conn_req(struct l2cap_conn *conn,\n\t\t\t\t       struct l2cap_cmd_hdr *cmd, u16 cmd_len,\n\t\t\t\t       u8 *data)\n{\n\tstruct l2cap_ecred_conn_req *req = (void *) data;\n\tstruct {\n\t\tstruct l2cap_ecred_conn_rsp rsp;\n\t\t__le16 dcid[L2CAP_ECRED_MAX_CID];\n\t} __packed pdu;\n\tstruct l2cap_chan *chan, *pchan;\n\tu16 mtu, mps;\n\t__le16 psm;\n\tu8 result, len = 0;\n\tint i, num_scid;\n\tbool defer = false;\n\n\tif (!enable_ecred)\n\t\treturn -EINVAL;\n\n\tif (cmd_len < sizeof(*req) || (cmd_len - sizeof(*req)) % sizeof(u16)) {\n\t\tresult = L2CAP_CR_LE_INVALID_PARAMS;\n\t\tgoto response;\n\t}\n\n\tcmd_len -= sizeof(*req);\n\tnum_scid = cmd_len / sizeof(u16);\n\n\tif (num_scid > ARRAY_SIZE(pdu.dcid)) {\n\t\tresult = L2CAP_CR_LE_INVALID_PARAMS;\n\t\tgoto response;\n\t}\n\n\tmtu  = __le16_to_cpu(req->mtu);\n\tmps  = __le16_to_cpu(req->mps);\n\n\tif (mtu < L2CAP_ECRED_MIN_MTU || mps < L2CAP_ECRED_MIN_MPS) {\n\t\tresult = L2CAP_CR_LE_UNACCEPT_PARAMS;\n\t\tgoto response;\n\t}\n\n\tpsm  = req->psm;\n\n\t/* BLUETOOTH CORE SPECIFICATION Version 5.3 | Vol 3, Part A\n\t * page 1059:\n\t *\n\t * Valid range: 0x0001-0x00ff\n\t *\n\t * Table 4.15: L2CAP_LE_CREDIT_BASED_CONNECTION_REQ SPSM ranges\n\t */\n\tif (!psm || __le16_to_cpu(psm) > L2CAP_PSM_LE_DYN_END) {\n\t\tresult = L2CAP_CR_LE_BAD_PSM;\n\t\tgoto response;\n\t}\n\n\tBT_DBG(\"psm 0x%2.2x mtu %u mps %u\", __le16_to_cpu(psm), mtu, mps);\n\n\tmemset(&pdu, 0, sizeof(pdu));\n\n\t/* Check if we have socket listening on psm */\n\tpchan = l2cap_global_chan_by_psm(BT_LISTEN, psm, &conn->hcon->src,\n\t\t\t\t\t &conn->hcon->dst, LE_LINK);\n\tif (!pchan) {\n\t\tresult = L2CAP_CR_LE_BAD_PSM;\n\t\tgoto response;\n\t}\n\n\tmutex_lock(&conn->chan_lock);\n\tl2cap_chan_lock(pchan);\n\n\tif (!smp_sufficient_security(conn->hcon, pchan->sec_level,\n\t\t\t\t     SMP_ALLOW_STK)) {\n\t\tresult = L2CAP_CR_LE_AUTHENTICATION;\n\t\tgoto unlock;\n\t}\n\n\tresult = L2CAP_CR_LE_SUCCESS;\n\n\tfor (i = 0; i < num_scid; i++) {\n\t\tu16 scid = __le16_to_cpu(req->scid[i]);\n\n\t\tBT_DBG(\"scid[%d] 0x%4.4x\", i, scid);\n\n\t\tpdu.dcid[i] = 0x0000;\n\t\tlen += sizeof(*pdu.dcid);\n\n\t\t/* Check for valid dynamic CID range */\n\t\tif (scid < L2CAP_CID_DYN_START || scid > L2CAP_CID_LE_DYN_END) {\n\t\t\tresult = L2CAP_CR_LE_INVALID_SCID;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Check if we already have channel with that dcid */\n\t\tif (__l2cap_get_chan_by_dcid(conn, scid)) {\n\t\t\tresult = L2CAP_CR_LE_SCID_IN_USE;\n\t\t\tcontinue;\n\t\t}\n\n\t\tchan = pchan->ops->new_connection(pchan);\n\t\tif (!chan) {\n\t\t\tresult = L2CAP_CR_LE_NO_MEM;\n\t\t\tcontinue;\n\t\t}\n\n\t\tbacpy(&chan->src, &conn->hcon->src);\n\t\tbacpy(&chan->dst, &conn->hcon->dst);\n\t\tchan->src_type = bdaddr_src_type(conn->hcon);\n\t\tchan->dst_type = bdaddr_dst_type(conn->hcon);\n\t\tchan->psm  = psm;\n\t\tchan->dcid = scid;\n\t\tchan->omtu = mtu;\n\t\tchan->remote_mps = mps;\n\n\t\t__l2cap_chan_add(conn, chan);\n\n\t\tl2cap_ecred_init(chan, __le16_to_cpu(req->credits));\n\n\t\t/* Init response */\n\t\tif (!pdu.rsp.credits) {\n\t\t\tpdu.rsp.mtu = cpu_to_le16(chan->imtu);\n\t\t\tpdu.rsp.mps = cpu_to_le16(chan->mps);\n\t\t\tpdu.rsp.credits = cpu_to_le16(chan->rx_credits);\n\t\t}\n\n\t\tpdu.dcid[i] = cpu_to_le16(chan->scid);\n\n\t\t__set_chan_timer(chan, chan->ops->get_sndtimeo(chan));\n\n\t\tchan->ident = cmd->ident;\n\n\t\tif (test_bit(FLAG_DEFER_SETUP, &chan->flags)) {\n\t\t\tl2cap_state_change(chan, BT_CONNECT2);\n\t\t\tdefer = true;\n\t\t\tchan->ops->defer(chan);\n\t\t} else {\n\t\t\tl2cap_chan_ready(chan);\n\t\t}\n\t}\n\nunlock:\n\tl2cap_chan_unlock(pchan);\n\tmutex_unlock(&conn->chan_lock);\n\tl2cap_chan_put(pchan);\n\nresponse:\n\tpdu.rsp.result = cpu_to_le16(result);\n\n\tif (defer)\n\t\treturn 0;\n\n\tl2cap_send_cmd(conn, cmd->ident, L2CAP_ECRED_CONN_RSP,\n\t\t       sizeof(pdu.rsp) + len, &pdu);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3894,
            "cve_id": "CVE-2022-4379",
            "code_snippet": "static __be32\nnfsd4_copy(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\tunion nfsd4_op_u *u)\n{\n\tstruct nfsd4_copy *copy = &u->copy;\n\t__be32 status;\n\tstruct nfsd4_copy *async_copy = NULL;\n\n\tif (nfsd4_ssc_is_inter(copy)) {\n\t\tif (!inter_copy_offload_enable || nfsd4_copy_is_sync(copy)) {\n\t\t\tstatus = nfserr_notsupp;\n\t\t\tgoto out;\n\t\t}\n\t\tstatus = nfsd4_setup_inter_ssc(rqstp, cstate, copy,\n\t\t\t\t&copy->ss_mnt);\n\t\tif (status)\n\t\t\treturn nfserr_offload_denied;\n\t} else {\n\t\tstatus = nfsd4_setup_intra_ssc(rqstp, cstate, copy);\n\t\tif (status)\n\t\t\treturn status;\n\t}\n\n\tcopy->cp_clp = cstate->clp;\n\tmemcpy(&copy->fh, &cstate->current_fh.fh_handle,\n\t\tsizeof(struct knfsd_fh));\n\tif (nfsd4_copy_is_async(copy)) {\n\t\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\t\tstatus = nfserrno(-ENOMEM);\n\t\tasync_copy = kzalloc(sizeof(struct nfsd4_copy), GFP_KERNEL);\n\t\tif (!async_copy)\n\t\t\tgoto out_err;\n\t\tasync_copy->cp_src = kmalloc(sizeof(*async_copy->cp_src), GFP_KERNEL);\n\t\tif (!async_copy->cp_src)\n\t\t\tgoto out_err;\n\t\tif (!nfs4_init_copy_state(nn, copy))\n\t\t\tgoto out_err;\n\t\trefcount_set(&async_copy->refcount, 1);\n\t\tmemcpy(&copy->cp_res.cb_stateid, &copy->cp_stateid.cs_stid,\n\t\t\tsizeof(copy->cp_res.cb_stateid));\n\t\tdup_copy_fields(copy, async_copy);\n\t\tasync_copy->copy_task = kthread_create(nfsd4_do_async_copy,\n\t\t\t\tasync_copy, \"%s\", \"copy thread\");\n\t\tif (IS_ERR(async_copy->copy_task))\n\t\t\tgoto out_err;\n\t\tspin_lock(&async_copy->cp_clp->async_lock);\n\t\tlist_add(&async_copy->copies,\n\t\t\t\t&async_copy->cp_clp->async_copies);\n\t\tspin_unlock(&async_copy->cp_clp->async_lock);\n\t\twake_up_process(async_copy->copy_task);\n\t\tstatus = nfs_ok;\n\t} else {\n\t\tstatus = nfsd4_do_copy(copy, copy->nf_src->nf_file,\n\t\t\t\t       copy->nf_dst->nf_file, true);\n\t\tnfsd4_cleanup_intra_ssc(copy->nf_src, copy->nf_dst);\n\t}\nout:\n\treturn status;\nout_err:\n\tif (async_copy)\n\t\tcleanup_async_copy(async_copy);\n\tstatus = nfserrno(-ENOMEM);\n\t/*\n\t * source's vfsmount of inter-copy will be unmounted\n\t * by the laundromat\n\t */\n\tgoto out;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3896,
            "cve_id": "CVE-2022-4382",
            "code_snippet": "static void\ngadgetfs_kill_sb (struct super_block *sb)\n{\n\tmutex_lock(&sb_mutex);\n\tkill_litter_super (sb);\n\tif (the_device) {\n\t\tput_dev (the_device);\n\t\tthe_device = NULL;\n\t}\n\tkfree(CHIP);\n\tCHIP = NULL;\n\tmutex_unlock(&sb_mutex);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3902,
            "cve_id": "CVE-2022-45888",
            "code_snippet": "static void xillyusb_disconnect(struct usb_interface *interface)\n{\n\tstruct xillyusb_dev *xdev = usb_get_intfdata(interface);\n\tstruct xillyusb_endpoint *msg_ep = xdev->msg_ep;\n\tstruct xillyfifo *fifo = &msg_ep->fifo;\n\tint rc;\n\tint i;\n\n\txillybus_cleanup_chrdev(xdev, &interface->dev);\n\n\t/*\n\t * Try to send OPCODE_QUIESCE, which will fail silently if the device\n\t * was disconnected, but makes sense on module unload.\n\t */\n\n\tmsg_ep->wake_on_drain = true;\n\txillyusb_send_opcode(xdev, ~0, OPCODE_QUIESCE, 0);\n\n\t/*\n\t * If the device has been disconnected, sending the opcode causes\n\t * a global device error with xdev->error, if such error didn't\n\t * occur earlier. Hence timing out means that the USB link is fine,\n\t * but somehow the message wasn't sent. Should never happen.\n\t */\n\n\trc = wait_event_interruptible_timeout(fifo->waitq,\n\t\t\t\t\t      msg_ep->drained || xdev->error,\n\t\t\t\t\t      XILLY_RESPONSE_TIMEOUT);\n\n\tif (!rc)\n\t\tdev_err(&interface->dev,\n\t\t\t\"Weird timeout condition on sending quiesce request.\\n\");\n\n\treport_io_error(xdev, -ENODEV); /* Discourage further activity */\n\n\t/*\n\t * This device driver is declared with soft_unbind set, or else\n\t * sending OPCODE_QUIESCE above would always fail. The price is\n\t * that the USB framework didn't kill outstanding URBs, so it has\n\t * to be done explicitly before returning from this call.\n\t */\n\n\tfor (i = 0; i < xdev->num_channels; i++) {\n\t\tstruct xillyusb_channel *chan = &xdev->channels[i];\n\n\t\t/*\n\t\t * Lock taken to prevent chan->out_ep from changing. It also\n\t\t * ensures xillyusb_open() and xillyusb_flush() don't access\n\t\t * xdev->dev after being nullified below.\n\t\t */\n\t\tmutex_lock(&chan->lock);\n\t\tif (chan->out_ep)\n\t\t\tendpoint_quiesce(chan->out_ep);\n\t\tmutex_unlock(&chan->lock);\n\t}\n\n\tendpoint_quiesce(xdev->in_ep);\n\tendpoint_quiesce(xdev->msg_ep);\n\n\tusb_set_intfdata(interface, NULL);\n\n\txdev->dev = NULL;\n\n\tmutex_lock(&kref_mutex);\n\tkref_put(&xdev->kref, cleanup_dev);\n\tmutex_unlock(&kref_mutex);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3904,
            "cve_id": "CVE-2022-45919",
            "code_snippet": "static int dvb_ca_en50221_io_release(struct inode *inode, struct file *file)\n{\n\tstruct dvb_device *dvbdev = file->private_data;\n\tstruct dvb_ca_private *ca = dvbdev->priv;\n\tint err;\n\n\tdprintk(\"%s\\n\", __func__);\n\n\tmutex_lock(&ca->remove_mutex);\n\n\t/* mark the CA device as closed */\n\tca->open = 0;\n\tdvb_ca_en50221_thread_update_delay(ca);\n\n\terr = dvb_generic_release(inode, file);\n\n\tmodule_put(ca->pub->owner);\n\n\tdvb_ca_private_put(ca);\n\n\tif (dvbdev->users == 1 && ca->exit == 1) {\n\t\tmutex_unlock(&ca->remove_mutex);\n\t\twake_up(&dvbdev->wait_queue);\n\t} else {\n\t\tmutex_unlock(&ca->remove_mutex);\n\t}\n\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3927,
            "cve_id": "CVE-2022-47946",
            "code_snippet": "static void io_ring_ctx_wait_and_kill(struct io_ring_ctx *ctx)\n{\n\tmutex_lock(&ctx->uring_lock);\n\tpercpu_ref_kill(&ctx->refs);\n\t/* if force is set, the ring is going away. always drop after that */\n\tctx->cq_overflow_flushed = 1;\n\tif (ctx->rings)\n\t\t__io_cqring_overflow_flush(ctx, true, NULL, NULL);\n\tidr_for_each(&ctx->personality_idr, io_remove_personalities, ctx);\n\tmutex_unlock(&ctx->uring_lock);\n\n\tio_kill_timeouts(ctx, NULL, NULL);\n\tio_poll_remove_all(ctx, NULL, NULL);\n\n\t/* if we failed setting up the ctx, we might not have any rings */\n\tio_iopoll_try_reap_events(ctx);\n\n\tINIT_WORK(&ctx->exit_work, io_ring_exit_work);\n\t/*\n\t * Use system_unbound_wq to avoid spawning tons of event kworkers\n\t * if we're exiting a ton of rings at the same time. It just adds\n\t * noise and overhead, there's no discernable change in runtime\n\t * over using system_wq.\n\t */\n\tqueue_work(system_unbound_wq, &ctx->exit_work);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3928,
            "cve_id": "CVE-2022-47946",
            "code_snippet": "SYSCALL_DEFINE6(io_uring_enter, unsigned int, fd, u32, to_submit,\n\t\tu32, min_complete, u32, flags, const void __user *, argp,\n\t\tsize_t, argsz)\n{\n\tstruct io_ring_ctx *ctx;\n\tlong ret = -EBADF;\n\tint submitted = 0;\n\tstruct fd f;\n\n\tio_run_task_work();\n\n\tif (flags & ~(IORING_ENTER_GETEVENTS | IORING_ENTER_SQ_WAKEUP |\n\t\t\tIORING_ENTER_SQ_WAIT | IORING_ENTER_EXT_ARG))\n\t\treturn -EINVAL;\n\n\tf = fdget(fd);\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tret = -EOPNOTSUPP;\n\tif (f.file->f_op != &io_uring_fops)\n\t\tgoto out_fput;\n\n\tret = -ENXIO;\n\tctx = f.file->private_data;\n\tif (!percpu_ref_tryget(&ctx->refs))\n\t\tgoto out_fput;\n\n\tret = -EBADFD;\n\tif (ctx->flags & IORING_SETUP_R_DISABLED)\n\t\tgoto out;\n\n\t/*\n\t * For SQ polling, the thread will do all submissions and completions.\n\t * Just return the requested submit count, and wake the thread if\n\t * we were asked to.\n\t */\n\tret = 0;\n\tif (ctx->flags & IORING_SETUP_SQPOLL) {\n\t\tio_cqring_overflow_flush(ctx, false, NULL, NULL);\n\n\t\tif (unlikely(ctx->sqo_exec)) {\n\t\t\tret = io_sq_thread_fork(ctx->sq_data, ctx);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tctx->sqo_exec = 0;\n\t\t}\n\t\tret = -EOWNERDEAD;\n\t\tif (flags & IORING_ENTER_SQ_WAKEUP)\n\t\t\twake_up(&ctx->sq_data->wait);\n\t\tif (flags & IORING_ENTER_SQ_WAIT) {\n\t\t\tret = io_sqpoll_wait_sq(ctx);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\t\tsubmitted = to_submit;\n\t} else if (to_submit) {\n\t\tret = io_uring_add_task_file(ctx, f.file);\n\t\tif (unlikely(ret))\n\t\t\tgoto out;\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tsubmitted = io_submit_sqes(ctx, to_submit);\n\t\tmutex_unlock(&ctx->uring_lock);\n\n\t\tif (submitted != to_submit)\n\t\t\tgoto out;\n\t}\n\tif (flags & IORING_ENTER_GETEVENTS) {\n\t\tconst sigset_t __user *sig;\n\t\tstruct __kernel_timespec __user *ts;\n\n\t\tret = io_get_ext_arg(flags, argp, &argsz, &ts, &sig);\n\t\tif (unlikely(ret))\n\t\t\tgoto out;\n\n\t\tmin_complete = min(min_complete, ctx->cq_entries);\n\n\t\t/*\n\t\t * When SETUP_IOPOLL and SETUP_SQPOLL are both enabled, user\n\t\t * space applications don't need to do io completion events\n\t\t * polling again, they can rely on io_sq_thread to do polling\n\t\t * work, which can reduce cpu usage and uring_lock contention.\n\t\t */\n\t\tif (ctx->flags & IORING_SETUP_IOPOLL &&\n\t\t    !(ctx->flags & IORING_SETUP_SQPOLL)) {\n\t\t\tret = io_iopoll_check(ctx, min_complete);\n\t\t} else {\n\t\t\tret = io_cqring_wait(ctx, min_complete, sig, argsz, ts);\n\t\t}\n\t}\n\nout:\n\tpercpu_ref_put(&ctx->refs);\nout_fput:\n\tfdput(f);\n\treturn submitted ? submitted : ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3929,
            "cve_id": "CVE-2022-47946",
            "code_snippet": "static int io_sq_thread_fork(struct io_sq_data *sqd, struct io_ring_ctx *ctx)\n{\n\tint ret;\n\n\tclear_bit(IO_SQ_THREAD_SHOULD_STOP, &sqd->state);\n\treinit_completion(&sqd->completion);\n\tctx->sqo_exec = 0;\n\tsqd->task_pid = current->pid;\n\tcurrent->flags |= PF_IO_WORKER;\n\tret = io_wq_fork_thread(io_sq_thread, sqd);\n\tcurrent->flags &= ~PF_IO_WORKER;\n\tif (ret < 0) {\n\t\tsqd->thread = NULL;\n\t\treturn ret;\n\t}\n\twait_for_completion(&sqd->completion);\n\treturn io_uring_alloc_task_context(sqd->thread, ctx);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3930,
            "cve_id": "CVE-2022-47946",
            "code_snippet": "static int io_uring_create(unsigned entries, struct io_uring_params *p,\n\t\t\t   struct io_uring_params __user *params)\n{\n\tstruct io_ring_ctx *ctx;\n\tstruct file *file;\n\tint ret;\n\n\tif (!entries)\n\t\treturn -EINVAL;\n\tif (entries > IORING_MAX_ENTRIES) {\n\t\tif (!(p->flags & IORING_SETUP_CLAMP))\n\t\t\treturn -EINVAL;\n\t\tentries = IORING_MAX_ENTRIES;\n\t}\n\n\t/*\n\t * Use twice as many entries for the CQ ring. It's possible for the\n\t * application to drive a higher depth than the size of the SQ ring,\n\t * since the sqes are only used at submission time. This allows for\n\t * some flexibility in overcommitting a bit. If the application has\n\t * set IORING_SETUP_CQSIZE, it will have passed in the desired number\n\t * of CQ ring entries manually.\n\t */\n\tp->sq_entries = roundup_pow_of_two(entries);\n\tif (p->flags & IORING_SETUP_CQSIZE) {\n\t\t/*\n\t\t * If IORING_SETUP_CQSIZE is set, we do the same roundup\n\t\t * to a power-of-two, if it isn't already. We do NOT impose\n\t\t * any cq vs sq ring sizing.\n\t\t */\n\t\tif (!p->cq_entries)\n\t\t\treturn -EINVAL;\n\t\tif (p->cq_entries > IORING_MAX_CQ_ENTRIES) {\n\t\t\tif (!(p->flags & IORING_SETUP_CLAMP))\n\t\t\t\treturn -EINVAL;\n\t\t\tp->cq_entries = IORING_MAX_CQ_ENTRIES;\n\t\t}\n\t\tp->cq_entries = roundup_pow_of_two(p->cq_entries);\n\t\tif (p->cq_entries < p->sq_entries)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tp->cq_entries = 2 * p->sq_entries;\n\t}\n\n\tctx = io_ring_ctx_alloc(p);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\tctx->compat = in_compat_syscall();\n\tif (!capable(CAP_IPC_LOCK))\n\t\tctx->user = get_uid(current_user());\n\tctx->sqo_task = current;\n\n\t/*\n\t * This is just grabbed for accounting purposes. When a process exits,\n\t * the mm is exited and dropped before the files, hence we need to hang\n\t * on to this mm purely for the purposes of being able to unaccount\n\t * memory (locked/pinned vm). It's not used for anything else.\n\t */\n\tmmgrab(current->mm);\n\tctx->mm_account = current->mm;\n\n\tret = io_allocate_scq_urings(ctx, p);\n\tif (ret)\n\t\tgoto err;\n\n\tret = io_sq_offload_create(ctx, p);\n\tif (ret)\n\t\tgoto err;\n\n\tif (!(p->flags & IORING_SETUP_R_DISABLED))\n\t\tio_sq_offload_start(ctx);\n\n\tmemset(&p->sq_off, 0, sizeof(p->sq_off));\n\tp->sq_off.head = offsetof(struct io_rings, sq.head);\n\tp->sq_off.tail = offsetof(struct io_rings, sq.tail);\n\tp->sq_off.ring_mask = offsetof(struct io_rings, sq_ring_mask);\n\tp->sq_off.ring_entries = offsetof(struct io_rings, sq_ring_entries);\n\tp->sq_off.flags = offsetof(struct io_rings, sq_flags);\n\tp->sq_off.dropped = offsetof(struct io_rings, sq_dropped);\n\tp->sq_off.array = (char *)ctx->sq_array - (char *)ctx->rings;\n\n\tmemset(&p->cq_off, 0, sizeof(p->cq_off));\n\tp->cq_off.head = offsetof(struct io_rings, cq.head);\n\tp->cq_off.tail = offsetof(struct io_rings, cq.tail);\n\tp->cq_off.ring_mask = offsetof(struct io_rings, cq_ring_mask);\n\tp->cq_off.ring_entries = offsetof(struct io_rings, cq_ring_entries);\n\tp->cq_off.overflow = offsetof(struct io_rings, cq_overflow);\n\tp->cq_off.cqes = offsetof(struct io_rings, cqes);\n\tp->cq_off.flags = offsetof(struct io_rings, cq_flags);\n\n\tp->features = IORING_FEAT_SINGLE_MMAP | IORING_FEAT_NODROP |\n\t\t\tIORING_FEAT_SUBMIT_STABLE | IORING_FEAT_RW_CUR_POS |\n\t\t\tIORING_FEAT_CUR_PERSONALITY | IORING_FEAT_FAST_POLL |\n\t\t\tIORING_FEAT_POLL_32BITS | IORING_FEAT_SQPOLL_NONFIXED |\n\t\t\tIORING_FEAT_EXT_ARG | IORING_FEAT_NATIVE_WORKERS;\n\n\tif (copy_to_user(params, p, sizeof(*p))) {\n\t\tret = -EFAULT;\n\t\tgoto err;\n\t}\n\n\tfile = io_uring_get_file(ctx);\n\tif (IS_ERR(file)) {\n\t\tret = PTR_ERR(file);\n\t\tgoto err;\n\t}\n\n\t/*\n\t * Install ring fd as the very last thing, so we don't risk someone\n\t * having closed it before we finish setup\n\t */\n\tret = io_uring_install_fd(ctx, file);\n\tif (ret < 0) {\n\t\t/* fput will clean it up */\n\t\tfput(file);\n\t\treturn ret;\n\t}\n\n\ttrace_io_uring_create(ret, ctx, p->sq_entries, p->cq_entries, p->flags);\n\treturn ret;\nerr:\n\tio_ring_ctx_wait_and_kill(ctx);\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3952,
            "cve_id": "CVE-2023-0030",
            "code_snippet": "int\nnvkm_vmm_get_locked(struct nvkm_vmm *vmm, bool getref, bool mapref, bool sparse,\n\t\t    u8 shift, u8 align, u64 size, struct nvkm_vma **pvma)\n{\n\tconst struct nvkm_vmm_page *page = &vmm->func->page[NVKM_VMA_PAGE_NONE];\n\tstruct rb_node *node = NULL, *temp;\n\tstruct nvkm_vma *vma = NULL, *tmp;\n\tu64 addr, tail;\n\tint ret;\n\n\tVMM_TRACE(vmm, \"getref %d mapref %d sparse %d \"\n\t\t       \"shift: %d align: %d size: %016llx\",\n\t\t  getref, mapref, sparse, shift, align, size);\n\n\t/* Zero-sized, or lazily-allocated sparse VMAs, make no sense. */\n\tif (unlikely(!size || (!getref && !mapref && sparse))) {\n\t\tVMM_DEBUG(vmm, \"args %016llx %d %d %d\",\n\t\t\t  size, getref, mapref, sparse);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Tesla-class GPUs can only select page size per-PDE, which means\n\t * we're required to know the mapping granularity up-front to find\n\t * a suitable region of address-space.\n\t *\n\t * The same goes if we're requesting up-front allocation of PTES.\n\t */\n\tif (unlikely((getref || vmm->func->page_block) && !shift)) {\n\t\tVMM_DEBUG(vmm, \"page size required: %d %016llx\",\n\t\t\t  getref, vmm->func->page_block);\n\t\treturn -EINVAL;\n\t}\n\n\t/* If a specific page size was requested, determine its index and\n\t * make sure the requested size is a multiple of the page size.\n\t */\n\tif (shift) {\n\t\tfor (page = vmm->func->page; page->shift; page++) {\n\t\t\tif (shift == page->shift)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (!page->shift || !IS_ALIGNED(size, 1ULL << page->shift)) {\n\t\t\tVMM_DEBUG(vmm, \"page %d %016llx\", shift, size);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\talign = max_t(u8, align, shift);\n\t} else {\n\t\talign = max_t(u8, align, 12);\n\t}\n\n\t/* Locate smallest block that can possibly satisfy the allocation. */\n\ttemp = vmm->free.rb_node;\n\twhile (temp) {\n\t\tstruct nvkm_vma *this = rb_entry(temp, typeof(*this), tree);\n\t\tif (this->size < size) {\n\t\t\ttemp = temp->rb_right;\n\t\t} else {\n\t\t\tnode = temp;\n\t\t\ttemp = temp->rb_left;\n\t\t}\n\t}\n\n\tif (unlikely(!node))\n\t\treturn -ENOSPC;\n\n\t/* Take into account alignment restrictions, trying larger blocks\n\t * in turn until we find a suitable free block.\n\t */\n\tdo {\n\t\tstruct nvkm_vma *this = rb_entry(node, typeof(*this), tree);\n\t\tstruct nvkm_vma *prev = node(this, prev);\n\t\tstruct nvkm_vma *next = node(this, next);\n\t\tconst int p = page - vmm->func->page;\n\n\t\taddr = this->addr;\n\t\tif (vmm->func->page_block && prev && prev->page != p)\n\t\t\taddr = ALIGN(addr, vmm->func->page_block);\n\t\taddr = ALIGN(addr, 1ULL << align);\n\n\t\ttail = this->addr + this->size;\n\t\tif (vmm->func->page_block && next && next->page != p)\n\t\t\ttail = ALIGN_DOWN(tail, vmm->func->page_block);\n\n\t\tif (addr <= tail && tail - addr >= size) {\n\t\t\tnvkm_vmm_free_remove(vmm, this);\n\t\t\tvma = this;\n\t\t\tbreak;\n\t\t}\n\t} while ((node = rb_next(node)));\n\n\tif (unlikely(!vma))\n\t\treturn -ENOSPC;\n\n\t/* If the VMA we found isn't already exactly the requested size,\n\t * it needs to be split, and the remaining free blocks returned.\n\t */\n\tif (addr != vma->addr) {\n\t\tif (!(tmp = nvkm_vma_tail(vma, vma->size + vma->addr - addr))) {\n\t\t\tnvkm_vmm_put_region(vmm, vma);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tnvkm_vmm_free_insert(vmm, vma);\n\t\tvma = tmp;\n\t}\n\n\tif (size != vma->size) {\n\t\tif (!(tmp = nvkm_vma_tail(vma, vma->size - size))) {\n\t\t\tnvkm_vmm_put_region(vmm, vma);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tnvkm_vmm_free_insert(vmm, tmp);\n\t}\n\n\t/* Pre-allocate page tables and/or setup sparse mappings. */\n\tif (sparse && getref)\n\t\tret = nvkm_vmm_ptes_sparse_get(vmm, page, vma->addr, vma->size);\n\telse if (sparse)\n\t\tret = nvkm_vmm_ptes_sparse(vmm, vma->addr, vma->size, true);\n\telse if (getref)\n\t\tret = nvkm_vmm_ptes_get(vmm, page, vma->addr, vma->size);\n\telse\n\t\tret = 0;\n\tif (ret) {\n\t\tnvkm_vmm_put_region(vmm, vma);\n\t\treturn ret;\n\t}\n\n\tvma->mapref = mapref && !getref;\n\tvma->sparse = sparse;\n\tvma->page = page - vmm->func->page;\n\tvma->refd = getref ? vma->page : NVKM_VMA_PAGE_NONE;\n\tvma->used = true;\n\tnvkm_vmm_node_insert(vmm, vma);\n\t*pvma = vma;\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3953,
            "cve_id": "CVE-2023-0030",
            "code_snippet": "void\nnvkm_vmm_unmap_region(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tstruct nvkm_vma *next = node(vma, next);\n\tstruct nvkm_vma *prev = NULL;\n\n\tnvkm_memory_tags_put(vma->memory, vmm->mmu->subdev.device, &vma->tags);\n\tnvkm_memory_unref(&vma->memory);\n\n\tif (!vma->part || ((prev = node(vma, prev)), prev->memory))\n\t\tprev = NULL;\n\tif (!next->part || next->memory)\n\t\tnext = NULL;\n\tnvkm_vmm_node_merge(vmm, prev, vma, next, vma->size);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3964,
            "cve_id": "CVE-2023-0240",
            "code_snippet": "static void io_prep_async_work(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_identity *id = &req->identity;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_req_init_async(req);\n\n\tif (req->flags & REQ_F_ISREG) {\n\t\tif (def->hash_reg_file || (ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tio_wq_hash_work(&req->work, file_inode(req->file));\n\t} else {\n\t\tif (def->unbound_nonreg_file)\n\t\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t}\n\n\t/* ->mm can never change on us */\n\tif (!(req->work.flags & IO_WQ_WORK_MM) &&\n\t    (def->work_flags & IO_WQ_WORK_MM)) {\n\t\tmmgrab(id->mm);\n\t\treq->work.flags |= IO_WQ_WORK_MM;\n\t}\n\n\t/* if we fail grabbing identity, we must COW, regrab, and retry */\n\tif (io_grab_identity(req))\n\t\treturn;\n\n\tif (!io_identity_cow(req))\n\t\treturn;\n\n\t/* can't fail at this point */\n\tif (!io_grab_identity(req))\n\t\tWARN_ON(1);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3965,
            "cve_id": "CVE-2023-0240",
            "code_snippet": "static inline void io_req_init_async(struct io_kiocb *req)\n{\n\tif (req->flags & REQ_F_WORK_INITIALIZED)\n\t\treturn;\n\n\tmemset(&req->work, 0, sizeof(req->work));\n\treq->flags |= REQ_F_WORK_INITIALIZED;\n\tio_init_identity(&req->identity);\n\treq->work.identity = &req->identity;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3970,
            "cve_id": "CVE-2023-0266",
            "code_snippet": "static int snd_ctl_elem_read_user(struct snd_card *card,\n\t\t\t\t  struct snd_ctl_elem_value __user *_control)\n{\n\tstruct snd_ctl_elem_value *control;\n\tint result;\n\n\tcontrol = memdup_user(_control, sizeof(*control));\n\tif (IS_ERR(control))\n\t\treturn PTR_ERR(control);\n\n\tresult = snd_ctl_elem_read(card, control);\n\tif (result < 0)\n\t\tgoto error;\n\n\tif (copy_to_user(_control, control, sizeof(*control)))\n\t\tresult = -EFAULT;\n error:\n\tkfree(control);\n\treturn result;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3977,
            "cve_id": "CVE-2023-0461",
            "code_snippet": "static int __tcp_set_ulp(struct sock *sk, const struct tcp_ulp_ops *ulp_ops)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tint err;\n\n\terr = -EEXIST;\n\tif (icsk->icsk_ulp_ops)\n\t\tgoto out_err;\n\n\tif (sk->sk_socket)\n\t\tclear_bit(SOCK_SUPPORT_ZC, &sk->sk_socket->flags);\n\n\terr = -EINVAL;\n\tif (!ulp_ops->clone && sk->sk_state == TCP_LISTEN)\n\t\tgoto out_err;\n\n\terr = ulp_ops->init(sk);\n\tif (err)\n\t\tgoto out_err;\n\n\ticsk->icsk_ulp_ops = ulp_ops;\n\treturn 0;\nout_err:\n\tmodule_put(ulp_ops->owner);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3979,
            "cve_id": "CVE-2023-0468",
            "code_snippet": "static inline bool io_poll_get_ownership(struct io_kiocb *req)\n{\n\tif (unlikely(atomic_read(&req->poll_refs) >= IO_POLL_REF_BIAS))\n\t\treturn io_poll_get_ownership_slowpath(req);\n\treturn !(atomic_fetch_inc(&req->poll_refs) & IO_POLL_REF_MASK);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 3995,
            "cve_id": "CVE-2023-1079",
            "code_snippet": "static void asus_kbd_backlight_work(struct work_struct *work)\n{\n\tstruct asus_kbd_leds *led = container_of(work, struct asus_kbd_leds, work);\n\tu8 buf[] = { FEATURE_KBD_REPORT_ID, 0xba, 0xc5, 0xc4, 0x00 };\n\tint ret;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&led->lock, flags);\n\tbuf[4] = led->brightness;\n\tspin_unlock_irqrestore(&led->lock, flags);\n\n\tret = asus_kbd_set_report(led->hdev, buf, sizeof(buf));\n\tif (ret < 0)\n\t\thid_err(led->hdev, \"Asus failed to set keyboard backlight: %d\\n\", ret);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4000,
            "cve_id": "CVE-2023-1193",
            "code_snippet": "void ksmbd_conn_enqueue_request(struct ksmbd_work *work)\n{\n\tstruct ksmbd_conn *conn = work->conn;\n\tstruct list_head *requests_queue = NULL;\n\n\tif (conn->ops->get_cmd_val(work) != SMB2_CANCEL_HE)\n\t\trequests_queue = &conn->requests;\n\n\tif (requests_queue) {\n\t\tatomic_inc(&conn->req_running);\n\t\tspin_lock(&conn->request_lock);\n\t\tlist_add_tail(&work->request_entry, requests_queue);\n\t\tspin_unlock(&conn->request_lock);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4007,
            "cve_id": "CVE-2023-1249",
            "code_snippet": "void do_coredump(const kernel_siginfo_t *siginfo)\n{\n\tstruct core_state core_state;\n\tstruct core_name cn;\n\tstruct mm_struct *mm = current->mm;\n\tstruct linux_binfmt * binfmt;\n\tconst struct cred *old_cred;\n\tstruct cred *cred;\n\tint retval = 0;\n\tint ispipe;\n\tsize_t *argv = NULL;\n\tint argc = 0;\n\t/* require nonrelative corefile path and be extra careful */\n\tbool need_suid_safe = false;\n\tbool core_dumped = false;\n\tstatic atomic_t core_dump_count = ATOMIC_INIT(0);\n\tstruct coredump_params cprm = {\n\t\t.siginfo = siginfo,\n\t\t.regs = signal_pt_regs(),\n\t\t.limit = rlimit(RLIMIT_CORE),\n\t\t/*\n\t\t * We must use the same mm->flags while dumping core to avoid\n\t\t * inconsistency of bit flags, since this flag is not protected\n\t\t * by any locks.\n\t\t */\n\t\t.mm_flags = mm->flags,\n\t\t.vma_meta = NULL,\n\t};\n\n\taudit_core_dumps(siginfo->si_signo);\n\n\tbinfmt = mm->binfmt;\n\tif (!binfmt || !binfmt->core_dump)\n\t\tgoto fail;\n\tif (!__get_dumpable(cprm.mm_flags))\n\t\tgoto fail;\n\n\tcred = prepare_creds();\n\tif (!cred)\n\t\tgoto fail;\n\t/*\n\t * We cannot trust fsuid as being the \"true\" uid of the process\n\t * nor do we know its entire history. We only know it was tainted\n\t * so we dump it as root in mode 2, and only into a controlled\n\t * environment (pipe handler or fully qualified path).\n\t */\n\tif (__get_dumpable(cprm.mm_flags) == SUID_DUMP_ROOT) {\n\t\t/* Setuid core dump mode */\n\t\tcred->fsuid = GLOBAL_ROOT_UID;\t/* Dump root private */\n\t\tneed_suid_safe = true;\n\t}\n\n\tretval = coredump_wait(siginfo->si_signo, &core_state);\n\tif (retval < 0)\n\t\tgoto fail_creds;\n\n\told_cred = override_creds(cred);\n\n\tispipe = format_corename(&cn, &cprm, &argv, &argc);\n\n\tif (ispipe) {\n\t\tint argi;\n\t\tint dump_count;\n\t\tchar **helper_argv;\n\t\tstruct subprocess_info *sub_info;\n\n\t\tif (ispipe < 0) {\n\t\t\tprintk(KERN_WARNING \"format_corename failed\\n\");\n\t\t\tprintk(KERN_WARNING \"Aborting core\\n\");\n\t\t\tgoto fail_unlock;\n\t\t}\n\n\t\tif (cprm.limit == 1) {\n\t\t\t/* See umh_pipe_setup() which sets RLIMIT_CORE = 1.\n\t\t\t *\n\t\t\t * Normally core limits are irrelevant to pipes, since\n\t\t\t * we're not writing to the file system, but we use\n\t\t\t * cprm.limit of 1 here as a special value, this is a\n\t\t\t * consistent way to catch recursive crashes.\n\t\t\t * We can still crash if the core_pattern binary sets\n\t\t\t * RLIM_CORE = !1, but it runs as root, and can do\n\t\t\t * lots of stupid things.\n\t\t\t *\n\t\t\t * Note that we use task_tgid_vnr here to grab the pid\n\t\t\t * of the process group leader.  That way we get the\n\t\t\t * right pid if a thread in a multi-threaded\n\t\t\t * core_pattern process dies.\n\t\t\t */\n\t\t\tprintk(KERN_WARNING\n\t\t\t\t\"Process %d(%s) has RLIMIT_CORE set to 1\\n\",\n\t\t\t\ttask_tgid_vnr(current), current->comm);\n\t\t\tprintk(KERN_WARNING \"Aborting core\\n\");\n\t\t\tgoto fail_unlock;\n\t\t}\n\t\tcprm.limit = RLIM_INFINITY;\n\n\t\tdump_count = atomic_inc_return(&core_dump_count);\n\t\tif (core_pipe_limit && (core_pipe_limit < dump_count)) {\n\t\t\tprintk(KERN_WARNING \"Pid %d(%s) over core_pipe_limit\\n\",\n\t\t\t       task_tgid_vnr(current), current->comm);\n\t\t\tprintk(KERN_WARNING \"Skipping core dump\\n\");\n\t\t\tgoto fail_dropcount;\n\t\t}\n\n\t\thelper_argv = kmalloc_array(argc + 1, sizeof(*helper_argv),\n\t\t\t\t\t    GFP_KERNEL);\n\t\tif (!helper_argv) {\n\t\t\tprintk(KERN_WARNING \"%s failed to allocate memory\\n\",\n\t\t\t       __func__);\n\t\t\tgoto fail_dropcount;\n\t\t}\n\t\tfor (argi = 0; argi < argc; argi++)\n\t\t\thelper_argv[argi] = cn.corename + argv[argi];\n\t\thelper_argv[argi] = NULL;\n\n\t\tretval = -ENOMEM;\n\t\tsub_info = call_usermodehelper_setup(helper_argv[0],\n\t\t\t\t\t\thelper_argv, NULL, GFP_KERNEL,\n\t\t\t\t\t\tumh_pipe_setup, NULL, &cprm);\n\t\tif (sub_info)\n\t\t\tretval = call_usermodehelper_exec(sub_info,\n\t\t\t\t\t\t\t  UMH_WAIT_EXEC);\n\n\t\tkfree(helper_argv);\n\t\tif (retval) {\n\t\t\tprintk(KERN_INFO \"Core dump to |%s pipe failed\\n\",\n\t\t\t       cn.corename);\n\t\t\tgoto close_fail;\n\t\t}\n\t} else {\n\t\tstruct user_namespace *mnt_userns;\n\t\tstruct inode *inode;\n\t\tint open_flags = O_CREAT | O_RDWR | O_NOFOLLOW |\n\t\t\t\t O_LARGEFILE | O_EXCL;\n\n\t\tif (cprm.limit < binfmt->min_coredump)\n\t\t\tgoto fail_unlock;\n\n\t\tif (need_suid_safe && cn.corename[0] != '/') {\n\t\t\tprintk(KERN_WARNING \"Pid %d(%s) can only dump core \"\\\n\t\t\t\t\"to fully qualified path!\\n\",\n\t\t\t\ttask_tgid_vnr(current), current->comm);\n\t\t\tprintk(KERN_WARNING \"Skipping core dump\\n\");\n\t\t\tgoto fail_unlock;\n\t\t}\n\n\t\t/*\n\t\t * Unlink the file if it exists unless this is a SUID\n\t\t * binary - in that case, we're running around with root\n\t\t * privs and don't want to unlink another user's coredump.\n\t\t */\n\t\tif (!need_suid_safe) {\n\t\t\t/*\n\t\t\t * If it doesn't exist, that's fine. If there's some\n\t\t\t * other problem, we'll catch it at the filp_open().\n\t\t\t */\n\t\t\tdo_unlinkat(AT_FDCWD, getname_kernel(cn.corename));\n\t\t}\n\n\t\t/*\n\t\t * There is a race between unlinking and creating the\n\t\t * file, but if that causes an EEXIST here, that's\n\t\t * fine - another process raced with us while creating\n\t\t * the corefile, and the other process won. To userspace,\n\t\t * what matters is that at least one of the two processes\n\t\t * writes its coredump successfully, not which one.\n\t\t */\n\t\tif (need_suid_safe) {\n\t\t\t/*\n\t\t\t * Using user namespaces, normal user tasks can change\n\t\t\t * their current->fs->root to point to arbitrary\n\t\t\t * directories. Since the intention of the \"only dump\n\t\t\t * with a fully qualified path\" rule is to control where\n\t\t\t * coredumps may be placed using root privileges,\n\t\t\t * current->fs->root must not be used. Instead, use the\n\t\t\t * root directory of init_task.\n\t\t\t */\n\t\t\tstruct path root;\n\n\t\t\ttask_lock(&init_task);\n\t\t\tget_fs_root(init_task.fs, &root);\n\t\t\ttask_unlock(&init_task);\n\t\t\tcprm.file = file_open_root(&root, cn.corename,\n\t\t\t\t\t\t   open_flags, 0600);\n\t\t\tpath_put(&root);\n\t\t} else {\n\t\t\tcprm.file = filp_open(cn.corename, open_flags, 0600);\n\t\t}\n\t\tif (IS_ERR(cprm.file))\n\t\t\tgoto fail_unlock;\n\n\t\tinode = file_inode(cprm.file);\n\t\tif (inode->i_nlink > 1)\n\t\t\tgoto close_fail;\n\t\tif (d_unhashed(cprm.file->f_path.dentry))\n\t\t\tgoto close_fail;\n\t\t/*\n\t\t * AK: actually i see no reason to not allow this for named\n\t\t * pipes etc, but keep the previous behaviour for now.\n\t\t */\n\t\tif (!S_ISREG(inode->i_mode))\n\t\t\tgoto close_fail;\n\t\t/*\n\t\t * Don't dump core if the filesystem changed owner or mode\n\t\t * of the file during file creation. This is an issue when\n\t\t * a process dumps core while its cwd is e.g. on a vfat\n\t\t * filesystem.\n\t\t */\n\t\tmnt_userns = file_mnt_user_ns(cprm.file);\n\t\tif (!uid_eq(i_uid_into_mnt(mnt_userns, inode),\n\t\t\t    current_fsuid())) {\n\t\t\tpr_info_ratelimited(\"Core dump to %s aborted: cannot preserve file owner\\n\",\n\t\t\t\t\t    cn.corename);\n\t\t\tgoto close_fail;\n\t\t}\n\t\tif ((inode->i_mode & 0677) != 0600) {\n\t\t\tpr_info_ratelimited(\"Core dump to %s aborted: cannot preserve file permissions\\n\",\n\t\t\t\t\t    cn.corename);\n\t\t\tgoto close_fail;\n\t\t}\n\t\tif (!(cprm.file->f_mode & FMODE_CAN_WRITE))\n\t\t\tgoto close_fail;\n\t\tif (do_truncate(mnt_userns, cprm.file->f_path.dentry,\n\t\t\t\t0, 0, cprm.file))\n\t\t\tgoto close_fail;\n\t}\n\n\t/* get us an unshared descriptor table; almost always a no-op */\n\t/* The cell spufs coredump code reads the file descriptor tables */\n\tretval = unshare_files();\n\tif (retval)\n\t\tgoto close_fail;\n\tif (!dump_interrupted()) {\n\t\t/*\n\t\t * umh disabled with CONFIG_STATIC_USERMODEHELPER_PATH=\"\" would\n\t\t * have this set to NULL.\n\t\t */\n\t\tif (!cprm.file) {\n\t\t\tpr_info(\"Core dump to |%s disabled\\n\", cn.corename);\n\t\t\tgoto close_fail;\n\t\t}\n\t\tif (!dump_vma_snapshot(&cprm))\n\t\t\tgoto close_fail;\n\n\t\tfile_start_write(cprm.file);\n\t\tcore_dumped = binfmt->core_dump(&cprm);\n\t\t/*\n\t\t * Ensures that file size is big enough to contain the current\n\t\t * file postion. This prevents gdb from complaining about\n\t\t * a truncated file if the last \"write\" to the file was\n\t\t * dump_skip.\n\t\t */\n\t\tif (cprm.to_skip) {\n\t\t\tcprm.to_skip--;\n\t\t\tdump_emit(&cprm, \"\", 1);\n\t\t}\n\t\tfile_end_write(cprm.file);\n\t\tfree_vma_snapshot(&cprm);\n\t}\n\tif (ispipe && core_pipe_limit)\n\t\twait_for_dump_helpers(cprm.file);\nclose_fail:\n\tif (cprm.file)\n\t\tfilp_close(cprm.file, NULL);\nfail_dropcount:\n\tif (ispipe)\n\t\tatomic_dec(&core_dump_count);\nfail_unlock:\n\tkfree(argv);\n\tkfree(cn.corename);\n\tcoredump_finish(core_dumped);\n\trevert_creds(old_cred);\nfail_creds:\n\tput_cred(cred);\nfail:\n\treturn;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4023,
            "cve_id": "CVE-2023-1611",
            "code_snippet": "int btrfs_run_qgroups(struct btrfs_trans_handle *trans)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tint ret = 0;\n\n\t/*\n\t * In case we are called from the qgroup assign ioctl, assert that we\n\t * are holding the qgroup_ioctl_lock, otherwise we can race with a quota\n\t * disable operation (ioctl) and access a freed quota root.\n\t */\n\tif (trans->transaction->state != TRANS_STATE_COMMIT_DOING)\n\t\tlockdep_assert_held(&fs_info->qgroup_ioctl_lock);\n\n\tif (!fs_info->quota_root)\n\t\treturn ret;\n\n\tspin_lock(&fs_info->qgroup_lock);\n\twhile (!list_empty(&fs_info->dirty_qgroups)) {\n\t\tstruct btrfs_qgroup *qgroup;\n\t\tqgroup = list_first_entry(&fs_info->dirty_qgroups,\n\t\t\t\t\t  struct btrfs_qgroup, dirty);\n\t\tlist_del_init(&qgroup->dirty);\n\t\tspin_unlock(&fs_info->qgroup_lock);\n\t\tret = update_qgroup_info_item(trans, qgroup);\n\t\tif (ret)\n\t\t\tqgroup_mark_inconsistent(fs_info);\n\t\tret = update_qgroup_limit_item(trans, qgroup);\n\t\tif (ret)\n\t\t\tqgroup_mark_inconsistent(fs_info);\n\t\tspin_lock(&fs_info->qgroup_lock);\n\t}\n\tif (test_bit(BTRFS_FS_QUOTA_ENABLED, &fs_info->flags))\n\t\tfs_info->qgroup_flags |= BTRFS_QGROUP_STATUS_FLAG_ON;\n\telse\n\t\tfs_info->qgroup_flags &= ~BTRFS_QGROUP_STATUS_FLAG_ON;\n\tspin_unlock(&fs_info->qgroup_lock);\n\n\tret = update_qgroup_status_item(trans);\n\tif (ret)\n\t\tqgroup_mark_inconsistent(fs_info);\n\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4031,
            "cve_id": "CVE-2023-1872",
            "code_snippet": "static void io_apoll_task_func(struct io_kiocb *req, bool *locked)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\tret = io_poll_check_events(req, *locked);\n\tif (ret > 0)\n\t\treturn;\n\n\tio_poll_remove_entries(req);\n\tspin_lock(&ctx->completion_lock);\n\thash_del(&req->hash_node);\n\tspin_unlock(&ctx->completion_lock);\n\n\tif (!ret)\n\t\tio_req_task_submit(req, locked);\n\telse\n\t\tio_req_complete_failed(req, ret);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4032,
            "cve_id": "CVE-2023-1872",
            "code_snippet": "static void io_poll_task_func(struct io_kiocb *req, bool *locked)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\tret = io_poll_check_events(req, *locked);\n\tif (ret > 0)\n\t\treturn;\n\n\tif (!ret) {\n\t\treq->result = mangle_poll(req->result & req->poll.events);\n\t} else {\n\t\treq->result = ret;\n\t\treq_set_fail(req);\n\t}\n\n\tio_poll_remove_entries(req);\n\tspin_lock(&ctx->completion_lock);\n\thash_del(&req->hash_node);\n\t__io_req_complete_post(req, req->result, 0);\n\tio_commit_cqring(ctx);\n\tspin_unlock(&ctx->completion_lock);\n\tio_cqring_ev_posted(ctx);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4056,
            "cve_id": "CVE-2023-20928",
            "code_snippet": "static inline void binder_alloc_set_vma(struct binder_alloc *alloc,\n\t\tstruct vm_area_struct *vma)\n{\n\tunsigned long vm_start = 0;\n\n\tif (vma) {\n\t\tvm_start = vma->vm_start;\n\t\talloc->vma_vm_mm = vma->vm_mm;\n\t}\n\n\tmmap_assert_write_locked(alloc->vma_vm_mm);\n\talloc->vma_addr = vm_start;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4057,
            "cve_id": "CVE-2023-20928",
            "code_snippet": "static int binder_update_page_range(struct binder_alloc *alloc, int allocate,\n\t\t\t\t    void __user *start, void __user *end)\n{\n\tvoid __user *page_addr;\n\tunsigned long user_page_addr;\n\tstruct binder_lru_page *page;\n\tstruct vm_area_struct *vma = NULL;\n\tstruct mm_struct *mm = NULL;\n\tbool need_mm = false;\n\n\tbinder_alloc_debug(BINDER_DEBUG_BUFFER_ALLOC,\n\t\t     \"%d: %s pages %pK-%pK\\n\", alloc->pid,\n\t\t     allocate ? \"allocate\" : \"free\", start, end);\n\n\tif (end <= start)\n\t\treturn 0;\n\n\ttrace_binder_update_page_range(alloc, allocate, start, end);\n\n\tif (allocate == 0)\n\t\tgoto free_range;\n\n\tfor (page_addr = start; page_addr < end; page_addr += PAGE_SIZE) {\n\t\tpage = &alloc->pages[(page_addr - alloc->buffer) / PAGE_SIZE];\n\t\tif (!page->page_ptr) {\n\t\t\tneed_mm = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (need_mm && mmget_not_zero(alloc->vma_vm_mm))\n\t\tmm = alloc->vma_vm_mm;\n\n\tif (mm) {\n\t\tmmap_read_lock(mm);\n\t\tvma = vma_lookup(mm, alloc->vma_addr);\n\t}\n\n\tif (!vma && need_mm) {\n\t\tbinder_alloc_debug(BINDER_DEBUG_USER_ERROR,\n\t\t\t\t   \"%d: binder_alloc_buf failed to map pages in userspace, no vma\\n\",\n\t\t\t\t   alloc->pid);\n\t\tgoto err_no_vma;\n\t}\n\n\tfor (page_addr = start; page_addr < end; page_addr += PAGE_SIZE) {\n\t\tint ret;\n\t\tbool on_lru;\n\t\tsize_t index;\n\n\t\tindex = (page_addr - alloc->buffer) / PAGE_SIZE;\n\t\tpage = &alloc->pages[index];\n\n\t\tif (page->page_ptr) {\n\t\t\ttrace_binder_alloc_lru_start(alloc, index);\n\n\t\t\ton_lru = list_lru_del(&binder_alloc_lru, &page->lru);\n\t\t\tWARN_ON(!on_lru);\n\n\t\t\ttrace_binder_alloc_lru_end(alloc, index);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (WARN_ON(!vma))\n\t\t\tgoto err_page_ptr_cleared;\n\n\t\ttrace_binder_alloc_page_start(alloc, index);\n\t\tpage->page_ptr = alloc_page(GFP_KERNEL |\n\t\t\t\t\t    __GFP_HIGHMEM |\n\t\t\t\t\t    __GFP_ZERO);\n\t\tif (!page->page_ptr) {\n\t\t\tpr_err(\"%d: binder_alloc_buf failed for page at %pK\\n\",\n\t\t\t\talloc->pid, page_addr);\n\t\t\tgoto err_alloc_page_failed;\n\t\t}\n\t\tpage->alloc = alloc;\n\t\tINIT_LIST_HEAD(&page->lru);\n\n\t\tuser_page_addr = (uintptr_t)page_addr;\n\t\tret = vm_insert_page(vma, user_page_addr, page[0].page_ptr);\n\t\tif (ret) {\n\t\t\tpr_err(\"%d: binder_alloc_buf failed to map page at %lx in userspace\\n\",\n\t\t\t       alloc->pid, user_page_addr);\n\t\t\tgoto err_vm_insert_page_failed;\n\t\t}\n\n\t\tif (index + 1 > alloc->pages_high)\n\t\t\talloc->pages_high = index + 1;\n\n\t\ttrace_binder_alloc_page_end(alloc, index);\n\t}\n\tif (mm) {\n\t\tmmap_read_unlock(mm);\n\t\tmmput(mm);\n\t}\n\treturn 0;\n\nfree_range:\n\tfor (page_addr = end - PAGE_SIZE; 1; page_addr -= PAGE_SIZE) {\n\t\tbool ret;\n\t\tsize_t index;\n\n\t\tindex = (page_addr - alloc->buffer) / PAGE_SIZE;\n\t\tpage = &alloc->pages[index];\n\n\t\ttrace_binder_free_lru_start(alloc, index);\n\n\t\tret = list_lru_add(&binder_alloc_lru, &page->lru);\n\t\tWARN_ON(!ret);\n\n\t\ttrace_binder_free_lru_end(alloc, index);\n\t\tif (page_addr == start)\n\t\t\tbreak;\n\t\tcontinue;\n\nerr_vm_insert_page_failed:\n\t\t__free_page(page->page_ptr);\n\t\tpage->page_ptr = NULL;\nerr_alloc_page_failed:\nerr_page_ptr_cleared:\n\t\tif (page_addr == start)\n\t\t\tbreak;\n\t}\nerr_no_vma:\n\tif (mm) {\n\t\tmmap_read_unlock(mm);\n\t\tmmput(mm);\n\t}\n\treturn vma ? -ENOMEM : -ESRCH;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4065,
            "cve_id": "CVE-2023-21255",
            "code_snippet": "static int binder_proc_transaction(struct binder_transaction *t,\n\t\t\t\t    struct binder_proc *proc,\n\t\t\t\t    struct binder_thread *thread)\n{\n\tstruct binder_node *node = t->buffer->target_node;\n\tbool oneway = !!(t->flags & TF_ONE_WAY);\n\tbool pending_async = false;\n\tstruct binder_transaction *t_outdated = NULL;\n\tbool frozen = false;\n\n\tBUG_ON(!node);\n\tbinder_node_lock(node);\n\tif (oneway) {\n\t\tBUG_ON(thread);\n\t\tif (node->has_async_transaction)\n\t\t\tpending_async = true;\n\t\telse\n\t\t\tnode->has_async_transaction = true;\n\t}\n\n\tbinder_inner_proc_lock(proc);\n\tif (proc->is_frozen) {\n\t\tfrozen = true;\n\t\tproc->sync_recv |= !oneway;\n\t\tproc->async_recv |= oneway;\n\t}\n\n\tif ((frozen && !oneway) || proc->is_dead ||\n\t\t\t(thread && thread->is_dead)) {\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_node_unlock(node);\n\t\treturn frozen ? BR_FROZEN_REPLY : BR_DEAD_REPLY;\n\t}\n\n\tif (!thread && !pending_async)\n\t\tthread = binder_select_thread_ilocked(proc);\n\n\tif (thread) {\n\t\tbinder_enqueue_thread_work_ilocked(thread, &t->work);\n\t} else if (!pending_async) {\n\t\tbinder_enqueue_work_ilocked(&t->work, &proc->todo);\n\t} else {\n\t\tif ((t->flags & TF_UPDATE_TXN) && frozen) {\n\t\t\tt_outdated = binder_find_outdated_transaction_ilocked(t,\n\t\t\t\t\t\t\t\t\t      &node->async_todo);\n\t\t\tif (t_outdated) {\n\t\t\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t\t\t     \"txn %d supersedes %d\\n\",\n\t\t\t\t\t     t->debug_id, t_outdated->debug_id);\n\t\t\t\tlist_del_init(&t_outdated->work.entry);\n\t\t\t\tproc->outstanding_txns--;\n\t\t\t}\n\t\t}\n\t\tbinder_enqueue_work_ilocked(&t->work, &node->async_todo);\n\t}\n\n\tif (!pending_async)\n\t\tbinder_wakeup_thread_ilocked(proc, thread, !oneway /* sync */);\n\n\tproc->outstanding_txns++;\n\tbinder_inner_proc_unlock(proc);\n\tbinder_node_unlock(node);\n\n\t/*\n\t * To reduce potential contention, free the outdated transaction and\n\t * buffer after releasing the locks.\n\t */\n\tif (t_outdated) {\n\t\tstruct binder_buffer *buffer = t_outdated->buffer;\n\n\t\tt_outdated->buffer = NULL;\n\t\tbuffer->transaction = NULL;\n\t\ttrace_binder_transaction_update_buffer_release(buffer);\n\t\tbinder_release_entire_buffer(proc, NULL, buffer, false);\n\t\tbinder_alloc_free_buf(&proc->alloc, buffer);\n\t\tkfree(t_outdated);\n\t\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\n\t}\n\n\tif (oneway && frozen)\n\t\treturn BR_TRANSACTION_PENDING_FROZEN;\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4072,
            "cve_id": "CVE-2023-2162",
            "code_snippet": "static struct iscsi_cls_session *\niscsi_sw_tcp_session_create(struct iscsi_endpoint *ep, uint16_t cmds_max,\n\t\t\t    uint16_t qdepth, uint32_t initial_cmdsn)\n{\n\tstruct iscsi_cls_session *cls_session;\n\tstruct iscsi_session *session;\n\tstruct iscsi_sw_tcp_host *tcp_sw_host;\n\tstruct Scsi_Host *shost;\n\tint rc;\n\n\tif (ep) {\n\t\tprintk(KERN_ERR \"iscsi_tcp: invalid ep %p.\\n\", ep);\n\t\treturn NULL;\n\t}\n\n\tshost = iscsi_host_alloc(&iscsi_sw_tcp_sht,\n\t\t\t\t sizeof(struct iscsi_sw_tcp_host), 1);\n\tif (!shost)\n\t\treturn NULL;\n\tshost->transportt = iscsi_sw_tcp_scsi_transport;\n\tshost->cmd_per_lun = qdepth;\n\tshost->max_lun = iscsi_max_lun;\n\tshost->max_id = 0;\n\tshost->max_channel = 0;\n\tshost->max_cmd_len = SCSI_MAX_VARLEN_CDB_SIZE;\n\n\trc = iscsi_host_get_max_scsi_cmds(shost, cmds_max);\n\tif (rc < 0)\n\t\tgoto free_host;\n\tshost->can_queue = rc;\n\n\tif (iscsi_host_add(shost, NULL))\n\t\tgoto free_host;\n\n\tcls_session = iscsi_session_setup(&iscsi_sw_tcp_transport, shost,\n\t\t\t\t\t  cmds_max, 0,\n\t\t\t\t\t  sizeof(struct iscsi_tcp_task) +\n\t\t\t\t\t  sizeof(struct iscsi_sw_tcp_hdrbuf),\n\t\t\t\t\t  initial_cmdsn, 0);\n\tif (!cls_session)\n\t\tgoto remove_host;\n\tsession = cls_session->dd_data;\n\n\tif (iscsi_tcp_r2tpool_alloc(session))\n\t\tgoto remove_session;\n\n\t/* We are now fully setup so expose the session to sysfs. */\n\ttcp_sw_host = iscsi_host_priv(shost);\n\ttcp_sw_host->session = session;\n\treturn cls_session;\n\nremove_session:\n\tiscsi_session_teardown(cls_session);\nremove_host:\n\tiscsi_host_remove(shost, false);\nfree_host:\n\tiscsi_host_free(shost);\n\treturn NULL;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4103,
            "cve_id": "CVE-2023-23586",
            "code_snippet": "static void io_worker_exit(struct io_worker *worker)\n{\n\tstruct io_wqe *wqe = worker->wqe;\n\tstruct io_wqe_acct *acct = io_wqe_get_acct(worker);\n\n\t/*\n\t * If we're not at zero, someone else is holding a brief reference\n\t * to the worker. Wait for that to go away.\n\t */\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tif (!refcount_dec_and_test(&worker->ref))\n\t\tschedule();\n\t__set_current_state(TASK_RUNNING);\n\n\tpreempt_disable();\n\tcurrent->flags &= ~PF_IO_WORKER;\n\tif (worker->flags & IO_WORKER_F_RUNNING)\n\t\tatomic_dec(&acct->nr_running);\n\tif (!(worker->flags & IO_WORKER_F_BOUND))\n\t\tatomic_dec(&wqe->wq->user->processes);\n\tworker->flags = 0;\n\tpreempt_enable();\n\n\tif (worker->saved_creds) {\n\t\trevert_creds(worker->saved_creds);\n\t\tworker->cur_creds = worker->saved_creds = NULL;\n\t}\n\n\traw_spin_lock_irq(&wqe->lock);\n\thlist_nulls_del_rcu(&worker->nulls_node);\n\tlist_del_rcu(&worker->all_list);\n\tacct->nr_workers--;\n\traw_spin_unlock_irq(&wqe->lock);\n\n\tkfree_rcu(worker, rcu);\n\tif (refcount_dec_and_test(&wqe->wq->refs))\n\t\tcomplete(&wqe->wq->done);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4104,
            "cve_id": "CVE-2023-23586",
            "code_snippet": "static void __io_worker_idle(struct io_wqe *wqe, struct io_worker *worker)\n\t__must_hold(wqe->lock)\n{\n\tif (!(worker->flags & IO_WORKER_F_FREE)) {\n\t\tworker->flags |= IO_WORKER_F_FREE;\n\t\thlist_nulls_add_head_rcu(&worker->nulls_node, &wqe->free_list);\n\t}\n\tif (worker->saved_creds) {\n\t\trevert_creds(worker->saved_creds);\n\t\tworker->cur_creds = worker->saved_creds = NULL;\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4105,
            "cve_id": "CVE-2023-23586",
            "code_snippet": "static bool io_match_task(struct io_kiocb *head,\n\t\t\t  struct task_struct *task,\n\t\t\t  struct files_struct *files)\n{\n\tstruct io_kiocb *req;\n\n\tif (task && head->task != task) {\n\t\t/* in terms of cancelation, always match if req task is dead */\n\t\tif (head->task->flags & PF_EXITING)\n\t\t\treturn true;\n\t\treturn false;\n\t}\n\tif (!files)\n\t\treturn true;\n\n\tio_for_each_link(req, head) {\n\t\tif (!(req->flags & REQ_F_WORK_INITIALIZED))\n\t\t\tcontinue;\n\t\tif (req->file && req->file->f_op == &io_uring_fops)\n\t\t\treturn true;\n\t\tif (req->task->files == files)\n\t\t\treturn true;\n\t}\n\treturn false;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4106,
            "cve_id": "CVE-2023-23586",
            "code_snippet": "static void io_prep_async_work(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_req_init_async(req);\n\n\tif (req->flags & REQ_F_FORCE_ASYNC)\n\t\treq->work.flags |= IO_WQ_WORK_CONCURRENT;\n\n\tif (req->flags & REQ_F_ISREG) {\n\t\tif (def->hash_reg_file || (ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tio_wq_hash_work(&req->work, file_inode(req->file));\n\t} else {\n\t\tif (def->unbound_nonreg_file)\n\t\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t}\n\tif (!req->work.creds)\n\t\treq->work.creds = get_current_cred();\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4107,
            "cve_id": "CVE-2023-23586",
            "code_snippet": "static int io_uring_show_cred(int id, void *p, void *data)\n{\n\tconst struct cred *cred = p;\n\tstruct seq_file *m = data;\n\tstruct user_namespace *uns = seq_user_ns(m);\n\tstruct group_info *gi;\n\tkernel_cap_t cap;\n\tunsigned __capi;\n\tint g;\n\n\tseq_printf(m, \"%5d\\n\", id);\n\tseq_put_decimal_ull(m, \"\\tUid:\\t\", from_kuid_munged(uns, cred->uid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->euid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->suid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->fsuid));\n\tseq_put_decimal_ull(m, \"\\n\\tGid:\\t\", from_kgid_munged(uns, cred->gid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->egid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->sgid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->fsgid));\n\tseq_puts(m, \"\\n\\tGroups:\\t\");\n\tgi = cred->group_info;\n\tfor (g = 0; g < gi->ngroups; g++) {\n\t\tseq_put_decimal_ull(m, g ? \" \" : \"\",\n\t\t\t\t\tfrom_kgid_munged(uns, gi->gid[g]));\n\t}\n\tseq_puts(m, \"\\n\\tCapEff:\\t\");\n\tcap = cred->cap_effective;\n\tCAP_FOR_EACH_U32(__capi)\n\t\tseq_put_hex_ll(m, NULL, cap.cap[CAP_LAST_U32 - __capi], 8);\n\tseq_putc(m, '\\n');\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4108,
            "cve_id": "CVE-2023-23586",
            "code_snippet": "static inline void io_req_init_async(struct io_kiocb *req)\n{\n\tif (req->flags & REQ_F_WORK_INITIALIZED)\n\t\treturn;\n\n\t__io_req_init_async(req);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4120,
            "cve_id": "CVE-2023-25012",
            "code_snippet": "static void bigben_set_led(struct led_classdev *led,\n\tenum led_brightness value)\n{\n\tstruct device *dev = led->dev->parent;\n\tstruct hid_device *hid = to_hid_device(dev);\n\tstruct bigben_device *bigben = hid_get_drvdata(hid);\n\tint n;\n\tbool work;\n\tunsigned long flags;\n\n\tif (!bigben) {\n\t\thid_err(hid, \"no device data\\n\");\n\t\treturn;\n\t}\n\n\tfor (n = 0; n < NUM_LEDS; n++) {\n\t\tif (led == bigben->leds[n]) {\n\t\t\tspin_lock_irqsave(&bigben->lock, flags);\n\t\t\tif (value == LED_OFF) {\n\t\t\t\twork = (bigben->led_state & BIT(n));\n\t\t\t\tbigben->led_state &= ~BIT(n);\n\t\t\t} else {\n\t\t\t\twork = !(bigben->led_state & BIT(n));\n\t\t\t\tbigben->led_state |= BIT(n);\n\t\t\t}\n\t\t\tspin_unlock_irqrestore(&bigben->lock, flags);\n\n\t\t\tif (work) {\n\t\t\t\tbigben->work_led = true;\n\t\t\t\tbigben_schedule_work(bigben);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4124,
            "cve_id": "CVE-2023-2513",
            "code_snippet": "int ext4_xattr_ibody_set(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_xattr_info *i,\n\t\t\t\tstruct ext4_xattr_ibody_find *is)\n{\n\tstruct ext4_xattr_ibody_header *header;\n\tstruct ext4_xattr_search *s = &is->s;\n\tint error;\n\n\tif (!EXT4_INODE_HAS_XATTR_SPACE(inode))\n\t\treturn -ENOSPC;\n\n\terror = ext4_xattr_set_entry(i, s, handle, inode, false /* is_block */);\n\tif (error)\n\t\treturn error;\n\theader = IHDR(inode, ext4_raw_inode(&is->iloc));\n\tif (!IS_LAST_ENTRY(s->first)) {\n\t\theader->h_magic = cpu_to_le32(EXT4_XATTR_MAGIC);\n\t\text4_set_inode_state(inode, EXT4_STATE_XATTR);\n\t} else {\n\t\theader->h_magic = cpu_to_le32(0);\n\t\text4_clear_inode_state(inode, EXT4_STATE_XATTR);\n\t}\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4170,
            "cve_id": "CVE-2023-32233",
            "code_snippet": "static void nft_dynset_activate(const struct nft_ctx *ctx,\n\t\t\t\tconst struct nft_expr *expr)\n{\n\tstruct nft_dynset *priv = nft_expr_priv(expr);\n\n\tnf_tables_activate_set(ctx, priv->set);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4229,
            "cve_id": "CVE-2023-3389",
            "code_snippet": "static __cold void io_uring_try_cancel_requests(struct io_ring_ctx *ctx,\n\t\t\t\t\t\tstruct task_struct *task,\n\t\t\t\t\t\tbool cancel_all)\n{\n\tstruct io_task_cancel cancel = { .task = task, .all = cancel_all, };\n\tstruct io_uring_task *tctx = task ? task->io_uring : NULL;\n\n\t/* failed during ring init, it couldn't have issued any requests */\n\tif (!ctx->rings)\n\t\treturn;\n\n\twhile (1) {\n\t\tenum io_wq_cancel cret;\n\t\tbool ret = false;\n\n\t\tif (!task) {\n\t\t\tret |= io_uring_try_cancel_iowq(ctx);\n\t\t} else if (tctx && tctx->io_wq) {\n\t\t\t/*\n\t\t\t * Cancels requests of all rings, not only @ctx, but\n\t\t\t * it's fine as the task is in exit/exec.\n\t\t\t */\n\t\t\tcret = io_wq_cancel_cb(tctx->io_wq, io_cancel_task_cb,\n\t\t\t\t\t       &cancel, true);\n\t\t\tret |= (cret != IO_WQ_CANCEL_NOTFOUND);\n\t\t}\n\n\t\t/* SQPOLL thread does its own polling */\n\t\tif ((!(ctx->flags & IORING_SETUP_SQPOLL) && cancel_all) ||\n\t\t    (ctx->sq_data && ctx->sq_data->thread == current)) {\n\t\t\twhile (!wq_list_empty(&ctx->iopoll_list)) {\n\t\t\t\tio_iopoll_try_reap_events(ctx);\n\t\t\t\tret = true;\n\t\t\t}\n\t\t}\n\n\t\tret |= io_cancel_defer_files(ctx, task, cancel_all);\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tret |= io_poll_remove_all(ctx, task, cancel_all);\n\t\tmutex_unlock(&ctx->uring_lock);\n\t\tret |= io_kill_timeouts(ctx, task, cancel_all);\n\t\tif (task)\n\t\t\tret |= io_run_task_work();\n\t\tif (!ret)\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4230,
            "cve_id": "CVE-2023-3389",
            "code_snippet": "static __cold struct io_ring_ctx *io_ring_ctx_alloc(struct io_uring_params *p)\n{\n\tstruct io_ring_ctx *ctx;\n\tint hash_bits;\n\n\tctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn NULL;\n\n\txa_init(&ctx->io_bl_xa);\n\n\t/*\n\t * Use 5 bits less than the max cq entries, that should give us around\n\t * 32 entries per hash list if totally full and uniformly spread, but\n\t * don't keep too many buckets to not overconsume memory.\n\t */\n\thash_bits = ilog2(p->cq_entries) - 5;\n\thash_bits = clamp(hash_bits, 1, 8);\n\tif (io_alloc_hash_table(&ctx->cancel_table, hash_bits))\n\t\tgoto err;\n\tif (io_alloc_hash_table(&ctx->cancel_table_locked, hash_bits))\n\t\tgoto err;\n\n\tctx->dummy_ubuf = kzalloc(sizeof(*ctx->dummy_ubuf), GFP_KERNEL);\n\tif (!ctx->dummy_ubuf)\n\t\tgoto err;\n\t/* set invalid range, so io_import_fixed() fails meeting it */\n\tctx->dummy_ubuf->ubuf = -1UL;\n\n\tif (percpu_ref_init(&ctx->refs, io_ring_ctx_ref_free,\n\t\t\t    PERCPU_REF_ALLOW_REINIT, GFP_KERNEL))\n\t\tgoto err;\n\n\tctx->flags = p->flags;\n\tinit_waitqueue_head(&ctx->sqo_sq_wait);\n\tINIT_LIST_HEAD(&ctx->sqd_list);\n\tINIT_LIST_HEAD(&ctx->cq_overflow_list);\n\tINIT_LIST_HEAD(&ctx->io_buffers_cache);\n\tINIT_LIST_HEAD(&ctx->apoll_cache);\n\tinit_completion(&ctx->ref_comp);\n\txa_init_flags(&ctx->personalities, XA_FLAGS_ALLOC1);\n\tmutex_init(&ctx->uring_lock);\n\tinit_waitqueue_head(&ctx->cq_wait);\n\tspin_lock_init(&ctx->completion_lock);\n\tspin_lock_init(&ctx->timeout_lock);\n\tINIT_WQ_LIST(&ctx->iopoll_list);\n\tINIT_LIST_HEAD(&ctx->io_buffers_pages);\n\tINIT_LIST_HEAD(&ctx->io_buffers_comp);\n\tINIT_LIST_HEAD(&ctx->defer_list);\n\tINIT_LIST_HEAD(&ctx->timeout_list);\n\tINIT_LIST_HEAD(&ctx->ltimeout_list);\n\tspin_lock_init(&ctx->rsrc_ref_lock);\n\tINIT_LIST_HEAD(&ctx->rsrc_ref_list);\n\tINIT_DELAYED_WORK(&ctx->rsrc_put_work, io_rsrc_put_work);\n\tinit_llist_head(&ctx->rsrc_put_llist);\n\tINIT_LIST_HEAD(&ctx->tctx_list);\n\tctx->submit_state.free_list.next = NULL;\n\tINIT_WQ_LIST(&ctx->locked_free_list);\n\tINIT_DELAYED_WORK(&ctx->fallback_work, io_fallback_req_func);\n\tINIT_WQ_LIST(&ctx->submit_state.compl_reqs);\n\treturn ctx;\nerr:\n\tkfree(ctx->dummy_ubuf);\n\tkfree(ctx->cancel_table.hbs);\n\tkfree(ctx->cancel_table_locked.hbs);\n\tkfree(ctx->io_bl);\n\txa_destroy(&ctx->io_bl_xa);\n\tkfree(ctx);\n\treturn NULL;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4231,
            "cve_id": "CVE-2023-3389",
            "code_snippet": "static __cold void io_ring_ctx_free(struct io_ring_ctx *ctx)\n{\n\tio_sq_thread_finish(ctx);\n\n\tif (ctx->mm_account) {\n\t\tmmdrop(ctx->mm_account);\n\t\tctx->mm_account = NULL;\n\t}\n\n\tio_rsrc_refs_drop(ctx);\n\t/* __io_rsrc_put_work() may need uring_lock to progress, wait w/o it */\n\tio_wait_rsrc_data(ctx->buf_data);\n\tio_wait_rsrc_data(ctx->file_data);\n\n\tmutex_lock(&ctx->uring_lock);\n\tif (ctx->buf_data)\n\t\t__io_sqe_buffers_unregister(ctx);\n\tif (ctx->file_data)\n\t\t__io_sqe_files_unregister(ctx);\n\tif (ctx->rings)\n\t\t__io_cqring_overflow_flush(ctx, true);\n\tio_eventfd_unregister(ctx);\n\tio_flush_apoll_cache(ctx);\n\tmutex_unlock(&ctx->uring_lock);\n\tio_destroy_buffers(ctx);\n\tif (ctx->sq_creds)\n\t\tput_cred(ctx->sq_creds);\n\tif (ctx->submitter_task)\n\t\tput_task_struct(ctx->submitter_task);\n\n\t/* there are no registered resources left, nobody uses it */\n\tif (ctx->rsrc_node)\n\t\tio_rsrc_node_destroy(ctx->rsrc_node);\n\tif (ctx->rsrc_backup_node)\n\t\tio_rsrc_node_destroy(ctx->rsrc_backup_node);\n\tflush_delayed_work(&ctx->rsrc_put_work);\n\tflush_delayed_work(&ctx->fallback_work);\n\n\tWARN_ON_ONCE(!list_empty(&ctx->rsrc_ref_list));\n\tWARN_ON_ONCE(!llist_empty(&ctx->rsrc_put_llist));\n\n#if defined(CONFIG_UNIX)\n\tif (ctx->ring_sock) {\n\t\tctx->ring_sock->file = NULL; /* so that iput() is called */\n\t\tsock_release(ctx->ring_sock);\n\t}\n#endif\n\tWARN_ON_ONCE(!list_empty(&ctx->ltimeout_list));\n\n\tio_mem_free(ctx->rings);\n\tio_mem_free(ctx->sq_sqes);\n\n\tpercpu_ref_exit(&ctx->refs);\n\tfree_uid(ctx->user);\n\tio_req_caches_free(ctx);\n\tif (ctx->hash_map)\n\t\tio_wq_put_hash(ctx->hash_map);\n\tkfree(ctx->cancel_table.hbs);\n\tkfree(ctx->cancel_table_locked.hbs);\n\tkfree(ctx->dummy_ubuf);\n\tkfree(ctx->io_bl);\n\txa_destroy(&ctx->io_bl_xa);\n\tkfree(ctx);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4232,
            "cve_id": "CVE-2023-3389",
            "code_snippet": "int io_poll_remove(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_poll_update *poll_update = io_kiocb_to_cmd(req);\n\tstruct io_cancel_data cd = { .data = poll_update->old_user_data, };\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_hash_bucket *bucket;\n\tstruct io_kiocb *preq;\n\tint ret2, ret = 0;\n\tbool locked;\n\n\tpreq = io_poll_find(ctx, true, &cd, &ctx->cancel_table, &bucket);\n\tret2 = io_poll_disarm(preq);\n\tif (bucket)\n\t\tspin_unlock(&bucket->lock);\n\tif (!ret2)\n\t\tgoto found;\n\tif (ret2 != -ENOENT) {\n\t\tret = ret2;\n\t\tgoto out;\n\t}\n\n\tio_ring_submit_lock(ctx, issue_flags);\n\tpreq = io_poll_find(ctx, true, &cd, &ctx->cancel_table_locked, &bucket);\n\tret2 = io_poll_disarm(preq);\n\tif (bucket)\n\t\tspin_unlock(&bucket->lock);\n\tio_ring_submit_unlock(ctx, issue_flags);\n\tif (ret2) {\n\t\tret = ret2;\n\t\tgoto out;\n\t}\n\nfound:\n\tif (poll_update->update_events || poll_update->update_user_data) {\n\t\t/* only mask one event flags, keep behavior flags */\n\t\tif (poll_update->update_events) {\n\t\t\tstruct io_poll *poll = io_kiocb_to_cmd(preq);\n\n\t\t\tpoll->events &= ~0xffff;\n\t\t\tpoll->events |= poll_update->events & 0xffff;\n\t\t\tpoll->events |= IO_POLL_UNMASK;\n\t\t}\n\t\tif (poll_update->update_user_data)\n\t\t\tpreq->cqe.user_data = poll_update->new_user_data;\n\n\t\tret2 = io_poll_add(preq, issue_flags);\n\t\t/* successfully updated, don't complete poll request */\n\t\tif (!ret2 || ret2 == -EIOCBQUEUED)\n\t\t\tgoto out;\n\t}\n\n\treq_set_fail(preq);\n\tio_req_set_res(preq, -ECANCELED, 0);\n\tlocked = !(issue_flags & IO_URING_F_UNLOCKED);\n\tio_req_task_complete(preq, &locked);\nout:\n\tif (ret < 0) {\n\t\treq_set_fail(req);\n\t\treturn ret;\n\t}\n\t/* complete update request, we're done with it */\n\tio_req_set_res(req, ret, 0);\n\treturn IOU_OK;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4233,
            "cve_id": "CVE-2023-3389",
            "code_snippet": "int io_arm_poll_handler(struct io_kiocb *req, unsigned issue_flags)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct async_poll *apoll;\n\tstruct io_poll_table ipt;\n\t__poll_t mask = POLLPRI | POLLERR | EPOLLET;\n\tint ret;\n\n\t/*\n\t * apoll requests already grab the mutex to complete in the tw handler,\n\t * so removal from the mutex-backed hash is free, use it by default.\n\t */\n\tif (issue_flags & IO_URING_F_UNLOCKED)\n\t\treq->flags &= ~REQ_F_HASH_LOCKED;\n\telse\n\t\treq->flags |= REQ_F_HASH_LOCKED;\n\n\tif (!def->pollin && !def->pollout)\n\t\treturn IO_APOLL_ABORTED;\n\tif (!file_can_poll(req->file))\n\t\treturn IO_APOLL_ABORTED;\n\tif ((req->flags & (REQ_F_POLLED|REQ_F_PARTIAL_IO)) == REQ_F_POLLED)\n\t\treturn IO_APOLL_ABORTED;\n\tif (!(req->flags & REQ_F_APOLL_MULTISHOT))\n\t\tmask |= EPOLLONESHOT;\n\n\tif (def->pollin) {\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\n\t\t/* If reading from MSG_ERRQUEUE using recvmsg, ignore POLLIN */\n\t\tif (req->flags & REQ_F_CLEAR_POLLIN)\n\t\t\tmask &= ~EPOLLIN;\n\t} else {\n\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\t}\n\tif (def->poll_exclusive)\n\t\tmask |= EPOLLEXCLUSIVE;\n\tif (req->flags & REQ_F_POLLED) {\n\t\tapoll = req->apoll;\n\t\tkfree(apoll->double_poll);\n\t} else if (!(issue_flags & IO_URING_F_UNLOCKED) &&\n\t\t   !list_empty(&ctx->apoll_cache)) {\n\t\tapoll = list_first_entry(&ctx->apoll_cache, struct async_poll,\n\t\t\t\t\t\tpoll.wait.entry);\n\t\tlist_del_init(&apoll->poll.wait.entry);\n\t} else {\n\t\tapoll = kmalloc(sizeof(*apoll), GFP_ATOMIC);\n\t\tif (unlikely(!apoll))\n\t\t\treturn IO_APOLL_ABORTED;\n\t}\n\tapoll->double_poll = NULL;\n\treq->apoll = apoll;\n\treq->flags |= REQ_F_POLLED;\n\tipt.pt._qproc = io_async_queue_proc;\n\n\tio_kbuf_recycle(req, issue_flags);\n\n\tret = __io_arm_poll_handler(req, &apoll->poll, &ipt, mask);\n\tif (ret || ipt.error)\n\t\treturn ret ? IO_APOLL_READY : IO_APOLL_ABORTED;\n\n\ttrace_io_uring_poll_arm(ctx, req, req->cqe.user_data, req->opcode,\n\t\t\t\tmask, apoll->poll.events);\n\treturn IO_APOLL_OK;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4268,
            "cve_id": "CVE-2023-3439",
            "code_snippet": "void mctp_dev_put(struct mctp_dev *mdev)\n{\n\tif (mdev && refcount_dec_and_test(&mdev->refs)) {\n\t\tkfree(mdev->addrs);\n\t\tdev_put(mdev->dev);\n\t\tkfree_rcu(mdev, rcu);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4273,
            "cve_id": "CVE-2023-35823",
            "code_snippet": "int saa7134_vbi_fini(struct saa7134_dev *dev)\n{\n\t/* nothing */\n\tdel_timer_sync(&dev->vbi_q.timeout);\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4282,
            "cve_id": "CVE-2023-3610",
            "code_snippet": "void nft_rule_expr_activate(const struct nft_ctx *ctx, struct nft_rule *rule)\n{\n\tstruct nft_expr *expr;\n\n\texpr = nft_expr_first(rule);\n\twhile (nft_expr_more(rule, expr)) {\n\t\tif (expr->ops->activate)\n\t\t\texpr->ops->activate(ctx, expr);\n\n\t\texpr = nft_expr_next(expr);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4283,
            "cve_id": "CVE-2023-3610",
            "code_snippet": "static struct nft_trans *nft_trans_chain_add(struct nft_ctx *ctx, int msg_type)\n{\n\tstruct nft_trans *trans;\n\n\ttrans = nft_trans_alloc(ctx, msg_type, sizeof(struct nft_trans_chain));\n\tif (trans == NULL)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (msg_type == NFT_MSG_NEWCHAIN) {\n\t\tnft_activate_next(ctx->net, ctx->chain);\n\n\t\tif (ctx->nla[NFTA_CHAIN_ID]) {\n\t\t\tnft_trans_chain_id(trans) =\n\t\t\t\tntohl(nla_get_be32(ctx->nla[NFTA_CHAIN_ID]));\n\t\t}\n\t}\n\tnft_trans_chain(trans) = ctx->chain;\n\tnft_trans_commit_list_add_tail(ctx->net, trans);\n\n\treturn trans;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4284,
            "cve_id": "CVE-2023-3610",
            "code_snippet": "void nft_data_hold(const struct nft_data *data, enum nft_data_types type)\n{\n\tstruct nft_chain *chain;\n\n\tif (type == NFT_DATA_VERDICT) {\n\t\tswitch (data->verdict.code) {\n\t\tcase NFT_JUMP:\n\t\tcase NFT_GOTO:\n\t\t\tchain = data->verdict.chain;\n\t\t\tchain->use++;\n\t\t\tbreak;\n\t\t}\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4285,
            "cve_id": "CVE-2023-3610",
            "code_snippet": "void nft_rule_expr_deactivate(const struct nft_ctx *ctx, struct nft_rule *rule,\n\t\t\t      enum nft_trans_phase phase)\n{\n\tstruct nft_expr *expr;\n\n\texpr = nft_expr_first(rule);\n\twhile (nft_expr_more(rule, expr)) {\n\t\tif (expr->ops->deactivate)\n\t\t\texpr->ops->deactivate(ctx, expr, phase);\n\n\t\texpr = nft_expr_next(expr);\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4286,
            "cve_id": "CVE-2023-3610",
            "code_snippet": "void nf_tables_rule_destroy(const struct nft_ctx *ctx, struct nft_rule *rule)\n{\n\tstruct nft_expr *expr, *next;\n\n\t/*\n\t * Careful: some expressions might not be initialized in case this\n\t * is called on error from nf_tables_newrule().\n\t */\n\texpr = nft_expr_first(rule);\n\twhile (nft_expr_more(rule, expr)) {\n\t\tnext = nft_expr_next(expr);\n\t\tnf_tables_expr_destroy(ctx, expr);\n\t\texpr = next;\n\t}\n\tkfree(rule);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4287,
            "cve_id": "CVE-2023-3610",
            "code_snippet": "static int __nf_tables_abort(struct net *net, enum nfnl_abort_action action)\n{\n\tstruct nftables_pernet *nft_net = nft_pernet(net);\n\tstruct nft_trans *trans, *next;\n\tLIST_HEAD(set_update_list);\n\tstruct nft_trans_elem *te;\n\n\tif (action == NFNL_ABORT_VALIDATE &&\n\t    nf_tables_validate(net) < 0)\n\t\treturn -EAGAIN;\n\n\tlist_for_each_entry_safe_reverse(trans, next, &nft_net->commit_list,\n\t\t\t\t\t list) {\n\t\tswitch (trans->msg_type) {\n\t\tcase NFT_MSG_NEWTABLE:\n\t\t\tif (nft_trans_table_update(trans)) {\n\t\t\t\tif (!(trans->ctx.table->flags & __NFT_TABLE_F_UPDATE)) {\n\t\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (trans->ctx.table->flags & __NFT_TABLE_F_WAS_DORMANT) {\n\t\t\t\t\tnf_tables_table_disable(net, trans->ctx.table);\n\t\t\t\t\ttrans->ctx.table->flags |= NFT_TABLE_F_DORMANT;\n\t\t\t\t} else if (trans->ctx.table->flags & __NFT_TABLE_F_WAS_AWAKEN) {\n\t\t\t\t\ttrans->ctx.table->flags &= ~NFT_TABLE_F_DORMANT;\n\t\t\t\t}\n\t\t\t\ttrans->ctx.table->flags &= ~__NFT_TABLE_F_UPDATE;\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t} else {\n\t\t\t\tlist_del_rcu(&trans->ctx.table->list);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELTABLE:\n\t\tcase NFT_MSG_DESTROYTABLE:\n\t\t\tnft_clear(trans->ctx.net, trans->ctx.table);\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWCHAIN:\n\t\t\tif (nft_trans_chain_update(trans)) {\n\t\t\t\tnft_netdev_unregister_hooks(net,\n\t\t\t\t\t\t\t    &nft_trans_chain_hooks(trans),\n\t\t\t\t\t\t\t    true);\n\t\t\t\tfree_percpu(nft_trans_chain_stats(trans));\n\t\t\t\tkfree(nft_trans_chain_name(trans));\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t} else {\n\t\t\t\tif (nft_trans_chain_bound(trans)) {\n\t\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\ttrans->ctx.table->use--;\n\t\t\t\tnft_chain_del(trans->ctx.chain);\n\t\t\t\tnf_tables_unregister_hook(trans->ctx.net,\n\t\t\t\t\t\t\t  trans->ctx.table,\n\t\t\t\t\t\t\t  trans->ctx.chain);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELCHAIN:\n\t\tcase NFT_MSG_DESTROYCHAIN:\n\t\t\tif (nft_trans_chain_update(trans)) {\n\t\t\t\tlist_splice(&nft_trans_chain_hooks(trans),\n\t\t\t\t\t    &nft_trans_basechain(trans)->hook_list);\n\t\t\t} else {\n\t\t\t\ttrans->ctx.table->use++;\n\t\t\t\tnft_clear(trans->ctx.net, trans->ctx.chain);\n\t\t\t}\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWRULE:\n\t\t\tif (nft_trans_rule_bound(trans)) {\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttrans->ctx.chain->use--;\n\t\t\tlist_del_rcu(&nft_trans_rule(trans)->list);\n\t\t\tnft_rule_expr_deactivate(&trans->ctx,\n\t\t\t\t\t\t nft_trans_rule(trans),\n\t\t\t\t\t\t NFT_TRANS_ABORT);\n\t\t\tif (trans->ctx.chain->flags & NFT_CHAIN_HW_OFFLOAD)\n\t\t\t\tnft_flow_rule_destroy(nft_trans_flow_rule(trans));\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELRULE:\n\t\tcase NFT_MSG_DESTROYRULE:\n\t\t\ttrans->ctx.chain->use++;\n\t\t\tnft_clear(trans->ctx.net, nft_trans_rule(trans));\n\t\t\tnft_rule_expr_activate(&trans->ctx, nft_trans_rule(trans));\n\t\t\tif (trans->ctx.chain->flags & NFT_CHAIN_HW_OFFLOAD)\n\t\t\t\tnft_flow_rule_destroy(nft_trans_flow_rule(trans));\n\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWSET:\n\t\t\tif (nft_trans_set_update(trans)) {\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttrans->ctx.table->use--;\n\t\t\tif (nft_trans_set_bound(trans)) {\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tlist_del_rcu(&nft_trans_set(trans)->list);\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELSET:\n\t\tcase NFT_MSG_DESTROYSET:\n\t\t\ttrans->ctx.table->use++;\n\t\t\tnft_clear(trans->ctx.net, nft_trans_set(trans));\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWSETELEM:\n\t\t\tif (nft_trans_elem_set_bound(trans)) {\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tte = (struct nft_trans_elem *)trans->data;\n\t\t\tnft_setelem_remove(net, te->set, &te->elem);\n\t\t\tif (!nft_setelem_is_catchall(te->set, &te->elem))\n\t\t\t\tatomic_dec(&te->set->nelems);\n\n\t\t\tif (te->set->ops->abort &&\n\t\t\t    list_empty(&te->set->pending_update)) {\n\t\t\t\tlist_add_tail(&te->set->pending_update,\n\t\t\t\t\t      &set_update_list);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELSETELEM:\n\t\tcase NFT_MSG_DESTROYSETELEM:\n\t\t\tte = (struct nft_trans_elem *)trans->data;\n\n\t\t\tnft_setelem_data_activate(net, te->set, &te->elem);\n\t\t\tnft_setelem_activate(net, te->set, &te->elem);\n\t\t\tif (!nft_setelem_is_catchall(te->set, &te->elem))\n\t\t\t\tte->set->ndeact--;\n\n\t\t\tif (te->set->ops->abort &&\n\t\t\t    list_empty(&te->set->pending_update)) {\n\t\t\t\tlist_add_tail(&te->set->pending_update,\n\t\t\t\t\t      &set_update_list);\n\t\t\t}\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWOBJ:\n\t\t\tif (nft_trans_obj_update(trans)) {\n\t\t\t\tnft_obj_destroy(&trans->ctx, nft_trans_obj_newobj(trans));\n\t\t\t\tnft_trans_destroy(trans);\n\t\t\t} else {\n\t\t\t\ttrans->ctx.table->use--;\n\t\t\t\tnft_obj_del(nft_trans_obj(trans));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELOBJ:\n\t\tcase NFT_MSG_DESTROYOBJ:\n\t\t\ttrans->ctx.table->use++;\n\t\t\tnft_clear(trans->ctx.net, nft_trans_obj(trans));\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\tcase NFT_MSG_NEWFLOWTABLE:\n\t\t\tif (nft_trans_flowtable_update(trans)) {\n\t\t\t\tnft_unregister_flowtable_net_hooks(net,\n\t\t\t\t\t\t&nft_trans_flowtable_hooks(trans));\n\t\t\t} else {\n\t\t\t\ttrans->ctx.table->use--;\n\t\t\t\tlist_del_rcu(&nft_trans_flowtable(trans)->list);\n\t\t\t\tnft_unregister_flowtable_net_hooks(net,\n\t\t\t\t\t\t&nft_trans_flowtable(trans)->hook_list);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_MSG_DELFLOWTABLE:\n\t\tcase NFT_MSG_DESTROYFLOWTABLE:\n\t\t\tif (nft_trans_flowtable_update(trans)) {\n\t\t\t\tlist_splice(&nft_trans_flowtable_hooks(trans),\n\t\t\t\t\t    &nft_trans_flowtable(trans)->hook_list);\n\t\t\t} else {\n\t\t\t\ttrans->ctx.table->use++;\n\t\t\t\tnft_clear(trans->ctx.net, nft_trans_flowtable(trans));\n\t\t\t}\n\t\t\tnft_trans_destroy(trans);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tnft_set_abort_update(&set_update_list);\n\n\tsynchronize_rcu();\n\n\tlist_for_each_entry_safe_reverse(trans, next,\n\t\t\t\t\t &nft_net->commit_list, list) {\n\t\tlist_del(&trans->list);\n\t\tnf_tables_abort_release(trans);\n\t}\n\n\tif (action == NFNL_ABORT_AUTOLOAD)\n\t\tnf_tables_module_autoload(net);\n\telse\n\t\tnf_tables_module_autoload_cleanup(net);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4307,
            "cve_id": "CVE-2023-3863",
            "code_snippet": "void nfc_llcp_mac_is_up(struct nfc_dev *dev, u32 target_idx,\n\t\t\tu8 comm_mode, u8 rf_mode)\n{\n\tstruct nfc_llcp_local *local;\n\n\tpr_debug(\"rf mode %d\\n\", rf_mode);\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL)\n\t\treturn;\n\n\tlocal->target_idx = target_idx;\n\tlocal->comm_mode = comm_mode;\n\tlocal->rf_mode = rf_mode;\n\n\tif (rf_mode == NFC_RF_INITIATOR) {\n\t\tpr_debug(\"Queueing Tx work\\n\");\n\n\t\tschedule_work(&local->tx_work);\n\t} else {\n\t\tmod_timer(&local->link_timer,\n\t\t\t  jiffies + msecs_to_jiffies(local->remote_lto));\n\t}\n\n\tnfc_llcp_local_put(local);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4308,
            "cve_id": "CVE-2023-3863",
            "code_snippet": "struct nfc_llcp_local *nfc_llcp_find_local(struct nfc_dev *dev)\n{\n\tstruct nfc_llcp_local *local;\n\tstruct nfc_llcp_local *res = NULL;\n\n\tspin_lock(&llcp_devices_lock);\n\tlist_for_each_entry(local, &llcp_devices, list)\n\t\tif (local->dev == dev) {\n\t\t\tres = nfc_llcp_local_get(local);\n\t\t\tbreak;\n\t\t}\n\tspin_unlock(&llcp_devices_lock);\n\n\treturn res;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4309,
            "cve_id": "CVE-2023-3863",
            "code_snippet": "void nfc_llcp_unregister_device(struct nfc_dev *dev)\n{\n\tstruct nfc_llcp_local *local = nfc_llcp_remove_local(dev);\n\n\tif (local == NULL) {\n\t\tpr_debug(\"No such device\\n\");\n\t\treturn;\n\t}\n\n\tlocal_cleanup(local);\n\n\tnfc_llcp_local_put(local);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4310,
            "cve_id": "CVE-2023-3863",
            "code_snippet": "static struct nfc_llcp_local *nfc_llcp_local_get(struct nfc_llcp_local *local)\n{\n\tkref_get(&local->ref);\n\n\treturn local;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4311,
            "cve_id": "CVE-2023-3863",
            "code_snippet": "u8 *nfc_llcp_general_bytes(struct nfc_dev *dev, size_t *general_bytes_len)\n{\n\tstruct nfc_llcp_local *local;\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL) {\n\t\t*general_bytes_len = 0;\n\t\treturn NULL;\n\t}\n\n\tnfc_llcp_build_gb(local);\n\n\t*general_bytes_len = local->gb_len;\n\n\tnfc_llcp_local_put(local);\n\n\treturn local->gb;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4312,
            "cve_id": "CVE-2023-3863",
            "code_snippet": "static void local_release(struct kref *ref)\n{\n\tstruct nfc_llcp_local *local;\n\n\tlocal = container_of(ref, struct nfc_llcp_local, ref);\n\n\tlocal_cleanup(local);\n\tkfree(local);\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4313,
            "cve_id": "CVE-2023-3863",
            "code_snippet": "int nfc_llcp_data_received(struct nfc_dev *dev, struct sk_buff *skb)\n{\n\tstruct nfc_llcp_local *local;\n\n\tlocal = nfc_llcp_find_local(dev);\n\tif (local == NULL) {\n\t\tkfree_skb(skb);\n\t\treturn -ENODEV;\n\t}\n\n\t__nfc_llcp_recv(local, skb);\n\n\tnfc_llcp_local_put(local);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4349,
            "cve_id": "CVE-2023-39198",
            "code_snippet": "int qxl_alloc_surf_ioctl(struct drm_device *dev, void *data, struct drm_file *file)\n{\n\tstruct qxl_device *qdev = to_qxl(dev);\n\tstruct drm_qxl_alloc_surf *param = data;\n\tint handle;\n\tint ret;\n\tint size, actual_stride;\n\tstruct qxl_surface surf;\n\n\t/* work out size allocate bo with handle */\n\tactual_stride = param->stride < 0 ? -param->stride : param->stride;\n\tsize = actual_stride * param->height + actual_stride;\n\n\tsurf.format = param->format;\n\tsurf.width = param->width;\n\tsurf.height = param->height;\n\tsurf.stride = param->stride;\n\tsurf.data = 0;\n\n\tret = qxl_gem_object_create_with_handle(qdev, file,\n\t\t\t\t\t\tQXL_GEM_DOMAIN_SURFACE,\n\t\t\t\t\t\tsize,\n\t\t\t\t\t\t&surf,\n\t\t\t\t\t\tNULL, &handle);\n\tif (ret) {\n\t\tDRM_ERROR(\"%s: failed to create gem ret=%d\\n\",\n\t\t\t  __func__, ret);\n\t\treturn -ENOMEM;\n\t} else\n\t\tparam->handle = handle;\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4365,
            "cve_id": "CVE-2023-4244",
            "code_snippet": "static int nft_rcv_nl_event(struct notifier_block *this, unsigned long event,\n\t\t\t    void *ptr)\n{\n\tstruct nft_table *table, *to_delete[8];\n\tstruct nftables_pernet *nft_net;\n\tstruct netlink_notify *n = ptr;\n\tstruct net *net = n->net;\n\tunsigned int deleted;\n\tbool restart = false;\n\tunsigned int gc_seq;\n\n\tif (event != NETLINK_URELEASE || n->protocol != NETLINK_NETFILTER)\n\t\treturn NOTIFY_DONE;\n\n\tnft_net = nft_pernet(net);\n\tdeleted = 0;\n\tmutex_lock(&nft_net->commit_mutex);\n\n\tgc_seq = nft_gc_seq_begin(nft_net);\n\n\tif (!list_empty(&nf_tables_destroy_list))\n\t\trcu_barrier();\nagain:\n\tlist_for_each_entry(table, &nft_net->tables, list) {\n\t\tif (nft_table_has_owner(table) &&\n\t\t    n->portid == table->nlpid) {\n\t\t\t__nft_release_hook(net, table);\n\t\t\tlist_del_rcu(&table->list);\n\t\t\tto_delete[deleted++] = table;\n\t\t\tif (deleted >= ARRAY_SIZE(to_delete))\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tif (deleted) {\n\t\trestart = deleted >= ARRAY_SIZE(to_delete);\n\t\tsynchronize_rcu();\n\t\twhile (deleted)\n\t\t\t__nft_release_table(net, to_delete[--deleted]);\n\n\t\tif (restart)\n\t\t\tgoto again;\n\t}\n\tnft_gc_seq_end(nft_net, gc_seq);\n\n\tmutex_unlock(&nft_net->commit_mutex);\n\n\treturn NOTIFY_DONE;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4391,
            "cve_id": "CVE-2023-45898",
            "code_snippet": "void ext4_es_remove_extent(struct inode *inode, ext4_lblk_t lblk,\n\t\t\t   ext4_lblk_t len)\n{\n\text4_lblk_t end;\n\tint err = 0;\n\tint reserved = 0;\n\tstruct extent_status *es = NULL;\n\n\tif (EXT4_SB(inode->i_sb)->s_mount_state & EXT4_FC_REPLAY)\n\t\treturn;\n\n\ttrace_ext4_es_remove_extent(inode, lblk, len);\n\tes_debug(\"remove [%u/%u) from extent status tree of inode %lu\\n\",\n\t\t lblk, len, inode->i_ino);\n\n\tif (!len)\n\t\treturn;\n\n\tend = lblk + len - 1;\n\tBUG_ON(end < lblk);\n\nretry:\n\tif (err && !es)\n\t\tes = __es_alloc_extent(true);\n\t/*\n\t * ext4_clear_inode() depends on us taking i_es_lock unconditionally\n\t * so that we are sure __es_shrink() is done with the inode before it\n\t * is reclaimed.\n\t */\n\twrite_lock(&EXT4_I(inode)->i_es_lock);\n\terr = __es_remove_extent(inode, lblk, end, &reserved, es);\n\t/* Free preallocated extent if it didn't get used. */\n\tif (es) {\n\t\tif (!es->es_len)\n\t\t\t__es_free_extent(es);\n\t\tes = NULL;\n\t}\n\twrite_unlock(&EXT4_I(inode)->i_es_lock);\n\tif (err)\n\t\tgoto retry;\n\n\text4_es_print_tree(inode);\n\text4_da_release_space(inode, reserved);\n\treturn;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4398,
            "cve_id": "CVE-2023-4611",
            "code_snippet": "\nSYSCALL_DEFINE4(set_mempolicy_home_node, unsigned long, start, unsigned long, len,\n\t\tunsigned long, home_node, unsigned long, flags)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma, *prev;\n\tstruct mempolicy *new, *old;\n\tunsigned long end;\n\tint err = -ENOENT;\n\tVMA_ITERATOR(vmi, mm, start);\n\n\tstart = untagged_addr(start);\n\tif (start & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\t/*\n\t * flags is used for future extension if any.\n\t */\n\tif (flags != 0)\n\t\treturn -EINVAL;\n\n\t/*\n\t * Check home_node is online to avoid accessing uninitialized\n\t * NODE_DATA.\n\t */\n\tif (home_node >= MAX_NUMNODES || !node_online(home_node))\n\t\treturn -EINVAL;\n\n\tlen = PAGE_ALIGN(len);\n\tend = start + len;\n\n\tif (end < start)\n\t\treturn -EINVAL;\n\tif (end == start)\n\t\treturn 0;\n\tmmap_write_lock(mm);\n\tprev = vma_prev(&vmi);\n\tfor_each_vma_range(vmi, vma, end) {\n\t\t/*\n\t\t * If any vma in the range got policy other than MPOL_BIND\n\t\t * or MPOL_PREFERRED_MANY we return error. We don't reset\n\t\t * the home node for vmas we already updated before.\n\t\t */\n\t\told = vma_policy(vma);\n\t\tif (!old)\n\t\t\tcontinue;\n\t\tif (old->mode != MPOL_BIND && old->mode != MPOL_PREFERRED_MANY) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tbreak;\n\t\t}\n\t\tnew = mpol_dup(old);\n\t\tif (IS_ERR(new)) {\n\t\t\terr = PTR_ERR(new);\n\t\t\tbreak;\n\t\t}\n\n\t\tvma_start_write(vma);\n\t\tnew->home_node = home_node;\n\t\terr = mbind_range(&vmi, vma, &prev, start, end, new);\n\t\tmpol_put(new);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\tmmap_write_unlock(mm);\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4417,
            "cve_id": "CVE-2023-4921",
            "code_snippet": "static struct sk_buff *qfq_dequeue(struct Qdisc *sch)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_aggregate *in_serv_agg = q->in_serv_agg;\n\tstruct qfq_class *cl;\n\tstruct sk_buff *skb = NULL;\n\t/* next-packet len, 0 means no more active classes in in-service agg */\n\tunsigned int len = 0;\n\n\tif (in_serv_agg == NULL)\n\t\treturn NULL;\n\n\tif (!list_empty(&in_serv_agg->active))\n\t\tskb = qfq_peek_skb(in_serv_agg, &cl, &len);\n\n\t/*\n\t * If there are no active classes in the in-service aggregate,\n\t * or if the aggregate has not enough budget to serve its next\n\t * class, then choose the next aggregate to serve.\n\t */\n\tif (len == 0 || in_serv_agg->budget < len) {\n\t\tcharge_actual_service(in_serv_agg);\n\n\t\t/* recharge the budget of the aggregate */\n\t\tin_serv_agg->initial_budget = in_serv_agg->budget =\n\t\t\tin_serv_agg->budgetmax;\n\n\t\tif (!list_empty(&in_serv_agg->active)) {\n\t\t\t/*\n\t\t\t * Still active: reschedule for\n\t\t\t * service. Possible optimization: if no other\n\t\t\t * aggregate is active, then there is no point\n\t\t\t * in rescheduling this aggregate, and we can\n\t\t\t * just keep it as the in-service one. This\n\t\t\t * should be however a corner case, and to\n\t\t\t * handle it, we would need to maintain an\n\t\t\t * extra num_active_aggs field.\n\t\t\t*/\n\t\t\tqfq_update_agg_ts(q, in_serv_agg, requeue);\n\t\t\tqfq_schedule_agg(q, in_serv_agg);\n\t\t} else if (sch->q.qlen == 0) { /* no aggregate to serve */\n\t\t\tq->in_serv_agg = NULL;\n\t\t\treturn NULL;\n\t\t}\n\n\t\t/*\n\t\t * If we get here, there are other aggregates queued:\n\t\t * choose the new aggregate to serve.\n\t\t */\n\t\tin_serv_agg = q->in_serv_agg = qfq_choose_next_agg(q);\n\t\tskb = qfq_peek_skb(in_serv_agg, &cl, &len);\n\t}\n\tif (!skb)\n\t\treturn NULL;\n\n\tsch->q.qlen--;\n\n\tskb = agg_dequeue(in_serv_agg, cl, len);\n\n\tif (!skb) {\n\t\tsch->q.qlen++;\n\t\treturn NULL;\n\t}\n\n\tqdisc_qstats_backlog_dec(sch, skb);\n\tqdisc_bstats_update(sch, skb);\n\n\t/* If lmax is lowered, through qfq_change_class, for a class\n\t * owning pending packets with larger size than the new value\n\t * of lmax, then the following condition may hold.\n\t */\n\tif (unlikely(in_serv_agg->budget < len))\n\t\tin_serv_agg->budget = 0;\n\telse\n\t\tin_serv_agg->budget -= len;\n\n\tq->V += (u64)len * q->iwsum;\n\tpr_debug(\"qfq dequeue: len %u F %lld now %lld\\n\",\n\t\t len, (unsigned long long) in_serv_agg->F,\n\t\t (unsigned long long) q->V);\n\n\treturn skb;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4425,
            "cve_id": "CVE-2023-5197",
            "code_snippet": "static int nft_flush_table(struct nft_ctx *ctx)\n{\n\tstruct nft_flowtable *flowtable, *nft;\n\tstruct nft_chain *chain, *nc;\n\tstruct nft_object *obj, *ne;\n\tstruct nft_set *set, *ns;\n\tint err;\n\n\tlist_for_each_entry(chain, &ctx->table->chains, list) {\n\t\tif (!nft_is_active_next(ctx->net, chain))\n\t\t\tcontinue;\n\n\t\tif (nft_chain_binding(chain))\n\t\t\tcontinue;\n\n\t\tctx->chain = chain;\n\n\t\terr = nft_delrule_by_chain(ctx);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tlist_for_each_entry_safe(set, ns, &ctx->table->sets, list) {\n\t\tif (!nft_is_active_next(ctx->net, set))\n\t\t\tcontinue;\n\n\t\tif (nft_set_is_anonymous(set) &&\n\t\t    !list_empty(&set->bindings))\n\t\t\tcontinue;\n\n\t\terr = nft_delset(ctx, set);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tlist_for_each_entry_safe(flowtable, nft, &ctx->table->flowtables, list) {\n\t\tif (!nft_is_active_next(ctx->net, flowtable))\n\t\t\tcontinue;\n\n\t\terr = nft_delflowtable(ctx, flowtable);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tlist_for_each_entry_safe(obj, ne, &ctx->table->objects, list) {\n\t\tif (!nft_is_active_next(ctx->net, obj))\n\t\t\tcontinue;\n\n\t\terr = nft_delobj(ctx, obj);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tlist_for_each_entry_safe(chain, nc, &ctx->table->chains, list) {\n\t\tif (!nft_is_active_next(ctx->net, chain))\n\t\t\tcontinue;\n\n\t\tif (nft_chain_binding(chain))\n\t\t\tcontinue;\n\n\t\tctx->chain = chain;\n\n\t\terr = nft_delchain(ctx);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\terr = nft_deltable(ctx);\nout:\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4426,
            "cve_id": "CVE-2023-5197",
            "code_snippet": "static int nf_tables_delrule(struct sk_buff *skb, const struct nfnl_info *info,\n\t\t\t     const struct nlattr * const nla[])\n{\n\tstruct netlink_ext_ack *extack = info->extack;\n\tu8 genmask = nft_genmask_next(info->net);\n\tu8 family = info->nfmsg->nfgen_family;\n\tstruct nft_chain *chain = NULL;\n\tstruct net *net = info->net;\n\tstruct nft_table *table;\n\tstruct nft_rule *rule;\n\tstruct nft_ctx ctx;\n\tint err = 0;\n\n\ttable = nft_table_lookup(net, nla[NFTA_RULE_TABLE], family, genmask,\n\t\t\t\t NETLINK_CB(skb).portid);\n\tif (IS_ERR(table)) {\n\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_TABLE]);\n\t\treturn PTR_ERR(table);\n\t}\n\n\tif (nla[NFTA_RULE_CHAIN]) {\n\t\tchain = nft_chain_lookup(net, table, nla[NFTA_RULE_CHAIN],\n\t\t\t\t\t genmask);\n\t\tif (IS_ERR(chain)) {\n\t\t\tif (PTR_ERR(chain) == -ENOENT &&\n\t\t\t    NFNL_MSG_TYPE(info->nlh->nlmsg_type) == NFT_MSG_DESTROYRULE)\n\t\t\t\treturn 0;\n\n\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_CHAIN]);\n\t\t\treturn PTR_ERR(chain);\n\t\t}\n\t\tif (nft_chain_binding(chain))\n\t\t\treturn -EOPNOTSUPP;\n\t}\n\n\tnft_ctx_init(&ctx, net, skb, info->nlh, family, table, chain, nla);\n\n\tif (chain) {\n\t\tif (nla[NFTA_RULE_HANDLE]) {\n\t\t\trule = nft_rule_lookup(chain, nla[NFTA_RULE_HANDLE]);\n\t\t\tif (IS_ERR(rule)) {\n\t\t\t\tif (PTR_ERR(rule) == -ENOENT &&\n\t\t\t\t    NFNL_MSG_TYPE(info->nlh->nlmsg_type) == NFT_MSG_DESTROYRULE)\n\t\t\t\t\treturn 0;\n\n\t\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_HANDLE]);\n\t\t\t\treturn PTR_ERR(rule);\n\t\t\t}\n\n\t\t\terr = nft_delrule(&ctx, rule);\n\t\t} else if (nla[NFTA_RULE_ID]) {\n\t\t\trule = nft_rule_lookup_byid(net, chain, nla[NFTA_RULE_ID]);\n\t\t\tif (IS_ERR(rule)) {\n\t\t\t\tNL_SET_BAD_ATTR(extack, nla[NFTA_RULE_ID]);\n\t\t\t\treturn PTR_ERR(rule);\n\t\t\t}\n\n\t\t\terr = nft_delrule(&ctx, rule);\n\t\t} else {\n\t\t\terr = nft_delrule_by_chain(&ctx);\n\t\t}\n\t} else {\n\t\tlist_for_each_entry(chain, &table->chains, list) {\n\t\t\tif (!nft_is_active_next(net, chain))\n\t\t\t\tcontinue;\n\t\t\tif (nft_chain_binding(chain))\n\t\t\t\tcontinue;\n\n\t\t\tctx.chain = chain;\n\t\t\terr = nft_delrule_by_chain(&ctx);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn err;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4431,
            "cve_id": "CVE-2023-5633",
            "code_snippet": "int vmw_user_bo_synccpu_ioctl(struct drm_device *dev, void *data,\n\t\t\t      struct drm_file *file_priv)\n{\n\tstruct drm_vmw_synccpu_arg *arg =\n\t\t(struct drm_vmw_synccpu_arg *) data;\n\tstruct vmw_bo *vbo;\n\tint ret;\n\n\tif ((arg->flags & (drm_vmw_synccpu_read | drm_vmw_synccpu_write)) == 0\n\t    || (arg->flags & ~(drm_vmw_synccpu_read | drm_vmw_synccpu_write |\n\t\t\t       drm_vmw_synccpu_dontblock |\n\t\t\t       drm_vmw_synccpu_allow_cs)) != 0) {\n\t\tDRM_ERROR(\"Illegal synccpu flags.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (arg->op) {\n\tcase drm_vmw_synccpu_grab:\n\t\tret = vmw_user_bo_lookup(file_priv, arg->handle, &vbo);\n\t\tif (unlikely(ret != 0))\n\t\t\treturn ret;\n\n\t\tret = vmw_user_bo_synccpu_grab(vbo, arg->flags);\n\t\tvmw_user_bo_unref(&vbo);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tif (ret == -ERESTARTSYS || ret == -EBUSY)\n\t\t\t\treturn -EBUSY;\n\t\t\tDRM_ERROR(\"Failed synccpu grab on handle 0x%08x.\\n\",\n\t\t\t\t  (unsigned int) arg->handle);\n\t\t\treturn ret;\n\t\t}\n\t\tbreak;\n\tcase drm_vmw_synccpu_release:\n\t\tret = vmw_user_bo_synccpu_release(file_priv,\n\t\t\t\t\t\t  arg->handle,\n\t\t\t\t\t\t  arg->flags);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Failed synccpu release on handle 0x%08x.\\n\",\n\t\t\t\t  (unsigned int) arg->handle);\n\t\t\treturn ret;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Invalid synccpu operation.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4432,
            "cve_id": "CVE-2023-5633",
            "code_snippet": "int vmw_user_bo_lookup(struct drm_file *filp,\n\t\t       u32 handle,\n\t\t       struct vmw_bo **out)\n{\n\tstruct drm_gem_object *gobj;\n\n\tgobj = drm_gem_object_lookup(filp, handle);\n\tif (!gobj) {\n\t\tDRM_ERROR(\"Invalid buffer object handle 0x%08lx.\\n\",\n\t\t\t  (unsigned long)handle);\n\t\treturn -ESRCH;\n\t}\n\n\t*out = to_vmw_bo(gobj);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4433,
            "cve_id": "CVE-2023-5633",
            "code_snippet": "static int vmw_user_bo_synccpu_release(struct drm_file *filp,\n\t\t\t\t       uint32_t handle,\n\t\t\t\t       uint32_t flags)\n{\n\tstruct vmw_bo *vmw_bo;\n\tint ret = vmw_user_bo_lookup(filp, handle, &vmw_bo);\n\n\tif (!ret) {\n\t\tif (!(flags & drm_vmw_synccpu_allow_cs)) {\n\t\t\tatomic_dec(&vmw_bo->cpu_writers);\n\t\t}\n\t\tvmw_user_bo_unref(&vmw_bo);\n\t}\n\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4434,
            "cve_id": "CVE-2023-5633",
            "code_snippet": "static int vmw_cotable_resize(struct vmw_resource *res, size_t new_size)\n{\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct vmw_private *dev_priv = res->dev_priv;\n\tstruct vmw_cotable *vcotbl = vmw_cotable(res);\n\tstruct vmw_bo *buf, *old_buf = res->guest_memory_bo;\n\tstruct ttm_buffer_object *bo, *old_bo = &res->guest_memory_bo->tbo;\n\tsize_t old_size = res->guest_memory_size;\n\tsize_t old_size_read_back = vcotbl->size_read_back;\n\tsize_t cur_size_read_back;\n\tstruct ttm_bo_kmap_obj old_map, new_map;\n\tint ret;\n\tsize_t i;\n\tstruct vmw_bo_params bo_params = {\n\t\t.domain = VMW_BO_DOMAIN_MOB,\n\t\t.busy_domain = VMW_BO_DOMAIN_MOB,\n\t\t.bo_type = ttm_bo_type_device,\n\t\t.size = new_size,\n\t\t.pin = true\n\t};\n\n\tMKS_STAT_TIME_DECL(MKSSTAT_KERN_COTABLE_RESIZE);\n\tMKS_STAT_TIME_PUSH(MKSSTAT_KERN_COTABLE_RESIZE);\n\n\tret = vmw_cotable_readback(res);\n\tif (ret)\n\t\tgoto out_done;\n\n\tcur_size_read_back = vcotbl->size_read_back;\n\tvcotbl->size_read_back = old_size_read_back;\n\n\t/*\n\t * While device is processing, Allocate and reserve a buffer object\n\t * for the new COTable. Initially pin the buffer object to make sure\n\t * we can use tryreserve without failure.\n\t */\n\tret = vmw_gem_object_create(dev_priv, &bo_params, &buf);\n\tif (ret) {\n\t\tDRM_ERROR(\"Failed initializing new cotable MOB.\\n\");\n\t\tgoto out_done;\n\t}\n\n\tbo = &buf->tbo;\n\tWARN_ON_ONCE(ttm_bo_reserve(bo, false, true, NULL));\n\n\tret = ttm_bo_wait(old_bo, false, false);\n\tif (unlikely(ret != 0)) {\n\t\tDRM_ERROR(\"Failed waiting for cotable unbind.\\n\");\n\t\tgoto out_wait;\n\t}\n\n\t/*\n\t * Do a page by page copy of COTables. This eliminates slow vmap()s.\n\t * This should really be a TTM utility.\n\t */\n\tfor (i = 0; i < PFN_UP(old_bo->resource->size); ++i) {\n\t\tbool dummy;\n\n\t\tret = ttm_bo_kmap(old_bo, i, 1, &old_map);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Failed mapping old COTable on resize.\\n\");\n\t\t\tgoto out_wait;\n\t\t}\n\t\tret = ttm_bo_kmap(bo, i, 1, &new_map);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Failed mapping new COTable on resize.\\n\");\n\t\t\tgoto out_map_new;\n\t\t}\n\t\tmemcpy(ttm_kmap_obj_virtual(&new_map, &dummy),\n\t\t       ttm_kmap_obj_virtual(&old_map, &dummy),\n\t\t       PAGE_SIZE);\n\t\tttm_bo_kunmap(&new_map);\n\t\tttm_bo_kunmap(&old_map);\n\t}\n\n\t/* Unpin new buffer, and switch backup buffers. */\n\tvmw_bo_placement_set(buf,\n\t\t\t     VMW_BO_DOMAIN_MOB,\n\t\t\t     VMW_BO_DOMAIN_MOB);\n\tret = ttm_bo_validate(bo, &buf->placement, &ctx);\n\tif (unlikely(ret != 0)) {\n\t\tDRM_ERROR(\"Failed validating new COTable backup buffer.\\n\");\n\t\tgoto out_wait;\n\t}\n\n\tvmw_resource_mob_detach(res);\n\tres->guest_memory_bo = buf;\n\tres->guest_memory_size = new_size;\n\tvcotbl->size_read_back = cur_size_read_back;\n\n\t/*\n\t * Now tell the device to switch. If this fails, then we need to\n\t * revert the full resize.\n\t */\n\tret = vmw_cotable_unscrub(res);\n\tif (ret) {\n\t\tDRM_ERROR(\"Failed switching COTable backup buffer.\\n\");\n\t\tres->guest_memory_bo = old_buf;\n\t\tres->guest_memory_size = old_size;\n\t\tvcotbl->size_read_back = old_size_read_back;\n\t\tvmw_resource_mob_attach(res);\n\t\tgoto out_wait;\n\t}\n\n\tvmw_resource_mob_attach(res);\n\t/* Let go of the old mob. */\n\tvmw_user_bo_unref(&old_buf);\n\tres->id = vcotbl->type;\n\n\tret = dma_resv_reserve_fences(bo->base.resv, 1);\n\tif (unlikely(ret))\n\t\tgoto out_wait;\n\n\t/* Release the pin acquired in vmw_bo_create */\n\tttm_bo_unpin(bo);\n\n\tMKS_STAT_TIME_POP(MKSSTAT_KERN_COTABLE_RESIZE);\n\n\treturn 0;\n\nout_map_new:\n\tttm_bo_kunmap(&old_map);\nout_wait:\n\tttm_bo_unpin(bo);\n\tttm_bo_unreserve(bo);\n\tvmw_user_bo_unref(&buf);\n\nout_done:\n\tMKS_STAT_TIME_POP(MKSSTAT_KERN_COTABLE_RESIZE);\n\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4435,
            "cve_id": "CVE-2023-5633",
            "code_snippet": "static int vmw_translate_guest_ptr(struct vmw_private *dev_priv,\n\t\t\t\t   struct vmw_sw_context *sw_context,\n\t\t\t\t   SVGAGuestPtr *ptr,\n\t\t\t\t   struct vmw_bo **vmw_bo_p)\n{\n\tstruct vmw_bo *vmw_bo, *tmp_bo;\n\tuint32_t handle = ptr->gmrId;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use GMR region.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM,\n\t\t\t     VMW_BO_DOMAIN_GMR | VMW_BO_DOMAIN_VRAM);\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n\ttmp_bo = vmw_bo;\n\tvmw_user_bo_unref(&tmp_bo);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->location = ptr;\n\treloc->vbo = vmw_bo;\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4436,
            "cve_id": "CVE-2023-5633",
            "code_snippet": "static int vmw_translate_mob_ptr(struct vmw_private *dev_priv,\n\t\t\t\t struct vmw_sw_context *sw_context,\n\t\t\t\t SVGAMobId *id,\n\t\t\t\t struct vmw_bo **vmw_bo_p)\n{\n\tstruct vmw_bo *vmw_bo, *tmp_bo;\n\tuint32_t handle = *id;\n\tstruct vmw_relocation *reloc;\n\tint ret;\n\n\tvmw_validation_preload_bo(sw_context->ctx);\n\tret = vmw_user_bo_lookup(sw_context->filp, handle, &vmw_bo);\n\tif (ret != 0) {\n\t\tdrm_dbg(&dev_priv->drm, \"Could not find or use MOB buffer.\\n\");\n\t\treturn PTR_ERR(vmw_bo);\n\t}\n\tvmw_bo_placement_set(vmw_bo, VMW_BO_DOMAIN_MOB, VMW_BO_DOMAIN_MOB);\n\tret = vmw_validation_add_bo(sw_context->ctx, vmw_bo);\n\ttmp_bo = vmw_bo;\n\tvmw_user_bo_unref(&tmp_bo);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\treloc = vmw_validation_mem_alloc(sw_context->ctx, sizeof(*reloc));\n\tif (!reloc)\n\t\treturn -ENOMEM;\n\n\treloc->mob_loc = id;\n\treloc->vbo = vmw_bo;\n\n\t*vmw_bo_p = vmw_bo;\n\tlist_add_tail(&reloc->head, &sw_context->bo_relocations);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4437,
            "cve_id": "CVE-2023-5633",
            "code_snippet": "int vmw_gem_object_create_with_handle(struct vmw_private *dev_priv,\n\t\t\t\t      struct drm_file *filp,\n\t\t\t\t      uint32_t size,\n\t\t\t\t      uint32_t *handle,\n\t\t\t\t      struct vmw_bo **p_vbo)\n{\n\tint ret;\n\tstruct vmw_bo_params params = {\n\t\t.domain = (dev_priv->has_mob) ? VMW_BO_DOMAIN_SYS : VMW_BO_DOMAIN_VRAM,\n\t\t.busy_domain = VMW_BO_DOMAIN_SYS,\n\t\t.bo_type = ttm_bo_type_device,\n\t\t.size = size,\n\t\t.pin = false\n\t};\n\n\tret = vmw_gem_object_create(dev_priv, &params, p_vbo);\n\tif (ret != 0)\n\t\tgoto out_no_bo;\n\n\tret = drm_gem_handle_create(filp, &(*p_vbo)->tbo.base, handle);\nout_no_bo:\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4438,
            "cve_id": "CVE-2023-5633",
            "code_snippet": "static int vmw_create_bo_proxy(struct drm_device *dev,\n\t\t\t       const struct drm_mode_fb_cmd2 *mode_cmd,\n\t\t\t       struct vmw_bo *bo_mob,\n\t\t\t       struct vmw_surface **srf_out)\n{\n\tstruct vmw_surface_metadata metadata = {0};\n\tuint32_t format;\n\tstruct vmw_resource *res;\n\tunsigned int bytes_pp;\n\tint ret;\n\n\tswitch (mode_cmd->pixel_format) {\n\tcase DRM_FORMAT_ARGB8888:\n\tcase DRM_FORMAT_XRGB8888:\n\t\tformat = SVGA3D_X8R8G8B8;\n\t\tbytes_pp = 4;\n\t\tbreak;\n\n\tcase DRM_FORMAT_RGB565:\n\tcase DRM_FORMAT_XRGB1555:\n\t\tformat = SVGA3D_R5G6B5;\n\t\tbytes_pp = 2;\n\t\tbreak;\n\n\tcase 8:\n\t\tformat = SVGA3D_P8;\n\t\tbytes_pp = 1;\n\t\tbreak;\n\n\tdefault:\n\t\tDRM_ERROR(\"Invalid framebuffer format %p4cc\\n\",\n\t\t\t  &mode_cmd->pixel_format);\n\t\treturn -EINVAL;\n\t}\n\n\tmetadata.format = format;\n\tmetadata.mip_levels[0] = 1;\n\tmetadata.num_sizes = 1;\n\tmetadata.base_size.width = mode_cmd->pitches[0] / bytes_pp;\n\tmetadata.base_size.height =  mode_cmd->height;\n\tmetadata.base_size.depth = 1;\n\tmetadata.scanout = true;\n\n\tret = vmw_gb_surface_define(vmw_priv(dev), &metadata, srf_out);\n\tif (ret) {\n\t\tDRM_ERROR(\"Failed to allocate proxy content buffer\\n\");\n\t\treturn ret;\n\t}\n\n\tres = &(*srf_out)->res;\n\n\t/* Reserve and switch the backing mob. */\n\tmutex_lock(&res->dev_priv->cmdbuf_mutex);\n\t(void) vmw_resource_reserve(res, false, true);\n\tvmw_user_bo_unref(&res->guest_memory_bo);\n\tres->guest_memory_bo = vmw_user_bo_ref(bo_mob);\n\tres->guest_memory_offset = 0;\n\tvmw_resource_unreserve(res, false, false, false, NULL, 0);\n\tmutex_unlock(&res->dev_priv->cmdbuf_mutex);\n\n\treturn 0;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4439,
            "cve_id": "CVE-2023-5633",
            "code_snippet": "static struct drm_framebuffer *vmw_kms_fb_create(struct drm_device *dev,\n\t\t\t\t\t\t struct drm_file *file_priv,\n\t\t\t\t\t\t const struct drm_mode_fb_cmd2 *mode_cmd)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct vmw_framebuffer *vfb = NULL;\n\tstruct vmw_surface *surface = NULL;\n\tstruct vmw_bo *bo = NULL;\n\tint ret;\n\n\t/* returns either a bo or surface */\n\tret = vmw_user_lookup_handle(dev_priv, file_priv,\n\t\t\t\t     mode_cmd->handles[0],\n\t\t\t\t     &surface, &bo);\n\tif (ret) {\n\t\tDRM_ERROR(\"Invalid buffer object handle %u (0x%x).\\n\",\n\t\t\t  mode_cmd->handles[0], mode_cmd->handles[0]);\n\t\tgoto err_out;\n\t}\n\n\n\tif (!bo &&\n\t    !vmw_kms_srf_ok(dev_priv, mode_cmd->width, mode_cmd->height)) {\n\t\tDRM_ERROR(\"Surface size cannot exceed %dx%d\\n\",\n\t\t\tdev_priv->texture_max_width,\n\t\t\tdev_priv->texture_max_height);\n\t\tgoto err_out;\n\t}\n\n\n\tvfb = vmw_kms_new_framebuffer(dev_priv, bo, surface,\n\t\t\t\t      !(dev_priv->capabilities & SVGA_CAP_3D),\n\t\t\t\t      mode_cmd);\n\tif (IS_ERR(vfb)) {\n\t\tret = PTR_ERR(vfb);\n\t\tgoto err_out;\n\t}\n\nerr_out:\n\t/* vmw_user_lookup_handle takes one ref so does new_fb */\n\tif (bo)\n\t\tvmw_user_bo_unref(&bo);\n\tif (surface)\n\t\tvmw_surface_unreference(&surface);\n\n\tif (ret) {\n\t\tDRM_ERROR(\"failed to create vmw_framebuffer: %i\\n\", ret);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\treturn &vfb->base;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4440,
            "cve_id": "CVE-2023-5633",
            "code_snippet": "int vmw_overlay_ioctl(struct drm_device *dev, void *data,\n\t\t      struct drm_file *file_priv)\n{\n\tstruct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct vmw_overlay *overlay = dev_priv->overlay_priv;\n\tstruct drm_vmw_control_stream_arg *arg =\n\t    (struct drm_vmw_control_stream_arg *)data;\n\tstruct vmw_bo *buf;\n\tstruct vmw_resource *res;\n\tint ret;\n\n\tif (!vmw_overlay_available(dev_priv))\n\t\treturn -ENOSYS;\n\n\tret = vmw_user_stream_lookup(dev_priv, tfile, &arg->stream_id, &res);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&overlay->mutex);\n\n\tif (!arg->enabled) {\n\t\tret = vmw_overlay_stop(dev_priv, arg->stream_id, false, true);\n\t\tgoto out_unlock;\n\t}\n\n\tret = vmw_user_bo_lookup(file_priv, arg->handle, &buf);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tret = vmw_overlay_update_stream(dev_priv, buf, arg, true);\n\n\tvmw_user_bo_unref(&buf);\n\nout_unlock:\n\tmutex_unlock(&overlay->mutex);\n\tvmw_resource_unreference(&res);\n\n\treturn ret;\n}",
            "detect_result": "NO",
            "final_result": 0
        },
        {
            "id": 4460,
            "cve_id": "CVE-2023-6111",
            "code_snippet": "static void nft_setelem_catchall_remove(const struct net *net,\n\t\t\t\t\tconst struct nft_set *set,\n\t\t\t\t\tstruct nft_elem_priv *elem_priv)\n{\n\tstruct nft_set_elem_catchall *catchall, *next;\n\n\tlist_for_each_entry_safe(catchall, next, &set->catchall_list, list) {\n\t\tif (catchall->elem == elem_priv) {\n\t\t\tnft_setelem_catchall_destroy(catchall);\n\t\t\tbreak;\n\t\t}\n\t}\n}",
            "detect_result": "NO",
            "final_result": 0
        }
    ]
}