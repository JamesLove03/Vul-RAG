{
    "vul_data": [
        {
            "id": 661,
            "cve_id": "CVE-2014-7826",
            "code_snippet": "static void perf_syscall_exit(void *ignore, struct pt_regs *regs, long ret)\n{\n\tstruct syscall_metadata *sys_data;\n\tstruct syscall_trace_exit *rec;\n\tstruct hlist_head *head;\n\tint syscall_nr;\n\tint rctx;\n\tint size;\n\n\tsyscall_nr = trace_get_syscall_nr(current, regs);\n\tif (syscall_nr < 0)\n\t\treturn;\n\tif (!test_bit(syscall_nr, enabled_perf_exit_syscalls))\n\t\treturn;\n\n\tsys_data = syscall_nr_to_meta(syscall_nr);\n\tif (!sys_data)\n\t\treturn;\n\n\thead = this_cpu_ptr(sys_data->exit_event->perf_events);\n\tif (hlist_empty(head))\n\t\treturn;\n\n\t/* We can probably do that at build time */\n\tsize = ALIGN(sizeof(*rec) + sizeof(u32), sizeof(u64));\n\tsize -= sizeof(u32);\n\n\trec = (struct syscall_trace_exit *)perf_trace_buf_prepare(size,\n\t\t\t\tsys_data->exit_event->event.type, regs, &rctx);\n\tif (!rec)\n\t\treturn;\n\n\trec->nr = syscall_nr;\n\trec->ret = syscall_get_return_value(current, regs);\n\tperf_trace_buf_submit(rec, size, rctx, 0, 1, regs, head, NULL);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static void perf_syscall_enter(void *ignore, struct pt_regs *regs, long id)\n{\n\tstruct syscall_metadata *sys_data;\n\tstruct syscall_trace_enter *rec;\n\tstruct hlist_head *head;\n\tint syscall_nr;\n\tint rctx;\n\tint size;\n\n\tsyscall_nr = trace_get_syscall_nr(current, regs);\n\tif (syscall_nr < 0)\n\t\treturn;\n\tif (!test_bit(syscall_nr, enabled_perf_enter_syscalls))\n\t\treturn;\n\n\tsys_data = syscall_nr_to_meta(syscall_nr);\n\tif (!sys_data)\n\t\treturn;\n\n\thead = this_cpu_ptr(sys_data->enter_event->perf_events);\n\tif (hlist_empty(head))\n\t\treturn;\n\n\t/* get the size after alignment with the u32 buffer size field */\n\tsize = sizeof(unsigned long) * sys_data->nb_args + sizeof(*rec);\n\tsize = ALIGN(size + sizeof(u32), sizeof(u64));\n\tsize -= sizeof(u32);\n\n\trec = (struct syscall_trace_enter *)perf_trace_buf_prepare(size,\n\t\t\t\tsys_data->enter_event->event.type, regs, &rctx);\n\tif (!rec)\n\t\treturn;\n\n\trec->nr = syscall_nr;\n\tsyscall_get_arguments(current, regs, 0, sys_data->nb_args,\n\t\t\t       (unsigned long *)&rec->args);\n\tperf_trace_buf_submit(rec, size, rctx, 0, 1, regs, head, NULL);\n}",
                        "code_after_change": "static void perf_syscall_enter(void *ignore, struct pt_regs *regs, long id)\n{\n\tstruct syscall_metadata *sys_data;\n\tstruct syscall_trace_enter *rec;\n\tstruct hlist_head *head;\n\tint syscall_nr;\n\tint rctx;\n\tint size;\n\n\tsyscall_nr = trace_get_syscall_nr(current, regs);\n\tif (syscall_nr < 0 || syscall_nr >= NR_syscalls)\n\t\treturn;\n\tif (!test_bit(syscall_nr, enabled_perf_enter_syscalls))\n\t\treturn;\n\n\tsys_data = syscall_nr_to_meta(syscall_nr);\n\tif (!sys_data)\n\t\treturn;\n\n\thead = this_cpu_ptr(sys_data->enter_event->perf_events);\n\tif (hlist_empty(head))\n\t\treturn;\n\n\t/* get the size after alignment with the u32 buffer size field */\n\tsize = sizeof(unsigned long) * sys_data->nb_args + sizeof(*rec);\n\tsize = ALIGN(size + sizeof(u32), sizeof(u64));\n\tsize -= sizeof(u32);\n\n\trec = (struct syscall_trace_enter *)perf_trace_buf_prepare(size,\n\t\t\t\tsys_data->enter_event->event.type, regs, &rctx);\n\tif (!rec)\n\t\treturn;\n\n\trec->nr = syscall_nr;\n\tsyscall_get_arguments(current, regs, 0, sys_data->nb_args,\n\t\t\t       (unsigned long *)&rec->args);\n\tperf_trace_buf_submit(rec, size, rctx, 0, 1, regs, head, NULL);\n}",
                        "cve_id": "CVE-2014-7826"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 944,
            "cve_id": "CVE-2015-8970",
            "code_snippet": "static void skcipher_release(void *private)\n{\n\tcrypto_free_skcipher(private);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int skcipher_setkey(void *private, const u8 *key, unsigned int keylen)\n{\n\treturn crypto_skcipher_setkey(private, key, keylen);\n}",
                        "code_after_change": "static int skcipher_setkey(void *private, const u8 *key, unsigned int keylen)\n{\n\tstruct skcipher_tfm *tfm = private;\n\tint err;\n\n\terr = crypto_skcipher_setkey(tfm->skcipher, key, keylen);\n\ttfm->has_key = !err;\n\n\treturn err;\n}",
                        "cve_id": "CVE-2015-8970"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int skcipher_accept_parent(void *private, struct sock *sk)\n{\n\tstruct skcipher_ctx *ctx;\n\tstruct alg_sock *ask = alg_sk(sk);\n\tunsigned int len = sizeof(*ctx) + crypto_skcipher_reqsize(private);\n\n\tctx = sock_kmalloc(sk, len, GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\tctx->iv = sock_kmalloc(sk, crypto_skcipher_ivsize(private),\n\t\t\t       GFP_KERNEL);\n\tif (!ctx->iv) {\n\t\tsock_kfree_s(sk, ctx, len);\n\t\treturn -ENOMEM;\n\t}\n\n\tmemset(ctx->iv, 0, crypto_skcipher_ivsize(private));\n\n\tINIT_LIST_HEAD(&ctx->tsgl);\n\tctx->len = len;\n\tctx->used = 0;\n\tctx->more = 0;\n\tctx->merge = 0;\n\tctx->enc = 0;\n\tatomic_set(&ctx->inflight, 0);\n\taf_alg_init_completion(&ctx->completion);\n\n\task->private = ctx;\n\n\tskcipher_request_set_tfm(&ctx->req, private);\n\tskcipher_request_set_callback(&ctx->req, CRYPTO_TFM_REQ_MAY_BACKLOG,\n\t\t\t\t      af_alg_complete, &ctx->completion);\n\n\tsk->sk_destruct = skcipher_sock_destruct;\n\n\treturn 0;\n}",
                        "code_after_change": "static int skcipher_accept_parent(void *private, struct sock *sk)\n{\n\tstruct skcipher_ctx *ctx;\n\tstruct alg_sock *ask = alg_sk(sk);\n\tstruct skcipher_tfm *tfm = private;\n\tstruct crypto_skcipher *skcipher = tfm->skcipher;\n\tunsigned int len = sizeof(*ctx) + crypto_skcipher_reqsize(skcipher);\n\n\tif (!tfm->has_key)\n\t\treturn -ENOKEY;\n\n\tctx = sock_kmalloc(sk, len, GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\tctx->iv = sock_kmalloc(sk, crypto_skcipher_ivsize(skcipher),\n\t\t\t       GFP_KERNEL);\n\tif (!ctx->iv) {\n\t\tsock_kfree_s(sk, ctx, len);\n\t\treturn -ENOMEM;\n\t}\n\n\tmemset(ctx->iv, 0, crypto_skcipher_ivsize(skcipher));\n\n\tINIT_LIST_HEAD(&ctx->tsgl);\n\tctx->len = len;\n\tctx->used = 0;\n\tctx->more = 0;\n\tctx->merge = 0;\n\tctx->enc = 0;\n\tatomic_set(&ctx->inflight, 0);\n\taf_alg_init_completion(&ctx->completion);\n\n\task->private = ctx;\n\n\tskcipher_request_set_tfm(&ctx->req, skcipher);\n\tskcipher_request_set_callback(&ctx->req, CRYPTO_TFM_REQ_MAY_BACKLOG,\n\t\t\t\t      af_alg_complete, &ctx->completion);\n\n\tsk->sk_destruct = skcipher_sock_destruct;\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2015-8970"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 963,
            "cve_id": "CVE-2016-10147",
            "code_snippet": "static inline void mcryptd_check_internal(struct rtattr **tb, u32 *type,\n\t\t\t\t\t  u32 *mask)\n{\n\tstruct crypto_attr_type *algt;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn;\n\tif ((algt->type & CRYPTO_ALG_INTERNAL))\n\t\t*type |= CRYPTO_ALG_INTERNAL;\n\tif ((algt->mask & CRYPTO_ALG_INTERNAL))\n\t\t*mask |= CRYPTO_ALG_INTERNAL;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int mcryptd_create_hash(struct crypto_template *tmpl, struct rtattr **tb,\n\t\t\t      struct mcryptd_queue *queue)\n{\n\tstruct hashd_instance_ctx *ctx;\n\tstruct ahash_instance *inst;\n\tstruct hash_alg_common *halg;\n\tstruct crypto_alg *alg;\n\tu32 type = 0;\n\tu32 mask = 0;\n\tint err;\n\n\tmcryptd_check_internal(tb, &type, &mask);\n\n\thalg = ahash_attr_alg(tb[1], type, mask);\n\tif (IS_ERR(halg))\n\t\treturn PTR_ERR(halg);\n\n\talg = &halg->base;\n\tpr_debug(\"crypto: mcryptd hash alg: %s\\n\", alg->cra_name);\n\tinst = mcryptd_alloc_instance(alg, ahash_instance_headroom(),\n\t\t\t\t\tsizeof(*ctx));\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tctx = ahash_instance_ctx(inst);\n\tctx->queue = queue;\n\n\terr = crypto_init_ahash_spawn(&ctx->spawn, halg,\n\t\t\t\t      ahash_crypto_instance(inst));\n\tif (err)\n\t\tgoto out_free_inst;\n\n\ttype = CRYPTO_ALG_ASYNC;\n\tif (alg->cra_flags & CRYPTO_ALG_INTERNAL)\n\t\ttype |= CRYPTO_ALG_INTERNAL;\n\tinst->alg.halg.base.cra_flags = type;\n\n\tinst->alg.halg.digestsize = halg->digestsize;\n\tinst->alg.halg.statesize = halg->statesize;\n\tinst->alg.halg.base.cra_ctxsize = sizeof(struct mcryptd_hash_ctx);\n\n\tinst->alg.halg.base.cra_init = mcryptd_hash_init_tfm;\n\tinst->alg.halg.base.cra_exit = mcryptd_hash_exit_tfm;\n\n\tinst->alg.init   = mcryptd_hash_init_enqueue;\n\tinst->alg.update = mcryptd_hash_update_enqueue;\n\tinst->alg.final  = mcryptd_hash_final_enqueue;\n\tinst->alg.finup  = mcryptd_hash_finup_enqueue;\n\tinst->alg.export = mcryptd_hash_export;\n\tinst->alg.import = mcryptd_hash_import;\n\tinst->alg.setkey = mcryptd_hash_setkey;\n\tinst->alg.digest = mcryptd_hash_digest_enqueue;\n\n\terr = ahash_register_instance(tmpl, inst);\n\tif (err) {\n\t\tcrypto_drop_ahash(&ctx->spawn);\nout_free_inst:\n\t\tkfree(inst);\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}",
                        "code_after_change": "static int mcryptd_create_hash(struct crypto_template *tmpl, struct rtattr **tb,\n\t\t\t      struct mcryptd_queue *queue)\n{\n\tstruct hashd_instance_ctx *ctx;\n\tstruct ahash_instance *inst;\n\tstruct hash_alg_common *halg;\n\tstruct crypto_alg *alg;\n\tu32 type = 0;\n\tu32 mask = 0;\n\tint err;\n\n\tif (!mcryptd_check_internal(tb, &type, &mask))\n\t\treturn -EINVAL;\n\n\thalg = ahash_attr_alg(tb[1], type, mask);\n\tif (IS_ERR(halg))\n\t\treturn PTR_ERR(halg);\n\n\talg = &halg->base;\n\tpr_debug(\"crypto: mcryptd hash alg: %s\\n\", alg->cra_name);\n\tinst = mcryptd_alloc_instance(alg, ahash_instance_headroom(),\n\t\t\t\t\tsizeof(*ctx));\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tctx = ahash_instance_ctx(inst);\n\tctx->queue = queue;\n\n\terr = crypto_init_ahash_spawn(&ctx->spawn, halg,\n\t\t\t\t      ahash_crypto_instance(inst));\n\tif (err)\n\t\tgoto out_free_inst;\n\n\ttype = CRYPTO_ALG_ASYNC;\n\tif (alg->cra_flags & CRYPTO_ALG_INTERNAL)\n\t\ttype |= CRYPTO_ALG_INTERNAL;\n\tinst->alg.halg.base.cra_flags = type;\n\n\tinst->alg.halg.digestsize = halg->digestsize;\n\tinst->alg.halg.statesize = halg->statesize;\n\tinst->alg.halg.base.cra_ctxsize = sizeof(struct mcryptd_hash_ctx);\n\n\tinst->alg.halg.base.cra_init = mcryptd_hash_init_tfm;\n\tinst->alg.halg.base.cra_exit = mcryptd_hash_exit_tfm;\n\n\tinst->alg.init   = mcryptd_hash_init_enqueue;\n\tinst->alg.update = mcryptd_hash_update_enqueue;\n\tinst->alg.final  = mcryptd_hash_final_enqueue;\n\tinst->alg.finup  = mcryptd_hash_finup_enqueue;\n\tinst->alg.export = mcryptd_hash_export;\n\tinst->alg.import = mcryptd_hash_import;\n\tinst->alg.setkey = mcryptd_hash_setkey;\n\tinst->alg.digest = mcryptd_hash_digest_enqueue;\n\n\terr = ahash_register_instance(tmpl, inst);\n\tif (err) {\n\t\tcrypto_drop_ahash(&ctx->spawn);\nout_free_inst:\n\t\tkfree(inst);\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}",
                        "cve_id": "CVE-2016-10147"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1028,
            "cve_id": "CVE-2016-3070",
            "code_snippet": "void migrate_page_copy(struct page *newpage, struct page *page)\n{\n\tint cpupid;\n\n\tif (PageHuge(page) || PageTransHuge(page))\n\t\tcopy_huge_page(newpage, page);\n\telse\n\t\tcopy_highpage(newpage, page);\n\n\tif (PageError(page))\n\t\tSetPageError(newpage);\n\tif (PageReferenced(page))\n\t\tSetPageReferenced(newpage);\n\tif (PageUptodate(page))\n\t\tSetPageUptodate(newpage);\n\tif (TestClearPageActive(page)) {\n\t\tVM_BUG_ON_PAGE(PageUnevictable(page), page);\n\t\tSetPageActive(newpage);\n\t} else if (TestClearPageUnevictable(page))\n\t\tSetPageUnevictable(newpage);\n\tif (PageChecked(page))\n\t\tSetPageChecked(newpage);\n\tif (PageMappedToDisk(page))\n\t\tSetPageMappedToDisk(newpage);\n\n\tif (PageDirty(page)) {\n\t\tclear_page_dirty_for_io(page);\n\t\t/*\n\t\t * Want to mark the page and the radix tree as dirty, and\n\t\t * redo the accounting that clear_page_dirty_for_io undid,\n\t\t * but we can't use set_page_dirty because that function\n\t\t * is actually a signal that all of the page has become dirty.\n\t\t * Whereas only part of our page may be dirty.\n\t\t */\n\t\tif (PageSwapBacked(page))\n\t\t\tSetPageDirty(newpage);\n\t\telse\n\t\t\t__set_page_dirty_nobuffers(newpage);\n \t}\n\n\tif (page_is_young(page))\n\t\tset_page_young(newpage);\n\tif (page_is_idle(page))\n\t\tset_page_idle(newpage);\n\n\t/*\n\t * Copy NUMA information to the new page, to prevent over-eager\n\t * future migrations of this same page.\n\t */\n\tcpupid = page_cpupid_xchg_last(page, -1);\n\tpage_cpupid_xchg_last(newpage, cpupid);\n\n\tksm_migrate_page(newpage, page);\n\t/*\n\t * Please do not reorder this without considering how mm/ksm.c's\n\t * get_ksm_page() depends upon ksm_migrate_page() and PageSwapCache().\n\t */\n\tif (PageSwapCache(page))\n\t\tClearPageSwapCache(page);\n\tClearPagePrivate(page);\n\tset_page_private(page, 0);\n\n\t/*\n\t * If any waiters have accumulated on the new page then\n\t * wake them up.\n\t */\n\tif (PageWriteback(newpage))\n\t\tend_page_writeback(newpage);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int migrate_page_move_mapping(struct address_space *mapping,\n\t\tstruct page *newpage, struct page *page,\n\t\tstruct buffer_head *head, enum migrate_mode mode,\n\t\tint extra_count)\n{\n\tint expected_count = 1 + extra_count;\n\tvoid **pslot;\n\n\tif (!mapping) {\n\t\t/* Anonymous page without mapping */\n\t\tif (page_count(page) != expected_count)\n\t\t\treturn -EAGAIN;\n\n\t\t/* No turning back from here */\n\t\tset_page_memcg(newpage, page_memcg(page));\n\t\tnewpage->index = page->index;\n\t\tnewpage->mapping = page->mapping;\n\t\tif (PageSwapBacked(page))\n\t\t\tSetPageSwapBacked(newpage);\n\n\t\treturn MIGRATEPAGE_SUCCESS;\n\t}\n\n\tspin_lock_irq(&mapping->tree_lock);\n\n\tpslot = radix_tree_lookup_slot(&mapping->page_tree,\n \t\t\t\t\tpage_index(page));\n\n\texpected_count += 1 + page_has_private(page);\n\tif (page_count(page) != expected_count ||\n\t\tradix_tree_deref_slot_protected(pslot, &mapping->tree_lock) != page) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\tif (!page_freeze_refs(page, expected_count)) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * In the async migration case of moving a page with buffers, lock the\n\t * buffers using trylock before the mapping is moved. If the mapping\n\t * was moved, we later failed to lock the buffers and could not move\n\t * the mapping back due to an elevated page count, we would have to\n\t * block waiting on other references to be dropped.\n\t */\n\tif (mode == MIGRATE_ASYNC && head &&\n\t\t\t!buffer_migrate_lock_buffers(head, mode)) {\n\t\tpage_unfreeze_refs(page, expected_count);\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * Now we know that no one else is looking at the page:\n\t * no turning back from here.\n\t */\n\tset_page_memcg(newpage, page_memcg(page));\n\tnewpage->index = page->index;\n\tnewpage->mapping = page->mapping;\n\tif (PageSwapBacked(page))\n\t\tSetPageSwapBacked(newpage);\n\n\tget_page(newpage);\t/* add cache reference */\n\tif (PageSwapCache(page)) {\n\t\tSetPageSwapCache(newpage);\n\t\tset_page_private(newpage, page_private(page));\n\t}\n\n\tradix_tree_replace_slot(pslot, newpage);\n\n\t/*\n\t * Drop cache reference from old page by unfreezing\n\t * to one less reference.\n\t * We know this isn't the last reference.\n\t */\n\tpage_unfreeze_refs(page, expected_count - 1);\n\n\t/*\n\t * If moved to a different zone then also account\n\t * the page for that zone. Other VM counters will be\n\t * taken care of when we establish references to the\n\t * new page and drop references to the old page.\n\t *\n\t * Note that anonymous pages are accounted for\n\t * via NR_FILE_PAGES and NR_ANON_PAGES if they\n\t * are mapped to swap space.\n\t */\n\t__dec_zone_page_state(page, NR_FILE_PAGES);\n\t__inc_zone_page_state(newpage, NR_FILE_PAGES);\n\tif (!PageSwapCache(page) && PageSwapBacked(page)) {\n\t\t__dec_zone_page_state(page, NR_SHMEM);\n\t\t__inc_zone_page_state(newpage, NR_SHMEM);\n\t}\n\tspin_unlock_irq(&mapping->tree_lock);\n\n\treturn MIGRATEPAGE_SUCCESS;\n}",
                        "code_after_change": "int migrate_page_move_mapping(struct address_space *mapping,\n\t\tstruct page *newpage, struct page *page,\n\t\tstruct buffer_head *head, enum migrate_mode mode,\n\t\tint extra_count)\n{\n\tstruct zone *oldzone, *newzone;\n\tint dirty;\n\tint expected_count = 1 + extra_count;\n\tvoid **pslot;\n\n\tif (!mapping) {\n\t\t/* Anonymous page without mapping */\n\t\tif (page_count(page) != expected_count)\n\t\t\treturn -EAGAIN;\n\n\t\t/* No turning back from here */\n\t\tset_page_memcg(newpage, page_memcg(page));\n\t\tnewpage->index = page->index;\n\t\tnewpage->mapping = page->mapping;\n\t\tif (PageSwapBacked(page))\n\t\t\tSetPageSwapBacked(newpage);\n\n\t\treturn MIGRATEPAGE_SUCCESS;\n\t}\n\n\toldzone = page_zone(page);\n\tnewzone = page_zone(newpage);\n\n\tspin_lock_irq(&mapping->tree_lock);\n\n\tpslot = radix_tree_lookup_slot(&mapping->page_tree,\n \t\t\t\t\tpage_index(page));\n\n\texpected_count += 1 + page_has_private(page);\n\tif (page_count(page) != expected_count ||\n\t\tradix_tree_deref_slot_protected(pslot, &mapping->tree_lock) != page) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\tif (!page_freeze_refs(page, expected_count)) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * In the async migration case of moving a page with buffers, lock the\n\t * buffers using trylock before the mapping is moved. If the mapping\n\t * was moved, we later failed to lock the buffers and could not move\n\t * the mapping back due to an elevated page count, we would have to\n\t * block waiting on other references to be dropped.\n\t */\n\tif (mode == MIGRATE_ASYNC && head &&\n\t\t\t!buffer_migrate_lock_buffers(head, mode)) {\n\t\tpage_unfreeze_refs(page, expected_count);\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * Now we know that no one else is looking at the page:\n\t * no turning back from here.\n\t */\n\tset_page_memcg(newpage, page_memcg(page));\n\tnewpage->index = page->index;\n\tnewpage->mapping = page->mapping;\n\tif (PageSwapBacked(page))\n\t\tSetPageSwapBacked(newpage);\n\n\tget_page(newpage);\t/* add cache reference */\n\tif (PageSwapCache(page)) {\n\t\tSetPageSwapCache(newpage);\n\t\tset_page_private(newpage, page_private(page));\n\t}\n\n\t/* Move dirty while page refs frozen and newpage not yet exposed */\n\tdirty = PageDirty(page);\n\tif (dirty) {\n\t\tClearPageDirty(page);\n\t\tSetPageDirty(newpage);\n\t}\n\n\tradix_tree_replace_slot(pslot, newpage);\n\n\t/*\n\t * Drop cache reference from old page by unfreezing\n\t * to one less reference.\n\t * We know this isn't the last reference.\n\t */\n\tpage_unfreeze_refs(page, expected_count - 1);\n\n\tspin_unlock(&mapping->tree_lock);\n\t/* Leave irq disabled to prevent preemption while updating stats */\n\n\t/*\n\t * If moved to a different zone then also account\n\t * the page for that zone. Other VM counters will be\n\t * taken care of when we establish references to the\n\t * new page and drop references to the old page.\n\t *\n\t * Note that anonymous pages are accounted for\n\t * via NR_FILE_PAGES and NR_ANON_PAGES if they\n\t * are mapped to swap space.\n\t */\n\tif (newzone != oldzone) {\n\t\t__dec_zone_state(oldzone, NR_FILE_PAGES);\n\t\t__inc_zone_state(newzone, NR_FILE_PAGES);\n\t\tif (PageSwapBacked(page) && !PageSwapCache(page)) {\n\t\t\t__dec_zone_state(oldzone, NR_SHMEM);\n\t\t\t__inc_zone_state(newzone, NR_SHMEM);\n\t\t}\n\t\tif (dirty && mapping_cap_account_dirty(mapping)) {\n\t\t\t__dec_zone_state(oldzone, NR_FILE_DIRTY);\n\t\t\t__inc_zone_state(newzone, NR_FILE_DIRTY);\n\t\t}\n\t}\n\tlocal_irq_enable();\n\n\treturn MIGRATEPAGE_SUCCESS;\n}",
                        "cve_id": "CVE-2016-3070"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1372,
            "cve_id": "CVE-2017-15116",
            "code_snippet": "static int crypto_rng_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_rng *rng = __crypto_rng_cast(tfm);\n\tstruct rng_alg *alg = crypto_rng_alg(rng);\n\tstruct old_rng_alg *oalg = crypto_old_rng_alg(rng);\n\n\tif (oalg->rng_make_random) {\n\t\trng->generate = generate;\n\t\trng->seed = rngapi_reset;\n\t\trng->seedsize = oalg->seedsize;\n\t\treturn 0;\n\t}\n\n\trng->generate = alg->generate;\n\trng->seed = alg->seed;\n\trng->seedsize = alg->seedsize;\n\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int crypto_skcipher_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_skcipher *skcipher = __crypto_skcipher_cast(tfm);\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(skcipher);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_blkcipher_type)\n\t\treturn crypto_init_skcipher_ops_blkcipher(tfm);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_ablkcipher_type ||\n\t    tfm->__crt_alg->cra_type == &crypto_givcipher_type)\n\t\treturn crypto_init_skcipher_ops_ablkcipher(tfm);\n\n\tskcipher->setkey = alg->setkey;\n\tskcipher->encrypt = alg->encrypt;\n\tskcipher->decrypt = alg->decrypt;\n\tskcipher->ivsize = alg->ivsize;\n\tskcipher->keysize = alg->max_keysize;\n\n\tif (alg->exit)\n\t\tskcipher->base.exit = crypto_skcipher_exit_tfm;\n\n\tif (alg->init)\n\t\treturn alg->init(skcipher);\n\n\treturn 0;\n}",
                        "code_after_change": "static int __init big_key_init(void)\n{\n\tstruct crypto_skcipher *cipher;\n\tstruct crypto_rng *rng;\n\tint ret;\n\n\trng = crypto_alloc_rng(big_key_rng_name, 0, 0);\n\tif (IS_ERR(rng)) {\n\t\tpr_err(\"Can't alloc rng: %ld\\n\", PTR_ERR(rng));\n\t\treturn PTR_ERR(rng);\n\t}\n\n\tbig_key_rng = rng;\n\n\t/* seed RNG */\n\tret = crypto_rng_reset(rng, NULL, crypto_rng_seedsize(rng));\n\tif (ret) {\n\t\tpr_err(\"Can't reset rng: %d\\n\", ret);\n\t\tgoto error_rng;\n\t}\n\n\t/* init block cipher */\n\tcipher = crypto_alloc_skcipher(big_key_alg_name, 0, CRYPTO_ALG_ASYNC);\n\tif (IS_ERR(cipher)) {\n\t\tret = PTR_ERR(cipher);\n\t\tpr_err(\"Can't alloc crypto: %d\\n\", ret);\n\t\tgoto error_rng;\n\t}\n\n\tbig_key_skcipher = cipher;\n\n\tret = register_key_type(&key_type_big_key);\n\tif (ret < 0) {\n\t\tpr_err(\"Can't register type: %d\\n\", ret);\n\t\tgoto error_cipher;\n\t}\n\n\treturn 0;\n\nerror_cipher:\n\tcrypto_free_skcipher(big_key_skcipher);\nerror_rng:\n\tcrypto_free_rng(big_key_rng);\n\treturn ret;\n}",
                        "cve_id": "CVE-2017-9211"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static inline int crypto_rng_seedsize(struct crypto_rng *tfm)\n{\n\treturn tfm->seedsize;\n}",
                        "code_after_change": "static inline int crypto_rng_seedsize(struct crypto_rng *tfm)\n{\n\treturn crypto_rng_alg(tfm)->seedsize;\n}",
                        "cve_id": "CVE-2017-15116"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1373,
            "cve_id": "CVE-2017-15116",
            "code_snippet": "static unsigned int seedsize(struct crypto_alg *alg)\n{\n\tstruct rng_alg *ralg = container_of(alg, struct rng_alg, base);\n\n\treturn alg->cra_rng.rng_make_random ?\n\t       alg->cra_rng.seedsize : ralg->seedsize;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int mcryptd_create_hash(struct crypto_template *tmpl, struct rtattr **tb,\n\t\t\t      struct mcryptd_queue *queue)\n{\n\tstruct hashd_instance_ctx *ctx;\n\tstruct ahash_instance *inst;\n\tstruct hash_alg_common *halg;\n\tstruct crypto_alg *alg;\n\tu32 type = 0;\n\tu32 mask = 0;\n\tint err;\n\n\tmcryptd_check_internal(tb, &type, &mask);\n\n\thalg = ahash_attr_alg(tb[1], type, mask);\n\tif (IS_ERR(halg))\n\t\treturn PTR_ERR(halg);\n\n\talg = &halg->base;\n\tpr_debug(\"crypto: mcryptd hash alg: %s\\n\", alg->cra_name);\n\tinst = mcryptd_alloc_instance(alg, ahash_instance_headroom(),\n\t\t\t\t\tsizeof(*ctx));\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tctx = ahash_instance_ctx(inst);\n\tctx->queue = queue;\n\n\terr = crypto_init_ahash_spawn(&ctx->spawn, halg,\n\t\t\t\t      ahash_crypto_instance(inst));\n\tif (err)\n\t\tgoto out_free_inst;\n\n\ttype = CRYPTO_ALG_ASYNC;\n\tif (alg->cra_flags & CRYPTO_ALG_INTERNAL)\n\t\ttype |= CRYPTO_ALG_INTERNAL;\n\tinst->alg.halg.base.cra_flags = type;\n\n\tinst->alg.halg.digestsize = halg->digestsize;\n\tinst->alg.halg.statesize = halg->statesize;\n\tinst->alg.halg.base.cra_ctxsize = sizeof(struct mcryptd_hash_ctx);\n\n\tinst->alg.halg.base.cra_init = mcryptd_hash_init_tfm;\n\tinst->alg.halg.base.cra_exit = mcryptd_hash_exit_tfm;\n\n\tinst->alg.init   = mcryptd_hash_init_enqueue;\n\tinst->alg.update = mcryptd_hash_update_enqueue;\n\tinst->alg.final  = mcryptd_hash_final_enqueue;\n\tinst->alg.finup  = mcryptd_hash_finup_enqueue;\n\tinst->alg.export = mcryptd_hash_export;\n\tinst->alg.import = mcryptd_hash_import;\n\tinst->alg.setkey = mcryptd_hash_setkey;\n\tinst->alg.digest = mcryptd_hash_digest_enqueue;\n\n\terr = ahash_register_instance(tmpl, inst);\n\tif (err) {\n\t\tcrypto_drop_ahash(&ctx->spawn);\nout_free_inst:\n\t\tkfree(inst);\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}",
                        "code_after_change": "static int mcryptd_create_hash(struct crypto_template *tmpl, struct rtattr **tb,\n\t\t\t      struct mcryptd_queue *queue)\n{\n\tstruct hashd_instance_ctx *ctx;\n\tstruct ahash_instance *inst;\n\tstruct hash_alg_common *halg;\n\tstruct crypto_alg *alg;\n\tu32 type = 0;\n\tu32 mask = 0;\n\tint err;\n\n\tif (!mcryptd_check_internal(tb, &type, &mask))\n\t\treturn -EINVAL;\n\n\thalg = ahash_attr_alg(tb[1], type, mask);\n\tif (IS_ERR(halg))\n\t\treturn PTR_ERR(halg);\n\n\talg = &halg->base;\n\tpr_debug(\"crypto: mcryptd hash alg: %s\\n\", alg->cra_name);\n\tinst = mcryptd_alloc_instance(alg, ahash_instance_headroom(),\n\t\t\t\t\tsizeof(*ctx));\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tctx = ahash_instance_ctx(inst);\n\tctx->queue = queue;\n\n\terr = crypto_init_ahash_spawn(&ctx->spawn, halg,\n\t\t\t\t      ahash_crypto_instance(inst));\n\tif (err)\n\t\tgoto out_free_inst;\n\n\ttype = CRYPTO_ALG_ASYNC;\n\tif (alg->cra_flags & CRYPTO_ALG_INTERNAL)\n\t\ttype |= CRYPTO_ALG_INTERNAL;\n\tinst->alg.halg.base.cra_flags = type;\n\n\tinst->alg.halg.digestsize = halg->digestsize;\n\tinst->alg.halg.statesize = halg->statesize;\n\tinst->alg.halg.base.cra_ctxsize = sizeof(struct mcryptd_hash_ctx);\n\n\tinst->alg.halg.base.cra_init = mcryptd_hash_init_tfm;\n\tinst->alg.halg.base.cra_exit = mcryptd_hash_exit_tfm;\n\n\tinst->alg.init   = mcryptd_hash_init_enqueue;\n\tinst->alg.update = mcryptd_hash_update_enqueue;\n\tinst->alg.final  = mcryptd_hash_final_enqueue;\n\tinst->alg.finup  = mcryptd_hash_finup_enqueue;\n\tinst->alg.export = mcryptd_hash_export;\n\tinst->alg.import = mcryptd_hash_import;\n\tinst->alg.setkey = mcryptd_hash_setkey;\n\tinst->alg.digest = mcryptd_hash_digest_enqueue;\n\n\terr = ahash_register_instance(tmpl, inst);\n\tif (err) {\n\t\tcrypto_drop_ahash(&ctx->spawn);\nout_free_inst:\n\t\tkfree(inst);\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}",
                        "cve_id": "CVE-2016-10147"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int crypto_skcipher_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_skcipher *skcipher = __crypto_skcipher_cast(tfm);\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(skcipher);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_blkcipher_type)\n\t\treturn crypto_init_skcipher_ops_blkcipher(tfm);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_ablkcipher_type ||\n\t    tfm->__crt_alg->cra_type == &crypto_givcipher_type)\n\t\treturn crypto_init_skcipher_ops_ablkcipher(tfm);\n\n\tskcipher->setkey = alg->setkey;\n\tskcipher->encrypt = alg->encrypt;\n\tskcipher->decrypt = alg->decrypt;\n\tskcipher->ivsize = alg->ivsize;\n\tskcipher->keysize = alg->max_keysize;\n\n\tif (alg->exit)\n\t\tskcipher->base.exit = crypto_skcipher_exit_tfm;\n\n\tif (alg->init)\n\t\treturn alg->init(skcipher);\n\n\treturn 0;\n}",
                        "code_after_change": "static int crypto_skcipher_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_skcipher *skcipher = __crypto_skcipher_cast(tfm);\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(skcipher);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_blkcipher_type)\n\t\treturn crypto_init_skcipher_ops_blkcipher(tfm);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_ablkcipher_type ||\n\t    tfm->__crt_alg->cra_type == &crypto_givcipher_type)\n\t\treturn crypto_init_skcipher_ops_ablkcipher(tfm);\n\n\tskcipher->setkey = skcipher_setkey;\n\tskcipher->encrypt = alg->encrypt;\n\tskcipher->decrypt = alg->decrypt;\n\tskcipher->ivsize = alg->ivsize;\n\tskcipher->keysize = alg->max_keysize;\n\n\tif (alg->exit)\n\t\tskcipher->base.exit = crypto_skcipher_exit_tfm;\n\n\tif (alg->init)\n\t\treturn alg->init(skcipher);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2017-9211"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static inline int crypto_rng_seedsize(struct crypto_rng *tfm)\n{\n\treturn tfm->seedsize;\n}",
                        "code_after_change": "static inline int crypto_rng_seedsize(struct crypto_rng *tfm)\n{\n\treturn crypto_rng_alg(tfm)->seedsize;\n}",
                        "cve_id": "CVE-2017-15116"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int do_tls_getsockopt_conf(struct sock *sk, char __user *optval,\n\t\t\t\t  int __user *optlen, int tx)\n{\n\tint rc = 0;\n\tstruct tls_context *ctx = tls_get_ctx(sk);\n\tstruct tls_crypto_info *crypto_info;\n\tstruct cipher_context *cctx;\n\tint len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tif (!optval || (len < sizeof(*crypto_info))) {\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (!ctx) {\n\t\trc = -EBUSY;\n\t\tgoto out;\n\t}\n\n\t/* get user crypto info */\n\tif (tx) {\n\t\tcrypto_info = &ctx->crypto_send.info;\n\t\tcctx = &ctx->tx;\n\t} else {\n\t\tcrypto_info = &ctx->crypto_recv.info;\n\t\tcctx = &ctx->rx;\n\t}\n\n\tif (!TLS_CRYPTO_INFO_READY(crypto_info)) {\n\t\trc = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tif (len == sizeof(*crypto_info)) {\n\t\tif (copy_to_user(optval, crypto_info, sizeof(*crypto_info)))\n\t\t\trc = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tswitch (crypto_info->cipher_type) {\n\tcase TLS_CIPHER_AES_GCM_128: {\n\t\tstruct tls12_crypto_info_aes_gcm_128 *\n\t\t  crypto_info_aes_gcm_128 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aes_gcm_128,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aes_gcm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(crypto_info_aes_gcm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_GCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_GCM_128_IV_SIZE);\n\t\tmemcpy(crypto_info_aes_gcm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_GCM_128_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aes_gcm_128,\n\t\t\t\t sizeof(*crypto_info_aes_gcm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_AES_GCM_256: {\n\t\tstruct tls12_crypto_info_aes_gcm_256 *\n\t\t  crypto_info_aes_gcm_256 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aes_gcm_256,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aes_gcm_256)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(crypto_info_aes_gcm_256->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_GCM_256_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_GCM_256_IV_SIZE);\n\t\tmemcpy(crypto_info_aes_gcm_256->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_GCM_256_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aes_gcm_256,\n\t\t\t\t sizeof(*crypto_info_aes_gcm_256)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_AES_CCM_128: {\n\t\tstruct tls12_crypto_info_aes_ccm_128 *aes_ccm_128 =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_aes_ccm_128, info);\n\n\t\tif (len != sizeof(*aes_ccm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(aes_ccm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_CCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_CCM_128_IV_SIZE);\n\t\tmemcpy(aes_ccm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_CCM_128_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval, aes_ccm_128, sizeof(*aes_ccm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_CHACHA20_POLY1305: {\n\t\tstruct tls12_crypto_info_chacha20_poly1305 *chacha20_poly1305 =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_chacha20_poly1305,\n\t\t\t\tinfo);\n\n\t\tif (len != sizeof(*chacha20_poly1305)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(chacha20_poly1305->iv,\n\t\t       cctx->iv + TLS_CIPHER_CHACHA20_POLY1305_SALT_SIZE,\n\t\t       TLS_CIPHER_CHACHA20_POLY1305_IV_SIZE);\n\t\tmemcpy(chacha20_poly1305->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_CHACHA20_POLY1305_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval, chacha20_poly1305,\n\t\t\t\tsizeof(*chacha20_poly1305)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_SM4_GCM: {\n\t\tstruct tls12_crypto_info_sm4_gcm *sm4_gcm_info =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_sm4_gcm, info);\n\n\t\tif (len != sizeof(*sm4_gcm_info)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(sm4_gcm_info->iv,\n\t\t       cctx->iv + TLS_CIPHER_SM4_GCM_SALT_SIZE,\n\t\t       TLS_CIPHER_SM4_GCM_IV_SIZE);\n\t\tmemcpy(sm4_gcm_info->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_SM4_GCM_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval, sm4_gcm_info, sizeof(*sm4_gcm_info)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_SM4_CCM: {\n\t\tstruct tls12_crypto_info_sm4_ccm *sm4_ccm_info =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_sm4_ccm, info);\n\n\t\tif (len != sizeof(*sm4_ccm_info)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(sm4_ccm_info->iv,\n\t\t       cctx->iv + TLS_CIPHER_SM4_CCM_SALT_SIZE,\n\t\t       TLS_CIPHER_SM4_CCM_IV_SIZE);\n\t\tmemcpy(sm4_ccm_info->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_SM4_CCM_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval, sm4_ccm_info, sizeof(*sm4_ccm_info)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_ARIA_GCM_128: {\n\t\tstruct tls12_crypto_info_aria_gcm_128 *\n\t\t  crypto_info_aria_gcm_128 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aria_gcm_128,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aria_gcm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(crypto_info_aria_gcm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_ARIA_GCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_ARIA_GCM_128_IV_SIZE);\n\t\tmemcpy(crypto_info_aria_gcm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_ARIA_GCM_128_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aria_gcm_128,\n\t\t\t\t sizeof(*crypto_info_aria_gcm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_ARIA_GCM_256: {\n\t\tstruct tls12_crypto_info_aria_gcm_256 *\n\t\t  crypto_info_aria_gcm_256 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aria_gcm_256,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aria_gcm_256)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(crypto_info_aria_gcm_256->iv,\n\t\t       cctx->iv + TLS_CIPHER_ARIA_GCM_256_SALT_SIZE,\n\t\t       TLS_CIPHER_ARIA_GCM_256_IV_SIZE);\n\t\tmemcpy(crypto_info_aria_gcm_256->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_ARIA_GCM_256_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aria_gcm_256,\n\t\t\t\t sizeof(*crypto_info_aria_gcm_256)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tdefault:\n\t\trc = -EINVAL;\n\t}\n\nout:\n\treturn rc;\n}",
                        "code_after_change": "static int do_tls_getsockopt_conf(struct sock *sk, char __user *optval,\n\t\t\t\t  int __user *optlen, int tx)\n{\n\tint rc = 0;\n\tstruct tls_context *ctx = tls_get_ctx(sk);\n\tstruct tls_crypto_info *crypto_info;\n\tstruct cipher_context *cctx;\n\tint len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tif (!optval || (len < sizeof(*crypto_info))) {\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (!ctx) {\n\t\trc = -EBUSY;\n\t\tgoto out;\n\t}\n\n\t/* get user crypto info */\n\tif (tx) {\n\t\tcrypto_info = &ctx->crypto_send.info;\n\t\tcctx = &ctx->tx;\n\t} else {\n\t\tcrypto_info = &ctx->crypto_recv.info;\n\t\tcctx = &ctx->rx;\n\t}\n\n\tif (!TLS_CRYPTO_INFO_READY(crypto_info)) {\n\t\trc = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tif (len == sizeof(*crypto_info)) {\n\t\tif (copy_to_user(optval, crypto_info, sizeof(*crypto_info)))\n\t\t\trc = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tswitch (crypto_info->cipher_type) {\n\tcase TLS_CIPHER_AES_GCM_128: {\n\t\tstruct tls12_crypto_info_aes_gcm_128 *\n\t\t  crypto_info_aes_gcm_128 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aes_gcm_128,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aes_gcm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(crypto_info_aes_gcm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_GCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_GCM_128_IV_SIZE);\n\t\tmemcpy(crypto_info_aes_gcm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_GCM_128_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aes_gcm_128,\n\t\t\t\t sizeof(*crypto_info_aes_gcm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_AES_GCM_256: {\n\t\tstruct tls12_crypto_info_aes_gcm_256 *\n\t\t  crypto_info_aes_gcm_256 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aes_gcm_256,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aes_gcm_256)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(crypto_info_aes_gcm_256->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_GCM_256_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_GCM_256_IV_SIZE);\n\t\tmemcpy(crypto_info_aes_gcm_256->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_GCM_256_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aes_gcm_256,\n\t\t\t\t sizeof(*crypto_info_aes_gcm_256)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_AES_CCM_128: {\n\t\tstruct tls12_crypto_info_aes_ccm_128 *aes_ccm_128 =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_aes_ccm_128, info);\n\n\t\tif (len != sizeof(*aes_ccm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(aes_ccm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_CCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_CCM_128_IV_SIZE);\n\t\tmemcpy(aes_ccm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_CCM_128_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval, aes_ccm_128, sizeof(*aes_ccm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_CHACHA20_POLY1305: {\n\t\tstruct tls12_crypto_info_chacha20_poly1305 *chacha20_poly1305 =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_chacha20_poly1305,\n\t\t\t\tinfo);\n\n\t\tif (len != sizeof(*chacha20_poly1305)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(chacha20_poly1305->iv,\n\t\t       cctx->iv + TLS_CIPHER_CHACHA20_POLY1305_SALT_SIZE,\n\t\t       TLS_CIPHER_CHACHA20_POLY1305_IV_SIZE);\n\t\tmemcpy(chacha20_poly1305->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_CHACHA20_POLY1305_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval, chacha20_poly1305,\n\t\t\t\tsizeof(*chacha20_poly1305)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_SM4_GCM: {\n\t\tstruct tls12_crypto_info_sm4_gcm *sm4_gcm_info =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_sm4_gcm, info);\n\n\t\tif (len != sizeof(*sm4_gcm_info)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(sm4_gcm_info->iv,\n\t\t       cctx->iv + TLS_CIPHER_SM4_GCM_SALT_SIZE,\n\t\t       TLS_CIPHER_SM4_GCM_IV_SIZE);\n\t\tmemcpy(sm4_gcm_info->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_SM4_GCM_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval, sm4_gcm_info, sizeof(*sm4_gcm_info)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_SM4_CCM: {\n\t\tstruct tls12_crypto_info_sm4_ccm *sm4_ccm_info =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_sm4_ccm, info);\n\n\t\tif (len != sizeof(*sm4_ccm_info)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(sm4_ccm_info->iv,\n\t\t       cctx->iv + TLS_CIPHER_SM4_CCM_SALT_SIZE,\n\t\t       TLS_CIPHER_SM4_CCM_IV_SIZE);\n\t\tmemcpy(sm4_ccm_info->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_SM4_CCM_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval, sm4_ccm_info, sizeof(*sm4_ccm_info)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_ARIA_GCM_128: {\n\t\tstruct tls12_crypto_info_aria_gcm_128 *\n\t\t  crypto_info_aria_gcm_128 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aria_gcm_128,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aria_gcm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(crypto_info_aria_gcm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_ARIA_GCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_ARIA_GCM_128_IV_SIZE);\n\t\tmemcpy(crypto_info_aria_gcm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_ARIA_GCM_128_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aria_gcm_128,\n\t\t\t\t sizeof(*crypto_info_aria_gcm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_ARIA_GCM_256: {\n\t\tstruct tls12_crypto_info_aria_gcm_256 *\n\t\t  crypto_info_aria_gcm_256 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aria_gcm_256,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aria_gcm_256)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(crypto_info_aria_gcm_256->iv,\n\t\t       cctx->iv + TLS_CIPHER_ARIA_GCM_256_SALT_SIZE,\n\t\t       TLS_CIPHER_ARIA_GCM_256_IV_SIZE);\n\t\tmemcpy(crypto_info_aria_gcm_256->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_ARIA_GCM_256_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aria_gcm_256,\n\t\t\t\t sizeof(*crypto_info_aria_gcm_256)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tdefault:\n\t\trc = -EINVAL;\n\t}\n\nout:\n\treturn rc;\n}",
                        "cve_id": "CVE-2023-28466"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int brcm_nvram_parse(struct brcm_nvram *priv)\n{\n\tstruct device *dev = priv->dev;\n\tstruct brcm_nvram_header header;\n\tuint8_t *data;\n\tsize_t len;\n\tint err;\n\n\tmemcpy_fromio(&header, priv->base, sizeof(header));\n\n\tif (memcmp(header.magic, NVRAM_MAGIC, 4)) {\n\t\tdev_err(dev, \"Invalid NVRAM magic\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tlen = le32_to_cpu(header.len);\n\n\tdata = kzalloc(len, GFP_KERNEL);\n\tmemcpy_fromio(data, priv->base, len);\n\tdata[len - 1] = '\\0';\n\n\terr = brcm_nvram_add_cells(priv, data, len);\n\tif (err) {\n\t\tdev_err(dev, \"Failed to add cells: %d\\n\", err);\n\t\treturn err;\n\t}\n\n\tkfree(data);\n\n\treturn 0;\n}",
                        "code_after_change": "static int brcm_nvram_parse(struct brcm_nvram *priv)\n{\n\tstruct device *dev = priv->dev;\n\tstruct brcm_nvram_header header;\n\tuint8_t *data;\n\tsize_t len;\n\tint err;\n\n\tmemcpy_fromio(&header, priv->base, sizeof(header));\n\n\tif (memcmp(header.magic, NVRAM_MAGIC, 4)) {\n\t\tdev_err(dev, \"Invalid NVRAM magic\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tlen = le32_to_cpu(header.len);\n\n\tdata = kzalloc(len, GFP_KERNEL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tmemcpy_fromio(data, priv->base, len);\n\tdata[len - 1] = '\\0';\n\n\terr = brcm_nvram_add_cells(priv, data, len);\n\tif (err) {\n\t\tdev_err(dev, \"Failed to add cells: %d\\n\", err);\n\t\treturn err;\n\t}\n\n\tkfree(data);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2023-3359"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 1384,
            "cve_id": "CVE-2017-15274",
            "code_snippet": " */\nSYSCALL_DEFINE5(add_key, const char __user *, _type,\n\t\tconst char __user *, _description,\n\t\tconst void __user *, _payload,\n\t\tsize_t, plen,\n\t\tkey_serial_t, ringid)\n{\n\tkey_ref_t keyring_ref, key_ref;\n\tchar type[32], *description;\n\tvoid *payload;\n\tlong ret;\n\n\tret = -EINVAL;\n\tif (plen > 1024 * 1024 - 1)\n\t\tgoto error;\n\n\t/* draw all the data into kernel space */\n\tret = key_get_type_from_user(type, _type, sizeof(type));\n\tif (ret < 0)\n\t\tgoto error;\n\n\tdescription = NULL;\n\tif (_description) {\n\t\tdescription = strndup_user(_description, KEY_MAX_DESC_SIZE);\n\t\tif (IS_ERR(description)) {\n\t\t\tret = PTR_ERR(description);\n\t\t\tgoto error;\n\t\t}\n\t\tif (!*description) {\n\t\t\tkfree(description);\n\t\t\tdescription = NULL;\n\t\t} else if ((description[0] == '.') &&\n\t\t\t   (strncmp(type, \"keyring\", 7) == 0)) {\n\t\t\tret = -EPERM;\n\t\t\tgoto error2;\n\t\t}\n\t}\n\n\t/* pull the payload in if one was supplied */\n\tpayload = NULL;\n\n\tif (_payload) {\n\t\tret = -ENOMEM;\n\t\tpayload = kvmalloc(plen, GFP_KERNEL);\n\t\tif (!payload)\n\t\t\tgoto error2;\n\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(payload, _payload, plen) != 0)\n\t\t\tgoto error3;\n\t}\n\n\t/* find the target keyring (which must be writable) */\n\tkeyring_ref = lookup_user_key(ringid, KEY_LOOKUP_CREATE, KEY_NEED_WRITE);\n\tif (IS_ERR(keyring_ref)) {\n\t\tret = PTR_ERR(keyring_ref);\n\t\tgoto error3;\n\t}\n\n\t/* create or update the requested key and add it to the target\n\t * keyring */\n\tkey_ref = key_create_or_update(keyring_ref, type, description,\n\t\t\t\t       payload, plen, KEY_PERM_UNDEF,\n\t\t\t\t       KEY_ALLOC_IN_QUOTA);\n\tif (!IS_ERR(key_ref)) {\n\t\tret = key_ref_to_ptr(key_ref)->serial;\n\t\tkey_ref_put(key_ref);\n\t}\n\telse {\n\t\tret = PTR_ERR(key_ref);\n\t}\n\n\tkey_ref_put(keyring_ref);\n error3:\n\tkvfree(payload);\n error2:\n\tkfree(description);\n error:\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "long keyctl_update_key(key_serial_t id,\n\t\t       const void __user *_payload,\n\t\t       size_t plen)\n{\n\tkey_ref_t key_ref;\n\tvoid *payload;\n\tlong ret;\n\n\tret = -EINVAL;\n\tif (plen > PAGE_SIZE)\n\t\tgoto error;\n\n\t/* pull the payload in if one was supplied */\n\tpayload = NULL;\n\tif (_payload) {\n\t\tret = -ENOMEM;\n\t\tpayload = kmalloc(plen, GFP_KERNEL);\n\t\tif (!payload)\n\t\t\tgoto error;\n\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(payload, _payload, plen) != 0)\n\t\t\tgoto error2;\n\t}\n\n\t/* find the target key (which must be writable) */\n\tkey_ref = lookup_user_key(id, 0, KEY_NEED_WRITE);\n\tif (IS_ERR(key_ref)) {\n\t\tret = PTR_ERR(key_ref);\n\t\tgoto error2;\n\t}\n\n\t/* update the key */\n\tret = key_update(key_ref, payload, plen);\n\n\tkey_ref_put(key_ref);\nerror2:\n\tkfree(payload);\nerror:\n\treturn ret;\n}",
                        "code_after_change": "long keyctl_update_key(key_serial_t id,\n\t\t       const void __user *_payload,\n\t\t       size_t plen)\n{\n\tkey_ref_t key_ref;\n\tvoid *payload;\n\tlong ret;\n\n\tret = -EINVAL;\n\tif (plen > PAGE_SIZE)\n\t\tgoto error;\n\n\t/* pull the payload in if one was supplied */\n\tpayload = NULL;\n\tif (plen) {\n\t\tret = -ENOMEM;\n\t\tpayload = kmalloc(plen, GFP_KERNEL);\n\t\tif (!payload)\n\t\t\tgoto error;\n\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(payload, _payload, plen) != 0)\n\t\t\tgoto error2;\n\t}\n\n\t/* find the target key (which must be writable) */\n\tkey_ref = lookup_user_key(id, 0, KEY_NEED_WRITE);\n\tif (IS_ERR(key_ref)) {\n\t\tret = PTR_ERR(key_ref);\n\t\tgoto error2;\n\t}\n\n\t/* update the key */\n\tret = key_update(key_ref, payload, plen);\n\n\tkey_ref_put(key_ref);\nerror2:\n\tkfree(payload);\nerror:\n\treturn ret;\n}",
                        "cve_id": "CVE-2017-15274"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1410,
            "cve_id": "CVE-2017-16646",
            "code_snippet": "static int stk7070p_frontend_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct usb_device_descriptor *p = &adap->dev->udev->descriptor;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\tif (p->idVendor  == cpu_to_le16(USB_VID_PINNACLE) &&\n\t    p->idProduct == cpu_to_le16(USB_PID_PINNACLE_PCTV72E))\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 0);\n\telse\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\n\tdib0700_ctrl_clock(adap->dev, 72, 1);\n\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 18,\n\t\t\t\t     &dib7070p_dib7000p_config) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\",\n\t\t    __func__);\n\t\tdvb_detach(&state->dib7000p_ops);\n\t\treturn -ENODEV;\n\t}\n\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x80,\n\t\t&dib7070p_dib7000p_config);\n\treturn adap->fe_adap[0].fe == NULL ? -ENODEV : 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int stk7770p_frontend_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct usb_device_descriptor *p = &adap->dev->udev->descriptor;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\tif (p->idVendor  == cpu_to_le16(USB_VID_PINNACLE) &&\n\t    p->idProduct == cpu_to_le16(USB_PID_PINNACLE_PCTV72E))\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 0);\n\telse\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\n\tdib0700_ctrl_clock(adap->dev, 72, 1);\n\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 18,\n\t\t\t\t     &dib7770p_dib7000p_config) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\",\n\t\t    __func__);\n\t\tdvb_detach(&state->dib7000p_ops);\n\t\treturn -ENODEV;\n\t}\n\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x80,\n\t\t&dib7770p_dib7000p_config);\n\treturn adap->fe_adap[0].fe == NULL ? -ENODEV : 0;\n}",
                        "code_after_change": "static int stk7770p_frontend_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct usb_device_descriptor *p = &adap->dev->udev->descriptor;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\tif (p->idVendor  == cpu_to_le16(USB_VID_PINNACLE) &&\n\t    p->idProduct == cpu_to_le16(USB_PID_PINNACLE_PCTV72E))\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 0);\n\telse\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\n\tdib0700_ctrl_clock(adap->dev, 72, 1);\n\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 18,\n\t\t\t\t     &dib7770p_dib7000p_config) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\",\n\t\t    __func__);\n\t\tdvb_detach(state->dib7000p_ops.set_wbd_ref);\n\t\treturn -ENODEV;\n\t}\n\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x80,\n\t\t&dib7770p_dib7000p_config);\n\treturn adap->fe_adap[0].fe == NULL ? -ENODEV : 0;\n}",
                        "cve_id": "CVE-2017-16646"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1411,
            "cve_id": "CVE-2017-16646",
            "code_snippet": "static int pctv340e_frontend_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct dib0700_state *st = adap->dev->priv;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\t/* Power Supply on */\n\tdib0700_set_gpio(adap->dev, GPIO6,  GPIO_OUT, 0);\n\tmsleep(50);\n\tdib0700_set_gpio(adap->dev, GPIO6,  GPIO_OUT, 1);\n\tmsleep(100); /* Allow power supply to settle before probing */\n\n\t/* cx25843 reset */\n\tdib0700_set_gpio(adap->dev, GPIO10,  GPIO_OUT, 0);\n\tmsleep(1); /* cx25843 datasheet say 350us required */\n\tdib0700_set_gpio(adap->dev, GPIO10,  GPIO_OUT, 1);\n\n\t/* LNA off for now */\n\tdib0700_set_gpio(adap->dev, GPIO8,  GPIO_OUT, 1);\n\n\t/* Put the CX25843 to sleep for now since we're in digital mode */\n\tdib0700_set_gpio(adap->dev, GPIO2, GPIO_OUT, 1);\n\n\t/* FIXME: not verified yet */\n\tdib0700_ctrl_clock(adap->dev, 72, 1);\n\n\tmsleep(500);\n\n\tif (state->dib7000p_ops.dib7000pc_detection(&adap->dev->i2c_adap) == 0) {\n\t\t/* Demodulator not found for some reason? */\n\t\tdvb_detach(&state->dib7000p_ops);\n\t\treturn -ENODEV;\n\t}\n\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x12,\n\t\t\t      &pctv_340e_config);\n\tst->is_dib7000pc = 1;\n\n\treturn adap->fe_adap[0].fe == NULL ? -ENODEV : 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tfe7090pvr_frontend0_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct dib0700_state *st = adap->dev->priv;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\t/* The TFE7090 requires the dib0700 to not be in master mode */\n\tst->disable_streaming_master_mode = 1;\n\n\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\t/* initialize IC 0 */\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 0x20, &tfe7090pvr_dib7000p_config[0]) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\", __func__);\n\t\tdvb_detach(&state->dib7000p_ops);\n\t\treturn -ENODEV;\n\t}\n\n\tdib0700_set_i2c_speed(adap->dev, 340);\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x90, &tfe7090pvr_dib7000p_config[0]);\n\tif (adap->fe_adap[0].fe == NULL)\n\t\treturn -ENODEV;\n\n\tstate->dib7000p_ops.slave_reset(adap->fe_adap[0].fe);\n\n\treturn 0;\n}",
                        "code_after_change": "static int tfe7090pvr_frontend0_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct dib0700_state *st = adap->dev->priv;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\t/* The TFE7090 requires the dib0700 to not be in master mode */\n\tst->disable_streaming_master_mode = 1;\n\n\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\t/* initialize IC 0 */\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 0x20, &tfe7090pvr_dib7000p_config[0]) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\", __func__);\n\t\tdvb_detach(state->dib7000p_ops.set_wbd_ref);\n\t\treturn -ENODEV;\n\t}\n\n\tdib0700_set_i2c_speed(adap->dev, 340);\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x90, &tfe7090pvr_dib7000p_config[0]);\n\tif (adap->fe_adap[0].fe == NULL)\n\t\treturn -ENODEV;\n\n\tstate->dib7000p_ops.slave_reset(adap->fe_adap[0].fe);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2017-16646"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1412,
            "cve_id": "CVE-2017-16646",
            "code_snippet": "static int tfe7790p_frontend_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct dib0700_state *st = adap->dev->priv;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\t/* The TFE7790P requires the dib0700 to not be in master mode */\n\tst->disable_streaming_master_mode = 1;\n\n\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\tmsleep(20);\n\tdib0700_ctrl_clock(adap->dev, 72, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap,\n\t\t\t\t1, 0x10, &tfe7790p_dib7000p_config) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\",\n\t\t\t\t__func__);\n\t\tdvb_detach(&state->dib7000p_ops);\n\t\treturn -ENODEV;\n\t}\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap,\n\t\t\t0x80, &tfe7790p_dib7000p_config);\n\n\treturn adap->fe_adap[0].fe == NULL ?  -ENODEV : 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tfe7090pvr_frontend0_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct dib0700_state *st = adap->dev->priv;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\t/* The TFE7090 requires the dib0700 to not be in master mode */\n\tst->disable_streaming_master_mode = 1;\n\n\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\t/* initialize IC 0 */\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 0x20, &tfe7090pvr_dib7000p_config[0]) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\", __func__);\n\t\tdvb_detach(&state->dib7000p_ops);\n\t\treturn -ENODEV;\n\t}\n\n\tdib0700_set_i2c_speed(adap->dev, 340);\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x90, &tfe7090pvr_dib7000p_config[0]);\n\tif (adap->fe_adap[0].fe == NULL)\n\t\treturn -ENODEV;\n\n\tstate->dib7000p_ops.slave_reset(adap->fe_adap[0].fe);\n\n\treturn 0;\n}",
                        "code_after_change": "static int tfe7090pvr_frontend0_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct dib0700_state *st = adap->dev->priv;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\t/* The TFE7090 requires the dib0700 to not be in master mode */\n\tst->disable_streaming_master_mode = 1;\n\n\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\t/* initialize IC 0 */\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 0x20, &tfe7090pvr_dib7000p_config[0]) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\", __func__);\n\t\tdvb_detach(state->dib7000p_ops.set_wbd_ref);\n\t\treturn -ENODEV;\n\t}\n\n\tdib0700_set_i2c_speed(adap->dev, 340);\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x90, &tfe7090pvr_dib7000p_config[0]);\n\tif (adap->fe_adap[0].fe == NULL)\n\t\treturn -ENODEV;\n\n\tstate->dib7000p_ops.slave_reset(adap->fe_adap[0].fe);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2017-16646"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1413,
            "cve_id": "CVE-2017-16646",
            "code_snippet": "static int stk7700ph_frontend_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct usb_device_descriptor *desc = &adap->dev->udev->descriptor;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\tif (desc->idVendor  == cpu_to_le16(USB_VID_PINNACLE) &&\n\t    desc->idProduct == cpu_to_le16(USB_PID_PINNACLE_EXPRESSCARD_320CX))\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 0);\n\telse\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\tmsleep(10);\n\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 18,\n\t\t\t\t     &stk7700ph_dib7700_xc3028_config) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\",\n\t\t    __func__);\n\t\tdvb_detach(&state->dib7000p_ops);\n\t\treturn -ENODEV;\n\t}\n\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x80,\n\t\t&stk7700ph_dib7700_xc3028_config);\n\n\treturn adap->fe_adap[0].fe == NULL ? -ENODEV : 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int stk7770p_frontend_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct usb_device_descriptor *p = &adap->dev->udev->descriptor;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\tif (p->idVendor  == cpu_to_le16(USB_VID_PINNACLE) &&\n\t    p->idProduct == cpu_to_le16(USB_PID_PINNACLE_PCTV72E))\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 0);\n\telse\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\n\tdib0700_ctrl_clock(adap->dev, 72, 1);\n\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 18,\n\t\t\t\t     &dib7770p_dib7000p_config) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\",\n\t\t    __func__);\n\t\tdvb_detach(&state->dib7000p_ops);\n\t\treturn -ENODEV;\n\t}\n\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x80,\n\t\t&dib7770p_dib7000p_config);\n\treturn adap->fe_adap[0].fe == NULL ? -ENODEV : 0;\n}",
                        "code_after_change": "static int stk7770p_frontend_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct usb_device_descriptor *p = &adap->dev->udev->descriptor;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\tif (p->idVendor  == cpu_to_le16(USB_VID_PINNACLE) &&\n\t    p->idProduct == cpu_to_le16(USB_PID_PINNACLE_PCTV72E))\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 0);\n\telse\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\n\tdib0700_ctrl_clock(adap->dev, 72, 1);\n\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 18,\n\t\t\t\t     &dib7770p_dib7000p_config) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\",\n\t\t    __func__);\n\t\tdvb_detach(state->dib7000p_ops.set_wbd_ref);\n\t\treturn -ENODEV;\n\t}\n\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x80,\n\t\t&dib7770p_dib7000p_config);\n\treturn adap->fe_adap[0].fe == NULL ? -ENODEV : 0;\n}",
                        "cve_id": "CVE-2017-16646"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1414,
            "cve_id": "CVE-2017-16646",
            "code_snippet": "static int tfe7090pvr_frontend1_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct i2c_adapter *i2c;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (adap->dev->adapter[0].fe_adap[0].fe == NULL) {\n\t\terr(\"the master dib7090 has to be initialized first\");\n\t\treturn -ENODEV; /* the master device has not been initialized */\n\t}\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\ti2c = state->dib7000p_ops.get_i2c_master(adap->dev->adapter[0].fe_adap[0].fe, DIBX000_I2C_INTERFACE_GPIO_6_7, 1);\n\tif (state->dib7000p_ops.i2c_enumeration(i2c, 1, 0x10, &tfe7090pvr_dib7000p_config[1]) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\", __func__);\n\t\tdvb_detach(&state->dib7000p_ops);\n\t\treturn -ENODEV;\n\t}\n\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(i2c, 0x92, &tfe7090pvr_dib7000p_config[1]);\n\tdib0700_set_i2c_speed(adap->dev, 200);\n\n\treturn adap->fe_adap[0].fe == NULL ? -ENODEV : 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tfe7090pvr_frontend0_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct dib0700_state *st = adap->dev->priv;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\t/* The TFE7090 requires the dib0700 to not be in master mode */\n\tst->disable_streaming_master_mode = 1;\n\n\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\t/* initialize IC 0 */\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 0x20, &tfe7090pvr_dib7000p_config[0]) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\", __func__);\n\t\tdvb_detach(&state->dib7000p_ops);\n\t\treturn -ENODEV;\n\t}\n\n\tdib0700_set_i2c_speed(adap->dev, 340);\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x90, &tfe7090pvr_dib7000p_config[0]);\n\tif (adap->fe_adap[0].fe == NULL)\n\t\treturn -ENODEV;\n\n\tstate->dib7000p_ops.slave_reset(adap->fe_adap[0].fe);\n\n\treturn 0;\n}",
                        "code_after_change": "static int tfe7090pvr_frontend0_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct dib0700_state *st = adap->dev->priv;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\t/* The TFE7090 requires the dib0700 to not be in master mode */\n\tst->disable_streaming_master_mode = 1;\n\n\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\t/* initialize IC 0 */\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 0x20, &tfe7090pvr_dib7000p_config[0]) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\", __func__);\n\t\tdvb_detach(state->dib7000p_ops.set_wbd_ref);\n\t\treturn -ENODEV;\n\t}\n\n\tdib0700_set_i2c_speed(adap->dev, 340);\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x90, &tfe7090pvr_dib7000p_config[0]);\n\tif (adap->fe_adap[0].fe == NULL)\n\t\treturn -ENODEV;\n\n\tstate->dib7000p_ops.slave_reset(adap->fe_adap[0].fe);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2017-16646"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1422,
            "cve_id": "CVE-2017-16647",
            "code_snippet": "static int asix_resume(struct usb_interface *intf)\n{\n\tstruct usbnet *dev = usb_get_intfdata(intf);\n\tstruct asix_common_private *priv = dev->driver_priv;\n\n\tif (priv->resume)\n\t\tpriv->resume(dev);\n\n\treturn usbnet_resume(intf);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int asix_suspend(struct usb_interface *intf, pm_message_t message)\n{\n\tstruct usbnet *dev = usb_get_intfdata(intf);\n\tstruct asix_common_private *priv = dev->driver_priv;\n\n\tif (priv->suspend)\n\t\tpriv->suspend(dev);\n\n\treturn usbnet_suspend(intf, message);\n}",
                        "code_after_change": "static int asix_suspend(struct usb_interface *intf, pm_message_t message)\n{\n\tstruct usbnet *dev = usb_get_intfdata(intf);\n\tstruct asix_common_private *priv = dev->driver_priv;\n\n\tif (priv && priv->suspend)\n\t\tpriv->suspend(dev);\n\n\treturn usbnet_suspend(intf, message);\n}",
                        "cve_id": "CVE-2017-16647"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1473,
            "cve_id": "CVE-2017-18079",
            "code_snippet": "static void i8042_stop(struct serio *serio)\n{\n\tstruct i8042_port *port = serio->port_data;\n\n\tport->exists = false;\n\n\t/*\n\t * We synchronize with both AUX and KBD IRQs because there is\n\t * a (very unlikely) chance that AUX IRQ is raised for KBD port\n\t * and vice versa.\n\t */\n\tsynchronize_irq(I8042_AUX_IRQ);\n\tsynchronize_irq(I8042_KBD_IRQ);\n\tport->serio = NULL;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static irqreturn_t i8042_interrupt(int irq, void *dev_id)\n{\n\tstruct i8042_port *port;\n\tstruct serio *serio;\n\tunsigned long flags;\n\tunsigned char str, data;\n\tunsigned int dfl;\n\tunsigned int port_no;\n\tbool filtered;\n\tint ret = 1;\n\n\tspin_lock_irqsave(&i8042_lock, flags);\n\n\tstr = i8042_read_status();\n\tif (unlikely(~str & I8042_STR_OBF)) {\n\t\tspin_unlock_irqrestore(&i8042_lock, flags);\n\t\tif (irq)\n\t\t\tdbg(\"Interrupt %d, without any data\\n\", irq);\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tdata = i8042_read_data();\n\n\tif (i8042_mux_present && (str & I8042_STR_AUXDATA)) {\n\t\tstatic unsigned long last_transmit;\n\t\tstatic unsigned char last_str;\n\n\t\tdfl = 0;\n\t\tif (str & I8042_STR_MUXERR) {\n\t\t\tdbg(\"MUX error, status is %02x, data is %02x\\n\",\n\t\t\t    str, data);\n/*\n * When MUXERR condition is signalled the data register can only contain\n * 0xfd, 0xfe or 0xff if implementation follows the spec. Unfortunately\n * it is not always the case. Some KBCs also report 0xfc when there is\n * nothing connected to the port while others sometimes get confused which\n * port the data came from and signal error leaving the data intact. They\n * _do not_ revert to legacy mode (actually I've never seen KBC reverting\n * to legacy mode yet, when we see one we'll add proper handling).\n * Anyway, we process 0xfc, 0xfd, 0xfe and 0xff as timeouts, and for the\n * rest assume that the data came from the same serio last byte\n * was transmitted (if transmission happened not too long ago).\n */\n\n\t\t\tswitch (data) {\n\t\t\t\tdefault:\n\t\t\t\t\tif (time_before(jiffies, last_transmit + HZ/10)) {\n\t\t\t\t\t\tstr = last_str;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\t/* fall through - report timeout */\n\t\t\t\tcase 0xfc:\n\t\t\t\tcase 0xfd:\n\t\t\t\tcase 0xfe: dfl = SERIO_TIMEOUT; data = 0xfe; break;\n\t\t\t\tcase 0xff: dfl = SERIO_PARITY;  data = 0xfe; break;\n\t\t\t}\n\t\t}\n\n\t\tport_no = I8042_MUX_PORT_NO + ((str >> 6) & 3);\n\t\tlast_str = str;\n\t\tlast_transmit = jiffies;\n\t} else {\n\n\t\tdfl = ((str & I8042_STR_PARITY) ? SERIO_PARITY : 0) |\n\t\t      ((str & I8042_STR_TIMEOUT && !i8042_notimeout) ? SERIO_TIMEOUT : 0);\n\n\t\tport_no = (str & I8042_STR_AUXDATA) ?\n\t\t\t\tI8042_AUX_PORT_NO : I8042_KBD_PORT_NO;\n\t}\n\n\tport = &i8042_ports[port_no];\n\tserio = port->exists ? port->serio : NULL;\n\n\tfilter_dbg(port->driver_bound, data, \"<- i8042 (interrupt, %d, %d%s%s)\\n\",\n\t\t   port_no, irq,\n\t\t   dfl & SERIO_PARITY ? \", bad parity\" : \"\",\n\t\t   dfl & SERIO_TIMEOUT ? \", timeout\" : \"\");\n\n\tfiltered = i8042_filter(data, str, serio);\n\n\tspin_unlock_irqrestore(&i8042_lock, flags);\n\n\tif (likely(port->exists && !filtered))\n\t\tserio_interrupt(serio, data, dfl);\n\n out:\n\treturn IRQ_RETVAL(ret);\n}",
                        "code_after_change": "static irqreturn_t i8042_interrupt(int irq, void *dev_id)\n{\n\tstruct i8042_port *port;\n\tstruct serio *serio;\n\tunsigned long flags;\n\tunsigned char str, data;\n\tunsigned int dfl;\n\tunsigned int port_no;\n\tbool filtered;\n\tint ret = 1;\n\n\tspin_lock_irqsave(&i8042_lock, flags);\n\n\tstr = i8042_read_status();\n\tif (unlikely(~str & I8042_STR_OBF)) {\n\t\tspin_unlock_irqrestore(&i8042_lock, flags);\n\t\tif (irq)\n\t\t\tdbg(\"Interrupt %d, without any data\\n\", irq);\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tdata = i8042_read_data();\n\n\tif (i8042_mux_present && (str & I8042_STR_AUXDATA)) {\n\t\tstatic unsigned long last_transmit;\n\t\tstatic unsigned char last_str;\n\n\t\tdfl = 0;\n\t\tif (str & I8042_STR_MUXERR) {\n\t\t\tdbg(\"MUX error, status is %02x, data is %02x\\n\",\n\t\t\t    str, data);\n/*\n * When MUXERR condition is signalled the data register can only contain\n * 0xfd, 0xfe or 0xff if implementation follows the spec. Unfortunately\n * it is not always the case. Some KBCs also report 0xfc when there is\n * nothing connected to the port while others sometimes get confused which\n * port the data came from and signal error leaving the data intact. They\n * _do not_ revert to legacy mode (actually I've never seen KBC reverting\n * to legacy mode yet, when we see one we'll add proper handling).\n * Anyway, we process 0xfc, 0xfd, 0xfe and 0xff as timeouts, and for the\n * rest assume that the data came from the same serio last byte\n * was transmitted (if transmission happened not too long ago).\n */\n\n\t\t\tswitch (data) {\n\t\t\t\tdefault:\n\t\t\t\t\tif (time_before(jiffies, last_transmit + HZ/10)) {\n\t\t\t\t\t\tstr = last_str;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\t/* fall through - report timeout */\n\t\t\t\tcase 0xfc:\n\t\t\t\tcase 0xfd:\n\t\t\t\tcase 0xfe: dfl = SERIO_TIMEOUT; data = 0xfe; break;\n\t\t\t\tcase 0xff: dfl = SERIO_PARITY;  data = 0xfe; break;\n\t\t\t}\n\t\t}\n\n\t\tport_no = I8042_MUX_PORT_NO + ((str >> 6) & 3);\n\t\tlast_str = str;\n\t\tlast_transmit = jiffies;\n\t} else {\n\n\t\tdfl = ((str & I8042_STR_PARITY) ? SERIO_PARITY : 0) |\n\t\t      ((str & I8042_STR_TIMEOUT && !i8042_notimeout) ? SERIO_TIMEOUT : 0);\n\n\t\tport_no = (str & I8042_STR_AUXDATA) ?\n\t\t\t\tI8042_AUX_PORT_NO : I8042_KBD_PORT_NO;\n\t}\n\n\tport = &i8042_ports[port_no];\n\tserio = port->exists ? port->serio : NULL;\n\n\tfilter_dbg(port->driver_bound, data, \"<- i8042 (interrupt, %d, %d%s%s)\\n\",\n\t\t   port_no, irq,\n\t\t   dfl & SERIO_PARITY ? \", bad parity\" : \"\",\n\t\t   dfl & SERIO_TIMEOUT ? \", timeout\" : \"\");\n\n\tfiltered = i8042_filter(data, str, serio);\n\n\tspin_unlock_irqrestore(&i8042_lock, flags);\n\n\tif (likely(serio && !filtered))\n\t\tserio_interrupt(serio, data, dfl);\n\n out:\n\treturn IRQ_RETVAL(ret);\n}",
                        "cve_id": "CVE-2017-18079"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1484,
            "cve_id": "CVE-2017-18216",
            "code_snippet": "static ssize_t o2nm_node_num_store(struct config_item *item, const char *page,\n\t\t\t\t   size_t count)\n{\n\tstruct o2nm_node *node = to_o2nm_node(item);\n\tstruct o2nm_cluster *cluster = to_o2nm_cluster_from_node(node);\n\tunsigned long tmp;\n\tchar *p = (char *)page;\n\tint ret = 0;\n\n\ttmp = simple_strtoul(p, &p, 0);\n\tif (!p || (*p && (*p != '\\n')))\n\t\treturn -EINVAL;\n\n\tif (tmp >= O2NM_MAX_NODES)\n\t\treturn -ERANGE;\n\n\t/* once we're in the cl_nodes tree networking can look us up by\n\t * node number and try to use our address and port attributes\n\t * to connect to this node.. make sure that they've been set\n\t * before writing the node attribute? */\n\tif (!test_bit(O2NM_NODE_ATTR_ADDRESS, &node->nd_set_attributes) ||\n\t    !test_bit(O2NM_NODE_ATTR_PORT, &node->nd_set_attributes))\n\t\treturn -EINVAL; /* XXX */\n\n\twrite_lock(&cluster->cl_nodes_lock);\n\tif (cluster->cl_nodes[tmp])\n\t\tret = -EEXIST;\n\telse if (test_and_set_bit(O2NM_NODE_ATTR_NUM,\n\t\t\t&node->nd_set_attributes))\n\t\tret = -EBUSY;\n\telse  {\n\t\tcluster->cl_nodes[tmp] = node;\n\t\tnode->nd_num = tmp;\n\t\tset_bit(tmp, cluster->cl_nodes_bitmap);\n\t}\n\twrite_unlock(&cluster->cl_nodes_lock);\n\tif (ret)\n\t\treturn ret;\n\n\treturn count;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static ssize_t o2nm_node_local_store(struct config_item *item, const char *page,\n\t\t\t\t     size_t count)\n{\n\tstruct o2nm_node *node = to_o2nm_node(item);\n\tstruct o2nm_cluster *cluster = to_o2nm_cluster_from_node(node);\n\tunsigned long tmp;\n\tchar *p = (char *)page;\n\tssize_t ret;\n\n\ttmp = simple_strtoul(p, &p, 0);\n\tif (!p || (*p && (*p != '\\n')))\n\t\treturn -EINVAL;\n\n\ttmp = !!tmp; /* boolean of whether this node wants to be local */\n\n\t/* setting local turns on networking rx for now so we require having\n\t * set everything else first */\n\tif (!test_bit(O2NM_NODE_ATTR_ADDRESS, &node->nd_set_attributes) ||\n\t    !test_bit(O2NM_NODE_ATTR_NUM, &node->nd_set_attributes) ||\n\t    !test_bit(O2NM_NODE_ATTR_PORT, &node->nd_set_attributes))\n\t\treturn -EINVAL; /* XXX */\n\n\t/* the only failure case is trying to set a new local node\n\t * when a different one is already set */\n\tif (tmp && tmp == cluster->cl_has_local &&\n\t    cluster->cl_local_node != node->nd_num)\n\t\treturn -EBUSY;\n\n\t/* bring up the rx thread if we're setting the new local node. */\n\tif (tmp && !cluster->cl_has_local) {\n\t\tret = o2net_start_listening(node);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (!tmp && cluster->cl_has_local &&\n\t    cluster->cl_local_node == node->nd_num) {\n\t\to2net_stop_listening(node);\n\t\tcluster->cl_local_node = O2NM_INVALID_NODE_NUM;\n\t}\n\n\tnode->nd_local = tmp;\n\tif (node->nd_local) {\n\t\tcluster->cl_has_local = tmp;\n\t\tcluster->cl_local_node = node->nd_num;\n\t}\n\n\treturn count;\n}",
                        "code_after_change": "static ssize_t o2nm_node_local_store(struct config_item *item, const char *page,\n\t\t\t\t     size_t count)\n{\n\tstruct o2nm_node *node = to_o2nm_node(item);\n\tstruct o2nm_cluster *cluster;\n\tunsigned long tmp;\n\tchar *p = (char *)page;\n\tssize_t ret;\n\n\ttmp = simple_strtoul(p, &p, 0);\n\tif (!p || (*p && (*p != '\\n')))\n\t\treturn -EINVAL;\n\n\ttmp = !!tmp; /* boolean of whether this node wants to be local */\n\n\t/* setting local turns on networking rx for now so we require having\n\t * set everything else first */\n\tif (!test_bit(O2NM_NODE_ATTR_ADDRESS, &node->nd_set_attributes) ||\n\t    !test_bit(O2NM_NODE_ATTR_NUM, &node->nd_set_attributes) ||\n\t    !test_bit(O2NM_NODE_ATTR_PORT, &node->nd_set_attributes))\n\t\treturn -EINVAL; /* XXX */\n\n\to2nm_lock_subsystem();\n\tcluster = to_o2nm_cluster_from_node(node);\n\tif (!cluster) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* the only failure case is trying to set a new local node\n\t * when a different one is already set */\n\tif (tmp && tmp == cluster->cl_has_local &&\n\t    cluster->cl_local_node != node->nd_num) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\t/* bring up the rx thread if we're setting the new local node. */\n\tif (tmp && !cluster->cl_has_local) {\n\t\tret = o2net_start_listening(node);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tif (!tmp && cluster->cl_has_local &&\n\t    cluster->cl_local_node == node->nd_num) {\n\t\to2net_stop_listening(node);\n\t\tcluster->cl_local_node = O2NM_INVALID_NODE_NUM;\n\t}\n\n\tnode->nd_local = tmp;\n\tif (node->nd_local) {\n\t\tcluster->cl_has_local = tmp;\n\t\tcluster->cl_local_node = node->nd_num;\n\t}\n\n\tret = count;\n\nout:\n\to2nm_unlock_subsystem();\n\treturn ret;\n}",
                        "cve_id": "CVE-2017-18216"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1507,
            "cve_id": "CVE-2017-18241",
            "code_snippet": "int build_segment_manager(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_super_block *raw_super = F2FS_RAW_SUPER(sbi);\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tstruct f2fs_sm_info *sm_info;\n\tint err;\n\n\tsm_info = kzalloc(sizeof(struct f2fs_sm_info), GFP_KERNEL);\n\tif (!sm_info)\n\t\treturn -ENOMEM;\n\n\t/* init sm info */\n\tsbi->sm_info = sm_info;\n\tsm_info->seg0_blkaddr = le32_to_cpu(raw_super->segment0_blkaddr);\n\tsm_info->main_blkaddr = le32_to_cpu(raw_super->main_blkaddr);\n\tsm_info->segment_count = le32_to_cpu(raw_super->segment_count);\n\tsm_info->reserved_segments = le32_to_cpu(ckpt->rsvd_segment_count);\n\tsm_info->ovp_segments = le32_to_cpu(ckpt->overprov_segment_count);\n\tsm_info->main_segments = le32_to_cpu(raw_super->segment_count_main);\n\tsm_info->ssa_blkaddr = le32_to_cpu(raw_super->ssa_blkaddr);\n\tsm_info->rec_prefree_segments = sm_info->main_segments *\n\t\t\t\t\tDEF_RECLAIM_PREFREE_SEGMENTS / 100;\n\tif (sm_info->rec_prefree_segments > DEF_MAX_RECLAIM_PREFREE_SEGMENTS)\n\t\tsm_info->rec_prefree_segments = DEF_MAX_RECLAIM_PREFREE_SEGMENTS;\n\n\tif (!test_opt(sbi, LFS))\n\t\tsm_info->ipu_policy = 1 << F2FS_IPU_FSYNC;\n\tsm_info->min_ipu_util = DEF_MIN_IPU_UTIL;\n\tsm_info->min_fsync_blocks = DEF_MIN_FSYNC_BLOCKS;\n\tsm_info->min_hot_blocks = DEF_MIN_HOT_BLOCKS;\n\n\tsm_info->trim_sections = DEF_BATCHED_TRIM_SECTIONS;\n\n\tINIT_LIST_HEAD(&sm_info->sit_entry_set);\n\n\tif (test_opt(sbi, FLUSH_MERGE) && !f2fs_readonly(sbi->sb)) {\n\t\terr = create_flush_cmd_control(sbi);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = create_discard_cmd_control(sbi);\n\tif (err)\n\t\treturn err;\n\n\terr = build_sit_info(sbi);\n\tif (err)\n\t\treturn err;\n\terr = build_free_segmap(sbi);\n\tif (err)\n\t\treturn err;\n\terr = build_curseg(sbi);\n\tif (err)\n\t\treturn err;\n\n\t/* reinit free segmap based on SIT */\n\tbuild_sit_entries(sbi);\n\n\tinit_free_segmap(sbi);\n\terr = build_dirty_segmap(sbi);\n\tif (err)\n\t\treturn err;\n\n\tinit_min_max_mtime(sbi);\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int f2fs_get_valid_checkpoint(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_checkpoint *cp_block;\n\tstruct f2fs_super_block *fsb = sbi->raw_super;\n\tstruct page *cp1, *cp2, *cur_page;\n\tunsigned long blk_size = sbi->blocksize;\n\tunsigned long long cp1_version = 0, cp2_version = 0;\n\tunsigned long long cp_start_blk_no;\n\tunsigned int cp_blks = 1 + __cp_payload(sbi);\n\tblock_t cp_blk_no;\n\tint i;\n\n\tsbi->ckpt = f2fs_kzalloc(sbi, array_size(blk_size, cp_blks),\n\t\t\t\t GFP_KERNEL);\n\tif (!sbi->ckpt)\n\t\treturn -ENOMEM;\n\t/*\n\t * Finding out valid cp block involves read both\n\t * sets( cp pack1 and cp pack 2)\n\t */\n\tcp_start_blk_no = le32_to_cpu(fsb->cp_blkaddr);\n\tcp1 = validate_checkpoint(sbi, cp_start_blk_no, &cp1_version);\n\n\t/* The second checkpoint pack should start at the next segment */\n\tcp_start_blk_no += ((unsigned long long)1) <<\n\t\t\t\tle32_to_cpu(fsb->log_blocks_per_seg);\n\tcp2 = validate_checkpoint(sbi, cp_start_blk_no, &cp2_version);\n\n\tif (cp1 && cp2) {\n\t\tif (ver_after(cp2_version, cp1_version))\n\t\t\tcur_page = cp2;\n\t\telse\n\t\t\tcur_page = cp1;\n\t} else if (cp1) {\n\t\tcur_page = cp1;\n\t} else if (cp2) {\n\t\tcur_page = cp2;\n\t} else {\n\t\tgoto fail_no_cp;\n\t}\n\n\tcp_block = (struct f2fs_checkpoint *)page_address(cur_page);\n\tmemcpy(sbi->ckpt, cp_block, blk_size);\n\n\t/* Sanity checking of checkpoint */\n\tif (f2fs_sanity_check_ckpt(sbi))\n\t\tgoto free_fail_no_cp;\n\n\tif (cur_page == cp1)\n\t\tsbi->cur_cp_pack = 1;\n\telse\n\t\tsbi->cur_cp_pack = 2;\n\n\tif (cp_blks <= 1)\n\t\tgoto done;\n\n\tcp_blk_no = le32_to_cpu(fsb->cp_blkaddr);\n\tif (cur_page == cp2)\n\t\tcp_blk_no += 1 << le32_to_cpu(fsb->log_blocks_per_seg);\n\n\tfor (i = 1; i < cp_blks; i++) {\n\t\tvoid *sit_bitmap_ptr;\n\t\tunsigned char *ckpt = (unsigned char *)sbi->ckpt;\n\n\t\tcur_page = f2fs_get_meta_page(sbi, cp_blk_no + i);\n\t\tif (IS_ERR(cur_page))\n\t\t\tgoto free_fail_no_cp;\n\t\tsit_bitmap_ptr = page_address(cur_page);\n\t\tmemcpy(ckpt + i * blk_size, sit_bitmap_ptr, blk_size);\n\t\tf2fs_put_page(cur_page, 1);\n\t}\ndone:\n\tf2fs_put_page(cp1, 1);\n\tf2fs_put_page(cp2, 1);\n\treturn 0;\n\nfree_fail_no_cp:\n\tf2fs_put_page(cp1, 1);\n\tf2fs_put_page(cp2, 1);\nfail_no_cp:\n\tkfree(sbi->ckpt);\n\treturn -EINVAL;\n}",
                        "code_after_change": "int f2fs_get_valid_checkpoint(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_checkpoint *cp_block;\n\tstruct f2fs_super_block *fsb = sbi->raw_super;\n\tstruct page *cp1, *cp2, *cur_page;\n\tunsigned long blk_size = sbi->blocksize;\n\tunsigned long long cp1_version = 0, cp2_version = 0;\n\tunsigned long long cp_start_blk_no;\n\tunsigned int cp_blks = 1 + __cp_payload(sbi);\n\tblock_t cp_blk_no;\n\tint i;\n\n\tsbi->ckpt = f2fs_kzalloc(sbi, array_size(blk_size, cp_blks),\n\t\t\t\t GFP_KERNEL);\n\tif (!sbi->ckpt)\n\t\treturn -ENOMEM;\n\t/*\n\t * Finding out valid cp block involves read both\n\t * sets( cp pack1 and cp pack 2)\n\t */\n\tcp_start_blk_no = le32_to_cpu(fsb->cp_blkaddr);\n\tcp1 = validate_checkpoint(sbi, cp_start_blk_no, &cp1_version);\n\n\t/* The second checkpoint pack should start at the next segment */\n\tcp_start_blk_no += ((unsigned long long)1) <<\n\t\t\t\tle32_to_cpu(fsb->log_blocks_per_seg);\n\tcp2 = validate_checkpoint(sbi, cp_start_blk_no, &cp2_version);\n\n\tif (cp1 && cp2) {\n\t\tif (ver_after(cp2_version, cp1_version))\n\t\t\tcur_page = cp2;\n\t\telse\n\t\t\tcur_page = cp1;\n\t} else if (cp1) {\n\t\tcur_page = cp1;\n\t} else if (cp2) {\n\t\tcur_page = cp2;\n\t} else {\n\t\tgoto fail_no_cp;\n\t}\n\n\tcp_block = (struct f2fs_checkpoint *)page_address(cur_page);\n\tmemcpy(sbi->ckpt, cp_block, blk_size);\n\n\tif (cur_page == cp1)\n\t\tsbi->cur_cp_pack = 1;\n\telse\n\t\tsbi->cur_cp_pack = 2;\n\n\t/* Sanity checking of checkpoint */\n\tif (f2fs_sanity_check_ckpt(sbi))\n\t\tgoto free_fail_no_cp;\n\n\tif (cp_blks <= 1)\n\t\tgoto done;\n\n\tcp_blk_no = le32_to_cpu(fsb->cp_blkaddr);\n\tif (cur_page == cp2)\n\t\tcp_blk_no += 1 << le32_to_cpu(fsb->log_blocks_per_seg);\n\n\tfor (i = 1; i < cp_blks; i++) {\n\t\tvoid *sit_bitmap_ptr;\n\t\tunsigned char *ckpt = (unsigned char *)sbi->ckpt;\n\n\t\tcur_page = f2fs_get_meta_page(sbi, cp_blk_no + i);\n\t\tif (IS_ERR(cur_page))\n\t\t\tgoto free_fail_no_cp;\n\t\tsit_bitmap_ptr = page_address(cur_page);\n\t\tmemcpy(ckpt + i * blk_size, sit_bitmap_ptr, blk_size);\n\t\tf2fs_put_page(cur_page, 1);\n\t}\ndone:\n\tf2fs_put_page(cp1, 1);\n\tf2fs_put_page(cp2, 1);\n\treturn 0;\n\nfree_fail_no_cp:\n\tf2fs_put_page(cp1, 1);\n\tf2fs_put_page(cp2, 1);\nfail_no_cp:\n\tkfree(sbi->ckpt);\n\treturn -EINVAL;\n}",
                        "cve_id": "CVE-2018-14614"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int create_flush_cmd_control(struct f2fs_sb_info *sbi)\n{\n\tdev_t dev = sbi->sb->s_bdev->bd_dev;\n\tstruct flush_cmd_control *fcc;\n\tint err = 0;\n\n\tif (SM_I(sbi)->fcc_info) {\n\t\tfcc = SM_I(sbi)->fcc_info;\n\t\tgoto init_thread;\n\t}\n\n\tfcc = kzalloc(sizeof(struct flush_cmd_control), GFP_KERNEL);\n\tif (!fcc)\n\t\treturn -ENOMEM;\n\tatomic_set(&fcc->issued_flush, 0);\n\tatomic_set(&fcc->issing_flush, 0);\n\tinit_waitqueue_head(&fcc->flush_wait_queue);\n\tinit_llist_head(&fcc->issue_list);\n\tSM_I(sbi)->fcc_info = fcc;\ninit_thread:\n\tfcc->f2fs_issue_flush = kthread_run(issue_flush_thread, sbi,\n\t\t\t\t\"f2fs_flush-%u:%u\", MAJOR(dev), MINOR(dev));\n\tif (IS_ERR(fcc->f2fs_issue_flush)) {\n\t\terr = PTR_ERR(fcc->f2fs_issue_flush);\n\t\tkfree(fcc);\n\t\tSM_I(sbi)->fcc_info = NULL;\n\t\treturn err;\n\t}\n\n\treturn err;\n}",
                        "code_after_change": "int create_flush_cmd_control(struct f2fs_sb_info *sbi)\n{\n\tdev_t dev = sbi->sb->s_bdev->bd_dev;\n\tstruct flush_cmd_control *fcc;\n\tint err = 0;\n\n\tif (SM_I(sbi)->fcc_info) {\n\t\tfcc = SM_I(sbi)->fcc_info;\n\t\tgoto init_thread;\n\t}\n\n\tfcc = kzalloc(sizeof(struct flush_cmd_control), GFP_KERNEL);\n\tif (!fcc)\n\t\treturn -ENOMEM;\n\tatomic_set(&fcc->issued_flush, 0);\n\tatomic_set(&fcc->issing_flush, 0);\n\tinit_waitqueue_head(&fcc->flush_wait_queue);\n\tinit_llist_head(&fcc->issue_list);\n\tSM_I(sbi)->fcc_info = fcc;\n\tif (!test_opt(sbi, FLUSH_MERGE))\n\t\treturn err;\n\ninit_thread:\n\tfcc->f2fs_issue_flush = kthread_run(issue_flush_thread, sbi,\n\t\t\t\t\"f2fs_flush-%u:%u\", MAJOR(dev), MINOR(dev));\n\tif (IS_ERR(fcc->f2fs_issue_flush)) {\n\t\terr = PTR_ERR(fcc->f2fs_issue_flush);\n\t\tkfree(fcc);\n\t\tSM_I(sbi)->fcc_info = NULL;\n\t\treturn err;\n\t}\n\n\treturn err;\n}",
                        "cve_id": "CVE-2017-18241"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1536,
            "cve_id": "CVE-2017-2647",
            "code_snippet": "key_ref_t key_create_or_update(key_ref_t keyring_ref,\n\t\t\t       const char *type,\n\t\t\t       const char *description,\n\t\t\t       const void *payload,\n\t\t\t       size_t plen,\n\t\t\t       key_perm_t perm,\n\t\t\t       unsigned long flags)\n{\n\tstruct keyring_index_key index_key = {\n\t\t.description\t= description,\n\t};\n\tstruct key_preparsed_payload prep;\n\tstruct assoc_array_edit *edit;\n\tconst struct cred *cred = current_cred();\n\tstruct key *keyring, *key = NULL;\n\tkey_ref_t key_ref;\n\tint ret;\n\n\t/* look up the key type to see if it's one of the registered kernel\n\t * types */\n\tindex_key.type = key_type_lookup(type);\n\tif (IS_ERR(index_key.type)) {\n\t\tkey_ref = ERR_PTR(-ENODEV);\n\t\tgoto error;\n\t}\n\n\tkey_ref = ERR_PTR(-EINVAL);\n\tif (!index_key.type->match || !index_key.type->instantiate ||\n\t    (!index_key.description && !index_key.type->preparse))\n\t\tgoto error_put_type;\n\n\tkeyring = key_ref_to_ptr(keyring_ref);\n\n\tkey_check(keyring);\n\n\tkey_ref = ERR_PTR(-ENOTDIR);\n\tif (keyring->type != &key_type_keyring)\n\t\tgoto error_put_type;\n\n\tmemset(&prep, 0, sizeof(prep));\n\tprep.data = payload;\n\tprep.datalen = plen;\n\tprep.quotalen = index_key.type->def_datalen;\n\tprep.trusted = flags & KEY_ALLOC_TRUSTED;\n\tprep.expiry = TIME_T_MAX;\n\tif (index_key.type->preparse) {\n\t\tret = index_key.type->preparse(&prep);\n\t\tif (ret < 0) {\n\t\t\tkey_ref = ERR_PTR(ret);\n\t\t\tgoto error_free_prep;\n\t\t}\n\t\tif (!index_key.description)\n\t\t\tindex_key.description = prep.description;\n\t\tkey_ref = ERR_PTR(-EINVAL);\n\t\tif (!index_key.description)\n\t\t\tgoto error_free_prep;\n\t}\n\tindex_key.desc_len = strlen(index_key.description);\n\n\tkey_ref = ERR_PTR(-EPERM);\n\tif (!prep.trusted && test_bit(KEY_FLAG_TRUSTED_ONLY, &keyring->flags))\n\t\tgoto error_free_prep;\n\tflags |= prep.trusted ? KEY_ALLOC_TRUSTED : 0;\n\n\tret = __key_link_begin(keyring, &index_key, &edit);\n\tif (ret < 0) {\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_free_prep;\n\t}\n\n\t/* if we're going to allocate a new key, we're going to have\n\t * to modify the keyring */\n\tret = key_permission(keyring_ref, KEY_NEED_WRITE);\n\tif (ret < 0) {\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_link_end;\n\t}\n\n\t/* if it's possible to update this type of key, search for an existing\n\t * key of the same type and description in the destination keyring and\n\t * update that instead if possible\n\t */\n\tif (index_key.type->update) {\n\t\tkey_ref = find_key_to_update(keyring_ref, &index_key);\n\t\tif (key_ref)\n\t\t\tgoto found_matching_key;\n\t}\n\n\t/* if the client doesn't provide, decide on the permissions we want */\n\tif (perm == KEY_PERM_UNDEF) {\n\t\tperm = KEY_POS_VIEW | KEY_POS_SEARCH | KEY_POS_LINK | KEY_POS_SETATTR;\n\t\tperm |= KEY_USR_VIEW;\n\n\t\tif (index_key.type->read)\n\t\t\tperm |= KEY_POS_READ;\n\n\t\tif (index_key.type == &key_type_keyring ||\n\t\t    index_key.type->update)\n\t\t\tperm |= KEY_POS_WRITE;\n\t}\n\n\t/* allocate a new key */\n\tkey = key_alloc(index_key.type, index_key.description,\n\t\t\tcred->fsuid, cred->fsgid, cred, perm, flags);\n\tif (IS_ERR(key)) {\n\t\tkey_ref = ERR_CAST(key);\n\t\tgoto error_link_end;\n\t}\n\n\t/* instantiate it and link it into the target keyring */\n\tret = __key_instantiate_and_link(key, &prep, keyring, NULL, &edit);\n\tif (ret < 0) {\n\t\tkey_put(key);\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_link_end;\n\t}\n\n\tkey_ref = make_key_ref(key, is_key_possessed(keyring_ref));\n\nerror_link_end:\n\t__key_link_end(keyring, &index_key, edit);\nerror_free_prep:\n\tif (index_key.type->preparse)\n\t\tindex_key.type->free_preparse(&prep);\nerror_put_type:\n\tkey_type_put(index_key.type);\nerror:\n\treturn key_ref;\n\n found_matching_key:\n\t/* we found a matching key, so we're going to try to update it\n\t * - we can drop the locks first as we have the key pinned\n\t */\n\t__key_link_end(keyring, &index_key, edit);\n\n\tkey_ref = __key_update(key_ref, &prep);\n\tgoto error_free_prep;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "key_ref_t key_create_or_update(key_ref_t keyring_ref,\n\t\t\t       const char *type,\n\t\t\t       const char *description,\n\t\t\t       const void *payload,\n\t\t\t       size_t plen,\n\t\t\t       key_perm_t perm,\n\t\t\t       unsigned long flags)\n{\n\tstruct keyring_index_key index_key = {\n\t\t.description\t= description,\n\t};\n\tstruct key_preparsed_payload prep;\n\tstruct assoc_array_edit *edit;\n\tconst struct cred *cred = current_cred();\n\tstruct key *keyring, *key = NULL;\n\tkey_ref_t key_ref;\n\tint ret;\n\tstruct key_restriction *restrict_link = NULL;\n\n\t/* look up the key type to see if it's one of the registered kernel\n\t * types */\n\tindex_key.type = key_type_lookup(type);\n\tif (IS_ERR(index_key.type)) {\n\t\tkey_ref = ERR_PTR(-ENODEV);\n\t\tgoto error;\n\t}\n\n\tkey_ref = ERR_PTR(-EINVAL);\n\tif (!index_key.type->instantiate ||\n\t    (!index_key.description && !index_key.type->preparse))\n\t\tgoto error_put_type;\n\n\tkeyring = key_ref_to_ptr(keyring_ref);\n\n\tkey_check(keyring);\n\n\tkey_ref = ERR_PTR(-EPERM);\n\tif (!(flags & KEY_ALLOC_BYPASS_RESTRICTION))\n\t\trestrict_link = keyring->restrict_link;\n\n\tkey_ref = ERR_PTR(-ENOTDIR);\n\tif (keyring->type != &key_type_keyring)\n\t\tgoto error_put_type;\n\n\tmemset(&prep, 0, sizeof(prep));\n\tprep.data = payload;\n\tprep.datalen = plen;\n\tprep.quotalen = index_key.type->def_datalen;\n\tprep.expiry = TIME_T_MAX;\n\tif (index_key.type->preparse) {\n\t\tret = index_key.type->preparse(&prep);\n\t\tif (ret < 0) {\n\t\t\tkey_ref = ERR_PTR(ret);\n\t\t\tgoto error_free_prep;\n\t\t}\n\t\tif (!index_key.description)\n\t\t\tindex_key.description = prep.description;\n\t\tkey_ref = ERR_PTR(-EINVAL);\n\t\tif (!index_key.description)\n\t\t\tgoto error_free_prep;\n\t}\n\tindex_key.desc_len = strlen(index_key.description);\n\n\tret = __key_link_begin(keyring, &index_key, &edit);\n\tif (ret < 0) {\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_free_prep;\n\t}\n\n\tif (restrict_link && restrict_link->check) {\n\t\tret = restrict_link->check(keyring, index_key.type,\n\t\t\t\t\t   &prep.payload, restrict_link->key);\n\t\tif (ret < 0) {\n\t\t\tkey_ref = ERR_PTR(ret);\n\t\t\tgoto error_link_end;\n\t\t}\n\t}\n\n\t/* if we're going to allocate a new key, we're going to have\n\t * to modify the keyring */\n\tret = key_permission(keyring_ref, KEY_NEED_WRITE);\n\tif (ret < 0) {\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_link_end;\n\t}\n\n\t/* if it's possible to update this type of key, search for an existing\n\t * key of the same type and description in the destination keyring and\n\t * update that instead if possible\n\t */\n\tif (index_key.type->update) {\n\t\tkey_ref = find_key_to_update(keyring_ref, &index_key);\n\t\tif (key_ref)\n\t\t\tgoto found_matching_key;\n\t}\n\n\t/* if the client doesn't provide, decide on the permissions we want */\n\tif (perm == KEY_PERM_UNDEF) {\n\t\tperm = KEY_POS_VIEW | KEY_POS_SEARCH | KEY_POS_LINK | KEY_POS_SETATTR;\n\t\tperm |= KEY_USR_VIEW;\n\n\t\tif (index_key.type->read)\n\t\t\tperm |= KEY_POS_READ;\n\n\t\tif (index_key.type == &key_type_keyring ||\n\t\t    index_key.type->update)\n\t\t\tperm |= KEY_POS_WRITE;\n\t}\n\n\t/* allocate a new key */\n\tkey = key_alloc(index_key.type, index_key.description,\n\t\t\tcred->fsuid, cred->fsgid, cred, perm, flags, NULL);\n\tif (IS_ERR(key)) {\n\t\tkey_ref = ERR_CAST(key);\n\t\tgoto error_link_end;\n\t}\n\n\t/* instantiate it and link it into the target keyring */\n\tret = __key_instantiate_and_link(key, &prep, keyring, NULL, &edit);\n\tif (ret < 0) {\n\t\tkey_put(key);\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_link_end;\n\t}\n\n\tkey_ref = make_key_ref(key, is_key_possessed(keyring_ref));\n\nerror_link_end:\n\t__key_link_end(keyring, &index_key, edit);\nerror_free_prep:\n\tif (index_key.type->preparse)\n\t\tindex_key.type->free_preparse(&prep);\nerror_put_type:\n\tkey_type_put(index_key.type);\nerror:\n\treturn key_ref;\n\n found_matching_key:\n\t/* we found a matching key, so we're going to try to update it\n\t * - we can drop the locks first as we have the key pinned\n\t */\n\t__key_link_end(keyring, &index_key, edit);\n\n\tkey_ref = __key_update(key_ref, &prep);\n\tgoto error_free_prep;\n}",
                        "code_after_change": "key_ref_t key_create_or_update(key_ref_t keyring_ref,\n\t\t\t       const char *type,\n\t\t\t       const char *description,\n\t\t\t       const void *payload,\n\t\t\t       size_t plen,\n\t\t\t       key_perm_t perm,\n\t\t\t       unsigned long flags)\n{\n\tstruct keyring_index_key index_key = {\n\t\t.description\t= description,\n\t};\n\tstruct key_preparsed_payload prep;\n\tstruct assoc_array_edit *edit;\n\tconst struct cred *cred = current_cred();\n\tstruct key *keyring, *key = NULL;\n\tkey_ref_t key_ref;\n\tint ret;\n\tstruct key_restriction *restrict_link = NULL;\n\n\t/* look up the key type to see if it's one of the registered kernel\n\t * types */\n\tindex_key.type = key_type_lookup(type);\n\tif (IS_ERR(index_key.type)) {\n\t\tkey_ref = ERR_PTR(-ENODEV);\n\t\tgoto error;\n\t}\n\n\tkey_ref = ERR_PTR(-EINVAL);\n\tif (!index_key.type->instantiate ||\n\t    (!index_key.description && !index_key.type->preparse))\n\t\tgoto error_put_type;\n\n\tkeyring = key_ref_to_ptr(keyring_ref);\n\n\tkey_check(keyring);\n\n\tkey_ref = ERR_PTR(-EPERM);\n\tif (!(flags & KEY_ALLOC_BYPASS_RESTRICTION))\n\t\trestrict_link = keyring->restrict_link;\n\n\tkey_ref = ERR_PTR(-ENOTDIR);\n\tif (keyring->type != &key_type_keyring)\n\t\tgoto error_put_type;\n\n\tmemset(&prep, 0, sizeof(prep));\n\tprep.data = payload;\n\tprep.datalen = plen;\n\tprep.quotalen = index_key.type->def_datalen;\n\tprep.expiry = TIME_T_MAX;\n\tif (index_key.type->preparse) {\n\t\tret = index_key.type->preparse(&prep);\n\t\tif (ret < 0) {\n\t\t\tkey_ref = ERR_PTR(ret);\n\t\t\tgoto error_free_prep;\n\t\t}\n\t\tif (!index_key.description)\n\t\t\tindex_key.description = prep.description;\n\t\tkey_ref = ERR_PTR(-EINVAL);\n\t\tif (!index_key.description)\n\t\t\tgoto error_free_prep;\n\t}\n\tindex_key.desc_len = strlen(index_key.description);\n\n\tret = __key_link_begin(keyring, &index_key, &edit);\n\tif (ret < 0) {\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_free_prep;\n\t}\n\n\tif (restrict_link && restrict_link->check) {\n\t\tret = restrict_link->check(keyring, index_key.type,\n\t\t\t\t\t   &prep.payload, restrict_link->key);\n\t\tif (ret < 0) {\n\t\t\tkey_ref = ERR_PTR(ret);\n\t\t\tgoto error_link_end;\n\t\t}\n\t}\n\n\t/* if we're going to allocate a new key, we're going to have\n\t * to modify the keyring */\n\tret = key_permission(keyring_ref, KEY_NEED_WRITE);\n\tif (ret < 0) {\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_link_end;\n\t}\n\n\t/* if it's possible to update this type of key, search for an existing\n\t * key of the same type and description in the destination keyring and\n\t * update that instead if possible\n\t */\n\tif (index_key.type->update) {\n\t\tkey_ref = find_key_to_update(keyring_ref, &index_key);\n\t\tif (key_ref)\n\t\t\tgoto found_matching_key;\n\t}\n\n\t/* if the client doesn't provide, decide on the permissions we want */\n\tif (perm == KEY_PERM_UNDEF) {\n\t\tperm = KEY_POS_VIEW | KEY_POS_SEARCH | KEY_POS_LINK | KEY_POS_SETATTR;\n\t\tperm |= KEY_USR_VIEW;\n\n\t\tif (index_key.type->read)\n\t\t\tperm |= KEY_POS_READ;\n\n\t\tif (index_key.type == &key_type_keyring ||\n\t\t    index_key.type->update)\n\t\t\tperm |= KEY_POS_WRITE;\n\t}\n\n\t/* allocate a new key */\n\tkey = key_alloc(index_key.type, index_key.description,\n\t\t\tcred->fsuid, cred->fsgid, cred, perm, flags, NULL);\n\tif (IS_ERR(key)) {\n\t\tkey_ref = ERR_CAST(key);\n\t\tgoto error_link_end;\n\t}\n\n\t/* instantiate it and link it into the target keyring */\n\tret = __key_instantiate_and_link(key, &prep, keyring, NULL, &edit);\n\tif (ret < 0) {\n\t\tkey_put(key);\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_link_end;\n\t}\n\n\tkey_ref = make_key_ref(key, is_key_possessed(keyring_ref));\n\nerror_link_end:\n\t__key_link_end(keyring, &index_key, edit);\nerror_free_prep:\n\tif (index_key.type->preparse)\n\t\tindex_key.type->free_preparse(&prep);\nerror_put_type:\n\tkey_type_put(index_key.type);\nerror:\n\treturn key_ref;\n\n found_matching_key:\n\t/* we found a matching key, so we're going to try to update it\n\t * - we can drop the locks first as we have the key pinned\n\t */\n\t__key_link_end(keyring, &index_key, edit);\n\n\tkey = key_ref_to_ptr(key_ref);\n\tif (test_bit(KEY_FLAG_USER_CONSTRUCT, &key->flags)) {\n\t\tret = wait_for_key_construction(key, true);\n\t\tif (ret < 0) {\n\t\t\tkey_ref_put(key_ref);\n\t\t\tkey_ref = ERR_PTR(ret);\n\t\t\tgoto error_free_prep;\n\t\t}\n\t}\n\n\tkey_ref = __key_update(key_ref, &prep);\n\tgoto error_free_prep;\n}",
                        "cve_id": "CVE-2017-15299"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1537,
            "cve_id": "CVE-2017-2647",
            "code_snippet": "key_ref_t keyring_search(key_ref_t keyring,\n\t\t\t struct key_type *type,\n\t\t\t const char *description)\n{\n\tstruct keyring_search_context ctx = {\n\t\t.index_key.type\t\t= type,\n\t\t.index_key.description\t= description,\n\t\t.cred\t\t\t= current_cred(),\n\t\t.match_data.cmp\t\t= type->match,\n\t\t.match_data.raw_data\t= description,\n\t\t.match_data.lookup_type\t= KEYRING_SEARCH_LOOKUP_DIRECT,\n\t\t.flags\t\t\t= KEYRING_SEARCH_DO_STATE_CHECK,\n\t};\n\tkey_ref_t key;\n\tint ret;\n\n\tif (!ctx.match_data.cmp)\n\t\treturn ERR_PTR(-ENOKEY);\n\n\tif (type->match_preparse) {\n\t\tret = type->match_preparse(&ctx.match_data);\n\t\tif (ret < 0)\n\t\t\treturn ERR_PTR(ret);\n\t}\n\n\tkey = keyring_search_aux(keyring, &ctx);\n\n\tif (type->match_free)\n\t\ttype->match_free(&ctx.match_data);\n\treturn key;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "struct key *request_key_and_link(struct key_type *type,\n\t\t\t\t const char *description,\n\t\t\t\t const void *callout_info,\n\t\t\t\t size_t callout_len,\n\t\t\t\t void *aux,\n\t\t\t\t struct key *dest_keyring,\n\t\t\t\t unsigned long flags)\n{\n\tstruct keyring_search_context ctx = {\n\t\t.index_key.type\t\t= type,\n\t\t.index_key.description\t= description,\n\t\t.cred\t\t\t= current_cred(),\n\t\t.match_data.cmp\t\t= type->match,\n\t\t.match_data.raw_data\t= description,\n\t\t.match_data.lookup_type\t= KEYRING_SEARCH_LOOKUP_DIRECT,\n\t};\n\tstruct key *key;\n\tkey_ref_t key_ref;\n\tint ret;\n\n\tkenter(\"%s,%s,%p,%zu,%p,%p,%lx\",\n\t       ctx.index_key.type->name, ctx.index_key.description,\n\t       callout_info, callout_len, aux, dest_keyring, flags);\n\n\tif (type->match_preparse) {\n\t\tret = type->match_preparse(&ctx.match_data);\n\t\tif (ret < 0) {\n\t\t\tkey = ERR_PTR(ret);\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\t/* search all the process keyrings for a key */\n\tkey_ref = search_process_keyrings(&ctx);\n\n\tif (!IS_ERR(key_ref)) {\n\t\tkey = key_ref_to_ptr(key_ref);\n\t\tif (dest_keyring) {\n\t\t\tconstruct_get_dest_keyring(&dest_keyring);\n\t\t\tret = key_link(dest_keyring, key);\n\t\t\tkey_put(dest_keyring);\n\t\t\tif (ret < 0) {\n\t\t\t\tkey_put(key);\n\t\t\t\tkey = ERR_PTR(ret);\n\t\t\t\tgoto error_free;\n\t\t\t}\n\t\t}\n\t} else if (PTR_ERR(key_ref) != -EAGAIN) {\n\t\tkey = ERR_CAST(key_ref);\n\t} else  {\n\t\t/* the search failed, but the keyrings were searchable, so we\n\t\t * should consult userspace if we can */\n\t\tkey = ERR_PTR(-ENOKEY);\n\t\tif (!callout_info)\n\t\t\tgoto error_free;\n\n\t\tkey = construct_key_and_link(&ctx, callout_info, callout_len,\n\t\t\t\t\t     aux, dest_keyring, flags);\n\t}\n\nerror_free:\n\tif (type->match_free)\n\t\ttype->match_free(&ctx.match_data);\nerror:\n\tkleave(\" = %p\", key);\n\treturn key;\n}",
                        "code_after_change": "struct key *request_key_and_link(struct key_type *type,\n\t\t\t\t const char *description,\n\t\t\t\t const void *callout_info,\n\t\t\t\t size_t callout_len,\n\t\t\t\t void *aux,\n\t\t\t\t struct key *dest_keyring,\n\t\t\t\t unsigned long flags)\n{\n\tstruct keyring_search_context ctx = {\n\t\t.index_key.type\t\t= type,\n\t\t.index_key.description\t= description,\n\t\t.cred\t\t\t= current_cred(),\n\t\t.match_data.cmp\t\t= key_default_cmp,\n\t\t.match_data.raw_data\t= description,\n\t\t.match_data.lookup_type\t= KEYRING_SEARCH_LOOKUP_DIRECT,\n\t};\n\tstruct key *key;\n\tkey_ref_t key_ref;\n\tint ret;\n\n\tkenter(\"%s,%s,%p,%zu,%p,%p,%lx\",\n\t       ctx.index_key.type->name, ctx.index_key.description,\n\t       callout_info, callout_len, aux, dest_keyring, flags);\n\n\tif (type->match_preparse) {\n\t\tret = type->match_preparse(&ctx.match_data);\n\t\tif (ret < 0) {\n\t\t\tkey = ERR_PTR(ret);\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\t/* search all the process keyrings for a key */\n\tkey_ref = search_process_keyrings(&ctx);\n\n\tif (!IS_ERR(key_ref)) {\n\t\tkey = key_ref_to_ptr(key_ref);\n\t\tif (dest_keyring) {\n\t\t\tconstruct_get_dest_keyring(&dest_keyring);\n\t\t\tret = key_link(dest_keyring, key);\n\t\t\tkey_put(dest_keyring);\n\t\t\tif (ret < 0) {\n\t\t\t\tkey_put(key);\n\t\t\t\tkey = ERR_PTR(ret);\n\t\t\t\tgoto error_free;\n\t\t\t}\n\t\t}\n\t} else if (PTR_ERR(key_ref) != -EAGAIN) {\n\t\tkey = ERR_CAST(key_ref);\n\t} else  {\n\t\t/* the search failed, but the keyrings were searchable, so we\n\t\t * should consult userspace if we can */\n\t\tkey = ERR_PTR(-ENOKEY);\n\t\tif (!callout_info)\n\t\t\tgoto error_free;\n\n\t\tkey = construct_key_and_link(&ctx, callout_info, callout_len,\n\t\t\t\t\t     aux, dest_keyring, flags);\n\t}\n\nerror_free:\n\tif (type->match_free)\n\t\ttype->match_free(&ctx.match_data);\nerror:\n\tkleave(\" = %p\", key);\n\treturn key;\n}",
                        "cve_id": "CVE-2017-2647"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "key_ref_t key_create_or_update(key_ref_t keyring_ref,\n\t\t\t       const char *type,\n\t\t\t       const char *description,\n\t\t\t       const void *payload,\n\t\t\t       size_t plen,\n\t\t\t       key_perm_t perm,\n\t\t\t       unsigned long flags)\n{\n\tstruct keyring_index_key index_key = {\n\t\t.description\t= description,\n\t};\n\tstruct key_preparsed_payload prep;\n\tstruct assoc_array_edit *edit;\n\tconst struct cred *cred = current_cred();\n\tstruct key *keyring, *key = NULL;\n\tkey_ref_t key_ref;\n\tint ret;\n\tstruct key_restriction *restrict_link = NULL;\n\n\t/* look up the key type to see if it's one of the registered kernel\n\t * types */\n\tindex_key.type = key_type_lookup(type);\n\tif (IS_ERR(index_key.type)) {\n\t\tkey_ref = ERR_PTR(-ENODEV);\n\t\tgoto error;\n\t}\n\n\tkey_ref = ERR_PTR(-EINVAL);\n\tif (!index_key.type->instantiate ||\n\t    (!index_key.description && !index_key.type->preparse))\n\t\tgoto error_put_type;\n\n\tkeyring = key_ref_to_ptr(keyring_ref);\n\n\tkey_check(keyring);\n\n\tkey_ref = ERR_PTR(-EPERM);\n\tif (!(flags & KEY_ALLOC_BYPASS_RESTRICTION))\n\t\trestrict_link = keyring->restrict_link;\n\n\tkey_ref = ERR_PTR(-ENOTDIR);\n\tif (keyring->type != &key_type_keyring)\n\t\tgoto error_put_type;\n\n\tmemset(&prep, 0, sizeof(prep));\n\tprep.data = payload;\n\tprep.datalen = plen;\n\tprep.quotalen = index_key.type->def_datalen;\n\tprep.expiry = TIME_T_MAX;\n\tif (index_key.type->preparse) {\n\t\tret = index_key.type->preparse(&prep);\n\t\tif (ret < 0) {\n\t\t\tkey_ref = ERR_PTR(ret);\n\t\t\tgoto error_free_prep;\n\t\t}\n\t\tif (!index_key.description)\n\t\t\tindex_key.description = prep.description;\n\t\tkey_ref = ERR_PTR(-EINVAL);\n\t\tif (!index_key.description)\n\t\t\tgoto error_free_prep;\n\t}\n\tindex_key.desc_len = strlen(index_key.description);\n\n\tret = __key_link_begin(keyring, &index_key, &edit);\n\tif (ret < 0) {\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_free_prep;\n\t}\n\n\tif (restrict_link && restrict_link->check) {\n\t\tret = restrict_link->check(keyring, index_key.type,\n\t\t\t\t\t   &prep.payload, restrict_link->key);\n\t\tif (ret < 0) {\n\t\t\tkey_ref = ERR_PTR(ret);\n\t\t\tgoto error_link_end;\n\t\t}\n\t}\n\n\t/* if we're going to allocate a new key, we're going to have\n\t * to modify the keyring */\n\tret = key_permission(keyring_ref, KEY_NEED_WRITE);\n\tif (ret < 0) {\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_link_end;\n\t}\n\n\t/* if it's possible to update this type of key, search for an existing\n\t * key of the same type and description in the destination keyring and\n\t * update that instead if possible\n\t */\n\tif (index_key.type->update) {\n\t\tkey_ref = find_key_to_update(keyring_ref, &index_key);\n\t\tif (key_ref)\n\t\t\tgoto found_matching_key;\n\t}\n\n\t/* if the client doesn't provide, decide on the permissions we want */\n\tif (perm == KEY_PERM_UNDEF) {\n\t\tperm = KEY_POS_VIEW | KEY_POS_SEARCH | KEY_POS_LINK | KEY_POS_SETATTR;\n\t\tperm |= KEY_USR_VIEW;\n\n\t\tif (index_key.type->read)\n\t\t\tperm |= KEY_POS_READ;\n\n\t\tif (index_key.type == &key_type_keyring ||\n\t\t    index_key.type->update)\n\t\t\tperm |= KEY_POS_WRITE;\n\t}\n\n\t/* allocate a new key */\n\tkey = key_alloc(index_key.type, index_key.description,\n\t\t\tcred->fsuid, cred->fsgid, cred, perm, flags, NULL);\n\tif (IS_ERR(key)) {\n\t\tkey_ref = ERR_CAST(key);\n\t\tgoto error_link_end;\n\t}\n\n\t/* instantiate it and link it into the target keyring */\n\tret = __key_instantiate_and_link(key, &prep, keyring, NULL, &edit);\n\tif (ret < 0) {\n\t\tkey_put(key);\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_link_end;\n\t}\n\n\tkey_ref = make_key_ref(key, is_key_possessed(keyring_ref));\n\nerror_link_end:\n\t__key_link_end(keyring, &index_key, edit);\nerror_free_prep:\n\tif (index_key.type->preparse)\n\t\tindex_key.type->free_preparse(&prep);\nerror_put_type:\n\tkey_type_put(index_key.type);\nerror:\n\treturn key_ref;\n\n found_matching_key:\n\t/* we found a matching key, so we're going to try to update it\n\t * - we can drop the locks first as we have the key pinned\n\t */\n\t__key_link_end(keyring, &index_key, edit);\n\n\tkey_ref = __key_update(key_ref, &prep);\n\tgoto error_free_prep;\n}",
                        "code_after_change": "key_ref_t key_create_or_update(key_ref_t keyring_ref,\n\t\t\t       const char *type,\n\t\t\t       const char *description,\n\t\t\t       const void *payload,\n\t\t\t       size_t plen,\n\t\t\t       key_perm_t perm,\n\t\t\t       unsigned long flags)\n{\n\tstruct keyring_index_key index_key = {\n\t\t.description\t= description,\n\t};\n\tstruct key_preparsed_payload prep;\n\tstruct assoc_array_edit *edit;\n\tconst struct cred *cred = current_cred();\n\tstruct key *keyring, *key = NULL;\n\tkey_ref_t key_ref;\n\tint ret;\n\tstruct key_restriction *restrict_link = NULL;\n\n\t/* look up the key type to see if it's one of the registered kernel\n\t * types */\n\tindex_key.type = key_type_lookup(type);\n\tif (IS_ERR(index_key.type)) {\n\t\tkey_ref = ERR_PTR(-ENODEV);\n\t\tgoto error;\n\t}\n\n\tkey_ref = ERR_PTR(-EINVAL);\n\tif (!index_key.type->instantiate ||\n\t    (!index_key.description && !index_key.type->preparse))\n\t\tgoto error_put_type;\n\n\tkeyring = key_ref_to_ptr(keyring_ref);\n\n\tkey_check(keyring);\n\n\tkey_ref = ERR_PTR(-EPERM);\n\tif (!(flags & KEY_ALLOC_BYPASS_RESTRICTION))\n\t\trestrict_link = keyring->restrict_link;\n\n\tkey_ref = ERR_PTR(-ENOTDIR);\n\tif (keyring->type != &key_type_keyring)\n\t\tgoto error_put_type;\n\n\tmemset(&prep, 0, sizeof(prep));\n\tprep.data = payload;\n\tprep.datalen = plen;\n\tprep.quotalen = index_key.type->def_datalen;\n\tprep.expiry = TIME_T_MAX;\n\tif (index_key.type->preparse) {\n\t\tret = index_key.type->preparse(&prep);\n\t\tif (ret < 0) {\n\t\t\tkey_ref = ERR_PTR(ret);\n\t\t\tgoto error_free_prep;\n\t\t}\n\t\tif (!index_key.description)\n\t\t\tindex_key.description = prep.description;\n\t\tkey_ref = ERR_PTR(-EINVAL);\n\t\tif (!index_key.description)\n\t\t\tgoto error_free_prep;\n\t}\n\tindex_key.desc_len = strlen(index_key.description);\n\n\tret = __key_link_begin(keyring, &index_key, &edit);\n\tif (ret < 0) {\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_free_prep;\n\t}\n\n\tif (restrict_link && restrict_link->check) {\n\t\tret = restrict_link->check(keyring, index_key.type,\n\t\t\t\t\t   &prep.payload, restrict_link->key);\n\t\tif (ret < 0) {\n\t\t\tkey_ref = ERR_PTR(ret);\n\t\t\tgoto error_link_end;\n\t\t}\n\t}\n\n\t/* if we're going to allocate a new key, we're going to have\n\t * to modify the keyring */\n\tret = key_permission(keyring_ref, KEY_NEED_WRITE);\n\tif (ret < 0) {\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_link_end;\n\t}\n\n\t/* if it's possible to update this type of key, search for an existing\n\t * key of the same type and description in the destination keyring and\n\t * update that instead if possible\n\t */\n\tif (index_key.type->update) {\n\t\tkey_ref = find_key_to_update(keyring_ref, &index_key);\n\t\tif (key_ref)\n\t\t\tgoto found_matching_key;\n\t}\n\n\t/* if the client doesn't provide, decide on the permissions we want */\n\tif (perm == KEY_PERM_UNDEF) {\n\t\tperm = KEY_POS_VIEW | KEY_POS_SEARCH | KEY_POS_LINK | KEY_POS_SETATTR;\n\t\tperm |= KEY_USR_VIEW;\n\n\t\tif (index_key.type->read)\n\t\t\tperm |= KEY_POS_READ;\n\n\t\tif (index_key.type == &key_type_keyring ||\n\t\t    index_key.type->update)\n\t\t\tperm |= KEY_POS_WRITE;\n\t}\n\n\t/* allocate a new key */\n\tkey = key_alloc(index_key.type, index_key.description,\n\t\t\tcred->fsuid, cred->fsgid, cred, perm, flags, NULL);\n\tif (IS_ERR(key)) {\n\t\tkey_ref = ERR_CAST(key);\n\t\tgoto error_link_end;\n\t}\n\n\t/* instantiate it and link it into the target keyring */\n\tret = __key_instantiate_and_link(key, &prep, keyring, NULL, &edit);\n\tif (ret < 0) {\n\t\tkey_put(key);\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_link_end;\n\t}\n\n\tkey_ref = make_key_ref(key, is_key_possessed(keyring_ref));\n\nerror_link_end:\n\t__key_link_end(keyring, &index_key, edit);\nerror_free_prep:\n\tif (index_key.type->preparse)\n\t\tindex_key.type->free_preparse(&prep);\nerror_put_type:\n\tkey_type_put(index_key.type);\nerror:\n\treturn key_ref;\n\n found_matching_key:\n\t/* we found a matching key, so we're going to try to update it\n\t * - we can drop the locks first as we have the key pinned\n\t */\n\t__key_link_end(keyring, &index_key, edit);\n\n\tkey = key_ref_to_ptr(key_ref);\n\tif (test_bit(KEY_FLAG_USER_CONSTRUCT, &key->flags)) {\n\t\tret = wait_for_key_construction(key, true);\n\t\tif (ret < 0) {\n\t\t\tkey_ref_put(key_ref);\n\t\t\tkey_ref = ERR_PTR(ret);\n\t\t\tgoto error_free_prep;\n\t\t}\n\t}\n\n\tkey_ref = __key_update(key_ref, &prep);\n\tgoto error_free_prep;\n}",
                        "cve_id": "CVE-2017-15299"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1584,
            "cve_id": "CVE-2017-7374",
            "code_snippet": "int fscrypt_setup_filename(struct inode *dir, const struct qstr *iname,\n\t\t\t      int lookup, struct fscrypt_name *fname)\n{\n\tint ret = 0, bigname = 0;\n\n\tmemset(fname, 0, sizeof(struct fscrypt_name));\n\tfname->usr_fname = iname;\n\n\tif (!dir->i_sb->s_cop->is_encrypted(dir) ||\n\t\t\t\tfscrypt_is_dot_dotdot(iname)) {\n\t\tfname->disk_name.name = (unsigned char *)iname->name;\n\t\tfname->disk_name.len = iname->len;\n\t\treturn 0;\n\t}\n\tret = fscrypt_get_crypt_info(dir);\n\tif (ret && ret != -EOPNOTSUPP)\n\t\treturn ret;\n\n\tif (dir->i_crypt_info) {\n\t\tret = fscrypt_fname_alloc_buffer(dir, iname->len,\n\t\t\t\t\t\t\t&fname->crypto_buf);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = fname_encrypt(dir, iname, &fname->crypto_buf);\n\t\tif (ret)\n\t\t\tgoto errout;\n\t\tfname->disk_name.name = fname->crypto_buf.name;\n\t\tfname->disk_name.len = fname->crypto_buf.len;\n\t\treturn 0;\n\t}\n\tif (!lookup)\n\t\treturn -ENOKEY;\n\n\t/*\n\t * We don't have the key and we are doing a lookup; decode the\n\t * user-supplied name\n\t */\n\tif (iname->name[0] == '_')\n\t\tbigname = 1;\n\tif ((bigname && (iname->len != 33)) || (!bigname && (iname->len > 43)))\n\t\treturn -ENOENT;\n\n\tfname->crypto_buf.name = kmalloc(32, GFP_KERNEL);\n\tif (fname->crypto_buf.name == NULL)\n\t\treturn -ENOMEM;\n\n\tret = digest_decode(iname->name + bigname, iname->len - bigname,\n\t\t\t\tfname->crypto_buf.name);\n\tif (ret < 0) {\n\t\tret = -ENOENT;\n\t\tgoto errout;\n\t}\n\tfname->crypto_buf.len = ret;\n\tif (bigname) {\n\t\tmemcpy(&fname->hash, fname->crypto_buf.name, 4);\n\t\tmemcpy(&fname->minor_hash, fname->crypto_buf.name + 4, 4);\n\t} else {\n\t\tfname->disk_name.name = fname->crypto_buf.name;\n\t\tfname->disk_name.len = fname->crypto_buf.len;\n\t}\n\treturn 0;\n\nerrout:\n\tfscrypt_fname_free_buffer(&fname->crypto_buf);\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static struct dentry *hfsplus_lookup(struct inode *dir, struct dentry *dentry,\n\t\t\t\t     unsigned int flags)\n{\n\tstruct inode *inode = NULL;\n\tstruct hfs_find_data fd;\n\tstruct super_block *sb;\n\thfsplus_cat_entry entry;\n\tint err;\n\tu32 cnid, linkid = 0;\n\tu16 type;\n\n\tsb = dir->i_sb;\n\n\tdentry->d_fsdata = NULL;\n\terr = hfs_find_init(HFSPLUS_SB(sb)->cat_tree, &fd);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\terr = hfsplus_cat_build_key(sb, fd.search_key, dir->i_ino,\n\t\t\t&dentry->d_name);\n\tif (unlikely(err < 0))\n\t\tgoto fail;\nagain:\n\terr = hfs_brec_read(&fd, &entry, sizeof(entry));\n\tif (err) {\n\t\tif (err == -ENOENT) {\n\t\t\thfs_find_exit(&fd);\n\t\t\t/* No such entry */\n\t\t\tinode = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tgoto fail;\n\t}\n\ttype = be16_to_cpu(entry.type);\n\tif (type == HFSPLUS_FOLDER) {\n\t\tif (fd.entrylength < sizeof(struct hfsplus_cat_folder)) {\n\t\t\terr = -EIO;\n\t\t\tgoto fail;\n\t\t}\n\t\tcnid = be32_to_cpu(entry.folder.id);\n\t\tdentry->d_fsdata = (void *)(unsigned long)cnid;\n\t} else if (type == HFSPLUS_FILE) {\n\t\tif (fd.entrylength < sizeof(struct hfsplus_cat_file)) {\n\t\t\terr = -EIO;\n\t\t\tgoto fail;\n\t\t}\n\t\tcnid = be32_to_cpu(entry.file.id);\n\t\tif (entry.file.user_info.fdType ==\n\t\t\t\tcpu_to_be32(HFSP_HARDLINK_TYPE) &&\n\t\t\t\tentry.file.user_info.fdCreator ==\n\t\t\t\tcpu_to_be32(HFSP_HFSPLUS_CREATOR) &&\n\t\t\t\t(entry.file.create_date ==\n\t\t\t\t\tHFSPLUS_I(HFSPLUS_SB(sb)->hidden_dir)->\n\t\t\t\t\t\tcreate_date ||\n\t\t\t\tentry.file.create_date ==\n\t\t\t\t\tHFSPLUS_I(d_inode(sb->s_root))->\n\t\t\t\t\t\tcreate_date) &&\n\t\t\t\tHFSPLUS_SB(sb)->hidden_dir) {\n\t\t\tstruct qstr str;\n\t\t\tchar name[32];\n\n\t\t\tif (dentry->d_fsdata) {\n\t\t\t\t/*\n\t\t\t\t * We found a link pointing to another link,\n\t\t\t\t * so ignore it and treat it as regular file.\n\t\t\t\t */\n\t\t\t\tcnid = (unsigned long)dentry->d_fsdata;\n\t\t\t\tlinkid = 0;\n\t\t\t} else {\n\t\t\t\tdentry->d_fsdata = (void *)(unsigned long)cnid;\n\t\t\t\tlinkid =\n\t\t\t\t\tbe32_to_cpu(entry.file.permissions.dev);\n\t\t\t\tstr.len = sprintf(name, \"iNode%d\", linkid);\n\t\t\t\tstr.name = name;\n\t\t\t\terr = hfsplus_cat_build_key(sb, fd.search_key,\n\t\t\t\t\tHFSPLUS_SB(sb)->hidden_dir->i_ino,\n\t\t\t\t\t&str);\n\t\t\t\tif (unlikely(err < 0))\n\t\t\t\t\tgoto fail;\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t} else if (!dentry->d_fsdata)\n\t\t\tdentry->d_fsdata = (void *)(unsigned long)cnid;\n\t} else {\n\t\tpr_err(\"invalid catalog entry type in lookup\\n\");\n\t\terr = -EIO;\n\t\tgoto fail;\n\t}\n\thfs_find_exit(&fd);\n\tinode = hfsplus_iget(dir->i_sb, cnid);\n\tif (IS_ERR(inode))\n\t\treturn ERR_CAST(inode);\n\tif (S_ISREG(inode->i_mode))\n\t\tHFSPLUS_I(inode)->linkid = linkid;\nout:\n\treturn d_splice_alias(inode, dentry);\nfail:\n\thfs_find_exit(&fd);\n\treturn ERR_PTR(err);\n}",
                        "code_after_change": "static int fscrypt_d_revalidate(struct dentry *dentry, unsigned int flags)\n{\n\tstruct dentry *dir;\n\tint dir_has_key, cached_with_key;\n\n\tif (flags & LOOKUP_RCU)\n\t\treturn -ECHILD;\n\n\tdir = dget_parent(dentry);\n\tif (!d_inode(dir)->i_sb->s_cop->is_encrypted(d_inode(dir))) {\n\t\tdput(dir);\n\t\treturn 0;\n\t}\n\n\t/* this should eventually be an flag in d_flags */\n\tspin_lock(&dentry->d_lock);\n\tcached_with_key = dentry->d_flags & DCACHE_ENCRYPTED_WITH_KEY;\n\tspin_unlock(&dentry->d_lock);\n\tdir_has_key = (d_inode(dir)->i_crypt_info != NULL);\n\tdput(dir);\n\n\t/*\n\t * If the dentry was cached without the key, and it is a\n\t * negative dentry, it might be a valid name.  We can't check\n\t * if the key has since been made available due to locking\n\t * reasons, so we fail the validation so ext4_lookup() can do\n\t * this check.\n\t *\n\t * We also fail the validation if the dentry was created with\n\t * the key present, but we no longer have the key, or vice versa.\n\t */\n\tif ((!cached_with_key && d_is_negative(dentry)) ||\n\t\t\t(!cached_with_key && dir_has_key) ||\n\t\t\t(cached_with_key && !dir_has_key))\n\t\treturn 0;\n\treturn 1;\n}",
                        "cve_id": "CVE-2018-14617"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1585,
            "cve_id": "CVE-2017-7374",
            "code_snippet": "int fscrypt_get_encryption_info(struct inode *inode)\n{\n\tstruct fscrypt_info *ci = inode->i_crypt_info;\n\n\tif (!ci ||\n\t\t(ci->ci_keyring_key &&\n\t\t (ci->ci_keyring_key->flags & ((1 << KEY_FLAG_INVALIDATED) |\n\t\t\t\t\t       (1 << KEY_FLAG_REVOKED) |\n\t\t\t\t\t       (1 << KEY_FLAG_DEAD)))))\n\t\treturn fscrypt_get_crypt_info(inode);\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int fscrypt_d_revalidate(struct dentry *dentry, unsigned int flags)\n{\n\tstruct dentry *dir;\n\tstruct fscrypt_info *ci;\n\tint dir_has_key, cached_with_key;\n\n\tif (flags & LOOKUP_RCU)\n\t\treturn -ECHILD;\n\n\tdir = dget_parent(dentry);\n\tif (!d_inode(dir)->i_sb->s_cop->is_encrypted(d_inode(dir))) {\n\t\tdput(dir);\n\t\treturn 0;\n\t}\n\n\tci = d_inode(dir)->i_crypt_info;\n\tif (ci && ci->ci_keyring_key &&\n\t    (ci->ci_keyring_key->flags & ((1 << KEY_FLAG_INVALIDATED) |\n\t\t\t\t\t  (1 << KEY_FLAG_REVOKED) |\n\t\t\t\t\t  (1 << KEY_FLAG_DEAD))))\n\t\tci = NULL;\n\n\t/* this should eventually be an flag in d_flags */\n\tspin_lock(&dentry->d_lock);\n\tcached_with_key = dentry->d_flags & DCACHE_ENCRYPTED_WITH_KEY;\n\tspin_unlock(&dentry->d_lock);\n\tdir_has_key = (ci != NULL);\n\tdput(dir);\n\n\t/*\n\t * If the dentry was cached without the key, and it is a\n\t * negative dentry, it might be a valid name.  We can't check\n\t * if the key has since been made available due to locking\n\t * reasons, so we fail the validation so ext4_lookup() can do\n\t * this check.\n\t *\n\t * We also fail the validation if the dentry was created with\n\t * the key present, but we no longer have the key, or vice versa.\n\t */\n\tif ((!cached_with_key && d_is_negative(dentry)) ||\n\t\t\t(!cached_with_key && dir_has_key) ||\n\t\t\t(cached_with_key && !dir_has_key))\n\t\treturn 0;\n\treturn 1;\n}",
                        "code_after_change": "static void put_crypt_info(struct fscrypt_info *ci)\n{\n\tif (!ci)\n\t\treturn;\n\n\tcrypto_free_skcipher(ci->ci_ctfm);\n\tkmem_cache_free(fscrypt_info_cachep, ci);\n}",
                        "cve_id": "CVE-2017-7374"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void put_crypt_info(struct fscrypt_info *ci)\n{\n\tif (!ci)\n\t\treturn;\n\n\tkey_put(ci->ci_keyring_key);\n\tcrypto_free_skcipher(ci->ci_ctfm);\n\tkmem_cache_free(fscrypt_info_cachep, ci);\n}",
                        "code_after_change": "bool ext4_empty_dir(struct inode *inode)\n{\n\tunsigned int offset;\n\tstruct buffer_head *bh;\n\tstruct ext4_dir_entry_2 *de;\n\tstruct super_block *sb;\n\n\tif (ext4_has_inline_data(inode)) {\n\t\tint has_inline_data = 1;\n\t\tint ret;\n\n\t\tret = empty_inline_dir(inode, &has_inline_data);\n\t\tif (has_inline_data)\n\t\t\treturn ret;\n\t}\n\n\tsb = inode->i_sb;\n\tif (inode->i_size < EXT4_DIR_REC_LEN(1) + EXT4_DIR_REC_LEN(2)) {\n\t\tEXT4_ERROR_INODE(inode, \"invalid size\");\n\t\treturn true;\n\t}\n\t/* The first directory block must not be a hole,\n\t * so treat it as DIRENT_HTREE\n\t */\n\tbh = ext4_read_dirblock(inode, 0, DIRENT_HTREE);\n\tif (IS_ERR(bh))\n\t\treturn true;\n\n\tde = (struct ext4_dir_entry_2 *) bh->b_data;\n\tif (ext4_check_dir_entry(inode, NULL, de, bh, bh->b_data, bh->b_size,\n\t\t\t\t 0) ||\n\t    le32_to_cpu(de->inode) != inode->i_ino || strcmp(\".\", de->name)) {\n\t\text4_warning_inode(inode, \"directory missing '.'\");\n\t\tbrelse(bh);\n\t\treturn true;\n\t}\n\toffset = ext4_rec_len_from_disk(de->rec_len, sb->s_blocksize);\n\tde = ext4_next_entry(de, sb->s_blocksize);\n\tif (ext4_check_dir_entry(inode, NULL, de, bh, bh->b_data, bh->b_size,\n\t\t\t\t offset) ||\n\t    le32_to_cpu(de->inode) == 0 || strcmp(\"..\", de->name)) {\n\t\text4_warning_inode(inode, \"directory missing '..'\");\n\t\tbrelse(bh);\n\t\treturn true;\n\t}\n\toffset += ext4_rec_len_from_disk(de->rec_len, sb->s_blocksize);\n\twhile (offset < inode->i_size) {\n\t\tif (!(offset & (sb->s_blocksize - 1))) {\n\t\t\tunsigned int lblock;\n\t\t\tbrelse(bh);\n\t\t\tlblock = offset >> EXT4_BLOCK_SIZE_BITS(sb);\n\t\t\tbh = ext4_read_dirblock(inode, lblock, EITHER);\n\t\t\tif (bh == NULL) {\n\t\t\t\toffset += sb->s_blocksize;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (IS_ERR(bh))\n\t\t\t\treturn true;\n\t\t}\n\t\tde = (struct ext4_dir_entry_2 *) (bh->b_data +\n\t\t\t\t\t(offset & (sb->s_blocksize - 1)));\n\t\tif (ext4_check_dir_entry(inode, NULL, de, bh,\n\t\t\t\t\t bh->b_data, bh->b_size, offset)) {\n\t\t\toffset = (offset | (sb->s_blocksize - 1)) + 1;\n\t\t\tcontinue;\n\t\t}\n\t\tif (le32_to_cpu(de->inode)) {\n\t\t\tbrelse(bh);\n\t\t\treturn false;\n\t\t}\n\t\toffset += ext4_rec_len_from_disk(de->rec_len, sb->s_blocksize);\n\t}\n\tbrelse(bh);\n\treturn true;\n}",
                        "cve_id": "CVE-2017-7374"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "bool ext4_empty_dir(struct inode *inode)\n{\n\tunsigned int offset;\n\tstruct buffer_head *bh;\n\tstruct ext4_dir_entry_2 *de, *de1;\n\tstruct super_block *sb;\n\n\tif (ext4_has_inline_data(inode)) {\n\t\tint has_inline_data = 1;\n\t\tint ret;\n\n\t\tret = empty_inline_dir(inode, &has_inline_data);\n\t\tif (has_inline_data)\n\t\t\treturn ret;\n\t}\n\n\tsb = inode->i_sb;\n\tif (inode->i_size < EXT4_DIR_REC_LEN(1) + EXT4_DIR_REC_LEN(2)) {\n\t\tEXT4_ERROR_INODE(inode, \"invalid size\");\n\t\treturn true;\n\t}\n\t/* The first directory block must not be a hole,\n\t * so treat it as DIRENT_HTREE\n\t */\n\tbh = ext4_read_dirblock(inode, 0, DIRENT_HTREE);\n\tif (IS_ERR(bh))\n\t\treturn true;\n\n\tde = (struct ext4_dir_entry_2 *) bh->b_data;\n\tde1 = ext4_next_entry(de, sb->s_blocksize);\n\tif (le32_to_cpu(de->inode) != inode->i_ino ||\n\t\t\tle32_to_cpu(de1->inode) == 0 ||\n\t\t\tstrcmp(\".\", de->name) || strcmp(\"..\", de1->name)) {\n\t\text4_warning_inode(inode, \"directory missing '.' and/or '..'\");\n\t\tbrelse(bh);\n\t\treturn true;\n\t}\n\toffset = ext4_rec_len_from_disk(de->rec_len, sb->s_blocksize) +\n\t\t ext4_rec_len_from_disk(de1->rec_len, sb->s_blocksize);\n\tde = ext4_next_entry(de1, sb->s_blocksize);\n\twhile (offset < inode->i_size) {\n\t\tif ((void *) de >= (void *) (bh->b_data+sb->s_blocksize)) {\n\t\t\tunsigned int lblock;\n\t\t\tbrelse(bh);\n\t\t\tlblock = offset >> EXT4_BLOCK_SIZE_BITS(sb);\n\t\t\tbh = ext4_read_dirblock(inode, lblock, EITHER);\n\t\t\tif (bh == NULL) {\n\t\t\t\toffset += sb->s_blocksize;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (IS_ERR(bh))\n\t\t\t\treturn true;\n\t\t\tde = (struct ext4_dir_entry_2 *) bh->b_data;\n\t\t}\n\t\tif (ext4_check_dir_entry(inode, NULL, de, bh,\n\t\t\t\t\t bh->b_data, bh->b_size, offset)) {\n\t\t\tde = (struct ext4_dir_entry_2 *)(bh->b_data +\n\t\t\t\t\t\t\t sb->s_blocksize);\n\t\t\toffset = (offset | (sb->s_blocksize - 1)) + 1;\n\t\t\tcontinue;\n\t\t}\n\t\tif (le32_to_cpu(de->inode)) {\n\t\t\tbrelse(bh);\n\t\t\treturn false;\n\t\t}\n\t\toffset += ext4_rec_len_from_disk(de->rec_len, sb->s_blocksize);\n\t\tde = ext4_next_entry(de, sb->s_blocksize);\n\t}\n\tbrelse(bh);\n\treturn true;\n}",
                        "code_after_change": "void jfs_evict_inode(struct inode *inode)\n{\n\tstruct jfs_inode_info *ji = JFS_IP(inode);\n\n\tjfs_info(\"In jfs_evict_inode, inode = 0x%p\", inode);\n\n\tif (!inode->i_nlink && !is_bad_inode(inode)) {\n\t\tdquot_initialize(inode);\n\n\t\tif (JFS_IP(inode)->fileset == FILESYSTEM_I) {\n\t\t\tstruct inode *ipimap = JFS_SBI(inode->i_sb)->ipimap;\n\t\t\ttruncate_inode_pages_final(&inode->i_data);\n\n\t\t\tif (test_cflag(COMMIT_Freewmap, inode))\n\t\t\t\tjfs_free_zero_link(inode);\n\n\t\t\tif (ipimap && JFS_IP(ipimap)->i_imap)\n\t\t\t\tdiFree(inode);\n\n\t\t\t/*\n\t\t\t * Free the inode from the quota allocation.\n\t\t\t */\n\t\t\tdquot_free_inode(inode);\n\t\t}\n\t} else {\n\t\ttruncate_inode_pages_final(&inode->i_data);\n\t}\n\tclear_inode(inode);\n\tdquot_drop(inode);\n\n\tBUG_ON(!list_empty(&ji->anon_inode_list));\n\n\tspin_lock_irq(&ji->ag_lock);\n\tif (ji->active_ag != -1) {\n\t\tstruct bmap *bmap = JFS_SBI(inode->i_sb)->bmap;\n\t\tatomic_dec(&bmap->db_active[ji->active_ag]);\n\t\tji->active_ag = -1;\n\t}\n\tspin_unlock_irq(&ji->ag_lock);\n}",
                        "cve_id": "CVE-2019-19037"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "void jfs_evict_inode(struct inode *inode)\n{\n\tstruct jfs_inode_info *ji = JFS_IP(inode);\n\n\tjfs_info(\"In jfs_evict_inode, inode = 0x%p\", inode);\n\n\tif (!inode->i_nlink && !is_bad_inode(inode)) {\n\t\tdquot_initialize(inode);\n\n\t\tif (JFS_IP(inode)->fileset == FILESYSTEM_I) {\n\t\t\ttruncate_inode_pages_final(&inode->i_data);\n\n\t\t\tif (test_cflag(COMMIT_Freewmap, inode))\n\t\t\t\tjfs_free_zero_link(inode);\n\n\t\t\tif (JFS_SBI(inode->i_sb)->ipimap)\n\t\t\t\tdiFree(inode);\n\n\t\t\t/*\n\t\t\t * Free the inode from the quota allocation.\n\t\t\t */\n\t\t\tdquot_free_inode(inode);\n\t\t}\n\t} else {\n\t\ttruncate_inode_pages_final(&inode->i_data);\n\t}\n\tclear_inode(inode);\n\tdquot_drop(inode);\n\n\tBUG_ON(!list_empty(&ji->anon_inode_list));\n\n\tspin_lock_irq(&ji->ag_lock);\n\tif (ji->active_ag != -1) {\n\t\tstruct bmap *bmap = JFS_SBI(inode->i_sb)->bmap;\n\t\tatomic_dec(&bmap->db_active[ji->active_ag]);\n\t\tji->active_ag = -1;\n\t}\n\tspin_unlock_irq(&ji->ag_lock);\n}",
                        "code_after_change": "static int gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,\n\t\tstruct gc_inode_list *gc_list, unsigned int segno, int gc_type,\n\t\tbool force_migrate)\n{\n\tstruct super_block *sb = sbi->sb;\n\tstruct f2fs_summary *entry;\n\tblock_t start_addr;\n\tint off;\n\tint phase = 0;\n\tint submitted = 0;\n\tunsigned int usable_blks_in_seg = f2fs_usable_blks_in_seg(sbi, segno);\n\n\tstart_addr = START_BLOCK(sbi, segno);\n\nnext_step:\n\tentry = sum;\n\n\tfor (off = 0; off < usable_blks_in_seg; off++, entry++) {\n\t\tstruct page *data_page;\n\t\tstruct inode *inode;\n\t\tstruct node_info dni; /* dnode info for the data */\n\t\tunsigned int ofs_in_node, nofs;\n\t\tblock_t start_bidx;\n\t\tnid_t nid = le32_to_cpu(entry->nid);\n\n\t\t/*\n\t\t * stop BG_GC if there is not enough free sections.\n\t\t * Or, stop GC if the segment becomes fully valid caused by\n\t\t * race condition along with SSR block allocation.\n\t\t */\n\t\tif ((gc_type == BG_GC && has_not_enough_free_secs(sbi, 0, 0)) ||\n\t\t\t(!force_migrate && get_valid_blocks(sbi, segno, true) ==\n\t\t\t\t\t\t\tBLKS_PER_SEC(sbi)))\n\t\t\treturn submitted;\n\n\t\tif (check_valid_map(sbi, segno, off) == 0)\n\t\t\tcontinue;\n\n\t\tif (phase == 0) {\n\t\t\tf2fs_ra_meta_pages(sbi, NAT_BLOCK_OFFSET(nid), 1,\n\t\t\t\t\t\t\tMETA_NAT, true);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (phase == 1) {\n\t\t\tf2fs_ra_node_page(sbi, nid);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Get an inode by ino with checking validity */\n\t\tif (!is_alive(sbi, entry, &dni, start_addr + off, &nofs))\n\t\t\tcontinue;\n\n\t\tif (phase == 2) {\n\t\t\tf2fs_ra_node_page(sbi, dni.ino);\n\t\t\tcontinue;\n\t\t}\n\n\t\tofs_in_node = le16_to_cpu(entry->ofs_in_node);\n\n\t\tif (phase == 3) {\n\t\t\tinode = f2fs_iget(sb, dni.ino);\n\t\t\tif (IS_ERR(inode) || is_bad_inode(inode) ||\n\t\t\t\t\tspecial_file(inode->i_mode))\n\t\t\t\tcontinue;\n\n\t\t\tif (!down_write_trylock(\n\t\t\t\t&F2FS_I(inode)->i_gc_rwsem[WRITE])) {\n\t\t\t\tiput(inode);\n\t\t\t\tsbi->skipped_gc_rwsem++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tstart_bidx = f2fs_start_bidx_of_node(nofs, inode) +\n\t\t\t\t\t\t\t\tofs_in_node;\n\n\t\t\tif (f2fs_post_read_required(inode)) {\n\t\t\t\tint err = ra_data_block(inode, start_bidx);\n\n\t\t\t\tup_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);\n\t\t\t\tif (err) {\n\t\t\t\t\tiput(inode);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tadd_gc_inode(gc_list, inode);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tdata_page = f2fs_get_read_data_page(inode,\n\t\t\t\t\t\tstart_bidx, REQ_RAHEAD, true);\n\t\t\tup_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);\n\t\t\tif (IS_ERR(data_page)) {\n\t\t\t\tiput(inode);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tf2fs_put_page(data_page, 0);\n\t\t\tadd_gc_inode(gc_list, inode);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* phase 4 */\n\t\tinode = find_gc_inode(gc_list, dni.ino);\n\t\tif (inode) {\n\t\t\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\t\t\tbool locked = false;\n\t\t\tint err;\n\n\t\t\tif (S_ISREG(inode->i_mode)) {\n\t\t\t\tif (!down_write_trylock(&fi->i_gc_rwsem[READ])) {\n\t\t\t\t\tsbi->skipped_gc_rwsem++;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tif (!down_write_trylock(\n\t\t\t\t\t\t&fi->i_gc_rwsem[WRITE])) {\n\t\t\t\t\tsbi->skipped_gc_rwsem++;\n\t\t\t\t\tup_write(&fi->i_gc_rwsem[READ]);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tlocked = true;\n\n\t\t\t\t/* wait for all inflight aio data */\n\t\t\t\tinode_dio_wait(inode);\n\t\t\t}\n\n\t\t\tstart_bidx = f2fs_start_bidx_of_node(nofs, inode)\n\t\t\t\t\t\t\t\t+ ofs_in_node;\n\t\t\tif (f2fs_post_read_required(inode))\n\t\t\t\terr = move_data_block(inode, start_bidx,\n\t\t\t\t\t\t\tgc_type, segno, off);\n\t\t\telse\n\t\t\t\terr = move_data_page(inode, start_bidx, gc_type,\n\t\t\t\t\t\t\t\tsegno, off);\n\n\t\t\tif (!err && (gc_type == FG_GC ||\n\t\t\t\t\tf2fs_post_read_required(inode)))\n\t\t\t\tsubmitted++;\n\n\t\t\tif (locked) {\n\t\t\t\tup_write(&fi->i_gc_rwsem[WRITE]);\n\t\t\t\tup_write(&fi->i_gc_rwsem[READ]);\n\t\t\t}\n\n\t\t\tstat_inc_data_blk_count(sbi, 1, gc_type);\n\t\t}\n\t}\n\n\tif (++phase < 5)\n\t\tgoto next_step;\n\n\treturn submitted;\n}",
                        "cve_id": "CVE-2022-3202"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,\n\t\tstruct gc_inode_list *gc_list, unsigned int segno, int gc_type,\n\t\tbool force_migrate)\n{\n\tstruct super_block *sb = sbi->sb;\n\tstruct f2fs_summary *entry;\n\tblock_t start_addr;\n\tint off;\n\tint phase = 0;\n\tint submitted = 0;\n\tunsigned int usable_blks_in_seg = f2fs_usable_blks_in_seg(sbi, segno);\n\n\tstart_addr = START_BLOCK(sbi, segno);\n\nnext_step:\n\tentry = sum;\n\n\tfor (off = 0; off < usable_blks_in_seg; off++, entry++) {\n\t\tstruct page *data_page;\n\t\tstruct inode *inode;\n\t\tstruct node_info dni; /* dnode info for the data */\n\t\tunsigned int ofs_in_node, nofs;\n\t\tblock_t start_bidx;\n\t\tnid_t nid = le32_to_cpu(entry->nid);\n\n\t\t/*\n\t\t * stop BG_GC if there is not enough free sections.\n\t\t * Or, stop GC if the segment becomes fully valid caused by\n\t\t * race condition along with SSR block allocation.\n\t\t */\n\t\tif ((gc_type == BG_GC && has_not_enough_free_secs(sbi, 0, 0)) ||\n\t\t\t(!force_migrate && get_valid_blocks(sbi, segno, true) ==\n\t\t\t\t\t\t\tBLKS_PER_SEC(sbi)))\n\t\t\treturn submitted;\n\n\t\tif (check_valid_map(sbi, segno, off) == 0)\n\t\t\tcontinue;\n\n\t\tif (phase == 0) {\n\t\t\tf2fs_ra_meta_pages(sbi, NAT_BLOCK_OFFSET(nid), 1,\n\t\t\t\t\t\t\tMETA_NAT, true);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (phase == 1) {\n\t\t\tf2fs_ra_node_page(sbi, nid);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Get an inode by ino with checking validity */\n\t\tif (!is_alive(sbi, entry, &dni, start_addr + off, &nofs))\n\t\t\tcontinue;\n\n\t\tif (phase == 2) {\n\t\t\tf2fs_ra_node_page(sbi, dni.ino);\n\t\t\tcontinue;\n\t\t}\n\n\t\tofs_in_node = le16_to_cpu(entry->ofs_in_node);\n\n\t\tif (phase == 3) {\n\t\t\tinode = f2fs_iget(sb, dni.ino);\n\t\t\tif (IS_ERR(inode) || is_bad_inode(inode))\n\t\t\t\tcontinue;\n\n\t\t\tif (!down_write_trylock(\n\t\t\t\t&F2FS_I(inode)->i_gc_rwsem[WRITE])) {\n\t\t\t\tiput(inode);\n\t\t\t\tsbi->skipped_gc_rwsem++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tstart_bidx = f2fs_start_bidx_of_node(nofs, inode) +\n\t\t\t\t\t\t\t\tofs_in_node;\n\n\t\t\tif (f2fs_post_read_required(inode)) {\n\t\t\t\tint err = ra_data_block(inode, start_bidx);\n\n\t\t\t\tup_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);\n\t\t\t\tif (err) {\n\t\t\t\t\tiput(inode);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tadd_gc_inode(gc_list, inode);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tdata_page = f2fs_get_read_data_page(inode,\n\t\t\t\t\t\tstart_bidx, REQ_RAHEAD, true);\n\t\t\tup_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);\n\t\t\tif (IS_ERR(data_page)) {\n\t\t\t\tiput(inode);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tf2fs_put_page(data_page, 0);\n\t\t\tadd_gc_inode(gc_list, inode);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* phase 4 */\n\t\tinode = find_gc_inode(gc_list, dni.ino);\n\t\tif (inode) {\n\t\t\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\t\t\tbool locked = false;\n\t\t\tint err;\n\n\t\t\tif (S_ISREG(inode->i_mode)) {\n\t\t\t\tif (!down_write_trylock(&fi->i_gc_rwsem[READ])) {\n\t\t\t\t\tsbi->skipped_gc_rwsem++;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tif (!down_write_trylock(\n\t\t\t\t\t\t&fi->i_gc_rwsem[WRITE])) {\n\t\t\t\t\tsbi->skipped_gc_rwsem++;\n\t\t\t\t\tup_write(&fi->i_gc_rwsem[READ]);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tlocked = true;\n\n\t\t\t\t/* wait for all inflight aio data */\n\t\t\t\tinode_dio_wait(inode);\n\t\t\t}\n\n\t\t\tstart_bidx = f2fs_start_bidx_of_node(nofs, inode)\n\t\t\t\t\t\t\t\t+ ofs_in_node;\n\t\t\tif (f2fs_post_read_required(inode))\n\t\t\t\terr = move_data_block(inode, start_bidx,\n\t\t\t\t\t\t\tgc_type, segno, off);\n\t\t\telse\n\t\t\t\terr = move_data_page(inode, start_bidx, gc_type,\n\t\t\t\t\t\t\t\tsegno, off);\n\n\t\t\tif (!err && (gc_type == FG_GC ||\n\t\t\t\t\tf2fs_post_read_required(inode)))\n\t\t\t\tsubmitted++;\n\n\t\t\tif (locked) {\n\t\t\t\tup_write(&fi->i_gc_rwsem[WRITE]);\n\t\t\t\tup_write(&fi->i_gc_rwsem[READ]);\n\t\t\t}\n\n\t\t\tstat_inc_data_blk_count(sbi, 1, gc_type);\n\t\t}\n\t}\n\n\tif (++phase < 5)\n\t\tgoto next_step;\n\n\treturn submitted;\n}",
                        "code_after_change": "struct inode *ext4_iget(struct super_block *sb, unsigned long ino)\n{\n\tstruct ext4_iloc iloc;\n\tstruct ext4_inode *raw_inode;\n\tstruct ext4_inode_info *ei;\n\tstruct inode *inode;\n\tjournal_t *journal = EXT4_SB(sb)->s_journal;\n\tlong ret;\n\tloff_t size;\n\tint block;\n\tuid_t i_uid;\n\tgid_t i_gid;\n\tprojid_t i_projid;\n\n\tinode = iget_locked(sb, ino);\n\tif (!inode)\n\t\treturn ERR_PTR(-ENOMEM);\n\tif (!(inode->i_state & I_NEW))\n\t\treturn inode;\n\n\tei = EXT4_I(inode);\n\tiloc.bh = NULL;\n\n\tret = __ext4_get_inode_loc(inode, &iloc, 0);\n\tif (ret < 0)\n\t\tgoto bad_inode;\n\traw_inode = ext4_raw_inode(&iloc);\n\n\tif ((ino == EXT4_ROOT_INO) && (raw_inode->i_links_count == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"root inode unallocated\");\n\t\tret = -EFSCORRUPTED;\n\t\tgoto bad_inode;\n\t}\n\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\tei->i_extra_isize = le16_to_cpu(raw_inode->i_extra_isize);\n\t\tif (EXT4_GOOD_OLD_INODE_SIZE + ei->i_extra_isize >\n\t\t\tEXT4_INODE_SIZE(inode->i_sb) ||\n\t\t    (ei->i_extra_isize & 3)) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"bad extra_isize %u (inode size %u)\",\n\t\t\t\t\t ei->i_extra_isize,\n\t\t\t\t\t EXT4_INODE_SIZE(inode->i_sb));\n\t\t\tret = -EFSCORRUPTED;\n\t\t\tgoto bad_inode;\n\t\t}\n\t} else\n\t\tei->i_extra_isize = 0;\n\n\t/* Precompute checksum seed for inode metadata */\n\tif (ext4_has_metadata_csum(sb)) {\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t\t__u32 csum;\n\t\t__le32 inum = cpu_to_le32(inode->i_ino);\n\t\t__le32 gen = raw_inode->i_generation;\n\t\tcsum = ext4_chksum(sbi, sbi->s_csum_seed, (__u8 *)&inum,\n\t\t\t\t   sizeof(inum));\n\t\tei->i_csum_seed = ext4_chksum(sbi, csum, (__u8 *)&gen,\n\t\t\t\t\t      sizeof(gen));\n\t}\n\n\tif (!ext4_inode_csum_verify(inode, raw_inode, ei)) {\n\t\tEXT4_ERROR_INODE(inode, \"checksum invalid\");\n\t\tret = -EFSBADCRC;\n\t\tgoto bad_inode;\n\t}\n\n\tinode->i_mode = le16_to_cpu(raw_inode->i_mode);\n\ti_uid = (uid_t)le16_to_cpu(raw_inode->i_uid_low);\n\ti_gid = (gid_t)le16_to_cpu(raw_inode->i_gid_low);\n\tif (ext4_has_feature_project(sb) &&\n\t    EXT4_INODE_SIZE(sb) > EXT4_GOOD_OLD_INODE_SIZE &&\n\t    EXT4_FITS_IN_INODE(raw_inode, ei, i_projid))\n\t\ti_projid = (projid_t)le32_to_cpu(raw_inode->i_projid);\n\telse\n\t\ti_projid = EXT4_DEF_PROJID;\n\n\tif (!(test_opt(inode->i_sb, NO_UID32))) {\n\t\ti_uid |= le16_to_cpu(raw_inode->i_uid_high) << 16;\n\t\ti_gid |= le16_to_cpu(raw_inode->i_gid_high) << 16;\n\t}\n\ti_uid_write(inode, i_uid);\n\ti_gid_write(inode, i_gid);\n\tei->i_projid = make_kprojid(&init_user_ns, i_projid);\n\tset_nlink(inode, le16_to_cpu(raw_inode->i_links_count));\n\n\text4_clear_state_flags(ei);\t/* Only relevant on 32-bit archs */\n\tei->i_inline_off = 0;\n\tei->i_dir_start_lookup = 0;\n\tei->i_dtime = le32_to_cpu(raw_inode->i_dtime);\n\t/* We now have enough fields to check if the inode was active or not.\n\t * This is needed because nfsd might try to access dead inodes\n\t * the test is that same one that e2fsck uses\n\t * NeilBrown 1999oct15\n\t */\n\tif (inode->i_nlink == 0) {\n\t\tif ((inode->i_mode == 0 ||\n\t\t     !(EXT4_SB(inode->i_sb)->s_mount_state & EXT4_ORPHAN_FS)) &&\n\t\t    ino != EXT4_BOOT_LOADER_INO) {\n\t\t\t/* this inode is deleted */\n\t\t\tret = -ESTALE;\n\t\t\tgoto bad_inode;\n\t\t}\n\t\t/* The only unlinked inodes we let through here have\n\t\t * valid i_mode and are being read by the orphan\n\t\t * recovery code: that's fine, we're about to complete\n\t\t * the process of deleting those.\n\t\t * OR it is the EXT4_BOOT_LOADER_INO which is\n\t\t * not initialized on a new filesystem. */\n\t}\n\tei->i_flags = le32_to_cpu(raw_inode->i_flags);\n\tinode->i_blocks = ext4_inode_blocks(raw_inode, ei);\n\tei->i_file_acl = le32_to_cpu(raw_inode->i_file_acl_lo);\n\tif (ext4_has_feature_64bit(sb))\n\t\tei->i_file_acl |=\n\t\t\t((__u64)le16_to_cpu(raw_inode->i_file_acl_high)) << 32;\n\tinode->i_size = ext4_isize(sb, raw_inode);\n\tif ((size = i_size_read(inode)) < 0) {\n\t\tEXT4_ERROR_INODE(inode, \"bad i_size value: %lld\", size);\n\t\tret = -EFSCORRUPTED;\n\t\tgoto bad_inode;\n\t}\n\tei->i_disksize = inode->i_size;\n#ifdef CONFIG_QUOTA\n\tei->i_reserved_quota = 0;\n#endif\n\tinode->i_generation = le32_to_cpu(raw_inode->i_generation);\n\tei->i_block_group = iloc.block_group;\n\tei->i_last_alloc_group = ~0;\n\t/*\n\t * NOTE! The in-memory inode i_data array is in little-endian order\n\t * even on big-endian machines: we do NOT byteswap the block numbers!\n\t */\n\tfor (block = 0; block < EXT4_N_BLOCKS; block++)\n\t\tei->i_data[block] = raw_inode->i_block[block];\n\tINIT_LIST_HEAD(&ei->i_orphan);\n\n\t/*\n\t * Set transaction id's of transactions that have to be committed\n\t * to finish f[data]sync. We set them to currently running transaction\n\t * as we cannot be sure that the inode or some of its metadata isn't\n\t * part of the transaction - the inode could have been reclaimed and\n\t * now it is reread from disk.\n\t */\n\tif (journal) {\n\t\ttransaction_t *transaction;\n\t\ttid_t tid;\n\n\t\tread_lock(&journal->j_state_lock);\n\t\tif (journal->j_running_transaction)\n\t\t\ttransaction = journal->j_running_transaction;\n\t\telse\n\t\t\ttransaction = journal->j_committing_transaction;\n\t\tif (transaction)\n\t\t\ttid = transaction->t_tid;\n\t\telse\n\t\t\ttid = journal->j_commit_sequence;\n\t\tread_unlock(&journal->j_state_lock);\n\t\tei->i_sync_tid = tid;\n\t\tei->i_datasync_tid = tid;\n\t}\n\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\tif (ei->i_extra_isize == 0) {\n\t\t\t/* The extra space is currently unused. Use it. */\n\t\t\tBUILD_BUG_ON(sizeof(struct ext4_inode) & 3);\n\t\t\tei->i_extra_isize = sizeof(struct ext4_inode) -\n\t\t\t\t\t    EXT4_GOOD_OLD_INODE_SIZE;\n\t\t} else {\n\t\t\text4_iget_extra_inode(inode, raw_inode, ei);\n\t\t}\n\t}\n\n\tEXT4_INODE_GET_XTIME(i_ctime, inode, raw_inode);\n\tEXT4_INODE_GET_XTIME(i_mtime, inode, raw_inode);\n\tEXT4_INODE_GET_XTIME(i_atime, inode, raw_inode);\n\tEXT4_EINODE_GET_XTIME(i_crtime, ei, raw_inode);\n\n\tif (likely(!test_opt2(inode->i_sb, HURD_COMPAT))) {\n\t\tu64 ivers = le32_to_cpu(raw_inode->i_disk_version);\n\n\t\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\t\tif (EXT4_FITS_IN_INODE(raw_inode, ei, i_version_hi))\n\t\t\t\tivers |=\n\t\t    (__u64)(le32_to_cpu(raw_inode->i_version_hi)) << 32;\n\t\t}\n\t\tinode_set_iversion_queried(inode, ivers);\n\t}\n\n\tret = 0;\n\tif (ei->i_file_acl &&\n\t    !ext4_data_block_valid(EXT4_SB(sb), ei->i_file_acl, 1)) {\n\t\tEXT4_ERROR_INODE(inode, \"bad extended attribute block %llu\",\n\t\t\t\t ei->i_file_acl);\n\t\tret = -EFSCORRUPTED;\n\t\tgoto bad_inode;\n\t} else if (!ext4_has_inline_data(inode)) {\n\t\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\t\tif ((S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||\n\t\t\t    (S_ISLNK(inode->i_mode) &&\n\t\t\t     !ext4_inode_is_fast_symlink(inode))))\n\t\t\t\t/* Validate extent which is part of inode */\n\t\t\t\tret = ext4_ext_check_inode(inode);\n\t\t} else if (S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||\n\t\t\t   (S_ISLNK(inode->i_mode) &&\n\t\t\t    !ext4_inode_is_fast_symlink(inode))) {\n\t\t\t/* Validate block references which are part of inode */\n\t\t\tret = ext4_ind_check_inode(inode);\n\t\t}\n\t}\n\tif (ret)\n\t\tgoto bad_inode;\n\n\tif (S_ISREG(inode->i_mode)) {\n\t\tinode->i_op = &ext4_file_inode_operations;\n\t\tinode->i_fop = &ext4_file_operations;\n\t\text4_set_aops(inode);\n\t} else if (S_ISDIR(inode->i_mode)) {\n\t\tinode->i_op = &ext4_dir_inode_operations;\n\t\tinode->i_fop = &ext4_dir_operations;\n\t} else if (S_ISLNK(inode->i_mode)) {\n\t\tif (ext4_encrypted_inode(inode)) {\n\t\t\tinode->i_op = &ext4_encrypted_symlink_inode_operations;\n\t\t\text4_set_aops(inode);\n\t\t} else if (ext4_inode_is_fast_symlink(inode)) {\n\t\t\tinode->i_link = (char *)ei->i_data;\n\t\t\tinode->i_op = &ext4_fast_symlink_inode_operations;\n\t\t\tnd_terminate_link(ei->i_data, inode->i_size,\n\t\t\t\tsizeof(ei->i_data) - 1);\n\t\t} else {\n\t\t\tinode->i_op = &ext4_symlink_inode_operations;\n\t\t\text4_set_aops(inode);\n\t\t}\n\t\tinode_nohighmem(inode);\n\t} else if (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode) ||\n\t      S_ISFIFO(inode->i_mode) || S_ISSOCK(inode->i_mode)) {\n\t\tinode->i_op = &ext4_special_inode_operations;\n\t\tif (raw_inode->i_block[0])\n\t\t\tinit_special_inode(inode, inode->i_mode,\n\t\t\t   old_decode_dev(le32_to_cpu(raw_inode->i_block[0])));\n\t\telse\n\t\t\tinit_special_inode(inode, inode->i_mode,\n\t\t\t   new_decode_dev(le32_to_cpu(raw_inode->i_block[1])));\n\t} else if (ino == EXT4_BOOT_LOADER_INO) {\n\t\tmake_bad_inode(inode);\n\t} else {\n\t\tret = -EFSCORRUPTED;\n\t\tEXT4_ERROR_INODE(inode, \"bogus i_mode (%o)\", inode->i_mode);\n\t\tgoto bad_inode;\n\t}\n\tbrelse(iloc.bh);\n\text4_set_inode_flags(inode);\n\n\tunlock_new_inode(inode);\n\treturn inode;\n\nbad_inode:\n\tbrelse(iloc.bh);\n\tiget_failed(inode);\n\treturn ERR_PTR(ret);\n}",
                        "cve_id": "CVE-2021-44879"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 1637,
            "cve_id": "CVE-2017-8106",
            "code_snippet": "static int handle_invept(struct kvm_vcpu *vcpu)\n{\n\tu32 vmx_instruction_info, types;\n\tunsigned long type;\n\tgva_t gva;\n\tstruct x86_exception e;\n\tstruct {\n\t\tu64 eptp, gpa;\n\t} operand;\n\tu64 eptp_mask = ((1ull << 51) - 1) & PAGE_MASK;\n\n\tif (!(nested_vmx_secondary_ctls_high & SECONDARY_EXEC_ENABLE_EPT) ||\n\t    !(nested_vmx_ept_caps & VMX_EPT_INVEPT_BIT)) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tif (!kvm_read_cr0_bits(vcpu, X86_CR0_PE)) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tvmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);\n\ttype = kvm_register_read(vcpu, (vmx_instruction_info >> 28) & 0xf);\n\n\ttypes = (nested_vmx_ept_caps >> VMX_EPT_EXTENT_SHIFT) & 6;\n\n\tif (!(types & (1UL << type))) {\n\t\tnested_vmx_failValid(vcpu,\n\t\t\t\tVMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);\n\t\treturn 1;\n\t}\n\n\t/* According to the Intel VMX instruction reference, the memory\n\t * operand is read even if it isn't needed (e.g., for type==global)\n\t */\n\tif (get_vmx_mem_address(vcpu, vmcs_readl(EXIT_QUALIFICATION),\n\t\t\tvmx_instruction_info, &gva))\n\t\treturn 1;\n\tif (kvm_read_guest_virt(&vcpu->arch.emulate_ctxt, gva, &operand,\n\t\t\t\tsizeof(operand), &e)) {\n\t\tkvm_inject_page_fault(vcpu, &e);\n\t\treturn 1;\n\t}\n\n\tswitch (type) {\n\tcase VMX_EPT_EXTENT_CONTEXT:\n\t\tif ((operand.eptp & eptp_mask) !=\n\t\t\t\t(nested_ept_get_cr3(vcpu) & eptp_mask))\n\t\t\tbreak;\n\tcase VMX_EPT_EXTENT_GLOBAL:\n\t\tkvm_mmu_sync_roots(vcpu);\n\t\tkvm_mmu_flush_tlb(vcpu);\n\t\tnested_vmx_succeed(vcpu);\n\t\tbreak;\n\tdefault:\n\t\tBUG_ON(1);\n\t\tbreak;\n\t}\n\n\tskip_emulated_instruction(vcpu);\n\treturn 1;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,\n\t\t\t    int emulation_type, void *insn, int insn_len)\n{\n\tint r;\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tbool writeback = true;\n\tbool write_fault_to_spt;\n\n\tif (unlikely(!kvm_can_emulate_insn(vcpu, emulation_type, insn, insn_len)))\n\t\treturn 1;\n\n\tvcpu->arch.l1tf_flush_l1d = true;\n\n\t/*\n\t * Clear write_fault_to_shadow_pgtable here to ensure it is\n\t * never reused.\n\t */\n\twrite_fault_to_spt = vcpu->arch.write_fault_to_shadow_pgtable;\n\tvcpu->arch.write_fault_to_shadow_pgtable = false;\n\n\tif (!(emulation_type & EMULTYPE_NO_DECODE)) {\n\t\tkvm_clear_exception_queue(vcpu);\n\n\t\tr = x86_decode_emulated_instruction(vcpu, emulation_type,\n\t\t\t\t\t\t    insn, insn_len);\n\t\tif (r != EMULATION_OK)  {\n\t\t\tif ((emulation_type & EMULTYPE_TRAP_UD) ||\n\t\t\t    (emulation_type & EMULTYPE_TRAP_UD_FORCED)) {\n\t\t\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\tif (reexecute_instruction(vcpu, cr2_or_gpa,\n\t\t\t\t\t\t  write_fault_to_spt,\n\t\t\t\t\t\t  emulation_type))\n\t\t\t\treturn 1;\n\t\t\tif (ctxt->have_exception) {\n\t\t\t\t/*\n\t\t\t\t * #UD should result in just EMULATION_FAILED, and trap-like\n\t\t\t\t * exception should not be encountered during decode.\n\t\t\t\t */\n\t\t\t\tWARN_ON_ONCE(ctxt->exception.vector == UD_VECTOR ||\n\t\t\t\t\t     exception_type(ctxt->exception.vector) == EXCPT_TRAP);\n\t\t\t\tinject_emulated_exception(vcpu);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t\t}\n\t}\n\n\tif ((emulation_type & EMULTYPE_VMWARE_GP) &&\n\t    !is_vmware_backdoor_opcode(ctxt)) {\n\t\tkvm_queue_exception_e(vcpu, GP_VECTOR, 0);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * EMULTYPE_SKIP without EMULTYPE_COMPLETE_USER_EXIT is intended for\n\t * use *only* by vendor callbacks for kvm_skip_emulated_instruction().\n\t * The caller is responsible for updating interruptibility state and\n\t * injecting single-step #DBs.\n\t */\n\tif (emulation_type & EMULTYPE_SKIP) {\n\t\tif (ctxt->mode != X86EMUL_MODE_PROT64)\n\t\t\tctxt->eip = (u32)ctxt->_eip;\n\t\telse\n\t\t\tctxt->eip = ctxt->_eip;\n\n\t\tif (emulation_type & EMULTYPE_COMPLETE_USER_EXIT) {\n\t\t\tr = 1;\n\t\t\tgoto writeback;\n\t\t}\n\n\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\tif (ctxt->eflags & X86_EFLAGS_RF)\n\t\t\tkvm_set_rflags(vcpu, ctxt->eflags & ~X86_EFLAGS_RF);\n\t\treturn 1;\n\t}\n\n\tif (retry_instruction(ctxt, cr2_or_gpa, emulation_type))\n\t\treturn 1;\n\n\t/* this is needed for vmware backdoor interface to work since it\n\t   changes registers values  during IO operation */\n\tif (vcpu->arch.emulate_regs_need_sync_from_vcpu) {\n\t\tvcpu->arch.emulate_regs_need_sync_from_vcpu = false;\n\t\temulator_invalidate_register_cache(ctxt);\n\t}\n\nrestart:\n\tif (emulation_type & EMULTYPE_PF) {\n\t\t/* Save the faulting GPA (cr2) in the address field */\n\t\tctxt->exception.address = cr2_or_gpa;\n\n\t\t/* With shadow page tables, cr2 contains a GVA or nGPA. */\n\t\tif (vcpu->arch.mmu->root_role.direct) {\n\t\t\tctxt->gpa_available = true;\n\t\t\tctxt->gpa_val = cr2_or_gpa;\n\t\t}\n\t} else {\n\t\t/* Sanitize the address out of an abundance of paranoia. */\n\t\tctxt->exception.address = 0;\n\t}\n\n\tr = x86_emulate_insn(ctxt);\n\n\tif (r == EMULATION_INTERCEPTED)\n\t\treturn 1;\n\n\tif (r == EMULATION_FAILED) {\n\t\tif (reexecute_instruction(vcpu, cr2_or_gpa, write_fault_to_spt,\n\t\t\t\t\temulation_type))\n\t\t\treturn 1;\n\n\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t}\n\n\tif (ctxt->have_exception) {\n\t\tr = 1;\n\t\tif (inject_emulated_exception(vcpu))\n\t\t\treturn r;\n\t} else if (vcpu->arch.pio.count) {\n\t\tif (!vcpu->arch.pio.in) {\n\t\t\t/* FIXME: return into emulator if single-stepping.  */\n\t\t\tvcpu->arch.pio.count = 0;\n\t\t} else {\n\t\t\twriteback = false;\n\t\t\tvcpu->arch.complete_userspace_io = complete_emulated_pio;\n\t\t}\n\t\tr = 0;\n\t} else if (vcpu->mmio_needed) {\n\t\t++vcpu->stat.mmio_exits;\n\n\t\tif (!vcpu->mmio_is_write)\n\t\t\twriteback = false;\n\t\tr = 0;\n\t\tvcpu->arch.complete_userspace_io = complete_emulated_mmio;\n\t} else if (vcpu->arch.complete_userspace_io) {\n\t\twriteback = false;\n\t\tr = 0;\n\t} else if (r == EMULATION_RESTART)\n\t\tgoto restart;\n\telse\n\t\tr = 1;\n\nwriteback:\n\tif (writeback) {\n\t\tunsigned long rflags = static_call(kvm_x86_get_rflags)(vcpu);\n\t\ttoggle_interruptibility(vcpu, ctxt->interruptibility);\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = false;\n\t\tif (!ctxt->have_exception ||\n\t\t    exception_type(ctxt->exception.vector) == EXCPT_TRAP) {\n\t\t\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);\n\t\t\tif (ctxt->is_branch)\n\t\t\t\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);\n\t\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\t\tif (r && (ctxt->tf || (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)))\n\t\t\t\tr = kvm_vcpu_do_singlestep(vcpu);\n\t\t\tstatic_call_cond(kvm_x86_update_emulated_instruction)(vcpu);\n\t\t\t__kvm_set_rflags(vcpu, ctxt->eflags);\n\t\t}\n\n\t\t/*\n\t\t * For STI, interrupts are shadowed; so KVM_REQ_EVENT will\n\t\t * do nothing, and it will be requested again as soon as\n\t\t * the shadow expires.  But we still need to check here,\n\t\t * because POPF has no interrupt shadow.\n\t\t */\n\t\tif (unlikely((ctxt->eflags & ~rflags) & X86_EFLAGS_IF))\n\t\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t} else\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = true;\n\n\treturn r;\n}",
                        "code_after_change": "int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,\n\t\t\t    int emulation_type, void *insn, int insn_len)\n{\n\tint r;\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tbool writeback = true;\n\tbool write_fault_to_spt;\n\n\tif (unlikely(!kvm_can_emulate_insn(vcpu, emulation_type, insn, insn_len)))\n\t\treturn 1;\n\n\tvcpu->arch.l1tf_flush_l1d = true;\n\n\t/*\n\t * Clear write_fault_to_shadow_pgtable here to ensure it is\n\t * never reused.\n\t */\n\twrite_fault_to_spt = vcpu->arch.write_fault_to_shadow_pgtable;\n\tvcpu->arch.write_fault_to_shadow_pgtable = false;\n\n\tif (!(emulation_type & EMULTYPE_NO_DECODE)) {\n\t\tkvm_clear_exception_queue(vcpu);\n\n\t\t/*\n\t\t * Return immediately if RIP hits a code breakpoint, such #DBs\n\t\t * are fault-like and are higher priority than any faults on\n\t\t * the code fetch itself.\n\t\t */\n\t\tif (!(emulation_type & EMULTYPE_SKIP) &&\n\t\t    kvm_vcpu_check_code_breakpoint(vcpu, &r))\n\t\t\treturn r;\n\n\t\tr = x86_decode_emulated_instruction(vcpu, emulation_type,\n\t\t\t\t\t\t    insn, insn_len);\n\t\tif (r != EMULATION_OK)  {\n\t\t\tif ((emulation_type & EMULTYPE_TRAP_UD) ||\n\t\t\t    (emulation_type & EMULTYPE_TRAP_UD_FORCED)) {\n\t\t\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\tif (reexecute_instruction(vcpu, cr2_or_gpa,\n\t\t\t\t\t\t  write_fault_to_spt,\n\t\t\t\t\t\t  emulation_type))\n\t\t\t\treturn 1;\n\t\t\tif (ctxt->have_exception) {\n\t\t\t\t/*\n\t\t\t\t * #UD should result in just EMULATION_FAILED, and trap-like\n\t\t\t\t * exception should not be encountered during decode.\n\t\t\t\t */\n\t\t\t\tWARN_ON_ONCE(ctxt->exception.vector == UD_VECTOR ||\n\t\t\t\t\t     exception_type(ctxt->exception.vector) == EXCPT_TRAP);\n\t\t\t\tinject_emulated_exception(vcpu);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t\t}\n\t}\n\n\tif ((emulation_type & EMULTYPE_VMWARE_GP) &&\n\t    !is_vmware_backdoor_opcode(ctxt)) {\n\t\tkvm_queue_exception_e(vcpu, GP_VECTOR, 0);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * EMULTYPE_SKIP without EMULTYPE_COMPLETE_USER_EXIT is intended for\n\t * use *only* by vendor callbacks for kvm_skip_emulated_instruction().\n\t * The caller is responsible for updating interruptibility state and\n\t * injecting single-step #DBs.\n\t */\n\tif (emulation_type & EMULTYPE_SKIP) {\n\t\tif (ctxt->mode != X86EMUL_MODE_PROT64)\n\t\t\tctxt->eip = (u32)ctxt->_eip;\n\t\telse\n\t\t\tctxt->eip = ctxt->_eip;\n\n\t\tif (emulation_type & EMULTYPE_COMPLETE_USER_EXIT) {\n\t\t\tr = 1;\n\t\t\tgoto writeback;\n\t\t}\n\n\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\tif (ctxt->eflags & X86_EFLAGS_RF)\n\t\t\tkvm_set_rflags(vcpu, ctxt->eflags & ~X86_EFLAGS_RF);\n\t\treturn 1;\n\t}\n\n\tif (retry_instruction(ctxt, cr2_or_gpa, emulation_type))\n\t\treturn 1;\n\n\t/* this is needed for vmware backdoor interface to work since it\n\t   changes registers values  during IO operation */\n\tif (vcpu->arch.emulate_regs_need_sync_from_vcpu) {\n\t\tvcpu->arch.emulate_regs_need_sync_from_vcpu = false;\n\t\temulator_invalidate_register_cache(ctxt);\n\t}\n\nrestart:\n\tif (emulation_type & EMULTYPE_PF) {\n\t\t/* Save the faulting GPA (cr2) in the address field */\n\t\tctxt->exception.address = cr2_or_gpa;\n\n\t\t/* With shadow page tables, cr2 contains a GVA or nGPA. */\n\t\tif (vcpu->arch.mmu->root_role.direct) {\n\t\t\tctxt->gpa_available = true;\n\t\t\tctxt->gpa_val = cr2_or_gpa;\n\t\t}\n\t} else {\n\t\t/* Sanitize the address out of an abundance of paranoia. */\n\t\tctxt->exception.address = 0;\n\t}\n\n\tr = x86_emulate_insn(ctxt);\n\n\tif (r == EMULATION_INTERCEPTED)\n\t\treturn 1;\n\n\tif (r == EMULATION_FAILED) {\n\t\tif (reexecute_instruction(vcpu, cr2_or_gpa, write_fault_to_spt,\n\t\t\t\t\temulation_type))\n\t\t\treturn 1;\n\n\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t}\n\n\tif (ctxt->have_exception) {\n\t\tr = 1;\n\t\tif (inject_emulated_exception(vcpu))\n\t\t\treturn r;\n\t} else if (vcpu->arch.pio.count) {\n\t\tif (!vcpu->arch.pio.in) {\n\t\t\t/* FIXME: return into emulator if single-stepping.  */\n\t\t\tvcpu->arch.pio.count = 0;\n\t\t} else {\n\t\t\twriteback = false;\n\t\t\tvcpu->arch.complete_userspace_io = complete_emulated_pio;\n\t\t}\n\t\tr = 0;\n\t} else if (vcpu->mmio_needed) {\n\t\t++vcpu->stat.mmio_exits;\n\n\t\tif (!vcpu->mmio_is_write)\n\t\t\twriteback = false;\n\t\tr = 0;\n\t\tvcpu->arch.complete_userspace_io = complete_emulated_mmio;\n\t} else if (vcpu->arch.complete_userspace_io) {\n\t\twriteback = false;\n\t\tr = 0;\n\t} else if (r == EMULATION_RESTART)\n\t\tgoto restart;\n\telse\n\t\tr = 1;\n\nwriteback:\n\tif (writeback) {\n\t\tunsigned long rflags = static_call(kvm_x86_get_rflags)(vcpu);\n\t\ttoggle_interruptibility(vcpu, ctxt->interruptibility);\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = false;\n\t\tif (!ctxt->have_exception ||\n\t\t    exception_type(ctxt->exception.vector) == EXCPT_TRAP) {\n\t\t\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);\n\t\t\tif (ctxt->is_branch)\n\t\t\t\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);\n\t\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\t\tif (r && (ctxt->tf || (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)))\n\t\t\t\tr = kvm_vcpu_do_singlestep(vcpu);\n\t\t\tstatic_call_cond(kvm_x86_update_emulated_instruction)(vcpu);\n\t\t\t__kvm_set_rflags(vcpu, ctxt->eflags);\n\t\t}\n\n\t\t/*\n\t\t * For STI, interrupts are shadowed; so KVM_REQ_EVENT will\n\t\t * do nothing, and it will be requested again as soon as\n\t\t * the shadow expires.  But we still need to check here,\n\t\t * because POPF has no interrupt shadow.\n\t\t */\n\t\tif (unlikely((ctxt->eflags & ~rflags) & X86_EFLAGS_IF))\n\t\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t} else\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = true;\n\n\treturn r;\n}",
                        "cve_id": "CVE-2022-1852"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1669,
            "cve_id": "CVE-2018-1000200",
            "code_snippet": "static void oom_reap_task(struct task_struct *tsk)\n{\n\tint attempts = 0;\n\tstruct mm_struct *mm = tsk->signal->oom_mm;\n\n\t/* Retry the down_read_trylock(mmap_sem) a few times */\n\twhile (attempts++ < MAX_OOM_REAP_RETRIES && !__oom_reap_task_mm(tsk, mm))\n\t\tschedule_timeout_idle(HZ/10);\n\n\tif (attempts <= MAX_OOM_REAP_RETRIES ||\n\t    test_bit(MMF_OOM_SKIP, &mm->flags))\n\t\tgoto done;\n\n\n\tpr_info(\"oom_reaper: unable to reap pid:%d (%s)\\n\",\n\t\ttask_pid_nr(tsk), tsk->comm);\n\tdebug_show_all_locks();\n\ndone:\n\ttsk->oom_reaper_list = NULL;\n\n\t/*\n\t * Hide this mm from OOM killer because it has been either reaped or\n\t * somebody can't call up_write(mmap_sem).\n\t */\n\tset_bit(MMF_OOM_SKIP, &mm->flags);\n\n\t/* Drop a reference taken by wake_oom_reaper */\n\tput_task_struct(tsk);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "void exit_mmap(struct mm_struct *mm)\n{\n\tstruct mmu_gather tlb;\n\tstruct vm_area_struct *vma;\n\tunsigned long nr_accounted = 0;\n\n\t/* mm's last user has gone, and its about to be pulled down */\n\tmmu_notifier_release(mm);\n\n\tif (mm->locked_vm) {\n\t\tvma = mm->mmap;\n\t\twhile (vma) {\n\t\t\tif (vma->vm_flags & VM_LOCKED)\n\t\t\t\tmunlock_vma_pages_all(vma);\n\t\t\tvma = vma->vm_next;\n\t\t}\n\t}\n\n\tarch_exit_mmap(mm);\n\n\tvma = mm->mmap;\n\tif (!vma)\t/* Can happen if dup_mmap() received an OOM */\n\t\treturn;\n\n\tlru_add_drain();\n\tflush_cache_mm(mm);\n\ttlb_gather_mmu(&tlb, mm, 0, -1);\n\t/* update_hiwater_rss(mm) here? but nobody should be looking */\n\t/* Use -1 here to ensure all VMAs in the mm are unmapped */\n\tunmap_vmas(&tlb, vma, 0, -1);\n\n\tif (unlikely(mm_is_oom_victim(mm))) {\n\t\t/*\n\t\t * Wait for oom_reap_task() to stop working on this\n\t\t * mm. Because MMF_OOM_SKIP is already set before\n\t\t * calling down_read(), oom_reap_task() will not run\n\t\t * on this \"mm\" post up_write().\n\t\t *\n\t\t * mm_is_oom_victim() cannot be set from under us\n\t\t * either because victim->mm is already set to NULL\n\t\t * under task_lock before calling mmput and oom_mm is\n\t\t * set not NULL by the OOM killer only if victim->mm\n\t\t * is found not NULL while holding the task_lock.\n\t\t */\n\t\tset_bit(MMF_OOM_SKIP, &mm->flags);\n\t\tdown_write(&mm->mmap_sem);\n\t\tup_write(&mm->mmap_sem);\n\t}\n\tfree_pgtables(&tlb, vma, FIRST_USER_ADDRESS, USER_PGTABLES_CEILING);\n\ttlb_finish_mmu(&tlb, 0, -1);\n\n\t/*\n\t * Walk the list again, actually closing and freeing it,\n\t * with preemption enabled, without holding any MM locks.\n\t */\n\twhile (vma) {\n\t\tif (vma->vm_flags & VM_ACCOUNT)\n\t\t\tnr_accounted += vma_pages(vma);\n\t\tvma = remove_vma(vma);\n\t}\n\tvm_unacct_memory(nr_accounted);\n}",
                        "code_after_change": "void exit_mmap(struct mm_struct *mm)\n{\n\tstruct mmu_gather tlb;\n\tstruct vm_area_struct *vma;\n\tunsigned long nr_accounted = 0;\n\n\t/* mm's last user has gone, and its about to be pulled down */\n\tmmu_notifier_release(mm);\n\n\tif (unlikely(mm_is_oom_victim(mm))) {\n\t\t/*\n\t\t * Manually reap the mm to free as much memory as possible.\n\t\t * Then, as the oom reaper does, set MMF_OOM_SKIP to disregard\n\t\t * this mm from further consideration.  Taking mm->mmap_sem for\n\t\t * write after setting MMF_OOM_SKIP will guarantee that the oom\n\t\t * reaper will not run on this mm again after mmap_sem is\n\t\t * dropped.\n\t\t *\n\t\t * Nothing can be holding mm->mmap_sem here and the above call\n\t\t * to mmu_notifier_release(mm) ensures mmu notifier callbacks in\n\t\t * __oom_reap_task_mm() will not block.\n\t\t *\n\t\t * This needs to be done before calling munlock_vma_pages_all(),\n\t\t * which clears VM_LOCKED, otherwise the oom reaper cannot\n\t\t * reliably test it.\n\t\t */\n\t\tmutex_lock(&oom_lock);\n\t\t__oom_reap_task_mm(mm);\n\t\tmutex_unlock(&oom_lock);\n\n\t\tset_bit(MMF_OOM_SKIP, &mm->flags);\n\t\tdown_write(&mm->mmap_sem);\n\t\tup_write(&mm->mmap_sem);\n\t}\n\n\tif (mm->locked_vm) {\n\t\tvma = mm->mmap;\n\t\twhile (vma) {\n\t\t\tif (vma->vm_flags & VM_LOCKED)\n\t\t\t\tmunlock_vma_pages_all(vma);\n\t\t\tvma = vma->vm_next;\n\t\t}\n\t}\n\n\tarch_exit_mmap(mm);\n\n\tvma = mm->mmap;\n\tif (!vma)\t/* Can happen if dup_mmap() received an OOM */\n\t\treturn;\n\n\tlru_add_drain();\n\tflush_cache_mm(mm);\n\ttlb_gather_mmu(&tlb, mm, 0, -1);\n\t/* update_hiwater_rss(mm) here? but nobody should be looking */\n\t/* Use -1 here to ensure all VMAs in the mm are unmapped */\n\tunmap_vmas(&tlb, vma, 0, -1);\n\tfree_pgtables(&tlb, vma, FIRST_USER_ADDRESS, USER_PGTABLES_CEILING);\n\ttlb_finish_mmu(&tlb, 0, -1);\n\n\t/*\n\t * Walk the list again, actually closing and freeing it,\n\t * with preemption enabled, without holding any MM locks.\n\t */\n\twhile (vma) {\n\t\tif (vma->vm_flags & VM_ACCOUNT)\n\t\t\tnr_accounted += vma_pages(vma);\n\t\tvma = remove_vma(vma);\n\t}\n\tvm_unacct_memory(nr_accounted);\n}",
                        "cve_id": "CVE-2018-1000200"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1681,
            "cve_id": "CVE-2018-1066",
            "code_snippet": "void build_ntlmssp_negotiate_blob(unsigned char *pbuffer,\n\t\t\t\t\t struct cifs_ses *ses)\n{\n\tNEGOTIATE_MESSAGE *sec_blob = (NEGOTIATE_MESSAGE *)pbuffer;\n\t__u32 flags;\n\n\tmemset(pbuffer, 0, sizeof(NEGOTIATE_MESSAGE));\n\tmemcpy(sec_blob->Signature, NTLMSSP_SIGNATURE, 8);\n\tsec_blob->MessageType = NtLmNegotiate;\n\n\t/* BB is NTLMV2 session security format easier to use here? */\n\tflags = NTLMSSP_NEGOTIATE_56 |\tNTLMSSP_REQUEST_TARGET |\n\t\tNTLMSSP_NEGOTIATE_128 | NTLMSSP_NEGOTIATE_UNICODE |\n\t\tNTLMSSP_NEGOTIATE_NTLM | NTLMSSP_NEGOTIATE_EXTENDED_SEC;\n\tif (ses->server->sign) {\n\t\tflags |= NTLMSSP_NEGOTIATE_SIGN;\n\t\tif (!ses->server->session_estab ||\n\t\t\t\tses->ntlmssp->sesskey_per_smbsess)\n\t\t\tflags |= NTLMSSP_NEGOTIATE_KEY_XCH;\n\t}\n\n\tsec_blob->NegotiateFlags = cpu_to_le32(flags);\n\n\tsec_blob->WorkstationName.BufferOffset = 0;\n\tsec_blob->WorkstationName.Length = 0;\n\tsec_blob->WorkstationName.MaximumLength = 0;\n\n\t/* Domain name is sent on the Challenge not Negotiate NTLMSSP request */\n\tsec_blob->DomainName.BufferOffset = 0;\n\tsec_blob->DomainName.Length = 0;\n\tsec_blob->DomainName.MaximumLength = 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int build_ntlmssp_auth_blob(unsigned char **pbuffer,\n\t\t\t\t\tu16 *buflen,\n\t\t\t\t   struct cifs_ses *ses,\n\t\t\t\t   const struct nls_table *nls_cp)\n{\n\tint rc;\n\tAUTHENTICATE_MESSAGE *sec_blob;\n\t__u32 flags;\n\tunsigned char *tmp;\n\n\trc = setup_ntlmv2_rsp(ses, nls_cp);\n\tif (rc) {\n\t\tcifs_dbg(VFS, \"Error %d during NTLMSSP authentication\\n\", rc);\n\t\t*buflen = 0;\n\t\tgoto setup_ntlmv2_ret;\n\t}\n\t*pbuffer = kmalloc(size_of_ntlmssp_blob(ses), GFP_KERNEL);\n\tsec_blob = (AUTHENTICATE_MESSAGE *)*pbuffer;\n\n\tmemcpy(sec_blob->Signature, NTLMSSP_SIGNATURE, 8);\n\tsec_blob->MessageType = NtLmAuthenticate;\n\n\tflags = NTLMSSP_NEGOTIATE_56 |\n\t\tNTLMSSP_REQUEST_TARGET | NTLMSSP_NEGOTIATE_TARGET_INFO |\n\t\tNTLMSSP_NEGOTIATE_128 | NTLMSSP_NEGOTIATE_UNICODE |\n\t\tNTLMSSP_NEGOTIATE_NTLM | NTLMSSP_NEGOTIATE_EXTENDED_SEC;\n\tif (ses->server->sign) {\n\t\tflags |= NTLMSSP_NEGOTIATE_SIGN;\n\t\tif (!ses->server->session_estab ||\n\t\t\t\tses->ntlmssp->sesskey_per_smbsess)\n\t\t\tflags |= NTLMSSP_NEGOTIATE_KEY_XCH;\n\t}\n\n\ttmp = *pbuffer + sizeof(AUTHENTICATE_MESSAGE);\n\tsec_blob->NegotiateFlags = cpu_to_le32(flags);\n\n\tsec_blob->LmChallengeResponse.BufferOffset =\n\t\t\t\tcpu_to_le32(sizeof(AUTHENTICATE_MESSAGE));\n\tsec_blob->LmChallengeResponse.Length = 0;\n\tsec_blob->LmChallengeResponse.MaximumLength = 0;\n\n\tsec_blob->NtChallengeResponse.BufferOffset =\n\t\t\t\tcpu_to_le32(tmp - *pbuffer);\n\tif (ses->user_name != NULL) {\n\t\tmemcpy(tmp, ses->auth_key.response + CIFS_SESS_KEY_SIZE,\n\t\t\t\tses->auth_key.len - CIFS_SESS_KEY_SIZE);\n\t\ttmp += ses->auth_key.len - CIFS_SESS_KEY_SIZE;\n\n\t\tsec_blob->NtChallengeResponse.Length =\n\t\t\t\tcpu_to_le16(ses->auth_key.len - CIFS_SESS_KEY_SIZE);\n\t\tsec_blob->NtChallengeResponse.MaximumLength =\n\t\t\t\tcpu_to_le16(ses->auth_key.len - CIFS_SESS_KEY_SIZE);\n\t} else {\n\t\t/*\n\t\t * don't send an NT Response for anonymous access\n\t\t */\n\t\tsec_blob->NtChallengeResponse.Length = 0;\n\t\tsec_blob->NtChallengeResponse.MaximumLength = 0;\n\t}\n\n\tif (ses->domainName == NULL) {\n\t\tsec_blob->DomainName.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\t\tsec_blob->DomainName.Length = 0;\n\t\tsec_blob->DomainName.MaximumLength = 0;\n\t\ttmp += 2;\n\t} else {\n\t\tint len;\n\t\tlen = cifs_strtoUTF16((__le16 *)tmp, ses->domainName,\n\t\t\t\t      CIFS_MAX_DOMAINNAME_LEN, nls_cp);\n\t\tlen *= 2; /* unicode is 2 bytes each */\n\t\tsec_blob->DomainName.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\t\tsec_blob->DomainName.Length = cpu_to_le16(len);\n\t\tsec_blob->DomainName.MaximumLength = cpu_to_le16(len);\n\t\ttmp += len;\n\t}\n\n\tif (ses->user_name == NULL) {\n\t\tsec_blob->UserName.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\t\tsec_blob->UserName.Length = 0;\n\t\tsec_blob->UserName.MaximumLength = 0;\n\t\ttmp += 2;\n\t} else {\n\t\tint len;\n\t\tlen = cifs_strtoUTF16((__le16 *)tmp, ses->user_name,\n\t\t\t\t      CIFS_MAX_USERNAME_LEN, nls_cp);\n\t\tlen *= 2; /* unicode is 2 bytes each */\n\t\tsec_blob->UserName.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\t\tsec_blob->UserName.Length = cpu_to_le16(len);\n\t\tsec_blob->UserName.MaximumLength = cpu_to_le16(len);\n\t\ttmp += len;\n\t}\n\n\tsec_blob->WorkstationName.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\tsec_blob->WorkstationName.Length = 0;\n\tsec_blob->WorkstationName.MaximumLength = 0;\n\ttmp += 2;\n\n\tif (((ses->ntlmssp->server_flags & NTLMSSP_NEGOTIATE_KEY_XCH) ||\n\t\t(ses->ntlmssp->server_flags & NTLMSSP_NEGOTIATE_EXTENDED_SEC))\n\t\t\t&& !calc_seckey(ses)) {\n\t\tmemcpy(tmp, ses->ntlmssp->ciphertext, CIFS_CPHTXT_SIZE);\n\t\tsec_blob->SessionKey.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\t\tsec_blob->SessionKey.Length = cpu_to_le16(CIFS_CPHTXT_SIZE);\n\t\tsec_blob->SessionKey.MaximumLength =\n\t\t\t\tcpu_to_le16(CIFS_CPHTXT_SIZE);\n\t\ttmp += CIFS_CPHTXT_SIZE;\n\t} else {\n\t\tsec_blob->SessionKey.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\t\tsec_blob->SessionKey.Length = 0;\n\t\tsec_blob->SessionKey.MaximumLength = 0;\n\t}\n\n\t*buflen = tmp - *pbuffer;\nsetup_ntlmv2_ret:\n\treturn rc;\n}",
                        "code_after_change": "int build_ntlmssp_auth_blob(unsigned char **pbuffer,\n\t\t\t\t\tu16 *buflen,\n\t\t\t\t   struct cifs_ses *ses,\n\t\t\t\t   const struct nls_table *nls_cp)\n{\n\tint rc;\n\tAUTHENTICATE_MESSAGE *sec_blob;\n\t__u32 flags;\n\tunsigned char *tmp;\n\n\trc = setup_ntlmv2_rsp(ses, nls_cp);\n\tif (rc) {\n\t\tcifs_dbg(VFS, \"Error %d during NTLMSSP authentication\\n\", rc);\n\t\t*buflen = 0;\n\t\tgoto setup_ntlmv2_ret;\n\t}\n\t*pbuffer = kmalloc(size_of_ntlmssp_blob(ses), GFP_KERNEL);\n\tsec_blob = (AUTHENTICATE_MESSAGE *)*pbuffer;\n\n\tmemcpy(sec_blob->Signature, NTLMSSP_SIGNATURE, 8);\n\tsec_blob->MessageType = NtLmAuthenticate;\n\n\tflags = NTLMSSP_NEGOTIATE_56 |\n\t\tNTLMSSP_REQUEST_TARGET | NTLMSSP_NEGOTIATE_TARGET_INFO |\n\t\tNTLMSSP_NEGOTIATE_128 | NTLMSSP_NEGOTIATE_UNICODE |\n\t\tNTLMSSP_NEGOTIATE_NTLM | NTLMSSP_NEGOTIATE_EXTENDED_SEC |\n\t\tNTLMSSP_NEGOTIATE_SEAL;\n\tif (ses->server->sign)\n\t\tflags |= NTLMSSP_NEGOTIATE_SIGN;\n\tif (!ses->server->session_estab || ses->ntlmssp->sesskey_per_smbsess)\n\t\tflags |= NTLMSSP_NEGOTIATE_KEY_XCH;\n\n\ttmp = *pbuffer + sizeof(AUTHENTICATE_MESSAGE);\n\tsec_blob->NegotiateFlags = cpu_to_le32(flags);\n\n\tsec_blob->LmChallengeResponse.BufferOffset =\n\t\t\t\tcpu_to_le32(sizeof(AUTHENTICATE_MESSAGE));\n\tsec_blob->LmChallengeResponse.Length = 0;\n\tsec_blob->LmChallengeResponse.MaximumLength = 0;\n\n\tsec_blob->NtChallengeResponse.BufferOffset =\n\t\t\t\tcpu_to_le32(tmp - *pbuffer);\n\tif (ses->user_name != NULL) {\n\t\tmemcpy(tmp, ses->auth_key.response + CIFS_SESS_KEY_SIZE,\n\t\t\t\tses->auth_key.len - CIFS_SESS_KEY_SIZE);\n\t\ttmp += ses->auth_key.len - CIFS_SESS_KEY_SIZE;\n\n\t\tsec_blob->NtChallengeResponse.Length =\n\t\t\t\tcpu_to_le16(ses->auth_key.len - CIFS_SESS_KEY_SIZE);\n\t\tsec_blob->NtChallengeResponse.MaximumLength =\n\t\t\t\tcpu_to_le16(ses->auth_key.len - CIFS_SESS_KEY_SIZE);\n\t} else {\n\t\t/*\n\t\t * don't send an NT Response for anonymous access\n\t\t */\n\t\tsec_blob->NtChallengeResponse.Length = 0;\n\t\tsec_blob->NtChallengeResponse.MaximumLength = 0;\n\t}\n\n\tif (ses->domainName == NULL) {\n\t\tsec_blob->DomainName.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\t\tsec_blob->DomainName.Length = 0;\n\t\tsec_blob->DomainName.MaximumLength = 0;\n\t\ttmp += 2;\n\t} else {\n\t\tint len;\n\t\tlen = cifs_strtoUTF16((__le16 *)tmp, ses->domainName,\n\t\t\t\t      CIFS_MAX_DOMAINNAME_LEN, nls_cp);\n\t\tlen *= 2; /* unicode is 2 bytes each */\n\t\tsec_blob->DomainName.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\t\tsec_blob->DomainName.Length = cpu_to_le16(len);\n\t\tsec_blob->DomainName.MaximumLength = cpu_to_le16(len);\n\t\ttmp += len;\n\t}\n\n\tif (ses->user_name == NULL) {\n\t\tsec_blob->UserName.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\t\tsec_blob->UserName.Length = 0;\n\t\tsec_blob->UserName.MaximumLength = 0;\n\t\ttmp += 2;\n\t} else {\n\t\tint len;\n\t\tlen = cifs_strtoUTF16((__le16 *)tmp, ses->user_name,\n\t\t\t\t      CIFS_MAX_USERNAME_LEN, nls_cp);\n\t\tlen *= 2; /* unicode is 2 bytes each */\n\t\tsec_blob->UserName.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\t\tsec_blob->UserName.Length = cpu_to_le16(len);\n\t\tsec_blob->UserName.MaximumLength = cpu_to_le16(len);\n\t\ttmp += len;\n\t}\n\n\tsec_blob->WorkstationName.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\tsec_blob->WorkstationName.Length = 0;\n\tsec_blob->WorkstationName.MaximumLength = 0;\n\ttmp += 2;\n\n\tif (((ses->ntlmssp->server_flags & NTLMSSP_NEGOTIATE_KEY_XCH) ||\n\t\t(ses->ntlmssp->server_flags & NTLMSSP_NEGOTIATE_EXTENDED_SEC))\n\t\t\t&& !calc_seckey(ses)) {\n\t\tmemcpy(tmp, ses->ntlmssp->ciphertext, CIFS_CPHTXT_SIZE);\n\t\tsec_blob->SessionKey.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\t\tsec_blob->SessionKey.Length = cpu_to_le16(CIFS_CPHTXT_SIZE);\n\t\tsec_blob->SessionKey.MaximumLength =\n\t\t\t\tcpu_to_le16(CIFS_CPHTXT_SIZE);\n\t\ttmp += CIFS_CPHTXT_SIZE;\n\t} else {\n\t\tsec_blob->SessionKey.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\t\tsec_blob->SessionKey.Length = 0;\n\t\tsec_blob->SessionKey.MaximumLength = 0;\n\t}\n\n\t*buflen = tmp - *pbuffer;\nsetup_ntlmv2_ret:\n\treturn rc;\n}",
                        "cve_id": "CVE-2018-1066"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1760,
            "cve_id": "CVE-2018-13093",
            "code_snippet": "static int\nxfs_iget_cache_hit(\n\tstruct xfs_perag\t*pag,\n\tstruct xfs_inode\t*ip,\n\txfs_ino_t\t\tino,\n\tint\t\t\tflags,\n\tint\t\t\tlock_flags) __releases(RCU)\n{\n\tstruct inode\t\t*inode = VFS_I(ip);\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tint\t\t\terror;\n\n\t/*\n\t * check for re-use of an inode within an RCU grace period due to the\n\t * radix tree nodes not being updated yet. We monitor for this by\n\t * setting the inode number to zero before freeing the inode structure.\n\t * If the inode has been reallocated and set up, then the inode number\n\t * will not match, so check for that, too.\n\t */\n\tspin_lock(&ip->i_flags_lock);\n\tif (ip->i_ino != ino) {\n\t\ttrace_xfs_iget_skip(ip);\n\t\tXFS_STATS_INC(mp, xs_ig_frecycle);\n\t\terror = -EAGAIN;\n\t\tgoto out_error;\n\t}\n\n\n\t/*\n\t * If we are racing with another cache hit that is currently\n\t * instantiating this inode or currently recycling it out of\n\t * reclaimabe state, wait for the initialisation to complete\n\t * before continuing.\n\t *\n\t * XXX(hch): eventually we should do something equivalent to\n\t *\t     wait_on_inode to wait for these flags to be cleared\n\t *\t     instead of polling for it.\n\t */\n\tif (ip->i_flags & (XFS_INEW|XFS_IRECLAIM)) {\n\t\ttrace_xfs_iget_skip(ip);\n\t\tXFS_STATS_INC(mp, xs_ig_frecycle);\n\t\terror = -EAGAIN;\n\t\tgoto out_error;\n\t}\n\n\t/*\n\t * If lookup is racing with unlink return an error immediately.\n\t */\n\tif (VFS_I(ip)->i_mode == 0 && !(flags & XFS_IGET_CREATE)) {\n\t\terror = -ENOENT;\n\t\tgoto out_error;\n\t}\n\n\t/*\n\t * If IRECLAIMABLE is set, we've torn down the VFS inode already.\n\t * Need to carefully get it back into useable state.\n\t */\n\tif (ip->i_flags & XFS_IRECLAIMABLE) {\n\t\ttrace_xfs_iget_reclaim(ip);\n\n\t\tif (flags & XFS_IGET_INCORE) {\n\t\t\terror = -EAGAIN;\n\t\t\tgoto out_error;\n\t\t}\n\n\t\t/*\n\t\t * We need to set XFS_IRECLAIM to prevent xfs_reclaim_inode\n\t\t * from stomping over us while we recycle the inode.  We can't\n\t\t * clear the radix tree reclaimable tag yet as it requires\n\t\t * pag_ici_lock to be held exclusive.\n\t\t */\n\t\tip->i_flags |= XFS_IRECLAIM;\n\n\t\tspin_unlock(&ip->i_flags_lock);\n\t\trcu_read_unlock();\n\n\t\terror = xfs_reinit_inode(mp, inode);\n\t\tif (error) {\n\t\t\tbool wake;\n\t\t\t/*\n\t\t\t * Re-initializing the inode failed, and we are in deep\n\t\t\t * trouble.  Try to re-add it to the reclaim list.\n\t\t\t */\n\t\t\trcu_read_lock();\n\t\t\tspin_lock(&ip->i_flags_lock);\n\t\t\twake = !!__xfs_iflags_test(ip, XFS_INEW);\n\t\t\tip->i_flags &= ~(XFS_INEW | XFS_IRECLAIM);\n\t\t\tif (wake)\n\t\t\t\twake_up_bit(&ip->i_flags, __XFS_INEW_BIT);\n\t\t\tASSERT(ip->i_flags & XFS_IRECLAIMABLE);\n\t\t\ttrace_xfs_iget_reclaim_fail(ip);\n\t\t\tgoto out_error;\n\t\t}\n\n\t\tspin_lock(&pag->pag_ici_lock);\n\t\tspin_lock(&ip->i_flags_lock);\n\n\t\t/*\n\t\t * Clear the per-lifetime state in the inode as we are now\n\t\t * effectively a new inode and need to return to the initial\n\t\t * state before reuse occurs.\n\t\t */\n\t\tip->i_flags &= ~XFS_IRECLAIM_RESET_FLAGS;\n\t\tip->i_flags |= XFS_INEW;\n\t\txfs_inode_clear_reclaim_tag(pag, ip->i_ino);\n\t\tinode->i_state = I_NEW;\n\n\t\tASSERT(!rwsem_is_locked(&inode->i_rwsem));\n\t\tinit_rwsem(&inode->i_rwsem);\n\n\t\tspin_unlock(&ip->i_flags_lock);\n\t\tspin_unlock(&pag->pag_ici_lock);\n\t} else {\n\t\t/* If the VFS inode is being torn down, pause and try again. */\n\t\tif (!igrab(inode)) {\n\t\t\ttrace_xfs_iget_skip(ip);\n\t\t\terror = -EAGAIN;\n\t\t\tgoto out_error;\n\t\t}\n\n\t\t/* We've got a live one. */\n\t\tspin_unlock(&ip->i_flags_lock);\n\t\trcu_read_unlock();\n\t\ttrace_xfs_iget_hit(ip);\n\t}\n\n\tif (lock_flags != 0)\n\t\txfs_ilock(ip, lock_flags);\n\n\tif (!(flags & XFS_IGET_INCORE))\n\t\txfs_iflags_clear(ip, XFS_ISTALE | XFS_IDONTCACHE);\n\tXFS_STATS_INC(mp, xs_ig_found);\n\n\treturn 0;\n\nout_error:\n\tspin_unlock(&ip->i_flags_lock);\n\trcu_read_unlock();\n\treturn error;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int\nxfs_iget_cache_miss(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_perag\t*pag,\n\txfs_trans_t\t\t*tp,\n\txfs_ino_t\t\tino,\n\tstruct xfs_inode\t**ipp,\n\tint\t\t\tflags,\n\tint\t\t\tlock_flags)\n{\n\tstruct xfs_inode\t*ip;\n\tint\t\t\terror;\n\txfs_agino_t\t\tagino = XFS_INO_TO_AGINO(mp, ino);\n\tint\t\t\tiflags;\n\n\tip = xfs_inode_alloc(mp, ino);\n\tif (!ip)\n\t\treturn -ENOMEM;\n\n\terror = xfs_iread(mp, tp, ip, flags);\n\tif (error)\n\t\tgoto out_destroy;\n\n\tif (!xfs_inode_verify_forks(ip)) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto out_destroy;\n\t}\n\n\ttrace_xfs_iget_miss(ip);\n\n\n\t/*\n\t * If we are allocating a new inode, then check what was returned is\n\t * actually a free, empty inode. If we are not allocating an inode,\n\t * the check we didn't find a free inode.\n\t */\n\tif (flags & XFS_IGET_CREATE) {\n\t\tif (VFS_I(ip)->i_mode != 0) {\n\t\t\txfs_warn(mp,\n\"Corruption detected! Free inode 0x%llx not marked free on disk\",\n\t\t\t\tino);\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto out_destroy;\n\t\t}\n\t\tif (ip->i_d.di_nblocks != 0) {\n\t\t\txfs_warn(mp,\n\"Corruption detected! Free inode 0x%llx has blocks allocated!\",\n\t\t\t\tino);\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto out_destroy;\n\t\t}\n\t} else if (VFS_I(ip)->i_mode == 0) {\n\t\terror = -ENOENT;\n\t\tgoto out_destroy;\n\t}\n\n\t/*\n\t * Preload the radix tree so we can insert safely under the\n\t * write spinlock. Note that we cannot sleep inside the preload\n\t * region. Since we can be called from transaction context, don't\n\t * recurse into the file system.\n\t */\n\tif (radix_tree_preload(GFP_NOFS)) {\n\t\terror = -EAGAIN;\n\t\tgoto out_destroy;\n\t}\n\n\t/*\n\t * Because the inode hasn't been added to the radix-tree yet it can't\n\t * be found by another thread, so we can do the non-sleeping lock here.\n\t */\n\tif (lock_flags) {\n\t\tif (!xfs_ilock_nowait(ip, lock_flags))\n\t\t\tBUG();\n\t}\n\n\t/*\n\t * These values must be set before inserting the inode into the radix\n\t * tree as the moment it is inserted a concurrent lookup (allowed by the\n\t * RCU locking mechanism) can find it and that lookup must see that this\n\t * is an inode currently under construction (i.e. that XFS_INEW is set).\n\t * The ip->i_flags_lock that protects the XFS_INEW flag forms the\n\t * memory barrier that ensures this detection works correctly at lookup\n\t * time.\n\t */\n\tiflags = XFS_INEW;\n\tif (flags & XFS_IGET_DONTCACHE)\n\t\tiflags |= XFS_IDONTCACHE;\n\tip->i_udquot = NULL;\n\tip->i_gdquot = NULL;\n\tip->i_pdquot = NULL;\n\txfs_iflags_set(ip, iflags);\n\n\t/* insert the new inode */\n\tspin_lock(&pag->pag_ici_lock);\n\terror = radix_tree_insert(&pag->pag_ici_root, agino, ip);\n\tif (unlikely(error)) {\n\t\tWARN_ON(error != -EEXIST);\n\t\tXFS_STATS_INC(mp, xs_ig_dup);\n\t\terror = -EAGAIN;\n\t\tgoto out_preload_end;\n\t}\n\tspin_unlock(&pag->pag_ici_lock);\n\tradix_tree_preload_end();\n\n\t*ipp = ip;\n\treturn 0;\n\nout_preload_end:\n\tspin_unlock(&pag->pag_ici_lock);\n\tradix_tree_preload_end();\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\nout_destroy:\n\t__destroy_inode(VFS_I(ip));\n\txfs_inode_free(ip);\n\treturn error;\n}",
                        "code_after_change": "static int\nxfs_iget_cache_miss(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_perag\t*pag,\n\txfs_trans_t\t\t*tp,\n\txfs_ino_t\t\tino,\n\tstruct xfs_inode\t**ipp,\n\tint\t\t\tflags,\n\tint\t\t\tlock_flags)\n{\n\tstruct xfs_inode\t*ip;\n\tint\t\t\terror;\n\txfs_agino_t\t\tagino = XFS_INO_TO_AGINO(mp, ino);\n\tint\t\t\tiflags;\n\n\tip = xfs_inode_alloc(mp, ino);\n\tif (!ip)\n\t\treturn -ENOMEM;\n\n\terror = xfs_iread(mp, tp, ip, flags);\n\tif (error)\n\t\tgoto out_destroy;\n\n\tif (!xfs_inode_verify_forks(ip)) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto out_destroy;\n\t}\n\n\ttrace_xfs_iget_miss(ip);\n\n\n\t/*\n\t * Check the inode free state is valid. This also detects lookup\n\t * racing with unlinks.\n\t */\n\terror = xfs_iget_check_free_state(ip, flags);\n\tif (error)\n\t\tgoto out_destroy;\n\n\t/*\n\t * Preload the radix tree so we can insert safely under the\n\t * write spinlock. Note that we cannot sleep inside the preload\n\t * region. Since we can be called from transaction context, don't\n\t * recurse into the file system.\n\t */\n\tif (radix_tree_preload(GFP_NOFS)) {\n\t\terror = -EAGAIN;\n\t\tgoto out_destroy;\n\t}\n\n\t/*\n\t * Because the inode hasn't been added to the radix-tree yet it can't\n\t * be found by another thread, so we can do the non-sleeping lock here.\n\t */\n\tif (lock_flags) {\n\t\tif (!xfs_ilock_nowait(ip, lock_flags))\n\t\t\tBUG();\n\t}\n\n\t/*\n\t * These values must be set before inserting the inode into the radix\n\t * tree as the moment it is inserted a concurrent lookup (allowed by the\n\t * RCU locking mechanism) can find it and that lookup must see that this\n\t * is an inode currently under construction (i.e. that XFS_INEW is set).\n\t * The ip->i_flags_lock that protects the XFS_INEW flag forms the\n\t * memory barrier that ensures this detection works correctly at lookup\n\t * time.\n\t */\n\tiflags = XFS_INEW;\n\tif (flags & XFS_IGET_DONTCACHE)\n\t\tiflags |= XFS_IDONTCACHE;\n\tip->i_udquot = NULL;\n\tip->i_gdquot = NULL;\n\tip->i_pdquot = NULL;\n\txfs_iflags_set(ip, iflags);\n\n\t/* insert the new inode */\n\tspin_lock(&pag->pag_ici_lock);\n\terror = radix_tree_insert(&pag->pag_ici_root, agino, ip);\n\tif (unlikely(error)) {\n\t\tWARN_ON(error != -EEXIST);\n\t\tXFS_STATS_INC(mp, xs_ig_dup);\n\t\terror = -EAGAIN;\n\t\tgoto out_preload_end;\n\t}\n\tspin_unlock(&pag->pag_ici_lock);\n\tradix_tree_preload_end();\n\n\t*ipp = ip;\n\treturn 0;\n\nout_preload_end:\n\tspin_unlock(&pag->pag_ici_lock);\n\tradix_tree_preload_end();\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\nout_destroy:\n\t__destroy_inode(VFS_I(ip));\n\txfs_inode_free(ip);\n\treturn error;\n}",
                        "cve_id": "CVE-2018-13093"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1775,
            "cve_id": "CVE-2018-14613",
            "code_snippet": "static int __btrfs_alloc_chunk(struct btrfs_trans_handle *trans,\n\t\t\t       u64 start, u64 type)\n{\n\tstruct btrfs_fs_info *info = trans->fs_info;\n\tstruct btrfs_fs_devices *fs_devices = info->fs_devices;\n\tstruct btrfs_device *device;\n\tstruct map_lookup *map = NULL;\n\tstruct extent_map_tree *em_tree;\n\tstruct extent_map *em;\n\tstruct btrfs_device_info *devices_info = NULL;\n\tu64 total_avail;\n\tint num_stripes;\t/* total number of stripes to allocate */\n\tint data_stripes;\t/* number of stripes that count for\n\t\t\t\t   block group size */\n\tint sub_stripes;\t/* sub_stripes info for map */\n\tint dev_stripes;\t/* stripes per dev */\n\tint devs_max;\t\t/* max devs to use */\n\tint devs_min;\t\t/* min devs needed */\n\tint devs_increment;\t/* ndevs has to be a multiple of this */\n\tint ncopies;\t\t/* how many copies to data has */\n\tint ret;\n\tu64 max_stripe_size;\n\tu64 max_chunk_size;\n\tu64 stripe_size;\n\tu64 num_bytes;\n\tint ndevs;\n\tint i;\n\tint j;\n\tint index;\n\n\tBUG_ON(!alloc_profile_is_valid(type, 0));\n\n\tif (list_empty(&fs_devices->alloc_list)) {\n\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG))\n\t\t\tbtrfs_debug(info, \"%s: no writable device\", __func__);\n\t\treturn -ENOSPC;\n\t}\n\n\tindex = btrfs_bg_flags_to_raid_index(type);\n\n\tsub_stripes = btrfs_raid_array[index].sub_stripes;\n\tdev_stripes = btrfs_raid_array[index].dev_stripes;\n\tdevs_max = btrfs_raid_array[index].devs_max;\n\tdevs_min = btrfs_raid_array[index].devs_min;\n\tdevs_increment = btrfs_raid_array[index].devs_increment;\n\tncopies = btrfs_raid_array[index].ncopies;\n\n\tif (type & BTRFS_BLOCK_GROUP_DATA) {\n\t\tmax_stripe_size = SZ_1G;\n\t\tmax_chunk_size = 10 * max_stripe_size;\n\t\tif (!devs_max)\n\t\t\tdevs_max = BTRFS_MAX_DEVS(info);\n\t} else if (type & BTRFS_BLOCK_GROUP_METADATA) {\n\t\t/* for larger filesystems, use larger metadata chunks */\n\t\tif (fs_devices->total_rw_bytes > 50ULL * SZ_1G)\n\t\t\tmax_stripe_size = SZ_1G;\n\t\telse\n\t\t\tmax_stripe_size = SZ_256M;\n\t\tmax_chunk_size = max_stripe_size;\n\t\tif (!devs_max)\n\t\t\tdevs_max = BTRFS_MAX_DEVS(info);\n\t} else if (type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\tmax_stripe_size = SZ_32M;\n\t\tmax_chunk_size = 2 * max_stripe_size;\n\t\tif (!devs_max)\n\t\t\tdevs_max = BTRFS_MAX_DEVS_SYS_CHUNK;\n\t} else {\n\t\tbtrfs_err(info, \"invalid chunk type 0x%llx requested\",\n\t\t       type);\n\t\tBUG_ON(1);\n\t}\n\n\t/* we don't want a chunk larger than 10% of writeable space */\n\tmax_chunk_size = min(div_factor(fs_devices->total_rw_bytes, 1),\n\t\t\t     max_chunk_size);\n\n\tdevices_info = kcalloc(fs_devices->rw_devices, sizeof(*devices_info),\n\t\t\t       GFP_NOFS);\n\tif (!devices_info)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * in the first pass through the devices list, we gather information\n\t * about the available holes on each device.\n\t */\n\tndevs = 0;\n\tlist_for_each_entry(device, &fs_devices->alloc_list, dev_alloc_list) {\n\t\tu64 max_avail;\n\t\tu64 dev_offset;\n\n\t\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\t\tWARN(1, KERN_ERR\n\t\t\t       \"BTRFS: read-only device in alloc_list\\n\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!test_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t\t&device->dev_state) ||\n\t\t    test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state))\n\t\t\tcontinue;\n\n\t\tif (device->total_bytes > device->bytes_used)\n\t\t\ttotal_avail = device->total_bytes - device->bytes_used;\n\t\telse\n\t\t\ttotal_avail = 0;\n\n\t\t/* If there is no space on this device, skip it. */\n\t\tif (total_avail == 0)\n\t\t\tcontinue;\n\n\t\tret = find_free_dev_extent(trans, device,\n\t\t\t\t\t   max_stripe_size * dev_stripes,\n\t\t\t\t\t   &dev_offset, &max_avail);\n\t\tif (ret && ret != -ENOSPC)\n\t\t\tgoto error;\n\n\t\tif (ret == 0)\n\t\t\tmax_avail = max_stripe_size * dev_stripes;\n\n\t\tif (max_avail < BTRFS_STRIPE_LEN * dev_stripes) {\n\t\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG))\n\t\t\t\tbtrfs_debug(info,\n\t\t\t\"%s: devid %llu has no free space, have=%llu want=%u\",\n\t\t\t\t\t    __func__, device->devid, max_avail,\n\t\t\t\t\t    BTRFS_STRIPE_LEN * dev_stripes);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (ndevs == fs_devices->rw_devices) {\n\t\t\tWARN(1, \"%s: found more than %llu devices\\n\",\n\t\t\t     __func__, fs_devices->rw_devices);\n\t\t\tbreak;\n\t\t}\n\t\tdevices_info[ndevs].dev_offset = dev_offset;\n\t\tdevices_info[ndevs].max_avail = max_avail;\n\t\tdevices_info[ndevs].total_avail = total_avail;\n\t\tdevices_info[ndevs].dev = device;\n\t\t++ndevs;\n\t}\n\n\t/*\n\t * now sort the devices by hole size / available space\n\t */\n\tsort(devices_info, ndevs, sizeof(struct btrfs_device_info),\n\t     btrfs_cmp_device_info, NULL);\n\n\t/* round down to number of usable stripes */\n\tndevs = round_down(ndevs, devs_increment);\n\n\tif (ndevs < devs_min) {\n\t\tret = -ENOSPC;\n\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG)) {\n\t\t\tbtrfs_debug(info,\n\t\"%s: not enough devices with free space: have=%d minimum required=%d\",\n\t\t\t\t    __func__, ndevs, devs_min);\n\t\t}\n\t\tgoto error;\n\t}\n\n\tndevs = min(ndevs, devs_max);\n\n\t/*\n\t * The primary goal is to maximize the number of stripes, so use as\n\t * many devices as possible, even if the stripes are not maximum sized.\n\t *\n\t * The DUP profile stores more than one stripe per device, the\n\t * max_avail is the total size so we have to adjust.\n\t */\n\tstripe_size = div_u64(devices_info[ndevs - 1].max_avail, dev_stripes);\n\tnum_stripes = ndevs * dev_stripes;\n\n\t/*\n\t * this will have to be fixed for RAID1 and RAID10 over\n\t * more drives\n\t */\n\tdata_stripes = num_stripes / ncopies;\n\n\tif (type & BTRFS_BLOCK_GROUP_RAID5)\n\t\tdata_stripes = num_stripes - 1;\n\n\tif (type & BTRFS_BLOCK_GROUP_RAID6)\n\t\tdata_stripes = num_stripes - 2;\n\n\t/*\n\t * Use the number of data stripes to figure out how big this chunk\n\t * is really going to be in terms of logical address space,\n\t * and compare that answer with the max chunk size\n\t */\n\tif (stripe_size * data_stripes > max_chunk_size) {\n\t\tstripe_size = div_u64(max_chunk_size, data_stripes);\n\n\t\t/* bump the answer up to a 16MB boundary */\n\t\tstripe_size = round_up(stripe_size, SZ_16M);\n\n\t\t/*\n\t\t * But don't go higher than the limits we found while searching\n\t\t * for free extents\n\t\t */\n\t\tstripe_size = min(devices_info[ndevs - 1].max_avail,\n\t\t\t\t  stripe_size);\n\t}\n\n\t/* align to BTRFS_STRIPE_LEN */\n\tstripe_size = round_down(stripe_size, BTRFS_STRIPE_LEN);\n\n\tmap = kmalloc(map_lookup_size(num_stripes), GFP_NOFS);\n\tif (!map) {\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\tmap->num_stripes = num_stripes;\n\n\tfor (i = 0; i < ndevs; ++i) {\n\t\tfor (j = 0; j < dev_stripes; ++j) {\n\t\t\tint s = i * dev_stripes + j;\n\t\t\tmap->stripes[s].dev = devices_info[i].dev;\n\t\t\tmap->stripes[s].physical = devices_info[i].dev_offset +\n\t\t\t\t\t\t   j * stripe_size;\n\t\t}\n\t}\n\tmap->stripe_len = BTRFS_STRIPE_LEN;\n\tmap->io_align = BTRFS_STRIPE_LEN;\n\tmap->io_width = BTRFS_STRIPE_LEN;\n\tmap->type = type;\n\tmap->sub_stripes = sub_stripes;\n\n\tnum_bytes = stripe_size * data_stripes;\n\n\ttrace_btrfs_chunk_alloc(info, map, start, num_bytes);\n\n\tem = alloc_extent_map();\n\tif (!em) {\n\t\tkfree(map);\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\tset_bit(EXTENT_FLAG_FS_MAPPING, &em->flags);\n\tem->map_lookup = map;\n\tem->start = start;\n\tem->len = num_bytes;\n\tem->block_start = 0;\n\tem->block_len = em->len;\n\tem->orig_block_len = stripe_size;\n\n\tem_tree = &info->mapping_tree.map_tree;\n\twrite_lock(&em_tree->lock);\n\tret = add_extent_mapping(em_tree, em, 0);\n\tif (ret) {\n\t\twrite_unlock(&em_tree->lock);\n\t\tfree_extent_map(em);\n\t\tgoto error;\n\t}\n\n\tlist_add_tail(&em->list, &trans->transaction->pending_chunks);\n\trefcount_inc(&em->refs);\n\twrite_unlock(&em_tree->lock);\n\n\tret = btrfs_make_block_group(trans, 0, type, start, num_bytes);\n\tif (ret)\n\t\tgoto error_del_extent;\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tnum_bytes = map->stripes[i].dev->bytes_used + stripe_size;\n\t\tbtrfs_device_set_bytes_used(map->stripes[i].dev, num_bytes);\n\t}\n\n\tatomic64_sub(stripe_size * map->num_stripes, &info->free_chunk_space);\n\n\tfree_extent_map(em);\n\tcheck_raid56_incompat_flag(info, type);\n\n\tkfree(devices_info);\n\treturn 0;\n\nerror_del_extent:\n\twrite_lock(&em_tree->lock);\n\tremove_extent_mapping(em_tree, em);\n\twrite_unlock(&em_tree->lock);\n\n\t/* One for our allocation */\n\tfree_extent_map(em);\n\t/* One for the tree reference */\n\tfree_extent_map(em);\n\t/* One for the pending_chunks list reference */\n\tfree_extent_map(em);\nerror:\n\tkfree(devices_info);\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int verify_one_dev_extent(struct btrfs_fs_info *fs_info,\n\t\t\t\t u64 chunk_offset, u64 devid,\n\t\t\t\t u64 physical_offset, u64 physical_len)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree.map_tree;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_device *dev;\n\tu64 stripe_len;\n\tbool found = false;\n\tint ret = 0;\n\tint i;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, chunk_offset, 1);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu doesn't have corresponding chunk\",\n\t\t\t  physical_offset, devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tmap = em->map_lookup;\n\tstripe_len = calc_stripe_length(map->type, em->len, map->num_stripes);\n\tif (physical_len != stripe_len) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu length doesn't match chunk %llu, have %llu expect %llu\",\n\t\t\t  physical_offset, devid, em->start, physical_len,\n\t\t\t  stripe_len);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (map->stripes[i].dev->devid == devid &&\n\t\t    map->stripes[i].physical == physical_offset) {\n\t\t\tfound = true;\n\t\t\tif (map->verified_stripes >= map->num_stripes) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"too many dev extents for chunk %llu found\",\n\t\t\t\t\t  em->start);\n\t\t\t\tret = -EUCLEAN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tmap->verified_stripes++;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found) {\n\t\tbtrfs_err(fs_info,\n\t\"dev extent physical offset %llu devid %llu has no corresponding chunk\",\n\t\t\tphysical_offset, devid);\n\t\tret = -EUCLEAN;\n\t}\n\n\t/* Make sure no dev extent is beyond device bondary */\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL);\n\tif (!dev) {\n\t\tbtrfs_err(fs_info, \"failed to find devid %llu\", devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\t/* It's possible this device is a dummy for seed device */\n\tif (dev->disk_total_bytes == 0) {\n\t\tdev = find_device(fs_info->fs_devices->seed, devid, NULL);\n\t\tif (!dev) {\n\t\t\tbtrfs_err(fs_info, \"failed to find seed devid %llu\",\n\t\t\t\t  devid);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (physical_offset + physical_len > dev->disk_total_bytes) {\n\t\tbtrfs_err(fs_info,\n\"dev extent devid %llu physical offset %llu len %llu is beyond device boundary %llu\",\n\t\t\t  devid, physical_offset, physical_len,\n\t\t\t  dev->disk_total_bytes);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\nout:\n\tfree_extent_map(em);\n\treturn ret;\n}",
                        "code_after_change": "static int verify_one_dev_extent(struct btrfs_fs_info *fs_info,\n\t\t\t\t u64 chunk_offset, u64 devid,\n\t\t\t\t u64 physical_offset, u64 physical_len)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree.map_tree;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_device *dev;\n\tu64 stripe_len;\n\tbool found = false;\n\tint ret = 0;\n\tint i;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, chunk_offset, 1);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu doesn't have corresponding chunk\",\n\t\t\t  physical_offset, devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tmap = em->map_lookup;\n\tstripe_len = calc_stripe_length(map->type, em->len, map->num_stripes);\n\tif (physical_len != stripe_len) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu length doesn't match chunk %llu, have %llu expect %llu\",\n\t\t\t  physical_offset, devid, em->start, physical_len,\n\t\t\t  stripe_len);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (map->stripes[i].dev->devid == devid &&\n\t\t    map->stripes[i].physical == physical_offset) {\n\t\t\tfound = true;\n\t\t\tif (map->verified_stripes >= map->num_stripes) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"too many dev extents for chunk %llu found\",\n\t\t\t\t\t  em->start);\n\t\t\t\tret = -EUCLEAN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tmap->verified_stripes++;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found) {\n\t\tbtrfs_err(fs_info,\n\t\"dev extent physical offset %llu devid %llu has no corresponding chunk\",\n\t\t\tphysical_offset, devid);\n\t\tret = -EUCLEAN;\n\t}\n\n\t/* Make sure no dev extent is beyond device bondary */\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL, true);\n\tif (!dev) {\n\t\tbtrfs_err(fs_info, \"failed to find devid %llu\", devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\t/* It's possible this device is a dummy for seed device */\n\tif (dev->disk_total_bytes == 0) {\n\t\tdev = btrfs_find_device(fs_info->fs_devices->seed, devid, NULL,\n\t\t\t\t\tNULL, false);\n\t\tif (!dev) {\n\t\t\tbtrfs_err(fs_info, \"failed to find seed devid %llu\",\n\t\t\t\t  devid);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (physical_offset + physical_len > dev->disk_total_bytes) {\n\t\tbtrfs_err(fs_info,\n\"dev extent devid %llu physical offset %llu len %llu is beyond device boundary %llu\",\n\t\t\t  devid, physical_offset, physical_len,\n\t\t\t  dev->disk_total_bytes);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\nout:\n\tfree_extent_map(em);\n\treturn ret;\n}",
                        "cve_id": "CVE-2019-18885"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1777,
            "cve_id": "CVE-2018-14614",
            "code_snippet": "int f2fs_sanity_check_ckpt(struct f2fs_sb_info *sbi)\n{\n\tunsigned int total, fsmeta;\n\tstruct f2fs_super_block *raw_super = F2FS_RAW_SUPER(sbi);\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tunsigned int ovp_segments, reserved_segments;\n\tunsigned int main_segs, blocks_per_seg;\n\tunsigned int sit_segs, nat_segs;\n\tunsigned int sit_bitmap_size, nat_bitmap_size;\n\tunsigned int log_blocks_per_seg;\n\tunsigned int segment_count_main;\n\tblock_t user_block_count;\n\tint i;\n\n\ttotal = le32_to_cpu(raw_super->segment_count);\n\tfsmeta = le32_to_cpu(raw_super->segment_count_ckpt);\n\tsit_segs = le32_to_cpu(raw_super->segment_count_sit);\n\tfsmeta += sit_segs;\n\tnat_segs = le32_to_cpu(raw_super->segment_count_nat);\n\tfsmeta += nat_segs;\n\tfsmeta += le32_to_cpu(ckpt->rsvd_segment_count);\n\tfsmeta += le32_to_cpu(raw_super->segment_count_ssa);\n\n\tif (unlikely(fsmeta >= total))\n\t\treturn 1;\n\n\tovp_segments = le32_to_cpu(ckpt->overprov_segment_count);\n\treserved_segments = le32_to_cpu(ckpt->rsvd_segment_count);\n\n\tif (unlikely(fsmeta < F2FS_MIN_SEGMENTS ||\n\t\t\tovp_segments == 0 || reserved_segments == 0)) {\n\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\"Wrong layout: check mkfs.f2fs version\");\n\t\treturn 1;\n\t}\n\n\tuser_block_count = le64_to_cpu(ckpt->user_block_count);\n\tsegment_count_main = le32_to_cpu(raw_super->segment_count_main);\n\tlog_blocks_per_seg = le32_to_cpu(raw_super->log_blocks_per_seg);\n\tif (!user_block_count || user_block_count >=\n\t\t\tsegment_count_main << log_blocks_per_seg) {\n\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\"Wrong user_block_count: %u\", user_block_count);\n\t\treturn 1;\n\t}\n\n\tmain_segs = le32_to_cpu(raw_super->segment_count_main);\n\tblocks_per_seg = sbi->blocks_per_seg;\n\n\tfor (i = 0; i < NR_CURSEG_NODE_TYPE; i++) {\n\t\tif (le32_to_cpu(ckpt->cur_node_segno[i]) >= main_segs ||\n\t\t\tle16_to_cpu(ckpt->cur_node_blkoff[i]) >= blocks_per_seg)\n\t\t\treturn 1;\n\t}\n\tfor (i = 0; i < NR_CURSEG_DATA_TYPE; i++) {\n\t\tif (le32_to_cpu(ckpt->cur_data_segno[i]) >= main_segs ||\n\t\t\tle16_to_cpu(ckpt->cur_data_blkoff[i]) >= blocks_per_seg)\n\t\t\treturn 1;\n\t}\n\n\tsit_bitmap_size = le32_to_cpu(ckpt->sit_ver_bitmap_bytesize);\n\tnat_bitmap_size = le32_to_cpu(ckpt->nat_ver_bitmap_bytesize);\n\n\tif (sit_bitmap_size != ((sit_segs / 2) << log_blocks_per_seg) / 8 ||\n\t\tnat_bitmap_size != ((nat_segs / 2) << log_blocks_per_seg) / 8) {\n\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\"Wrong bitmap size: sit: %u, nat:%u\",\n\t\t\tsit_bitmap_size, nat_bitmap_size);\n\t\treturn 1;\n\t}\n\n\tif (unlikely(f2fs_cp_error(sbi))) {\n\t\tf2fs_msg(sbi->sb, KERN_ERR, \"A bug case: need to run fsck\");\n\t\treturn 1;\n\t}\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int f2fs_get_valid_checkpoint(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_checkpoint *cp_block;\n\tstruct f2fs_super_block *fsb = sbi->raw_super;\n\tstruct page *cp1, *cp2, *cur_page;\n\tunsigned long blk_size = sbi->blocksize;\n\tunsigned long long cp1_version = 0, cp2_version = 0;\n\tunsigned long long cp_start_blk_no;\n\tunsigned int cp_blks = 1 + __cp_payload(sbi);\n\tblock_t cp_blk_no;\n\tint i;\n\n\tsbi->ckpt = f2fs_kzalloc(sbi, array_size(blk_size, cp_blks),\n\t\t\t\t GFP_KERNEL);\n\tif (!sbi->ckpt)\n\t\treturn -ENOMEM;\n\t/*\n\t * Finding out valid cp block involves read both\n\t * sets( cp pack1 and cp pack 2)\n\t */\n\tcp_start_blk_no = le32_to_cpu(fsb->cp_blkaddr);\n\tcp1 = validate_checkpoint(sbi, cp_start_blk_no, &cp1_version);\n\n\t/* The second checkpoint pack should start at the next segment */\n\tcp_start_blk_no += ((unsigned long long)1) <<\n\t\t\t\tle32_to_cpu(fsb->log_blocks_per_seg);\n\tcp2 = validate_checkpoint(sbi, cp_start_blk_no, &cp2_version);\n\n\tif (cp1 && cp2) {\n\t\tif (ver_after(cp2_version, cp1_version))\n\t\t\tcur_page = cp2;\n\t\telse\n\t\t\tcur_page = cp1;\n\t} else if (cp1) {\n\t\tcur_page = cp1;\n\t} else if (cp2) {\n\t\tcur_page = cp2;\n\t} else {\n\t\tgoto fail_no_cp;\n\t}\n\n\tcp_block = (struct f2fs_checkpoint *)page_address(cur_page);\n\tmemcpy(sbi->ckpt, cp_block, blk_size);\n\n\t/* Sanity checking of checkpoint */\n\tif (f2fs_sanity_check_ckpt(sbi))\n\t\tgoto free_fail_no_cp;\n\n\tif (cur_page == cp1)\n\t\tsbi->cur_cp_pack = 1;\n\telse\n\t\tsbi->cur_cp_pack = 2;\n\n\tif (cp_blks <= 1)\n\t\tgoto done;\n\n\tcp_blk_no = le32_to_cpu(fsb->cp_blkaddr);\n\tif (cur_page == cp2)\n\t\tcp_blk_no += 1 << le32_to_cpu(fsb->log_blocks_per_seg);\n\n\tfor (i = 1; i < cp_blks; i++) {\n\t\tvoid *sit_bitmap_ptr;\n\t\tunsigned char *ckpt = (unsigned char *)sbi->ckpt;\n\n\t\tcur_page = f2fs_get_meta_page(sbi, cp_blk_no + i);\n\t\tif (IS_ERR(cur_page))\n\t\t\tgoto free_fail_no_cp;\n\t\tsit_bitmap_ptr = page_address(cur_page);\n\t\tmemcpy(ckpt + i * blk_size, sit_bitmap_ptr, blk_size);\n\t\tf2fs_put_page(cur_page, 1);\n\t}\ndone:\n\tf2fs_put_page(cp1, 1);\n\tf2fs_put_page(cp2, 1);\n\treturn 0;\n\nfree_fail_no_cp:\n\tf2fs_put_page(cp1, 1);\n\tf2fs_put_page(cp2, 1);\nfail_no_cp:\n\tkfree(sbi->ckpt);\n\treturn -EINVAL;\n}",
                        "code_after_change": "int f2fs_get_valid_checkpoint(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_checkpoint *cp_block;\n\tstruct f2fs_super_block *fsb = sbi->raw_super;\n\tstruct page *cp1, *cp2, *cur_page;\n\tunsigned long blk_size = sbi->blocksize;\n\tunsigned long long cp1_version = 0, cp2_version = 0;\n\tunsigned long long cp_start_blk_no;\n\tunsigned int cp_blks = 1 + __cp_payload(sbi);\n\tblock_t cp_blk_no;\n\tint i;\n\n\tsbi->ckpt = f2fs_kzalloc(sbi, array_size(blk_size, cp_blks),\n\t\t\t\t GFP_KERNEL);\n\tif (!sbi->ckpt)\n\t\treturn -ENOMEM;\n\t/*\n\t * Finding out valid cp block involves read both\n\t * sets( cp pack1 and cp pack 2)\n\t */\n\tcp_start_blk_no = le32_to_cpu(fsb->cp_blkaddr);\n\tcp1 = validate_checkpoint(sbi, cp_start_blk_no, &cp1_version);\n\n\t/* The second checkpoint pack should start at the next segment */\n\tcp_start_blk_no += ((unsigned long long)1) <<\n\t\t\t\tle32_to_cpu(fsb->log_blocks_per_seg);\n\tcp2 = validate_checkpoint(sbi, cp_start_blk_no, &cp2_version);\n\n\tif (cp1 && cp2) {\n\t\tif (ver_after(cp2_version, cp1_version))\n\t\t\tcur_page = cp2;\n\t\telse\n\t\t\tcur_page = cp1;\n\t} else if (cp1) {\n\t\tcur_page = cp1;\n\t} else if (cp2) {\n\t\tcur_page = cp2;\n\t} else {\n\t\tgoto fail_no_cp;\n\t}\n\n\tcp_block = (struct f2fs_checkpoint *)page_address(cur_page);\n\tmemcpy(sbi->ckpt, cp_block, blk_size);\n\n\tif (cur_page == cp1)\n\t\tsbi->cur_cp_pack = 1;\n\telse\n\t\tsbi->cur_cp_pack = 2;\n\n\t/* Sanity checking of checkpoint */\n\tif (f2fs_sanity_check_ckpt(sbi))\n\t\tgoto free_fail_no_cp;\n\n\tif (cp_blks <= 1)\n\t\tgoto done;\n\n\tcp_blk_no = le32_to_cpu(fsb->cp_blkaddr);\n\tif (cur_page == cp2)\n\t\tcp_blk_no += 1 << le32_to_cpu(fsb->log_blocks_per_seg);\n\n\tfor (i = 1; i < cp_blks; i++) {\n\t\tvoid *sit_bitmap_ptr;\n\t\tunsigned char *ckpt = (unsigned char *)sbi->ckpt;\n\n\t\tcur_page = f2fs_get_meta_page(sbi, cp_blk_no + i);\n\t\tif (IS_ERR(cur_page))\n\t\t\tgoto free_fail_no_cp;\n\t\tsit_bitmap_ptr = page_address(cur_page);\n\t\tmemcpy(ckpt + i * blk_size, sit_bitmap_ptr, blk_size);\n\t\tf2fs_put_page(cur_page, 1);\n\t}\ndone:\n\tf2fs_put_page(cp1, 1);\n\tf2fs_put_page(cp2, 1);\n\treturn 0;\n\nfree_fail_no_cp:\n\tf2fs_put_page(cp1, 1);\n\tf2fs_put_page(cp2, 1);\nfail_no_cp:\n\tkfree(sbi->ckpt);\n\treturn -EINVAL;\n}",
                        "cve_id": "CVE-2018-14614"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1780,
            "cve_id": "CVE-2018-14616",
            "code_snippet": "static int do_read_inode(struct inode *inode)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\tstruct page *node_page;\n\tstruct f2fs_inode *ri;\n\tprojid_t i_projid;\n\n\t/* Check if ino is within scope */\n\tif (f2fs_check_nid_range(sbi, inode->i_ino))\n\t\treturn -EINVAL;\n\n\tnode_page = f2fs_get_node_page(sbi, inode->i_ino);\n\tif (IS_ERR(node_page))\n\t\treturn PTR_ERR(node_page);\n\n\tri = F2FS_INODE(node_page);\n\n\tinode->i_mode = le16_to_cpu(ri->i_mode);\n\ti_uid_write(inode, le32_to_cpu(ri->i_uid));\n\ti_gid_write(inode, le32_to_cpu(ri->i_gid));\n\tset_nlink(inode, le32_to_cpu(ri->i_links));\n\tinode->i_size = le64_to_cpu(ri->i_size);\n\tinode->i_blocks = SECTOR_FROM_BLOCK(le64_to_cpu(ri->i_blocks) - 1);\n\n\tinode->i_atime.tv_sec = le64_to_cpu(ri->i_atime);\n\tinode->i_ctime.tv_sec = le64_to_cpu(ri->i_ctime);\n\tinode->i_mtime.tv_sec = le64_to_cpu(ri->i_mtime);\n\tinode->i_atime.tv_nsec = le32_to_cpu(ri->i_atime_nsec);\n\tinode->i_ctime.tv_nsec = le32_to_cpu(ri->i_ctime_nsec);\n\tinode->i_mtime.tv_nsec = le32_to_cpu(ri->i_mtime_nsec);\n\tinode->i_generation = le32_to_cpu(ri->i_generation);\n\tif (S_ISDIR(inode->i_mode))\n\t\tfi->i_current_depth = le32_to_cpu(ri->i_current_depth);\n\telse if (S_ISREG(inode->i_mode))\n\t\tfi->i_gc_failures[GC_FAILURE_PIN] =\n\t\t\t\t\tle16_to_cpu(ri->i_gc_failures);\n\tfi->i_xattr_nid = le32_to_cpu(ri->i_xattr_nid);\n\tfi->i_flags = le32_to_cpu(ri->i_flags);\n\tfi->flags = 0;\n\tfi->i_advise = ri->i_advise;\n\tfi->i_pino = le32_to_cpu(ri->i_pino);\n\tfi->i_dir_level = ri->i_dir_level;\n\n\tif (f2fs_init_extent_tree(inode, &ri->i_ext))\n\t\tset_page_dirty(node_page);\n\n\tget_inline_info(inode, ri);\n\n\tfi->i_extra_isize = f2fs_has_extra_attr(inode) ?\n\t\t\t\t\tle16_to_cpu(ri->i_extra_isize) : 0;\n\n\tif (f2fs_sb_has_flexible_inline_xattr(sbi->sb)) {\n\t\tfi->i_inline_xattr_size = le16_to_cpu(ri->i_inline_xattr_size);\n\t} else if (f2fs_has_inline_xattr(inode) ||\n\t\t\t\tf2fs_has_inline_dentry(inode)) {\n\t\tfi->i_inline_xattr_size = DEFAULT_INLINE_XATTR_ADDRS;\n\t} else {\n\n\t\t/*\n\t\t * Previous inline data or directory always reserved 200 bytes\n\t\t * in inode layout, even if inline_xattr is disabled. In order\n\t\t * to keep inline_dentry's structure for backward compatibility,\n\t\t * we get the space back only from inline_data.\n\t\t */\n\t\tfi->i_inline_xattr_size = 0;\n\t}\n\n\tif (!sanity_check_inode(inode, node_page)) {\n\t\tf2fs_put_page(node_page, 1);\n\t\treturn -EINVAL;\n\t}\n\n\t/* check data exist */\n\tif (f2fs_has_inline_data(inode) && !f2fs_exist_data(inode))\n\t\t__recover_inline_status(inode, node_page);\n\n\t/* get rdev by using inline_info */\n\t__get_inode_rdev(inode, ri);\n\n\tif (__written_first_block(sbi, ri))\n\t\tset_inode_flag(inode, FI_FIRST_BLOCK_WRITTEN);\n\n\tif (!f2fs_need_inode_block_update(sbi, inode->i_ino))\n\t\tfi->last_disk_size = inode->i_size;\n\n\tif (fi->i_flags & F2FS_PROJINHERIT_FL)\n\t\tset_inode_flag(inode, FI_PROJ_INHERIT);\n\n\tif (f2fs_has_extra_attr(inode) && f2fs_sb_has_project_quota(sbi->sb) &&\n\t\t\tF2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_projid))\n\t\ti_projid = (projid_t)le32_to_cpu(ri->i_projid);\n\telse\n\t\ti_projid = F2FS_DEF_PROJID;\n\tfi->i_projid = make_kprojid(&init_user_ns, i_projid);\n\n\tif (f2fs_has_extra_attr(inode) && f2fs_sb_has_inode_crtime(sbi->sb) &&\n\t\t\tF2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_crtime)) {\n\t\tfi->i_crtime.tv_sec = le64_to_cpu(ri->i_crtime);\n\t\tfi->i_crtime.tv_nsec = le32_to_cpu(ri->i_crtime_nsec);\n\t}\n\n\tF2FS_I(inode)->i_disk_time[0] = inode->i_atime;\n\tF2FS_I(inode)->i_disk_time[1] = inode->i_ctime;\n\tF2FS_I(inode)->i_disk_time[2] = inode->i_mtime;\n\tF2FS_I(inode)->i_disk_time[3] = F2FS_I(inode)->i_crtime;\n\tf2fs_put_page(node_page, 1);\n\n\tstat_inc_inline_xattr(inode);\n\tstat_inc_inline_inode(inode);\n\tstat_inc_inline_dir(inode);\n\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,\n\t\tstruct gc_inode_list *gc_list, unsigned int segno, int gc_type,\n\t\tbool force_migrate)\n{\n\tstruct super_block *sb = sbi->sb;\n\tstruct f2fs_summary *entry;\n\tblock_t start_addr;\n\tint off;\n\tint phase = 0;\n\tint submitted = 0;\n\tunsigned int usable_blks_in_seg = f2fs_usable_blks_in_seg(sbi, segno);\n\n\tstart_addr = START_BLOCK(sbi, segno);\n\nnext_step:\n\tentry = sum;\n\n\tfor (off = 0; off < usable_blks_in_seg; off++, entry++) {\n\t\tstruct page *data_page;\n\t\tstruct inode *inode;\n\t\tstruct node_info dni; /* dnode info for the data */\n\t\tunsigned int ofs_in_node, nofs;\n\t\tblock_t start_bidx;\n\t\tnid_t nid = le32_to_cpu(entry->nid);\n\n\t\t/*\n\t\t * stop BG_GC if there is not enough free sections.\n\t\t * Or, stop GC if the segment becomes fully valid caused by\n\t\t * race condition along with SSR block allocation.\n\t\t */\n\t\tif ((gc_type == BG_GC && has_not_enough_free_secs(sbi, 0, 0)) ||\n\t\t\t(!force_migrate && get_valid_blocks(sbi, segno, true) ==\n\t\t\t\t\t\t\tBLKS_PER_SEC(sbi)))\n\t\t\treturn submitted;\n\n\t\tif (check_valid_map(sbi, segno, off) == 0)\n\t\t\tcontinue;\n\n\t\tif (phase == 0) {\n\t\t\tf2fs_ra_meta_pages(sbi, NAT_BLOCK_OFFSET(nid), 1,\n\t\t\t\t\t\t\tMETA_NAT, true);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (phase == 1) {\n\t\t\tf2fs_ra_node_page(sbi, nid);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Get an inode by ino with checking validity */\n\t\tif (!is_alive(sbi, entry, &dni, start_addr + off, &nofs))\n\t\t\tcontinue;\n\n\t\tif (phase == 2) {\n\t\t\tf2fs_ra_node_page(sbi, dni.ino);\n\t\t\tcontinue;\n\t\t}\n\n\t\tofs_in_node = le16_to_cpu(entry->ofs_in_node);\n\n\t\tif (phase == 3) {\n\t\t\tinode = f2fs_iget(sb, dni.ino);\n\t\t\tif (IS_ERR(inode) || is_bad_inode(inode))\n\t\t\t\tcontinue;\n\n\t\t\tif (!down_write_trylock(\n\t\t\t\t&F2FS_I(inode)->i_gc_rwsem[WRITE])) {\n\t\t\t\tiput(inode);\n\t\t\t\tsbi->skipped_gc_rwsem++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tstart_bidx = f2fs_start_bidx_of_node(nofs, inode) +\n\t\t\t\t\t\t\t\tofs_in_node;\n\n\t\t\tif (f2fs_post_read_required(inode)) {\n\t\t\t\tint err = ra_data_block(inode, start_bidx);\n\n\t\t\t\tup_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);\n\t\t\t\tif (err) {\n\t\t\t\t\tiput(inode);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tadd_gc_inode(gc_list, inode);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tdata_page = f2fs_get_read_data_page(inode,\n\t\t\t\t\t\tstart_bidx, REQ_RAHEAD, true);\n\t\t\tup_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);\n\t\t\tif (IS_ERR(data_page)) {\n\t\t\t\tiput(inode);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tf2fs_put_page(data_page, 0);\n\t\t\tadd_gc_inode(gc_list, inode);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* phase 4 */\n\t\tinode = find_gc_inode(gc_list, dni.ino);\n\t\tif (inode) {\n\t\t\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\t\t\tbool locked = false;\n\t\t\tint err;\n\n\t\t\tif (S_ISREG(inode->i_mode)) {\n\t\t\t\tif (!down_write_trylock(&fi->i_gc_rwsem[READ])) {\n\t\t\t\t\tsbi->skipped_gc_rwsem++;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tif (!down_write_trylock(\n\t\t\t\t\t\t&fi->i_gc_rwsem[WRITE])) {\n\t\t\t\t\tsbi->skipped_gc_rwsem++;\n\t\t\t\t\tup_write(&fi->i_gc_rwsem[READ]);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tlocked = true;\n\n\t\t\t\t/* wait for all inflight aio data */\n\t\t\t\tinode_dio_wait(inode);\n\t\t\t}\n\n\t\t\tstart_bidx = f2fs_start_bidx_of_node(nofs, inode)\n\t\t\t\t\t\t\t\t+ ofs_in_node;\n\t\t\tif (f2fs_post_read_required(inode))\n\t\t\t\terr = move_data_block(inode, start_bidx,\n\t\t\t\t\t\t\tgc_type, segno, off);\n\t\t\telse\n\t\t\t\terr = move_data_page(inode, start_bidx, gc_type,\n\t\t\t\t\t\t\t\tsegno, off);\n\n\t\t\tif (!err && (gc_type == FG_GC ||\n\t\t\t\t\tf2fs_post_read_required(inode)))\n\t\t\t\tsubmitted++;\n\n\t\t\tif (locked) {\n\t\t\t\tup_write(&fi->i_gc_rwsem[WRITE]);\n\t\t\t\tup_write(&fi->i_gc_rwsem[READ]);\n\t\t\t}\n\n\t\t\tstat_inc_data_blk_count(sbi, 1, gc_type);\n\t\t}\n\t}\n\n\tif (++phase < 5)\n\t\tgoto next_step;\n\n\treturn submitted;\n}",
                        "code_after_change": "static int gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,\n\t\tstruct gc_inode_list *gc_list, unsigned int segno, int gc_type,\n\t\tbool force_migrate)\n{\n\tstruct super_block *sb = sbi->sb;\n\tstruct f2fs_summary *entry;\n\tblock_t start_addr;\n\tint off;\n\tint phase = 0;\n\tint submitted = 0;\n\tunsigned int usable_blks_in_seg = f2fs_usable_blks_in_seg(sbi, segno);\n\n\tstart_addr = START_BLOCK(sbi, segno);\n\nnext_step:\n\tentry = sum;\n\n\tfor (off = 0; off < usable_blks_in_seg; off++, entry++) {\n\t\tstruct page *data_page;\n\t\tstruct inode *inode;\n\t\tstruct node_info dni; /* dnode info for the data */\n\t\tunsigned int ofs_in_node, nofs;\n\t\tblock_t start_bidx;\n\t\tnid_t nid = le32_to_cpu(entry->nid);\n\n\t\t/*\n\t\t * stop BG_GC if there is not enough free sections.\n\t\t * Or, stop GC if the segment becomes fully valid caused by\n\t\t * race condition along with SSR block allocation.\n\t\t */\n\t\tif ((gc_type == BG_GC && has_not_enough_free_secs(sbi, 0, 0)) ||\n\t\t\t(!force_migrate && get_valid_blocks(sbi, segno, true) ==\n\t\t\t\t\t\t\tBLKS_PER_SEC(sbi)))\n\t\t\treturn submitted;\n\n\t\tif (check_valid_map(sbi, segno, off) == 0)\n\t\t\tcontinue;\n\n\t\tif (phase == 0) {\n\t\t\tf2fs_ra_meta_pages(sbi, NAT_BLOCK_OFFSET(nid), 1,\n\t\t\t\t\t\t\tMETA_NAT, true);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (phase == 1) {\n\t\t\tf2fs_ra_node_page(sbi, nid);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Get an inode by ino with checking validity */\n\t\tif (!is_alive(sbi, entry, &dni, start_addr + off, &nofs))\n\t\t\tcontinue;\n\n\t\tif (phase == 2) {\n\t\t\tf2fs_ra_node_page(sbi, dni.ino);\n\t\t\tcontinue;\n\t\t}\n\n\t\tofs_in_node = le16_to_cpu(entry->ofs_in_node);\n\n\t\tif (phase == 3) {\n\t\t\tinode = f2fs_iget(sb, dni.ino);\n\t\t\tif (IS_ERR(inode) || is_bad_inode(inode) ||\n\t\t\t\t\tspecial_file(inode->i_mode))\n\t\t\t\tcontinue;\n\n\t\t\tif (!down_write_trylock(\n\t\t\t\t&F2FS_I(inode)->i_gc_rwsem[WRITE])) {\n\t\t\t\tiput(inode);\n\t\t\t\tsbi->skipped_gc_rwsem++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tstart_bidx = f2fs_start_bidx_of_node(nofs, inode) +\n\t\t\t\t\t\t\t\tofs_in_node;\n\n\t\t\tif (f2fs_post_read_required(inode)) {\n\t\t\t\tint err = ra_data_block(inode, start_bidx);\n\n\t\t\t\tup_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);\n\t\t\t\tif (err) {\n\t\t\t\t\tiput(inode);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tadd_gc_inode(gc_list, inode);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tdata_page = f2fs_get_read_data_page(inode,\n\t\t\t\t\t\tstart_bidx, REQ_RAHEAD, true);\n\t\t\tup_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);\n\t\t\tif (IS_ERR(data_page)) {\n\t\t\t\tiput(inode);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tf2fs_put_page(data_page, 0);\n\t\t\tadd_gc_inode(gc_list, inode);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* phase 4 */\n\t\tinode = find_gc_inode(gc_list, dni.ino);\n\t\tif (inode) {\n\t\t\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\t\t\tbool locked = false;\n\t\t\tint err;\n\n\t\t\tif (S_ISREG(inode->i_mode)) {\n\t\t\t\tif (!down_write_trylock(&fi->i_gc_rwsem[READ])) {\n\t\t\t\t\tsbi->skipped_gc_rwsem++;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tif (!down_write_trylock(\n\t\t\t\t\t\t&fi->i_gc_rwsem[WRITE])) {\n\t\t\t\t\tsbi->skipped_gc_rwsem++;\n\t\t\t\t\tup_write(&fi->i_gc_rwsem[READ]);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tlocked = true;\n\n\t\t\t\t/* wait for all inflight aio data */\n\t\t\t\tinode_dio_wait(inode);\n\t\t\t}\n\n\t\t\tstart_bidx = f2fs_start_bidx_of_node(nofs, inode)\n\t\t\t\t\t\t\t\t+ ofs_in_node;\n\t\t\tif (f2fs_post_read_required(inode))\n\t\t\t\terr = move_data_block(inode, start_bidx,\n\t\t\t\t\t\t\tgc_type, segno, off);\n\t\t\telse\n\t\t\t\terr = move_data_page(inode, start_bidx, gc_type,\n\t\t\t\t\t\t\t\tsegno, off);\n\n\t\t\tif (!err && (gc_type == FG_GC ||\n\t\t\t\t\tf2fs_post_read_required(inode)))\n\t\t\t\tsubmitted++;\n\n\t\t\tif (locked) {\n\t\t\t\tup_write(&fi->i_gc_rwsem[WRITE]);\n\t\t\t\tup_write(&fi->i_gc_rwsem[READ]);\n\t\t\t}\n\n\t\t\tstat_inc_data_blk_count(sbi, 1, gc_type);\n\t\t}\n\t}\n\n\tif (++phase < 5)\n\t\tgoto next_step;\n\n\treturn submitted;\n}",
                        "cve_id": "CVE-2021-44879"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "struct inode *ext4_iget(struct super_block *sb, unsigned long ino)\n{\n\tstruct ext4_iloc iloc;\n\tstruct ext4_inode *raw_inode;\n\tstruct ext4_inode_info *ei;\n\tstruct inode *inode;\n\tjournal_t *journal = EXT4_SB(sb)->s_journal;\n\tlong ret;\n\tloff_t size;\n\tint block;\n\tuid_t i_uid;\n\tgid_t i_gid;\n\tprojid_t i_projid;\n\n\tinode = iget_locked(sb, ino);\n\tif (!inode)\n\t\treturn ERR_PTR(-ENOMEM);\n\tif (!(inode->i_state & I_NEW))\n\t\treturn inode;\n\n\tei = EXT4_I(inode);\n\tiloc.bh = NULL;\n\n\tret = __ext4_get_inode_loc(inode, &iloc, 0);\n\tif (ret < 0)\n\t\tgoto bad_inode;\n\traw_inode = ext4_raw_inode(&iloc);\n\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\tei->i_extra_isize = le16_to_cpu(raw_inode->i_extra_isize);\n\t\tif (EXT4_GOOD_OLD_INODE_SIZE + ei->i_extra_isize >\n\t\t\tEXT4_INODE_SIZE(inode->i_sb) ||\n\t\t    (ei->i_extra_isize & 3)) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"bad extra_isize %u (inode size %u)\",\n\t\t\t\t\t ei->i_extra_isize,\n\t\t\t\t\t EXT4_INODE_SIZE(inode->i_sb));\n\t\t\tret = -EFSCORRUPTED;\n\t\t\tgoto bad_inode;\n\t\t}\n\t} else\n\t\tei->i_extra_isize = 0;\n\n\t/* Precompute checksum seed for inode metadata */\n\tif (ext4_has_metadata_csum(sb)) {\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t\t__u32 csum;\n\t\t__le32 inum = cpu_to_le32(inode->i_ino);\n\t\t__le32 gen = raw_inode->i_generation;\n\t\tcsum = ext4_chksum(sbi, sbi->s_csum_seed, (__u8 *)&inum,\n\t\t\t\t   sizeof(inum));\n\t\tei->i_csum_seed = ext4_chksum(sbi, csum, (__u8 *)&gen,\n\t\t\t\t\t      sizeof(gen));\n\t}\n\n\tif (!ext4_inode_csum_verify(inode, raw_inode, ei)) {\n\t\tEXT4_ERROR_INODE(inode, \"checksum invalid\");\n\t\tret = -EFSBADCRC;\n\t\tgoto bad_inode;\n\t}\n\n\tinode->i_mode = le16_to_cpu(raw_inode->i_mode);\n\ti_uid = (uid_t)le16_to_cpu(raw_inode->i_uid_low);\n\ti_gid = (gid_t)le16_to_cpu(raw_inode->i_gid_low);\n\tif (ext4_has_feature_project(sb) &&\n\t    EXT4_INODE_SIZE(sb) > EXT4_GOOD_OLD_INODE_SIZE &&\n\t    EXT4_FITS_IN_INODE(raw_inode, ei, i_projid))\n\t\ti_projid = (projid_t)le32_to_cpu(raw_inode->i_projid);\n\telse\n\t\ti_projid = EXT4_DEF_PROJID;\n\n\tif (!(test_opt(inode->i_sb, NO_UID32))) {\n\t\ti_uid |= le16_to_cpu(raw_inode->i_uid_high) << 16;\n\t\ti_gid |= le16_to_cpu(raw_inode->i_gid_high) << 16;\n\t}\n\ti_uid_write(inode, i_uid);\n\ti_gid_write(inode, i_gid);\n\tei->i_projid = make_kprojid(&init_user_ns, i_projid);\n\tset_nlink(inode, le16_to_cpu(raw_inode->i_links_count));\n\n\text4_clear_state_flags(ei);\t/* Only relevant on 32-bit archs */\n\tei->i_inline_off = 0;\n\tei->i_dir_start_lookup = 0;\n\tei->i_dtime = le32_to_cpu(raw_inode->i_dtime);\n\t/* We now have enough fields to check if the inode was active or not.\n\t * This is needed because nfsd might try to access dead inodes\n\t * the test is that same one that e2fsck uses\n\t * NeilBrown 1999oct15\n\t */\n\tif (inode->i_nlink == 0) {\n\t\tif ((inode->i_mode == 0 ||\n\t\t     !(EXT4_SB(inode->i_sb)->s_mount_state & EXT4_ORPHAN_FS)) &&\n\t\t    ino != EXT4_BOOT_LOADER_INO) {\n\t\t\t/* this inode is deleted */\n\t\t\tret = -ESTALE;\n\t\t\tgoto bad_inode;\n\t\t}\n\t\t/* The only unlinked inodes we let through here have\n\t\t * valid i_mode and are being read by the orphan\n\t\t * recovery code: that's fine, we're about to complete\n\t\t * the process of deleting those.\n\t\t * OR it is the EXT4_BOOT_LOADER_INO which is\n\t\t * not initialized on a new filesystem. */\n\t}\n\tei->i_flags = le32_to_cpu(raw_inode->i_flags);\n\tinode->i_blocks = ext4_inode_blocks(raw_inode, ei);\n\tei->i_file_acl = le32_to_cpu(raw_inode->i_file_acl_lo);\n\tif (ext4_has_feature_64bit(sb))\n\t\tei->i_file_acl |=\n\t\t\t((__u64)le16_to_cpu(raw_inode->i_file_acl_high)) << 32;\n\tinode->i_size = ext4_isize(sb, raw_inode);\n\tif ((size = i_size_read(inode)) < 0) {\n\t\tEXT4_ERROR_INODE(inode, \"bad i_size value: %lld\", size);\n\t\tret = -EFSCORRUPTED;\n\t\tgoto bad_inode;\n\t}\n\tei->i_disksize = inode->i_size;\n#ifdef CONFIG_QUOTA\n\tei->i_reserved_quota = 0;\n#endif\n\tinode->i_generation = le32_to_cpu(raw_inode->i_generation);\n\tei->i_block_group = iloc.block_group;\n\tei->i_last_alloc_group = ~0;\n\t/*\n\t * NOTE! The in-memory inode i_data array is in little-endian order\n\t * even on big-endian machines: we do NOT byteswap the block numbers!\n\t */\n\tfor (block = 0; block < EXT4_N_BLOCKS; block++)\n\t\tei->i_data[block] = raw_inode->i_block[block];\n\tINIT_LIST_HEAD(&ei->i_orphan);\n\n\t/*\n\t * Set transaction id's of transactions that have to be committed\n\t * to finish f[data]sync. We set them to currently running transaction\n\t * as we cannot be sure that the inode or some of its metadata isn't\n\t * part of the transaction - the inode could have been reclaimed and\n\t * now it is reread from disk.\n\t */\n\tif (journal) {\n\t\ttransaction_t *transaction;\n\t\ttid_t tid;\n\n\t\tread_lock(&journal->j_state_lock);\n\t\tif (journal->j_running_transaction)\n\t\t\ttransaction = journal->j_running_transaction;\n\t\telse\n\t\t\ttransaction = journal->j_committing_transaction;\n\t\tif (transaction)\n\t\t\ttid = transaction->t_tid;\n\t\telse\n\t\t\ttid = journal->j_commit_sequence;\n\t\tread_unlock(&journal->j_state_lock);\n\t\tei->i_sync_tid = tid;\n\t\tei->i_datasync_tid = tid;\n\t}\n\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\tif (ei->i_extra_isize == 0) {\n\t\t\t/* The extra space is currently unused. Use it. */\n\t\t\tBUILD_BUG_ON(sizeof(struct ext4_inode) & 3);\n\t\t\tei->i_extra_isize = sizeof(struct ext4_inode) -\n\t\t\t\t\t    EXT4_GOOD_OLD_INODE_SIZE;\n\t\t} else {\n\t\t\text4_iget_extra_inode(inode, raw_inode, ei);\n\t\t}\n\t}\n\n\tEXT4_INODE_GET_XTIME(i_ctime, inode, raw_inode);\n\tEXT4_INODE_GET_XTIME(i_mtime, inode, raw_inode);\n\tEXT4_INODE_GET_XTIME(i_atime, inode, raw_inode);\n\tEXT4_EINODE_GET_XTIME(i_crtime, ei, raw_inode);\n\n\tif (likely(!test_opt2(inode->i_sb, HURD_COMPAT))) {\n\t\tu64 ivers = le32_to_cpu(raw_inode->i_disk_version);\n\n\t\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\t\tif (EXT4_FITS_IN_INODE(raw_inode, ei, i_version_hi))\n\t\t\t\tivers |=\n\t\t    (__u64)(le32_to_cpu(raw_inode->i_version_hi)) << 32;\n\t\t}\n\t\tinode_set_iversion_queried(inode, ivers);\n\t}\n\n\tret = 0;\n\tif (ei->i_file_acl &&\n\t    !ext4_data_block_valid(EXT4_SB(sb), ei->i_file_acl, 1)) {\n\t\tEXT4_ERROR_INODE(inode, \"bad extended attribute block %llu\",\n\t\t\t\t ei->i_file_acl);\n\t\tret = -EFSCORRUPTED;\n\t\tgoto bad_inode;\n\t} else if (!ext4_has_inline_data(inode)) {\n\t\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\t\tif ((S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||\n\t\t\t    (S_ISLNK(inode->i_mode) &&\n\t\t\t     !ext4_inode_is_fast_symlink(inode))))\n\t\t\t\t/* Validate extent which is part of inode */\n\t\t\t\tret = ext4_ext_check_inode(inode);\n\t\t} else if (S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||\n\t\t\t   (S_ISLNK(inode->i_mode) &&\n\t\t\t    !ext4_inode_is_fast_symlink(inode))) {\n\t\t\t/* Validate block references which are part of inode */\n\t\t\tret = ext4_ind_check_inode(inode);\n\t\t}\n\t}\n\tif (ret)\n\t\tgoto bad_inode;\n\n\tif (S_ISREG(inode->i_mode)) {\n\t\tinode->i_op = &ext4_file_inode_operations;\n\t\tinode->i_fop = &ext4_file_operations;\n\t\text4_set_aops(inode);\n\t} else if (S_ISDIR(inode->i_mode)) {\n\t\tinode->i_op = &ext4_dir_inode_operations;\n\t\tinode->i_fop = &ext4_dir_operations;\n\t} else if (S_ISLNK(inode->i_mode)) {\n\t\tif (ext4_encrypted_inode(inode)) {\n\t\t\tinode->i_op = &ext4_encrypted_symlink_inode_operations;\n\t\t\text4_set_aops(inode);\n\t\t} else if (ext4_inode_is_fast_symlink(inode)) {\n\t\t\tinode->i_link = (char *)ei->i_data;\n\t\t\tinode->i_op = &ext4_fast_symlink_inode_operations;\n\t\t\tnd_terminate_link(ei->i_data, inode->i_size,\n\t\t\t\tsizeof(ei->i_data) - 1);\n\t\t} else {\n\t\t\tinode->i_op = &ext4_symlink_inode_operations;\n\t\t\text4_set_aops(inode);\n\t\t}\n\t\tinode_nohighmem(inode);\n\t} else if (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode) ||\n\t      S_ISFIFO(inode->i_mode) || S_ISSOCK(inode->i_mode)) {\n\t\tinode->i_op = &ext4_special_inode_operations;\n\t\tif (raw_inode->i_block[0])\n\t\t\tinit_special_inode(inode, inode->i_mode,\n\t\t\t   old_decode_dev(le32_to_cpu(raw_inode->i_block[0])));\n\t\telse\n\t\t\tinit_special_inode(inode, inode->i_mode,\n\t\t\t   new_decode_dev(le32_to_cpu(raw_inode->i_block[1])));\n\t} else if (ino == EXT4_BOOT_LOADER_INO) {\n\t\tmake_bad_inode(inode);\n\t} else {\n\t\tret = -EFSCORRUPTED;\n\t\tEXT4_ERROR_INODE(inode, \"bogus i_mode (%o)\", inode->i_mode);\n\t\tgoto bad_inode;\n\t}\n\tbrelse(iloc.bh);\n\text4_set_inode_flags(inode);\n\n\tunlock_new_inode(inode);\n\treturn inode;\n\nbad_inode:\n\tbrelse(iloc.bh);\n\tiget_failed(inode);\n\treturn ERR_PTR(ret);\n}",
                        "code_after_change": "struct inode *ext4_iget(struct super_block *sb, unsigned long ino)\n{\n\tstruct ext4_iloc iloc;\n\tstruct ext4_inode *raw_inode;\n\tstruct ext4_inode_info *ei;\n\tstruct inode *inode;\n\tjournal_t *journal = EXT4_SB(sb)->s_journal;\n\tlong ret;\n\tloff_t size;\n\tint block;\n\tuid_t i_uid;\n\tgid_t i_gid;\n\tprojid_t i_projid;\n\n\tinode = iget_locked(sb, ino);\n\tif (!inode)\n\t\treturn ERR_PTR(-ENOMEM);\n\tif (!(inode->i_state & I_NEW))\n\t\treturn inode;\n\n\tei = EXT4_I(inode);\n\tiloc.bh = NULL;\n\n\tret = __ext4_get_inode_loc(inode, &iloc, 0);\n\tif (ret < 0)\n\t\tgoto bad_inode;\n\traw_inode = ext4_raw_inode(&iloc);\n\n\tif ((ino == EXT4_ROOT_INO) && (raw_inode->i_links_count == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"root inode unallocated\");\n\t\tret = -EFSCORRUPTED;\n\t\tgoto bad_inode;\n\t}\n\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\tei->i_extra_isize = le16_to_cpu(raw_inode->i_extra_isize);\n\t\tif (EXT4_GOOD_OLD_INODE_SIZE + ei->i_extra_isize >\n\t\t\tEXT4_INODE_SIZE(inode->i_sb) ||\n\t\t    (ei->i_extra_isize & 3)) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"bad extra_isize %u (inode size %u)\",\n\t\t\t\t\t ei->i_extra_isize,\n\t\t\t\t\t EXT4_INODE_SIZE(inode->i_sb));\n\t\t\tret = -EFSCORRUPTED;\n\t\t\tgoto bad_inode;\n\t\t}\n\t} else\n\t\tei->i_extra_isize = 0;\n\n\t/* Precompute checksum seed for inode metadata */\n\tif (ext4_has_metadata_csum(sb)) {\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t\t__u32 csum;\n\t\t__le32 inum = cpu_to_le32(inode->i_ino);\n\t\t__le32 gen = raw_inode->i_generation;\n\t\tcsum = ext4_chksum(sbi, sbi->s_csum_seed, (__u8 *)&inum,\n\t\t\t\t   sizeof(inum));\n\t\tei->i_csum_seed = ext4_chksum(sbi, csum, (__u8 *)&gen,\n\t\t\t\t\t      sizeof(gen));\n\t}\n\n\tif (!ext4_inode_csum_verify(inode, raw_inode, ei)) {\n\t\tEXT4_ERROR_INODE(inode, \"checksum invalid\");\n\t\tret = -EFSBADCRC;\n\t\tgoto bad_inode;\n\t}\n\n\tinode->i_mode = le16_to_cpu(raw_inode->i_mode);\n\ti_uid = (uid_t)le16_to_cpu(raw_inode->i_uid_low);\n\ti_gid = (gid_t)le16_to_cpu(raw_inode->i_gid_low);\n\tif (ext4_has_feature_project(sb) &&\n\t    EXT4_INODE_SIZE(sb) > EXT4_GOOD_OLD_INODE_SIZE &&\n\t    EXT4_FITS_IN_INODE(raw_inode, ei, i_projid))\n\t\ti_projid = (projid_t)le32_to_cpu(raw_inode->i_projid);\n\telse\n\t\ti_projid = EXT4_DEF_PROJID;\n\n\tif (!(test_opt(inode->i_sb, NO_UID32))) {\n\t\ti_uid |= le16_to_cpu(raw_inode->i_uid_high) << 16;\n\t\ti_gid |= le16_to_cpu(raw_inode->i_gid_high) << 16;\n\t}\n\ti_uid_write(inode, i_uid);\n\ti_gid_write(inode, i_gid);\n\tei->i_projid = make_kprojid(&init_user_ns, i_projid);\n\tset_nlink(inode, le16_to_cpu(raw_inode->i_links_count));\n\n\text4_clear_state_flags(ei);\t/* Only relevant on 32-bit archs */\n\tei->i_inline_off = 0;\n\tei->i_dir_start_lookup = 0;\n\tei->i_dtime = le32_to_cpu(raw_inode->i_dtime);\n\t/* We now have enough fields to check if the inode was active or not.\n\t * This is needed because nfsd might try to access dead inodes\n\t * the test is that same one that e2fsck uses\n\t * NeilBrown 1999oct15\n\t */\n\tif (inode->i_nlink == 0) {\n\t\tif ((inode->i_mode == 0 ||\n\t\t     !(EXT4_SB(inode->i_sb)->s_mount_state & EXT4_ORPHAN_FS)) &&\n\t\t    ino != EXT4_BOOT_LOADER_INO) {\n\t\t\t/* this inode is deleted */\n\t\t\tret = -ESTALE;\n\t\t\tgoto bad_inode;\n\t\t}\n\t\t/* The only unlinked inodes we let through here have\n\t\t * valid i_mode and are being read by the orphan\n\t\t * recovery code: that's fine, we're about to complete\n\t\t * the process of deleting those.\n\t\t * OR it is the EXT4_BOOT_LOADER_INO which is\n\t\t * not initialized on a new filesystem. */\n\t}\n\tei->i_flags = le32_to_cpu(raw_inode->i_flags);\n\tinode->i_blocks = ext4_inode_blocks(raw_inode, ei);\n\tei->i_file_acl = le32_to_cpu(raw_inode->i_file_acl_lo);\n\tif (ext4_has_feature_64bit(sb))\n\t\tei->i_file_acl |=\n\t\t\t((__u64)le16_to_cpu(raw_inode->i_file_acl_high)) << 32;\n\tinode->i_size = ext4_isize(sb, raw_inode);\n\tif ((size = i_size_read(inode)) < 0) {\n\t\tEXT4_ERROR_INODE(inode, \"bad i_size value: %lld\", size);\n\t\tret = -EFSCORRUPTED;\n\t\tgoto bad_inode;\n\t}\n\tei->i_disksize = inode->i_size;\n#ifdef CONFIG_QUOTA\n\tei->i_reserved_quota = 0;\n#endif\n\tinode->i_generation = le32_to_cpu(raw_inode->i_generation);\n\tei->i_block_group = iloc.block_group;\n\tei->i_last_alloc_group = ~0;\n\t/*\n\t * NOTE! The in-memory inode i_data array is in little-endian order\n\t * even on big-endian machines: we do NOT byteswap the block numbers!\n\t */\n\tfor (block = 0; block < EXT4_N_BLOCKS; block++)\n\t\tei->i_data[block] = raw_inode->i_block[block];\n\tINIT_LIST_HEAD(&ei->i_orphan);\n\n\t/*\n\t * Set transaction id's of transactions that have to be committed\n\t * to finish f[data]sync. We set them to currently running transaction\n\t * as we cannot be sure that the inode or some of its metadata isn't\n\t * part of the transaction - the inode could have been reclaimed and\n\t * now it is reread from disk.\n\t */\n\tif (journal) {\n\t\ttransaction_t *transaction;\n\t\ttid_t tid;\n\n\t\tread_lock(&journal->j_state_lock);\n\t\tif (journal->j_running_transaction)\n\t\t\ttransaction = journal->j_running_transaction;\n\t\telse\n\t\t\ttransaction = journal->j_committing_transaction;\n\t\tif (transaction)\n\t\t\ttid = transaction->t_tid;\n\t\telse\n\t\t\ttid = journal->j_commit_sequence;\n\t\tread_unlock(&journal->j_state_lock);\n\t\tei->i_sync_tid = tid;\n\t\tei->i_datasync_tid = tid;\n\t}\n\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\tif (ei->i_extra_isize == 0) {\n\t\t\t/* The extra space is currently unused. Use it. */\n\t\t\tBUILD_BUG_ON(sizeof(struct ext4_inode) & 3);\n\t\t\tei->i_extra_isize = sizeof(struct ext4_inode) -\n\t\t\t\t\t    EXT4_GOOD_OLD_INODE_SIZE;\n\t\t} else {\n\t\t\text4_iget_extra_inode(inode, raw_inode, ei);\n\t\t}\n\t}\n\n\tEXT4_INODE_GET_XTIME(i_ctime, inode, raw_inode);\n\tEXT4_INODE_GET_XTIME(i_mtime, inode, raw_inode);\n\tEXT4_INODE_GET_XTIME(i_atime, inode, raw_inode);\n\tEXT4_EINODE_GET_XTIME(i_crtime, ei, raw_inode);\n\n\tif (likely(!test_opt2(inode->i_sb, HURD_COMPAT))) {\n\t\tu64 ivers = le32_to_cpu(raw_inode->i_disk_version);\n\n\t\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\t\tif (EXT4_FITS_IN_INODE(raw_inode, ei, i_version_hi))\n\t\t\t\tivers |=\n\t\t    (__u64)(le32_to_cpu(raw_inode->i_version_hi)) << 32;\n\t\t}\n\t\tinode_set_iversion_queried(inode, ivers);\n\t}\n\n\tret = 0;\n\tif (ei->i_file_acl &&\n\t    !ext4_data_block_valid(EXT4_SB(sb), ei->i_file_acl, 1)) {\n\t\tEXT4_ERROR_INODE(inode, \"bad extended attribute block %llu\",\n\t\t\t\t ei->i_file_acl);\n\t\tret = -EFSCORRUPTED;\n\t\tgoto bad_inode;\n\t} else if (!ext4_has_inline_data(inode)) {\n\t\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\t\tif ((S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||\n\t\t\t    (S_ISLNK(inode->i_mode) &&\n\t\t\t     !ext4_inode_is_fast_symlink(inode))))\n\t\t\t\t/* Validate extent which is part of inode */\n\t\t\t\tret = ext4_ext_check_inode(inode);\n\t\t} else if (S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||\n\t\t\t   (S_ISLNK(inode->i_mode) &&\n\t\t\t    !ext4_inode_is_fast_symlink(inode))) {\n\t\t\t/* Validate block references which are part of inode */\n\t\t\tret = ext4_ind_check_inode(inode);\n\t\t}\n\t}\n\tif (ret)\n\t\tgoto bad_inode;\n\n\tif (S_ISREG(inode->i_mode)) {\n\t\tinode->i_op = &ext4_file_inode_operations;\n\t\tinode->i_fop = &ext4_file_operations;\n\t\text4_set_aops(inode);\n\t} else if (S_ISDIR(inode->i_mode)) {\n\t\tinode->i_op = &ext4_dir_inode_operations;\n\t\tinode->i_fop = &ext4_dir_operations;\n\t} else if (S_ISLNK(inode->i_mode)) {\n\t\tif (ext4_encrypted_inode(inode)) {\n\t\t\tinode->i_op = &ext4_encrypted_symlink_inode_operations;\n\t\t\text4_set_aops(inode);\n\t\t} else if (ext4_inode_is_fast_symlink(inode)) {\n\t\t\tinode->i_link = (char *)ei->i_data;\n\t\t\tinode->i_op = &ext4_fast_symlink_inode_operations;\n\t\t\tnd_terminate_link(ei->i_data, inode->i_size,\n\t\t\t\tsizeof(ei->i_data) - 1);\n\t\t} else {\n\t\t\tinode->i_op = &ext4_symlink_inode_operations;\n\t\t\text4_set_aops(inode);\n\t\t}\n\t\tinode_nohighmem(inode);\n\t} else if (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode) ||\n\t      S_ISFIFO(inode->i_mode) || S_ISSOCK(inode->i_mode)) {\n\t\tinode->i_op = &ext4_special_inode_operations;\n\t\tif (raw_inode->i_block[0])\n\t\t\tinit_special_inode(inode, inode->i_mode,\n\t\t\t   old_decode_dev(le32_to_cpu(raw_inode->i_block[0])));\n\t\telse\n\t\t\tinit_special_inode(inode, inode->i_mode,\n\t\t\t   new_decode_dev(le32_to_cpu(raw_inode->i_block[1])));\n\t} else if (ino == EXT4_BOOT_LOADER_INO) {\n\t\tmake_bad_inode(inode);\n\t} else {\n\t\tret = -EFSCORRUPTED;\n\t\tEXT4_ERROR_INODE(inode, \"bogus i_mode (%o)\", inode->i_mode);\n\t\tgoto bad_inode;\n\t}\n\tbrelse(iloc.bh);\n\text4_set_inode_flags(inode);\n\n\tunlock_new_inode(inode);\n\treturn inode;\n\nbad_inode:\n\tbrelse(iloc.bh);\n\tiget_failed(inode);\n\treturn ERR_PTR(ret);\n}",
                        "cve_id": "CVE-2018-1092"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static bool __written_first_block(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct f2fs_inode *ri)\n{\n\tblock_t addr = le32_to_cpu(ri->i_addr[offset_in_addr(ri)]);\n\n\tif (is_valid_data_blkaddr(sbi, addr))\n\t\treturn true;\n\treturn false;\n}",
                        "code_after_change": "static int __written_first_block(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct f2fs_inode *ri)\n{\n\tblock_t addr = le32_to_cpu(ri->i_addr[offset_in_addr(ri)]);\n\n\tif (!__is_valid_data_blkaddr(addr))\n\t\treturn 1;\n\tif (!f2fs_is_valid_blkaddr(sbi, addr, DATA_GENERIC))\n\t\treturn -EFAULT;\n\treturn 0;\n}",
                        "cve_id": "CVE-2018-14616"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1794,
            "cve_id": "CVE-2018-14646",
            "code_snippet": "static int rtnl_dump_ifinfo(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct net *tgt_net = net;\n\tint h, s_h;\n\tint idx = 0, s_idx;\n\tstruct net_device *dev;\n\tstruct hlist_head *head;\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tu32 ext_filter_mask = 0;\n\tconst struct rtnl_link_ops *kind_ops = NULL;\n\tunsigned int flags = NLM_F_MULTI;\n\tint master_idx = 0;\n\tint netnsid = -1;\n\tint err;\n\tint hdrlen;\n\n\ts_h = cb->args[0];\n\ts_idx = cb->args[1];\n\n\t/* A hack to preserve kernel<->userspace interface.\n\t * The correct header is ifinfomsg. It is consistent with rtnl_getlink.\n\t * However, before Linux v3.9 the code here assumed rtgenmsg and that's\n\t * what iproute2 < v3.9.0 used.\n\t * We can detect the old iproute2. Even including the IFLA_EXT_MASK\n\t * attribute, its netlink message is shorter than struct ifinfomsg.\n\t */\n\thdrlen = nlmsg_len(cb->nlh) < sizeof(struct ifinfomsg) ?\n\t\t sizeof(struct rtgenmsg) : sizeof(struct ifinfomsg);\n\n\tif (nlmsg_parse(cb->nlh, hdrlen, tb, IFLA_MAX,\n\t\t\tifla_policy, NULL) >= 0) {\n\t\tif (tb[IFLA_IF_NETNSID]) {\n\t\t\tnetnsid = nla_get_s32(tb[IFLA_IF_NETNSID]);\n\t\t\ttgt_net = get_target_net(skb, netnsid);\n\t\t\tif (IS_ERR(tgt_net)) {\n\t\t\t\ttgt_net = net;\n\t\t\t\tnetnsid = -1;\n\t\t\t}\n\t\t}\n\n\t\tif (tb[IFLA_EXT_MASK])\n\t\t\text_filter_mask = nla_get_u32(tb[IFLA_EXT_MASK]);\n\n\t\tif (tb[IFLA_MASTER])\n\t\t\tmaster_idx = nla_get_u32(tb[IFLA_MASTER]);\n\n\t\tif (tb[IFLA_LINKINFO])\n\t\t\tkind_ops = linkinfo_to_kind_ops(tb[IFLA_LINKINFO]);\n\n\t\tif (master_idx || kind_ops)\n\t\t\tflags |= NLM_F_DUMP_FILTERED;\n\t}\n\n\tfor (h = s_h; h < NETDEV_HASHENTRIES; h++, s_idx = 0) {\n\t\tidx = 0;\n\t\thead = &tgt_net->dev_index_head[h];\n\t\thlist_for_each_entry(dev, head, index_hlist) {\n\t\t\tif (link_dump_filtered(dev, master_idx, kind_ops))\n\t\t\t\tgoto cont;\n\t\t\tif (idx < s_idx)\n\t\t\t\tgoto cont;\n\t\t\terr = rtnl_fill_ifinfo(skb, dev, net,\n\t\t\t\t\t       RTM_NEWLINK,\n\t\t\t\t\t       NETLINK_CB(cb->skb).portid,\n\t\t\t\t\t       cb->nlh->nlmsg_seq, 0,\n\t\t\t\t\t       flags,\n\t\t\t\t\t       ext_filter_mask, 0, NULL,\n\t\t\t\t\t       netnsid);\n\n\t\t\tif (err < 0) {\n\t\t\t\tif (likely(skb->len))\n\t\t\t\t\tgoto out;\n\n\t\t\t\tgoto out_err;\n\t\t\t}\ncont:\n\t\t\tidx++;\n\t\t}\n\t}\nout:\n\terr = skb->len;\nout_err:\n\tcb->args[1] = idx;\n\tcb->args[0] = h;\n\tcb->seq = net->dev_base_seq;\n\tnl_dump_check_consistent(cb, nlmsg_hdr(skb));\n\tif (netnsid >= 0)\n\t\tput_net(tgt_net);\n\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int rtnl_getlink(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct net *tgt_net = net;\n\tstruct ifinfomsg *ifm;\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct net_device *dev = NULL;\n\tstruct sk_buff *nskb;\n\tint netnsid = -1;\n\tint err;\n\tu32 ext_filter_mask = 0;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IF_NETNSID]) {\n\t\tnetnsid = nla_get_s32(tb[IFLA_IF_NETNSID]);\n\t\ttgt_net = get_target_net(skb, netnsid);\n\t\tif (IS_ERR(tgt_net))\n\t\t\treturn PTR_ERR(tgt_net);\n\t}\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\n\tif (tb[IFLA_EXT_MASK])\n\t\text_filter_mask = nla_get_u32(tb[IFLA_EXT_MASK]);\n\n\terr = -EINVAL;\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(tgt_net, ifm->ifi_index);\n\telse if (tb[IFLA_IFNAME])\n\t\tdev = __dev_get_by_name(tgt_net, ifname);\n\telse\n\t\tgoto out;\n\n\terr = -ENODEV;\n\tif (dev == NULL)\n\t\tgoto out;\n\n\terr = -ENOBUFS;\n\tnskb = nlmsg_new(if_nlmsg_size(dev, ext_filter_mask), GFP_KERNEL);\n\tif (nskb == NULL)\n\t\tgoto out;\n\n\terr = rtnl_fill_ifinfo(nskb, dev, net,\n\t\t\t       RTM_NEWLINK, NETLINK_CB(skb).portid,\n\t\t\t       nlh->nlmsg_seq, 0, 0, ext_filter_mask,\n\t\t\t       0, NULL, netnsid);\n\tif (err < 0) {\n\t\t/* -EMSGSIZE implies BUG in if_nlmsg_size */\n\t\tWARN_ON(err == -EMSGSIZE);\n\t\tkfree_skb(nskb);\n\t} else\n\t\terr = rtnl_unicast(nskb, net, NETLINK_CB(skb).portid);\nout:\n\tif (netnsid >= 0)\n\t\tput_net(tgt_net);\n\n\treturn err;\n}",
                        "code_after_change": "static int rtnl_getlink(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct net *tgt_net = net;\n\tstruct ifinfomsg *ifm;\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct net_device *dev = NULL;\n\tstruct sk_buff *nskb;\n\tint netnsid = -1;\n\tint err;\n\tu32 ext_filter_mask = 0;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IF_NETNSID]) {\n\t\tnetnsid = nla_get_s32(tb[IFLA_IF_NETNSID]);\n\t\ttgt_net = get_target_net(NETLINK_CB(skb).sk, netnsid);\n\t\tif (IS_ERR(tgt_net))\n\t\t\treturn PTR_ERR(tgt_net);\n\t}\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\n\tif (tb[IFLA_EXT_MASK])\n\t\text_filter_mask = nla_get_u32(tb[IFLA_EXT_MASK]);\n\n\terr = -EINVAL;\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(tgt_net, ifm->ifi_index);\n\telse if (tb[IFLA_IFNAME])\n\t\tdev = __dev_get_by_name(tgt_net, ifname);\n\telse\n\t\tgoto out;\n\n\terr = -ENODEV;\n\tif (dev == NULL)\n\t\tgoto out;\n\n\terr = -ENOBUFS;\n\tnskb = nlmsg_new(if_nlmsg_size(dev, ext_filter_mask), GFP_KERNEL);\n\tif (nskb == NULL)\n\t\tgoto out;\n\n\terr = rtnl_fill_ifinfo(nskb, dev, net,\n\t\t\t       RTM_NEWLINK, NETLINK_CB(skb).portid,\n\t\t\t       nlh->nlmsg_seq, 0, 0, ext_filter_mask,\n\t\t\t       0, NULL, netnsid);\n\tif (err < 0) {\n\t\t/* -EMSGSIZE implies BUG in if_nlmsg_size */\n\t\tWARN_ON(err == -EMSGSIZE);\n\t\tkfree_skb(nskb);\n\t} else\n\t\terr = rtnl_unicast(nskb, net, NETLINK_CB(skb).portid);\nout:\n\tif (netnsid >= 0)\n\t\tput_net(tgt_net);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2018-14646"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1942,
            "cve_id": "CVE-2018-7191",
            "code_snippet": "static int dev_get_valid_name(struct net *net,\n\t\t\t      struct net_device *dev,\n\t\t\t      const char *name)\n{\n\tBUG_ON(!net);\n\n\tif (!dev_valid_name(name))\n\t\treturn -EINVAL;\n\n\tif (strchr(name, '%'))\n\t\treturn dev_alloc_name_ns(net, dev, name);\n\telse if (__dev_get_by_name(net, name))\n\t\treturn -EEXIST;\n\telse if (dev->name != name)\n\t\tstrlcpy(dev->name, name, IFNAMSIZ);\n\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tun_set_iff(struct net *net, struct file *file, struct ifreq *ifr)\n{\n\tstruct tun_struct *tun;\n\tstruct tun_file *tfile = file->private_data;\n\tstruct net_device *dev;\n\tint err;\n\n\tif (tfile->detached)\n\t\treturn -EINVAL;\n\n\tdev = __dev_get_by_name(net, ifr->ifr_name);\n\tif (dev) {\n\t\tif (ifr->ifr_flags & IFF_TUN_EXCL)\n\t\t\treturn -EBUSY;\n\t\tif ((ifr->ifr_flags & IFF_TUN) && dev->netdev_ops == &tun_netdev_ops)\n\t\t\ttun = netdev_priv(dev);\n\t\telse if ((ifr->ifr_flags & IFF_TAP) && dev->netdev_ops == &tap_netdev_ops)\n\t\t\ttun = netdev_priv(dev);\n\t\telse\n\t\t\treturn -EINVAL;\n\n\t\tif (!!(ifr->ifr_flags & IFF_MULTI_QUEUE) !=\n\t\t    !!(tun->flags & IFF_MULTI_QUEUE))\n\t\t\treturn -EINVAL;\n\n\t\tif (tun_not_capable(tun))\n\t\t\treturn -EPERM;\n\t\terr = security_tun_dev_open(tun->security);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\terr = tun_attach(tun, file, ifr->ifr_flags & IFF_NOFILTER);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\tif (tun->flags & IFF_MULTI_QUEUE &&\n\t\t    (tun->numqueues + tun->numdisabled > 1)) {\n\t\t\t/* One or more queue has already been attached, no need\n\t\t\t * to initialize the device again.\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\t}\n\telse {\n\t\tchar *name;\n\t\tunsigned long flags = 0;\n\t\tint queues = ifr->ifr_flags & IFF_MULTI_QUEUE ?\n\t\t\t     MAX_TAP_QUEUES : 1;\n\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\treturn -EPERM;\n\t\terr = security_tun_dev_create();\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\t/* Set dev type */\n\t\tif (ifr->ifr_flags & IFF_TUN) {\n\t\t\t/* TUN device */\n\t\t\tflags |= IFF_TUN;\n\t\t\tname = \"tun%d\";\n\t\t} else if (ifr->ifr_flags & IFF_TAP) {\n\t\t\t/* TAP device */\n\t\t\tflags |= IFF_TAP;\n\t\t\tname = \"tap%d\";\n\t\t} else\n\t\t\treturn -EINVAL;\n\n\t\tif (*ifr->ifr_name)\n\t\t\tname = ifr->ifr_name;\n\n\t\tdev = alloc_netdev_mqs(sizeof(struct tun_struct), name,\n\t\t\t\t       NET_NAME_UNKNOWN, tun_setup, queues,\n\t\t\t\t       queues);\n\n\t\tif (!dev)\n\t\t\treturn -ENOMEM;\n\n\t\tdev_net_set(dev, net);\n\t\tdev->rtnl_link_ops = &tun_link_ops;\n\t\tdev->ifindex = tfile->ifindex;\n\t\tdev->sysfs_groups[0] = &tun_attr_group;\n\n\t\ttun = netdev_priv(dev);\n\t\ttun->dev = dev;\n\t\ttun->flags = flags;\n\t\ttun->txflt.count = 0;\n\t\ttun->vnet_hdr_sz = sizeof(struct virtio_net_hdr);\n\n\t\ttun->align = NET_SKB_PAD;\n\t\ttun->filter_attached = false;\n\t\ttun->sndbuf = tfile->socket.sk->sk_sndbuf;\n\t\ttun->rx_batched = 0;\n\n\t\ttun->pcpu_stats = netdev_alloc_pcpu_stats(struct tun_pcpu_stats);\n\t\tif (!tun->pcpu_stats) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_free_dev;\n\t\t}\n\n\t\tspin_lock_init(&tun->lock);\n\n\t\terr = security_tun_dev_alloc_security(&tun->security);\n\t\tif (err < 0)\n\t\t\tgoto err_free_stat;\n\n\t\ttun_net_init(dev);\n\t\ttun_flow_init(tun);\n\n\t\tdev->hw_features = NETIF_F_SG | NETIF_F_FRAGLIST |\n\t\t\t\t   TUN_USER_FEATURES | NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t   NETIF_F_HW_VLAN_STAG_TX;\n\t\tdev->features = dev->hw_features | NETIF_F_LLTX;\n\t\tdev->vlan_features = dev->features &\n\t\t\t\t     ~(NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t       NETIF_F_HW_VLAN_STAG_TX);\n\n\t\tINIT_LIST_HEAD(&tun->disabled);\n\t\terr = tun_attach(tun, file, false);\n\t\tif (err < 0)\n\t\t\tgoto err_free_flow;\n\n\t\terr = register_netdevice(tun->dev);\n\t\tif (err < 0)\n\t\t\tgoto err_detach;\n\t}\n\n\tnetif_carrier_on(tun->dev);\n\n\ttun_debug(KERN_INFO, tun, \"tun_set_iff\\n\");\n\n\ttun->flags = (tun->flags & ~TUN_FEATURES) |\n\t\t(ifr->ifr_flags & TUN_FEATURES);\n\n\t/* Make sure persistent devices do not get stuck in\n\t * xoff state.\n\t */\n\tif (netif_running(tun->dev))\n\t\tnetif_tx_wake_all_queues(tun->dev);\n\n\tstrcpy(ifr->ifr_name, tun->dev->name);\n\treturn 0;\n\nerr_detach:\n\ttun_detach_all(dev);\n\t/* register_netdevice() already called tun_free_netdev() */\n\tgoto err_free_dev;\n\nerr_free_flow:\n\ttun_flow_uninit(tun);\n\tsecurity_tun_dev_free_security(tun->security);\nerr_free_stat:\n\tfree_percpu(tun->pcpu_stats);\nerr_free_dev:\n\tfree_netdev(dev);\n\treturn err;\n}",
                        "code_after_change": "static int tun_set_iff(struct net *net, struct file *file, struct ifreq *ifr)\n{\n\tstruct tun_struct *tun;\n\tstruct tun_file *tfile = file->private_data;\n\tstruct net_device *dev;\n\tint err;\n\n\tif (tfile->detached)\n\t\treturn -EINVAL;\n\n\tdev = __dev_get_by_name(net, ifr->ifr_name);\n\tif (dev) {\n\t\tif (ifr->ifr_flags & IFF_TUN_EXCL)\n\t\t\treturn -EBUSY;\n\t\tif ((ifr->ifr_flags & IFF_TUN) && dev->netdev_ops == &tun_netdev_ops)\n\t\t\ttun = netdev_priv(dev);\n\t\telse if ((ifr->ifr_flags & IFF_TAP) && dev->netdev_ops == &tap_netdev_ops)\n\t\t\ttun = netdev_priv(dev);\n\t\telse\n\t\t\treturn -EINVAL;\n\n\t\tif (!!(ifr->ifr_flags & IFF_MULTI_QUEUE) !=\n\t\t    !!(tun->flags & IFF_MULTI_QUEUE))\n\t\t\treturn -EINVAL;\n\n\t\tif (tun_not_capable(tun))\n\t\t\treturn -EPERM;\n\t\terr = security_tun_dev_open(tun->security);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\terr = tun_attach(tun, file, ifr->ifr_flags & IFF_NOFILTER);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\tif (tun->flags & IFF_MULTI_QUEUE &&\n\t\t    (tun->numqueues + tun->numdisabled > 1)) {\n\t\t\t/* One or more queue has already been attached, no need\n\t\t\t * to initialize the device again.\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\t}\n\telse {\n\t\tchar *name;\n\t\tunsigned long flags = 0;\n\t\tint queues = ifr->ifr_flags & IFF_MULTI_QUEUE ?\n\t\t\t     MAX_TAP_QUEUES : 1;\n\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\treturn -EPERM;\n\t\terr = security_tun_dev_create();\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\t/* Set dev type */\n\t\tif (ifr->ifr_flags & IFF_TUN) {\n\t\t\t/* TUN device */\n\t\t\tflags |= IFF_TUN;\n\t\t\tname = \"tun%d\";\n\t\t} else if (ifr->ifr_flags & IFF_TAP) {\n\t\t\t/* TAP device */\n\t\t\tflags |= IFF_TAP;\n\t\t\tname = \"tap%d\";\n\t\t} else\n\t\t\treturn -EINVAL;\n\n\t\tif (*ifr->ifr_name)\n\t\t\tname = ifr->ifr_name;\n\n\t\tdev = alloc_netdev_mqs(sizeof(struct tun_struct), name,\n\t\t\t\t       NET_NAME_UNKNOWN, tun_setup, queues,\n\t\t\t\t       queues);\n\n\t\tif (!dev)\n\t\t\treturn -ENOMEM;\n\t\terr = dev_get_valid_name(net, dev, name);\n\t\tif (err)\n\t\t\tgoto err_free_dev;\n\n\t\tdev_net_set(dev, net);\n\t\tdev->rtnl_link_ops = &tun_link_ops;\n\t\tdev->ifindex = tfile->ifindex;\n\t\tdev->sysfs_groups[0] = &tun_attr_group;\n\n\t\ttun = netdev_priv(dev);\n\t\ttun->dev = dev;\n\t\ttun->flags = flags;\n\t\ttun->txflt.count = 0;\n\t\ttun->vnet_hdr_sz = sizeof(struct virtio_net_hdr);\n\n\t\ttun->align = NET_SKB_PAD;\n\t\ttun->filter_attached = false;\n\t\ttun->sndbuf = tfile->socket.sk->sk_sndbuf;\n\t\ttun->rx_batched = 0;\n\n\t\ttun->pcpu_stats = netdev_alloc_pcpu_stats(struct tun_pcpu_stats);\n\t\tif (!tun->pcpu_stats) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_free_dev;\n\t\t}\n\n\t\tspin_lock_init(&tun->lock);\n\n\t\terr = security_tun_dev_alloc_security(&tun->security);\n\t\tif (err < 0)\n\t\t\tgoto err_free_stat;\n\n\t\ttun_net_init(dev);\n\t\ttun_flow_init(tun);\n\n\t\tdev->hw_features = NETIF_F_SG | NETIF_F_FRAGLIST |\n\t\t\t\t   TUN_USER_FEATURES | NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t   NETIF_F_HW_VLAN_STAG_TX;\n\t\tdev->features = dev->hw_features | NETIF_F_LLTX;\n\t\tdev->vlan_features = dev->features &\n\t\t\t\t     ~(NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t       NETIF_F_HW_VLAN_STAG_TX);\n\n\t\tINIT_LIST_HEAD(&tun->disabled);\n\t\terr = tun_attach(tun, file, false);\n\t\tif (err < 0)\n\t\t\tgoto err_free_flow;\n\n\t\terr = register_netdevice(tun->dev);\n\t\tif (err < 0)\n\t\t\tgoto err_detach;\n\t}\n\n\tnetif_carrier_on(tun->dev);\n\n\ttun_debug(KERN_INFO, tun, \"tun_set_iff\\n\");\n\n\ttun->flags = (tun->flags & ~TUN_FEATURES) |\n\t\t(ifr->ifr_flags & TUN_FEATURES);\n\n\t/* Make sure persistent devices do not get stuck in\n\t * xoff state.\n\t */\n\tif (netif_running(tun->dev))\n\t\tnetif_tx_wake_all_queues(tun->dev);\n\n\tstrcpy(ifr->ifr_name, tun->dev->name);\n\treturn 0;\n\nerr_detach:\n\ttun_detach_all(dev);\n\t/* register_netdevice() already called tun_free_netdev() */\n\tgoto err_free_dev;\n\nerr_free_flow:\n\ttun_flow_uninit(tun);\n\tsecurity_tun_dev_free_security(tun->security);\nerr_free_stat:\n\tfree_percpu(tun->pcpu_stats);\nerr_free_dev:\n\tfree_netdev(dev);\n\treturn err;\n}",
                        "cve_id": "CVE-2018-7191"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int rtnl_getlink(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct net *tgt_net = net;\n\tstruct ifinfomsg *ifm;\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct net_device *dev = NULL;\n\tstruct sk_buff *nskb;\n\tint netnsid = -1;\n\tint err;\n\tu32 ext_filter_mask = 0;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IF_NETNSID]) {\n\t\tnetnsid = nla_get_s32(tb[IFLA_IF_NETNSID]);\n\t\ttgt_net = get_target_net(skb, netnsid);\n\t\tif (IS_ERR(tgt_net))\n\t\t\treturn PTR_ERR(tgt_net);\n\t}\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\n\tif (tb[IFLA_EXT_MASK])\n\t\text_filter_mask = nla_get_u32(tb[IFLA_EXT_MASK]);\n\n\terr = -EINVAL;\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(tgt_net, ifm->ifi_index);\n\telse if (tb[IFLA_IFNAME])\n\t\tdev = __dev_get_by_name(tgt_net, ifname);\n\telse\n\t\tgoto out;\n\n\terr = -ENODEV;\n\tif (dev == NULL)\n\t\tgoto out;\n\n\terr = -ENOBUFS;\n\tnskb = nlmsg_new(if_nlmsg_size(dev, ext_filter_mask), GFP_KERNEL);\n\tif (nskb == NULL)\n\t\tgoto out;\n\n\terr = rtnl_fill_ifinfo(nskb, dev, net,\n\t\t\t       RTM_NEWLINK, NETLINK_CB(skb).portid,\n\t\t\t       nlh->nlmsg_seq, 0, 0, ext_filter_mask,\n\t\t\t       0, NULL, netnsid);\n\tif (err < 0) {\n\t\t/* -EMSGSIZE implies BUG in if_nlmsg_size */\n\t\tWARN_ON(err == -EMSGSIZE);\n\t\tkfree_skb(nskb);\n\t} else\n\t\terr = rtnl_unicast(nskb, net, NETLINK_CB(skb).portid);\nout:\n\tif (netnsid >= 0)\n\t\tput_net(tgt_net);\n\n\treturn err;\n}",
                        "code_after_change": "static int rtnl_getlink(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct net *tgt_net = net;\n\tstruct ifinfomsg *ifm;\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct net_device *dev = NULL;\n\tstruct sk_buff *nskb;\n\tint netnsid = -1;\n\tint err;\n\tu32 ext_filter_mask = 0;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IF_NETNSID]) {\n\t\tnetnsid = nla_get_s32(tb[IFLA_IF_NETNSID]);\n\t\ttgt_net = get_target_net(NETLINK_CB(skb).sk, netnsid);\n\t\tif (IS_ERR(tgt_net))\n\t\t\treturn PTR_ERR(tgt_net);\n\t}\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\n\tif (tb[IFLA_EXT_MASK])\n\t\text_filter_mask = nla_get_u32(tb[IFLA_EXT_MASK]);\n\n\terr = -EINVAL;\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(tgt_net, ifm->ifi_index);\n\telse if (tb[IFLA_IFNAME])\n\t\tdev = __dev_get_by_name(tgt_net, ifname);\n\telse\n\t\tgoto out;\n\n\terr = -ENODEV;\n\tif (dev == NULL)\n\t\tgoto out;\n\n\terr = -ENOBUFS;\n\tnskb = nlmsg_new(if_nlmsg_size(dev, ext_filter_mask), GFP_KERNEL);\n\tif (nskb == NULL)\n\t\tgoto out;\n\n\terr = rtnl_fill_ifinfo(nskb, dev, net,\n\t\t\t       RTM_NEWLINK, NETLINK_CB(skb).portid,\n\t\t\t       nlh->nlmsg_seq, 0, 0, ext_filter_mask,\n\t\t\t       0, NULL, netnsid);\n\tif (err < 0) {\n\t\t/* -EMSGSIZE implies BUG in if_nlmsg_size */\n\t\tWARN_ON(err == -EMSGSIZE);\n\t\tkfree_skb(nskb);\n\t} else\n\t\terr = rtnl_unicast(nskb, net, NETLINK_CB(skb).portid);\nout:\n\tif (netnsid >= 0)\n\t\tput_net(tgt_net);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2018-14646"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static struct device_node *\ntegra_xusb_find_port_node(struct tegra_xusb_padctl *padctl, const char *type,\n\t\t\t  unsigned int index)\n{\n\tstruct device_node *ports, *np;\n\tchar *name;\n\n\tports = of_get_child_by_name(padctl->dev->of_node, \"ports\");\n\tif (!ports)\n\t\treturn NULL;\n\n\tname = kasprintf(GFP_KERNEL, \"%s-%u\", type, index);\n\tif (!name) {\n\t\tof_node_put(ports);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tnp = of_get_child_by_name(ports, name);\n\tkfree(name);\n\tof_node_put(ports);\n\n\treturn np;\n}",
                        "code_after_change": "static struct device_node *\ntegra_xusb_find_port_node(struct tegra_xusb_padctl *padctl, const char *type,\n\t\t\t  unsigned int index)\n{\n\tstruct device_node *ports, *np;\n\tchar *name;\n\n\tports = of_get_child_by_name(padctl->dev->of_node, \"ports\");\n\tif (!ports)\n\t\treturn NULL;\n\n\tname = kasprintf(GFP_KERNEL, \"%s-%u\", type, index);\n\tif (!name) {\n\t\tof_node_put(ports);\n\t\treturn NULL;\n\t}\n\tnp = of_get_child_by_name(ports, name);\n\tkfree(name);\n\tof_node_put(ports);\n\n\treturn np;\n}",
                        "cve_id": "CVE-2023-23000"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static noinline struct btrfs_device *device_list_add(const char *path,\n\t\t\t   struct btrfs_super_block *disk_super,\n\t\t\t   bool *new_device_added)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *fs_devices = NULL;\n\tstruct rcu_string *name;\n\tu64 found_transid = btrfs_super_generation(disk_super);\n\tu64 devid = btrfs_stack_device_id(&disk_super->dev_item);\n\tbool has_metadata_uuid = (btrfs_super_incompat_flags(disk_super) &\n\t\tBTRFS_FEATURE_INCOMPAT_METADATA_UUID);\n\tbool fsid_change_in_progress = (btrfs_super_flags(disk_super) &\n\t\t\t\t\tBTRFS_SUPER_FLAG_CHANGING_FSID_V2);\n\n\tif (fsid_change_in_progress) {\n\t\tif (!has_metadata_uuid) {\n\t\t\t/*\n\t\t\t * When we have an image which has CHANGING_FSID_V2 set\n\t\t\t * it might belong to either a filesystem which has\n\t\t\t * disks with completed fsid change or it might belong\n\t\t\t * to fs with no UUID changes in effect, handle both.\n\t\t\t */\n\t\t\tfs_devices = find_fsid_inprogress(disk_super);\n\t\t\tif (!fs_devices)\n\t\t\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t\t} else {\n\t\t\tfs_devices = find_fsid_changed(disk_super);\n\t\t}\n\t} else if (has_metadata_uuid) {\n\t\tfs_devices = find_fsid(disk_super->fsid,\n\t\t\t\t       disk_super->metadata_uuid);\n\t} else {\n\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t}\n\n\n\tif (!fs_devices) {\n\t\tif (has_metadata_uuid)\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid,\n\t\t\t\t\t\t      disk_super->metadata_uuid);\n\t\telse\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid, NULL);\n\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn ERR_CAST(fs_devices);\n\n\t\tfs_devices->fsid_change = fsid_change_in_progress;\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_add(&fs_devices->fs_list, &fs_uuids);\n\n\t\tdevice = NULL;\n\t} else {\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tdevice = find_device(fs_devices, devid,\n\t\t\t\tdisk_super->dev_item.uuid);\n\n\t\t/*\n\t\t * If this disk has been pulled into an fs devices created by\n\t\t * a device which had the CHANGING_FSID_V2 flag then replace the\n\t\t * metadata_uuid/fsid values of the fs_devices.\n\t\t */\n\t\tif (has_metadata_uuid && fs_devices->fsid_change &&\n\t\t    found_transid > fs_devices->latest_generation) {\n\t\t\tmemcpy(fs_devices->fsid, disk_super->fsid,\n\t\t\t\t\tBTRFS_FSID_SIZE);\n\t\t\tmemcpy(fs_devices->metadata_uuid,\n\t\t\t\t\tdisk_super->metadata_uuid, BTRFS_FSID_SIZE);\n\n\t\t\tfs_devices->fsid_change = false;\n\t\t}\n\t}\n\n\tif (!device) {\n\t\tif (fs_devices->opened) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EBUSY);\n\t\t}\n\n\t\tdevice = btrfs_alloc_device(NULL, &devid,\n\t\t\t\t\t    disk_super->dev_item.uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t/* we can safely leave the fs_devices entry around */\n\t\t\treturn device;\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tbtrfs_free_device(device);\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_assign_pointer(device->name, name);\n\n\t\tlist_add_rcu(&device->dev_list, &fs_devices->devices);\n\t\tfs_devices->num_devices++;\n\n\t\tdevice->fs_devices = fs_devices;\n\t\t*new_device_added = true;\n\n\t\tif (disk_super->label[0])\n\t\t\tpr_info(\"BTRFS: device label %s devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->label, devid, found_transid, path);\n\t\telse\n\t\t\tpr_info(\"BTRFS: device fsid %pU devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->fsid, devid, found_transid, path);\n\n\t} else if (!device->name || strcmp(device->name->str, path)) {\n\t\t/*\n\t\t * When FS is already mounted.\n\t\t * 1. If you are here and if the device->name is NULL that\n\t\t *    means this device was missing at time of FS mount.\n\t\t * 2. If you are here and if the device->name is different\n\t\t *    from 'path' that means either\n\t\t *      a. The same device disappeared and reappeared with\n\t\t *         different name. or\n\t\t *      b. The missing-disk-which-was-replaced, has\n\t\t *         reappeared now.\n\t\t *\n\t\t * We must allow 1 and 2a above. But 2b would be a spurious\n\t\t * and unintentional.\n\t\t *\n\t\t * Further in case of 1 and 2a above, the disk at 'path'\n\t\t * would have missed some transaction when it was away and\n\t\t * in case of 2a the stale bdev has to be updated as well.\n\t\t * 2b must not be allowed at all time.\n\t\t */\n\n\t\t/*\n\t\t * For now, we do allow update to btrfs_fs_device through the\n\t\t * btrfs dev scan cli after FS has been mounted.  We're still\n\t\t * tracking a problem where systems fail mount by subvolume id\n\t\t * when we reject replacement on a mounted FS.\n\t\t */\n\t\tif (!fs_devices->opened && found_transid < device->generation) {\n\t\t\t/*\n\t\t\t * That is if the FS is _not_ mounted and if you\n\t\t\t * are here, that means there is more than one\n\t\t\t * disk with same uuid and devid.We keep the one\n\t\t\t * with larger generation number or the last-in if\n\t\t\t * generation are equal.\n\t\t\t */\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EEXIST);\n\t\t}\n\n\t\t/*\n\t\t * We are going to replace the device path for a given devid,\n\t\t * make sure it's the same device if the device is mounted\n\t\t */\n\t\tif (device->bdev) {\n\t\t\tstruct block_device *path_bdev;\n\n\t\t\tpath_bdev = lookup_bdev(path);\n\t\t\tif (IS_ERR(path_bdev)) {\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\treturn ERR_CAST(path_bdev);\n\t\t\t}\n\n\t\t\tif (device->bdev != path_bdev) {\n\t\t\t\tbdput(path_bdev);\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\tbtrfs_warn_in_rcu(device->fs_info,\n\t\t\t\"duplicate device fsid:devid for %pU:%llu old:%s new:%s\",\n\t\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\t\trcu_str_deref(device->name), path);\n\t\t\t\treturn ERR_PTR(-EEXIST);\n\t\t\t}\n\t\t\tbdput(path_bdev);\n\t\t\tbtrfs_info_in_rcu(device->fs_info,\n\t\t\t\t\"device fsid %pU devid %llu moved old:%s new:%s\",\n\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\trcu_str_deref(device->name), path);\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_string_free(device->name);\n\t\trcu_assign_pointer(device->name, name);\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\tfs_devices->missing_devices--;\n\t\t\tclear_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\t}\n\n\t/*\n\t * Unmount does not free the btrfs_device struct but would zero\n\t * generation along with most of the other members. So just update\n\t * it back. We need it to pick the disk with largest generation\n\t * (as above).\n\t */\n\tif (!fs_devices->opened) {\n\t\tdevice->generation = found_transid;\n\t\tfs_devices->latest_generation = max_t(u64, found_transid,\n\t\t\t\t\t\tfs_devices->latest_generation);\n\t}\n\n\tfs_devices->total_devices = btrfs_super_num_devices(disk_super);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\treturn device;\n}",
                        "code_after_change": "static noinline struct btrfs_device *device_list_add(const char *path,\n\t\t\t   struct btrfs_super_block *disk_super,\n\t\t\t   bool *new_device_added)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *fs_devices = NULL;\n\tstruct rcu_string *name;\n\tu64 found_transid = btrfs_super_generation(disk_super);\n\tu64 devid = btrfs_stack_device_id(&disk_super->dev_item);\n\tbool has_metadata_uuid = (btrfs_super_incompat_flags(disk_super) &\n\t\tBTRFS_FEATURE_INCOMPAT_METADATA_UUID);\n\tbool fsid_change_in_progress = (btrfs_super_flags(disk_super) &\n\t\t\t\t\tBTRFS_SUPER_FLAG_CHANGING_FSID_V2);\n\n\tif (fsid_change_in_progress) {\n\t\tif (!has_metadata_uuid) {\n\t\t\t/*\n\t\t\t * When we have an image which has CHANGING_FSID_V2 set\n\t\t\t * it might belong to either a filesystem which has\n\t\t\t * disks with completed fsid change or it might belong\n\t\t\t * to fs with no UUID changes in effect, handle both.\n\t\t\t */\n\t\t\tfs_devices = find_fsid_inprogress(disk_super);\n\t\t\tif (!fs_devices)\n\t\t\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t\t} else {\n\t\t\tfs_devices = find_fsid_changed(disk_super);\n\t\t}\n\t} else if (has_metadata_uuid) {\n\t\tfs_devices = find_fsid(disk_super->fsid,\n\t\t\t\t       disk_super->metadata_uuid);\n\t} else {\n\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t}\n\n\n\tif (!fs_devices) {\n\t\tif (has_metadata_uuid)\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid,\n\t\t\t\t\t\t      disk_super->metadata_uuid);\n\t\telse\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid, NULL);\n\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn ERR_CAST(fs_devices);\n\n\t\tfs_devices->fsid_change = fsid_change_in_progress;\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_add(&fs_devices->fs_list, &fs_uuids);\n\n\t\tdevice = NULL;\n\t} else {\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tdevice = btrfs_find_device(fs_devices, devid,\n\t\t\t\tdisk_super->dev_item.uuid, NULL, false);\n\n\t\t/*\n\t\t * If this disk has been pulled into an fs devices created by\n\t\t * a device which had the CHANGING_FSID_V2 flag then replace the\n\t\t * metadata_uuid/fsid values of the fs_devices.\n\t\t */\n\t\tif (has_metadata_uuid && fs_devices->fsid_change &&\n\t\t    found_transid > fs_devices->latest_generation) {\n\t\t\tmemcpy(fs_devices->fsid, disk_super->fsid,\n\t\t\t\t\tBTRFS_FSID_SIZE);\n\t\t\tmemcpy(fs_devices->metadata_uuid,\n\t\t\t\t\tdisk_super->metadata_uuid, BTRFS_FSID_SIZE);\n\n\t\t\tfs_devices->fsid_change = false;\n\t\t}\n\t}\n\n\tif (!device) {\n\t\tif (fs_devices->opened) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EBUSY);\n\t\t}\n\n\t\tdevice = btrfs_alloc_device(NULL, &devid,\n\t\t\t\t\t    disk_super->dev_item.uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t/* we can safely leave the fs_devices entry around */\n\t\t\treturn device;\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tbtrfs_free_device(device);\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_assign_pointer(device->name, name);\n\n\t\tlist_add_rcu(&device->dev_list, &fs_devices->devices);\n\t\tfs_devices->num_devices++;\n\n\t\tdevice->fs_devices = fs_devices;\n\t\t*new_device_added = true;\n\n\t\tif (disk_super->label[0])\n\t\t\tpr_info(\"BTRFS: device label %s devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->label, devid, found_transid, path);\n\t\telse\n\t\t\tpr_info(\"BTRFS: device fsid %pU devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->fsid, devid, found_transid, path);\n\n\t} else if (!device->name || strcmp(device->name->str, path)) {\n\t\t/*\n\t\t * When FS is already mounted.\n\t\t * 1. If you are here and if the device->name is NULL that\n\t\t *    means this device was missing at time of FS mount.\n\t\t * 2. If you are here and if the device->name is different\n\t\t *    from 'path' that means either\n\t\t *      a. The same device disappeared and reappeared with\n\t\t *         different name. or\n\t\t *      b. The missing-disk-which-was-replaced, has\n\t\t *         reappeared now.\n\t\t *\n\t\t * We must allow 1 and 2a above. But 2b would be a spurious\n\t\t * and unintentional.\n\t\t *\n\t\t * Further in case of 1 and 2a above, the disk at 'path'\n\t\t * would have missed some transaction when it was away and\n\t\t * in case of 2a the stale bdev has to be updated as well.\n\t\t * 2b must not be allowed at all time.\n\t\t */\n\n\t\t/*\n\t\t * For now, we do allow update to btrfs_fs_device through the\n\t\t * btrfs dev scan cli after FS has been mounted.  We're still\n\t\t * tracking a problem where systems fail mount by subvolume id\n\t\t * when we reject replacement on a mounted FS.\n\t\t */\n\t\tif (!fs_devices->opened && found_transid < device->generation) {\n\t\t\t/*\n\t\t\t * That is if the FS is _not_ mounted and if you\n\t\t\t * are here, that means there is more than one\n\t\t\t * disk with same uuid and devid.We keep the one\n\t\t\t * with larger generation number or the last-in if\n\t\t\t * generation are equal.\n\t\t\t */\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EEXIST);\n\t\t}\n\n\t\t/*\n\t\t * We are going to replace the device path for a given devid,\n\t\t * make sure it's the same device if the device is mounted\n\t\t */\n\t\tif (device->bdev) {\n\t\t\tstruct block_device *path_bdev;\n\n\t\t\tpath_bdev = lookup_bdev(path);\n\t\t\tif (IS_ERR(path_bdev)) {\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\treturn ERR_CAST(path_bdev);\n\t\t\t}\n\n\t\t\tif (device->bdev != path_bdev) {\n\t\t\t\tbdput(path_bdev);\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\tbtrfs_warn_in_rcu(device->fs_info,\n\t\t\t\"duplicate device fsid:devid for %pU:%llu old:%s new:%s\",\n\t\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\t\trcu_str_deref(device->name), path);\n\t\t\t\treturn ERR_PTR(-EEXIST);\n\t\t\t}\n\t\t\tbdput(path_bdev);\n\t\t\tbtrfs_info_in_rcu(device->fs_info,\n\t\t\t\t\"device fsid %pU devid %llu moved old:%s new:%s\",\n\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\trcu_str_deref(device->name), path);\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_string_free(device->name);\n\t\trcu_assign_pointer(device->name, name);\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\tfs_devices->missing_devices--;\n\t\t\tclear_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\t}\n\n\t/*\n\t * Unmount does not free the btrfs_device struct but would zero\n\t * generation along with most of the other members. So just update\n\t * it back. We need it to pick the disk with largest generation\n\t * (as above).\n\t */\n\tif (!fs_devices->opened) {\n\t\tdevice->generation = found_transid;\n\t\tfs_devices->latest_generation = max_t(u64, found_transid,\n\t\t\t\t\t\tfs_devices->latest_generation);\n\t}\n\n\tfs_devices->total_devices = btrfs_super_num_devices(disk_super);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\treturn device;\n}",
                        "cve_id": "CVE-2019-18885"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "unsigned int\nipt_do_table(struct sk_buff *skb,\n\t     const struct nf_hook_state *state,\n\t     struct xt_table *table)\n{\n\tunsigned int hook = state->hook;\n\tstatic const char nulldevname[IFNAMSIZ] __attribute__((aligned(sizeof(long))));\n\tconst struct iphdr *ip;\n\t/* Initializing verdict to NF_DROP keeps gcc happy. */\n\tunsigned int verdict = NF_DROP;\n\tconst char *indev, *outdev;\n\tconst void *table_base;\n\tstruct ipt_entry *e, **jumpstack;\n\tunsigned int stackidx, cpu;\n\tconst struct xt_table_info *private;\n\tstruct xt_action_param acpar;\n\tunsigned int addend;\n\n\t/* Initialization */\n\tstackidx = 0;\n\tip = ip_hdr(skb);\n\tindev = state->in ? state->in->name : nulldevname;\n\toutdev = state->out ? state->out->name : nulldevname;\n\t/* We handle fragments by dealing with the first fragment as\n\t * if it was a normal packet.  All other fragments are treated\n\t * normally, except that they will NEVER match rules that ask\n\t * things we don't know, ie. tcp syn flag or ports).  If the\n\t * rule is also a fragment-specific rule, non-fragments won't\n\t * match it. */\n\tacpar.fragoff = ntohs(ip->frag_off) & IP_OFFSET;\n\tacpar.thoff   = ip_hdrlen(skb);\n\tacpar.hotdrop = false;\n\tacpar.state   = state;\n\n\tWARN_ON(!(table->valid_hooks & (1 << hook)));\n\tlocal_bh_disable();\n\taddend = xt_write_recseq_begin();\n\tprivate = READ_ONCE(table->private); /* Address dependency. */\n\tcpu        = smp_processor_id();\n\ttable_base = private->entries;\n\tjumpstack  = (struct ipt_entry **)private->jumpstack[cpu];\n\n\t/* Switch to alternate jumpstack if we're being invoked via TEE.\n\t * TEE issues XT_CONTINUE verdict on original skb so we must not\n\t * clobber the jumpstack.\n\t *\n\t * For recursion via REJECT or SYNPROXY the stack will be clobbered\n\t * but it is no problem since absolute verdict is issued by these.\n\t */\n\tif (static_key_false(&xt_tee_enabled))\n\t\tjumpstack += private->stacksize * __this_cpu_read(nf_skb_duplicated);\n\n\te = get_entry(table_base, private->hook_entry[hook]);\n\n\tdo {\n\t\tconst struct xt_entry_target *t;\n\t\tconst struct xt_entry_match *ematch;\n\t\tstruct xt_counters *counter;\n\n\t\tWARN_ON(!e);\n\t\tif (!ip_packet_match(ip, indev, outdev,\n\t\t    &e->ip, acpar.fragoff)) {\n no_match:\n\t\t\te = ipt_next_entry(e);\n\t\t\tcontinue;\n\t\t}\n\n\t\txt_ematch_foreach(ematch, e) {\n\t\t\tacpar.match     = ematch->u.kernel.match;\n\t\t\tacpar.matchinfo = ematch->data;\n\t\t\tif (!acpar.match->match(skb, &acpar))\n\t\t\t\tgoto no_match;\n\t\t}\n\n\t\tcounter = xt_get_this_cpu_counter(&e->counters);\n\t\tADD_COUNTER(*counter, skb->len, 1);\n\n\t\tt = ipt_get_target(e);\n\t\tWARN_ON(!t->u.kernel.target);\n\n#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE)\n\t\t/* The packet is traced: log it */\n\t\tif (unlikely(skb->nf_trace))\n\t\t\ttrace_packet(state->net, skb, hook, state->in,\n\t\t\t\t     state->out, table->name, private, e);\n#endif\n\t\t/* Standard target? */\n\t\tif (!t->u.kernel.target->target) {\n\t\t\tint v;\n\n\t\t\tv = ((struct xt_standard_target *)t)->verdict;\n\t\t\tif (v < 0) {\n\t\t\t\t/* Pop from stack? */\n\t\t\t\tif (v != XT_RETURN) {\n\t\t\t\t\tverdict = (unsigned int)(-v) - 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (stackidx == 0) {\n\t\t\t\t\te = get_entry(table_base,\n\t\t\t\t\t    private->underflow[hook]);\n\t\t\t\t} else {\n\t\t\t\t\te = jumpstack[--stackidx];\n\t\t\t\t\te = ipt_next_entry(e);\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (table_base + v != ipt_next_entry(e) &&\n\t\t\t    !(e->ip.flags & IPT_F_GOTO))\n\t\t\t\tjumpstack[stackidx++] = e;\n\n\t\t\te = get_entry(table_base, v);\n\t\t\tcontinue;\n\t\t}\n\n\t\tacpar.target   = t->u.kernel.target;\n\t\tacpar.targinfo = t->data;\n\n\t\tverdict = t->u.kernel.target->target(skb, &acpar);\n\t\tif (verdict == XT_CONTINUE) {\n\t\t\t/* Target might have changed stuff. */\n\t\t\tip = ip_hdr(skb);\n\t\t\te = ipt_next_entry(e);\n\t\t} else {\n\t\t\t/* Verdict */\n\t\t\tbreak;\n\t\t}\n\t} while (!acpar.hotdrop);\n\n\txt_write_recseq_end(addend);\n\tlocal_bh_enable();\n\n\tif (acpar.hotdrop)\n\t\treturn NF_DROP;\n\telse return verdict;\n}",
                        "code_after_change": "unsigned int\nipt_do_table(struct sk_buff *skb,\n\t     const struct nf_hook_state *state,\n\t     struct xt_table *table)\n{\n\tunsigned int hook = state->hook;\n\tstatic const char nulldevname[IFNAMSIZ] __attribute__((aligned(sizeof(long))));\n\tconst struct iphdr *ip;\n\t/* Initializing verdict to NF_DROP keeps gcc happy. */\n\tunsigned int verdict = NF_DROP;\n\tconst char *indev, *outdev;\n\tconst void *table_base;\n\tstruct ipt_entry *e, **jumpstack;\n\tunsigned int stackidx, cpu;\n\tconst struct xt_table_info *private;\n\tstruct xt_action_param acpar;\n\tunsigned int addend;\n\n\t/* Initialization */\n\tstackidx = 0;\n\tip = ip_hdr(skb);\n\tindev = state->in ? state->in->name : nulldevname;\n\toutdev = state->out ? state->out->name : nulldevname;\n\t/* We handle fragments by dealing with the first fragment as\n\t * if it was a normal packet.  All other fragments are treated\n\t * normally, except that they will NEVER match rules that ask\n\t * things we don't know, ie. tcp syn flag or ports).  If the\n\t * rule is also a fragment-specific rule, non-fragments won't\n\t * match it. */\n\tacpar.fragoff = ntohs(ip->frag_off) & IP_OFFSET;\n\tacpar.thoff   = ip_hdrlen(skb);\n\tacpar.hotdrop = false;\n\tacpar.state   = state;\n\n\tWARN_ON(!(table->valid_hooks & (1 << hook)));\n\tlocal_bh_disable();\n\taddend = xt_write_recseq_begin();\n\tprivate = READ_ONCE(table->private); /* Address dependency. */\n\tcpu        = smp_processor_id();\n\ttable_base = private->entries;\n\tjumpstack  = (struct ipt_entry **)private->jumpstack[cpu];\n\n\t/* Switch to alternate jumpstack if we're being invoked via TEE.\n\t * TEE issues XT_CONTINUE verdict on original skb so we must not\n\t * clobber the jumpstack.\n\t *\n\t * For recursion via REJECT or SYNPROXY the stack will be clobbered\n\t * but it is no problem since absolute verdict is issued by these.\n\t */\n\tif (static_key_false(&xt_tee_enabled))\n\t\tjumpstack += private->stacksize * __this_cpu_read(nf_skb_duplicated);\n\n\te = get_entry(table_base, private->hook_entry[hook]);\n\n\tdo {\n\t\tconst struct xt_entry_target *t;\n\t\tconst struct xt_entry_match *ematch;\n\t\tstruct xt_counters *counter;\n\n\t\tWARN_ON(!e);\n\t\tif (!ip_packet_match(ip, indev, outdev,\n\t\t    &e->ip, acpar.fragoff)) {\n no_match:\n\t\t\te = ipt_next_entry(e);\n\t\t\tcontinue;\n\t\t}\n\n\t\txt_ematch_foreach(ematch, e) {\n\t\t\tacpar.match     = ematch->u.kernel.match;\n\t\t\tacpar.matchinfo = ematch->data;\n\t\t\tif (!acpar.match->match(skb, &acpar))\n\t\t\t\tgoto no_match;\n\t\t}\n\n\t\tcounter = xt_get_this_cpu_counter(&e->counters);\n\t\tADD_COUNTER(*counter, skb->len, 1);\n\n\t\tt = ipt_get_target(e);\n\t\tWARN_ON(!t->u.kernel.target);\n\n#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE)\n\t\t/* The packet is traced: log it */\n\t\tif (unlikely(skb->nf_trace))\n\t\t\ttrace_packet(state->net, skb, hook, state->in,\n\t\t\t\t     state->out, table->name, private, e);\n#endif\n\t\t/* Standard target? */\n\t\tif (!t->u.kernel.target->target) {\n\t\t\tint v;\n\n\t\t\tv = ((struct xt_standard_target *)t)->verdict;\n\t\t\tif (v < 0) {\n\t\t\t\t/* Pop from stack? */\n\t\t\t\tif (v != XT_RETURN) {\n\t\t\t\t\tverdict = (unsigned int)(-v) - 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (stackidx == 0) {\n\t\t\t\t\te = get_entry(table_base,\n\t\t\t\t\t    private->underflow[hook]);\n\t\t\t\t} else {\n\t\t\t\t\te = jumpstack[--stackidx];\n\t\t\t\t\te = ipt_next_entry(e);\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (table_base + v != ipt_next_entry(e) &&\n\t\t\t    !(e->ip.flags & IPT_F_GOTO)) {\n\t\t\t\tif (unlikely(stackidx >= private->stacksize)) {\n\t\t\t\t\tverdict = NF_DROP;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tjumpstack[stackidx++] = e;\n\t\t\t}\n\n\t\t\te = get_entry(table_base, v);\n\t\t\tcontinue;\n\t\t}\n\n\t\tacpar.target   = t->u.kernel.target;\n\t\tacpar.targinfo = t->data;\n\n\t\tverdict = t->u.kernel.target->target(skb, &acpar);\n\t\tif (verdict == XT_CONTINUE) {\n\t\t\t/* Target might have changed stuff. */\n\t\t\tip = ip_hdr(skb);\n\t\t\te = ipt_next_entry(e);\n\t\t} else {\n\t\t\t/* Verdict */\n\t\t\tbreak;\n\t\t}\n\t} while (!acpar.hotdrop);\n\n\txt_write_recseq_end(addend);\n\tlocal_bh_enable();\n\n\tif (acpar.hotdrop)\n\t\treturn NF_DROP;\n\telse return verdict;\n}",
                        "cve_id": "CVE-2018-1065"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 1998,
            "cve_id": "CVE-2019-10207",
            "code_snippet": "static int bcm_open(struct hci_uart *hu)\n{\n\tstruct bcm_data *bcm;\n\tstruct list_head *p;\n\tint err;\n\n\tbt_dev_dbg(hu->hdev, \"hu %p\", hu);\n\n\tbcm = kzalloc(sizeof(*bcm), GFP_KERNEL);\n\tif (!bcm)\n\t\treturn -ENOMEM;\n\n\tskb_queue_head_init(&bcm->txq);\n\n\thu->priv = bcm;\n\n\tmutex_lock(&bcm_device_lock);\n\n\tif (hu->serdev) {\n\t\tbcm->dev = serdev_device_get_drvdata(hu->serdev);\n\t\tgoto out;\n\t}\n\n\tif (!hu->tty->dev)\n\t\tgoto out;\n\n\tlist_for_each(p, &bcm_device_list) {\n\t\tstruct bcm_device *dev = list_entry(p, struct bcm_device, list);\n\n\t\t/* Retrieve saved bcm_device based on parent of the\n\t\t * platform device (saved during device probe) and\n\t\t * parent of tty device used by hci_uart\n\t\t */\n\t\tif (hu->tty->dev->parent == dev->dev->parent) {\n\t\t\tbcm->dev = dev;\n#ifdef CONFIG_PM\n\t\t\tdev->hu = hu;\n#endif\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\tif (bcm->dev) {\n\t\thu->init_speed = bcm->dev->init_speed;\n\t\thu->oper_speed = bcm->dev->oper_speed;\n\t\terr = bcm_gpio_set_power(bcm->dev, true);\n\t\tif (err)\n\t\t\tgoto err_unset_hu;\n\t}\n\n\tmutex_unlock(&bcm_device_lock);\n\treturn 0;\n\nerr_unset_hu:\n#ifdef CONFIG_PM\n\tif (!hu->serdev)\n\t\tbcm->dev->hu = NULL;\n#endif\n\tmutex_unlock(&bcm_device_lock);\n\thu->priv = NULL;\n\tkfree(bcm);\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int mrvl_open(struct hci_uart *hu)\n{\n\tstruct mrvl_data *mrvl;\n\tint ret;\n\n\tBT_DBG(\"hu %p\", hu);\n\n\tmrvl = kzalloc(sizeof(*mrvl), GFP_KERNEL);\n\tif (!mrvl)\n\t\treturn -ENOMEM;\n\n\tskb_queue_head_init(&mrvl->txq);\n\tskb_queue_head_init(&mrvl->rawq);\n\n\tset_bit(STATE_CHIP_VER_PENDING, &mrvl->flags);\n\n\thu->priv = mrvl;\n\n\tif (hu->serdev) {\n\t\tret = serdev_device_open(hu->serdev);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\nerr:\n\tkfree(mrvl);\n\n\treturn ret;\n}",
                        "code_after_change": "static int mrvl_open(struct hci_uart *hu)\n{\n\tstruct mrvl_data *mrvl;\n\tint ret;\n\n\tBT_DBG(\"hu %p\", hu);\n\n\tif (!hci_uart_has_flow_control(hu))\n\t\treturn -EOPNOTSUPP;\n\n\tmrvl = kzalloc(sizeof(*mrvl), GFP_KERNEL);\n\tif (!mrvl)\n\t\treturn -ENOMEM;\n\n\tskb_queue_head_init(&mrvl->txq);\n\tskb_queue_head_init(&mrvl->rawq);\n\n\tset_bit(STATE_CHIP_VER_PENDING, &mrvl->flags);\n\n\thu->priv = mrvl;\n\n\tif (hu->serdev) {\n\t\tret = serdev_device_open(hu->serdev);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\nerr:\n\tkfree(mrvl);\n\n\treturn ret;\n}",
                        "cve_id": "CVE-2019-10207"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1999,
            "cve_id": "CVE-2019-10207",
            "code_snippet": "static int intel_open(struct hci_uart *hu)\n{\n\tstruct intel_data *intel;\n\n\tBT_DBG(\"hu %p\", hu);\n\n\tintel = kzalloc(sizeof(*intel), GFP_KERNEL);\n\tif (!intel)\n\t\treturn -ENOMEM;\n\n\tskb_queue_head_init(&intel->txq);\n\tINIT_WORK(&intel->busy_work, intel_busy_work);\n\n\tintel->hu = hu;\n\n\thu->priv = intel;\n\n\tif (!intel_set_power(hu, true))\n\t\tset_bit(STATE_BOOTING, &intel->flags);\n\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int mrvl_open(struct hci_uart *hu)\n{\n\tstruct mrvl_data *mrvl;\n\tint ret;\n\n\tBT_DBG(\"hu %p\", hu);\n\n\tmrvl = kzalloc(sizeof(*mrvl), GFP_KERNEL);\n\tif (!mrvl)\n\t\treturn -ENOMEM;\n\n\tskb_queue_head_init(&mrvl->txq);\n\tskb_queue_head_init(&mrvl->rawq);\n\n\tset_bit(STATE_CHIP_VER_PENDING, &mrvl->flags);\n\n\thu->priv = mrvl;\n\n\tif (hu->serdev) {\n\t\tret = serdev_device_open(hu->serdev);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\nerr:\n\tkfree(mrvl);\n\n\treturn ret;\n}",
                        "code_after_change": "static int mrvl_open(struct hci_uart *hu)\n{\n\tstruct mrvl_data *mrvl;\n\tint ret;\n\n\tBT_DBG(\"hu %p\", hu);\n\n\tif (!hci_uart_has_flow_control(hu))\n\t\treturn -EOPNOTSUPP;\n\n\tmrvl = kzalloc(sizeof(*mrvl), GFP_KERNEL);\n\tif (!mrvl)\n\t\treturn -ENOMEM;\n\n\tskb_queue_head_init(&mrvl->txq);\n\tskb_queue_head_init(&mrvl->rawq);\n\n\tset_bit(STATE_CHIP_VER_PENDING, &mrvl->flags);\n\n\thu->priv = mrvl;\n\n\tif (hu->serdev) {\n\t\tret = serdev_device_open(hu->serdev);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\nerr:\n\tkfree(mrvl);\n\n\treturn ret;\n}",
                        "cve_id": "CVE-2019-10207"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2053,
            "cve_id": "CVE-2019-12818",
            "code_snippet": "int nfc_llcp_send_connect(struct nfc_llcp_sock *sock)\n{\n\tstruct nfc_llcp_local *local;\n\tstruct sk_buff *skb;\n\tu8 *service_name_tlv = NULL, service_name_tlv_length;\n\tu8 *miux_tlv = NULL, miux_tlv_length;\n\tu8 *rw_tlv = NULL, rw_tlv_length, rw;\n\tint err;\n\tu16 size = 0;\n\t__be16 miux;\n\n\tpr_debug(\"Sending CONNECT\\n\");\n\n\tlocal = sock->local;\n\tif (local == NULL)\n\t\treturn -ENODEV;\n\n\tif (sock->service_name != NULL) {\n\t\tservice_name_tlv = nfc_llcp_build_tlv(LLCP_TLV_SN,\n\t\t\t\t\t\t      sock->service_name,\n\t\t\t\t\t\t      sock->service_name_len,\n\t\t\t\t\t\t      &service_name_tlv_length);\n\t\tsize += service_name_tlv_length;\n\t}\n\n\t/* If the socket parameters are not set, use the local ones */\n\tmiux = be16_to_cpu(sock->miux) > LLCP_MAX_MIUX ?\n\t\tlocal->miux : sock->miux;\n\trw = sock->rw > LLCP_MAX_RW ? local->rw : sock->rw;\n\n\tmiux_tlv = nfc_llcp_build_tlv(LLCP_TLV_MIUX, (u8 *)&miux, 0,\n\t\t\t\t      &miux_tlv_length);\n\tsize += miux_tlv_length;\n\n\trw_tlv = nfc_llcp_build_tlv(LLCP_TLV_RW, &rw, 0, &rw_tlv_length);\n\tsize += rw_tlv_length;\n\n\tpr_debug(\"SKB size %d SN length %zu\\n\", size, sock->service_name_len);\n\n\tskb = llcp_allocate_pdu(sock, LLCP_PDU_CONNECT, size);\n\tif (skb == NULL) {\n\t\terr = -ENOMEM;\n\t\tgoto error_tlv;\n\t}\n\n\tllcp_add_tlv(skb, service_name_tlv, service_name_tlv_length);\n\tllcp_add_tlv(skb, miux_tlv, miux_tlv_length);\n\tllcp_add_tlv(skb, rw_tlv, rw_tlv_length);\n\n\tskb_queue_tail(&local->tx_queue, skb);\n\n\terr = 0;\n\nerror_tlv:\n\tif (err)\n\t\tpr_err(\"error %d\\n\", err);\n\n\tkfree(service_name_tlv);\n\tkfree(miux_tlv);\n\tkfree(rw_tlv);\n\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int nfc_llcp_send_cc(struct nfc_llcp_sock *sock)\n{\n\tstruct nfc_llcp_local *local;\n\tstruct sk_buff *skb;\n\tu8 *miux_tlv = NULL, miux_tlv_length;\n\tu8 *rw_tlv = NULL, rw_tlv_length, rw;\n\tint err;\n\tu16 size = 0;\n\t__be16 miux;\n\n\tpr_debug(\"Sending CC\\n\");\n\n\tlocal = sock->local;\n\tif (local == NULL)\n\t\treturn -ENODEV;\n\n\t/* If the socket parameters are not set, use the local ones */\n\tmiux = be16_to_cpu(sock->miux) > LLCP_MAX_MIUX ?\n\t\tlocal->miux : sock->miux;\n\trw = sock->rw > LLCP_MAX_RW ? local->rw : sock->rw;\n\n\tmiux_tlv = nfc_llcp_build_tlv(LLCP_TLV_MIUX, (u8 *)&miux, 0,\n\t\t\t\t      &miux_tlv_length);\n\tsize += miux_tlv_length;\n\n\trw_tlv = nfc_llcp_build_tlv(LLCP_TLV_RW, &rw, 0, &rw_tlv_length);\n\tsize += rw_tlv_length;\n\n\tskb = llcp_allocate_pdu(sock, LLCP_PDU_CC, size);\n\tif (skb == NULL) {\n\t\terr = -ENOMEM;\n\t\tgoto error_tlv;\n\t}\n\n\tllcp_add_tlv(skb, miux_tlv, miux_tlv_length);\n\tllcp_add_tlv(skb, rw_tlv, rw_tlv_length);\n\n\tskb_queue_tail(&local->tx_queue, skb);\n\n\terr = 0;\n\nerror_tlv:\n\tif (err)\n\t\tpr_err(\"error %d\\n\", err);\n\n\tkfree(miux_tlv);\n\tkfree(rw_tlv);\n\n\treturn err;\n}",
                        "code_after_change": "int nfc_llcp_send_cc(struct nfc_llcp_sock *sock)\n{\n\tstruct nfc_llcp_local *local;\n\tstruct sk_buff *skb;\n\tu8 *miux_tlv = NULL, miux_tlv_length;\n\tu8 *rw_tlv = NULL, rw_tlv_length, rw;\n\tint err;\n\tu16 size = 0;\n\t__be16 miux;\n\n\tpr_debug(\"Sending CC\\n\");\n\n\tlocal = sock->local;\n\tif (local == NULL)\n\t\treturn -ENODEV;\n\n\t/* If the socket parameters are not set, use the local ones */\n\tmiux = be16_to_cpu(sock->miux) > LLCP_MAX_MIUX ?\n\t\tlocal->miux : sock->miux;\n\trw = sock->rw > LLCP_MAX_RW ? local->rw : sock->rw;\n\n\tmiux_tlv = nfc_llcp_build_tlv(LLCP_TLV_MIUX, (u8 *)&miux, 0,\n\t\t\t\t      &miux_tlv_length);\n\tif (!miux_tlv) {\n\t\terr = -ENOMEM;\n\t\tgoto error_tlv;\n\t}\n\tsize += miux_tlv_length;\n\n\trw_tlv = nfc_llcp_build_tlv(LLCP_TLV_RW, &rw, 0, &rw_tlv_length);\n\tif (!rw_tlv) {\n\t\terr = -ENOMEM;\n\t\tgoto error_tlv;\n\t}\n\tsize += rw_tlv_length;\n\n\tskb = llcp_allocate_pdu(sock, LLCP_PDU_CC, size);\n\tif (skb == NULL) {\n\t\terr = -ENOMEM;\n\t\tgoto error_tlv;\n\t}\n\n\tllcp_add_tlv(skb, miux_tlv, miux_tlv_length);\n\tllcp_add_tlv(skb, rw_tlv, rw_tlv_length);\n\n\tskb_queue_tail(&local->tx_queue, skb);\n\n\terr = 0;\n\nerror_tlv:\n\tif (err)\n\t\tpr_err(\"error %d\\n\", err);\n\n\tkfree(miux_tlv);\n\tkfree(rw_tlv);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2019-12818"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2095,
            "cve_id": "CVE-2019-15098",
            "code_snippet": "static struct ath6kl_urb_context *\nath6kl_usb_alloc_urb_from_pipe(struct ath6kl_usb_pipe *pipe)\n{\n\tstruct ath6kl_urb_context *urb_context = NULL;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&pipe->ar_usb->cs_lock, flags);\n\tif (!list_empty(&pipe->urb_list_head)) {\n\t\turb_context =\n\t\t    list_first_entry(&pipe->urb_list_head,\n\t\t\t\t     struct ath6kl_urb_context, link);\n\t\tlist_del(&urb_context->link);\n\t\tpipe->urb_cnt--;\n\t}\n\tspin_unlock_irqrestore(&pipe->ar_usb->cs_lock, flags);\n\n\treturn urb_context;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static void ath6kl_usb_free_urb_to_pipe(struct ath6kl_usb_pipe *pipe,\n\t\t\t\t\tstruct ath6kl_urb_context *urb_context)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&pipe->ar_usb->cs_lock, flags);\n\tpipe->urb_cnt++;\n\n\tlist_add(&urb_context->link, &pipe->urb_list_head);\n\tspin_unlock_irqrestore(&pipe->ar_usb->cs_lock, flags);\n}",
                        "code_after_change": "static void ath6kl_usb_free_urb_to_pipe(struct ath6kl_usb_pipe *pipe,\n\t\t\t\t\tstruct ath6kl_urb_context *urb_context)\n{\n\tunsigned long flags;\n\n\t/* bail if this pipe is not initialized */\n\tif (!pipe->ar_usb)\n\t\treturn;\n\n\tspin_lock_irqsave(&pipe->ar_usb->cs_lock, flags);\n\tpipe->urb_cnt++;\n\n\tlist_add(&urb_context->link, &pipe->urb_list_head);\n\tspin_unlock_irqrestore(&pipe->ar_usb->cs_lock, flags);\n}",
                        "cve_id": "CVE-2019-15098"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2097,
            "cve_id": "CVE-2019-15099",
            "code_snippet": "static void ath10k_usb_free_urb_to_pipe(struct ath10k_usb_pipe *pipe,\n\t\t\t\t\tstruct ath10k_urb_context *urb_context)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&pipe->ar_usb->cs_lock, flags);\n\n\tpipe->urb_cnt++;\n\tlist_add(&urb_context->link, &pipe->urb_list_head);\n\n\tspin_unlock_irqrestore(&pipe->ar_usb->cs_lock, flags);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static struct ath10k_urb_context *\nath10k_usb_alloc_urb_from_pipe(struct ath10k_usb_pipe *pipe)\n{\n\tstruct ath10k_urb_context *urb_context = NULL;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&pipe->ar_usb->cs_lock, flags);\n\tif (!list_empty(&pipe->urb_list_head)) {\n\t\turb_context = list_first_entry(&pipe->urb_list_head,\n\t\t\t\t\t       struct ath10k_urb_context, link);\n\t\tlist_del(&urb_context->link);\n\t\tpipe->urb_cnt--;\n\t}\n\tspin_unlock_irqrestore(&pipe->ar_usb->cs_lock, flags);\n\n\treturn urb_context;\n}",
                        "code_after_change": "static struct ath10k_urb_context *\nath10k_usb_alloc_urb_from_pipe(struct ath10k_usb_pipe *pipe)\n{\n\tstruct ath10k_urb_context *urb_context = NULL;\n\tunsigned long flags;\n\n\t/* bail if this pipe is not initialized */\n\tif (!pipe->ar_usb)\n\t\treturn NULL;\n\n\tspin_lock_irqsave(&pipe->ar_usb->cs_lock, flags);\n\tif (!list_empty(&pipe->urb_list_head)) {\n\t\turb_context = list_first_entry(&pipe->urb_list_head,\n\t\t\t\t\t       struct ath10k_urb_context, link);\n\t\tlist_del(&urb_context->link);\n\t\tpipe->urb_cnt--;\n\t}\n\tspin_unlock_irqrestore(&pipe->ar_usb->cs_lock, flags);\n\n\treturn urb_context;\n}",
                        "cve_id": "CVE-2019-15099"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void ath6kl_usb_free_urb_to_pipe(struct ath6kl_usb_pipe *pipe,\n\t\t\t\t\tstruct ath6kl_urb_context *urb_context)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&pipe->ar_usb->cs_lock, flags);\n\tpipe->urb_cnt++;\n\n\tlist_add(&urb_context->link, &pipe->urb_list_head);\n\tspin_unlock_irqrestore(&pipe->ar_usb->cs_lock, flags);\n}",
                        "code_after_change": "static void ath6kl_usb_free_urb_to_pipe(struct ath6kl_usb_pipe *pipe,\n\t\t\t\t\tstruct ath6kl_urb_context *urb_context)\n{\n\tunsigned long flags;\n\n\t/* bail if this pipe is not initialized */\n\tif (!pipe->ar_usb)\n\t\treturn;\n\n\tspin_lock_irqsave(&pipe->ar_usb->cs_lock, flags);\n\tpipe->urb_cnt++;\n\n\tlist_add(&urb_context->link, &pipe->urb_list_head);\n\tspin_unlock_irqrestore(&pipe->ar_usb->cs_lock, flags);\n}",
                        "cve_id": "CVE-2019-15098"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2116,
            "cve_id": "CVE-2019-15223",
            "code_snippet": "int line6_probe(struct usb_interface *interface,\n\t\tconst struct usb_device_id *id,\n\t\tconst char *driver_name,\n\t\tconst struct line6_properties *properties,\n\t\tint (*private_init)(struct usb_line6 *, const struct usb_device_id *id),\n\t\tsize_t data_size)\n{\n\tstruct usb_device *usbdev = interface_to_usbdev(interface);\n\tstruct snd_card *card;\n\tstruct usb_line6 *line6;\n\tint interface_number;\n\tint ret;\n\n\tif (WARN_ON(data_size < sizeof(*line6)))\n\t\treturn -EINVAL;\n\n\t/* we don't handle multiple configurations */\n\tif (usbdev->descriptor.bNumConfigurations != 1)\n\t\treturn -ENODEV;\n\n\tret = snd_card_new(&interface->dev,\n\t\t\t   SNDRV_DEFAULT_IDX1, SNDRV_DEFAULT_STR1,\n\t\t\t   THIS_MODULE, data_size, &card);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/* store basic data: */\n\tline6 = card->private_data;\n\tline6->card = card;\n\tline6->properties = properties;\n\tline6->usbdev = usbdev;\n\tline6->ifcdev = &interface->dev;\n\n\tstrcpy(card->id, properties->id);\n\tstrcpy(card->driver, driver_name);\n\tstrcpy(card->shortname, properties->name);\n\tsprintf(card->longname, \"Line 6 %s at USB %s\", properties->name,\n\t\tdev_name(line6->ifcdev));\n\tcard->private_free = line6_destruct;\n\n\tusb_set_intfdata(interface, line6);\n\n\t/* increment reference counters: */\n\tusb_get_dev(usbdev);\n\n\t/* initialize device info: */\n\tdev_info(&interface->dev, \"Line 6 %s found\\n\", properties->name);\n\n\t/* query interface number */\n\tinterface_number = interface->cur_altsetting->desc.bInterfaceNumber;\n\n\t/* TODO reserves the bus bandwidth even without actual transfer */\n\tret = usb_set_interface(usbdev, interface_number,\n\t\t\t\tproperties->altsetting);\n\tif (ret < 0) {\n\t\tdev_err(&interface->dev, \"set_interface failed\\n\");\n\t\tgoto error;\n\t}\n\n\tline6_get_usb_properties(line6);\n\n\tif (properties->capabilities & LINE6_CAP_CONTROL) {\n\t\tret = line6_init_cap_control(line6);\n\t\tif (ret < 0)\n\t\t\tgoto error;\n\t}\n\n\t/* initialize device data based on device: */\n\tret = private_init(line6, id);\n\tif (ret < 0)\n\t\tgoto error;\n\n\t/* creation of additional special files should go here */\n\n\tdev_info(&interface->dev, \"Line 6 %s now attached\\n\",\n\t\t properties->name);\n\n\treturn 0;\n\n error:\n\t/* we can call disconnect callback here because no close-sync is\n\t * needed yet at this point\n\t */\n\tline6_disconnect(interface);\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "void line6_disconnect(struct usb_interface *interface)\n{\n\tstruct usb_line6 *line6 = usb_get_intfdata(interface);\n\tstruct usb_device *usbdev = interface_to_usbdev(interface);\n\n\tif (!line6)\n\t\treturn;\n\n\tif (WARN_ON(usbdev != line6->usbdev))\n\t\treturn;\n\n\tif (line6->urb_listen != NULL)\n\t\tline6_stop_listen(line6);\n\n\tsnd_card_disconnect(line6->card);\n\tif (line6->line6pcm)\n\t\tline6_pcm_disconnect(line6->line6pcm);\n\tif (line6->disconnect)\n\t\tline6->disconnect(line6);\n\n\tdev_info(&interface->dev, \"Line 6 %s now disconnected\\n\",\n\t\t line6->properties->name);\n\n\t/* make sure the device isn't destructed twice: */\n\tusb_set_intfdata(interface, NULL);\n\n\tsnd_card_free_when_closed(line6->card);\n}",
                        "code_after_change": "void line6_disconnect(struct usb_interface *interface)\n{\n\tstruct usb_line6 *line6 = usb_get_intfdata(interface);\n\tstruct usb_device *usbdev = interface_to_usbdev(interface);\n\n\tif (!line6)\n\t\treturn;\n\n\tif (WARN_ON(usbdev != line6->usbdev))\n\t\treturn;\n\n\tcancel_delayed_work(&line6->startup_work);\n\n\tif (line6->urb_listen != NULL)\n\t\tline6_stop_listen(line6);\n\n\tsnd_card_disconnect(line6->card);\n\tif (line6->line6pcm)\n\t\tline6_pcm_disconnect(line6->line6pcm);\n\tif (line6->disconnect)\n\t\tline6->disconnect(line6);\n\n\tdev_info(&interface->dev, \"Line 6 %s now disconnected\\n\",\n\t\t line6->properties->name);\n\n\t/* make sure the device isn't destructed twice: */\n\tusb_set_intfdata(interface, NULL);\n\n\tsnd_card_free_when_closed(line6->card);\n}",
                        "cve_id": "CVE-2019-15223"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2117,
            "cve_id": "CVE-2019-15223",
            "code_snippet": "static void line6_toneport_disconnect(struct usb_line6 *line6)\n{\n\tstruct usb_line6_toneport *toneport =\n\t\t(struct usb_line6_toneport *)line6;\n\n\tcancel_delayed_work_sync(&toneport->pcm_work);\n\n\tif (toneport_has_led(toneport))\n\t\ttoneport_remove_leds(toneport);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int toneport_init(struct usb_line6 *line6,\n\t\t\t const struct usb_device_id *id)\n{\n\tint err;\n\tstruct usb_line6_toneport *toneport =  (struct usb_line6_toneport *) line6;\n\n\ttoneport->type = id->driver_info;\n\tINIT_DELAYED_WORK(&toneport->pcm_work, toneport_start_pcm);\n\n\tline6->disconnect = line6_toneport_disconnect;\n\n\t/* initialize PCM subsystem: */\n\terr = line6_init_pcm(line6, &toneport_pcm_properties);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* register monitor control: */\n\terr = snd_ctl_add(line6->card,\n\t\t\t  snd_ctl_new1(&toneport_control_monitor,\n\t\t\t\t       line6->line6pcm));\n\tif (err < 0)\n\t\treturn err;\n\n\t/* register source select control: */\n\tif (toneport_has_source_select(toneport)) {\n\t\terr =\n\t\t    snd_ctl_add(line6->card,\n\t\t\t\tsnd_ctl_new1(&toneport_control_source,\n\t\t\t\t\t     line6->line6pcm));\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\tline6_read_serial_number(line6, &toneport->serial_number);\n\tline6_read_data(line6, 0x80c2, &toneport->firmware_version, 1);\n\n\tif (toneport_has_led(toneport)) {\n\t\terr = toneport_init_leds(toneport);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\terr = toneport_setup(toneport);\n\tif (err)\n\t\treturn err;\n\n\t/* register audio system: */\n\treturn snd_card_register(line6->card);\n}",
                        "code_after_change": "static int toneport_init(struct usb_line6 *line6,\n\t\t\t const struct usb_device_id *id)\n{\n\tint err;\n\tstruct usb_line6_toneport *toneport =  (struct usb_line6_toneport *) line6;\n\n\ttoneport->type = id->driver_info;\n\n\tline6->disconnect = line6_toneport_disconnect;\n\tline6->startup = toneport_startup;\n\n\t/* initialize PCM subsystem: */\n\terr = line6_init_pcm(line6, &toneport_pcm_properties);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* register monitor control: */\n\terr = snd_ctl_add(line6->card,\n\t\t\t  snd_ctl_new1(&toneport_control_monitor,\n\t\t\t\t       line6->line6pcm));\n\tif (err < 0)\n\t\treturn err;\n\n\t/* register source select control: */\n\tif (toneport_has_source_select(toneport)) {\n\t\terr =\n\t\t    snd_ctl_add(line6->card,\n\t\t\t\tsnd_ctl_new1(&toneport_control_source,\n\t\t\t\t\t     line6->line6pcm));\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\tline6_read_serial_number(line6, &toneport->serial_number);\n\tline6_read_data(line6, 0x80c2, &toneport->firmware_version, 1);\n\n\tif (toneport_has_led(toneport)) {\n\t\terr = toneport_init_leds(toneport);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\terr = toneport_setup(toneport);\n\tif (err)\n\t\treturn err;\n\n\t/* register audio system: */\n\treturn snd_card_register(line6->card);\n}",
                        "cve_id": "CVE-2019-15223"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2137,
            "cve_id": "CVE-2019-15922",
            "code_snippet": "static void __exit pf_exit(void)\n{\n\tstruct pf_unit *pf;\n\tint unit;\n\tunregister_blkdev(major, name);\n\tfor (pf = units, unit = 0; unit < PF_UNITS; pf++, unit++) {\n\t\tif (pf->present)\n\t\t\tdel_gendisk(pf->disk);\n\n\t\tblk_cleanup_queue(pf->disk->queue);\n\t\tblk_mq_free_tag_set(&pf->tag_set);\n\t\tput_disk(pf->disk);\n\n\t\tif (pf->present)\n\t\t\tpi_release(pf->pi);\n\t}\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int pf_detect(void)\n{\n\tstruct pf_unit *pf = units;\n\tint k, unit;\n\n\tprintk(\"%s: %s version %s, major %d, cluster %d, nice %d\\n\",\n\t       name, name, PF_VERSION, major, cluster, nice);\n\n\tpar_drv = pi_register_driver(name);\n\tif (!par_drv) {\n\t\tpr_err(\"failed to register %s driver\\n\", name);\n\t\treturn -1;\n\t}\n\tk = 0;\n\tif (pf_drive_count == 0) {\n\t\tif (pi_init(pf->pi, 1, -1, -1, -1, -1, -1, pf_scratch, PI_PF,\n\t\t\t    verbose, pf->name)) {\n\t\t\tif (!pf_probe(pf) && pf->disk) {\n\t\t\t\tpf->present = 1;\n\t\t\t\tk++;\n\t\t\t} else\n\t\t\t\tpi_release(pf->pi);\n\t\t}\n\n\t} else\n\t\tfor (unit = 0; unit < PF_UNITS; unit++, pf++) {\n\t\t\tint *conf = *drives[unit];\n\t\t\tif (!conf[D_PRT])\n\t\t\t\tcontinue;\n\t\t\tif (pi_init(pf->pi, 0, conf[D_PRT], conf[D_MOD],\n\t\t\t\t    conf[D_UNI], conf[D_PRO], conf[D_DLY],\n\t\t\t\t    pf_scratch, PI_PF, verbose, pf->name)) {\n\t\t\t\tif (pf->disk && !pf_probe(pf)) {\n\t\t\t\t\tpf->present = 1;\n\t\t\t\t\tk++;\n\t\t\t\t} else\n\t\t\t\t\tpi_release(pf->pi);\n\t\t\t}\n\t\t}\n\tif (k)\n\t\treturn 0;\n\n\tprintk(\"%s: No ATAPI disk detected\\n\", name);\n\tfor (pf = units, unit = 0; unit < PF_UNITS; pf++, unit++) {\n\t\tblk_cleanup_queue(pf->disk->queue);\n\t\tpf->disk->queue = NULL;\n\t\tblk_mq_free_tag_set(&pf->tag_set);\n\t\tput_disk(pf->disk);\n\t}\n\tpi_unregister_driver(par_drv);\n\treturn -1;\n}",
                        "code_after_change": "static int pf_detect(void)\n{\n\tstruct pf_unit *pf = units;\n\tint k, unit;\n\n\tprintk(\"%s: %s version %s, major %d, cluster %d, nice %d\\n\",\n\t       name, name, PF_VERSION, major, cluster, nice);\n\n\tpar_drv = pi_register_driver(name);\n\tif (!par_drv) {\n\t\tpr_err(\"failed to register %s driver\\n\", name);\n\t\treturn -1;\n\t}\n\tk = 0;\n\tif (pf_drive_count == 0) {\n\t\tif (pi_init(pf->pi, 1, -1, -1, -1, -1, -1, pf_scratch, PI_PF,\n\t\t\t    verbose, pf->name)) {\n\t\t\tif (!pf_probe(pf) && pf->disk) {\n\t\t\t\tpf->present = 1;\n\t\t\t\tk++;\n\t\t\t} else\n\t\t\t\tpi_release(pf->pi);\n\t\t}\n\n\t} else\n\t\tfor (unit = 0; unit < PF_UNITS; unit++, pf++) {\n\t\t\tint *conf = *drives[unit];\n\t\t\tif (!conf[D_PRT])\n\t\t\t\tcontinue;\n\t\t\tif (pi_init(pf->pi, 0, conf[D_PRT], conf[D_MOD],\n\t\t\t\t    conf[D_UNI], conf[D_PRO], conf[D_DLY],\n\t\t\t\t    pf_scratch, PI_PF, verbose, pf->name)) {\n\t\t\t\tif (pf->disk && !pf_probe(pf)) {\n\t\t\t\t\tpf->present = 1;\n\t\t\t\t\tk++;\n\t\t\t\t} else\n\t\t\t\t\tpi_release(pf->pi);\n\t\t\t}\n\t\t}\n\tif (k)\n\t\treturn 0;\n\n\tprintk(\"%s: No ATAPI disk detected\\n\", name);\n\tfor (pf = units, unit = 0; unit < PF_UNITS; pf++, unit++) {\n\t\tif (!pf->disk)\n\t\t\tcontinue;\n\t\tblk_cleanup_queue(pf->disk->queue);\n\t\tpf->disk->queue = NULL;\n\t\tblk_mq_free_tag_set(&pf->tag_set);\n\t\tput_disk(pf->disk);\n\t}\n\tpi_unregister_driver(par_drv);\n\treturn -1;\n}",
                        "cve_id": "CVE-2019-15922"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2140,
            "cve_id": "CVE-2019-15923",
            "code_snippet": "static void pcd_init_units(void)\n{\n\tstruct pcd_unit *cd;\n\tint unit;\n\n\tpcd_drive_count = 0;\n\tfor (unit = 0, cd = pcd; unit < PCD_UNITS; unit++, cd++) {\n\t\tstruct gendisk *disk = alloc_disk(1);\n\n\t\tif (!disk)\n\t\t\tcontinue;\n\n\t\tdisk->queue = blk_mq_init_sq_queue(&cd->tag_set, &pcd_mq_ops,\n\t\t\t\t\t\t   1, BLK_MQ_F_SHOULD_MERGE);\n\t\tif (IS_ERR(disk->queue)) {\n\t\t\tdisk->queue = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\tINIT_LIST_HEAD(&cd->rq_list);\n\t\tdisk->queue->queuedata = cd;\n\t\tblk_queue_bounce_limit(disk->queue, BLK_BOUNCE_HIGH);\n\t\tcd->disk = disk;\n\t\tcd->pi = &cd->pia;\n\t\tcd->present = 0;\n\t\tcd->last_sense = 0;\n\t\tcd->changed = 1;\n\t\tcd->drive = (*drives[unit])[D_SLV];\n\t\tif ((*drives[unit])[D_PRT])\n\t\t\tpcd_drive_count++;\n\n\t\tcd->name = &cd->info.name[0];\n\t\tsnprintf(cd->name, sizeof(cd->info.name), \"%s%d\", name, unit);\n\t\tcd->info.ops = &pcd_dops;\n\t\tcd->info.handle = cd;\n\t\tcd->info.speed = 0;\n\t\tcd->info.capacity = 1;\n\t\tcd->info.mask = 0;\n\t\tdisk->major = major;\n\t\tdisk->first_minor = unit;\n\t\tstrcpy(disk->disk_name, cd->name);\t/* umm... */\n\t\tdisk->fops = &pcd_bdops;\n\t\tdisk->flags = GENHD_FL_BLOCK_EVENTS_ON_EXCL_WRITE;\n\t}\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int pcd_detect(void)\n{\n\tchar id[18];\n\tint k, unit;\n\tstruct pcd_unit *cd;\n\n\tprintk(\"%s: %s version %s, major %d, nice %d\\n\",\n\t       name, name, PCD_VERSION, major, nice);\n\n\tpar_drv = pi_register_driver(name);\n\tif (!par_drv) {\n\t\tpr_err(\"failed to register %s driver\\n\", name);\n\t\treturn -1;\n\t}\n\n\tk = 0;\n\tif (pcd_drive_count == 0) { /* nothing spec'd - so autoprobe for 1 */\n\t\tcd = pcd;\n\t\tif (pi_init(cd->pi, 1, -1, -1, -1, -1, -1, pcd_buffer,\n\t\t\t    PI_PCD, verbose, cd->name)) {\n\t\t\tif (!pcd_probe(cd, -1, id) && cd->disk) {\n\t\t\t\tcd->present = 1;\n\t\t\t\tk++;\n\t\t\t} else\n\t\t\t\tpi_release(cd->pi);\n\t\t}\n\t} else {\n\t\tfor (unit = 0, cd = pcd; unit < PCD_UNITS; unit++, cd++) {\n\t\t\tint *conf = *drives[unit];\n\t\t\tif (!conf[D_PRT])\n\t\t\t\tcontinue;\n\t\t\tif (!pi_init(cd->pi, 0, conf[D_PRT], conf[D_MOD],\n\t\t\t\t     conf[D_UNI], conf[D_PRO], conf[D_DLY],\n\t\t\t\t     pcd_buffer, PI_PCD, verbose, cd->name)) \n\t\t\t\tcontinue;\n\t\t\tif (!pcd_probe(cd, conf[D_SLV], id) && cd->disk) {\n\t\t\t\tcd->present = 1;\n\t\t\t\tk++;\n\t\t\t} else\n\t\t\t\tpi_release(cd->pi);\n\t\t}\n\t}\n\tif (k)\n\t\treturn 0;\n\n\tprintk(\"%s: No CD-ROM drive found\\n\", name);\n\tfor (unit = 0, cd = pcd; unit < PCD_UNITS; unit++, cd++) {\n\t\tblk_cleanup_queue(cd->disk->queue);\n\t\tcd->disk->queue = NULL;\n\t\tblk_mq_free_tag_set(&cd->tag_set);\n\t\tput_disk(cd->disk);\n\t}\n\tpi_unregister_driver(par_drv);\n\treturn -1;\n}",
                        "code_after_change": "static int pcd_detect(void)\n{\n\tchar id[18];\n\tint k, unit;\n\tstruct pcd_unit *cd;\n\n\tprintk(\"%s: %s version %s, major %d, nice %d\\n\",\n\t       name, name, PCD_VERSION, major, nice);\n\n\tpar_drv = pi_register_driver(name);\n\tif (!par_drv) {\n\t\tpr_err(\"failed to register %s driver\\n\", name);\n\t\treturn -1;\n\t}\n\n\tk = 0;\n\tif (pcd_drive_count == 0) { /* nothing spec'd - so autoprobe for 1 */\n\t\tcd = pcd;\n\t\tif (pi_init(cd->pi, 1, -1, -1, -1, -1, -1, pcd_buffer,\n\t\t\t    PI_PCD, verbose, cd->name)) {\n\t\t\tif (!pcd_probe(cd, -1, id) && cd->disk) {\n\t\t\t\tcd->present = 1;\n\t\t\t\tk++;\n\t\t\t} else\n\t\t\t\tpi_release(cd->pi);\n\t\t}\n\t} else {\n\t\tfor (unit = 0, cd = pcd; unit < PCD_UNITS; unit++, cd++) {\n\t\t\tint *conf = *drives[unit];\n\t\t\tif (!conf[D_PRT])\n\t\t\t\tcontinue;\n\t\t\tif (!pi_init(cd->pi, 0, conf[D_PRT], conf[D_MOD],\n\t\t\t\t     conf[D_UNI], conf[D_PRO], conf[D_DLY],\n\t\t\t\t     pcd_buffer, PI_PCD, verbose, cd->name)) \n\t\t\t\tcontinue;\n\t\t\tif (!pcd_probe(cd, conf[D_SLV], id) && cd->disk) {\n\t\t\t\tcd->present = 1;\n\t\t\t\tk++;\n\t\t\t} else\n\t\t\t\tpi_release(cd->pi);\n\t\t}\n\t}\n\tif (k)\n\t\treturn 0;\n\n\tprintk(\"%s: No CD-ROM drive found\\n\", name);\n\tfor (unit = 0, cd = pcd; unit < PCD_UNITS; unit++, cd++) {\n\t\tif (!cd->disk)\n\t\t\tcontinue;\n\t\tblk_cleanup_queue(cd->disk->queue);\n\t\tcd->disk->queue = NULL;\n\t\tblk_mq_free_tag_set(&cd->tag_set);\n\t\tput_disk(cd->disk);\n\t}\n\tpi_unregister_driver(par_drv);\n\treturn -1;\n}",
                        "cve_id": "CVE-2019-15923"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2216,
            "cve_id": "CVE-2019-18885",
            "code_snippet": "static noinline int btrfs_ioctl_resize(struct file *file,\n\t\t\t\t\tvoid __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tu64 new_size;\n\tu64 old_size;\n\tu64 devid = 1;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_ioctl_vol_args *vol_args;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_device *device = NULL;\n\tchar *sizestr;\n\tchar *retptr;\n\tchar *devstr = NULL;\n\tint ret = 0;\n\tint mod = 0;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tif (test_and_set_bit(BTRFS_FS_EXCL_OP, &fs_info->flags)) {\n\t\tmnt_drop_write_file(file);\n\t\treturn BTRFS_ERROR_DEV_EXCL_RUN_IN_PROGRESS;\n\t}\n\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args)) {\n\t\tret = PTR_ERR(vol_args);\n\t\tgoto out;\n\t}\n\n\tvol_args->name[BTRFS_PATH_NAME_MAX] = '\\0';\n\n\tsizestr = vol_args->name;\n\tdevstr = strchr(sizestr, ':');\n\tif (devstr) {\n\t\tsizestr = devstr + 1;\n\t\t*devstr = '\\0';\n\t\tdevstr = vol_args->name;\n\t\tret = kstrtoull(devstr, 10, &devid);\n\t\tif (ret)\n\t\t\tgoto out_free;\n\t\tif (!devid) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tbtrfs_info(fs_info, \"resizing devid %llu\", devid);\n\t}\n\n\tdevice = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL);\n\tif (!device) {\n\t\tbtrfs_info(fs_info, \"resizer unable to find device %llu\",\n\t\t\t   devid);\n\t\tret = -ENODEV;\n\t\tgoto out_free;\n\t}\n\n\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tbtrfs_info(fs_info,\n\t\t\t   \"resizer unable to apply on readonly device %llu\",\n\t\t       devid);\n\t\tret = -EPERM;\n\t\tgoto out_free;\n\t}\n\n\tif (!strcmp(sizestr, \"max\"))\n\t\tnew_size = device->bdev->bd_inode->i_size;\n\telse {\n\t\tif (sizestr[0] == '-') {\n\t\t\tmod = -1;\n\t\t\tsizestr++;\n\t\t} else if (sizestr[0] == '+') {\n\t\t\tmod = 1;\n\t\t\tsizestr++;\n\t\t}\n\t\tnew_size = memparse(sizestr, &retptr);\n\t\tif (*retptr != '\\0' || new_size == 0) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = -EPERM;\n\t\tgoto out_free;\n\t}\n\n\told_size = btrfs_device_get_total_bytes(device);\n\n\tif (mod < 0) {\n\t\tif (new_size > old_size) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tnew_size = old_size - new_size;\n\t} else if (mod > 0) {\n\t\tif (new_size > ULLONG_MAX - old_size) {\n\t\t\tret = -ERANGE;\n\t\t\tgoto out_free;\n\t\t}\n\t\tnew_size = old_size + new_size;\n\t}\n\n\tif (new_size < SZ_256M) {\n\t\tret = -EINVAL;\n\t\tgoto out_free;\n\t}\n\tif (new_size > device->bdev->bd_inode->i_size) {\n\t\tret = -EFBIG;\n\t\tgoto out_free;\n\t}\n\n\tnew_size = round_down(new_size, fs_info->sectorsize);\n\n\tbtrfs_info_in_rcu(fs_info, \"new size for %s is %llu\",\n\t\t\t  rcu_str_deref(device->name), new_size);\n\n\tif (new_size > old_size) {\n\t\ttrans = btrfs_start_transaction(root, 0);\n\t\tif (IS_ERR(trans)) {\n\t\t\tret = PTR_ERR(trans);\n\t\t\tgoto out_free;\n\t\t}\n\t\tret = btrfs_grow_device(trans, device, new_size);\n\t\tbtrfs_commit_transaction(trans);\n\t} else if (new_size < old_size) {\n\t\tret = btrfs_shrink_device(device, new_size);\n\t} /* equal, nothing need to do */\n\nout_free:\n\tkfree(vol_args);\nout:\n\tclear_bit(BTRFS_FS_EXCL_OP, &fs_info->flags);\n\tmnt_drop_write_file(file);\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int btrfs_rm_device(struct btrfs_fs_info *fs_info, const char *device_path,\n\t\t    u64 devid)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *cur_devices;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 num_devices;\n\tint ret = 0;\n\n\tmutex_lock(&uuid_mutex);\n\n\tnum_devices = btrfs_num_devices(fs_info);\n\n\tret = btrfs_check_raid_min_devices(fs_info, num_devices - 1);\n\tif (ret)\n\t\tgoto out;\n\n\tdevice = btrfs_find_device_by_devspec(fs_info, devid, device_path);\n\n\tif (IS_ERR(device)) {\n\t\tif (PTR_ERR(device) == -ENOENT &&\n\t\t    strcmp(device_path, \"missing\") == 0)\n\t\t\tret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND;\n\t\telse\n\t\t\tret = PTR_ERR(device);\n\t\tgoto out;\n\t}\n\n\tif (btrfs_pinned_by_swapfile(fs_info, device)) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t  \"cannot remove device %s (devid %llu) due to active swapfile\",\n\t\t\t\t  rcu_str_deref(device->name), device->devid);\n\t\tret = -ETXTBSY;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = BTRFS_ERROR_DEV_TGT_REPLACE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    fs_info->fs_devices->rw_devices == 1) {\n\t\tret = BTRFS_ERROR_DEV_ONLY_WRITABLE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tdevice->fs_devices->rw_devices--;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\n\tmutex_unlock(&uuid_mutex);\n\tret = btrfs_shrink_device(device, 0);\n\tif (!ret)\n\t\tbtrfs_reada_remove_dev(device);\n\tmutex_lock(&uuid_mutex);\n\tif (ret)\n\t\tgoto error_undo;\n\n\t/*\n\t * TODO: the superblock still includes this device in its num_devices\n\t * counter although write_all_supers() is not locked out. This\n\t * could give a filesystem state which requires a degraded mount.\n\t */\n\tret = btrfs_rm_dev_item(device);\n\tif (ret)\n\t\tgoto error_undo;\n\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tbtrfs_scrub_cancel_dev(device);\n\n\t/*\n\t * the device list mutex makes sure that we don't change\n\t * the device list while someone else is writing out all\n\t * the device supers. Whoever is writing all supers, should\n\t * lock the device list mutex before getting the number of\n\t * devices in the super block (super_copy). Conversely,\n\t * whoever updates the number of devices in the super block\n\t * (super_copy) should hold the device list mutex.\n\t */\n\n\t/*\n\t * In normal cases the cur_devices == fs_devices. But in case\n\t * of deleting a seed device, the cur_devices should point to\n\t * its own fs_devices listed under the fs_devices->seed.\n\t */\n\tcur_devices = device->fs_devices;\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_del_rcu(&device->dev_list);\n\n\tcur_devices->num_devices--;\n\tcur_devices->total_devices--;\n\t/* Update total_devices of the parent fs_devices if it's seed */\n\tif (cur_devices != fs_devices)\n\t\tfs_devices->total_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tcur_devices->missing_devices--;\n\n\tbtrfs_assign_next_active_device(device, NULL);\n\n\tif (device->bdev) {\n\t\tcur_devices->open_devices--;\n\t\t/* remove sysfs entry */\n\t\tbtrfs_sysfs_remove_device(device);\n\t}\n\n\tnum_devices = btrfs_super_num_devices(fs_info->super_copy) - 1;\n\tbtrfs_set_super_num_devices(fs_info->super_copy, num_devices);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * at this point, the device is zero sized and detached from\n\t * the devices list.  All that's left is to zero out the old\n\t * supers and free the device.\n\t */\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\tbtrfs_scratch_superblocks(fs_info, device->bdev,\n\t\t\t\t\t  device->name->str);\n\n\tbtrfs_close_bdev(device);\n\tsynchronize_rcu();\n\tbtrfs_free_device(device);\n\n\tif (cur_devices->open_devices == 0) {\n\t\tlist_del_init(&cur_devices->seed_list);\n\t\tclose_fs_devices(cur_devices);\n\t\tfree_fs_devices(cur_devices);\n\t}\n\nout:\n\tmutex_unlock(&uuid_mutex);\n\treturn ret;\n\nerror_undo:\n\tbtrfs_reada_undo_remove_dev(device);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_add(&device->dev_alloc_list,\n\t\t\t &fs_devices->alloc_list);\n\t\tdevice->fs_devices->rw_devices++;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\tgoto out;\n}",
                        "code_after_change": "int btrfs_rm_device(struct btrfs_fs_info *fs_info, const char *device_path,\n\t\t    u64 devid)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *cur_devices;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 num_devices;\n\tint ret = 0;\n\n\tmutex_lock(&uuid_mutex);\n\n\tnum_devices = btrfs_num_devices(fs_info);\n\n\tret = btrfs_check_raid_min_devices(fs_info, num_devices - 1);\n\tif (ret)\n\t\tgoto out;\n\n\tdevice = btrfs_find_device_by_devspec(fs_info, devid, device_path);\n\n\tif (IS_ERR(device)) {\n\t\tif (PTR_ERR(device) == -ENOENT &&\n\t\t    device_path && strcmp(device_path, \"missing\") == 0)\n\t\t\tret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND;\n\t\telse\n\t\t\tret = PTR_ERR(device);\n\t\tgoto out;\n\t}\n\n\tif (btrfs_pinned_by_swapfile(fs_info, device)) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t  \"cannot remove device %s (devid %llu) due to active swapfile\",\n\t\t\t\t  rcu_str_deref(device->name), device->devid);\n\t\tret = -ETXTBSY;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = BTRFS_ERROR_DEV_TGT_REPLACE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    fs_info->fs_devices->rw_devices == 1) {\n\t\tret = BTRFS_ERROR_DEV_ONLY_WRITABLE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tdevice->fs_devices->rw_devices--;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\n\tmutex_unlock(&uuid_mutex);\n\tret = btrfs_shrink_device(device, 0);\n\tif (!ret)\n\t\tbtrfs_reada_remove_dev(device);\n\tmutex_lock(&uuid_mutex);\n\tif (ret)\n\t\tgoto error_undo;\n\n\t/*\n\t * TODO: the superblock still includes this device in its num_devices\n\t * counter although write_all_supers() is not locked out. This\n\t * could give a filesystem state which requires a degraded mount.\n\t */\n\tret = btrfs_rm_dev_item(device);\n\tif (ret)\n\t\tgoto error_undo;\n\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tbtrfs_scrub_cancel_dev(device);\n\n\t/*\n\t * the device list mutex makes sure that we don't change\n\t * the device list while someone else is writing out all\n\t * the device supers. Whoever is writing all supers, should\n\t * lock the device list mutex before getting the number of\n\t * devices in the super block (super_copy). Conversely,\n\t * whoever updates the number of devices in the super block\n\t * (super_copy) should hold the device list mutex.\n\t */\n\n\t/*\n\t * In normal cases the cur_devices == fs_devices. But in case\n\t * of deleting a seed device, the cur_devices should point to\n\t * its own fs_devices listed under the fs_devices->seed.\n\t */\n\tcur_devices = device->fs_devices;\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_del_rcu(&device->dev_list);\n\n\tcur_devices->num_devices--;\n\tcur_devices->total_devices--;\n\t/* Update total_devices of the parent fs_devices if it's seed */\n\tif (cur_devices != fs_devices)\n\t\tfs_devices->total_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tcur_devices->missing_devices--;\n\n\tbtrfs_assign_next_active_device(device, NULL);\n\n\tif (device->bdev) {\n\t\tcur_devices->open_devices--;\n\t\t/* remove sysfs entry */\n\t\tbtrfs_sysfs_remove_device(device);\n\t}\n\n\tnum_devices = btrfs_super_num_devices(fs_info->super_copy) - 1;\n\tbtrfs_set_super_num_devices(fs_info->super_copy, num_devices);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * at this point, the device is zero sized and detached from\n\t * the devices list.  All that's left is to zero out the old\n\t * supers and free the device.\n\t */\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\tbtrfs_scratch_superblocks(fs_info, device->bdev,\n\t\t\t\t\t  device->name->str);\n\n\tbtrfs_close_bdev(device);\n\tsynchronize_rcu();\n\tbtrfs_free_device(device);\n\n\tif (cur_devices->open_devices == 0) {\n\t\tlist_del_init(&cur_devices->seed_list);\n\t\tclose_fs_devices(cur_devices);\n\t\tfree_fs_devices(cur_devices);\n\t}\n\nout:\n\tmutex_unlock(&uuid_mutex);\n\treturn ret;\n\nerror_undo:\n\tbtrfs_reada_undo_remove_dev(device);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_add(&device->dev_alloc_list,\n\t\t\t &fs_devices->alloc_list);\n\t\tdevice->fs_devices->rw_devices++;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\tgoto out;\n}",
                        "cve_id": "CVE-2021-3739"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2217,
            "cve_id": "CVE-2019-18885",
            "code_snippet": "static long btrfs_ioctl_dev_info(struct btrfs_fs_info *fs_info,\n\t\t\t\t void __user *arg)\n{\n\tstruct btrfs_ioctl_dev_info_args *di_args;\n\tstruct btrfs_device *dev;\n\tint ret = 0;\n\tchar *s_uuid = NULL;\n\n\tdi_args = memdup_user(arg, sizeof(*di_args));\n\tif (IS_ERR(di_args))\n\t\treturn PTR_ERR(di_args);\n\n\tif (!btrfs_is_empty_uuid(di_args->uuid))\n\t\ts_uuid = di_args->uuid;\n\n\trcu_read_lock();\n\tdev = btrfs_find_device(fs_info->fs_devices, di_args->devid, s_uuid,\n\t\t\t\tNULL);\n\n\tif (!dev) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tdi_args->devid = dev->devid;\n\tdi_args->bytes_used = btrfs_device_get_bytes_used(dev);\n\tdi_args->total_bytes = btrfs_device_get_total_bytes(dev);\n\tmemcpy(di_args->uuid, dev->uuid, sizeof(di_args->uuid));\n\tif (dev->name) {\n\t\tstrncpy(di_args->path, rcu_str_deref(dev->name),\n\t\t\t\tsizeof(di_args->path) - 1);\n\t\tdi_args->path[sizeof(di_args->path) - 1] = 0;\n\t} else {\n\t\tdi_args->path[0] = '\\0';\n\t}\n\nout:\n\trcu_read_unlock();\n\tif (ret == 0 && copy_to_user(arg, di_args, sizeof(*di_args)))\n\t\tret = -EFAULT;\n\n\tkfree(di_args);\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int read_one_chunk(struct btrfs_fs_info *fs_info, struct btrfs_key *key,\n\t\t\t  struct extent_buffer *leaf,\n\t\t\t  struct btrfs_chunk *chunk)\n{\n\tstruct btrfs_mapping_tree *map_tree = &fs_info->mapping_tree;\n\tstruct map_lookup *map;\n\tstruct extent_map *em;\n\tu64 logical;\n\tu64 length;\n\tu64 devid;\n\tu8 uuid[BTRFS_UUID_SIZE];\n\tint num_stripes;\n\tint ret;\n\tint i;\n\n\tlogical = key->offset;\n\tlength = btrfs_chunk_length(leaf, chunk);\n\tnum_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\n\tret = btrfs_check_chunk_valid(fs_info, leaf, chunk, logical);\n\tif (ret)\n\t\treturn ret;\n\n\tread_lock(&map_tree->map_tree.lock);\n\tem = lookup_extent_mapping(&map_tree->map_tree, logical, 1);\n\tread_unlock(&map_tree->map_tree.lock);\n\n\t/* already mapped? */\n\tif (em && em->start <= logical && em->start + em->len > logical) {\n\t\tfree_extent_map(em);\n\t\treturn 0;\n\t} else if (em) {\n\t\tfree_extent_map(em);\n\t}\n\n\tem = alloc_extent_map();\n\tif (!em)\n\t\treturn -ENOMEM;\n\tmap = kmalloc(map_lookup_size(num_stripes), GFP_NOFS);\n\tif (!map) {\n\t\tfree_extent_map(em);\n\t\treturn -ENOMEM;\n\t}\n\n\tset_bit(EXTENT_FLAG_FS_MAPPING, &em->flags);\n\tem->map_lookup = map;\n\tem->start = logical;\n\tem->len = length;\n\tem->orig_start = 0;\n\tem->block_start = 0;\n\tem->block_len = em->len;\n\n\tmap->num_stripes = num_stripes;\n\tmap->io_width = btrfs_chunk_io_width(leaf, chunk);\n\tmap->io_align = btrfs_chunk_io_align(leaf, chunk);\n\tmap->stripe_len = btrfs_chunk_stripe_len(leaf, chunk);\n\tmap->type = btrfs_chunk_type(leaf, chunk);\n\tmap->sub_stripes = btrfs_chunk_sub_stripes(leaf, chunk);\n\tmap->verified_stripes = 0;\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tmap->stripes[i].physical =\n\t\t\tbtrfs_stripe_offset_nr(leaf, chunk, i);\n\t\tdevid = btrfs_stripe_devid_nr(leaf, chunk, i);\n\t\tread_extent_buffer(leaf, uuid, (unsigned long)\n\t\t\t\t   btrfs_stripe_dev_uuid_nr(chunk, i),\n\t\t\t\t   BTRFS_UUID_SIZE);\n\t\tmap->stripes[i].dev = btrfs_find_device(fs_info->fs_devices,\n\t\t\t\t\t\t\tdevid, uuid, NULL);\n\t\tif (!map->stripes[i].dev &&\n\t\t    !btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tfree_extent_map(em);\n\t\t\tbtrfs_report_missing_device(fs_info, devid, uuid, true);\n\t\t\treturn -ENOENT;\n\t\t}\n\t\tif (!map->stripes[i].dev) {\n\t\t\tmap->stripes[i].dev =\n\t\t\t\tadd_missing_dev(fs_info->fs_devices, devid,\n\t\t\t\t\t\tuuid);\n\t\t\tif (IS_ERR(map->stripes[i].dev)) {\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\t\"failed to init missing dev %llu: %ld\",\n\t\t\t\t\tdevid, PTR_ERR(map->stripes[i].dev));\n\t\t\t\treturn PTR_ERR(map->stripes[i].dev);\n\t\t\t}\n\t\t\tbtrfs_report_missing_device(fs_info, devid, uuid, false);\n\t\t}\n\t\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t&(map->stripes[i].dev->dev_state));\n\n\t}\n\n\twrite_lock(&map_tree->map_tree.lock);\n\tret = add_extent_mapping(&map_tree->map_tree, em, 0);\n\twrite_unlock(&map_tree->map_tree.lock);\n\tif (ret < 0) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"failed to add chunk map, start=%llu len=%llu: %d\",\n\t\t\t  em->start, em->len, ret);\n\t}\n\tfree_extent_map(em);\n\n\treturn ret;\n}",
                        "code_after_change": "static int read_one_chunk(struct btrfs_fs_info *fs_info, struct btrfs_key *key,\n\t\t\t  struct extent_buffer *leaf,\n\t\t\t  struct btrfs_chunk *chunk)\n{\n\tstruct btrfs_mapping_tree *map_tree = &fs_info->mapping_tree;\n\tstruct map_lookup *map;\n\tstruct extent_map *em;\n\tu64 logical;\n\tu64 length;\n\tu64 devid;\n\tu8 uuid[BTRFS_UUID_SIZE];\n\tint num_stripes;\n\tint ret;\n\tint i;\n\n\tlogical = key->offset;\n\tlength = btrfs_chunk_length(leaf, chunk);\n\tnum_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\n\tret = btrfs_check_chunk_valid(fs_info, leaf, chunk, logical);\n\tif (ret)\n\t\treturn ret;\n\n\tread_lock(&map_tree->map_tree.lock);\n\tem = lookup_extent_mapping(&map_tree->map_tree, logical, 1);\n\tread_unlock(&map_tree->map_tree.lock);\n\n\t/* already mapped? */\n\tif (em && em->start <= logical && em->start + em->len > logical) {\n\t\tfree_extent_map(em);\n\t\treturn 0;\n\t} else if (em) {\n\t\tfree_extent_map(em);\n\t}\n\n\tem = alloc_extent_map();\n\tif (!em)\n\t\treturn -ENOMEM;\n\tmap = kmalloc(map_lookup_size(num_stripes), GFP_NOFS);\n\tif (!map) {\n\t\tfree_extent_map(em);\n\t\treturn -ENOMEM;\n\t}\n\n\tset_bit(EXTENT_FLAG_FS_MAPPING, &em->flags);\n\tem->map_lookup = map;\n\tem->start = logical;\n\tem->len = length;\n\tem->orig_start = 0;\n\tem->block_start = 0;\n\tem->block_len = em->len;\n\n\tmap->num_stripes = num_stripes;\n\tmap->io_width = btrfs_chunk_io_width(leaf, chunk);\n\tmap->io_align = btrfs_chunk_io_align(leaf, chunk);\n\tmap->stripe_len = btrfs_chunk_stripe_len(leaf, chunk);\n\tmap->type = btrfs_chunk_type(leaf, chunk);\n\tmap->sub_stripes = btrfs_chunk_sub_stripes(leaf, chunk);\n\tmap->verified_stripes = 0;\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tmap->stripes[i].physical =\n\t\t\tbtrfs_stripe_offset_nr(leaf, chunk, i);\n\t\tdevid = btrfs_stripe_devid_nr(leaf, chunk, i);\n\t\tread_extent_buffer(leaf, uuid, (unsigned long)\n\t\t\t\t   btrfs_stripe_dev_uuid_nr(chunk, i),\n\t\t\t\t   BTRFS_UUID_SIZE);\n\t\tmap->stripes[i].dev = btrfs_find_device(fs_info->fs_devices,\n\t\t\t\t\t\t\tdevid, uuid, NULL, true);\n\t\tif (!map->stripes[i].dev &&\n\t\t    !btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tfree_extent_map(em);\n\t\t\tbtrfs_report_missing_device(fs_info, devid, uuid, true);\n\t\t\treturn -ENOENT;\n\t\t}\n\t\tif (!map->stripes[i].dev) {\n\t\t\tmap->stripes[i].dev =\n\t\t\t\tadd_missing_dev(fs_info->fs_devices, devid,\n\t\t\t\t\t\tuuid);\n\t\t\tif (IS_ERR(map->stripes[i].dev)) {\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\t\"failed to init missing dev %llu: %ld\",\n\t\t\t\t\tdevid, PTR_ERR(map->stripes[i].dev));\n\t\t\t\treturn PTR_ERR(map->stripes[i].dev);\n\t\t\t}\n\t\t\tbtrfs_report_missing_device(fs_info, devid, uuid, false);\n\t\t}\n\t\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t&(map->stripes[i].dev->dev_state));\n\n\t}\n\n\twrite_lock(&map_tree->map_tree.lock);\n\tret = add_extent_mapping(&map_tree->map_tree, em, 0);\n\twrite_unlock(&map_tree->map_tree.lock);\n\tif (ret < 0) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"failed to add chunk map, start=%llu len=%llu: %d\",\n\t\t\t  em->start, em->len, ret);\n\t}\n\tfree_extent_map(em);\n\n\treturn ret;\n}",
                        "cve_id": "CVE-2019-18885"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2218,
            "cve_id": "CVE-2019-18885",
            "code_snippet": "int btrfs_scrub_dev(struct btrfs_fs_info *fs_info, u64 devid, u64 start,\n\t\t    u64 end, struct btrfs_scrub_progress *progress,\n\t\t    int readonly, int is_dev_replace)\n{\n\tstruct scrub_ctx *sctx;\n\tint ret;\n\tstruct btrfs_device *dev;\n\tunsigned int nofs_flag;\n\n\tif (btrfs_fs_closing(fs_info))\n\t\treturn -EINVAL;\n\n\tif (fs_info->nodesize > BTRFS_STRIPE_LEN) {\n\t\t/*\n\t\t * in this case scrub is unable to calculate the checksum\n\t\t * the way scrub is implemented. Do not handle this\n\t\t * situation at all because it won't ever happen.\n\t\t */\n\t\tbtrfs_err(fs_info,\n\t\t\t   \"scrub: size assumption nodesize <= BTRFS_STRIPE_LEN (%d <= %d) fails\",\n\t\t       fs_info->nodesize,\n\t\t       BTRFS_STRIPE_LEN);\n\t\treturn -EINVAL;\n\t}\n\n\tif (fs_info->sectorsize != PAGE_SIZE) {\n\t\t/* not supported for data w/o checksums */\n\t\tbtrfs_err_rl(fs_info,\n\t\t\t   \"scrub: size assumption sectorsize != PAGE_SIZE (%d != %lu) fails\",\n\t\t       fs_info->sectorsize, PAGE_SIZE);\n\t\treturn -EINVAL;\n\t}\n\n\tif (fs_info->nodesize >\n\t    PAGE_SIZE * SCRUB_MAX_PAGES_PER_BLOCK ||\n\t    fs_info->sectorsize > PAGE_SIZE * SCRUB_MAX_PAGES_PER_BLOCK) {\n\t\t/*\n\t\t * would exhaust the array bounds of pagev member in\n\t\t * struct scrub_block\n\t\t */\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"scrub: size assumption nodesize and sectorsize <= SCRUB_MAX_PAGES_PER_BLOCK (%d <= %d && %d <= %d) fails\",\n\t\t       fs_info->nodesize,\n\t\t       SCRUB_MAX_PAGES_PER_BLOCK,\n\t\t       fs_info->sectorsize,\n\t\t       SCRUB_MAX_PAGES_PER_BLOCK);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Allocate outside of device_list_mutex */\n\tsctx = scrub_setup_ctx(fs_info, is_dev_replace);\n\tif (IS_ERR(sctx))\n\t\treturn PTR_ERR(sctx);\n\n\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL);\n\tif (!dev || (test_bit(BTRFS_DEV_STATE_MISSING, &dev->dev_state) &&\n\t\t     !is_dev_replace)) {\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tret = -ENODEV;\n\t\tgoto out_free_ctx;\n\t}\n\n\tif (!is_dev_replace && !readonly &&\n\t    !test_bit(BTRFS_DEV_STATE_WRITEABLE, &dev->dev_state)) {\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tbtrfs_err_in_rcu(fs_info, \"scrub: device %s is not writable\",\n\t\t\t\trcu_str_deref(dev->name));\n\t\tret = -EROFS;\n\t\tgoto out_free_ctx;\n\t}\n\n\tmutex_lock(&fs_info->scrub_lock);\n\tif (!test_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &dev->dev_state) ||\n\t    test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &dev->dev_state)) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tret = -EIO;\n\t\tgoto out_free_ctx;\n\t}\n\n\tdown_read(&fs_info->dev_replace.rwsem);\n\tif (dev->scrub_ctx ||\n\t    (!is_dev_replace &&\n\t     btrfs_dev_replace_is_ongoing(&fs_info->dev_replace))) {\n\t\tup_read(&fs_info->dev_replace.rwsem);\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tret = -EINPROGRESS;\n\t\tgoto out_free_ctx;\n\t}\n\tup_read(&fs_info->dev_replace.rwsem);\n\n\tret = scrub_workers_get(fs_info, is_dev_replace);\n\tif (ret) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tgoto out_free_ctx;\n\t}\n\n\tsctx->readonly = readonly;\n\tdev->scrub_ctx = sctx;\n\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\n\t/*\n\t * checking @scrub_pause_req here, we can avoid\n\t * race between committing transaction and scrubbing.\n\t */\n\t__scrub_blocked_if_needed(fs_info);\n\tatomic_inc(&fs_info->scrubs_running);\n\tmutex_unlock(&fs_info->scrub_lock);\n\n\t/*\n\t * In order to avoid deadlock with reclaim when there is a transaction\n\t * trying to pause scrub, make sure we use GFP_NOFS for all the\n\t * allocations done at btrfs_scrub_pages() and scrub_pages_for_parity()\n\t * invoked by our callees. The pausing request is done when the\n\t * transaction commit starts, and it blocks the transaction until scrub\n\t * is paused (done at specific points at scrub_stripe() or right above\n\t * before incrementing fs_info->scrubs_running).\n\t */\n\tnofs_flag = memalloc_nofs_save();\n\tif (!is_dev_replace) {\n\t\t/*\n\t\t * by holding device list mutex, we can\n\t\t * kick off writing super in log tree sync.\n\t\t */\n\t\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\t\tret = scrub_supers(sctx, dev);\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t}\n\n\tif (!ret)\n\t\tret = scrub_enumerate_chunks(sctx, dev, start, end);\n\tmemalloc_nofs_restore(nofs_flag);\n\n\twait_event(sctx->list_wait, atomic_read(&sctx->bios_in_flight) == 0);\n\tatomic_dec(&fs_info->scrubs_running);\n\twake_up(&fs_info->scrub_pause_wait);\n\n\twait_event(sctx->list_wait, atomic_read(&sctx->workers_pending) == 0);\n\n\tif (progress)\n\t\tmemcpy(progress, &sctx->stat, sizeof(*progress));\n\n\tmutex_lock(&fs_info->scrub_lock);\n\tdev->scrub_ctx = NULL;\n\tscrub_workers_put(fs_info);\n\tmutex_unlock(&fs_info->scrub_lock);\n\n\tscrub_put_ctx(sctx);\n\n\treturn ret;\n\nout_free_ctx:\n\tscrub_free_ctx(sctx);\n\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int btrfs_rm_device(struct btrfs_fs_info *fs_info, const char *device_path,\n\t\t    u64 devid)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *cur_devices;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 num_devices;\n\tint ret = 0;\n\n\tmutex_lock(&uuid_mutex);\n\n\tnum_devices = btrfs_num_devices(fs_info);\n\n\tret = btrfs_check_raid_min_devices(fs_info, num_devices - 1);\n\tif (ret)\n\t\tgoto out;\n\n\tdevice = btrfs_find_device_by_devspec(fs_info, devid, device_path);\n\n\tif (IS_ERR(device)) {\n\t\tif (PTR_ERR(device) == -ENOENT &&\n\t\t    strcmp(device_path, \"missing\") == 0)\n\t\t\tret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND;\n\t\telse\n\t\t\tret = PTR_ERR(device);\n\t\tgoto out;\n\t}\n\n\tif (btrfs_pinned_by_swapfile(fs_info, device)) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t  \"cannot remove device %s (devid %llu) due to active swapfile\",\n\t\t\t\t  rcu_str_deref(device->name), device->devid);\n\t\tret = -ETXTBSY;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = BTRFS_ERROR_DEV_TGT_REPLACE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    fs_info->fs_devices->rw_devices == 1) {\n\t\tret = BTRFS_ERROR_DEV_ONLY_WRITABLE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tdevice->fs_devices->rw_devices--;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\n\tmutex_unlock(&uuid_mutex);\n\tret = btrfs_shrink_device(device, 0);\n\tif (!ret)\n\t\tbtrfs_reada_remove_dev(device);\n\tmutex_lock(&uuid_mutex);\n\tif (ret)\n\t\tgoto error_undo;\n\n\t/*\n\t * TODO: the superblock still includes this device in its num_devices\n\t * counter although write_all_supers() is not locked out. This\n\t * could give a filesystem state which requires a degraded mount.\n\t */\n\tret = btrfs_rm_dev_item(device);\n\tif (ret)\n\t\tgoto error_undo;\n\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tbtrfs_scrub_cancel_dev(device);\n\n\t/*\n\t * the device list mutex makes sure that we don't change\n\t * the device list while someone else is writing out all\n\t * the device supers. Whoever is writing all supers, should\n\t * lock the device list mutex before getting the number of\n\t * devices in the super block (super_copy). Conversely,\n\t * whoever updates the number of devices in the super block\n\t * (super_copy) should hold the device list mutex.\n\t */\n\n\t/*\n\t * In normal cases the cur_devices == fs_devices. But in case\n\t * of deleting a seed device, the cur_devices should point to\n\t * its own fs_devices listed under the fs_devices->seed.\n\t */\n\tcur_devices = device->fs_devices;\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_del_rcu(&device->dev_list);\n\n\tcur_devices->num_devices--;\n\tcur_devices->total_devices--;\n\t/* Update total_devices of the parent fs_devices if it's seed */\n\tif (cur_devices != fs_devices)\n\t\tfs_devices->total_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tcur_devices->missing_devices--;\n\n\tbtrfs_assign_next_active_device(device, NULL);\n\n\tif (device->bdev) {\n\t\tcur_devices->open_devices--;\n\t\t/* remove sysfs entry */\n\t\tbtrfs_sysfs_remove_device(device);\n\t}\n\n\tnum_devices = btrfs_super_num_devices(fs_info->super_copy) - 1;\n\tbtrfs_set_super_num_devices(fs_info->super_copy, num_devices);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * at this point, the device is zero sized and detached from\n\t * the devices list.  All that's left is to zero out the old\n\t * supers and free the device.\n\t */\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\tbtrfs_scratch_superblocks(fs_info, device->bdev,\n\t\t\t\t\t  device->name->str);\n\n\tbtrfs_close_bdev(device);\n\tsynchronize_rcu();\n\tbtrfs_free_device(device);\n\n\tif (cur_devices->open_devices == 0) {\n\t\tlist_del_init(&cur_devices->seed_list);\n\t\tclose_fs_devices(cur_devices);\n\t\tfree_fs_devices(cur_devices);\n\t}\n\nout:\n\tmutex_unlock(&uuid_mutex);\n\treturn ret;\n\nerror_undo:\n\tbtrfs_reada_undo_remove_dev(device);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_add(&device->dev_alloc_list,\n\t\t\t &fs_devices->alloc_list);\n\t\tdevice->fs_devices->rw_devices++;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\tgoto out;\n}",
                        "code_after_change": "int btrfs_rm_device(struct btrfs_fs_info *fs_info, const char *device_path,\n\t\t    u64 devid)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *cur_devices;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 num_devices;\n\tint ret = 0;\n\n\tmutex_lock(&uuid_mutex);\n\n\tnum_devices = btrfs_num_devices(fs_info);\n\n\tret = btrfs_check_raid_min_devices(fs_info, num_devices - 1);\n\tif (ret)\n\t\tgoto out;\n\n\tdevice = btrfs_find_device_by_devspec(fs_info, devid, device_path);\n\n\tif (IS_ERR(device)) {\n\t\tif (PTR_ERR(device) == -ENOENT &&\n\t\t    device_path && strcmp(device_path, \"missing\") == 0)\n\t\t\tret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND;\n\t\telse\n\t\t\tret = PTR_ERR(device);\n\t\tgoto out;\n\t}\n\n\tif (btrfs_pinned_by_swapfile(fs_info, device)) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t  \"cannot remove device %s (devid %llu) due to active swapfile\",\n\t\t\t\t  rcu_str_deref(device->name), device->devid);\n\t\tret = -ETXTBSY;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = BTRFS_ERROR_DEV_TGT_REPLACE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    fs_info->fs_devices->rw_devices == 1) {\n\t\tret = BTRFS_ERROR_DEV_ONLY_WRITABLE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tdevice->fs_devices->rw_devices--;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\n\tmutex_unlock(&uuid_mutex);\n\tret = btrfs_shrink_device(device, 0);\n\tif (!ret)\n\t\tbtrfs_reada_remove_dev(device);\n\tmutex_lock(&uuid_mutex);\n\tif (ret)\n\t\tgoto error_undo;\n\n\t/*\n\t * TODO: the superblock still includes this device in its num_devices\n\t * counter although write_all_supers() is not locked out. This\n\t * could give a filesystem state which requires a degraded mount.\n\t */\n\tret = btrfs_rm_dev_item(device);\n\tif (ret)\n\t\tgoto error_undo;\n\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tbtrfs_scrub_cancel_dev(device);\n\n\t/*\n\t * the device list mutex makes sure that we don't change\n\t * the device list while someone else is writing out all\n\t * the device supers. Whoever is writing all supers, should\n\t * lock the device list mutex before getting the number of\n\t * devices in the super block (super_copy). Conversely,\n\t * whoever updates the number of devices in the super block\n\t * (super_copy) should hold the device list mutex.\n\t */\n\n\t/*\n\t * In normal cases the cur_devices == fs_devices. But in case\n\t * of deleting a seed device, the cur_devices should point to\n\t * its own fs_devices listed under the fs_devices->seed.\n\t */\n\tcur_devices = device->fs_devices;\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_del_rcu(&device->dev_list);\n\n\tcur_devices->num_devices--;\n\tcur_devices->total_devices--;\n\t/* Update total_devices of the parent fs_devices if it's seed */\n\tif (cur_devices != fs_devices)\n\t\tfs_devices->total_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tcur_devices->missing_devices--;\n\n\tbtrfs_assign_next_active_device(device, NULL);\n\n\tif (device->bdev) {\n\t\tcur_devices->open_devices--;\n\t\t/* remove sysfs entry */\n\t\tbtrfs_sysfs_remove_device(device);\n\t}\n\n\tnum_devices = btrfs_super_num_devices(fs_info->super_copy) - 1;\n\tbtrfs_set_super_num_devices(fs_info->super_copy, num_devices);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * at this point, the device is zero sized and detached from\n\t * the devices list.  All that's left is to zero out the old\n\t * supers and free the device.\n\t */\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\tbtrfs_scratch_superblocks(fs_info, device->bdev,\n\t\t\t\t\t  device->name->str);\n\n\tbtrfs_close_bdev(device);\n\tsynchronize_rcu();\n\tbtrfs_free_device(device);\n\n\tif (cur_devices->open_devices == 0) {\n\t\tlist_del_init(&cur_devices->seed_list);\n\t\tclose_fs_devices(cur_devices);\n\t\tfree_fs_devices(cur_devices);\n\t}\n\nout:\n\tmutex_unlock(&uuid_mutex);\n\treturn ret;\n\nerror_undo:\n\tbtrfs_reada_undo_remove_dev(device);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_add(&device->dev_alloc_list,\n\t\t\t &fs_devices->alloc_list);\n\t\tdevice->fs_devices->rw_devices++;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\tgoto out;\n}",
                        "cve_id": "CVE-2021-3739"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static noinline struct btrfs_device *device_list_add(const char *path,\n\t\t\t   struct btrfs_super_block *disk_super,\n\t\t\t   bool *new_device_added)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *fs_devices = NULL;\n\tstruct rcu_string *name;\n\tu64 found_transid = btrfs_super_generation(disk_super);\n\tu64 devid = btrfs_stack_device_id(&disk_super->dev_item);\n\tbool has_metadata_uuid = (btrfs_super_incompat_flags(disk_super) &\n\t\tBTRFS_FEATURE_INCOMPAT_METADATA_UUID);\n\tbool fsid_change_in_progress = (btrfs_super_flags(disk_super) &\n\t\t\t\t\tBTRFS_SUPER_FLAG_CHANGING_FSID_V2);\n\n\tif (fsid_change_in_progress) {\n\t\tif (!has_metadata_uuid) {\n\t\t\t/*\n\t\t\t * When we have an image which has CHANGING_FSID_V2 set\n\t\t\t * it might belong to either a filesystem which has\n\t\t\t * disks with completed fsid change or it might belong\n\t\t\t * to fs with no UUID changes in effect, handle both.\n\t\t\t */\n\t\t\tfs_devices = find_fsid_inprogress(disk_super);\n\t\t\tif (!fs_devices)\n\t\t\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t\t} else {\n\t\t\tfs_devices = find_fsid_changed(disk_super);\n\t\t}\n\t} else if (has_metadata_uuid) {\n\t\tfs_devices = find_fsid(disk_super->fsid,\n\t\t\t\t       disk_super->metadata_uuid);\n\t} else {\n\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t}\n\n\n\tif (!fs_devices) {\n\t\tif (has_metadata_uuid)\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid,\n\t\t\t\t\t\t      disk_super->metadata_uuid);\n\t\telse\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid, NULL);\n\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn ERR_CAST(fs_devices);\n\n\t\tfs_devices->fsid_change = fsid_change_in_progress;\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_add(&fs_devices->fs_list, &fs_uuids);\n\n\t\tdevice = NULL;\n\t} else {\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tdevice = find_device(fs_devices, devid,\n\t\t\t\tdisk_super->dev_item.uuid);\n\n\t\t/*\n\t\t * If this disk has been pulled into an fs devices created by\n\t\t * a device which had the CHANGING_FSID_V2 flag then replace the\n\t\t * metadata_uuid/fsid values of the fs_devices.\n\t\t */\n\t\tif (has_metadata_uuid && fs_devices->fsid_change &&\n\t\t    found_transid > fs_devices->latest_generation) {\n\t\t\tmemcpy(fs_devices->fsid, disk_super->fsid,\n\t\t\t\t\tBTRFS_FSID_SIZE);\n\t\t\tmemcpy(fs_devices->metadata_uuid,\n\t\t\t\t\tdisk_super->metadata_uuid, BTRFS_FSID_SIZE);\n\n\t\t\tfs_devices->fsid_change = false;\n\t\t}\n\t}\n\n\tif (!device) {\n\t\tif (fs_devices->opened) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EBUSY);\n\t\t}\n\n\t\tdevice = btrfs_alloc_device(NULL, &devid,\n\t\t\t\t\t    disk_super->dev_item.uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t/* we can safely leave the fs_devices entry around */\n\t\t\treturn device;\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tbtrfs_free_device(device);\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_assign_pointer(device->name, name);\n\n\t\tlist_add_rcu(&device->dev_list, &fs_devices->devices);\n\t\tfs_devices->num_devices++;\n\n\t\tdevice->fs_devices = fs_devices;\n\t\t*new_device_added = true;\n\n\t\tif (disk_super->label[0])\n\t\t\tpr_info(\"BTRFS: device label %s devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->label, devid, found_transid, path);\n\t\telse\n\t\t\tpr_info(\"BTRFS: device fsid %pU devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->fsid, devid, found_transid, path);\n\n\t} else if (!device->name || strcmp(device->name->str, path)) {\n\t\t/*\n\t\t * When FS is already mounted.\n\t\t * 1. If you are here and if the device->name is NULL that\n\t\t *    means this device was missing at time of FS mount.\n\t\t * 2. If you are here and if the device->name is different\n\t\t *    from 'path' that means either\n\t\t *      a. The same device disappeared and reappeared with\n\t\t *         different name. or\n\t\t *      b. The missing-disk-which-was-replaced, has\n\t\t *         reappeared now.\n\t\t *\n\t\t * We must allow 1 and 2a above. But 2b would be a spurious\n\t\t * and unintentional.\n\t\t *\n\t\t * Further in case of 1 and 2a above, the disk at 'path'\n\t\t * would have missed some transaction when it was away and\n\t\t * in case of 2a the stale bdev has to be updated as well.\n\t\t * 2b must not be allowed at all time.\n\t\t */\n\n\t\t/*\n\t\t * For now, we do allow update to btrfs_fs_device through the\n\t\t * btrfs dev scan cli after FS has been mounted.  We're still\n\t\t * tracking a problem where systems fail mount by subvolume id\n\t\t * when we reject replacement on a mounted FS.\n\t\t */\n\t\tif (!fs_devices->opened && found_transid < device->generation) {\n\t\t\t/*\n\t\t\t * That is if the FS is _not_ mounted and if you\n\t\t\t * are here, that means there is more than one\n\t\t\t * disk with same uuid and devid.We keep the one\n\t\t\t * with larger generation number or the last-in if\n\t\t\t * generation are equal.\n\t\t\t */\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EEXIST);\n\t\t}\n\n\t\t/*\n\t\t * We are going to replace the device path for a given devid,\n\t\t * make sure it's the same device if the device is mounted\n\t\t */\n\t\tif (device->bdev) {\n\t\t\tstruct block_device *path_bdev;\n\n\t\t\tpath_bdev = lookup_bdev(path);\n\t\t\tif (IS_ERR(path_bdev)) {\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\treturn ERR_CAST(path_bdev);\n\t\t\t}\n\n\t\t\tif (device->bdev != path_bdev) {\n\t\t\t\tbdput(path_bdev);\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\tbtrfs_warn_in_rcu(device->fs_info,\n\t\t\t\"duplicate device fsid:devid for %pU:%llu old:%s new:%s\",\n\t\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\t\trcu_str_deref(device->name), path);\n\t\t\t\treturn ERR_PTR(-EEXIST);\n\t\t\t}\n\t\t\tbdput(path_bdev);\n\t\t\tbtrfs_info_in_rcu(device->fs_info,\n\t\t\t\t\"device fsid %pU devid %llu moved old:%s new:%s\",\n\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\trcu_str_deref(device->name), path);\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_string_free(device->name);\n\t\trcu_assign_pointer(device->name, name);\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\tfs_devices->missing_devices--;\n\t\t\tclear_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\t}\n\n\t/*\n\t * Unmount does not free the btrfs_device struct but would zero\n\t * generation along with most of the other members. So just update\n\t * it back. We need it to pick the disk with largest generation\n\t * (as above).\n\t */\n\tif (!fs_devices->opened) {\n\t\tdevice->generation = found_transid;\n\t\tfs_devices->latest_generation = max_t(u64, found_transid,\n\t\t\t\t\t\tfs_devices->latest_generation);\n\t}\n\n\tfs_devices->total_devices = btrfs_super_num_devices(disk_super);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\treturn device;\n}",
                        "code_after_change": "static noinline struct btrfs_device *device_list_add(const char *path,\n\t\t\t   struct btrfs_super_block *disk_super,\n\t\t\t   bool *new_device_added)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *fs_devices = NULL;\n\tstruct rcu_string *name;\n\tu64 found_transid = btrfs_super_generation(disk_super);\n\tu64 devid = btrfs_stack_device_id(&disk_super->dev_item);\n\tbool has_metadata_uuid = (btrfs_super_incompat_flags(disk_super) &\n\t\tBTRFS_FEATURE_INCOMPAT_METADATA_UUID);\n\tbool fsid_change_in_progress = (btrfs_super_flags(disk_super) &\n\t\t\t\t\tBTRFS_SUPER_FLAG_CHANGING_FSID_V2);\n\n\tif (fsid_change_in_progress) {\n\t\tif (!has_metadata_uuid) {\n\t\t\t/*\n\t\t\t * When we have an image which has CHANGING_FSID_V2 set\n\t\t\t * it might belong to either a filesystem which has\n\t\t\t * disks with completed fsid change or it might belong\n\t\t\t * to fs with no UUID changes in effect, handle both.\n\t\t\t */\n\t\t\tfs_devices = find_fsid_inprogress(disk_super);\n\t\t\tif (!fs_devices)\n\t\t\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t\t} else {\n\t\t\tfs_devices = find_fsid_changed(disk_super);\n\t\t}\n\t} else if (has_metadata_uuid) {\n\t\tfs_devices = find_fsid(disk_super->fsid,\n\t\t\t\t       disk_super->metadata_uuid);\n\t} else {\n\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t}\n\n\n\tif (!fs_devices) {\n\t\tif (has_metadata_uuid)\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid,\n\t\t\t\t\t\t      disk_super->metadata_uuid);\n\t\telse\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid, NULL);\n\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn ERR_CAST(fs_devices);\n\n\t\tfs_devices->fsid_change = fsid_change_in_progress;\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_add(&fs_devices->fs_list, &fs_uuids);\n\n\t\tdevice = NULL;\n\t} else {\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tdevice = btrfs_find_device(fs_devices, devid,\n\t\t\t\tdisk_super->dev_item.uuid, NULL, false);\n\n\t\t/*\n\t\t * If this disk has been pulled into an fs devices created by\n\t\t * a device which had the CHANGING_FSID_V2 flag then replace the\n\t\t * metadata_uuid/fsid values of the fs_devices.\n\t\t */\n\t\tif (has_metadata_uuid && fs_devices->fsid_change &&\n\t\t    found_transid > fs_devices->latest_generation) {\n\t\t\tmemcpy(fs_devices->fsid, disk_super->fsid,\n\t\t\t\t\tBTRFS_FSID_SIZE);\n\t\t\tmemcpy(fs_devices->metadata_uuid,\n\t\t\t\t\tdisk_super->metadata_uuid, BTRFS_FSID_SIZE);\n\n\t\t\tfs_devices->fsid_change = false;\n\t\t}\n\t}\n\n\tif (!device) {\n\t\tif (fs_devices->opened) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EBUSY);\n\t\t}\n\n\t\tdevice = btrfs_alloc_device(NULL, &devid,\n\t\t\t\t\t    disk_super->dev_item.uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t/* we can safely leave the fs_devices entry around */\n\t\t\treturn device;\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tbtrfs_free_device(device);\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_assign_pointer(device->name, name);\n\n\t\tlist_add_rcu(&device->dev_list, &fs_devices->devices);\n\t\tfs_devices->num_devices++;\n\n\t\tdevice->fs_devices = fs_devices;\n\t\t*new_device_added = true;\n\n\t\tif (disk_super->label[0])\n\t\t\tpr_info(\"BTRFS: device label %s devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->label, devid, found_transid, path);\n\t\telse\n\t\t\tpr_info(\"BTRFS: device fsid %pU devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->fsid, devid, found_transid, path);\n\n\t} else if (!device->name || strcmp(device->name->str, path)) {\n\t\t/*\n\t\t * When FS is already mounted.\n\t\t * 1. If you are here and if the device->name is NULL that\n\t\t *    means this device was missing at time of FS mount.\n\t\t * 2. If you are here and if the device->name is different\n\t\t *    from 'path' that means either\n\t\t *      a. The same device disappeared and reappeared with\n\t\t *         different name. or\n\t\t *      b. The missing-disk-which-was-replaced, has\n\t\t *         reappeared now.\n\t\t *\n\t\t * We must allow 1 and 2a above. But 2b would be a spurious\n\t\t * and unintentional.\n\t\t *\n\t\t * Further in case of 1 and 2a above, the disk at 'path'\n\t\t * would have missed some transaction when it was away and\n\t\t * in case of 2a the stale bdev has to be updated as well.\n\t\t * 2b must not be allowed at all time.\n\t\t */\n\n\t\t/*\n\t\t * For now, we do allow update to btrfs_fs_device through the\n\t\t * btrfs dev scan cli after FS has been mounted.  We're still\n\t\t * tracking a problem where systems fail mount by subvolume id\n\t\t * when we reject replacement on a mounted FS.\n\t\t */\n\t\tif (!fs_devices->opened && found_transid < device->generation) {\n\t\t\t/*\n\t\t\t * That is if the FS is _not_ mounted and if you\n\t\t\t * are here, that means there is more than one\n\t\t\t * disk with same uuid and devid.We keep the one\n\t\t\t * with larger generation number or the last-in if\n\t\t\t * generation are equal.\n\t\t\t */\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EEXIST);\n\t\t}\n\n\t\t/*\n\t\t * We are going to replace the device path for a given devid,\n\t\t * make sure it's the same device if the device is mounted\n\t\t */\n\t\tif (device->bdev) {\n\t\t\tstruct block_device *path_bdev;\n\n\t\t\tpath_bdev = lookup_bdev(path);\n\t\t\tif (IS_ERR(path_bdev)) {\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\treturn ERR_CAST(path_bdev);\n\t\t\t}\n\n\t\t\tif (device->bdev != path_bdev) {\n\t\t\t\tbdput(path_bdev);\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\tbtrfs_warn_in_rcu(device->fs_info,\n\t\t\t\"duplicate device fsid:devid for %pU:%llu old:%s new:%s\",\n\t\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\t\trcu_str_deref(device->name), path);\n\t\t\t\treturn ERR_PTR(-EEXIST);\n\t\t\t}\n\t\t\tbdput(path_bdev);\n\t\t\tbtrfs_info_in_rcu(device->fs_info,\n\t\t\t\t\"device fsid %pU devid %llu moved old:%s new:%s\",\n\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\trcu_str_deref(device->name), path);\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_string_free(device->name);\n\t\trcu_assign_pointer(device->name, name);\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\tfs_devices->missing_devices--;\n\t\t\tclear_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\t}\n\n\t/*\n\t * Unmount does not free the btrfs_device struct but would zero\n\t * generation along with most of the other members. So just update\n\t * it back. We need it to pick the disk with largest generation\n\t * (as above).\n\t */\n\tif (!fs_devices->opened) {\n\t\tdevice->generation = found_transid;\n\t\tfs_devices->latest_generation = max_t(u64, found_transid,\n\t\t\t\t\t\tfs_devices->latest_generation);\n\t}\n\n\tfs_devices->total_devices = btrfs_super_num_devices(disk_super);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\treturn device;\n}",
                        "cve_id": "CVE-2019-18885"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int btrfs_init_dev_replace(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_key key;\n\tstruct btrfs_root *dev_root = fs_info->dev_root;\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\tstruct extent_buffer *eb;\n\tint slot;\n\tint ret = 0;\n\tstruct btrfs_path *path = NULL;\n\tint item_size;\n\tstruct btrfs_dev_replace_item *ptr;\n\tu64 src_devid;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tkey.objectid = 0;\n\tkey.type = BTRFS_DEV_REPLACE_KEY;\n\tkey.offset = 0;\n\tret = btrfs_search_slot(NULL, dev_root, &key, path, 0, 0);\n\tif (ret) {\nno_valid_dev_replace_entry_found:\n\t\tret = 0;\n\t\tdev_replace->replace_state =\n\t\t\tBTRFS_DEV_REPLACE_ITEM_STATE_NEVER_STARTED;\n\t\tdev_replace->cont_reading_from_srcdev_mode =\n\t\t    BTRFS_DEV_REPLACE_ITEM_CONT_READING_FROM_SRCDEV_MODE_ALWAYS;\n\t\tdev_replace->time_started = 0;\n\t\tdev_replace->time_stopped = 0;\n\t\tatomic64_set(&dev_replace->num_write_errors, 0);\n\t\tatomic64_set(&dev_replace->num_uncorrectable_read_errors, 0);\n\t\tdev_replace->cursor_left = 0;\n\t\tdev_replace->committed_cursor_left = 0;\n\t\tdev_replace->cursor_left_last_write_of_item = 0;\n\t\tdev_replace->cursor_right = 0;\n\t\tdev_replace->srcdev = NULL;\n\t\tdev_replace->tgtdev = NULL;\n\t\tdev_replace->is_valid = 0;\n\t\tdev_replace->item_needs_writeback = 0;\n\t\tgoto out;\n\t}\n\tslot = path->slots[0];\n\teb = path->nodes[0];\n\titem_size = btrfs_item_size_nr(eb, slot);\n\tptr = btrfs_item_ptr(eb, slot, struct btrfs_dev_replace_item);\n\n\tif (item_size != sizeof(struct btrfs_dev_replace_item)) {\n\t\tbtrfs_warn(fs_info,\n\t\t\t\"dev_replace entry found has unexpected size, ignore entry\");\n\t\tgoto no_valid_dev_replace_entry_found;\n\t}\n\n\tsrc_devid = btrfs_dev_replace_src_devid(eb, ptr);\n\tdev_replace->cont_reading_from_srcdev_mode =\n\t\tbtrfs_dev_replace_cont_reading_from_srcdev_mode(eb, ptr);\n\tdev_replace->replace_state = btrfs_dev_replace_replace_state(eb, ptr);\n\tdev_replace->time_started = btrfs_dev_replace_time_started(eb, ptr);\n\tdev_replace->time_stopped =\n\t\tbtrfs_dev_replace_time_stopped(eb, ptr);\n\tatomic64_set(&dev_replace->num_write_errors,\n\t\t     btrfs_dev_replace_num_write_errors(eb, ptr));\n\tatomic64_set(&dev_replace->num_uncorrectable_read_errors,\n\t\t     btrfs_dev_replace_num_uncorrectable_read_errors(eb, ptr));\n\tdev_replace->cursor_left = btrfs_dev_replace_cursor_left(eb, ptr);\n\tdev_replace->committed_cursor_left = dev_replace->cursor_left;\n\tdev_replace->cursor_left_last_write_of_item = dev_replace->cursor_left;\n\tdev_replace->cursor_right = btrfs_dev_replace_cursor_right(eb, ptr);\n\tdev_replace->is_valid = 1;\n\n\tdev_replace->item_needs_writeback = 0;\n\tswitch (dev_replace->replace_state) {\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_NEVER_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_FINISHED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_CANCELED:\n\t\tdev_replace->srcdev = NULL;\n\t\tdev_replace->tgtdev = NULL;\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED:\n\t\tdev_replace->srcdev = btrfs_find_device(fs_info->fs_devices,\n\t\t\t\t\t\t\tsrc_devid, NULL, NULL);\n\t\tdev_replace->tgtdev = btrfs_find_device(fs_info->fs_devices,\n\t\t\t\t\t\t\tBTRFS_DEV_REPLACE_DEVID,\n\t\t\t\t\t\t\tNULL, NULL);\n\t\t/*\n\t\t * allow 'btrfs dev replace_cancel' if src/tgt device is\n\t\t * missing\n\t\t */\n\t\tif (!dev_replace->srcdev &&\n\t\t    !btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tret = -EIO;\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"cannot mount because device replace operation is ongoing and\");\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"srcdev (devid %llu) is missing, need to run 'btrfs dev scan'?\",\n\t\t\t   src_devid);\n\t\t}\n\t\tif (!dev_replace->tgtdev &&\n\t\t    !btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tret = -EIO;\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"cannot mount because device replace operation is ongoing and\");\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"tgtdev (devid %llu) is missing, need to run 'btrfs dev scan'?\",\n\t\t\t\tBTRFS_DEV_REPLACE_DEVID);\n\t\t}\n\t\tif (dev_replace->tgtdev) {\n\t\t\tif (dev_replace->srcdev) {\n\t\t\t\tdev_replace->tgtdev->total_bytes =\n\t\t\t\t\tdev_replace->srcdev->total_bytes;\n\t\t\t\tdev_replace->tgtdev->disk_total_bytes =\n\t\t\t\t\tdev_replace->srcdev->disk_total_bytes;\n\t\t\t\tdev_replace->tgtdev->commit_total_bytes =\n\t\t\t\t\tdev_replace->srcdev->commit_total_bytes;\n\t\t\t\tdev_replace->tgtdev->bytes_used =\n\t\t\t\t\tdev_replace->srcdev->bytes_used;\n\t\t\t\tdev_replace->tgtdev->commit_bytes_used =\n\t\t\t\t\tdev_replace->srcdev->commit_bytes_used;\n\t\t\t}\n\t\t\tset_bit(BTRFS_DEV_STATE_REPLACE_TGT,\n\t\t\t\t&dev_replace->tgtdev->dev_state);\n\n\t\t\tWARN_ON(fs_info->fs_devices->rw_devices == 0);\n\t\t\tdev_replace->tgtdev->io_width = fs_info->sectorsize;\n\t\t\tdev_replace->tgtdev->io_align = fs_info->sectorsize;\n\t\t\tdev_replace->tgtdev->sector_size = fs_info->sectorsize;\n\t\t\tdev_replace->tgtdev->fs_info = fs_info;\n\t\t\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t&dev_replace->tgtdev->dev_state);\n\t\t}\n\t\tbreak;\n\t}\n\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}",
                        "code_after_change": "int btrfs_init_dev_replace(struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_key key;\n\tstruct btrfs_root *dev_root = fs_info->dev_root;\n\tstruct btrfs_dev_replace *dev_replace = &fs_info->dev_replace;\n\tstruct extent_buffer *eb;\n\tint slot;\n\tint ret = 0;\n\tstruct btrfs_path *path = NULL;\n\tint item_size;\n\tstruct btrfs_dev_replace_item *ptr;\n\tu64 src_devid;\n\n\tpath = btrfs_alloc_path();\n\tif (!path) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tkey.objectid = 0;\n\tkey.type = BTRFS_DEV_REPLACE_KEY;\n\tkey.offset = 0;\n\tret = btrfs_search_slot(NULL, dev_root, &key, path, 0, 0);\n\tif (ret) {\nno_valid_dev_replace_entry_found:\n\t\tret = 0;\n\t\tdev_replace->replace_state =\n\t\t\tBTRFS_DEV_REPLACE_ITEM_STATE_NEVER_STARTED;\n\t\tdev_replace->cont_reading_from_srcdev_mode =\n\t\t    BTRFS_DEV_REPLACE_ITEM_CONT_READING_FROM_SRCDEV_MODE_ALWAYS;\n\t\tdev_replace->time_started = 0;\n\t\tdev_replace->time_stopped = 0;\n\t\tatomic64_set(&dev_replace->num_write_errors, 0);\n\t\tatomic64_set(&dev_replace->num_uncorrectable_read_errors, 0);\n\t\tdev_replace->cursor_left = 0;\n\t\tdev_replace->committed_cursor_left = 0;\n\t\tdev_replace->cursor_left_last_write_of_item = 0;\n\t\tdev_replace->cursor_right = 0;\n\t\tdev_replace->srcdev = NULL;\n\t\tdev_replace->tgtdev = NULL;\n\t\tdev_replace->is_valid = 0;\n\t\tdev_replace->item_needs_writeback = 0;\n\t\tgoto out;\n\t}\n\tslot = path->slots[0];\n\teb = path->nodes[0];\n\titem_size = btrfs_item_size_nr(eb, slot);\n\tptr = btrfs_item_ptr(eb, slot, struct btrfs_dev_replace_item);\n\n\tif (item_size != sizeof(struct btrfs_dev_replace_item)) {\n\t\tbtrfs_warn(fs_info,\n\t\t\t\"dev_replace entry found has unexpected size, ignore entry\");\n\t\tgoto no_valid_dev_replace_entry_found;\n\t}\n\n\tsrc_devid = btrfs_dev_replace_src_devid(eb, ptr);\n\tdev_replace->cont_reading_from_srcdev_mode =\n\t\tbtrfs_dev_replace_cont_reading_from_srcdev_mode(eb, ptr);\n\tdev_replace->replace_state = btrfs_dev_replace_replace_state(eb, ptr);\n\tdev_replace->time_started = btrfs_dev_replace_time_started(eb, ptr);\n\tdev_replace->time_stopped =\n\t\tbtrfs_dev_replace_time_stopped(eb, ptr);\n\tatomic64_set(&dev_replace->num_write_errors,\n\t\t     btrfs_dev_replace_num_write_errors(eb, ptr));\n\tatomic64_set(&dev_replace->num_uncorrectable_read_errors,\n\t\t     btrfs_dev_replace_num_uncorrectable_read_errors(eb, ptr));\n\tdev_replace->cursor_left = btrfs_dev_replace_cursor_left(eb, ptr);\n\tdev_replace->committed_cursor_left = dev_replace->cursor_left;\n\tdev_replace->cursor_left_last_write_of_item = dev_replace->cursor_left;\n\tdev_replace->cursor_right = btrfs_dev_replace_cursor_right(eb, ptr);\n\tdev_replace->is_valid = 1;\n\n\tdev_replace->item_needs_writeback = 0;\n\tswitch (dev_replace->replace_state) {\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_NEVER_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_FINISHED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_CANCELED:\n\t\tdev_replace->srcdev = NULL;\n\t\tdev_replace->tgtdev = NULL;\n\t\tbreak;\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED:\n\tcase BTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED:\n\t\tdev_replace->srcdev = btrfs_find_device(fs_info->fs_devices,\n\t\t\t\t\t\tsrc_devid, NULL, NULL, true);\n\t\tdev_replace->tgtdev = btrfs_find_device(fs_info->fs_devices,\n\t\t\t\t\t\t\tBTRFS_DEV_REPLACE_DEVID,\n\t\t\t\t\t\t\tNULL, NULL, true);\n\t\t/*\n\t\t * allow 'btrfs dev replace_cancel' if src/tgt device is\n\t\t * missing\n\t\t */\n\t\tif (!dev_replace->srcdev &&\n\t\t    !btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tret = -EIO;\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"cannot mount because device replace operation is ongoing and\");\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"srcdev (devid %llu) is missing, need to run 'btrfs dev scan'?\",\n\t\t\t   src_devid);\n\t\t}\n\t\tif (!dev_replace->tgtdev &&\n\t\t    !btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tret = -EIO;\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"cannot mount because device replace operation is ongoing and\");\n\t\t\tbtrfs_warn(fs_info,\n\t\t\t   \"tgtdev (devid %llu) is missing, need to run 'btrfs dev scan'?\",\n\t\t\t\tBTRFS_DEV_REPLACE_DEVID);\n\t\t}\n\t\tif (dev_replace->tgtdev) {\n\t\t\tif (dev_replace->srcdev) {\n\t\t\t\tdev_replace->tgtdev->total_bytes =\n\t\t\t\t\tdev_replace->srcdev->total_bytes;\n\t\t\t\tdev_replace->tgtdev->disk_total_bytes =\n\t\t\t\t\tdev_replace->srcdev->disk_total_bytes;\n\t\t\t\tdev_replace->tgtdev->commit_total_bytes =\n\t\t\t\t\tdev_replace->srcdev->commit_total_bytes;\n\t\t\t\tdev_replace->tgtdev->bytes_used =\n\t\t\t\t\tdev_replace->srcdev->bytes_used;\n\t\t\t\tdev_replace->tgtdev->commit_bytes_used =\n\t\t\t\t\tdev_replace->srcdev->commit_bytes_used;\n\t\t\t}\n\t\t\tset_bit(BTRFS_DEV_STATE_REPLACE_TGT,\n\t\t\t\t&dev_replace->tgtdev->dev_state);\n\n\t\t\tWARN_ON(fs_info->fs_devices->rw_devices == 0);\n\t\t\tdev_replace->tgtdev->io_width = fs_info->sectorsize;\n\t\t\tdev_replace->tgtdev->io_align = fs_info->sectorsize;\n\t\t\tdev_replace->tgtdev->sector_size = fs_info->sectorsize;\n\t\t\tdev_replace->tgtdev->fs_info = fs_info;\n\t\t\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t&dev_replace->tgtdev->dev_state);\n\t\t}\n\t\tbreak;\n\t}\n\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}",
                        "cve_id": "CVE-2019-18885"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int verify_one_dev_extent(struct btrfs_fs_info *fs_info,\n\t\t\t\t u64 chunk_offset, u64 devid,\n\t\t\t\t u64 physical_offset, u64 physical_len)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree.map_tree;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_device *dev;\n\tu64 stripe_len;\n\tbool found = false;\n\tint ret = 0;\n\tint i;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, chunk_offset, 1);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu doesn't have corresponding chunk\",\n\t\t\t  physical_offset, devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tmap = em->map_lookup;\n\tstripe_len = calc_stripe_length(map->type, em->len, map->num_stripes);\n\tif (physical_len != stripe_len) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu length doesn't match chunk %llu, have %llu expect %llu\",\n\t\t\t  physical_offset, devid, em->start, physical_len,\n\t\t\t  stripe_len);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (map->stripes[i].dev->devid == devid &&\n\t\t    map->stripes[i].physical == physical_offset) {\n\t\t\tfound = true;\n\t\t\tif (map->verified_stripes >= map->num_stripes) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"too many dev extents for chunk %llu found\",\n\t\t\t\t\t  em->start);\n\t\t\t\tret = -EUCLEAN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tmap->verified_stripes++;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found) {\n\t\tbtrfs_err(fs_info,\n\t\"dev extent physical offset %llu devid %llu has no corresponding chunk\",\n\t\t\tphysical_offset, devid);\n\t\tret = -EUCLEAN;\n\t}\n\n\t/* Make sure no dev extent is beyond device bondary */\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL);\n\tif (!dev) {\n\t\tbtrfs_err(fs_info, \"failed to find devid %llu\", devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\t/* It's possible this device is a dummy for seed device */\n\tif (dev->disk_total_bytes == 0) {\n\t\tdev = find_device(fs_info->fs_devices->seed, devid, NULL);\n\t\tif (!dev) {\n\t\t\tbtrfs_err(fs_info, \"failed to find seed devid %llu\",\n\t\t\t\t  devid);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (physical_offset + physical_len > dev->disk_total_bytes) {\n\t\tbtrfs_err(fs_info,\n\"dev extent devid %llu physical offset %llu len %llu is beyond device boundary %llu\",\n\t\t\t  devid, physical_offset, physical_len,\n\t\t\t  dev->disk_total_bytes);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\nout:\n\tfree_extent_map(em);\n\treturn ret;\n}",
                        "code_after_change": "static int verify_one_dev_extent(struct btrfs_fs_info *fs_info,\n\t\t\t\t u64 chunk_offset, u64 devid,\n\t\t\t\t u64 physical_offset, u64 physical_len)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree.map_tree;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_device *dev;\n\tu64 stripe_len;\n\tbool found = false;\n\tint ret = 0;\n\tint i;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, chunk_offset, 1);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu doesn't have corresponding chunk\",\n\t\t\t  physical_offset, devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tmap = em->map_lookup;\n\tstripe_len = calc_stripe_length(map->type, em->len, map->num_stripes);\n\tif (physical_len != stripe_len) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu length doesn't match chunk %llu, have %llu expect %llu\",\n\t\t\t  physical_offset, devid, em->start, physical_len,\n\t\t\t  stripe_len);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (map->stripes[i].dev->devid == devid &&\n\t\t    map->stripes[i].physical == physical_offset) {\n\t\t\tfound = true;\n\t\t\tif (map->verified_stripes >= map->num_stripes) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"too many dev extents for chunk %llu found\",\n\t\t\t\t\t  em->start);\n\t\t\t\tret = -EUCLEAN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tmap->verified_stripes++;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found) {\n\t\tbtrfs_err(fs_info,\n\t\"dev extent physical offset %llu devid %llu has no corresponding chunk\",\n\t\t\tphysical_offset, devid);\n\t\tret = -EUCLEAN;\n\t}\n\n\t/* Make sure no dev extent is beyond device bondary */\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL, true);\n\tif (!dev) {\n\t\tbtrfs_err(fs_info, \"failed to find devid %llu\", devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\t/* It's possible this device is a dummy for seed device */\n\tif (dev->disk_total_bytes == 0) {\n\t\tdev = btrfs_find_device(fs_info->fs_devices->seed, devid, NULL,\n\t\t\t\t\tNULL, false);\n\t\tif (!dev) {\n\t\t\tbtrfs_err(fs_info, \"failed to find seed devid %llu\",\n\t\t\t\t  devid);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (physical_offset + physical_len > dev->disk_total_bytes) {\n\t\tbtrfs_err(fs_info,\n\"dev extent devid %llu physical offset %llu len %llu is beyond device boundary %llu\",\n\t\t\t  devid, physical_offset, physical_len,\n\t\t\t  dev->disk_total_bytes);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\nout:\n\tfree_extent_map(em);\n\treturn ret;\n}",
                        "cve_id": "CVE-2019-18885"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2219,
            "cve_id": "CVE-2019-18885",
            "code_snippet": "int btrfs_scrub_progress(struct btrfs_fs_info *fs_info, u64 devid,\n\t\t\t struct btrfs_scrub_progress *progress)\n{\n\tstruct btrfs_device *dev;\n\tstruct scrub_ctx *sctx = NULL;\n\n\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL);\n\tif (dev)\n\t\tsctx = dev->scrub_ctx;\n\tif (sctx)\n\t\tmemcpy(progress, &sctx->stat, sizeof(*progress));\n\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\n\treturn dev ? (sctx ? 0 : -ENOTCONN) : -ENODEV;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int btrfs_rm_device(struct btrfs_fs_info *fs_info, const char *device_path,\n\t\t    u64 devid)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *cur_devices;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 num_devices;\n\tint ret = 0;\n\n\tmutex_lock(&uuid_mutex);\n\n\tnum_devices = btrfs_num_devices(fs_info);\n\n\tret = btrfs_check_raid_min_devices(fs_info, num_devices - 1);\n\tif (ret)\n\t\tgoto out;\n\n\tdevice = btrfs_find_device_by_devspec(fs_info, devid, device_path);\n\n\tif (IS_ERR(device)) {\n\t\tif (PTR_ERR(device) == -ENOENT &&\n\t\t    strcmp(device_path, \"missing\") == 0)\n\t\t\tret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND;\n\t\telse\n\t\t\tret = PTR_ERR(device);\n\t\tgoto out;\n\t}\n\n\tif (btrfs_pinned_by_swapfile(fs_info, device)) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t  \"cannot remove device %s (devid %llu) due to active swapfile\",\n\t\t\t\t  rcu_str_deref(device->name), device->devid);\n\t\tret = -ETXTBSY;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = BTRFS_ERROR_DEV_TGT_REPLACE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    fs_info->fs_devices->rw_devices == 1) {\n\t\tret = BTRFS_ERROR_DEV_ONLY_WRITABLE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tdevice->fs_devices->rw_devices--;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\n\tmutex_unlock(&uuid_mutex);\n\tret = btrfs_shrink_device(device, 0);\n\tif (!ret)\n\t\tbtrfs_reada_remove_dev(device);\n\tmutex_lock(&uuid_mutex);\n\tif (ret)\n\t\tgoto error_undo;\n\n\t/*\n\t * TODO: the superblock still includes this device in its num_devices\n\t * counter although write_all_supers() is not locked out. This\n\t * could give a filesystem state which requires a degraded mount.\n\t */\n\tret = btrfs_rm_dev_item(device);\n\tif (ret)\n\t\tgoto error_undo;\n\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tbtrfs_scrub_cancel_dev(device);\n\n\t/*\n\t * the device list mutex makes sure that we don't change\n\t * the device list while someone else is writing out all\n\t * the device supers. Whoever is writing all supers, should\n\t * lock the device list mutex before getting the number of\n\t * devices in the super block (super_copy). Conversely,\n\t * whoever updates the number of devices in the super block\n\t * (super_copy) should hold the device list mutex.\n\t */\n\n\t/*\n\t * In normal cases the cur_devices == fs_devices. But in case\n\t * of deleting a seed device, the cur_devices should point to\n\t * its own fs_devices listed under the fs_devices->seed.\n\t */\n\tcur_devices = device->fs_devices;\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_del_rcu(&device->dev_list);\n\n\tcur_devices->num_devices--;\n\tcur_devices->total_devices--;\n\t/* Update total_devices of the parent fs_devices if it's seed */\n\tif (cur_devices != fs_devices)\n\t\tfs_devices->total_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tcur_devices->missing_devices--;\n\n\tbtrfs_assign_next_active_device(device, NULL);\n\n\tif (device->bdev) {\n\t\tcur_devices->open_devices--;\n\t\t/* remove sysfs entry */\n\t\tbtrfs_sysfs_remove_device(device);\n\t}\n\n\tnum_devices = btrfs_super_num_devices(fs_info->super_copy) - 1;\n\tbtrfs_set_super_num_devices(fs_info->super_copy, num_devices);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * at this point, the device is zero sized and detached from\n\t * the devices list.  All that's left is to zero out the old\n\t * supers and free the device.\n\t */\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\tbtrfs_scratch_superblocks(fs_info, device->bdev,\n\t\t\t\t\t  device->name->str);\n\n\tbtrfs_close_bdev(device);\n\tsynchronize_rcu();\n\tbtrfs_free_device(device);\n\n\tif (cur_devices->open_devices == 0) {\n\t\tlist_del_init(&cur_devices->seed_list);\n\t\tclose_fs_devices(cur_devices);\n\t\tfree_fs_devices(cur_devices);\n\t}\n\nout:\n\tmutex_unlock(&uuid_mutex);\n\treturn ret;\n\nerror_undo:\n\tbtrfs_reada_undo_remove_dev(device);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_add(&device->dev_alloc_list,\n\t\t\t &fs_devices->alloc_list);\n\t\tdevice->fs_devices->rw_devices++;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\tgoto out;\n}",
                        "code_after_change": "static noinline struct btrfs_device *device_list_add(const char *path,\n\t\t\t   struct btrfs_super_block *disk_super,\n\t\t\t   bool *new_device_added)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *fs_devices = NULL;\n\tstruct rcu_string *name;\n\tu64 found_transid = btrfs_super_generation(disk_super);\n\tu64 devid = btrfs_stack_device_id(&disk_super->dev_item);\n\tbool has_metadata_uuid = (btrfs_super_incompat_flags(disk_super) &\n\t\tBTRFS_FEATURE_INCOMPAT_METADATA_UUID);\n\tbool fsid_change_in_progress = (btrfs_super_flags(disk_super) &\n\t\t\t\t\tBTRFS_SUPER_FLAG_CHANGING_FSID_V2);\n\n\tif (fsid_change_in_progress) {\n\t\tif (!has_metadata_uuid) {\n\t\t\t/*\n\t\t\t * When we have an image which has CHANGING_FSID_V2 set\n\t\t\t * it might belong to either a filesystem which has\n\t\t\t * disks with completed fsid change or it might belong\n\t\t\t * to fs with no UUID changes in effect, handle both.\n\t\t\t */\n\t\t\tfs_devices = find_fsid_inprogress(disk_super);\n\t\t\tif (!fs_devices)\n\t\t\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t\t} else {\n\t\t\tfs_devices = find_fsid_changed(disk_super);\n\t\t}\n\t} else if (has_metadata_uuid) {\n\t\tfs_devices = find_fsid(disk_super->fsid,\n\t\t\t\t       disk_super->metadata_uuid);\n\t} else {\n\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t}\n\n\n\tif (!fs_devices) {\n\t\tif (has_metadata_uuid)\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid,\n\t\t\t\t\t\t      disk_super->metadata_uuid);\n\t\telse\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid, NULL);\n\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn ERR_CAST(fs_devices);\n\n\t\tfs_devices->fsid_change = fsid_change_in_progress;\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_add(&fs_devices->fs_list, &fs_uuids);\n\n\t\tdevice = NULL;\n\t} else {\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tdevice = btrfs_find_device(fs_devices, devid,\n\t\t\t\tdisk_super->dev_item.uuid, NULL, false);\n\n\t\t/*\n\t\t * If this disk has been pulled into an fs devices created by\n\t\t * a device which had the CHANGING_FSID_V2 flag then replace the\n\t\t * metadata_uuid/fsid values of the fs_devices.\n\t\t */\n\t\tif (has_metadata_uuid && fs_devices->fsid_change &&\n\t\t    found_transid > fs_devices->latest_generation) {\n\t\t\tmemcpy(fs_devices->fsid, disk_super->fsid,\n\t\t\t\t\tBTRFS_FSID_SIZE);\n\t\t\tmemcpy(fs_devices->metadata_uuid,\n\t\t\t\t\tdisk_super->metadata_uuid, BTRFS_FSID_SIZE);\n\n\t\t\tfs_devices->fsid_change = false;\n\t\t}\n\t}\n\n\tif (!device) {\n\t\tif (fs_devices->opened) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EBUSY);\n\t\t}\n\n\t\tdevice = btrfs_alloc_device(NULL, &devid,\n\t\t\t\t\t    disk_super->dev_item.uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t/* we can safely leave the fs_devices entry around */\n\t\t\treturn device;\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tbtrfs_free_device(device);\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_assign_pointer(device->name, name);\n\n\t\tlist_add_rcu(&device->dev_list, &fs_devices->devices);\n\t\tfs_devices->num_devices++;\n\n\t\tdevice->fs_devices = fs_devices;\n\t\t*new_device_added = true;\n\n\t\tif (disk_super->label[0])\n\t\t\tpr_info(\"BTRFS: device label %s devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->label, devid, found_transid, path);\n\t\telse\n\t\t\tpr_info(\"BTRFS: device fsid %pU devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->fsid, devid, found_transid, path);\n\n\t} else if (!device->name || strcmp(device->name->str, path)) {\n\t\t/*\n\t\t * When FS is already mounted.\n\t\t * 1. If you are here and if the device->name is NULL that\n\t\t *    means this device was missing at time of FS mount.\n\t\t * 2. If you are here and if the device->name is different\n\t\t *    from 'path' that means either\n\t\t *      a. The same device disappeared and reappeared with\n\t\t *         different name. or\n\t\t *      b. The missing-disk-which-was-replaced, has\n\t\t *         reappeared now.\n\t\t *\n\t\t * We must allow 1 and 2a above. But 2b would be a spurious\n\t\t * and unintentional.\n\t\t *\n\t\t * Further in case of 1 and 2a above, the disk at 'path'\n\t\t * would have missed some transaction when it was away and\n\t\t * in case of 2a the stale bdev has to be updated as well.\n\t\t * 2b must not be allowed at all time.\n\t\t */\n\n\t\t/*\n\t\t * For now, we do allow update to btrfs_fs_device through the\n\t\t * btrfs dev scan cli after FS has been mounted.  We're still\n\t\t * tracking a problem where systems fail mount by subvolume id\n\t\t * when we reject replacement on a mounted FS.\n\t\t */\n\t\tif (!fs_devices->opened && found_transid < device->generation) {\n\t\t\t/*\n\t\t\t * That is if the FS is _not_ mounted and if you\n\t\t\t * are here, that means there is more than one\n\t\t\t * disk with same uuid and devid.We keep the one\n\t\t\t * with larger generation number or the last-in if\n\t\t\t * generation are equal.\n\t\t\t */\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EEXIST);\n\t\t}\n\n\t\t/*\n\t\t * We are going to replace the device path for a given devid,\n\t\t * make sure it's the same device if the device is mounted\n\t\t */\n\t\tif (device->bdev) {\n\t\t\tstruct block_device *path_bdev;\n\n\t\t\tpath_bdev = lookup_bdev(path);\n\t\t\tif (IS_ERR(path_bdev)) {\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\treturn ERR_CAST(path_bdev);\n\t\t\t}\n\n\t\t\tif (device->bdev != path_bdev) {\n\t\t\t\tbdput(path_bdev);\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\tbtrfs_warn_in_rcu(device->fs_info,\n\t\t\t\"duplicate device fsid:devid for %pU:%llu old:%s new:%s\",\n\t\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\t\trcu_str_deref(device->name), path);\n\t\t\t\treturn ERR_PTR(-EEXIST);\n\t\t\t}\n\t\t\tbdput(path_bdev);\n\t\t\tbtrfs_info_in_rcu(device->fs_info,\n\t\t\t\t\"device fsid %pU devid %llu moved old:%s new:%s\",\n\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\trcu_str_deref(device->name), path);\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_string_free(device->name);\n\t\trcu_assign_pointer(device->name, name);\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\tfs_devices->missing_devices--;\n\t\t\tclear_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\t}\n\n\t/*\n\t * Unmount does not free the btrfs_device struct but would zero\n\t * generation along with most of the other members. So just update\n\t * it back. We need it to pick the disk with largest generation\n\t * (as above).\n\t */\n\tif (!fs_devices->opened) {\n\t\tdevice->generation = found_transid;\n\t\tfs_devices->latest_generation = max_t(u64, found_transid,\n\t\t\t\t\t\tfs_devices->latest_generation);\n\t}\n\n\tfs_devices->total_devices = btrfs_super_num_devices(disk_super);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\treturn device;\n}",
                        "cve_id": "CVE-2021-3739"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int verify_one_dev_extent(struct btrfs_fs_info *fs_info,\n\t\t\t\t u64 chunk_offset, u64 devid,\n\t\t\t\t u64 physical_offset, u64 physical_len)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree.map_tree;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_device *dev;\n\tu64 stripe_len;\n\tbool found = false;\n\tint ret = 0;\n\tint i;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, chunk_offset, 1);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu doesn't have corresponding chunk\",\n\t\t\t  physical_offset, devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tmap = em->map_lookup;\n\tstripe_len = calc_stripe_length(map->type, em->len, map->num_stripes);\n\tif (physical_len != stripe_len) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu length doesn't match chunk %llu, have %llu expect %llu\",\n\t\t\t  physical_offset, devid, em->start, physical_len,\n\t\t\t  stripe_len);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (map->stripes[i].dev->devid == devid &&\n\t\t    map->stripes[i].physical == physical_offset) {\n\t\t\tfound = true;\n\t\t\tif (map->verified_stripes >= map->num_stripes) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"too many dev extents for chunk %llu found\",\n\t\t\t\t\t  em->start);\n\t\t\t\tret = -EUCLEAN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tmap->verified_stripes++;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found) {\n\t\tbtrfs_err(fs_info,\n\t\"dev extent physical offset %llu devid %llu has no corresponding chunk\",\n\t\t\tphysical_offset, devid);\n\t\tret = -EUCLEAN;\n\t}\n\n\t/* Make sure no dev extent is beyond device bondary */\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL);\n\tif (!dev) {\n\t\tbtrfs_err(fs_info, \"failed to find devid %llu\", devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\t/* It's possible this device is a dummy for seed device */\n\tif (dev->disk_total_bytes == 0) {\n\t\tdev = find_device(fs_info->fs_devices->seed, devid, NULL);\n\t\tif (!dev) {\n\t\t\tbtrfs_err(fs_info, \"failed to find seed devid %llu\",\n\t\t\t\t  devid);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (physical_offset + physical_len > dev->disk_total_bytes) {\n\t\tbtrfs_err(fs_info,\n\"dev extent devid %llu physical offset %llu len %llu is beyond device boundary %llu\",\n\t\t\t  devid, physical_offset, physical_len,\n\t\t\t  dev->disk_total_bytes);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\nout:\n\tfree_extent_map(em);\n\treturn ret;\n}",
                        "code_after_change": "int btrfs_rm_device(struct btrfs_fs_info *fs_info, const char *device_path,\n\t\t    u64 devid)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *cur_devices;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 num_devices;\n\tint ret = 0;\n\n\tmutex_lock(&uuid_mutex);\n\n\tnum_devices = btrfs_num_devices(fs_info);\n\n\tret = btrfs_check_raid_min_devices(fs_info, num_devices - 1);\n\tif (ret)\n\t\tgoto out;\n\n\tdevice = btrfs_find_device_by_devspec(fs_info, devid, device_path);\n\n\tif (IS_ERR(device)) {\n\t\tif (PTR_ERR(device) == -ENOENT &&\n\t\t    device_path && strcmp(device_path, \"missing\") == 0)\n\t\t\tret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND;\n\t\telse\n\t\t\tret = PTR_ERR(device);\n\t\tgoto out;\n\t}\n\n\tif (btrfs_pinned_by_swapfile(fs_info, device)) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t  \"cannot remove device %s (devid %llu) due to active swapfile\",\n\t\t\t\t  rcu_str_deref(device->name), device->devid);\n\t\tret = -ETXTBSY;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = BTRFS_ERROR_DEV_TGT_REPLACE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    fs_info->fs_devices->rw_devices == 1) {\n\t\tret = BTRFS_ERROR_DEV_ONLY_WRITABLE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tdevice->fs_devices->rw_devices--;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\n\tmutex_unlock(&uuid_mutex);\n\tret = btrfs_shrink_device(device, 0);\n\tif (!ret)\n\t\tbtrfs_reada_remove_dev(device);\n\tmutex_lock(&uuid_mutex);\n\tif (ret)\n\t\tgoto error_undo;\n\n\t/*\n\t * TODO: the superblock still includes this device in its num_devices\n\t * counter although write_all_supers() is not locked out. This\n\t * could give a filesystem state which requires a degraded mount.\n\t */\n\tret = btrfs_rm_dev_item(device);\n\tif (ret)\n\t\tgoto error_undo;\n\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tbtrfs_scrub_cancel_dev(device);\n\n\t/*\n\t * the device list mutex makes sure that we don't change\n\t * the device list while someone else is writing out all\n\t * the device supers. Whoever is writing all supers, should\n\t * lock the device list mutex before getting the number of\n\t * devices in the super block (super_copy). Conversely,\n\t * whoever updates the number of devices in the super block\n\t * (super_copy) should hold the device list mutex.\n\t */\n\n\t/*\n\t * In normal cases the cur_devices == fs_devices. But in case\n\t * of deleting a seed device, the cur_devices should point to\n\t * its own fs_devices listed under the fs_devices->seed.\n\t */\n\tcur_devices = device->fs_devices;\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_del_rcu(&device->dev_list);\n\n\tcur_devices->num_devices--;\n\tcur_devices->total_devices--;\n\t/* Update total_devices of the parent fs_devices if it's seed */\n\tif (cur_devices != fs_devices)\n\t\tfs_devices->total_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tcur_devices->missing_devices--;\n\n\tbtrfs_assign_next_active_device(device, NULL);\n\n\tif (device->bdev) {\n\t\tcur_devices->open_devices--;\n\t\t/* remove sysfs entry */\n\t\tbtrfs_sysfs_remove_device(device);\n\t}\n\n\tnum_devices = btrfs_super_num_devices(fs_info->super_copy) - 1;\n\tbtrfs_set_super_num_devices(fs_info->super_copy, num_devices);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * at this point, the device is zero sized and detached from\n\t * the devices list.  All that's left is to zero out the old\n\t * supers and free the device.\n\t */\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\tbtrfs_scratch_superblocks(fs_info, device->bdev,\n\t\t\t\t\t  device->name->str);\n\n\tbtrfs_close_bdev(device);\n\tsynchronize_rcu();\n\tbtrfs_free_device(device);\n\n\tif (cur_devices->open_devices == 0) {\n\t\tlist_del_init(&cur_devices->seed_list);\n\t\tclose_fs_devices(cur_devices);\n\t\tfree_fs_devices(cur_devices);\n\t}\n\nout:\n\tmutex_unlock(&uuid_mutex);\n\treturn ret;\n\nerror_undo:\n\tbtrfs_reada_undo_remove_dev(device);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_add(&device->dev_alloc_list,\n\t\t\t &fs_devices->alloc_list);\n\t\tdevice->fs_devices->rw_devices++;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\tgoto out;\n}",
                        "cve_id": "CVE-2019-18885"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static noinline struct btrfs_device *device_list_add(const char *path,\n\t\t\t   struct btrfs_super_block *disk_super,\n\t\t\t   bool *new_device_added)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *fs_devices = NULL;\n\tstruct rcu_string *name;\n\tu64 found_transid = btrfs_super_generation(disk_super);\n\tu64 devid = btrfs_stack_device_id(&disk_super->dev_item);\n\tbool has_metadata_uuid = (btrfs_super_incompat_flags(disk_super) &\n\t\tBTRFS_FEATURE_INCOMPAT_METADATA_UUID);\n\tbool fsid_change_in_progress = (btrfs_super_flags(disk_super) &\n\t\t\t\t\tBTRFS_SUPER_FLAG_CHANGING_FSID_V2);\n\n\tif (fsid_change_in_progress) {\n\t\tif (!has_metadata_uuid) {\n\t\t\t/*\n\t\t\t * When we have an image which has CHANGING_FSID_V2 set\n\t\t\t * it might belong to either a filesystem which has\n\t\t\t * disks with completed fsid change or it might belong\n\t\t\t * to fs with no UUID changes in effect, handle both.\n\t\t\t */\n\t\t\tfs_devices = find_fsid_inprogress(disk_super);\n\t\t\tif (!fs_devices)\n\t\t\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t\t} else {\n\t\t\tfs_devices = find_fsid_changed(disk_super);\n\t\t}\n\t} else if (has_metadata_uuid) {\n\t\tfs_devices = find_fsid(disk_super->fsid,\n\t\t\t\t       disk_super->metadata_uuid);\n\t} else {\n\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t}\n\n\n\tif (!fs_devices) {\n\t\tif (has_metadata_uuid)\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid,\n\t\t\t\t\t\t      disk_super->metadata_uuid);\n\t\telse\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid, NULL);\n\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn ERR_CAST(fs_devices);\n\n\t\tfs_devices->fsid_change = fsid_change_in_progress;\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_add(&fs_devices->fs_list, &fs_uuids);\n\n\t\tdevice = NULL;\n\t} else {\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tdevice = find_device(fs_devices, devid,\n\t\t\t\tdisk_super->dev_item.uuid);\n\n\t\t/*\n\t\t * If this disk has been pulled into an fs devices created by\n\t\t * a device which had the CHANGING_FSID_V2 flag then replace the\n\t\t * metadata_uuid/fsid values of the fs_devices.\n\t\t */\n\t\tif (has_metadata_uuid && fs_devices->fsid_change &&\n\t\t    found_transid > fs_devices->latest_generation) {\n\t\t\tmemcpy(fs_devices->fsid, disk_super->fsid,\n\t\t\t\t\tBTRFS_FSID_SIZE);\n\t\t\tmemcpy(fs_devices->metadata_uuid,\n\t\t\t\t\tdisk_super->metadata_uuid, BTRFS_FSID_SIZE);\n\n\t\t\tfs_devices->fsid_change = false;\n\t\t}\n\t}\n\n\tif (!device) {\n\t\tif (fs_devices->opened) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EBUSY);\n\t\t}\n\n\t\tdevice = btrfs_alloc_device(NULL, &devid,\n\t\t\t\t\t    disk_super->dev_item.uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t/* we can safely leave the fs_devices entry around */\n\t\t\treturn device;\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tbtrfs_free_device(device);\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_assign_pointer(device->name, name);\n\n\t\tlist_add_rcu(&device->dev_list, &fs_devices->devices);\n\t\tfs_devices->num_devices++;\n\n\t\tdevice->fs_devices = fs_devices;\n\t\t*new_device_added = true;\n\n\t\tif (disk_super->label[0])\n\t\t\tpr_info(\"BTRFS: device label %s devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->label, devid, found_transid, path);\n\t\telse\n\t\t\tpr_info(\"BTRFS: device fsid %pU devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->fsid, devid, found_transid, path);\n\n\t} else if (!device->name || strcmp(device->name->str, path)) {\n\t\t/*\n\t\t * When FS is already mounted.\n\t\t * 1. If you are here and if the device->name is NULL that\n\t\t *    means this device was missing at time of FS mount.\n\t\t * 2. If you are here and if the device->name is different\n\t\t *    from 'path' that means either\n\t\t *      a. The same device disappeared and reappeared with\n\t\t *         different name. or\n\t\t *      b. The missing-disk-which-was-replaced, has\n\t\t *         reappeared now.\n\t\t *\n\t\t * We must allow 1 and 2a above. But 2b would be a spurious\n\t\t * and unintentional.\n\t\t *\n\t\t * Further in case of 1 and 2a above, the disk at 'path'\n\t\t * would have missed some transaction when it was away and\n\t\t * in case of 2a the stale bdev has to be updated as well.\n\t\t * 2b must not be allowed at all time.\n\t\t */\n\n\t\t/*\n\t\t * For now, we do allow update to btrfs_fs_device through the\n\t\t * btrfs dev scan cli after FS has been mounted.  We're still\n\t\t * tracking a problem where systems fail mount by subvolume id\n\t\t * when we reject replacement on a mounted FS.\n\t\t */\n\t\tif (!fs_devices->opened && found_transid < device->generation) {\n\t\t\t/*\n\t\t\t * That is if the FS is _not_ mounted and if you\n\t\t\t * are here, that means there is more than one\n\t\t\t * disk with same uuid and devid.We keep the one\n\t\t\t * with larger generation number or the last-in if\n\t\t\t * generation are equal.\n\t\t\t */\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EEXIST);\n\t\t}\n\n\t\t/*\n\t\t * We are going to replace the device path for a given devid,\n\t\t * make sure it's the same device if the device is mounted\n\t\t */\n\t\tif (device->bdev) {\n\t\t\tstruct block_device *path_bdev;\n\n\t\t\tpath_bdev = lookup_bdev(path);\n\t\t\tif (IS_ERR(path_bdev)) {\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\treturn ERR_CAST(path_bdev);\n\t\t\t}\n\n\t\t\tif (device->bdev != path_bdev) {\n\t\t\t\tbdput(path_bdev);\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\tbtrfs_warn_in_rcu(device->fs_info,\n\t\t\t\"duplicate device fsid:devid for %pU:%llu old:%s new:%s\",\n\t\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\t\trcu_str_deref(device->name), path);\n\t\t\t\treturn ERR_PTR(-EEXIST);\n\t\t\t}\n\t\t\tbdput(path_bdev);\n\t\t\tbtrfs_info_in_rcu(device->fs_info,\n\t\t\t\t\"device fsid %pU devid %llu moved old:%s new:%s\",\n\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\trcu_str_deref(device->name), path);\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_string_free(device->name);\n\t\trcu_assign_pointer(device->name, name);\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\tfs_devices->missing_devices--;\n\t\t\tclear_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\t}\n\n\t/*\n\t * Unmount does not free the btrfs_device struct but would zero\n\t * generation along with most of the other members. So just update\n\t * it back. We need it to pick the disk with largest generation\n\t * (as above).\n\t */\n\tif (!fs_devices->opened) {\n\t\tdevice->generation = found_transid;\n\t\tfs_devices->latest_generation = max_t(u64, found_transid,\n\t\t\t\t\t\tfs_devices->latest_generation);\n\t}\n\n\tfs_devices->total_devices = btrfs_super_num_devices(disk_super);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\treturn device;\n}",
                        "code_after_change": "static int verify_one_dev_extent(struct btrfs_fs_info *fs_info,\n\t\t\t\t u64 chunk_offset, u64 devid,\n\t\t\t\t u64 physical_offset, u64 physical_len)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree.map_tree;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_device *dev;\n\tu64 stripe_len;\n\tbool found = false;\n\tint ret = 0;\n\tint i;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, chunk_offset, 1);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu doesn't have corresponding chunk\",\n\t\t\t  physical_offset, devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tmap = em->map_lookup;\n\tstripe_len = calc_stripe_length(map->type, em->len, map->num_stripes);\n\tif (physical_len != stripe_len) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu length doesn't match chunk %llu, have %llu expect %llu\",\n\t\t\t  physical_offset, devid, em->start, physical_len,\n\t\t\t  stripe_len);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (map->stripes[i].dev->devid == devid &&\n\t\t    map->stripes[i].physical == physical_offset) {\n\t\t\tfound = true;\n\t\t\tif (map->verified_stripes >= map->num_stripes) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"too many dev extents for chunk %llu found\",\n\t\t\t\t\t  em->start);\n\t\t\t\tret = -EUCLEAN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tmap->verified_stripes++;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found) {\n\t\tbtrfs_err(fs_info,\n\t\"dev extent physical offset %llu devid %llu has no corresponding chunk\",\n\t\t\tphysical_offset, devid);\n\t\tret = -EUCLEAN;\n\t}\n\n\t/* Make sure no dev extent is beyond device bondary */\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL, true);\n\tif (!dev) {\n\t\tbtrfs_err(fs_info, \"failed to find devid %llu\", devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\t/* It's possible this device is a dummy for seed device */\n\tif (dev->disk_total_bytes == 0) {\n\t\tdev = btrfs_find_device(fs_info->fs_devices->seed, devid, NULL,\n\t\t\t\t\tNULL, false);\n\t\tif (!dev) {\n\t\t\tbtrfs_err(fs_info, \"failed to find seed devid %llu\",\n\t\t\t\t  devid);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (physical_offset + physical_len > dev->disk_total_bytes) {\n\t\tbtrfs_err(fs_info,\n\"dev extent devid %llu physical offset %llu len %llu is beyond device boundary %llu\",\n\t\t\t  devid, physical_offset, physical_len,\n\t\t\t  dev->disk_total_bytes);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\nout:\n\tfree_extent_map(em);\n\treturn ret;\n}",
                        "cve_id": "CVE-2019-18885"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2220,
            "cve_id": "CVE-2019-18885",
            "code_snippet": "int btrfs_get_dev_stats(struct btrfs_fs_info *fs_info,\n\t\t\tstruct btrfs_ioctl_get_dev_stats *stats)\n{\n\tstruct btrfs_device *dev;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tint i;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tdev = btrfs_find_device(fs_info->fs_devices, stats->devid, NULL, NULL);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tif (!dev) {\n\t\tbtrfs_warn(fs_info, \"get dev_stats failed, device not found\");\n\t\treturn -ENODEV;\n\t} else if (!dev->dev_stats_valid) {\n\t\tbtrfs_warn(fs_info, \"get dev_stats failed, not yet valid\");\n\t\treturn -ENODEV;\n\t} else if (stats->flags & BTRFS_DEV_STATS_RESET) {\n\t\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++) {\n\t\t\tif (stats->nr_items > i)\n\t\t\t\tstats->values[i] =\n\t\t\t\t\tbtrfs_dev_stat_read_and_reset(dev, i);\n\t\t\telse\n\t\t\t\tbtrfs_dev_stat_reset(dev, i);\n\t\t}\n\t} else {\n\t\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++)\n\t\t\tif (stats->nr_items > i)\n\t\t\t\tstats->values[i] = btrfs_dev_stat_read(dev, i);\n\t}\n\tif (stats->nr_items > BTRFS_DEV_STAT_VALUES_MAX)\n\t\tstats->nr_items = BTRFS_DEV_STAT_VALUES_MAX;\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int verify_one_dev_extent(struct btrfs_fs_info *fs_info,\n\t\t\t\t u64 chunk_offset, u64 devid,\n\t\t\t\t u64 physical_offset, u64 physical_len)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree.map_tree;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_device *dev;\n\tu64 stripe_len;\n\tbool found = false;\n\tint ret = 0;\n\tint i;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, chunk_offset, 1);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu doesn't have corresponding chunk\",\n\t\t\t  physical_offset, devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tmap = em->map_lookup;\n\tstripe_len = calc_stripe_length(map->type, em->len, map->num_stripes);\n\tif (physical_len != stripe_len) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu length doesn't match chunk %llu, have %llu expect %llu\",\n\t\t\t  physical_offset, devid, em->start, physical_len,\n\t\t\t  stripe_len);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (map->stripes[i].dev->devid == devid &&\n\t\t    map->stripes[i].physical == physical_offset) {\n\t\t\tfound = true;\n\t\t\tif (map->verified_stripes >= map->num_stripes) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"too many dev extents for chunk %llu found\",\n\t\t\t\t\t  em->start);\n\t\t\t\tret = -EUCLEAN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tmap->verified_stripes++;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found) {\n\t\tbtrfs_err(fs_info,\n\t\"dev extent physical offset %llu devid %llu has no corresponding chunk\",\n\t\t\tphysical_offset, devid);\n\t\tret = -EUCLEAN;\n\t}\n\n\t/* Make sure no dev extent is beyond device bondary */\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL);\n\tif (!dev) {\n\t\tbtrfs_err(fs_info, \"failed to find devid %llu\", devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\t/* It's possible this device is a dummy for seed device */\n\tif (dev->disk_total_bytes == 0) {\n\t\tdev = find_device(fs_info->fs_devices->seed, devid, NULL);\n\t\tif (!dev) {\n\t\t\tbtrfs_err(fs_info, \"failed to find seed devid %llu\",\n\t\t\t\t  devid);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (physical_offset + physical_len > dev->disk_total_bytes) {\n\t\tbtrfs_err(fs_info,\n\"dev extent devid %llu physical offset %llu len %llu is beyond device boundary %llu\",\n\t\t\t  devid, physical_offset, physical_len,\n\t\t\t  dev->disk_total_bytes);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\nout:\n\tfree_extent_map(em);\n\treturn ret;\n}",
                        "code_after_change": "static int verify_one_dev_extent(struct btrfs_fs_info *fs_info,\n\t\t\t\t u64 chunk_offset, u64 devid,\n\t\t\t\t u64 physical_offset, u64 physical_len)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree.map_tree;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_device *dev;\n\tu64 stripe_len;\n\tbool found = false;\n\tint ret = 0;\n\tint i;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, chunk_offset, 1);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu doesn't have corresponding chunk\",\n\t\t\t  physical_offset, devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tmap = em->map_lookup;\n\tstripe_len = calc_stripe_length(map->type, em->len, map->num_stripes);\n\tif (physical_len != stripe_len) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu length doesn't match chunk %llu, have %llu expect %llu\",\n\t\t\t  physical_offset, devid, em->start, physical_len,\n\t\t\t  stripe_len);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (map->stripes[i].dev->devid == devid &&\n\t\t    map->stripes[i].physical == physical_offset) {\n\t\t\tfound = true;\n\t\t\tif (map->verified_stripes >= map->num_stripes) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"too many dev extents for chunk %llu found\",\n\t\t\t\t\t  em->start);\n\t\t\t\tret = -EUCLEAN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tmap->verified_stripes++;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found) {\n\t\tbtrfs_err(fs_info,\n\t\"dev extent physical offset %llu devid %llu has no corresponding chunk\",\n\t\t\tphysical_offset, devid);\n\t\tret = -EUCLEAN;\n\t}\n\n\t/* Make sure no dev extent is beyond device bondary */\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL, true);\n\tif (!dev) {\n\t\tbtrfs_err(fs_info, \"failed to find devid %llu\", devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\t/* It's possible this device is a dummy for seed device */\n\tif (dev->disk_total_bytes == 0) {\n\t\tdev = btrfs_find_device(fs_info->fs_devices->seed, devid, NULL,\n\t\t\t\t\tNULL, false);\n\t\tif (!dev) {\n\t\t\tbtrfs_err(fs_info, \"failed to find seed devid %llu\",\n\t\t\t\t  devid);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (physical_offset + physical_len > dev->disk_total_bytes) {\n\t\tbtrfs_err(fs_info,\n\"dev extent devid %llu physical offset %llu len %llu is beyond device boundary %llu\",\n\t\t\t  devid, physical_offset, physical_len,\n\t\t\t  dev->disk_total_bytes);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\nout:\n\tfree_extent_map(em);\n\treturn ret;\n}",
                        "cve_id": "CVE-2019-18885"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int read_one_chunk(struct btrfs_fs_info *fs_info, struct btrfs_key *key,\n\t\t\t  struct extent_buffer *leaf,\n\t\t\t  struct btrfs_chunk *chunk)\n{\n\tstruct btrfs_mapping_tree *map_tree = &fs_info->mapping_tree;\n\tstruct map_lookup *map;\n\tstruct extent_map *em;\n\tu64 logical;\n\tu64 length;\n\tu64 devid;\n\tu8 uuid[BTRFS_UUID_SIZE];\n\tint num_stripes;\n\tint ret;\n\tint i;\n\n\tlogical = key->offset;\n\tlength = btrfs_chunk_length(leaf, chunk);\n\tnum_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\n\tret = btrfs_check_chunk_valid(fs_info, leaf, chunk, logical);\n\tif (ret)\n\t\treturn ret;\n\n\tread_lock(&map_tree->map_tree.lock);\n\tem = lookup_extent_mapping(&map_tree->map_tree, logical, 1);\n\tread_unlock(&map_tree->map_tree.lock);\n\n\t/* already mapped? */\n\tif (em && em->start <= logical && em->start + em->len > logical) {\n\t\tfree_extent_map(em);\n\t\treturn 0;\n\t} else if (em) {\n\t\tfree_extent_map(em);\n\t}\n\n\tem = alloc_extent_map();\n\tif (!em)\n\t\treturn -ENOMEM;\n\tmap = kmalloc(map_lookup_size(num_stripes), GFP_NOFS);\n\tif (!map) {\n\t\tfree_extent_map(em);\n\t\treturn -ENOMEM;\n\t}\n\n\tset_bit(EXTENT_FLAG_FS_MAPPING, &em->flags);\n\tem->map_lookup = map;\n\tem->start = logical;\n\tem->len = length;\n\tem->orig_start = 0;\n\tem->block_start = 0;\n\tem->block_len = em->len;\n\n\tmap->num_stripes = num_stripes;\n\tmap->io_width = btrfs_chunk_io_width(leaf, chunk);\n\tmap->io_align = btrfs_chunk_io_align(leaf, chunk);\n\tmap->stripe_len = btrfs_chunk_stripe_len(leaf, chunk);\n\tmap->type = btrfs_chunk_type(leaf, chunk);\n\tmap->sub_stripes = btrfs_chunk_sub_stripes(leaf, chunk);\n\tmap->verified_stripes = 0;\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tmap->stripes[i].physical =\n\t\t\tbtrfs_stripe_offset_nr(leaf, chunk, i);\n\t\tdevid = btrfs_stripe_devid_nr(leaf, chunk, i);\n\t\tread_extent_buffer(leaf, uuid, (unsigned long)\n\t\t\t\t   btrfs_stripe_dev_uuid_nr(chunk, i),\n\t\t\t\t   BTRFS_UUID_SIZE);\n\t\tmap->stripes[i].dev = btrfs_find_device(fs_info->fs_devices,\n\t\t\t\t\t\t\tdevid, uuid, NULL);\n\t\tif (!map->stripes[i].dev &&\n\t\t    !btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tfree_extent_map(em);\n\t\t\tbtrfs_report_missing_device(fs_info, devid, uuid, true);\n\t\t\treturn -ENOENT;\n\t\t}\n\t\tif (!map->stripes[i].dev) {\n\t\t\tmap->stripes[i].dev =\n\t\t\t\tadd_missing_dev(fs_info->fs_devices, devid,\n\t\t\t\t\t\tuuid);\n\t\t\tif (IS_ERR(map->stripes[i].dev)) {\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\t\"failed to init missing dev %llu: %ld\",\n\t\t\t\t\tdevid, PTR_ERR(map->stripes[i].dev));\n\t\t\t\treturn PTR_ERR(map->stripes[i].dev);\n\t\t\t}\n\t\t\tbtrfs_report_missing_device(fs_info, devid, uuid, false);\n\t\t}\n\t\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t&(map->stripes[i].dev->dev_state));\n\n\t}\n\n\twrite_lock(&map_tree->map_tree.lock);\n\tret = add_extent_mapping(&map_tree->map_tree, em, 0);\n\twrite_unlock(&map_tree->map_tree.lock);\n\tif (ret < 0) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"failed to add chunk map, start=%llu len=%llu: %d\",\n\t\t\t  em->start, em->len, ret);\n\t}\n\tfree_extent_map(em);\n\n\treturn ret;\n}",
                        "code_after_change": "static int read_one_chunk(struct btrfs_fs_info *fs_info, struct btrfs_key *key,\n\t\t\t  struct extent_buffer *leaf,\n\t\t\t  struct btrfs_chunk *chunk)\n{\n\tstruct btrfs_mapping_tree *map_tree = &fs_info->mapping_tree;\n\tstruct map_lookup *map;\n\tstruct extent_map *em;\n\tu64 logical;\n\tu64 length;\n\tu64 devid;\n\tu8 uuid[BTRFS_UUID_SIZE];\n\tint num_stripes;\n\tint ret;\n\tint i;\n\n\tlogical = key->offset;\n\tlength = btrfs_chunk_length(leaf, chunk);\n\tnum_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\n\tret = btrfs_check_chunk_valid(fs_info, leaf, chunk, logical);\n\tif (ret)\n\t\treturn ret;\n\n\tread_lock(&map_tree->map_tree.lock);\n\tem = lookup_extent_mapping(&map_tree->map_tree, logical, 1);\n\tread_unlock(&map_tree->map_tree.lock);\n\n\t/* already mapped? */\n\tif (em && em->start <= logical && em->start + em->len > logical) {\n\t\tfree_extent_map(em);\n\t\treturn 0;\n\t} else if (em) {\n\t\tfree_extent_map(em);\n\t}\n\n\tem = alloc_extent_map();\n\tif (!em)\n\t\treturn -ENOMEM;\n\tmap = kmalloc(map_lookup_size(num_stripes), GFP_NOFS);\n\tif (!map) {\n\t\tfree_extent_map(em);\n\t\treturn -ENOMEM;\n\t}\n\n\tset_bit(EXTENT_FLAG_FS_MAPPING, &em->flags);\n\tem->map_lookup = map;\n\tem->start = logical;\n\tem->len = length;\n\tem->orig_start = 0;\n\tem->block_start = 0;\n\tem->block_len = em->len;\n\n\tmap->num_stripes = num_stripes;\n\tmap->io_width = btrfs_chunk_io_width(leaf, chunk);\n\tmap->io_align = btrfs_chunk_io_align(leaf, chunk);\n\tmap->stripe_len = btrfs_chunk_stripe_len(leaf, chunk);\n\tmap->type = btrfs_chunk_type(leaf, chunk);\n\tmap->sub_stripes = btrfs_chunk_sub_stripes(leaf, chunk);\n\tmap->verified_stripes = 0;\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tmap->stripes[i].physical =\n\t\t\tbtrfs_stripe_offset_nr(leaf, chunk, i);\n\t\tdevid = btrfs_stripe_devid_nr(leaf, chunk, i);\n\t\tread_extent_buffer(leaf, uuid, (unsigned long)\n\t\t\t\t   btrfs_stripe_dev_uuid_nr(chunk, i),\n\t\t\t\t   BTRFS_UUID_SIZE);\n\t\tmap->stripes[i].dev = btrfs_find_device(fs_info->fs_devices,\n\t\t\t\t\t\t\tdevid, uuid, NULL, true);\n\t\tif (!map->stripes[i].dev &&\n\t\t    !btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tfree_extent_map(em);\n\t\t\tbtrfs_report_missing_device(fs_info, devid, uuid, true);\n\t\t\treturn -ENOENT;\n\t\t}\n\t\tif (!map->stripes[i].dev) {\n\t\t\tmap->stripes[i].dev =\n\t\t\t\tadd_missing_dev(fs_info->fs_devices, devid,\n\t\t\t\t\t\tuuid);\n\t\t\tif (IS_ERR(map->stripes[i].dev)) {\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\t\"failed to init missing dev %llu: %ld\",\n\t\t\t\t\tdevid, PTR_ERR(map->stripes[i].dev));\n\t\t\t\treturn PTR_ERR(map->stripes[i].dev);\n\t\t\t}\n\t\t\tbtrfs_report_missing_device(fs_info, devid, uuid, false);\n\t\t}\n\t\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t&(map->stripes[i].dev->dev_state));\n\n\t}\n\n\twrite_lock(&map_tree->map_tree.lock);\n\tret = add_extent_mapping(&map_tree->map_tree, em, 0);\n\twrite_unlock(&map_tree->map_tree.lock);\n\tif (ret < 0) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"failed to add chunk map, start=%llu len=%llu: %d\",\n\t\t\t  em->start, em->len, ret);\n\t}\n\tfree_extent_map(em);\n\n\treturn ret;\n}",
                        "cve_id": "CVE-2019-18885"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int btrfs_rm_device(struct btrfs_fs_info *fs_info, const char *device_path,\n\t\t    u64 devid)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *cur_devices;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 num_devices;\n\tint ret = 0;\n\n\tmutex_lock(&uuid_mutex);\n\n\tnum_devices = btrfs_num_devices(fs_info);\n\n\tret = btrfs_check_raid_min_devices(fs_info, num_devices - 1);\n\tif (ret)\n\t\tgoto out;\n\n\tdevice = btrfs_find_device_by_devspec(fs_info, devid, device_path);\n\n\tif (IS_ERR(device)) {\n\t\tif (PTR_ERR(device) == -ENOENT &&\n\t\t    strcmp(device_path, \"missing\") == 0)\n\t\t\tret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND;\n\t\telse\n\t\t\tret = PTR_ERR(device);\n\t\tgoto out;\n\t}\n\n\tif (btrfs_pinned_by_swapfile(fs_info, device)) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t  \"cannot remove device %s (devid %llu) due to active swapfile\",\n\t\t\t\t  rcu_str_deref(device->name), device->devid);\n\t\tret = -ETXTBSY;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = BTRFS_ERROR_DEV_TGT_REPLACE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    fs_info->fs_devices->rw_devices == 1) {\n\t\tret = BTRFS_ERROR_DEV_ONLY_WRITABLE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tdevice->fs_devices->rw_devices--;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\n\tmutex_unlock(&uuid_mutex);\n\tret = btrfs_shrink_device(device, 0);\n\tif (!ret)\n\t\tbtrfs_reada_remove_dev(device);\n\tmutex_lock(&uuid_mutex);\n\tif (ret)\n\t\tgoto error_undo;\n\n\t/*\n\t * TODO: the superblock still includes this device in its num_devices\n\t * counter although write_all_supers() is not locked out. This\n\t * could give a filesystem state which requires a degraded mount.\n\t */\n\tret = btrfs_rm_dev_item(device);\n\tif (ret)\n\t\tgoto error_undo;\n\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tbtrfs_scrub_cancel_dev(device);\n\n\t/*\n\t * the device list mutex makes sure that we don't change\n\t * the device list while someone else is writing out all\n\t * the device supers. Whoever is writing all supers, should\n\t * lock the device list mutex before getting the number of\n\t * devices in the super block (super_copy). Conversely,\n\t * whoever updates the number of devices in the super block\n\t * (super_copy) should hold the device list mutex.\n\t */\n\n\t/*\n\t * In normal cases the cur_devices == fs_devices. But in case\n\t * of deleting a seed device, the cur_devices should point to\n\t * its own fs_devices listed under the fs_devices->seed.\n\t */\n\tcur_devices = device->fs_devices;\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_del_rcu(&device->dev_list);\n\n\tcur_devices->num_devices--;\n\tcur_devices->total_devices--;\n\t/* Update total_devices of the parent fs_devices if it's seed */\n\tif (cur_devices != fs_devices)\n\t\tfs_devices->total_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tcur_devices->missing_devices--;\n\n\tbtrfs_assign_next_active_device(device, NULL);\n\n\tif (device->bdev) {\n\t\tcur_devices->open_devices--;\n\t\t/* remove sysfs entry */\n\t\tbtrfs_sysfs_remove_device(device);\n\t}\n\n\tnum_devices = btrfs_super_num_devices(fs_info->super_copy) - 1;\n\tbtrfs_set_super_num_devices(fs_info->super_copy, num_devices);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * at this point, the device is zero sized and detached from\n\t * the devices list.  All that's left is to zero out the old\n\t * supers and free the device.\n\t */\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\tbtrfs_scratch_superblocks(fs_info, device->bdev,\n\t\t\t\t\t  device->name->str);\n\n\tbtrfs_close_bdev(device);\n\tsynchronize_rcu();\n\tbtrfs_free_device(device);\n\n\tif (cur_devices->open_devices == 0) {\n\t\tlist_del_init(&cur_devices->seed_list);\n\t\tclose_fs_devices(cur_devices);\n\t\tfree_fs_devices(cur_devices);\n\t}\n\nout:\n\tmutex_unlock(&uuid_mutex);\n\treturn ret;\n\nerror_undo:\n\tbtrfs_reada_undo_remove_dev(device);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_add(&device->dev_alloc_list,\n\t\t\t &fs_devices->alloc_list);\n\t\tdevice->fs_devices->rw_devices++;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\tgoto out;\n}",
                        "code_after_change": "static noinline struct btrfs_device *device_list_add(const char *path,\n\t\t\t   struct btrfs_super_block *disk_super,\n\t\t\t   bool *new_device_added)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *fs_devices = NULL;\n\tstruct rcu_string *name;\n\tu64 found_transid = btrfs_super_generation(disk_super);\n\tu64 devid = btrfs_stack_device_id(&disk_super->dev_item);\n\tbool has_metadata_uuid = (btrfs_super_incompat_flags(disk_super) &\n\t\tBTRFS_FEATURE_INCOMPAT_METADATA_UUID);\n\tbool fsid_change_in_progress = (btrfs_super_flags(disk_super) &\n\t\t\t\t\tBTRFS_SUPER_FLAG_CHANGING_FSID_V2);\n\n\tif (fsid_change_in_progress) {\n\t\tif (!has_metadata_uuid) {\n\t\t\t/*\n\t\t\t * When we have an image which has CHANGING_FSID_V2 set\n\t\t\t * it might belong to either a filesystem which has\n\t\t\t * disks with completed fsid change or it might belong\n\t\t\t * to fs with no UUID changes in effect, handle both.\n\t\t\t */\n\t\t\tfs_devices = find_fsid_inprogress(disk_super);\n\t\t\tif (!fs_devices)\n\t\t\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t\t} else {\n\t\t\tfs_devices = find_fsid_changed(disk_super);\n\t\t}\n\t} else if (has_metadata_uuid) {\n\t\tfs_devices = find_fsid(disk_super->fsid,\n\t\t\t\t       disk_super->metadata_uuid);\n\t} else {\n\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t}\n\n\n\tif (!fs_devices) {\n\t\tif (has_metadata_uuid)\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid,\n\t\t\t\t\t\t      disk_super->metadata_uuid);\n\t\telse\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid, NULL);\n\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn ERR_CAST(fs_devices);\n\n\t\tfs_devices->fsid_change = fsid_change_in_progress;\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_add(&fs_devices->fs_list, &fs_uuids);\n\n\t\tdevice = NULL;\n\t} else {\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tdevice = btrfs_find_device(fs_devices, devid,\n\t\t\t\tdisk_super->dev_item.uuid, NULL, false);\n\n\t\t/*\n\t\t * If this disk has been pulled into an fs devices created by\n\t\t * a device which had the CHANGING_FSID_V2 flag then replace the\n\t\t * metadata_uuid/fsid values of the fs_devices.\n\t\t */\n\t\tif (has_metadata_uuid && fs_devices->fsid_change &&\n\t\t    found_transid > fs_devices->latest_generation) {\n\t\t\tmemcpy(fs_devices->fsid, disk_super->fsid,\n\t\t\t\t\tBTRFS_FSID_SIZE);\n\t\t\tmemcpy(fs_devices->metadata_uuid,\n\t\t\t\t\tdisk_super->metadata_uuid, BTRFS_FSID_SIZE);\n\n\t\t\tfs_devices->fsid_change = false;\n\t\t}\n\t}\n\n\tif (!device) {\n\t\tif (fs_devices->opened) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EBUSY);\n\t\t}\n\n\t\tdevice = btrfs_alloc_device(NULL, &devid,\n\t\t\t\t\t    disk_super->dev_item.uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t/* we can safely leave the fs_devices entry around */\n\t\t\treturn device;\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tbtrfs_free_device(device);\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_assign_pointer(device->name, name);\n\n\t\tlist_add_rcu(&device->dev_list, &fs_devices->devices);\n\t\tfs_devices->num_devices++;\n\n\t\tdevice->fs_devices = fs_devices;\n\t\t*new_device_added = true;\n\n\t\tif (disk_super->label[0])\n\t\t\tpr_info(\"BTRFS: device label %s devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->label, devid, found_transid, path);\n\t\telse\n\t\t\tpr_info(\"BTRFS: device fsid %pU devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->fsid, devid, found_transid, path);\n\n\t} else if (!device->name || strcmp(device->name->str, path)) {\n\t\t/*\n\t\t * When FS is already mounted.\n\t\t * 1. If you are here and if the device->name is NULL that\n\t\t *    means this device was missing at time of FS mount.\n\t\t * 2. If you are here and if the device->name is different\n\t\t *    from 'path' that means either\n\t\t *      a. The same device disappeared and reappeared with\n\t\t *         different name. or\n\t\t *      b. The missing-disk-which-was-replaced, has\n\t\t *         reappeared now.\n\t\t *\n\t\t * We must allow 1 and 2a above. But 2b would be a spurious\n\t\t * and unintentional.\n\t\t *\n\t\t * Further in case of 1 and 2a above, the disk at 'path'\n\t\t * would have missed some transaction when it was away and\n\t\t * in case of 2a the stale bdev has to be updated as well.\n\t\t * 2b must not be allowed at all time.\n\t\t */\n\n\t\t/*\n\t\t * For now, we do allow update to btrfs_fs_device through the\n\t\t * btrfs dev scan cli after FS has been mounted.  We're still\n\t\t * tracking a problem where systems fail mount by subvolume id\n\t\t * when we reject replacement on a mounted FS.\n\t\t */\n\t\tif (!fs_devices->opened && found_transid < device->generation) {\n\t\t\t/*\n\t\t\t * That is if the FS is _not_ mounted and if you\n\t\t\t * are here, that means there is more than one\n\t\t\t * disk with same uuid and devid.We keep the one\n\t\t\t * with larger generation number or the last-in if\n\t\t\t * generation are equal.\n\t\t\t */\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EEXIST);\n\t\t}\n\n\t\t/*\n\t\t * We are going to replace the device path for a given devid,\n\t\t * make sure it's the same device if the device is mounted\n\t\t */\n\t\tif (device->bdev) {\n\t\t\tstruct block_device *path_bdev;\n\n\t\t\tpath_bdev = lookup_bdev(path);\n\t\t\tif (IS_ERR(path_bdev)) {\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\treturn ERR_CAST(path_bdev);\n\t\t\t}\n\n\t\t\tif (device->bdev != path_bdev) {\n\t\t\t\tbdput(path_bdev);\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\tbtrfs_warn_in_rcu(device->fs_info,\n\t\t\t\"duplicate device fsid:devid for %pU:%llu old:%s new:%s\",\n\t\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\t\trcu_str_deref(device->name), path);\n\t\t\t\treturn ERR_PTR(-EEXIST);\n\t\t\t}\n\t\t\tbdput(path_bdev);\n\t\t\tbtrfs_info_in_rcu(device->fs_info,\n\t\t\t\t\"device fsid %pU devid %llu moved old:%s new:%s\",\n\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\trcu_str_deref(device->name), path);\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_string_free(device->name);\n\t\trcu_assign_pointer(device->name, name);\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\tfs_devices->missing_devices--;\n\t\t\tclear_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\t}\n\n\t/*\n\t * Unmount does not free the btrfs_device struct but would zero\n\t * generation along with most of the other members. So just update\n\t * it back. We need it to pick the disk with largest generation\n\t * (as above).\n\t */\n\tif (!fs_devices->opened) {\n\t\tdevice->generation = found_transid;\n\t\tfs_devices->latest_generation = max_t(u64, found_transid,\n\t\t\t\t\t\tfs_devices->latest_generation);\n\t}\n\n\tfs_devices->total_devices = btrfs_super_num_devices(disk_super);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\treturn device;\n}",
                        "cve_id": "CVE-2021-3739"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void netvsc_get_ethtool_stats(struct net_device *dev,\n\t\t\t\t     struct ethtool_stats *stats, u64 *data)\n{\n\tstruct net_device_context *ndc = netdev_priv(dev);\n\tstruct netvsc_device *nvdev = rtnl_dereference(ndc->nvdev);\n\tconst void *nds = &ndc->eth_stats;\n\tconst struct netvsc_stats *qstats;\n\tstruct netvsc_vf_pcpu_stats sum;\n\tstruct netvsc_ethtool_pcpu_stats *pcpu_sum;\n\tunsigned int start;\n\tu64 packets, bytes;\n\tu64 xdp_drop;\n\tint i, j, cpu;\n\n\tif (!nvdev)\n\t\treturn;\n\n\tfor (i = 0; i < NETVSC_GLOBAL_STATS_LEN; i++)\n\t\tdata[i] = *(unsigned long *)(nds + netvsc_stats[i].offset);\n\n\tnetvsc_get_vf_stats(dev, &sum);\n\tfor (j = 0; j < NETVSC_VF_STATS_LEN; j++)\n\t\tdata[i++] = *(u64 *)((void *)&sum + vf_stats[j].offset);\n\n\tfor (j = 0; j < nvdev->num_chn; j++) {\n\t\tqstats = &nvdev->chan_table[j].tx_stats;\n\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin_irq(&qstats->syncp);\n\t\t\tpackets = qstats->packets;\n\t\t\tbytes = qstats->bytes;\n\t\t} while (u64_stats_fetch_retry_irq(&qstats->syncp, start));\n\t\tdata[i++] = packets;\n\t\tdata[i++] = bytes;\n\n\t\tqstats = &nvdev->chan_table[j].rx_stats;\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin_irq(&qstats->syncp);\n\t\t\tpackets = qstats->packets;\n\t\t\tbytes = qstats->bytes;\n\t\t\txdp_drop = qstats->xdp_drop;\n\t\t} while (u64_stats_fetch_retry_irq(&qstats->syncp, start));\n\t\tdata[i++] = packets;\n\t\tdata[i++] = bytes;\n\t\tdata[i++] = xdp_drop;\n\t}\n\n\tpcpu_sum = kvmalloc_array(num_possible_cpus(),\n\t\t\t\t  sizeof(struct netvsc_ethtool_pcpu_stats),\n\t\t\t\t  GFP_KERNEL);\n\tnetvsc_get_pcpu_stats(dev, pcpu_sum);\n\tfor_each_present_cpu(cpu) {\n\t\tstruct netvsc_ethtool_pcpu_stats *this_sum = &pcpu_sum[cpu];\n\n\t\tfor (j = 0; j < ARRAY_SIZE(pcpu_stats); j++)\n\t\t\tdata[i++] = *(u64 *)((void *)this_sum\n\t\t\t\t\t     + pcpu_stats[j].offset);\n\t}\n\tkvfree(pcpu_sum);\n}",
                        "code_after_change": "int btrfs_rm_device(struct btrfs_fs_info *fs_info, const char *device_path,\n\t\t    u64 devid)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *cur_devices;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 num_devices;\n\tint ret = 0;\n\n\tmutex_lock(&uuid_mutex);\n\n\tnum_devices = btrfs_num_devices(fs_info);\n\n\tret = btrfs_check_raid_min_devices(fs_info, num_devices - 1);\n\tif (ret)\n\t\tgoto out;\n\n\tdevice = btrfs_find_device_by_devspec(fs_info, devid, device_path);\n\n\tif (IS_ERR(device)) {\n\t\tif (PTR_ERR(device) == -ENOENT &&\n\t\t    device_path && strcmp(device_path, \"missing\") == 0)\n\t\t\tret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND;\n\t\telse\n\t\t\tret = PTR_ERR(device);\n\t\tgoto out;\n\t}\n\n\tif (btrfs_pinned_by_swapfile(fs_info, device)) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t  \"cannot remove device %s (devid %llu) due to active swapfile\",\n\t\t\t\t  rcu_str_deref(device->name), device->devid);\n\t\tret = -ETXTBSY;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = BTRFS_ERROR_DEV_TGT_REPLACE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    fs_info->fs_devices->rw_devices == 1) {\n\t\tret = BTRFS_ERROR_DEV_ONLY_WRITABLE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tdevice->fs_devices->rw_devices--;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\n\tmutex_unlock(&uuid_mutex);\n\tret = btrfs_shrink_device(device, 0);\n\tif (!ret)\n\t\tbtrfs_reada_remove_dev(device);\n\tmutex_lock(&uuid_mutex);\n\tif (ret)\n\t\tgoto error_undo;\n\n\t/*\n\t * TODO: the superblock still includes this device in its num_devices\n\t * counter although write_all_supers() is not locked out. This\n\t * could give a filesystem state which requires a degraded mount.\n\t */\n\tret = btrfs_rm_dev_item(device);\n\tif (ret)\n\t\tgoto error_undo;\n\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tbtrfs_scrub_cancel_dev(device);\n\n\t/*\n\t * the device list mutex makes sure that we don't change\n\t * the device list while someone else is writing out all\n\t * the device supers. Whoever is writing all supers, should\n\t * lock the device list mutex before getting the number of\n\t * devices in the super block (super_copy). Conversely,\n\t * whoever updates the number of devices in the super block\n\t * (super_copy) should hold the device list mutex.\n\t */\n\n\t/*\n\t * In normal cases the cur_devices == fs_devices. But in case\n\t * of deleting a seed device, the cur_devices should point to\n\t * its own fs_devices listed under the fs_devices->seed.\n\t */\n\tcur_devices = device->fs_devices;\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_del_rcu(&device->dev_list);\n\n\tcur_devices->num_devices--;\n\tcur_devices->total_devices--;\n\t/* Update total_devices of the parent fs_devices if it's seed */\n\tif (cur_devices != fs_devices)\n\t\tfs_devices->total_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tcur_devices->missing_devices--;\n\n\tbtrfs_assign_next_active_device(device, NULL);\n\n\tif (device->bdev) {\n\t\tcur_devices->open_devices--;\n\t\t/* remove sysfs entry */\n\t\tbtrfs_sysfs_remove_device(device);\n\t}\n\n\tnum_devices = btrfs_super_num_devices(fs_info->super_copy) - 1;\n\tbtrfs_set_super_num_devices(fs_info->super_copy, num_devices);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * at this point, the device is zero sized and detached from\n\t * the devices list.  All that's left is to zero out the old\n\t * supers and free the device.\n\t */\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\tbtrfs_scratch_superblocks(fs_info, device->bdev,\n\t\t\t\t\t  device->name->str);\n\n\tbtrfs_close_bdev(device);\n\tsynchronize_rcu();\n\tbtrfs_free_device(device);\n\n\tif (cur_devices->open_devices == 0) {\n\t\tlist_del_init(&cur_devices->seed_list);\n\t\tclose_fs_devices(cur_devices);\n\t\tfree_fs_devices(cur_devices);\n\t}\n\nout:\n\tmutex_unlock(&uuid_mutex);\n\treturn ret;\n\nerror_undo:\n\tbtrfs_reada_undo_remove_dev(device);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_add(&device->dev_alloc_list,\n\t\t\t &fs_devices->alloc_list);\n\t\tdevice->fs_devices->rw_devices++;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\tgoto out;\n}",
                        "cve_id": "CVE-2022-3107"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static noinline struct btrfs_device *device_list_add(const char *path,\n\t\t\t   struct btrfs_super_block *disk_super,\n\t\t\t   bool *new_device_added)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *fs_devices = NULL;\n\tstruct rcu_string *name;\n\tu64 found_transid = btrfs_super_generation(disk_super);\n\tu64 devid = btrfs_stack_device_id(&disk_super->dev_item);\n\tbool has_metadata_uuid = (btrfs_super_incompat_flags(disk_super) &\n\t\tBTRFS_FEATURE_INCOMPAT_METADATA_UUID);\n\tbool fsid_change_in_progress = (btrfs_super_flags(disk_super) &\n\t\t\t\t\tBTRFS_SUPER_FLAG_CHANGING_FSID_V2);\n\n\tif (fsid_change_in_progress) {\n\t\tif (!has_metadata_uuid) {\n\t\t\t/*\n\t\t\t * When we have an image which has CHANGING_FSID_V2 set\n\t\t\t * it might belong to either a filesystem which has\n\t\t\t * disks with completed fsid change or it might belong\n\t\t\t * to fs with no UUID changes in effect, handle both.\n\t\t\t */\n\t\t\tfs_devices = find_fsid_inprogress(disk_super);\n\t\t\tif (!fs_devices)\n\t\t\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t\t} else {\n\t\t\tfs_devices = find_fsid_changed(disk_super);\n\t\t}\n\t} else if (has_metadata_uuid) {\n\t\tfs_devices = find_fsid(disk_super->fsid,\n\t\t\t\t       disk_super->metadata_uuid);\n\t} else {\n\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t}\n\n\n\tif (!fs_devices) {\n\t\tif (has_metadata_uuid)\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid,\n\t\t\t\t\t\t      disk_super->metadata_uuid);\n\t\telse\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid, NULL);\n\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn ERR_CAST(fs_devices);\n\n\t\tfs_devices->fsid_change = fsid_change_in_progress;\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_add(&fs_devices->fs_list, &fs_uuids);\n\n\t\tdevice = NULL;\n\t} else {\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tdevice = find_device(fs_devices, devid,\n\t\t\t\tdisk_super->dev_item.uuid);\n\n\t\t/*\n\t\t * If this disk has been pulled into an fs devices created by\n\t\t * a device which had the CHANGING_FSID_V2 flag then replace the\n\t\t * metadata_uuid/fsid values of the fs_devices.\n\t\t */\n\t\tif (has_metadata_uuid && fs_devices->fsid_change &&\n\t\t    found_transid > fs_devices->latest_generation) {\n\t\t\tmemcpy(fs_devices->fsid, disk_super->fsid,\n\t\t\t\t\tBTRFS_FSID_SIZE);\n\t\t\tmemcpy(fs_devices->metadata_uuid,\n\t\t\t\t\tdisk_super->metadata_uuid, BTRFS_FSID_SIZE);\n\n\t\t\tfs_devices->fsid_change = false;\n\t\t}\n\t}\n\n\tif (!device) {\n\t\tif (fs_devices->opened) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EBUSY);\n\t\t}\n\n\t\tdevice = btrfs_alloc_device(NULL, &devid,\n\t\t\t\t\t    disk_super->dev_item.uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t/* we can safely leave the fs_devices entry around */\n\t\t\treturn device;\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tbtrfs_free_device(device);\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_assign_pointer(device->name, name);\n\n\t\tlist_add_rcu(&device->dev_list, &fs_devices->devices);\n\t\tfs_devices->num_devices++;\n\n\t\tdevice->fs_devices = fs_devices;\n\t\t*new_device_added = true;\n\n\t\tif (disk_super->label[0])\n\t\t\tpr_info(\"BTRFS: device label %s devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->label, devid, found_transid, path);\n\t\telse\n\t\t\tpr_info(\"BTRFS: device fsid %pU devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->fsid, devid, found_transid, path);\n\n\t} else if (!device->name || strcmp(device->name->str, path)) {\n\t\t/*\n\t\t * When FS is already mounted.\n\t\t * 1. If you are here and if the device->name is NULL that\n\t\t *    means this device was missing at time of FS mount.\n\t\t * 2. If you are here and if the device->name is different\n\t\t *    from 'path' that means either\n\t\t *      a. The same device disappeared and reappeared with\n\t\t *         different name. or\n\t\t *      b. The missing-disk-which-was-replaced, has\n\t\t *         reappeared now.\n\t\t *\n\t\t * We must allow 1 and 2a above. But 2b would be a spurious\n\t\t * and unintentional.\n\t\t *\n\t\t * Further in case of 1 and 2a above, the disk at 'path'\n\t\t * would have missed some transaction when it was away and\n\t\t * in case of 2a the stale bdev has to be updated as well.\n\t\t * 2b must not be allowed at all time.\n\t\t */\n\n\t\t/*\n\t\t * For now, we do allow update to btrfs_fs_device through the\n\t\t * btrfs dev scan cli after FS has been mounted.  We're still\n\t\t * tracking a problem where systems fail mount by subvolume id\n\t\t * when we reject replacement on a mounted FS.\n\t\t */\n\t\tif (!fs_devices->opened && found_transid < device->generation) {\n\t\t\t/*\n\t\t\t * That is if the FS is _not_ mounted and if you\n\t\t\t * are here, that means there is more than one\n\t\t\t * disk with same uuid and devid.We keep the one\n\t\t\t * with larger generation number or the last-in if\n\t\t\t * generation are equal.\n\t\t\t */\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EEXIST);\n\t\t}\n\n\t\t/*\n\t\t * We are going to replace the device path for a given devid,\n\t\t * make sure it's the same device if the device is mounted\n\t\t */\n\t\tif (device->bdev) {\n\t\t\tstruct block_device *path_bdev;\n\n\t\t\tpath_bdev = lookup_bdev(path);\n\t\t\tif (IS_ERR(path_bdev)) {\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\treturn ERR_CAST(path_bdev);\n\t\t\t}\n\n\t\t\tif (device->bdev != path_bdev) {\n\t\t\t\tbdput(path_bdev);\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\tbtrfs_warn_in_rcu(device->fs_info,\n\t\t\t\"duplicate device fsid:devid for %pU:%llu old:%s new:%s\",\n\t\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\t\trcu_str_deref(device->name), path);\n\t\t\t\treturn ERR_PTR(-EEXIST);\n\t\t\t}\n\t\t\tbdput(path_bdev);\n\t\t\tbtrfs_info_in_rcu(device->fs_info,\n\t\t\t\t\"device fsid %pU devid %llu moved old:%s new:%s\",\n\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\trcu_str_deref(device->name), path);\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_string_free(device->name);\n\t\trcu_assign_pointer(device->name, name);\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\tfs_devices->missing_devices--;\n\t\t\tclear_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\t}\n\n\t/*\n\t * Unmount does not free the btrfs_device struct but would zero\n\t * generation along with most of the other members. So just update\n\t * it back. We need it to pick the disk with largest generation\n\t * (as above).\n\t */\n\tif (!fs_devices->opened) {\n\t\tdevice->generation = found_transid;\n\t\tfs_devices->latest_generation = max_t(u64, found_transid,\n\t\t\t\t\t\tfs_devices->latest_generation);\n\t}\n\n\tfs_devices->total_devices = btrfs_super_num_devices(disk_super);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\treturn device;\n}",
                        "code_after_change": "static void netvsc_get_ethtool_stats(struct net_device *dev,\n\t\t\t\t     struct ethtool_stats *stats, u64 *data)\n{\n\tstruct net_device_context *ndc = netdev_priv(dev);\n\tstruct netvsc_device *nvdev = rtnl_dereference(ndc->nvdev);\n\tconst void *nds = &ndc->eth_stats;\n\tconst struct netvsc_stats *qstats;\n\tstruct netvsc_vf_pcpu_stats sum;\n\tstruct netvsc_ethtool_pcpu_stats *pcpu_sum;\n\tunsigned int start;\n\tu64 packets, bytes;\n\tu64 xdp_drop;\n\tint i, j, cpu;\n\n\tif (!nvdev)\n\t\treturn;\n\n\tfor (i = 0; i < NETVSC_GLOBAL_STATS_LEN; i++)\n\t\tdata[i] = *(unsigned long *)(nds + netvsc_stats[i].offset);\n\n\tnetvsc_get_vf_stats(dev, &sum);\n\tfor (j = 0; j < NETVSC_VF_STATS_LEN; j++)\n\t\tdata[i++] = *(u64 *)((void *)&sum + vf_stats[j].offset);\n\n\tfor (j = 0; j < nvdev->num_chn; j++) {\n\t\tqstats = &nvdev->chan_table[j].tx_stats;\n\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin_irq(&qstats->syncp);\n\t\t\tpackets = qstats->packets;\n\t\t\tbytes = qstats->bytes;\n\t\t} while (u64_stats_fetch_retry_irq(&qstats->syncp, start));\n\t\tdata[i++] = packets;\n\t\tdata[i++] = bytes;\n\n\t\tqstats = &nvdev->chan_table[j].rx_stats;\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin_irq(&qstats->syncp);\n\t\t\tpackets = qstats->packets;\n\t\t\tbytes = qstats->bytes;\n\t\t\txdp_drop = qstats->xdp_drop;\n\t\t} while (u64_stats_fetch_retry_irq(&qstats->syncp, start));\n\t\tdata[i++] = packets;\n\t\tdata[i++] = bytes;\n\t\tdata[i++] = xdp_drop;\n\t}\n\n\tpcpu_sum = kvmalloc_array(num_possible_cpus(),\n\t\t\t\t  sizeof(struct netvsc_ethtool_pcpu_stats),\n\t\t\t\t  GFP_KERNEL);\n\tif (!pcpu_sum)\n\t\treturn;\n\n\tnetvsc_get_pcpu_stats(dev, pcpu_sum);\n\tfor_each_present_cpu(cpu) {\n\t\tstruct netvsc_ethtool_pcpu_stats *this_sum = &pcpu_sum[cpu];\n\n\t\tfor (j = 0; j < ARRAY_SIZE(pcpu_stats); j++)\n\t\t\tdata[i++] = *(u64 *)((void *)this_sum\n\t\t\t\t\t     + pcpu_stats[j].offset);\n\t}\n\tkvfree(pcpu_sum);\n}",
                        "cve_id": "CVE-2019-18885"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 2221,
            "cve_id": "CVE-2019-18885",
            "code_snippet": "static int read_one_dev(struct btrfs_fs_info *fs_info,\n\t\t\tstruct extent_buffer *leaf,\n\t\t\tstruct btrfs_dev_item *dev_item)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_device *device;\n\tu64 devid;\n\tint ret;\n\tu8 fs_uuid[BTRFS_FSID_SIZE];\n\tu8 dev_uuid[BTRFS_UUID_SIZE];\n\n\tdevid = btrfs_device_id(leaf, dev_item);\n\tread_extent_buffer(leaf, dev_uuid, btrfs_device_uuid(dev_item),\n\t\t\t   BTRFS_UUID_SIZE);\n\tread_extent_buffer(leaf, fs_uuid, btrfs_device_fsid(dev_item),\n\t\t\t   BTRFS_FSID_SIZE);\n\n\tif (memcmp(fs_uuid, fs_devices->metadata_uuid, BTRFS_FSID_SIZE)) {\n\t\tfs_devices = open_seed_devices(fs_info, fs_uuid);\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn PTR_ERR(fs_devices);\n\t}\n\n\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t   fs_uuid);\n\tif (!device) {\n\t\tif (!btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tbtrfs_report_missing_device(fs_info, devid,\n\t\t\t\t\t\t\tdev_uuid, true);\n\t\t\treturn -ENOENT;\n\t\t}\n\n\t\tdevice = add_missing_dev(fs_devices, devid, dev_uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"failed to add missing dev %llu: %ld\",\n\t\t\t\tdevid, PTR_ERR(device));\n\t\t\treturn PTR_ERR(device);\n\t\t}\n\t\tbtrfs_report_missing_device(fs_info, devid, dev_uuid, false);\n\t} else {\n\t\tif (!device->bdev) {\n\t\t\tif (!btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\t\tbtrfs_report_missing_device(fs_info,\n\t\t\t\t\t\tdevid, dev_uuid, true);\n\t\t\t\treturn -ENOENT;\n\t\t\t}\n\t\t\tbtrfs_report_missing_device(fs_info, devid,\n\t\t\t\t\t\t\tdev_uuid, false);\n\t\t}\n\n\t\tif (!device->bdev &&\n\t\t    !test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\t/*\n\t\t\t * this happens when a device that was properly setup\n\t\t\t * in the device info lists suddenly goes bad.\n\t\t\t * device->bdev is NULL, and so we have to set\n\t\t\t * device->missing to one here\n\t\t\t */\n\t\t\tdevice->fs_devices->missing_devices++;\n\t\t\tset_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\n\t\t/* Move the device to its own fs_devices */\n\t\tif (device->fs_devices != fs_devices) {\n\t\t\tASSERT(test_bit(BTRFS_DEV_STATE_MISSING,\n\t\t\t\t\t\t\t&device->dev_state));\n\n\t\t\tlist_move(&device->dev_list, &fs_devices->devices);\n\t\t\tdevice->fs_devices->num_devices--;\n\t\t\tfs_devices->num_devices++;\n\n\t\t\tdevice->fs_devices->missing_devices--;\n\t\t\tfs_devices->missing_devices++;\n\n\t\t\tdevice->fs_devices = fs_devices;\n\t\t}\n\t}\n\n\tif (device->fs_devices != fs_info->fs_devices) {\n\t\tBUG_ON(test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state));\n\t\tif (device->generation !=\n\t\t    btrfs_device_generation(leaf, dev_item))\n\t\t\treturn -EINVAL;\n\t}\n\n\tfill_device_from_item(leaf, dev_item, device);\n\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t   !test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tdevice->fs_devices->total_rw_bytes += device->total_bytes;\n\t\tatomic64_add(device->total_bytes - device->bytes_used,\n\t\t\t\t&fs_info->free_chunk_space);\n\t}\n\tret = 0;\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int btrfs_finish_sprout(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_dev_item *dev_item;\n\tstruct btrfs_device *device;\n\tstruct btrfs_key key;\n\tu8 fs_uuid[BTRFS_FSID_SIZE];\n\tu8 dev_uuid[BTRFS_UUID_SIZE];\n\tu64 devid;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.offset = 0;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\n\twhile (1) {\n\t\tret = btrfs_search_slot(trans, root, &key, path, 0, 1);\n\t\tif (ret < 0)\n\t\t\tgoto error;\n\n\t\tleaf = path->nodes[0];\nnext_slot:\n\t\tif (path->slots[0] >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret > 0)\n\t\t\t\tbreak;\n\t\t\tif (ret < 0)\n\t\t\t\tgoto error;\n\t\t\tleaf = path->nodes[0];\n\t\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\t\tbtrfs_release_path(path);\n\t\t\tcontinue;\n\t\t}\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\tif (key.objectid != BTRFS_DEV_ITEMS_OBJECTID ||\n\t\t    key.type != BTRFS_DEV_ITEM_KEY)\n\t\t\tbreak;\n\n\t\tdev_item = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\t  struct btrfs_dev_item);\n\t\tdevid = btrfs_device_id(leaf, dev_item);\n\t\tread_extent_buffer(leaf, dev_uuid, btrfs_device_uuid(dev_item),\n\t\t\t\t   BTRFS_UUID_SIZE);\n\t\tread_extent_buffer(leaf, fs_uuid, btrfs_device_fsid(dev_item),\n\t\t\t\t   BTRFS_FSID_SIZE);\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t\t   fs_uuid);\n\t\tBUG_ON(!device); /* Logic error */\n\n\t\tif (device->fs_devices->seeding) {\n\t\t\tbtrfs_set_device_generation(leaf, dev_item,\n\t\t\t\t\t\t    device->generation);\n\t\t\tbtrfs_mark_buffer_dirty(leaf);\n\t\t}\n\n\t\tpath->slots[0]++;\n\t\tgoto next_slot;\n\t}\n\tret = 0;\nerror:\n\tbtrfs_free_path(path);\n\treturn ret;\n}",
                        "code_after_change": "static int btrfs_finish_sprout(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_dev_item *dev_item;\n\tstruct btrfs_device *device;\n\tstruct btrfs_key key;\n\tu8 fs_uuid[BTRFS_FSID_SIZE];\n\tu8 dev_uuid[BTRFS_UUID_SIZE];\n\tu64 devid;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.offset = 0;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\n\twhile (1) {\n\t\tret = btrfs_search_slot(trans, root, &key, path, 0, 1);\n\t\tif (ret < 0)\n\t\t\tgoto error;\n\n\t\tleaf = path->nodes[0];\nnext_slot:\n\t\tif (path->slots[0] >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret > 0)\n\t\t\t\tbreak;\n\t\t\tif (ret < 0)\n\t\t\t\tgoto error;\n\t\t\tleaf = path->nodes[0];\n\t\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\t\tbtrfs_release_path(path);\n\t\t\tcontinue;\n\t\t}\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\tif (key.objectid != BTRFS_DEV_ITEMS_OBJECTID ||\n\t\t    key.type != BTRFS_DEV_ITEM_KEY)\n\t\t\tbreak;\n\n\t\tdev_item = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\t  struct btrfs_dev_item);\n\t\tdevid = btrfs_device_id(leaf, dev_item);\n\t\tread_extent_buffer(leaf, dev_uuid, btrfs_device_uuid(dev_item),\n\t\t\t\t   BTRFS_UUID_SIZE);\n\t\tread_extent_buffer(leaf, fs_uuid, btrfs_device_fsid(dev_item),\n\t\t\t\t   BTRFS_FSID_SIZE);\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t\t   fs_uuid, true);\n\t\tBUG_ON(!device); /* Logic error */\n\n\t\tif (device->fs_devices->seeding) {\n\t\t\tbtrfs_set_device_generation(leaf, dev_item,\n\t\t\t\t\t\t    device->generation);\n\t\t\tbtrfs_mark_buffer_dirty(leaf);\n\t\t}\n\n\t\tpath->slots[0]++;\n\t\tgoto next_slot;\n\t}\n\tret = 0;\nerror:\n\tbtrfs_free_path(path);\n\treturn ret;\n}",
                        "cve_id": "CVE-2019-18885"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2229,
            "cve_id": "CVE-2019-19036",
            "code_snippet": "static int check_leaf(struct extent_buffer *leaf, bool check_item_data)\n{\n\tstruct btrfs_fs_info *fs_info = leaf->fs_info;\n\t/* No valid key type is 0, so all key should be larger than this key */\n\tstruct btrfs_key prev_key = {0, 0, 0};\n\tstruct btrfs_key key;\n\tu32 nritems = btrfs_header_nritems(leaf);\n\tint slot;\n\n\tif (btrfs_header_level(leaf) != 0) {\n\t\tgeneric_err(leaf, 0,\n\t\t\t\"invalid level for leaf, have %d expect 0\",\n\t\t\tbtrfs_header_level(leaf));\n\t\treturn -EUCLEAN;\n\t}\n\n\t/*\n\t * Extent buffers from a relocation tree have a owner field that\n\t * corresponds to the subvolume tree they are based on. So just from an\n\t * extent buffer alone we can not find out what is the id of the\n\t * corresponding subvolume tree, so we can not figure out if the extent\n\t * buffer corresponds to the root of the relocation tree or not. So\n\t * skip this check for relocation trees.\n\t */\n\tif (nritems == 0 && !btrfs_header_flag(leaf, BTRFS_HEADER_FLAG_RELOC)) {\n\t\tu64 owner = btrfs_header_owner(leaf);\n\n\t\t/* These trees must never be empty */\n\t\tif (owner == BTRFS_ROOT_TREE_OBJECTID ||\n\t\t    owner == BTRFS_CHUNK_TREE_OBJECTID ||\n\t\t    owner == BTRFS_EXTENT_TREE_OBJECTID ||\n\t\t    owner == BTRFS_DEV_TREE_OBJECTID ||\n\t\t    owner == BTRFS_FS_TREE_OBJECTID ||\n\t\t    owner == BTRFS_DATA_RELOC_TREE_OBJECTID) {\n\t\t\tgeneric_err(leaf, 0,\n\t\t\t\"invalid root, root %llu must never be empty\",\n\t\t\t\t    owner);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (nritems == 0)\n\t\treturn 0;\n\n\t/*\n\t * Check the following things to make sure this is a good leaf, and\n\t * leaf users won't need to bother with similar sanity checks:\n\t *\n\t * 1) key ordering\n\t * 2) item offset and size\n\t *    No overlap, no hole, all inside the leaf.\n\t * 3) item content\n\t *    If possible, do comprehensive sanity check.\n\t *    NOTE: All checks must only rely on the item data itself.\n\t */\n\tfor (slot = 0; slot < nritems; slot++) {\n\t\tu32 item_end_expected;\n\t\tint ret;\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\n\t\t/* Make sure the keys are in the right order */\n\t\tif (btrfs_comp_cpu_keys(&prev_key, &key) >= 0) {\n\t\t\tgeneric_err(leaf, slot,\n\t\"bad key order, prev (%llu %u %llu) current (%llu %u %llu)\",\n\t\t\t\tprev_key.objectid, prev_key.type,\n\t\t\t\tprev_key.offset, key.objectid, key.type,\n\t\t\t\tkey.offset);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/*\n\t\t * Make sure the offset and ends are right, remember that the\n\t\t * item data starts at the end of the leaf and grows towards the\n\t\t * front.\n\t\t */\n\t\tif (slot == 0)\n\t\t\titem_end_expected = BTRFS_LEAF_DATA_SIZE(fs_info);\n\t\telse\n\t\t\titem_end_expected = btrfs_item_offset_nr(leaf,\n\t\t\t\t\t\t\t\t slot - 1);\n\t\tif (btrfs_item_end_nr(leaf, slot) != item_end_expected) {\n\t\t\tgeneric_err(leaf, slot,\n\t\t\t\t\"unexpected item end, have %u expect %u\",\n\t\t\t\tbtrfs_item_end_nr(leaf, slot),\n\t\t\t\titem_end_expected);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/*\n\t\t * Check to make sure that we don't point outside of the leaf,\n\t\t * just in case all the items are consistent to each other, but\n\t\t * all point outside of the leaf.\n\t\t */\n\t\tif (btrfs_item_end_nr(leaf, slot) >\n\t\t    BTRFS_LEAF_DATA_SIZE(fs_info)) {\n\t\t\tgeneric_err(leaf, slot,\n\t\t\t\"slot end outside of leaf, have %u expect range [0, %u]\",\n\t\t\t\tbtrfs_item_end_nr(leaf, slot),\n\t\t\t\tBTRFS_LEAF_DATA_SIZE(fs_info));\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/* Also check if the item pointer overlaps with btrfs item. */\n\t\tif (btrfs_item_nr_offset(slot) + sizeof(struct btrfs_item) >\n\t\t    btrfs_item_ptr_offset(leaf, slot)) {\n\t\t\tgeneric_err(leaf, slot,\n\t\t\"slot overlaps with its data, item end %lu data start %lu\",\n\t\t\t\tbtrfs_item_nr_offset(slot) +\n\t\t\t\tsizeof(struct btrfs_item),\n\t\t\t\tbtrfs_item_ptr_offset(leaf, slot));\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\tif (check_item_data) {\n\t\t\t/*\n\t\t\t * Check if the item size and content meet other\n\t\t\t * criteria\n\t\t\t */\n\t\t\tret = check_leaf_item(leaf, &key, slot, &prev_key);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tprev_key.objectid = key.objectid;\n\t\tprev_key.type = key.type;\n\t\tprev_key.offset = key.offset;\n\t}\n\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int check_leaf(struct btrfs_fs_info *fs_info, struct extent_buffer *leaf,\n\t\t      bool check_item_data)\n{\n\t/* No valid key type is 0, so all key should be larger than this key */\n\tstruct btrfs_key prev_key = {0, 0, 0};\n\tstruct btrfs_key key;\n\tu32 nritems = btrfs_header_nritems(leaf);\n\tint slot;\n\n\t/*\n\t * Extent buffers from a relocation tree have a owner field that\n\t * corresponds to the subvolume tree they are based on. So just from an\n\t * extent buffer alone we can not find out what is the id of the\n\t * corresponding subvolume tree, so we can not figure out if the extent\n\t * buffer corresponds to the root of the relocation tree or not. So\n\t * skip this check for relocation trees.\n\t */\n\tif (nritems == 0 && !btrfs_header_flag(leaf, BTRFS_HEADER_FLAG_RELOC)) {\n\t\tstruct btrfs_root *check_root;\n\n\t\tkey.objectid = btrfs_header_owner(leaf);\n\t\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\t\tkey.offset = (u64)-1;\n\n\t\tcheck_root = btrfs_get_fs_root(fs_info, &key, false);\n\t\t/*\n\t\t * The only reason we also check NULL here is that during\n\t\t * open_ctree() some roots has not yet been set up.\n\t\t */\n\t\tif (!IS_ERR_OR_NULL(check_root)) {\n\t\t\tstruct extent_buffer *eb;\n\n\t\t\teb = btrfs_root_node(check_root);\n\t\t\t/* if leaf is the root, then it's fine */\n\t\t\tif (leaf != eb) {\n\t\t\t\tgeneric_err(fs_info, leaf, 0,\n\t\t\"invalid nritems, have %u should not be 0 for non-root leaf\",\n\t\t\t\t\tnritems);\n\t\t\t\tfree_extent_buffer(eb);\n\t\t\t\treturn -EUCLEAN;\n\t\t\t}\n\t\t\tfree_extent_buffer(eb);\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (nritems == 0)\n\t\treturn 0;\n\n\t/*\n\t * Check the following things to make sure this is a good leaf, and\n\t * leaf users won't need to bother with similar sanity checks:\n\t *\n\t * 1) key ordering\n\t * 2) item offset and size\n\t *    No overlap, no hole, all inside the leaf.\n\t * 3) item content\n\t *    If possible, do comprehensive sanity check.\n\t *    NOTE: All checks must only rely on the item data itself.\n\t */\n\tfor (slot = 0; slot < nritems; slot++) {\n\t\tu32 item_end_expected;\n\t\tint ret;\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\n\t\t/* Make sure the keys are in the right order */\n\t\tif (btrfs_comp_cpu_keys(&prev_key, &key) >= 0) {\n\t\t\tgeneric_err(fs_info, leaf, slot,\n\t\"bad key order, prev (%llu %u %llu) current (%llu %u %llu)\",\n\t\t\t\tprev_key.objectid, prev_key.type,\n\t\t\t\tprev_key.offset, key.objectid, key.type,\n\t\t\t\tkey.offset);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/*\n\t\t * Make sure the offset and ends are right, remember that the\n\t\t * item data starts at the end of the leaf and grows towards the\n\t\t * front.\n\t\t */\n\t\tif (slot == 0)\n\t\t\titem_end_expected = BTRFS_LEAF_DATA_SIZE(fs_info);\n\t\telse\n\t\t\titem_end_expected = btrfs_item_offset_nr(leaf,\n\t\t\t\t\t\t\t\t slot - 1);\n\t\tif (btrfs_item_end_nr(leaf, slot) != item_end_expected) {\n\t\t\tgeneric_err(fs_info, leaf, slot,\n\t\t\t\t\"unexpected item end, have %u expect %u\",\n\t\t\t\tbtrfs_item_end_nr(leaf, slot),\n\t\t\t\titem_end_expected);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/*\n\t\t * Check to make sure that we don't point outside of the leaf,\n\t\t * just in case all the items are consistent to each other, but\n\t\t * all point outside of the leaf.\n\t\t */\n\t\tif (btrfs_item_end_nr(leaf, slot) >\n\t\t    BTRFS_LEAF_DATA_SIZE(fs_info)) {\n\t\t\tgeneric_err(fs_info, leaf, slot,\n\t\t\t\"slot end outside of leaf, have %u expect range [0, %u]\",\n\t\t\t\tbtrfs_item_end_nr(leaf, slot),\n\t\t\t\tBTRFS_LEAF_DATA_SIZE(fs_info));\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/* Also check if the item pointer overlaps with btrfs item. */\n\t\tif (btrfs_item_nr_offset(slot) + sizeof(struct btrfs_item) >\n\t\t    btrfs_item_ptr_offset(leaf, slot)) {\n\t\t\tgeneric_err(fs_info, leaf, slot,\n\t\t\"slot overlaps with its data, item end %lu data start %lu\",\n\t\t\t\tbtrfs_item_nr_offset(slot) +\n\t\t\t\tsizeof(struct btrfs_item),\n\t\t\t\tbtrfs_item_ptr_offset(leaf, slot));\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\tif (check_item_data) {\n\t\t\t/*\n\t\t\t * Check if the item size and content meet other\n\t\t\t * criteria\n\t\t\t */\n\t\t\tret = check_leaf_item(fs_info, leaf, &key, slot);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tprev_key.objectid = key.objectid;\n\t\tprev_key.type = key.type;\n\t\tprev_key.offset = key.offset;\n\t}\n\n\treturn 0;\n}",
                        "code_after_change": "static int check_leaf(struct btrfs_fs_info *fs_info, struct extent_buffer *leaf,\n\t\t      bool check_item_data)\n{\n\t/* No valid key type is 0, so all key should be larger than this key */\n\tstruct btrfs_key prev_key = {0, 0, 0};\n\tstruct btrfs_key key;\n\tu32 nritems = btrfs_header_nritems(leaf);\n\tint slot;\n\n\t/*\n\t * Extent buffers from a relocation tree have a owner field that\n\t * corresponds to the subvolume tree they are based on. So just from an\n\t * extent buffer alone we can not find out what is the id of the\n\t * corresponding subvolume tree, so we can not figure out if the extent\n\t * buffer corresponds to the root of the relocation tree or not. So\n\t * skip this check for relocation trees.\n\t */\n\tif (nritems == 0 && !btrfs_header_flag(leaf, BTRFS_HEADER_FLAG_RELOC)) {\n\t\tu64 owner = btrfs_header_owner(leaf);\n\t\tstruct btrfs_root *check_root;\n\n\t\t/* These trees must never be empty */\n\t\tif (owner == BTRFS_ROOT_TREE_OBJECTID ||\n\t\t    owner == BTRFS_CHUNK_TREE_OBJECTID ||\n\t\t    owner == BTRFS_EXTENT_TREE_OBJECTID ||\n\t\t    owner == BTRFS_DEV_TREE_OBJECTID ||\n\t\t    owner == BTRFS_FS_TREE_OBJECTID ||\n\t\t    owner == BTRFS_DATA_RELOC_TREE_OBJECTID) {\n\t\t\tgeneric_err(fs_info, leaf, 0,\n\t\t\t\"invalid root, root %llu must never be empty\",\n\t\t\t\t    owner);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\t\tkey.objectid = owner;\n\t\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\t\tkey.offset = (u64)-1;\n\n\t\tcheck_root = btrfs_get_fs_root(fs_info, &key, false);\n\t\t/*\n\t\t * The only reason we also check NULL here is that during\n\t\t * open_ctree() some roots has not yet been set up.\n\t\t */\n\t\tif (!IS_ERR_OR_NULL(check_root)) {\n\t\t\tstruct extent_buffer *eb;\n\n\t\t\teb = btrfs_root_node(check_root);\n\t\t\t/* if leaf is the root, then it's fine */\n\t\t\tif (leaf != eb) {\n\t\t\t\tgeneric_err(fs_info, leaf, 0,\n\t\t\"invalid nritems, have %u should not be 0 for non-root leaf\",\n\t\t\t\t\tnritems);\n\t\t\t\tfree_extent_buffer(eb);\n\t\t\t\treturn -EUCLEAN;\n\t\t\t}\n\t\t\tfree_extent_buffer(eb);\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (nritems == 0)\n\t\treturn 0;\n\n\t/*\n\t * Check the following things to make sure this is a good leaf, and\n\t * leaf users won't need to bother with similar sanity checks:\n\t *\n\t * 1) key ordering\n\t * 2) item offset and size\n\t *    No overlap, no hole, all inside the leaf.\n\t * 3) item content\n\t *    If possible, do comprehensive sanity check.\n\t *    NOTE: All checks must only rely on the item data itself.\n\t */\n\tfor (slot = 0; slot < nritems; slot++) {\n\t\tu32 item_end_expected;\n\t\tint ret;\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\n\t\t/* Make sure the keys are in the right order */\n\t\tif (btrfs_comp_cpu_keys(&prev_key, &key) >= 0) {\n\t\t\tgeneric_err(fs_info, leaf, slot,\n\t\"bad key order, prev (%llu %u %llu) current (%llu %u %llu)\",\n\t\t\t\tprev_key.objectid, prev_key.type,\n\t\t\t\tprev_key.offset, key.objectid, key.type,\n\t\t\t\tkey.offset);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/*\n\t\t * Make sure the offset and ends are right, remember that the\n\t\t * item data starts at the end of the leaf and grows towards the\n\t\t * front.\n\t\t */\n\t\tif (slot == 0)\n\t\t\titem_end_expected = BTRFS_LEAF_DATA_SIZE(fs_info);\n\t\telse\n\t\t\titem_end_expected = btrfs_item_offset_nr(leaf,\n\t\t\t\t\t\t\t\t slot - 1);\n\t\tif (btrfs_item_end_nr(leaf, slot) != item_end_expected) {\n\t\t\tgeneric_err(fs_info, leaf, slot,\n\t\t\t\t\"unexpected item end, have %u expect %u\",\n\t\t\t\tbtrfs_item_end_nr(leaf, slot),\n\t\t\t\titem_end_expected);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/*\n\t\t * Check to make sure that we don't point outside of the leaf,\n\t\t * just in case all the items are consistent to each other, but\n\t\t * all point outside of the leaf.\n\t\t */\n\t\tif (btrfs_item_end_nr(leaf, slot) >\n\t\t    BTRFS_LEAF_DATA_SIZE(fs_info)) {\n\t\t\tgeneric_err(fs_info, leaf, slot,\n\t\t\t\"slot end outside of leaf, have %u expect range [0, %u]\",\n\t\t\t\tbtrfs_item_end_nr(leaf, slot),\n\t\t\t\tBTRFS_LEAF_DATA_SIZE(fs_info));\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/* Also check if the item pointer overlaps with btrfs item. */\n\t\tif (btrfs_item_nr_offset(slot) + sizeof(struct btrfs_item) >\n\t\t    btrfs_item_ptr_offset(leaf, slot)) {\n\t\t\tgeneric_err(fs_info, leaf, slot,\n\t\t\"slot overlaps with its data, item end %lu data start %lu\",\n\t\t\t\tbtrfs_item_nr_offset(slot) +\n\t\t\t\tsizeof(struct btrfs_item),\n\t\t\t\tbtrfs_item_ptr_offset(leaf, slot));\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\tif (check_item_data) {\n\t\t\t/*\n\t\t\t * Check if the item size and content meet other\n\t\t\t * criteria\n\t\t\t */\n\t\t\tret = check_leaf_item(fs_info, leaf, &key, slot);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tprev_key.objectid = key.objectid;\n\t\tprev_key.type = key.type;\n\t\tprev_key.offset = key.offset;\n\t}\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2018-14612"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2286,
            "cve_id": "CVE-2019-19227",
            "code_snippet": "static int __init atalk_init(void)\n{\n\tint rc;\n\n\trc = proto_register(&ddp_proto, 0);\n\tif (rc)\n\t\tgoto out;\n\n\trc = sock_register(&atalk_family_ops);\n\tif (rc)\n\t\tgoto out_proto;\n\n\tddp_dl = register_snap_client(ddp_snap_id, atalk_rcv);\n\tif (!ddp_dl)\n\t\tprintk(atalk_err_snap);\n\n\tdev_add_pack(&ltalk_packet_type);\n\tdev_add_pack(&ppptalk_packet_type);\n\n\trc = register_netdevice_notifier(&ddp_notifier);\n\tif (rc)\n\t\tgoto out_sock;\n\n\taarp_proto_init();\n\trc = atalk_proc_init();\n\tif (rc)\n\t\tgoto out_aarp;\n\n\trc = atalk_register_sysctl();\n\tif (rc)\n\t\tgoto out_proc;\nout:\n\treturn rc;\nout_proc:\n\tatalk_proc_exit();\nout_aarp:\n\taarp_cleanup_module();\n\tunregister_netdevice_notifier(&ddp_notifier);\nout_sock:\n\tdev_remove_pack(&ppptalk_packet_type);\n\tdev_remove_pack(&ltalk_packet_type);\n\tunregister_snap_client(ddp_dl);\n\tsock_unregister(PF_APPLETALK);\nout_proto:\n\tproto_unregister(&ddp_proto);\n\tgoto out;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int do_tls_getsockopt_conf(struct sock *sk, char __user *optval,\n\t\t\t\t  int __user *optlen, int tx)\n{\n\tint rc = 0;\n\tstruct tls_context *ctx = tls_get_ctx(sk);\n\tstruct tls_crypto_info *crypto_info;\n\tstruct cipher_context *cctx;\n\tint len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tif (!optval || (len < sizeof(*crypto_info))) {\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (!ctx) {\n\t\trc = -EBUSY;\n\t\tgoto out;\n\t}\n\n\t/* get user crypto info */\n\tif (tx) {\n\t\tcrypto_info = &ctx->crypto_send.info;\n\t\tcctx = &ctx->tx;\n\t} else {\n\t\tcrypto_info = &ctx->crypto_recv.info;\n\t\tcctx = &ctx->rx;\n\t}\n\n\tif (!TLS_CRYPTO_INFO_READY(crypto_info)) {\n\t\trc = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tif (len == sizeof(*crypto_info)) {\n\t\tif (copy_to_user(optval, crypto_info, sizeof(*crypto_info)))\n\t\t\trc = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tswitch (crypto_info->cipher_type) {\n\tcase TLS_CIPHER_AES_GCM_128: {\n\t\tstruct tls12_crypto_info_aes_gcm_128 *\n\t\t  crypto_info_aes_gcm_128 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aes_gcm_128,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aes_gcm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(crypto_info_aes_gcm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_GCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_GCM_128_IV_SIZE);\n\t\tmemcpy(crypto_info_aes_gcm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_GCM_128_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aes_gcm_128,\n\t\t\t\t sizeof(*crypto_info_aes_gcm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_AES_GCM_256: {\n\t\tstruct tls12_crypto_info_aes_gcm_256 *\n\t\t  crypto_info_aes_gcm_256 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aes_gcm_256,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aes_gcm_256)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(crypto_info_aes_gcm_256->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_GCM_256_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_GCM_256_IV_SIZE);\n\t\tmemcpy(crypto_info_aes_gcm_256->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_GCM_256_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aes_gcm_256,\n\t\t\t\t sizeof(*crypto_info_aes_gcm_256)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_AES_CCM_128: {\n\t\tstruct tls12_crypto_info_aes_ccm_128 *aes_ccm_128 =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_aes_ccm_128, info);\n\n\t\tif (len != sizeof(*aes_ccm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(aes_ccm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_CCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_CCM_128_IV_SIZE);\n\t\tmemcpy(aes_ccm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_CCM_128_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval, aes_ccm_128, sizeof(*aes_ccm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_CHACHA20_POLY1305: {\n\t\tstruct tls12_crypto_info_chacha20_poly1305 *chacha20_poly1305 =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_chacha20_poly1305,\n\t\t\t\tinfo);\n\n\t\tif (len != sizeof(*chacha20_poly1305)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(chacha20_poly1305->iv,\n\t\t       cctx->iv + TLS_CIPHER_CHACHA20_POLY1305_SALT_SIZE,\n\t\t       TLS_CIPHER_CHACHA20_POLY1305_IV_SIZE);\n\t\tmemcpy(chacha20_poly1305->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_CHACHA20_POLY1305_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval, chacha20_poly1305,\n\t\t\t\tsizeof(*chacha20_poly1305)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_SM4_GCM: {\n\t\tstruct tls12_crypto_info_sm4_gcm *sm4_gcm_info =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_sm4_gcm, info);\n\n\t\tif (len != sizeof(*sm4_gcm_info)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(sm4_gcm_info->iv,\n\t\t       cctx->iv + TLS_CIPHER_SM4_GCM_SALT_SIZE,\n\t\t       TLS_CIPHER_SM4_GCM_IV_SIZE);\n\t\tmemcpy(sm4_gcm_info->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_SM4_GCM_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval, sm4_gcm_info, sizeof(*sm4_gcm_info)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_SM4_CCM: {\n\t\tstruct tls12_crypto_info_sm4_ccm *sm4_ccm_info =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_sm4_ccm, info);\n\n\t\tif (len != sizeof(*sm4_ccm_info)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(sm4_ccm_info->iv,\n\t\t       cctx->iv + TLS_CIPHER_SM4_CCM_SALT_SIZE,\n\t\t       TLS_CIPHER_SM4_CCM_IV_SIZE);\n\t\tmemcpy(sm4_ccm_info->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_SM4_CCM_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval, sm4_ccm_info, sizeof(*sm4_ccm_info)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_ARIA_GCM_128: {\n\t\tstruct tls12_crypto_info_aria_gcm_128 *\n\t\t  crypto_info_aria_gcm_128 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aria_gcm_128,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aria_gcm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(crypto_info_aria_gcm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_ARIA_GCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_ARIA_GCM_128_IV_SIZE);\n\t\tmemcpy(crypto_info_aria_gcm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_ARIA_GCM_128_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aria_gcm_128,\n\t\t\t\t sizeof(*crypto_info_aria_gcm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_ARIA_GCM_256: {\n\t\tstruct tls12_crypto_info_aria_gcm_256 *\n\t\t  crypto_info_aria_gcm_256 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aria_gcm_256,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aria_gcm_256)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(crypto_info_aria_gcm_256->iv,\n\t\t       cctx->iv + TLS_CIPHER_ARIA_GCM_256_SALT_SIZE,\n\t\t       TLS_CIPHER_ARIA_GCM_256_IV_SIZE);\n\t\tmemcpy(crypto_info_aria_gcm_256->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_ARIA_GCM_256_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aria_gcm_256,\n\t\t\t\t sizeof(*crypto_info_aria_gcm_256)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tdefault:\n\t\trc = -EINVAL;\n\t}\n\nout:\n\treturn rc;\n}",
                        "code_after_change": "int __init aarp_proto_init(void)\n{\n\tint rc;\n\n\taarp_dl = register_snap_client(aarp_snap_id, aarp_rcv);\n\tif (!aarp_dl) {\n\t\tprintk(KERN_CRIT \"Unable to register AARP with SNAP.\\n\");\n\t\treturn -ENOMEM;\n\t}\n\ttimer_setup(&aarp_timer, aarp_expire_timeout, 0);\n\taarp_timer.expires  = jiffies + sysctl_aarp_expiry_time;\n\tadd_timer(&aarp_timer);\n\trc = register_netdevice_notifier(&aarp_notifier);\n\tif (rc) {\n\t\tdel_timer_sync(&aarp_timer);\n\t\tunregister_snap_client(aarp_dl);\n\t}\n\treturn rc;\n}",
                        "cve_id": "CVE-2023-28466"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static ssize_t module_gzip_decompress(struct load_info *info,\n\t\t\t\t      const void *buf, size_t size)\n{\n\tstruct z_stream_s s = { 0 };\n\tsize_t new_size = 0;\n\tsize_t gzip_hdr_len;\n\tssize_t retval;\n\tint rc;\n\n\tgzip_hdr_len = module_gzip_header_len(buf, size);\n\tif (!gzip_hdr_len) {\n\t\tpr_err(\"not a gzip compressed module\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\ts.next_in = buf + gzip_hdr_len;\n\ts.avail_in = size - gzip_hdr_len;\n\n\ts.workspace = kmalloc(zlib_inflate_workspacesize(), GFP_KERNEL);\n\tif (!s.workspace)\n\t\treturn -ENOMEM;\n\n\trc = zlib_inflateInit2(&s, -MAX_WBITS);\n\tif (rc != Z_OK) {\n\t\tpr_err(\"failed to initialize decompressor: %d\\n\", rc);\n\t\tretval = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tdo {\n\t\tstruct page *page = module_get_next_page(info);\n\n\t\tif (!page) {\n\t\t\tretval = -ENOMEM;\n\t\t\tgoto out_inflate_end;\n\t\t}\n\n\t\ts.next_out = kmap_local_page(page);\n\t\ts.avail_out = PAGE_SIZE;\n\t\trc = zlib_inflate(&s, 0);\n\t\tkunmap_local(s.next_out);\n\n\t\tnew_size += PAGE_SIZE - s.avail_out;\n\t} while (rc == Z_OK);\n\n\tif (rc != Z_STREAM_END) {\n\t\tpr_err(\"decompression failed with status %d\\n\", rc);\n\t\tretval = -EINVAL;\n\t\tgoto out_inflate_end;\n\t}\n\n\tretval = new_size;\n\nout_inflate_end:\n\tzlib_inflateEnd(&s);\nout:\n\tkfree(s.workspace);\n\treturn retval;\n}",
                        "code_after_change": "static int do_tls_getsockopt_conf(struct sock *sk, char __user *optval,\n\t\t\t\t  int __user *optlen, int tx)\n{\n\tint rc = 0;\n\tstruct tls_context *ctx = tls_get_ctx(sk);\n\tstruct tls_crypto_info *crypto_info;\n\tstruct cipher_context *cctx;\n\tint len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tif (!optval || (len < sizeof(*crypto_info))) {\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (!ctx) {\n\t\trc = -EBUSY;\n\t\tgoto out;\n\t}\n\n\t/* get user crypto info */\n\tif (tx) {\n\t\tcrypto_info = &ctx->crypto_send.info;\n\t\tcctx = &ctx->tx;\n\t} else {\n\t\tcrypto_info = &ctx->crypto_recv.info;\n\t\tcctx = &ctx->rx;\n\t}\n\n\tif (!TLS_CRYPTO_INFO_READY(crypto_info)) {\n\t\trc = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tif (len == sizeof(*crypto_info)) {\n\t\tif (copy_to_user(optval, crypto_info, sizeof(*crypto_info)))\n\t\t\trc = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tswitch (crypto_info->cipher_type) {\n\tcase TLS_CIPHER_AES_GCM_128: {\n\t\tstruct tls12_crypto_info_aes_gcm_128 *\n\t\t  crypto_info_aes_gcm_128 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aes_gcm_128,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aes_gcm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(crypto_info_aes_gcm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_GCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_GCM_128_IV_SIZE);\n\t\tmemcpy(crypto_info_aes_gcm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_GCM_128_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aes_gcm_128,\n\t\t\t\t sizeof(*crypto_info_aes_gcm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_AES_GCM_256: {\n\t\tstruct tls12_crypto_info_aes_gcm_256 *\n\t\t  crypto_info_aes_gcm_256 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aes_gcm_256,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aes_gcm_256)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(crypto_info_aes_gcm_256->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_GCM_256_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_GCM_256_IV_SIZE);\n\t\tmemcpy(crypto_info_aes_gcm_256->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_GCM_256_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aes_gcm_256,\n\t\t\t\t sizeof(*crypto_info_aes_gcm_256)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_AES_CCM_128: {\n\t\tstruct tls12_crypto_info_aes_ccm_128 *aes_ccm_128 =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_aes_ccm_128, info);\n\n\t\tif (len != sizeof(*aes_ccm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(aes_ccm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_CCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_CCM_128_IV_SIZE);\n\t\tmemcpy(aes_ccm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_CCM_128_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval, aes_ccm_128, sizeof(*aes_ccm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_CHACHA20_POLY1305: {\n\t\tstruct tls12_crypto_info_chacha20_poly1305 *chacha20_poly1305 =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_chacha20_poly1305,\n\t\t\t\tinfo);\n\n\t\tif (len != sizeof(*chacha20_poly1305)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(chacha20_poly1305->iv,\n\t\t       cctx->iv + TLS_CIPHER_CHACHA20_POLY1305_SALT_SIZE,\n\t\t       TLS_CIPHER_CHACHA20_POLY1305_IV_SIZE);\n\t\tmemcpy(chacha20_poly1305->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_CHACHA20_POLY1305_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval, chacha20_poly1305,\n\t\t\t\tsizeof(*chacha20_poly1305)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_SM4_GCM: {\n\t\tstruct tls12_crypto_info_sm4_gcm *sm4_gcm_info =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_sm4_gcm, info);\n\n\t\tif (len != sizeof(*sm4_gcm_info)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(sm4_gcm_info->iv,\n\t\t       cctx->iv + TLS_CIPHER_SM4_GCM_SALT_SIZE,\n\t\t       TLS_CIPHER_SM4_GCM_IV_SIZE);\n\t\tmemcpy(sm4_gcm_info->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_SM4_GCM_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval, sm4_gcm_info, sizeof(*sm4_gcm_info)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_SM4_CCM: {\n\t\tstruct tls12_crypto_info_sm4_ccm *sm4_ccm_info =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_sm4_ccm, info);\n\n\t\tif (len != sizeof(*sm4_ccm_info)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(sm4_ccm_info->iv,\n\t\t       cctx->iv + TLS_CIPHER_SM4_CCM_SALT_SIZE,\n\t\t       TLS_CIPHER_SM4_CCM_IV_SIZE);\n\t\tmemcpy(sm4_ccm_info->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_SM4_CCM_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval, sm4_ccm_info, sizeof(*sm4_ccm_info)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_ARIA_GCM_128: {\n\t\tstruct tls12_crypto_info_aria_gcm_128 *\n\t\t  crypto_info_aria_gcm_128 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aria_gcm_128,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aria_gcm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(crypto_info_aria_gcm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_ARIA_GCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_ARIA_GCM_128_IV_SIZE);\n\t\tmemcpy(crypto_info_aria_gcm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_ARIA_GCM_128_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aria_gcm_128,\n\t\t\t\t sizeof(*crypto_info_aria_gcm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_ARIA_GCM_256: {\n\t\tstruct tls12_crypto_info_aria_gcm_256 *\n\t\t  crypto_info_aria_gcm_256 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aria_gcm_256,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aria_gcm_256)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(crypto_info_aria_gcm_256->iv,\n\t\t       cctx->iv + TLS_CIPHER_ARIA_GCM_256_SALT_SIZE,\n\t\t       TLS_CIPHER_ARIA_GCM_256_IV_SIZE);\n\t\tmemcpy(crypto_info_aria_gcm_256->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_ARIA_GCM_256_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aria_gcm_256,\n\t\t\t\t sizeof(*crypto_info_aria_gcm_256)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tdefault:\n\t\trc = -EINVAL;\n\t}\n\nout:\n\treturn rc;\n}",
                        "cve_id": "CVE-2023-22997"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int dccp_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tconst struct dccp_sock *dp = dccp_sk(sk);\n\tconst int flags = msg->msg_flags;\n\tconst int noblock = flags & MSG_DONTWAIT;\n\tstruct sk_buff *skb;\n\tint rc, size;\n\tlong timeo;\n\n\ttrace_dccp_probe(sk, len);\n\n\tif (len > dp->dccps_mss_cache)\n\t\treturn -EMSGSIZE;\n\n\tlock_sock(sk);\n\n\tif (dccp_qpolicy_full(sk)) {\n\t\trc = -EAGAIN;\n\t\tgoto out_release;\n\t}\n\n\ttimeo = sock_sndtimeo(sk, noblock);\n\n\t/*\n\t * We have to use sk_stream_wait_connect here to set sk_write_pending,\n\t * so that the trick in dccp_rcv_request_sent_state_process.\n\t */\n\t/* Wait for a connection to finish. */\n\tif ((1 << sk->sk_state) & ~(DCCPF_OPEN | DCCPF_PARTOPEN))\n\t\tif ((rc = sk_stream_wait_connect(sk, &timeo)) != 0)\n\t\t\tgoto out_release;\n\n\tsize = sk->sk_prot->max_header + len;\n\trelease_sock(sk);\n\tskb = sock_alloc_send_skb(sk, size, noblock, &rc);\n\tlock_sock(sk);\n\tif (skb == NULL)\n\t\tgoto out_release;\n\n\tskb_reserve(skb, sk->sk_prot->max_header);\n\trc = memcpy_from_msg(skb_put(skb, len), msg, len);\n\tif (rc != 0)\n\t\tgoto out_discard;\n\n\trc = dccp_msghdr_parse(msg, skb);\n\tif (rc != 0)\n\t\tgoto out_discard;\n\n\tdccp_qpolicy_push(sk, skb);\n\t/*\n\t * The xmit_timer is set if the TX CCID is rate-based and will expire\n\t * when congestion control permits to release further packets into the\n\t * network. Window-based CCIDs do not use this timer.\n\t */\n\tif (!timer_pending(&dp->dccps_xmit_timer))\n\t\tdccp_write_xmit(sk);\nout_release:\n\trelease_sock(sk);\n\treturn rc ? : len;\nout_discard:\n\tkfree_skb(skb);\n\tgoto out_release;\n}",
                        "code_after_change": "static ssize_t module_gzip_decompress(struct load_info *info,\n\t\t\t\t      const void *buf, size_t size)\n{\n\tstruct z_stream_s s = { 0 };\n\tsize_t new_size = 0;\n\tsize_t gzip_hdr_len;\n\tssize_t retval;\n\tint rc;\n\n\tgzip_hdr_len = module_gzip_header_len(buf, size);\n\tif (!gzip_hdr_len) {\n\t\tpr_err(\"not a gzip compressed module\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\ts.next_in = buf + gzip_hdr_len;\n\ts.avail_in = size - gzip_hdr_len;\n\n\ts.workspace = kmalloc(zlib_inflate_workspacesize(), GFP_KERNEL);\n\tif (!s.workspace)\n\t\treturn -ENOMEM;\n\n\trc = zlib_inflateInit2(&s, -MAX_WBITS);\n\tif (rc != Z_OK) {\n\t\tpr_err(\"failed to initialize decompressor: %d\\n\", rc);\n\t\tretval = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tdo {\n\t\tstruct page *page = module_get_next_page(info);\n\n\t\tif (IS_ERR(page)) {\n\t\t\tretval = PTR_ERR(page);\n\t\t\tgoto out_inflate_end;\n\t\t}\n\n\t\ts.next_out = kmap_local_page(page);\n\t\ts.avail_out = PAGE_SIZE;\n\t\trc = zlib_inflate(&s, 0);\n\t\tkunmap_local(s.next_out);\n\n\t\tnew_size += PAGE_SIZE - s.avail_out;\n\t} while (rc == Z_OK);\n\n\tif (rc != Z_STREAM_END) {\n\t\tpr_err(\"decompression failed with status %d\\n\", rc);\n\t\tretval = -EINVAL;\n\t\tgoto out_inflate_end;\n\t}\n\n\tretval = new_size;\n\nout_inflate_end:\n\tzlib_inflateEnd(&s);\nout:\n\tkfree(s.workspace);\n\treturn retval;\n}",
                        "cve_id": "CVE-2018-1130"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int\nsmb2_ioctl_query_info(const unsigned int xid,\n\t\t      struct cifs_tcon *tcon,\n\t\t      struct cifs_sb_info *cifs_sb,\n\t\t      __le16 *path, int is_dir,\n\t\t      unsigned long p)\n{\n\tstruct iqi_vars *vars;\n\tstruct smb_rqst *rqst;\n\tstruct kvec *rsp_iov;\n\tstruct cifs_ses *ses = tcon->ses;\n\tstruct TCP_Server_Info *server = cifs_pick_channel(ses);\n\tchar __user *arg = (char __user *)p;\n\tstruct smb_query_info qi;\n\tstruct smb_query_info __user *pqi;\n\tint rc = 0;\n\tint flags = CIFS_CP_CREATE_CLOSE_OP;\n\tstruct smb2_query_info_rsp *qi_rsp = NULL;\n\tstruct smb2_ioctl_rsp *io_rsp = NULL;\n\tvoid *buffer = NULL;\n\tint resp_buftype[3];\n\tstruct cifs_open_parms oparms;\n\tu8 oplock = SMB2_OPLOCK_LEVEL_NONE;\n\tstruct cifs_fid fid;\n\tunsigned int size[2];\n\tvoid *data[2];\n\tint create_options = is_dir ? CREATE_NOT_FILE : CREATE_NOT_DIR;\n\n\tvars = kzalloc(sizeof(*vars), GFP_ATOMIC);\n\tif (vars == NULL)\n\t\treturn -ENOMEM;\n\trqst = &vars->rqst[0];\n\trsp_iov = &vars->rsp_iov[0];\n\n\tresp_buftype[0] = resp_buftype[1] = resp_buftype[2] = CIFS_NO_BUFFER;\n\n\tif (copy_from_user(&qi, arg, sizeof(struct smb_query_info)))\n\t\tgoto e_fault;\n\n\tif (qi.output_buffer_length > 1024) {\n\t\tkfree(vars);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ses || !server) {\n\t\tkfree(vars);\n\t\treturn -EIO;\n\t}\n\n\tif (smb3_encryption_required(tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\tif (qi.output_buffer_length) {\n\t\tbuffer = memdup_user(arg + sizeof(struct smb_query_info), qi.output_buffer_length);\n\t\tif (IS_ERR(buffer)) {\n\t\t\tkfree(vars);\n\t\t\treturn PTR_ERR(buffer);\n\t\t}\n\t}\n\n\t/* Open */\n\trqst[0].rq_iov = &vars->open_iov[0];\n\trqst[0].rq_nvec = SMB2_CREATE_IOV_SIZE;\n\n\tmemset(&oparms, 0, sizeof(oparms));\n\toparms.tcon = tcon;\n\toparms.disposition = FILE_OPEN;\n\toparms.create_options = cifs_create_options(cifs_sb, create_options);\n\toparms.fid = &fid;\n\toparms.reconnect = false;\n\n\tif (qi.flags & PASSTHRU_FSCTL) {\n\t\tswitch (qi.info_type & FSCTL_DEVICE_ACCESS_MASK) {\n\t\tcase FSCTL_DEVICE_ACCESS_FILE_READ_WRITE_ACCESS:\n\t\t\toparms.desired_access = FILE_READ_DATA | FILE_WRITE_DATA | FILE_READ_ATTRIBUTES | SYNCHRONIZE;\n\t\t\tbreak;\n\t\tcase FSCTL_DEVICE_ACCESS_FILE_ANY_ACCESS:\n\t\t\toparms.desired_access = GENERIC_ALL;\n\t\t\tbreak;\n\t\tcase FSCTL_DEVICE_ACCESS_FILE_READ_ACCESS:\n\t\t\toparms.desired_access = GENERIC_READ;\n\t\t\tbreak;\n\t\tcase FSCTL_DEVICE_ACCESS_FILE_WRITE_ACCESS:\n\t\t\toparms.desired_access = GENERIC_WRITE;\n\t\t\tbreak;\n\t\t}\n\t} else if (qi.flags & PASSTHRU_SET_INFO) {\n\t\toparms.desired_access = GENERIC_WRITE;\n\t} else {\n\t\toparms.desired_access = FILE_READ_ATTRIBUTES | READ_CONTROL;\n\t}\n\n\trc = SMB2_open_init(tcon, server,\n\t\t\t    &rqst[0], &oplock, &oparms, path);\n\tif (rc)\n\t\tgoto iqinf_exit;\n\tsmb2_set_next_command(tcon, &rqst[0]);\n\n\t/* Query */\n\tif (qi.flags & PASSTHRU_FSCTL) {\n\t\t/* Can eventually relax perm check since server enforces too */\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\trc = -EPERM;\n\t\telse  {\n\t\t\trqst[1].rq_iov = &vars->io_iov[0];\n\t\t\trqst[1].rq_nvec = SMB2_IOCTL_IOV_SIZE;\n\n\t\t\trc = SMB2_ioctl_init(tcon, server,\n\t\t\t\t\t     &rqst[1],\n\t\t\t\t\t     COMPOUND_FID, COMPOUND_FID,\n\t\t\t\t\t     qi.info_type, true, buffer,\n\t\t\t\t\t     qi.output_buffer_length,\n\t\t\t\t\t     CIFSMaxBufSize -\n\t\t\t\t\t     MAX_SMB2_CREATE_RESPONSE_SIZE -\n\t\t\t\t\t     MAX_SMB2_CLOSE_RESPONSE_SIZE);\n\t\t}\n\t} else if (qi.flags == PASSTHRU_SET_INFO) {\n\t\t/* Can eventually relax perm check since server enforces too */\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\trc = -EPERM;\n\t\telse if (qi.output_buffer_length < 8)\n\t\t\trc = -EINVAL;\n\t\telse {\n\t\t\trqst[1].rq_iov = &vars->si_iov[0];\n\t\t\trqst[1].rq_nvec = 1;\n\n\t\t\t/* MS-FSCC 2.4.13 FileEndOfFileInformation */\n\t\t\tsize[0] = 8;\n\t\t\tdata[0] = buffer;\n\n\t\t\trc = SMB2_set_info_init(tcon, server,\n\t\t\t\t\t&rqst[1],\n\t\t\t\t\tCOMPOUND_FID, COMPOUND_FID,\n\t\t\t\t\tcurrent->tgid,\n\t\t\t\t\tFILE_END_OF_FILE_INFORMATION,\n\t\t\t\t\tSMB2_O_INFO_FILE, 0, data, size);\n\t\t}\n\t} else if (qi.flags == PASSTHRU_QUERY_INFO) {\n\t\trqst[1].rq_iov = &vars->qi_iov[0];\n\t\trqst[1].rq_nvec = 1;\n\n\t\trc = SMB2_query_info_init(tcon, server,\n\t\t\t\t  &rqst[1], COMPOUND_FID,\n\t\t\t\t  COMPOUND_FID, qi.file_info_class,\n\t\t\t\t  qi.info_type, qi.additional_information,\n\t\t\t\t  qi.input_buffer_length,\n\t\t\t\t  qi.output_buffer_length, buffer);\n\t} else { /* unknown flags */\n\t\tcifs_tcon_dbg(VFS, \"Invalid passthru query flags: 0x%x\\n\",\n\t\t\t      qi.flags);\n\t\trc = -EINVAL;\n\t}\n\n\tif (rc)\n\t\tgoto iqinf_exit;\n\tsmb2_set_next_command(tcon, &rqst[1]);\n\tsmb2_set_related(&rqst[1]);\n\n\t/* Close */\n\trqst[2].rq_iov = &vars->close_iov[0];\n\trqst[2].rq_nvec = 1;\n\n\trc = SMB2_close_init(tcon, server,\n\t\t\t     &rqst[2], COMPOUND_FID, COMPOUND_FID, false);\n\tif (rc)\n\t\tgoto iqinf_exit;\n\tsmb2_set_related(&rqst[2]);\n\n\trc = compound_send_recv(xid, ses, server,\n\t\t\t\tflags, 3, rqst,\n\t\t\t\tresp_buftype, rsp_iov);\n\tif (rc)\n\t\tgoto iqinf_exit;\n\n\t/* No need to bump num_remote_opens since handle immediately closed */\n\tif (qi.flags & PASSTHRU_FSCTL) {\n\t\tpqi = (struct smb_query_info __user *)arg;\n\t\tio_rsp = (struct smb2_ioctl_rsp *)rsp_iov[1].iov_base;\n\t\tif (le32_to_cpu(io_rsp->OutputCount) < qi.input_buffer_length)\n\t\t\tqi.input_buffer_length = le32_to_cpu(io_rsp->OutputCount);\n\t\tif (qi.input_buffer_length > 0 &&\n\t\t    le32_to_cpu(io_rsp->OutputOffset) + qi.input_buffer_length\n\t\t    > rsp_iov[1].iov_len)\n\t\t\tgoto e_fault;\n\n\t\tif (copy_to_user(&pqi->input_buffer_length,\n\t\t\t\t &qi.input_buffer_length,\n\t\t\t\t sizeof(qi.input_buffer_length)))\n\t\t\tgoto e_fault;\n\n\t\tif (copy_to_user((void __user *)pqi + sizeof(struct smb_query_info),\n\t\t\t\t (const void *)io_rsp + le32_to_cpu(io_rsp->OutputOffset),\n\t\t\t\t qi.input_buffer_length))\n\t\t\tgoto e_fault;\n\t} else {\n\t\tpqi = (struct smb_query_info __user *)arg;\n\t\tqi_rsp = (struct smb2_query_info_rsp *)rsp_iov[1].iov_base;\n\t\tif (le32_to_cpu(qi_rsp->OutputBufferLength) < qi.input_buffer_length)\n\t\t\tqi.input_buffer_length = le32_to_cpu(qi_rsp->OutputBufferLength);\n\t\tif (copy_to_user(&pqi->input_buffer_length,\n\t\t\t\t &qi.input_buffer_length,\n\t\t\t\t sizeof(qi.input_buffer_length)))\n\t\t\tgoto e_fault;\n\n\t\tif (copy_to_user(pqi + 1, qi_rsp->Buffer,\n\t\t\t\t qi.input_buffer_length))\n\t\t\tgoto e_fault;\n\t}\n\n iqinf_exit:\n\tcifs_small_buf_release(rqst[0].rq_iov[0].iov_base);\n\tcifs_small_buf_release(rqst[1].rq_iov[0].iov_base);\n\tcifs_small_buf_release(rqst[2].rq_iov[0].iov_base);\n\tfree_rsp_buf(resp_buftype[0], rsp_iov[0].iov_base);\n\tfree_rsp_buf(resp_buftype[1], rsp_iov[1].iov_base);\n\tfree_rsp_buf(resp_buftype[2], rsp_iov[2].iov_base);\n\tkfree(vars);\n\tkfree(buffer);\n\treturn rc;\n\ne_fault:\n\trc = -EFAULT;\n\tgoto iqinf_exit;\n}",
                        "code_after_change": "static int\nsmb2_ioctl_query_info(const unsigned int xid,\n\t\t      struct cifs_tcon *tcon,\n\t\t      struct cifs_sb_info *cifs_sb,\n\t\t      __le16 *path, int is_dir,\n\t\t      unsigned long p)\n{\n\tstruct iqi_vars *vars;\n\tstruct smb_rqst *rqst;\n\tstruct kvec *rsp_iov;\n\tstruct cifs_ses *ses = tcon->ses;\n\tstruct TCP_Server_Info *server = cifs_pick_channel(ses);\n\tchar __user *arg = (char __user *)p;\n\tstruct smb_query_info qi;\n\tstruct smb_query_info __user *pqi;\n\tint rc = 0;\n\tint flags = CIFS_CP_CREATE_CLOSE_OP;\n\tstruct smb2_query_info_rsp *qi_rsp = NULL;\n\tstruct smb2_ioctl_rsp *io_rsp = NULL;\n\tvoid *buffer = NULL;\n\tint resp_buftype[3];\n\tstruct cifs_open_parms oparms;\n\tu8 oplock = SMB2_OPLOCK_LEVEL_NONE;\n\tstruct cifs_fid fid;\n\tunsigned int size[2];\n\tvoid *data[2];\n\tint create_options = is_dir ? CREATE_NOT_FILE : CREATE_NOT_DIR;\n\tvoid (*free_req1_func)(struct smb_rqst *r);\n\n\tvars = kzalloc(sizeof(*vars), GFP_ATOMIC);\n\tif (vars == NULL)\n\t\treturn -ENOMEM;\n\trqst = &vars->rqst[0];\n\trsp_iov = &vars->rsp_iov[0];\n\n\tresp_buftype[0] = resp_buftype[1] = resp_buftype[2] = CIFS_NO_BUFFER;\n\n\tif (copy_from_user(&qi, arg, sizeof(struct smb_query_info))) {\n\t\trc = -EFAULT;\n\t\tgoto free_vars;\n\t}\n\tif (qi.output_buffer_length > 1024) {\n\t\trc = -EINVAL;\n\t\tgoto free_vars;\n\t}\n\n\tif (!ses || !server) {\n\t\trc = -EIO;\n\t\tgoto free_vars;\n\t}\n\n\tif (smb3_encryption_required(tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\tif (qi.output_buffer_length) {\n\t\tbuffer = memdup_user(arg + sizeof(struct smb_query_info), qi.output_buffer_length);\n\t\tif (IS_ERR(buffer)) {\n\t\t\trc = PTR_ERR(buffer);\n\t\t\tgoto free_vars;\n\t\t}\n\t}\n\n\t/* Open */\n\trqst[0].rq_iov = &vars->open_iov[0];\n\trqst[0].rq_nvec = SMB2_CREATE_IOV_SIZE;\n\n\tmemset(&oparms, 0, sizeof(oparms));\n\toparms.tcon = tcon;\n\toparms.disposition = FILE_OPEN;\n\toparms.create_options = cifs_create_options(cifs_sb, create_options);\n\toparms.fid = &fid;\n\toparms.reconnect = false;\n\n\tif (qi.flags & PASSTHRU_FSCTL) {\n\t\tswitch (qi.info_type & FSCTL_DEVICE_ACCESS_MASK) {\n\t\tcase FSCTL_DEVICE_ACCESS_FILE_READ_WRITE_ACCESS:\n\t\t\toparms.desired_access = FILE_READ_DATA | FILE_WRITE_DATA | FILE_READ_ATTRIBUTES | SYNCHRONIZE;\n\t\t\tbreak;\n\t\tcase FSCTL_DEVICE_ACCESS_FILE_ANY_ACCESS:\n\t\t\toparms.desired_access = GENERIC_ALL;\n\t\t\tbreak;\n\t\tcase FSCTL_DEVICE_ACCESS_FILE_READ_ACCESS:\n\t\t\toparms.desired_access = GENERIC_READ;\n\t\t\tbreak;\n\t\tcase FSCTL_DEVICE_ACCESS_FILE_WRITE_ACCESS:\n\t\t\toparms.desired_access = GENERIC_WRITE;\n\t\t\tbreak;\n\t\t}\n\t} else if (qi.flags & PASSTHRU_SET_INFO) {\n\t\toparms.desired_access = GENERIC_WRITE;\n\t} else {\n\t\toparms.desired_access = FILE_READ_ATTRIBUTES | READ_CONTROL;\n\t}\n\n\trc = SMB2_open_init(tcon, server,\n\t\t\t    &rqst[0], &oplock, &oparms, path);\n\tif (rc)\n\t\tgoto free_output_buffer;\n\tsmb2_set_next_command(tcon, &rqst[0]);\n\n\t/* Query */\n\tif (qi.flags & PASSTHRU_FSCTL) {\n\t\t/* Can eventually relax perm check since server enforces too */\n\t\tif (!capable(CAP_SYS_ADMIN)) {\n\t\t\trc = -EPERM;\n\t\t\tgoto free_open_req;\n\t\t}\n\t\trqst[1].rq_iov = &vars->io_iov[0];\n\t\trqst[1].rq_nvec = SMB2_IOCTL_IOV_SIZE;\n\n\t\trc = SMB2_ioctl_init(tcon, server, &rqst[1], COMPOUND_FID, COMPOUND_FID,\n\t\t\t\t     qi.info_type, true, buffer, qi.output_buffer_length,\n\t\t\t\t     CIFSMaxBufSize - MAX_SMB2_CREATE_RESPONSE_SIZE -\n\t\t\t\t     MAX_SMB2_CLOSE_RESPONSE_SIZE);\n\t\tfree_req1_func = SMB2_ioctl_free;\n\t} else if (qi.flags == PASSTHRU_SET_INFO) {\n\t\t/* Can eventually relax perm check since server enforces too */\n\t\tif (!capable(CAP_SYS_ADMIN)) {\n\t\t\trc = -EPERM;\n\t\t\tgoto free_open_req;\n\t\t}\n\t\tif (qi.output_buffer_length < 8) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto free_open_req;\n\t\t}\n\t\trqst[1].rq_iov = &vars->si_iov[0];\n\t\trqst[1].rq_nvec = 1;\n\n\t\t/* MS-FSCC 2.4.13 FileEndOfFileInformation */\n\t\tsize[0] = 8;\n\t\tdata[0] = buffer;\n\n\t\trc = SMB2_set_info_init(tcon, server, &rqst[1], COMPOUND_FID, COMPOUND_FID,\n\t\t\t\t\tcurrent->tgid, FILE_END_OF_FILE_INFORMATION,\n\t\t\t\t\tSMB2_O_INFO_FILE, 0, data, size);\n\t\tfree_req1_func = SMB2_set_info_free;\n\t} else if (qi.flags == PASSTHRU_QUERY_INFO) {\n\t\trqst[1].rq_iov = &vars->qi_iov[0];\n\t\trqst[1].rq_nvec = 1;\n\n\t\trc = SMB2_query_info_init(tcon, server,\n\t\t\t\t  &rqst[1], COMPOUND_FID,\n\t\t\t\t  COMPOUND_FID, qi.file_info_class,\n\t\t\t\t  qi.info_type, qi.additional_information,\n\t\t\t\t  qi.input_buffer_length,\n\t\t\t\t  qi.output_buffer_length, buffer);\n\t\tfree_req1_func = SMB2_query_info_free;\n\t} else { /* unknown flags */\n\t\tcifs_tcon_dbg(VFS, \"Invalid passthru query flags: 0x%x\\n\",\n\t\t\t      qi.flags);\n\t\trc = -EINVAL;\n\t}\n\n\tif (rc)\n\t\tgoto free_open_req;\n\tsmb2_set_next_command(tcon, &rqst[1]);\n\tsmb2_set_related(&rqst[1]);\n\n\t/* Close */\n\trqst[2].rq_iov = &vars->close_iov[0];\n\trqst[2].rq_nvec = 1;\n\n\trc = SMB2_close_init(tcon, server,\n\t\t\t     &rqst[2], COMPOUND_FID, COMPOUND_FID, false);\n\tif (rc)\n\t\tgoto free_req_1;\n\tsmb2_set_related(&rqst[2]);\n\n\trc = compound_send_recv(xid, ses, server,\n\t\t\t\tflags, 3, rqst,\n\t\t\t\tresp_buftype, rsp_iov);\n\tif (rc)\n\t\tgoto out;\n\n\t/* No need to bump num_remote_opens since handle immediately closed */\n\tif (qi.flags & PASSTHRU_FSCTL) {\n\t\tpqi = (struct smb_query_info __user *)arg;\n\t\tio_rsp = (struct smb2_ioctl_rsp *)rsp_iov[1].iov_base;\n\t\tif (le32_to_cpu(io_rsp->OutputCount) < qi.input_buffer_length)\n\t\t\tqi.input_buffer_length = le32_to_cpu(io_rsp->OutputCount);\n\t\tif (qi.input_buffer_length > 0 &&\n\t\t    le32_to_cpu(io_rsp->OutputOffset) + qi.input_buffer_length\n\t\t    > rsp_iov[1].iov_len) {\n\t\t\trc = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (copy_to_user(&pqi->input_buffer_length,\n\t\t\t\t &qi.input_buffer_length,\n\t\t\t\t sizeof(qi.input_buffer_length))) {\n\t\t\trc = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (copy_to_user((void __user *)pqi + sizeof(struct smb_query_info),\n\t\t\t\t (const void *)io_rsp + le32_to_cpu(io_rsp->OutputOffset),\n\t\t\t\t qi.input_buffer_length))\n\t\t\trc = -EFAULT;\n\t} else {\n\t\tpqi = (struct smb_query_info __user *)arg;\n\t\tqi_rsp = (struct smb2_query_info_rsp *)rsp_iov[1].iov_base;\n\t\tif (le32_to_cpu(qi_rsp->OutputBufferLength) < qi.input_buffer_length)\n\t\t\tqi.input_buffer_length = le32_to_cpu(qi_rsp->OutputBufferLength);\n\t\tif (copy_to_user(&pqi->input_buffer_length,\n\t\t\t\t &qi.input_buffer_length,\n\t\t\t\t sizeof(qi.input_buffer_length))) {\n\t\t\trc = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (copy_to_user(pqi + 1, qi_rsp->Buffer,\n\t\t\t\t qi.input_buffer_length))\n\t\t\trc = -EFAULT;\n\t}\n\nout:\n\tfree_rsp_buf(resp_buftype[0], rsp_iov[0].iov_base);\n\tfree_rsp_buf(resp_buftype[1], rsp_iov[1].iov_base);\n\tfree_rsp_buf(resp_buftype[2], rsp_iov[2].iov_base);\n\tSMB2_close_free(&rqst[2]);\nfree_req_1:\n\tfree_req1_func(&rqst[1]);\nfree_open_req:\n\tSMB2_open_free(&rqst[0]);\nfree_output_buffer:\n\tkfree(buffer);\nfree_vars:\n\tkfree(vars);\n\treturn rc;\n}",
                        "cve_id": "CVE-2022-0168"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int smb2_handle_negotiate(struct ksmbd_work *work)\n{\n\tstruct ksmbd_conn *conn = work->conn;\n\tstruct smb2_negotiate_req *req = smb2_get_msg(work->request_buf);\n\tstruct smb2_negotiate_rsp *rsp = smb2_get_msg(work->response_buf);\n\tint rc = 0;\n\tunsigned int smb2_buf_len, smb2_neg_size;\n\t__le32 status;\n\n\tksmbd_debug(SMB, \"Received negotiate request\\n\");\n\tconn->need_neg = false;\n\tif (ksmbd_conn_good(work)) {\n\t\tpr_err(\"conn->tcp_status is already in CifsGood State\\n\");\n\t\twork->send_no_response = 1;\n\t\treturn rc;\n\t}\n\n\tif (req->DialectCount == 0) {\n\t\tpr_err(\"malformed packet\\n\");\n\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tsmb2_buf_len = get_rfc1002_len(work->request_buf);\n\tsmb2_neg_size = offsetof(struct smb2_negotiate_req, Dialects);\n\tif (smb2_neg_size > smb2_buf_len) {\n\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tif (conn->dialect == SMB311_PROT_ID) {\n\t\tunsigned int nego_ctxt_off = le32_to_cpu(req->NegotiateContextOffset);\n\n\t\tif (smb2_buf_len < nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (smb2_neg_size > nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (smb2_neg_size + le16_to_cpu(req->DialectCount) * sizeof(__le16) >\n\t\t    nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t} else {\n\t\tif (smb2_neg_size + le16_to_cpu(req->DialectCount) * sizeof(__le16) >\n\t\t    smb2_buf_len) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\tconn->cli_cap = le32_to_cpu(req->Capabilities);\n\tswitch (conn->dialect) {\n\tcase SMB311_PROT_ID:\n\t\tconn->preauth_info =\n\t\t\tkzalloc(sizeof(struct preauth_integrity_info),\n\t\t\t\tGFP_KERNEL);\n\t\tif (!conn->preauth_info) {\n\t\t\trc = -ENOMEM;\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tstatus = deassemble_neg_contexts(conn, req,\n\t\t\t\t\t\t get_rfc1002_len(work->request_buf));\n\t\tif (status != STATUS_SUCCESS) {\n\t\t\tpr_err(\"deassemble_neg_contexts error(0x%x)\\n\",\n\t\t\t       status);\n\t\t\trsp->hdr.Status = status;\n\t\t\trc = -EINVAL;\n\t\t\tkfree(conn->preauth_info);\n\t\t\tconn->preauth_info = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\trc = init_smb3_11_server(conn);\n\t\tif (rc < 0) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\tkfree(conn->preauth_info);\n\t\t\tconn->preauth_info = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tksmbd_gen_preauth_integrity_hash(conn,\n\t\t\t\t\t\t work->request_buf,\n\t\t\t\t\t\t conn->preauth_info->Preauth_HashValue);\n\t\trsp->NegotiateContextOffset =\n\t\t\t\tcpu_to_le32(OFFSET_OF_NEG_CONTEXT);\n\t\tassemble_neg_contexts(conn, rsp, work->response_buf);\n\t\tbreak;\n\tcase SMB302_PROT_ID:\n\t\tinit_smb3_02_server(conn);\n\t\tbreak;\n\tcase SMB30_PROT_ID:\n\t\tinit_smb3_0_server(conn);\n\t\tbreak;\n\tcase SMB21_PROT_ID:\n\t\tinit_smb2_1_server(conn);\n\t\tbreak;\n\tcase SMB2X_PROT_ID:\n\tcase BAD_PROT_ID:\n\tdefault:\n\t\tksmbd_debug(SMB, \"Server dialect :0x%x not supported\\n\",\n\t\t\t    conn->dialect);\n\t\trsp->hdr.Status = STATUS_NOT_SUPPORTED;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\n\t/* For stats */\n\tconn->connection_type = conn->dialect;\n\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\tmemcpy(conn->ClientGUID, req->ClientGUID,\n\t\t\tSMB2_CLIENT_GUID_SIZE);\n\tconn->cli_sec_mode = le16_to_cpu(req->SecurityMode);\n\n\trsp->StructureSize = cpu_to_le16(65);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying server\n\t */\n\tmemset(rsp->ServerGUID, 0, SMB2_CLIENT_GUID_SIZE);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\tksmbd_debug(SMB, \"negotiate context offset %d, count %d\\n\",\n\t\t    le32_to_cpu(rsp->NegotiateContextOffset),\n\t\t    le16_to_cpu(rsp->NegotiateContextCount));\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\t\t\t  le16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf, sizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tconn->use_spnego = true;\n\n\tif ((server_conf.signing == KSMBD_CONFIG_OPT_AUTO ||\n\t     server_conf.signing == KSMBD_CONFIG_OPT_DISABLED) &&\n\t    req->SecurityMode & SMB2_NEGOTIATE_SIGNING_REQUIRED_LE)\n\t\tconn->sign = true;\n\telse if (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY) {\n\t\tserver_conf.enforced_signing = true;\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\t\tconn->sign = true;\n\t}\n\n\tconn->srv_sec_mode = le16_to_cpu(rsp->SecurityMode);\n\tksmbd_conn_set_need_negotiate(work);\n\nerr_out:\n\tif (rc < 0)\n\t\tsmb2_set_err_rsp(work);\n\n\treturn rc;\n}",
                        "code_after_change": "int dccp_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tconst struct dccp_sock *dp = dccp_sk(sk);\n\tconst int flags = msg->msg_flags;\n\tconst int noblock = flags & MSG_DONTWAIT;\n\tstruct sk_buff *skb;\n\tint rc, size;\n\tlong timeo;\n\n\ttrace_dccp_probe(sk, len);\n\n\tif (len > dp->dccps_mss_cache)\n\t\treturn -EMSGSIZE;\n\n\tlock_sock(sk);\n\n\tif (dccp_qpolicy_full(sk)) {\n\t\trc = -EAGAIN;\n\t\tgoto out_release;\n\t}\n\n\ttimeo = sock_sndtimeo(sk, noblock);\n\n\t/*\n\t * We have to use sk_stream_wait_connect here to set sk_write_pending,\n\t * so that the trick in dccp_rcv_request_sent_state_process.\n\t */\n\t/* Wait for a connection to finish. */\n\tif ((1 << sk->sk_state) & ~(DCCPF_OPEN | DCCPF_PARTOPEN))\n\t\tif ((rc = sk_stream_wait_connect(sk, &timeo)) != 0)\n\t\t\tgoto out_release;\n\n\tsize = sk->sk_prot->max_header + len;\n\trelease_sock(sk);\n\tskb = sock_alloc_send_skb(sk, size, noblock, &rc);\n\tlock_sock(sk);\n\tif (skb == NULL)\n\t\tgoto out_release;\n\n\tif (sk->sk_state == DCCP_CLOSED) {\n\t\trc = -ENOTCONN;\n\t\tgoto out_discard;\n\t}\n\n\tskb_reserve(skb, sk->sk_prot->max_header);\n\trc = memcpy_from_msg(skb_put(skb, len), msg, len);\n\tif (rc != 0)\n\t\tgoto out_discard;\n\n\trc = dccp_msghdr_parse(msg, skb);\n\tif (rc != 0)\n\t\tgoto out_discard;\n\n\tdccp_qpolicy_push(sk, skb);\n\t/*\n\t * The xmit_timer is set if the TX CCID is rate-based and will expire\n\t * when congestion control permits to release further packets into the\n\t * network. Window-based CCIDs do not use this timer.\n\t */\n\tif (!timer_pending(&dp->dccps_xmit_timer))\n\t\tdccp_write_xmit(sk);\nout_release:\n\trelease_sock(sk);\n\treturn rc ? : len;\nout_discard:\n\tkfree_skb(skb);\n\tgoto out_release;\n}",
                        "cve_id": "CVE-2023-32252"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 2357,
            "cve_id": "CVE-2019-19815",
            "code_snippet": "static int f2fs_read_single_page(struct inode *inode, struct page *page,\n\t\t\t\t\tunsigned nr_pages,\n\t\t\t\t\tstruct f2fs_map_blocks *map,\n\t\t\t\t\tstruct bio **bio_ret,\n\t\t\t\t\tsector_t *last_block_in_bio,\n\t\t\t\t\tbool is_readahead)\n{\n\tstruct bio *bio = *bio_ret;\n\tconst unsigned blkbits = inode->i_blkbits;\n\tconst unsigned blocksize = 1 << blkbits;\n\tsector_t block_in_file;\n\tsector_t last_block;\n\tsector_t last_block_in_file;\n\tsector_t block_nr;\n\tint ret = 0;\n\n\tblock_in_file = (sector_t)page->index;\n\tlast_block = block_in_file + nr_pages;\n\tlast_block_in_file = (i_size_read(inode) + blocksize - 1) >>\n\t\t\t\t\t\t\tblkbits;\n\tif (last_block > last_block_in_file)\n\t\tlast_block = last_block_in_file;\n\n\t/* just zeroing out page which is beyond EOF */\n\tif (block_in_file >= last_block)\n\t\tgoto zero_out;\n\t/*\n\t * Map blocks using the previous result first.\n\t */\n\tif ((map->m_flags & F2FS_MAP_MAPPED) &&\n\t\t\tblock_in_file > map->m_lblk &&\n\t\t\tblock_in_file < (map->m_lblk + map->m_len))\n\t\tgoto got_it;\n\n\t/*\n\t * Then do more f2fs_map_blocks() calls until we are\n\t * done with this page.\n\t */\n\tmap->m_lblk = block_in_file;\n\tmap->m_len = last_block - block_in_file;\n\n\tret = f2fs_map_blocks(inode, map, 0, F2FS_GET_BLOCK_DEFAULT);\n\tif (ret)\n\t\tgoto out;\ngot_it:\n\tif ((map->m_flags & F2FS_MAP_MAPPED)) {\n\t\tblock_nr = map->m_pblk + block_in_file - map->m_lblk;\n\t\tSetPageMappedToDisk(page);\n\n\t\tif (!PageUptodate(page) && !cleancache_get_page(page)) {\n\t\t\tSetPageUptodate(page);\n\t\t\tgoto confused;\n\t\t}\n\n\t\tif (!f2fs_is_valid_blkaddr(F2FS_I_SB(inode), block_nr,\n\t\t\t\t\t\tDATA_GENERIC_ENHANCE_READ)) {\n\t\t\tret = -EFSCORRUPTED;\n\t\t\tgoto out;\n\t\t}\n\t} else {\nzero_out:\n\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\tif (!PageUptodate(page))\n\t\t\tSetPageUptodate(page);\n\t\tunlock_page(page);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * This page will go to BIO.  Do we need to send this\n\t * BIO off first?\n\t */\n\tif (bio && (*last_block_in_bio != block_nr - 1 ||\n\t\t!__same_bdev(F2FS_I_SB(inode), block_nr, bio))) {\nsubmit_and_realloc:\n\t\t__submit_bio(F2FS_I_SB(inode), bio, DATA);\n\t\tbio = NULL;\n\t}\n\tif (bio == NULL) {\n\t\tbio = f2fs_grab_read_bio(inode, block_nr, nr_pages,\n\t\t\t\tis_readahead ? REQ_RAHEAD : 0);\n\t\tif (IS_ERR(bio)) {\n\t\t\tret = PTR_ERR(bio);\n\t\t\tbio = NULL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/*\n\t * If the page is under writeback, we need to wait for\n\t * its completion to see the correct decrypted data.\n\t */\n\tf2fs_wait_on_block_writeback(inode, block_nr);\n\n\tif (bio_add_page(bio, page, blocksize, 0) < blocksize)\n\t\tgoto submit_and_realloc;\n\n\tinc_page_count(F2FS_I_SB(inode), F2FS_RD_DATA);\n\tClearPageError(page);\n\t*last_block_in_bio = block_nr;\n\tgoto out;\nconfused:\n\tif (bio) {\n\t\t__submit_bio(F2FS_I_SB(inode), bio, DATA);\n\t\tbio = NULL;\n\t}\n\tunlock_page(page);\nout:\n\t*bio_ret = bio;\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,\n\t\t\t\t\tunsigned nr_pages, unsigned op_flag)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct bio *bio;\n\tstruct bio_post_read_ctx *ctx;\n\tunsigned int post_read_steps = 0;\n\n\tbio = f2fs_bio_alloc(sbi, min_t(int, nr_pages, BIO_MAX_PAGES), false);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\tf2fs_target_device(sbi, blkaddr, bio);\n\tbio->bi_end_io = f2fs_read_end_io;\n\tbio_set_op_attrs(bio, REQ_OP_READ, op_flag);\n\n\tif (f2fs_encrypted_file(inode))\n\t\tpost_read_steps |= 1 << STEP_DECRYPT;\n\tif (post_read_steps) {\n\t\tctx = mempool_alloc(bio_post_read_ctx_pool, GFP_NOFS);\n\t\tif (!ctx) {\n\t\t\tbio_put(bio);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\tctx->bio = bio;\n\t\tctx->enabled_steps = post_read_steps;\n\t\tbio->bi_private = ctx;\n\n\t\t/* wait the page to be moved by cleaning */\n\t\tf2fs_wait_on_block_writeback(sbi, blkaddr);\n\t}\n\n\treturn bio;\n}",
                        "code_after_change": "static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,\n\t\t\t\t\tunsigned nr_pages, unsigned op_flag)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct bio *bio;\n\tstruct bio_post_read_ctx *ctx;\n\tunsigned int post_read_steps = 0;\n\n\tif (!f2fs_is_valid_blkaddr(sbi, blkaddr, DATA_GENERIC))\n\t\treturn ERR_PTR(-EFAULT);\n\n\tbio = f2fs_bio_alloc(sbi, min_t(int, nr_pages, BIO_MAX_PAGES), false);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\tf2fs_target_device(sbi, blkaddr, bio);\n\tbio->bi_end_io = f2fs_read_end_io;\n\tbio_set_op_attrs(bio, REQ_OP_READ, op_flag);\n\n\tif (f2fs_encrypted_file(inode))\n\t\tpost_read_steps |= 1 << STEP_DECRYPT;\n\tif (post_read_steps) {\n\t\tctx = mempool_alloc(bio_post_read_ctx_pool, GFP_NOFS);\n\t\tif (!ctx) {\n\t\t\tbio_put(bio);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\tctx->bio = bio;\n\t\tctx->enabled_steps = post_read_steps;\n\t\tbio->bi_private = ctx;\n\n\t\t/* wait the page to be moved by cleaning */\n\t\tf2fs_wait_on_block_writeback(sbi, blkaddr);\n\t}\n\n\treturn bio;\n}",
                        "cve_id": "CVE-2018-14616"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2358,
            "cve_id": "CVE-2019-19815",
            "code_snippet": "static int f2fs_set_data_page_dirty(struct page *page)\n{\n\tstruct address_space *mapping = page->mapping;\n\tstruct inode *inode = mapping->host;\n\n\ttrace_f2fs_set_page_dirty(page, DATA);\n\n\tif (!PageUptodate(page))\n\t\tSetPageUptodate(page);\n\n\tif (f2fs_is_atomic_file(inode) && !f2fs_is_commit_atomic_write(inode)) {\n\t\tif (!IS_ATOMIC_WRITTEN_PAGE(page)) {\n\t\t\tf2fs_register_inmem_page(inode, page);\n\t\t\treturn 1;\n\t\t}\n\t\t/*\n\t\t * Previously, this page has been registered, we just\n\t\t * return here.\n\t\t */\n\t\treturn 0;\n\t}\n\n\tif (!PageDirty(page)) {\n\t\t__set_page_dirty_nobuffers(page);\n\t\tf2fs_update_dirty_page(inode, page);\n\t\treturn 1;\n\t}\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int f2fs_read_data_page(struct file *file, struct page *page)\n{\n\tstruct inode *inode = page->mapping->host;\n\tint ret = -EAGAIN;\n\n\ttrace_f2fs_readpage(page, DATA);\n\n\t/* If the file has inline data, try to read it directly */\n\tif (f2fs_has_inline_data(inode))\n\t\tret = f2fs_read_inline_data(inode, page);\n\tif (ret == -EAGAIN)\n\t\tret = f2fs_mpage_readpages(page->mapping, NULL, page, 1, false);\n\treturn ret;\n}",
                        "code_after_change": "static enum count_type __read_io_type(struct page *page)\n{\n\tstruct address_space *mapping = page_file_mapping(page);\n\n\tif (mapping) {\n\t\tstruct inode *inode = mapping->host;\n\t\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\t\tif (inode->i_ino == F2FS_META_INO(sbi))\n\t\t\treturn F2FS_RD_META;\n\n\t\tif (inode->i_ino == F2FS_NODE_INO(sbi))\n\t\t\treturn F2FS_RD_NODE;\n\t}\n\treturn F2FS_RD_DATA;\n}",
                        "cve_id": "CVE-2019-19815"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static enum count_type __read_io_type(struct page *page)\n{\n\tstruct address_space *mapping = page->mapping;\n\n\tif (mapping) {\n\t\tstruct inode *inode = mapping->host;\n\t\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\t\tif (inode->i_ino == F2FS_META_INO(sbi))\n\t\t\treturn F2FS_RD_META;\n\n\t\tif (inode->i_ino == F2FS_NODE_INO(sbi))\n\t\t\treturn F2FS_RD_NODE;\n\t}\n\treturn F2FS_RD_DATA;\n}",
                        "code_after_change": "int udf_expand_file_adinicb(struct inode *inode)\n{\n\tstruct page *page;\n\tchar *kaddr;\n\tstruct udf_inode_info *iinfo = UDF_I(inode);\n\tint err;\n\n\tWARN_ON_ONCE(!inode_is_locked(inode));\n\tif (!iinfo->i_lenAlloc) {\n\t\tif (UDF_QUERY_FLAG(inode->i_sb, UDF_FLAG_USE_SHORT_AD))\n\t\t\tiinfo->i_alloc_type = ICBTAG_FLAG_AD_SHORT;\n\t\telse\n\t\t\tiinfo->i_alloc_type = ICBTAG_FLAG_AD_LONG;\n\t\t/* from now on we have normal address_space methods */\n\t\tinode->i_data.a_ops = &udf_aops;\n\t\tup_write(&iinfo->i_data_sem);\n\t\tmark_inode_dirty(inode);\n\t\treturn 0;\n\t}\n\t/*\n\t * Release i_data_sem so that we can lock a page - page lock ranks\n\t * above i_data_sem. i_mutex still protects us against file changes.\n\t */\n\tup_write(&iinfo->i_data_sem);\n\n\tpage = find_or_create_page(inode->i_mapping, 0, GFP_NOFS);\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tif (!PageUptodate(page)) {\n\t\tkaddr = kmap_atomic(page);\n\t\tmemset(kaddr + iinfo->i_lenAlloc, 0x00,\n\t\t       PAGE_SIZE - iinfo->i_lenAlloc);\n\t\tmemcpy(kaddr, iinfo->i_data + iinfo->i_lenEAttr,\n\t\t\tiinfo->i_lenAlloc);\n\t\tflush_dcache_page(page);\n\t\tSetPageUptodate(page);\n\t\tkunmap_atomic(kaddr);\n\t}\n\tdown_write(&iinfo->i_data_sem);\n\tmemset(iinfo->i_data + iinfo->i_lenEAttr, 0x00,\n\t       iinfo->i_lenAlloc);\n\tiinfo->i_lenAlloc = 0;\n\tif (UDF_QUERY_FLAG(inode->i_sb, UDF_FLAG_USE_SHORT_AD))\n\t\tiinfo->i_alloc_type = ICBTAG_FLAG_AD_SHORT;\n\telse\n\t\tiinfo->i_alloc_type = ICBTAG_FLAG_AD_LONG;\n\t/* from now on we have normal address_space methods */\n\tinode->i_data.a_ops = &udf_aops;\n\tset_page_dirty(page);\n\tunlock_page(page);\n\tup_write(&iinfo->i_data_sem);\n\terr = filemap_fdatawrite(inode->i_mapping);\n\tif (err) {\n\t\t/* Restore everything back so that we don't lose data... */\n\t\tlock_page(page);\n\t\tdown_write(&iinfo->i_data_sem);\n\t\tkaddr = kmap_atomic(page);\n\t\tmemcpy(iinfo->i_data + iinfo->i_lenEAttr, kaddr, inode->i_size);\n\t\tkunmap_atomic(kaddr);\n\t\tunlock_page(page);\n\t\tiinfo->i_alloc_type = ICBTAG_FLAG_AD_IN_ICB;\n\t\tinode->i_data.a_ops = &udf_adinicb_aops;\n\t\tup_write(&iinfo->i_data_sem);\n\t}\n\tput_page(page);\n\tmark_inode_dirty(inode);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2019-19815"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int udf_expand_file_adinicb(struct inode *inode)\n{\n\tstruct page *page;\n\tchar *kaddr;\n\tstruct udf_inode_info *iinfo = UDF_I(inode);\n\tint err;\n\tstruct writeback_control udf_wbc = {\n\t\t.sync_mode = WB_SYNC_NONE,\n\t\t.nr_to_write = 1,\n\t};\n\n\tWARN_ON_ONCE(!inode_is_locked(inode));\n\tif (!iinfo->i_lenAlloc) {\n\t\tif (UDF_QUERY_FLAG(inode->i_sb, UDF_FLAG_USE_SHORT_AD))\n\t\t\tiinfo->i_alloc_type = ICBTAG_FLAG_AD_SHORT;\n\t\telse\n\t\t\tiinfo->i_alloc_type = ICBTAG_FLAG_AD_LONG;\n\t\t/* from now on we have normal address_space methods */\n\t\tinode->i_data.a_ops = &udf_aops;\n\t\tup_write(&iinfo->i_data_sem);\n\t\tmark_inode_dirty(inode);\n\t\treturn 0;\n\t}\n\t/*\n\t * Release i_data_sem so that we can lock a page - page lock ranks\n\t * above i_data_sem. i_mutex still protects us against file changes.\n\t */\n\tup_write(&iinfo->i_data_sem);\n\n\tpage = find_or_create_page(inode->i_mapping, 0, GFP_NOFS);\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tif (!PageUptodate(page)) {\n\t\tkaddr = kmap_atomic(page);\n\t\tmemset(kaddr + iinfo->i_lenAlloc, 0x00,\n\t\t       PAGE_SIZE - iinfo->i_lenAlloc);\n\t\tmemcpy(kaddr, iinfo->i_data + iinfo->i_lenEAttr,\n\t\t\tiinfo->i_lenAlloc);\n\t\tflush_dcache_page(page);\n\t\tSetPageUptodate(page);\n\t\tkunmap_atomic(kaddr);\n\t}\n\tdown_write(&iinfo->i_data_sem);\n\tmemset(iinfo->i_data + iinfo->i_lenEAttr, 0x00,\n\t       iinfo->i_lenAlloc);\n\tiinfo->i_lenAlloc = 0;\n\tif (UDF_QUERY_FLAG(inode->i_sb, UDF_FLAG_USE_SHORT_AD))\n\t\tiinfo->i_alloc_type = ICBTAG_FLAG_AD_SHORT;\n\telse\n\t\tiinfo->i_alloc_type = ICBTAG_FLAG_AD_LONG;\n\t/* from now on we have normal address_space methods */\n\tinode->i_data.a_ops = &udf_aops;\n\tup_write(&iinfo->i_data_sem);\n\terr = inode->i_data.a_ops->writepage(page, &udf_wbc);\n\tif (err) {\n\t\t/* Restore everything back so that we don't lose data... */\n\t\tlock_page(page);\n\t\tdown_write(&iinfo->i_data_sem);\n\t\tkaddr = kmap_atomic(page);\n\t\tmemcpy(iinfo->i_data + iinfo->i_lenEAttr, kaddr, inode->i_size);\n\t\tkunmap_atomic(kaddr);\n\t\tunlock_page(page);\n\t\tiinfo->i_alloc_type = ICBTAG_FLAG_AD_IN_ICB;\n\t\tinode->i_data.a_ops = &udf_adinicb_aops;\n\t\tup_write(&iinfo->i_data_sem);\n\t}\n\tput_page(page);\n\tmark_inode_dirty(inode);\n\n\treturn err;\n}",
                        "code_after_change": "static int f2fs_read_data_page(struct file *file, struct page *page)\n{\n\tstruct inode *inode = page_file_mapping(page)->host;\n\tint ret = -EAGAIN;\n\n\ttrace_f2fs_readpage(page, DATA);\n\n\t/* If the file has inline data, try to read it directly */\n\tif (f2fs_has_inline_data(inode))\n\t\tret = f2fs_read_inline_data(inode, page);\n\tif (ret == -EAGAIN)\n\t\tret = f2fs_mpage_readpages(page_file_mapping(page),\n\t\t\t\t\t\tNULL, page, 1, false);\n\treturn ret;\n}",
                        "cve_id": "CVE-2022-0617"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int migrate_page_move_mapping(struct address_space *mapping,\n\t\tstruct page *newpage, struct page *page,\n\t\tstruct buffer_head *head, enum migrate_mode mode,\n\t\tint extra_count)\n{\n\tint expected_count = 1 + extra_count;\n\tvoid **pslot;\n\n\tif (!mapping) {\n\t\t/* Anonymous page without mapping */\n\t\tif (page_count(page) != expected_count)\n\t\t\treturn -EAGAIN;\n\n\t\t/* No turning back from here */\n\t\tset_page_memcg(newpage, page_memcg(page));\n\t\tnewpage->index = page->index;\n\t\tnewpage->mapping = page->mapping;\n\t\tif (PageSwapBacked(page))\n\t\t\tSetPageSwapBacked(newpage);\n\n\t\treturn MIGRATEPAGE_SUCCESS;\n\t}\n\n\tspin_lock_irq(&mapping->tree_lock);\n\n\tpslot = radix_tree_lookup_slot(&mapping->page_tree,\n \t\t\t\t\tpage_index(page));\n\n\texpected_count += 1 + page_has_private(page);\n\tif (page_count(page) != expected_count ||\n\t\tradix_tree_deref_slot_protected(pslot, &mapping->tree_lock) != page) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\tif (!page_freeze_refs(page, expected_count)) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * In the async migration case of moving a page with buffers, lock the\n\t * buffers using trylock before the mapping is moved. If the mapping\n\t * was moved, we later failed to lock the buffers and could not move\n\t * the mapping back due to an elevated page count, we would have to\n\t * block waiting on other references to be dropped.\n\t */\n\tif (mode == MIGRATE_ASYNC && head &&\n\t\t\t!buffer_migrate_lock_buffers(head, mode)) {\n\t\tpage_unfreeze_refs(page, expected_count);\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * Now we know that no one else is looking at the page:\n\t * no turning back from here.\n\t */\n\tset_page_memcg(newpage, page_memcg(page));\n\tnewpage->index = page->index;\n\tnewpage->mapping = page->mapping;\n\tif (PageSwapBacked(page))\n\t\tSetPageSwapBacked(newpage);\n\n\tget_page(newpage);\t/* add cache reference */\n\tif (PageSwapCache(page)) {\n\t\tSetPageSwapCache(newpage);\n\t\tset_page_private(newpage, page_private(page));\n\t}\n\n\tradix_tree_replace_slot(pslot, newpage);\n\n\t/*\n\t * Drop cache reference from old page by unfreezing\n\t * to one less reference.\n\t * We know this isn't the last reference.\n\t */\n\tpage_unfreeze_refs(page, expected_count - 1);\n\n\t/*\n\t * If moved to a different zone then also account\n\t * the page for that zone. Other VM counters will be\n\t * taken care of when we establish references to the\n\t * new page and drop references to the old page.\n\t *\n\t * Note that anonymous pages are accounted for\n\t * via NR_FILE_PAGES and NR_ANON_PAGES if they\n\t * are mapped to swap space.\n\t */\n\t__dec_zone_page_state(page, NR_FILE_PAGES);\n\t__inc_zone_page_state(newpage, NR_FILE_PAGES);\n\tif (!PageSwapCache(page) && PageSwapBacked(page)) {\n\t\t__dec_zone_page_state(page, NR_SHMEM);\n\t\t__inc_zone_page_state(newpage, NR_SHMEM);\n\t}\n\tspin_unlock_irq(&mapping->tree_lock);\n\n\treturn MIGRATEPAGE_SUCCESS;\n}",
                        "code_after_change": "int migrate_page_move_mapping(struct address_space *mapping,\n\t\tstruct page *newpage, struct page *page,\n\t\tstruct buffer_head *head, enum migrate_mode mode,\n\t\tint extra_count)\n{\n\tstruct zone *oldzone, *newzone;\n\tint dirty;\n\tint expected_count = 1 + extra_count;\n\tvoid **pslot;\n\n\tif (!mapping) {\n\t\t/* Anonymous page without mapping */\n\t\tif (page_count(page) != expected_count)\n\t\t\treturn -EAGAIN;\n\n\t\t/* No turning back from here */\n\t\tset_page_memcg(newpage, page_memcg(page));\n\t\tnewpage->index = page->index;\n\t\tnewpage->mapping = page->mapping;\n\t\tif (PageSwapBacked(page))\n\t\t\tSetPageSwapBacked(newpage);\n\n\t\treturn MIGRATEPAGE_SUCCESS;\n\t}\n\n\toldzone = page_zone(page);\n\tnewzone = page_zone(newpage);\n\n\tspin_lock_irq(&mapping->tree_lock);\n\n\tpslot = radix_tree_lookup_slot(&mapping->page_tree,\n \t\t\t\t\tpage_index(page));\n\n\texpected_count += 1 + page_has_private(page);\n\tif (page_count(page) != expected_count ||\n\t\tradix_tree_deref_slot_protected(pslot, &mapping->tree_lock) != page) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\tif (!page_freeze_refs(page, expected_count)) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * In the async migration case of moving a page with buffers, lock the\n\t * buffers using trylock before the mapping is moved. If the mapping\n\t * was moved, we later failed to lock the buffers and could not move\n\t * the mapping back due to an elevated page count, we would have to\n\t * block waiting on other references to be dropped.\n\t */\n\tif (mode == MIGRATE_ASYNC && head &&\n\t\t\t!buffer_migrate_lock_buffers(head, mode)) {\n\t\tpage_unfreeze_refs(page, expected_count);\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * Now we know that no one else is looking at the page:\n\t * no turning back from here.\n\t */\n\tset_page_memcg(newpage, page_memcg(page));\n\tnewpage->index = page->index;\n\tnewpage->mapping = page->mapping;\n\tif (PageSwapBacked(page))\n\t\tSetPageSwapBacked(newpage);\n\n\tget_page(newpage);\t/* add cache reference */\n\tif (PageSwapCache(page)) {\n\t\tSetPageSwapCache(newpage);\n\t\tset_page_private(newpage, page_private(page));\n\t}\n\n\t/* Move dirty while page refs frozen and newpage not yet exposed */\n\tdirty = PageDirty(page);\n\tif (dirty) {\n\t\tClearPageDirty(page);\n\t\tSetPageDirty(newpage);\n\t}\n\n\tradix_tree_replace_slot(pslot, newpage);\n\n\t/*\n\t * Drop cache reference from old page by unfreezing\n\t * to one less reference.\n\t * We know this isn't the last reference.\n\t */\n\tpage_unfreeze_refs(page, expected_count - 1);\n\n\tspin_unlock(&mapping->tree_lock);\n\t/* Leave irq disabled to prevent preemption while updating stats */\n\n\t/*\n\t * If moved to a different zone then also account\n\t * the page for that zone. Other VM counters will be\n\t * taken care of when we establish references to the\n\t * new page and drop references to the old page.\n\t *\n\t * Note that anonymous pages are accounted for\n\t * via NR_FILE_PAGES and NR_ANON_PAGES if they\n\t * are mapped to swap space.\n\t */\n\tif (newzone != oldzone) {\n\t\t__dec_zone_state(oldzone, NR_FILE_PAGES);\n\t\t__inc_zone_state(newzone, NR_FILE_PAGES);\n\t\tif (PageSwapBacked(page) && !PageSwapCache(page)) {\n\t\t\t__dec_zone_state(oldzone, NR_SHMEM);\n\t\t\t__inc_zone_state(newzone, NR_SHMEM);\n\t\t}\n\t\tif (dirty && mapping_cap_account_dirty(mapping)) {\n\t\t\t__dec_zone_state(oldzone, NR_FILE_DIRTY);\n\t\t\t__inc_zone_state(newzone, NR_FILE_DIRTY);\n\t\t}\n\t}\n\tlocal_irq_enable();\n\n\treturn MIGRATEPAGE_SUCCESS;\n}",
                        "cve_id": "CVE-2016-3070"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static inline struct f2fs_sb_info *F2FS_P_SB(struct page *page)\n{\n\treturn F2FS_M_SB(page->mapping);\n}",
                        "code_after_change": "static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,\n\t\t\t\t\tunsigned nr_pages, unsigned op_flag)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct bio *bio;\n\tstruct bio_post_read_ctx *ctx;\n\tunsigned int post_read_steps = 0;\n\n\tif (!f2fs_is_valid_blkaddr(sbi, blkaddr, DATA_GENERIC))\n\t\treturn ERR_PTR(-EFAULT);\n\n\tbio = f2fs_bio_alloc(sbi, min_t(int, nr_pages, BIO_MAX_PAGES), false);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\tf2fs_target_device(sbi, blkaddr, bio);\n\tbio->bi_end_io = f2fs_read_end_io;\n\tbio_set_op_attrs(bio, REQ_OP_READ, op_flag);\n\n\tif (f2fs_encrypted_file(inode))\n\t\tpost_read_steps |= 1 << STEP_DECRYPT;\n\tif (post_read_steps) {\n\t\tctx = mempool_alloc(bio_post_read_ctx_pool, GFP_NOFS);\n\t\tif (!ctx) {\n\t\t\tbio_put(bio);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\tctx->bio = bio;\n\t\tctx->enabled_steps = post_read_steps;\n\t\tbio->bi_private = ctx;\n\n\t\t/* wait the page to be moved by cleaning */\n\t\tf2fs_wait_on_block_writeback(sbi, blkaddr);\n\t}\n\n\treturn bio;\n}",
                        "cve_id": "CVE-2019-19815"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 2359,
            "cve_id": "CVE-2019-19815",
            "code_snippet": "static int f2fs_mpage_readpages(struct address_space *mapping,\n\t\t\tstruct list_head *pages, struct page *page,\n\t\t\tunsigned nr_pages, bool is_readahead)\n{\n\tstruct bio *bio = NULL;\n\tsector_t last_block_in_bio = 0;\n\tstruct inode *inode = mapping->host;\n\tstruct f2fs_map_blocks map;\n\tint ret = 0;\n\n\tmap.m_pblk = 0;\n\tmap.m_lblk = 0;\n\tmap.m_len = 0;\n\tmap.m_flags = 0;\n\tmap.m_next_pgofs = NULL;\n\tmap.m_next_extent = NULL;\n\tmap.m_seg_type = NO_CHECK_TYPE;\n\tmap.m_may_create = false;\n\n\tfor (; nr_pages; nr_pages--) {\n\t\tif (pages) {\n\t\t\tpage = list_last_entry(pages, struct page, lru);\n\n\t\t\tprefetchw(&page->flags);\n\t\t\tlist_del(&page->lru);\n\t\t\tif (add_to_page_cache_lru(page, mapping,\n\t\t\t\t\t\t  page->index,\n\t\t\t\t\t\t  readahead_gfp_mask(mapping)))\n\t\t\t\tgoto next_page;\n\t\t}\n\n\t\tret = f2fs_read_single_page(inode, page, nr_pages, &map, &bio,\n\t\t\t\t\t&last_block_in_bio, is_readahead);\n\t\tif (ret) {\n\t\t\tSetPageError(page);\n\t\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\t\tunlock_page(page);\n\t\t}\nnext_page:\n\t\tif (pages)\n\t\t\tput_page(page);\n\t}\n\tBUG_ON(pages && !list_empty(pages));\n\tif (bio)\n\t\t__submit_bio(F2FS_I_SB(inode), bio, DATA);\n\treturn pages ? 0 : ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int __rds_rdma_map(struct rds_sock *rs, struct rds_get_mr_args *args,\n\t\t\t\tu64 *cookie_ret, struct rds_mr **mr_ret)\n{\n\tstruct rds_mr *mr = NULL, *found;\n\tunsigned int nr_pages;\n\tstruct page **pages = NULL;\n\tstruct scatterlist *sg;\n\tvoid *trans_private;\n\tunsigned long flags;\n\trds_rdma_cookie_t cookie;\n\tunsigned int nents;\n\tlong i;\n\tint ret;\n\n\tif (rs->rs_bound_addr == 0) {\n\t\tret = -ENOTCONN; /* XXX not a great errno */\n\t\tgoto out;\n\t}\n\n\tif (!rs->rs_transport->get_mr) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tnr_pages = rds_pages_in_vec(&args->vec);\n\tif (nr_pages == 0) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* Restrict the size of mr irrespective of underlying transport\n\t * To account for unaligned mr regions, subtract one from nr_pages\n\t */\n\tif ((nr_pages - 1) > (RDS_MAX_MSG_SIZE >> PAGE_SHIFT)) {\n\t\tret = -EMSGSIZE;\n\t\tgoto out;\n\t}\n\n\trdsdebug(\"RDS: get_mr addr %llx len %llu nr_pages %u\\n\",\n\t\targs->vec.addr, args->vec.bytes, nr_pages);\n\n\t/* XXX clamp nr_pages to limit the size of this alloc? */\n\tpages = kcalloc(nr_pages, sizeof(struct page *), GFP_KERNEL);\n\tif (!pages) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tmr = kzalloc(sizeof(struct rds_mr), GFP_KERNEL);\n\tif (!mr) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trefcount_set(&mr->r_refcount, 1);\n\tRB_CLEAR_NODE(&mr->r_rb_node);\n\tmr->r_trans = rs->rs_transport;\n\tmr->r_sock = rs;\n\n\tif (args->flags & RDS_RDMA_USE_ONCE)\n\t\tmr->r_use_once = 1;\n\tif (args->flags & RDS_RDMA_INVALIDATE)\n\t\tmr->r_invalidate = 1;\n\tif (args->flags & RDS_RDMA_READWRITE)\n\t\tmr->r_write = 1;\n\n\t/*\n\t * Pin the pages that make up the user buffer and transfer the page\n\t * pointers to the mr's sg array.  We check to see if we've mapped\n\t * the whole region after transferring the partial page references\n\t * to the sg array so that we can have one page ref cleanup path.\n\t *\n\t * For now we have no flag that tells us whether the mapping is\n\t * r/o or r/w. We need to assume r/w, or we'll do a lot of RDMA to\n\t * the zero page.\n\t */\n\tret = rds_pin_pages(args->vec.addr, nr_pages, pages, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tnents = ret;\n\tsg = kcalloc(nents, sizeof(*sg), GFP_KERNEL);\n\tif (!sg) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tWARN_ON(!nents);\n\tsg_init_table(sg, nents);\n\n\t/* Stick all pages into the scatterlist */\n\tfor (i = 0 ; i < nents; i++)\n\t\tsg_set_page(&sg[i], pages[i], PAGE_SIZE, 0);\n\n\trdsdebug(\"RDS: trans_private nents is %u\\n\", nents);\n\n\t/* Obtain a transport specific MR. If this succeeds, the\n\t * s/g list is now owned by the MR.\n\t * Note that dma_map() implies that pending writes are\n\t * flushed to RAM, so no dma_sync is needed here. */\n\ttrans_private = rs->rs_transport->get_mr(sg, nents, rs,\n\t\t\t\t\t\t &mr->r_key);\n\n\tif (IS_ERR(trans_private)) {\n\t\tfor (i = 0 ; i < nents; i++)\n\t\t\tput_page(sg_page(&sg[i]));\n\t\tkfree(sg);\n\t\tret = PTR_ERR(trans_private);\n\t\tgoto out;\n\t}\n\n\tmr->r_trans_private = trans_private;\n\n\trdsdebug(\"RDS: get_mr put_user key is %x cookie_addr %p\\n\",\n\t       mr->r_key, (void *)(unsigned long) args->cookie_addr);\n\n\t/* The user may pass us an unaligned address, but we can only\n\t * map page aligned regions. So we keep the offset, and build\n\t * a 64bit cookie containing <R_Key, offset> and pass that\n\t * around. */\n\tcookie = rds_rdma_make_cookie(mr->r_key, args->vec.addr & ~PAGE_MASK);\n\tif (cookie_ret)\n\t\t*cookie_ret = cookie;\n\n\tif (args->cookie_addr && put_user(cookie, (u64 __user *)(unsigned long) args->cookie_addr)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\t/* Inserting the new MR into the rbtree bumps its\n\t * reference count. */\n\tspin_lock_irqsave(&rs->rs_rdma_lock, flags);\n\tfound = rds_mr_tree_walk(&rs->rs_rdma_keys, mr->r_key, mr);\n\tspin_unlock_irqrestore(&rs->rs_rdma_lock, flags);\n\n\tBUG_ON(found && found != mr);\n\n\trdsdebug(\"RDS: get_mr key is %x\\n\", mr->r_key);\n\tif (mr_ret) {\n\t\trefcount_inc(&mr->r_refcount);\n\t\t*mr_ret = mr;\n\t}\n\n\tret = 0;\nout:\n\tkfree(pages);\n\tif (mr)\n\t\trds_mr_put(mr);\n\treturn ret;\n}",
                        "code_after_change": "static int __rds_rdma_map(struct rds_sock *rs, struct rds_get_mr_args *args,\n\t\t\t\tu64 *cookie_ret, struct rds_mr **mr_ret)\n{\n\tstruct rds_mr *mr = NULL, *found;\n\tunsigned int nr_pages;\n\tstruct page **pages = NULL;\n\tstruct scatterlist *sg;\n\tvoid *trans_private;\n\tunsigned long flags;\n\trds_rdma_cookie_t cookie;\n\tunsigned int nents;\n\tlong i;\n\tint ret;\n\n\tif (rs->rs_bound_addr == 0 || !rs->rs_transport) {\n\t\tret = -ENOTCONN; /* XXX not a great errno */\n\t\tgoto out;\n\t}\n\n\tif (!rs->rs_transport->get_mr) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tnr_pages = rds_pages_in_vec(&args->vec);\n\tif (nr_pages == 0) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* Restrict the size of mr irrespective of underlying transport\n\t * To account for unaligned mr regions, subtract one from nr_pages\n\t */\n\tif ((nr_pages - 1) > (RDS_MAX_MSG_SIZE >> PAGE_SHIFT)) {\n\t\tret = -EMSGSIZE;\n\t\tgoto out;\n\t}\n\n\trdsdebug(\"RDS: get_mr addr %llx len %llu nr_pages %u\\n\",\n\t\targs->vec.addr, args->vec.bytes, nr_pages);\n\n\t/* XXX clamp nr_pages to limit the size of this alloc? */\n\tpages = kcalloc(nr_pages, sizeof(struct page *), GFP_KERNEL);\n\tif (!pages) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tmr = kzalloc(sizeof(struct rds_mr), GFP_KERNEL);\n\tif (!mr) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trefcount_set(&mr->r_refcount, 1);\n\tRB_CLEAR_NODE(&mr->r_rb_node);\n\tmr->r_trans = rs->rs_transport;\n\tmr->r_sock = rs;\n\n\tif (args->flags & RDS_RDMA_USE_ONCE)\n\t\tmr->r_use_once = 1;\n\tif (args->flags & RDS_RDMA_INVALIDATE)\n\t\tmr->r_invalidate = 1;\n\tif (args->flags & RDS_RDMA_READWRITE)\n\t\tmr->r_write = 1;\n\n\t/*\n\t * Pin the pages that make up the user buffer and transfer the page\n\t * pointers to the mr's sg array.  We check to see if we've mapped\n\t * the whole region after transferring the partial page references\n\t * to the sg array so that we can have one page ref cleanup path.\n\t *\n\t * For now we have no flag that tells us whether the mapping is\n\t * r/o or r/w. We need to assume r/w, or we'll do a lot of RDMA to\n\t * the zero page.\n\t */\n\tret = rds_pin_pages(args->vec.addr, nr_pages, pages, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tnents = ret;\n\tsg = kcalloc(nents, sizeof(*sg), GFP_KERNEL);\n\tif (!sg) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tWARN_ON(!nents);\n\tsg_init_table(sg, nents);\n\n\t/* Stick all pages into the scatterlist */\n\tfor (i = 0 ; i < nents; i++)\n\t\tsg_set_page(&sg[i], pages[i], PAGE_SIZE, 0);\n\n\trdsdebug(\"RDS: trans_private nents is %u\\n\", nents);\n\n\t/* Obtain a transport specific MR. If this succeeds, the\n\t * s/g list is now owned by the MR.\n\t * Note that dma_map() implies that pending writes are\n\t * flushed to RAM, so no dma_sync is needed here. */\n\ttrans_private = rs->rs_transport->get_mr(sg, nents, rs,\n\t\t\t\t\t\t &mr->r_key);\n\n\tif (IS_ERR(trans_private)) {\n\t\tfor (i = 0 ; i < nents; i++)\n\t\t\tput_page(sg_page(&sg[i]));\n\t\tkfree(sg);\n\t\tret = PTR_ERR(trans_private);\n\t\tgoto out;\n\t}\n\n\tmr->r_trans_private = trans_private;\n\n\trdsdebug(\"RDS: get_mr put_user key is %x cookie_addr %p\\n\",\n\t       mr->r_key, (void *)(unsigned long) args->cookie_addr);\n\n\t/* The user may pass us an unaligned address, but we can only\n\t * map page aligned regions. So we keep the offset, and build\n\t * a 64bit cookie containing <R_Key, offset> and pass that\n\t * around. */\n\tcookie = rds_rdma_make_cookie(mr->r_key, args->vec.addr & ~PAGE_MASK);\n\tif (cookie_ret)\n\t\t*cookie_ret = cookie;\n\n\tif (args->cookie_addr && put_user(cookie, (u64 __user *)(unsigned long) args->cookie_addr)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\t/* Inserting the new MR into the rbtree bumps its\n\t * reference count. */\n\tspin_lock_irqsave(&rs->rs_rdma_lock, flags);\n\tfound = rds_mr_tree_walk(&rs->rs_rdma_keys, mr->r_key, mr);\n\tspin_unlock_irqrestore(&rs->rs_rdma_lock, flags);\n\n\tBUG_ON(found && found != mr);\n\n\trdsdebug(\"RDS: get_mr key is %x\\n\", mr->r_key);\n\tif (mr_ret) {\n\t\trefcount_inc(&mr->r_refcount);\n\t\t*mr_ret = mr;\n\t}\n\n\tret = 0;\nout:\n\tkfree(pages);\n\tif (mr)\n\t\trds_mr_put(mr);\n\treturn ret;\n}",
                        "cve_id": "CVE-2018-7492"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,\n\t\t\t\t\tunsigned nr_pages, unsigned op_flag)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct bio *bio;\n\tstruct bio_post_read_ctx *ctx;\n\tunsigned int post_read_steps = 0;\n\n\tbio = f2fs_bio_alloc(sbi, min_t(int, nr_pages, BIO_MAX_PAGES), false);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\tf2fs_target_device(sbi, blkaddr, bio);\n\tbio->bi_end_io = f2fs_read_end_io;\n\tbio_set_op_attrs(bio, REQ_OP_READ, op_flag);\n\n\tif (f2fs_encrypted_file(inode))\n\t\tpost_read_steps |= 1 << STEP_DECRYPT;\n\tif (post_read_steps) {\n\t\tctx = mempool_alloc(bio_post_read_ctx_pool, GFP_NOFS);\n\t\tif (!ctx) {\n\t\t\tbio_put(bio);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\tctx->bio = bio;\n\t\tctx->enabled_steps = post_read_steps;\n\t\tbio->bi_private = ctx;\n\n\t\t/* wait the page to be moved by cleaning */\n\t\tf2fs_wait_on_block_writeback(sbi, blkaddr);\n\t}\n\n\treturn bio;\n}",
                        "code_after_change": "static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,\n\t\t\t\t\tunsigned nr_pages, unsigned op_flag)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct bio *bio;\n\tstruct bio_post_read_ctx *ctx;\n\tunsigned int post_read_steps = 0;\n\n\tif (!f2fs_is_valid_blkaddr(sbi, blkaddr, DATA_GENERIC))\n\t\treturn ERR_PTR(-EFAULT);\n\n\tbio = f2fs_bio_alloc(sbi, min_t(int, nr_pages, BIO_MAX_PAGES), false);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\tf2fs_target_device(sbi, blkaddr, bio);\n\tbio->bi_end_io = f2fs_read_end_io;\n\tbio_set_op_attrs(bio, REQ_OP_READ, op_flag);\n\n\tif (f2fs_encrypted_file(inode))\n\t\tpost_read_steps |= 1 << STEP_DECRYPT;\n\tif (post_read_steps) {\n\t\tctx = mempool_alloc(bio_post_read_ctx_pool, GFP_NOFS);\n\t\tif (!ctx) {\n\t\t\tbio_put(bio);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\tctx->bio = bio;\n\t\tctx->enabled_steps = post_read_steps;\n\t\tbio->bi_private = ctx;\n\n\t\t/* wait the page to be moved by cleaning */\n\t\tf2fs_wait_on_block_writeback(sbi, blkaddr);\n\t}\n\n\treturn bio;\n}",
                        "cve_id": "CVE-2018-14616"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2516,
            "cve_id": "CVE-2020-10711",
            "code_snippet": "static int cipso_v4_parsetag_rbm(const struct cipso_v4_doi *doi_def,\n\t\t\t\t const unsigned char *tag,\n\t\t\t\t struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tu8 tag_len = tag[1];\n\tu32 level;\n\n\tret_val = cipso_v4_map_lvl_ntoh(doi_def, tag[3], &level);\n\tif (ret_val != 0)\n\t\treturn ret_val;\n\tsecattr->attr.mls.lvl = level;\n\tsecattr->flags |= NETLBL_SECATTR_MLS_LVL;\n\n\tif (tag_len > 4) {\n\t\tret_val = cipso_v4_map_cat_rbm_ntoh(doi_def,\n\t\t\t\t\t\t    &tag[4],\n\t\t\t\t\t\t    tag_len - 4,\n\t\t\t\t\t\t    secattr);\n\t\tif (ret_val != 0) {\n\t\t\tnetlbl_catmap_free(secattr->attr.mls.cat);\n\t\t\treturn ret_val;\n\t\t}\n\n\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;\n\t}\n\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int cipso_v4_parsetag_rng(const struct cipso_v4_doi *doi_def,\n\t\t\t\t const unsigned char *tag,\n\t\t\t\t struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tu8 tag_len = tag[1];\n\tu32 level;\n\n\tret_val = cipso_v4_map_lvl_ntoh(doi_def, tag[3], &level);\n\tif (ret_val != 0)\n\t\treturn ret_val;\n\tsecattr->attr.mls.lvl = level;\n\tsecattr->flags |= NETLBL_SECATTR_MLS_LVL;\n\n\tif (tag_len > 4) {\n\t\tret_val = cipso_v4_map_cat_rng_ntoh(doi_def,\n\t\t\t\t\t\t    &tag[4],\n\t\t\t\t\t\t    tag_len - 4,\n\t\t\t\t\t\t    secattr);\n\t\tif (ret_val != 0) {\n\t\t\tnetlbl_catmap_free(secattr->attr.mls.cat);\n\t\t\treturn ret_val;\n\t\t}\n\n\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;\n\t}\n\n\treturn 0;\n}",
                        "code_after_change": "static int cipso_v4_parsetag_rng(const struct cipso_v4_doi *doi_def,\n\t\t\t\t const unsigned char *tag,\n\t\t\t\t struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tu8 tag_len = tag[1];\n\tu32 level;\n\n\tret_val = cipso_v4_map_lvl_ntoh(doi_def, tag[3], &level);\n\tif (ret_val != 0)\n\t\treturn ret_val;\n\tsecattr->attr.mls.lvl = level;\n\tsecattr->flags |= NETLBL_SECATTR_MLS_LVL;\n\n\tif (tag_len > 4) {\n\t\tret_val = cipso_v4_map_cat_rng_ntoh(doi_def,\n\t\t\t\t\t\t    &tag[4],\n\t\t\t\t\t\t    tag_len - 4,\n\t\t\t\t\t\t    secattr);\n\t\tif (ret_val != 0) {\n\t\t\tnetlbl_catmap_free(secattr->attr.mls.cat);\n\t\t\treturn ret_val;\n\t\t}\n\n\t\tif (secattr->attr.mls.cat)\n\t\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;\n\t}\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2020-10711"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2540,
            "cve_id": "CVE-2020-11608",
            "code_snippet": "static void ov511_mode_init_regs(struct sd *sd)\n{\n\tstruct gspca_dev *gspca_dev = (struct gspca_dev *)sd;\n\tint hsegs, vsegs, packet_size, fps, needed;\n\tint interlaced = 0;\n\tstruct usb_host_interface *alt;\n\tstruct usb_interface *intf;\n\n\tintf = usb_ifnum_to_if(sd->gspca_dev.dev, sd->gspca_dev.iface);\n\talt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);\n\tif (!alt) {\n\t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n\t\tsd->gspca_dev.usb_err = -EIO;\n\t\treturn;\n\t}\n\n\tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n\treg_w(sd, R51x_FIFO_PSIZE, packet_size >> 5);\n\n\treg_w(sd, R511_CAM_UV_EN, 0x01);\n\treg_w(sd, R511_SNAP_UV_EN, 0x01);\n\treg_w(sd, R511_SNAP_OPTS, 0x03);\n\n\t/* Here I'm assuming that snapshot size == image size.\n\t * I hope that's always true. --claudio\n\t */\n\thsegs = (sd->gspca_dev.pixfmt.width >> 3) - 1;\n\tvsegs = (sd->gspca_dev.pixfmt.height >> 3) - 1;\n\n\treg_w(sd, R511_CAM_PXCNT, hsegs);\n\treg_w(sd, R511_CAM_LNCNT, vsegs);\n\treg_w(sd, R511_CAM_PXDIV, 0x00);\n\treg_w(sd, R511_CAM_LNDIV, 0x00);\n\n\t/* YUV420, low pass filter on */\n\treg_w(sd, R511_CAM_OPTS, 0x03);\n\n\t/* Snapshot additions */\n\treg_w(sd, R511_SNAP_PXCNT, hsegs);\n\treg_w(sd, R511_SNAP_LNCNT, vsegs);\n\treg_w(sd, R511_SNAP_PXDIV, 0x00);\n\treg_w(sd, R511_SNAP_LNDIV, 0x00);\n\n\t/******** Set the framerate ********/\n\tif (frame_rate > 0)\n\t\tsd->frame_rate = frame_rate;\n\n\tswitch (sd->sensor) {\n\tcase SEN_OV6620:\n\t\t/* No framerate control, doesn't like higher rates yet */\n\t\tsd->clockdiv = 3;\n\t\tbreak;\n\n\t/* Note once the FIXME's in mode_init_ov_sensor_regs() are fixed\n\t   for more sensors we need to do this for them too */\n\tcase SEN_OV7620:\n\tcase SEN_OV7620AE:\n\tcase SEN_OV7640:\n\tcase SEN_OV7648:\n\tcase SEN_OV76BE:\n\t\tif (sd->gspca_dev.pixfmt.width == 320)\n\t\t\tinterlaced = 1;\n\t\t/* Fall through */\n\tcase SEN_OV6630:\n\tcase SEN_OV7610:\n\tcase SEN_OV7670:\n\t\tswitch (sd->frame_rate) {\n\t\tcase 30:\n\t\tcase 25:\n\t\t\t/* Not enough bandwidth to do 640x480 @ 30 fps */\n\t\t\tif (sd->gspca_dev.pixfmt.width != 640) {\n\t\t\t\tsd->clockdiv = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* For 640x480 case */\n\t\t\t/* fall through */\n\t\tdefault:\n/*\t\tcase 20: */\n/*\t\tcase 15: */\n\t\t\tsd->clockdiv = 1;\n\t\t\tbreak;\n\t\tcase 10:\n\t\t\tsd->clockdiv = 2;\n\t\t\tbreak;\n\t\tcase 5:\n\t\t\tsd->clockdiv = 5;\n\t\t\tbreak;\n\t\t}\n\t\tif (interlaced) {\n\t\t\tsd->clockdiv = (sd->clockdiv + 1) * 2 - 1;\n\t\t\t/* Higher then 10 does not work */\n\t\t\tif (sd->clockdiv > 10)\n\t\t\t\tsd->clockdiv = 10;\n\t\t}\n\t\tbreak;\n\n\tcase SEN_OV8610:\n\t\t/* No framerate control ?? */\n\t\tsd->clockdiv = 0;\n\t\tbreak;\n\t}\n\n\t/* Check if we have enough bandwidth to disable compression */\n\tfps = (interlaced ? 60 : 30) / (sd->clockdiv + 1) + 1;\n\tneeded = fps * sd->gspca_dev.pixfmt.width *\n\t\t\tsd->gspca_dev.pixfmt.height * 3 / 2;\n\t/* 1000 isoc packets/sec */\n\tif (needed > 1000 * packet_size) {\n\t\t/* Enable Y and UV quantization and compression */\n\t\treg_w(sd, R511_COMP_EN, 0x07);\n\t\treg_w(sd, R511_COMP_LUT_EN, 0x03);\n\t} else {\n\t\treg_w(sd, R511_COMP_EN, 0x06);\n\t\treg_w(sd, R511_COMP_LUT_EN, 0x00);\n\t}\n\n\treg_w(sd, R51x_SYS_RESET, OV511_RESET_OMNICE);\n\treg_w(sd, R51x_SYS_RESET, 0);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static void ov518_mode_init_regs(struct sd *sd)\n{\n\tstruct gspca_dev *gspca_dev = (struct gspca_dev *)sd;\n\tint hsegs, vsegs, packet_size;\n\tstruct usb_host_interface *alt;\n\tstruct usb_interface *intf;\n\n\tintf = usb_ifnum_to_if(sd->gspca_dev.dev, sd->gspca_dev.iface);\n\talt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);\n\tif (!alt) {\n\t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n\t\tsd->gspca_dev.usb_err = -EIO;\n\t\treturn;\n\t}\n\n\tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n\tov518_reg_w32(sd, R51x_FIFO_PSIZE, packet_size & ~7, 2);\n\n\t/******** Set the mode ********/\n\treg_w(sd, 0x2b, 0);\n\treg_w(sd, 0x2c, 0);\n\treg_w(sd, 0x2d, 0);\n\treg_w(sd, 0x2e, 0);\n\treg_w(sd, 0x3b, 0);\n\treg_w(sd, 0x3c, 0);\n\treg_w(sd, 0x3d, 0);\n\treg_w(sd, 0x3e, 0);\n\n\tif (sd->bridge == BRIDGE_OV518) {\n\t\t/* Set 8-bit (YVYU) input format */\n\t\treg_w_mask(sd, 0x20, 0x08, 0x08);\n\n\t\t/* Set 12-bit (4:2:0) output format */\n\t\treg_w_mask(sd, 0x28, 0x80, 0xf0);\n\t\treg_w_mask(sd, 0x38, 0x80, 0xf0);\n\t} else {\n\t\treg_w(sd, 0x28, 0x80);\n\t\treg_w(sd, 0x38, 0x80);\n\t}\n\n\thsegs = sd->gspca_dev.pixfmt.width / 16;\n\tvsegs = sd->gspca_dev.pixfmt.height / 4;\n\n\treg_w(sd, 0x29, hsegs);\n\treg_w(sd, 0x2a, vsegs);\n\n\treg_w(sd, 0x39, hsegs);\n\treg_w(sd, 0x3a, vsegs);\n\n\t/* Windows driver does this here; who knows why */\n\treg_w(sd, 0x2f, 0x80);\n\n\t/******** Set the framerate ********/\n\tif (sd->bridge == BRIDGE_OV518PLUS && sd->revision == 0 &&\n\t\t\t\t\t      sd->sensor == SEN_OV7620AE)\n\t\tsd->clockdiv = 0;\n\telse\n\t\tsd->clockdiv = 1;\n\n\t/* Mode independent, but framerate dependent, regs */\n\t/* 0x51: Clock divider; Only works on some cams which use 2 crystals */\n\treg_w(sd, 0x51, 0x04);\n\treg_w(sd, 0x22, 0x18);\n\treg_w(sd, 0x23, 0xff);\n\n\tif (sd->bridge == BRIDGE_OV518PLUS) {\n\t\tswitch (sd->sensor) {\n\t\tcase SEN_OV7620AE:\n\t\t\t/*\n\t\t\t * HdG: 640x480 needs special handling on device\n\t\t\t * revision 2, we check for device revision > 0 to\n\t\t\t * avoid regressions, as we don't know the correct\n\t\t\t * thing todo for revision 1.\n\t\t\t *\n\t\t\t * Also this likely means we don't need to\n\t\t\t * differentiate between the OV7620 and OV7620AE,\n\t\t\t * earlier testing hitting this same problem likely\n\t\t\t * happened to be with revision < 2 cams using an\n\t\t\t * OV7620 and revision 2 cams using an OV7620AE.\n\t\t\t */\n\t\t\tif (sd->revision > 0 &&\n\t\t\t\t\tsd->gspca_dev.pixfmt.width == 640) {\n\t\t\t\treg_w(sd, 0x20, 0x60);\n\t\t\t\treg_w(sd, 0x21, 0x1f);\n\t\t\t} else {\n\t\t\t\treg_w(sd, 0x20, 0x00);\n\t\t\t\treg_w(sd, 0x21, 0x19);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase SEN_OV7620:\n\t\t\treg_w(sd, 0x20, 0x00);\n\t\t\treg_w(sd, 0x21, 0x19);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treg_w(sd, 0x21, 0x19);\n\t\t}\n\t} else\n\t\treg_w(sd, 0x71, 0x17);\t/* Compression-related? */\n\n\t/* FIXME: Sensor-specific */\n\t/* Bit 5 is what matters here. Of course, it is \"reserved\" */\n\ti2c_w(sd, 0x54, 0x23);\n\n\treg_w(sd, 0x2f, 0x80);\n\n\tif (sd->bridge == BRIDGE_OV518PLUS) {\n\t\treg_w(sd, 0x24, 0x94);\n\t\treg_w(sd, 0x25, 0x90);\n\t\tov518_reg_w32(sd, 0xc4,    400, 2);\t/* 190h   */\n\t\tov518_reg_w32(sd, 0xc6,    540, 2);\t/* 21ch   */\n\t\tov518_reg_w32(sd, 0xc7,    540, 2);\t/* 21ch   */\n\t\tov518_reg_w32(sd, 0xc8,    108, 2);\t/* 6ch    */\n\t\tov518_reg_w32(sd, 0xca, 131098, 3);\t/* 2001ah */\n\t\tov518_reg_w32(sd, 0xcb,    532, 2);\t/* 214h   */\n\t\tov518_reg_w32(sd, 0xcc,   2400, 2);\t/* 960h   */\n\t\tov518_reg_w32(sd, 0xcd,     32, 2);\t/* 20h    */\n\t\tov518_reg_w32(sd, 0xce,    608, 2);\t/* 260h   */\n\t} else {\n\t\treg_w(sd, 0x24, 0x9f);\n\t\treg_w(sd, 0x25, 0x90);\n\t\tov518_reg_w32(sd, 0xc4,    400, 2);\t/* 190h   */\n\t\tov518_reg_w32(sd, 0xc6,    381, 2);\t/* 17dh   */\n\t\tov518_reg_w32(sd, 0xc7,    381, 2);\t/* 17dh   */\n\t\tov518_reg_w32(sd, 0xc8,    128, 2);\t/* 80h    */\n\t\tov518_reg_w32(sd, 0xca, 183331, 3);\t/* 2cc23h */\n\t\tov518_reg_w32(sd, 0xcb,    746, 2);\t/* 2eah   */\n\t\tov518_reg_w32(sd, 0xcc,   1750, 2);\t/* 6d6h   */\n\t\tov518_reg_w32(sd, 0xcd,     45, 2);\t/* 2dh    */\n\t\tov518_reg_w32(sd, 0xce,    851, 2);\t/* 353h   */\n\t}\n\n\treg_w(sd, 0x2f, 0x80);\n}",
                        "code_after_change": "static void ov518_mode_init_regs(struct sd *sd)\n{\n\tstruct gspca_dev *gspca_dev = (struct gspca_dev *)sd;\n\tint hsegs, vsegs, packet_size;\n\tstruct usb_host_interface *alt;\n\tstruct usb_interface *intf;\n\n\tintf = usb_ifnum_to_if(sd->gspca_dev.dev, sd->gspca_dev.iface);\n\talt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);\n\tif (!alt) {\n\t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n\t\tsd->gspca_dev.usb_err = -EIO;\n\t\treturn;\n\t}\n\n\tif (alt->desc.bNumEndpoints < 1) {\n\t\tsd->gspca_dev.usb_err = -ENODEV;\n\t\treturn;\n\t}\n\n\tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n\tov518_reg_w32(sd, R51x_FIFO_PSIZE, packet_size & ~7, 2);\n\n\t/******** Set the mode ********/\n\treg_w(sd, 0x2b, 0);\n\treg_w(sd, 0x2c, 0);\n\treg_w(sd, 0x2d, 0);\n\treg_w(sd, 0x2e, 0);\n\treg_w(sd, 0x3b, 0);\n\treg_w(sd, 0x3c, 0);\n\treg_w(sd, 0x3d, 0);\n\treg_w(sd, 0x3e, 0);\n\n\tif (sd->bridge == BRIDGE_OV518) {\n\t\t/* Set 8-bit (YVYU) input format */\n\t\treg_w_mask(sd, 0x20, 0x08, 0x08);\n\n\t\t/* Set 12-bit (4:2:0) output format */\n\t\treg_w_mask(sd, 0x28, 0x80, 0xf0);\n\t\treg_w_mask(sd, 0x38, 0x80, 0xf0);\n\t} else {\n\t\treg_w(sd, 0x28, 0x80);\n\t\treg_w(sd, 0x38, 0x80);\n\t}\n\n\thsegs = sd->gspca_dev.pixfmt.width / 16;\n\tvsegs = sd->gspca_dev.pixfmt.height / 4;\n\n\treg_w(sd, 0x29, hsegs);\n\treg_w(sd, 0x2a, vsegs);\n\n\treg_w(sd, 0x39, hsegs);\n\treg_w(sd, 0x3a, vsegs);\n\n\t/* Windows driver does this here; who knows why */\n\treg_w(sd, 0x2f, 0x80);\n\n\t/******** Set the framerate ********/\n\tif (sd->bridge == BRIDGE_OV518PLUS && sd->revision == 0 &&\n\t\t\t\t\t      sd->sensor == SEN_OV7620AE)\n\t\tsd->clockdiv = 0;\n\telse\n\t\tsd->clockdiv = 1;\n\n\t/* Mode independent, but framerate dependent, regs */\n\t/* 0x51: Clock divider; Only works on some cams which use 2 crystals */\n\treg_w(sd, 0x51, 0x04);\n\treg_w(sd, 0x22, 0x18);\n\treg_w(sd, 0x23, 0xff);\n\n\tif (sd->bridge == BRIDGE_OV518PLUS) {\n\t\tswitch (sd->sensor) {\n\t\tcase SEN_OV7620AE:\n\t\t\t/*\n\t\t\t * HdG: 640x480 needs special handling on device\n\t\t\t * revision 2, we check for device revision > 0 to\n\t\t\t * avoid regressions, as we don't know the correct\n\t\t\t * thing todo for revision 1.\n\t\t\t *\n\t\t\t * Also this likely means we don't need to\n\t\t\t * differentiate between the OV7620 and OV7620AE,\n\t\t\t * earlier testing hitting this same problem likely\n\t\t\t * happened to be with revision < 2 cams using an\n\t\t\t * OV7620 and revision 2 cams using an OV7620AE.\n\t\t\t */\n\t\t\tif (sd->revision > 0 &&\n\t\t\t\t\tsd->gspca_dev.pixfmt.width == 640) {\n\t\t\t\treg_w(sd, 0x20, 0x60);\n\t\t\t\treg_w(sd, 0x21, 0x1f);\n\t\t\t} else {\n\t\t\t\treg_w(sd, 0x20, 0x00);\n\t\t\t\treg_w(sd, 0x21, 0x19);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase SEN_OV7620:\n\t\t\treg_w(sd, 0x20, 0x00);\n\t\t\treg_w(sd, 0x21, 0x19);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treg_w(sd, 0x21, 0x19);\n\t\t}\n\t} else\n\t\treg_w(sd, 0x71, 0x17);\t/* Compression-related? */\n\n\t/* FIXME: Sensor-specific */\n\t/* Bit 5 is what matters here. Of course, it is \"reserved\" */\n\ti2c_w(sd, 0x54, 0x23);\n\n\treg_w(sd, 0x2f, 0x80);\n\n\tif (sd->bridge == BRIDGE_OV518PLUS) {\n\t\treg_w(sd, 0x24, 0x94);\n\t\treg_w(sd, 0x25, 0x90);\n\t\tov518_reg_w32(sd, 0xc4,    400, 2);\t/* 190h   */\n\t\tov518_reg_w32(sd, 0xc6,    540, 2);\t/* 21ch   */\n\t\tov518_reg_w32(sd, 0xc7,    540, 2);\t/* 21ch   */\n\t\tov518_reg_w32(sd, 0xc8,    108, 2);\t/* 6ch    */\n\t\tov518_reg_w32(sd, 0xca, 131098, 3);\t/* 2001ah */\n\t\tov518_reg_w32(sd, 0xcb,    532, 2);\t/* 214h   */\n\t\tov518_reg_w32(sd, 0xcc,   2400, 2);\t/* 960h   */\n\t\tov518_reg_w32(sd, 0xcd,     32, 2);\t/* 20h    */\n\t\tov518_reg_w32(sd, 0xce,    608, 2);\t/* 260h   */\n\t} else {\n\t\treg_w(sd, 0x24, 0x9f);\n\t\treg_w(sd, 0x25, 0x90);\n\t\tov518_reg_w32(sd, 0xc4,    400, 2);\t/* 190h   */\n\t\tov518_reg_w32(sd, 0xc6,    381, 2);\t/* 17dh   */\n\t\tov518_reg_w32(sd, 0xc7,    381, 2);\t/* 17dh   */\n\t\tov518_reg_w32(sd, 0xc8,    128, 2);\t/* 80h    */\n\t\tov518_reg_w32(sd, 0xca, 183331, 3);\t/* 2cc23h */\n\t\tov518_reg_w32(sd, 0xcb,    746, 2);\t/* 2eah   */\n\t\tov518_reg_w32(sd, 0xcc,   1750, 2);\t/* 6d6h   */\n\t\tov518_reg_w32(sd, 0xcd,     45, 2);\t/* 2dh    */\n\t\tov518_reg_w32(sd, 0xce,    851, 2);\t/* 353h   */\n\t}\n\n\treg_w(sd, 0x2f, 0x80);\n}",
                        "cve_id": "CVE-2020-11608"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2542,
            "cve_id": "CVE-2020-11609",
            "code_snippet": "static int stv06xx_isoc_init(struct gspca_dev *gspca_dev)\n{\n\tstruct usb_host_interface *alt;\n\tstruct sd *sd = (struct sd *) gspca_dev;\n\n\t/* Start isoc bandwidth \"negotiation\" at max isoc bandwidth */\n\talt = &gspca_dev->dev->actconfig->intf_cache[0]->altsetting[1];\n\talt->endpoint[0].desc.wMaxPacketSize =\n\t\tcpu_to_le16(sd->sensor->max_packet_size[gspca_dev->curr_mode]);\n\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int sd_isoc_init(struct gspca_dev *gspca_dev)\n{\n\tstruct usb_host_interface *alt;\n\tint max_packet_size;\n\n\tswitch (gspca_dev->pixfmt.width) {\n\tcase 160:\n\t\tmax_packet_size = 450;\n\t\tbreak;\n\tcase 176:\n\t\tmax_packet_size = 600;\n\t\tbreak;\n\tdefault:\n\t\tmax_packet_size = 1022;\n\t\tbreak;\n\t}\n\n\t/* Start isoc bandwidth \"negotiation\" at max isoc bandwidth */\n\talt = &gspca_dev->dev->actconfig->intf_cache[0]->altsetting[1];\n\talt->endpoint[0].desc.wMaxPacketSize = cpu_to_le16(max_packet_size);\n\n\treturn 0;\n}",
                        "code_after_change": "static int sd_isoc_init(struct gspca_dev *gspca_dev)\n{\n\tstruct usb_interface_cache *intfc;\n\tstruct usb_host_interface *alt;\n\tint max_packet_size;\n\n\tswitch (gspca_dev->pixfmt.width) {\n\tcase 160:\n\t\tmax_packet_size = 450;\n\t\tbreak;\n\tcase 176:\n\t\tmax_packet_size = 600;\n\t\tbreak;\n\tdefault:\n\t\tmax_packet_size = 1022;\n\t\tbreak;\n\t}\n\n\tintfc = gspca_dev->dev->actconfig->intf_cache[0];\n\n\tif (intfc->num_altsetting < 2)\n\t\treturn -ENODEV;\n\n\talt = &intfc->altsetting[1];\n\n\tif (alt->desc.bNumEndpoints < 1)\n\t\treturn -ENODEV;\n\n\t/* Start isoc bandwidth \"negotiation\" at max isoc bandwidth */\n\talt->endpoint[0].desc.wMaxPacketSize = cpu_to_le16(max_packet_size);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2020-11668"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2546,
            "cve_id": "CVE-2020-11668",
            "code_snippet": "static int cit_get_packet_size(struct gspca_dev *gspca_dev)\n{\n\tstruct usb_host_interface *alt;\n\tstruct usb_interface *intf;\n\n\tintf = usb_ifnum_to_if(gspca_dev->dev, gspca_dev->iface);\n\talt = usb_altnum_to_altsetting(intf, gspca_dev->alt);\n\tif (!alt) {\n\t\tpr_err(\"Couldn't get altsetting\\n\");\n\t\treturn -EIO;\n\t}\n\n\treturn le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int stv06xx_start(struct gspca_dev *gspca_dev)\n{\n\tstruct sd *sd = (struct sd *) gspca_dev;\n\tstruct usb_host_interface *alt;\n\tstruct usb_interface *intf;\n\tint err, packet_size;\n\n\tintf = usb_ifnum_to_if(sd->gspca_dev.dev, sd->gspca_dev.iface);\n\talt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);\n\tif (!alt) {\n\t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n\t\treturn -EIO;\n\t}\n\n\tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n\terr = stv06xx_write_bridge(sd, STV_ISO_SIZE_L, packet_size);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* Prepare the sensor for start */\n\terr = sd->sensor->start(sd);\n\tif (err < 0)\n\t\tgoto out;\n\n\t/* Start isochronous streaming */\n\terr = stv06xx_write_bridge(sd, STV_ISO_ENABLE, 1);\n\nout:\n\tif (err < 0)\n\t\tgspca_dbg(gspca_dev, D_STREAM, \"Starting stream failed\\n\");\n\telse\n\t\tgspca_dbg(gspca_dev, D_STREAM, \"Started streaming\\n\");\n\n\treturn (err < 0) ? err : 0;\n}",
                        "code_after_change": "static int stv06xx_start(struct gspca_dev *gspca_dev)\n{\n\tstruct sd *sd = (struct sd *) gspca_dev;\n\tstruct usb_host_interface *alt;\n\tstruct usb_interface *intf;\n\tint err, packet_size;\n\n\tintf = usb_ifnum_to_if(sd->gspca_dev.dev, sd->gspca_dev.iface);\n\talt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);\n\tif (!alt) {\n\t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n\t\treturn -EIO;\n\t}\n\n\tif (alt->desc.bNumEndpoints < 1)\n\t\treturn -ENODEV;\n\n\tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n\terr = stv06xx_write_bridge(sd, STV_ISO_SIZE_L, packet_size);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* Prepare the sensor for start */\n\terr = sd->sensor->start(sd);\n\tif (err < 0)\n\t\tgoto out;\n\n\t/* Start isochronous streaming */\n\terr = stv06xx_write_bridge(sd, STV_ISO_ENABLE, 1);\n\nout:\n\tif (err < 0)\n\t\tgspca_dbg(gspca_dev, D_STREAM, \"Starting stream failed\\n\");\n\telse\n\t\tgspca_dbg(gspca_dev, D_STREAM, \"Started streaming\\n\");\n\n\treturn (err < 0) ? err : 0;\n}",
                        "cve_id": "CVE-2020-11609"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2577,
            "cve_id": "CVE-2020-12364",
            "code_snippet": "static void guc_init_params(struct intel_guc *guc)\n{\n\tu32 *params = guc->params;\n\tint i;\n\n\tBUILD_BUG_ON(sizeof(guc->params) != GUC_CTL_MAX_DWORDS * sizeof(u32));\n\n\tparams[GUC_CTL_CTXINFO] = guc_ctl_ctxinfo_flags(guc);\n\tparams[GUC_CTL_LOG_PARAMS] = guc_ctl_log_params_flags(guc);\n\tparams[GUC_CTL_FEATURE] = guc_ctl_feature_flags(guc);\n\tparams[GUC_CTL_DEBUG] = guc_ctl_debug_flags(guc);\n\tparams[GUC_CTL_ADS] = guc_ctl_ads_flags(guc);\n\n\tfor (i = 0; i < GUC_CTL_MAX_DWORDS; i++)\n\t\tDRM_DEBUG_DRIVER(\"param[%2d] = %#x\\n\", i, params[i]);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int smsusb_init_device(struct usb_interface *intf, int board_id)\n{\n\tstruct smsdevice_params_t params;\n\tstruct smsusb_device_t *dev;\n\tvoid *mdev;\n\tint i, rc;\n\n\t/* create device object */\n\tdev = kzalloc(sizeof(struct smsusb_device_t), GFP_KERNEL);\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\tmemset(&params, 0, sizeof(params));\n\tusb_set_intfdata(intf, dev);\n\tdev->udev = interface_to_usbdev(intf);\n\tdev->state = SMSUSB_DISCONNECTED;\n\n\tparams.device_type = sms_get_board(board_id)->type;\n\n\tswitch (params.device_type) {\n\tcase SMS_STELLAR:\n\t\tdev->buffer_size = USB1_BUFFER_SIZE;\n\n\t\tparams.setmode_handler = smsusb1_setmode;\n\t\tparams.detectmode_handler = smsusb1_detectmode;\n\t\tbreak;\n\tcase SMS_UNKNOWN_TYPE:\n\t\tpr_err(\"Unspecified sms device type!\\n\");\n\t\t/* fall-thru */\n\tdefault:\n\t\tdev->buffer_size = USB2_BUFFER_SIZE;\n\t\tdev->response_alignment =\n\t\t    le16_to_cpu(dev->udev->ep_in[1]->desc.wMaxPacketSize) -\n\t\t    sizeof(struct sms_msg_hdr);\n\n\t\tparams.flags |= SMS_DEVICE_FAMILY2;\n\t\tbreak;\n\t}\n\n\tfor (i = 0; i < intf->cur_altsetting->desc.bNumEndpoints; i++) {\n\t\tif (intf->cur_altsetting->endpoint[i].desc. bEndpointAddress & USB_DIR_IN)\n\t\t\tdev->in_ep = intf->cur_altsetting->endpoint[i].desc.bEndpointAddress;\n\t\telse\n\t\t\tdev->out_ep = intf->cur_altsetting->endpoint[i].desc.bEndpointAddress;\n\t}\n\n\tpr_debug(\"in_ep = %02x, out_ep = %02x\\n\",\n\t\tdev->in_ep, dev->out_ep);\n\n\tparams.device = &dev->udev->dev;\n\tparams.usb_device = dev->udev;\n\tparams.buffer_size = dev->buffer_size;\n\tparams.num_buffers = MAX_BUFFERS;\n\tparams.sendrequest_handler = smsusb_sendrequest;\n\tparams.context = dev;\n\tusb_make_path(dev->udev, params.devpath, sizeof(params.devpath));\n\n\tmdev = siano_media_device_register(dev, board_id);\n\n\t/* register in smscore */\n\trc = smscore_register_device(&params, &dev->coredev, 0, mdev);\n\tif (rc < 0) {\n\t\tpr_err(\"smscore_register_device(...) failed, rc %d\\n\", rc);\n\t\tsmsusb_term_device(intf);\n#ifdef CONFIG_MEDIA_CONTROLLER_DVB\n\t\tmedia_device_unregister(mdev);\n#endif\n\t\tkfree(mdev);\n\t\treturn rc;\n\t}\n\n\tsmscore_set_board_id(dev->coredev, board_id);\n\n\tdev->coredev->is_usb_device = true;\n\n\t/* initialize urbs */\n\tfor (i = 0; i < MAX_URBS; i++) {\n\t\tdev->surbs[i].dev = dev;\n\t\tusb_init_urb(&dev->surbs[i].urb);\n\t}\n\n\tpr_debug(\"smsusb_start_streaming(...).\\n\");\n\trc = smsusb_start_streaming(dev);\n\tif (rc < 0) {\n\t\tpr_err(\"smsusb_start_streaming(...) failed\\n\");\n\t\tsmsusb_term_device(intf);\n\t\treturn rc;\n\t}\n\n\tdev->state = SMSUSB_ACTIVE;\n\n\trc = smscore_start_device(dev->coredev);\n\tif (rc < 0) {\n\t\tpr_err(\"smscore_start_device(...) failed\\n\");\n\t\tsmsusb_term_device(intf);\n\t\treturn rc;\n\t}\n\n\tpr_debug(\"device 0x%p created\\n\", dev);\n\n\treturn rc;\n}",
                        "code_after_change": "static int smsusb_init_device(struct usb_interface *intf, int board_id)\n{\n\tstruct smsdevice_params_t params;\n\tstruct smsusb_device_t *dev;\n\tvoid *mdev;\n\tint i, rc;\n\tint in_maxp;\n\n\t/* create device object */\n\tdev = kzalloc(sizeof(struct smsusb_device_t), GFP_KERNEL);\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\tmemset(&params, 0, sizeof(params));\n\tusb_set_intfdata(intf, dev);\n\tdev->udev = interface_to_usbdev(intf);\n\tdev->state = SMSUSB_DISCONNECTED;\n\n\tfor (i = 0; i < intf->cur_altsetting->desc.bNumEndpoints; i++) {\n\t\tstruct usb_endpoint_descriptor *desc =\n\t\t\t\t&intf->cur_altsetting->endpoint[i].desc;\n\n\t\tif (desc->bEndpointAddress & USB_DIR_IN) {\n\t\t\tdev->in_ep = desc->bEndpointAddress;\n\t\t\tin_maxp = usb_endpoint_maxp(desc);\n\t\t} else {\n\t\t\tdev->out_ep = desc->bEndpointAddress;\n\t\t}\n\t}\n\n\tpr_debug(\"in_ep = %02x, out_ep = %02x\\n\", dev->in_ep, dev->out_ep);\n\tif (!dev->in_ep || !dev->out_ep) {\t/* Missing endpoints? */\n\t\tsmsusb_term_device(intf);\n\t\treturn -ENODEV;\n\t}\n\n\tparams.device_type = sms_get_board(board_id)->type;\n\n\tswitch (params.device_type) {\n\tcase SMS_STELLAR:\n\t\tdev->buffer_size = USB1_BUFFER_SIZE;\n\n\t\tparams.setmode_handler = smsusb1_setmode;\n\t\tparams.detectmode_handler = smsusb1_detectmode;\n\t\tbreak;\n\tcase SMS_UNKNOWN_TYPE:\n\t\tpr_err(\"Unspecified sms device type!\\n\");\n\t\t/* fall-thru */\n\tdefault:\n\t\tdev->buffer_size = USB2_BUFFER_SIZE;\n\t\tdev->response_alignment = in_maxp - sizeof(struct sms_msg_hdr);\n\n\t\tparams.flags |= SMS_DEVICE_FAMILY2;\n\t\tbreak;\n\t}\n\n\tparams.device = &dev->udev->dev;\n\tparams.usb_device = dev->udev;\n\tparams.buffer_size = dev->buffer_size;\n\tparams.num_buffers = MAX_BUFFERS;\n\tparams.sendrequest_handler = smsusb_sendrequest;\n\tparams.context = dev;\n\tusb_make_path(dev->udev, params.devpath, sizeof(params.devpath));\n\n\tmdev = siano_media_device_register(dev, board_id);\n\n\t/* register in smscore */\n\trc = smscore_register_device(&params, &dev->coredev, 0, mdev);\n\tif (rc < 0) {\n\t\tpr_err(\"smscore_register_device(...) failed, rc %d\\n\", rc);\n\t\tsmsusb_term_device(intf);\n#ifdef CONFIG_MEDIA_CONTROLLER_DVB\n\t\tmedia_device_unregister(mdev);\n#endif\n\t\tkfree(mdev);\n\t\treturn rc;\n\t}\n\n\tsmscore_set_board_id(dev->coredev, board_id);\n\n\tdev->coredev->is_usb_device = true;\n\n\t/* initialize urbs */\n\tfor (i = 0; i < MAX_URBS; i++) {\n\t\tdev->surbs[i].dev = dev;\n\t\tusb_init_urb(&dev->surbs[i].urb);\n\t}\n\n\tpr_debug(\"smsusb_start_streaming(...).\\n\");\n\trc = smsusb_start_streaming(dev);\n\tif (rc < 0) {\n\t\tpr_err(\"smsusb_start_streaming(...) failed\\n\");\n\t\tsmsusb_term_device(intf);\n\t\treturn rc;\n\t}\n\n\tdev->state = SMSUSB_ACTIVE;\n\n\trc = smscore_start_device(dev->coredev);\n\tif (rc < 0) {\n\t\tpr_err(\"smscore_start_device(...) failed\\n\");\n\t\tsmsusb_term_device(intf);\n\t\treturn rc;\n\t}\n\n\tpr_debug(\"device 0x%p created\\n\", dev);\n\n\treturn rc;\n}",
                        "cve_id": "CVE-2019-15218"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int intel_guc_ads_create(struct intel_guc *guc)\n{\n\tconst u32 size = PAGE_ALIGN(sizeof(struct __guc_ads_blob));\n\tint ret;\n\n\tGEM_BUG_ON(guc->ads_vma);\n\n\tret = intel_guc_allocate_and_map_vma(guc, size, &guc->ads_vma,\n\t\t\t\t\t     (void **)&guc->ads_blob);\n\n\tif (ret)\n\t\treturn ret;\n\n\t__guc_ads_init(guc);\n\n\treturn 0;\n}",
                        "code_after_change": "int intel_guc_ads_create(struct intel_guc *guc)\n{\n\tu32 size;\n\tint ret;\n\n\tGEM_BUG_ON(guc->ads_vma);\n\n\tsize = guc_ads_blob_size(guc);\n\n\tret = intel_guc_allocate_and_map_vma(guc, size, &guc->ads_vma,\n\t\t\t\t\t     (void **)&guc->ads_blob);\n\tif (ret)\n\t\treturn ret;\n\n\t__guc_ads_init(guc);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2020-12364"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "void intel_guc_ads_reset(struct intel_guc *guc)\n{\n\tif (!guc->ads_vma)\n\t\treturn;\n\t__guc_ads_init(guc);\n}",
                        "code_after_change": "void intel_guc_ads_reset(struct intel_guc *guc)\n{\n\tif (!guc->ads_vma)\n\t\treturn;\n\n\t__guc_ads_init(guc);\n\n\tguc_ads_private_data_reset(guc);\n}",
                        "cve_id": "CVE-2020-12364"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int amd_sfh_hid_client_init(struct amd_mp2_dev *privdata)\n{\n\tstruct amd_input_data *in_data = &privdata->in_data;\n\tstruct amdtp_cl_data *cl_data = privdata->cl_data;\n\tstruct amd_mp2_ops *mp2_ops = privdata->mp2_ops;\n\tstruct amd_mp2_sensor_info info;\n\tstruct request_list *req_list;\n\tstruct device *dev;\n\tu32 feature_report_size;\n\tu32 input_report_size;\n\tint rc, i, status;\n\tu8 cl_idx;\n\n\treq_list = &cl_data->req_list;\n\tdev = &privdata->pdev->dev;\n\tamd_sfh_set_desc_ops(mp2_ops);\n\n\tmp2_ops->suspend = amd_sfh_suspend;\n\tmp2_ops->resume = amd_sfh_resume;\n\n\tcl_data->num_hid_devices = amd_mp2_get_sensor_num(privdata, &cl_data->sensor_idx[0]);\n\tif (cl_data->num_hid_devices == 0)\n\t\treturn -ENODEV;\n\n\tINIT_DELAYED_WORK(&cl_data->work, amd_sfh_work);\n\tINIT_DELAYED_WORK(&cl_data->work_buffer, amd_sfh_work_buffer);\n\tINIT_LIST_HEAD(&req_list->list);\n\tcl_data->in_data = in_data;\n\n\tfor (i = 0; i < cl_data->num_hid_devices; i++) {\n\t\tin_data->sensor_virt_addr[i] = dma_alloc_coherent(dev, sizeof(int) * 8,\n\t\t\t\t\t\t\t\t  &cl_data->sensor_dma_addr[i],\n\t\t\t\t\t\t\t\t  GFP_KERNEL);\n\t\tcl_data->sensor_sts[i] = SENSOR_DISABLED;\n\t\tcl_data->sensor_requested_cnt[i] = 0;\n\t\tcl_data->cur_hid_dev = i;\n\t\tcl_idx = cl_data->sensor_idx[i];\n\t\tcl_data->report_descr_sz[i] = mp2_ops->get_desc_sz(cl_idx, descr_size);\n\t\tif (!cl_data->report_descr_sz[i]) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tfeature_report_size = mp2_ops->get_desc_sz(cl_idx, feature_size);\n\t\tif (!feature_report_size) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tinput_report_size =  mp2_ops->get_desc_sz(cl_idx, input_size);\n\t\tif (!input_report_size) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tcl_data->feature_report[i] = devm_kzalloc(dev, feature_report_size, GFP_KERNEL);\n\t\tif (!cl_data->feature_report[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tin_data->input_report[i] = devm_kzalloc(dev, input_report_size, GFP_KERNEL);\n\t\tif (!in_data->input_report[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tinfo.period = AMD_SFH_IDLE_LOOP;\n\t\tinfo.sensor_idx = cl_idx;\n\t\tinfo.dma_address = cl_data->sensor_dma_addr[i];\n\n\t\tcl_data->report_descr[i] =\n\t\t\tdevm_kzalloc(dev, cl_data->report_descr_sz[i], GFP_KERNEL);\n\t\tif (!cl_data->report_descr[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\trc = mp2_ops->get_rep_desc(cl_idx, cl_data->report_descr[i]);\n\t\tif (rc)\n\t\t\treturn rc;\n\t\tmp2_ops->start(privdata, info);\n\t\tstatus = amd_sfh_wait_for_response\n\t\t\t\t(privdata, cl_data->sensor_idx[i], SENSOR_ENABLED);\n\t\tif (status == SENSOR_ENABLED) {\n\t\t\tcl_data->sensor_sts[i] = SENSOR_ENABLED;\n\t\t\trc = amdtp_hid_probe(cl_data->cur_hid_dev, cl_data);\n\t\t\tif (rc) {\n\t\t\t\tmp2_ops->stop(privdata, cl_data->sensor_idx[i]);\n\t\t\t\tstatus = amd_sfh_wait_for_response\n\t\t\t\t\t(privdata, cl_data->sensor_idx[i], SENSOR_DISABLED);\n\t\t\t\tif (status != SENSOR_ENABLED)\n\t\t\t\t\tcl_data->sensor_sts[i] = SENSOR_DISABLED;\n\t\t\t\tdev_dbg(dev, \"sid 0x%x (%s) status 0x%x\\n\",\n\t\t\t\t\tcl_data->sensor_idx[i],\n\t\t\t\t\tget_sensor_name(cl_data->sensor_idx[i]),\n\t\t\t\t\tcl_data->sensor_sts[i]);\n\t\t\t\tgoto cleanup;\n\t\t\t}\n\t\t}\n\t\tdev_dbg(dev, \"sid 0x%x (%s) status 0x%x\\n\",\n\t\t\tcl_data->sensor_idx[i], get_sensor_name(cl_data->sensor_idx[i]),\n\t\t\tcl_data->sensor_sts[i]);\n\t}\n\tif (mp2_ops->discovery_status && mp2_ops->discovery_status(privdata) == 0) {\n\t\tamd_sfh_hid_client_deinit(privdata);\n\t\tfor (i = 0; i < cl_data->num_hid_devices; i++) {\n\t\t\tdevm_kfree(dev, cl_data->feature_report[i]);\n\t\t\tdevm_kfree(dev, in_data->input_report[i]);\n\t\t\tdevm_kfree(dev, cl_data->report_descr[i]);\n\t\t}\n\t\tdev_warn(dev, \"Failed to discover, sensors not enabled\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\tschedule_delayed_work(&cl_data->work_buffer, msecs_to_jiffies(AMD_SFH_IDLE_LOOP));\n\treturn 0;\n\ncleanup:\n\tfor (i = 0; i < cl_data->num_hid_devices; i++) {\n\t\tif (in_data->sensor_virt_addr[i]) {\n\t\t\tdma_free_coherent(&privdata->pdev->dev, 8 * sizeof(int),\n\t\t\t\t\t  in_data->sensor_virt_addr[i],\n\t\t\t\t\t  cl_data->sensor_dma_addr[i]);\n\t\t}\n\t\tdevm_kfree(dev, cl_data->feature_report[i]);\n\t\tdevm_kfree(dev, in_data->input_report[i]);\n\t\tdevm_kfree(dev, cl_data->report_descr[i]);\n\t}\n\treturn rc;\n}",
                        "code_after_change": "int amd_sfh_hid_client_init(struct amd_mp2_dev *privdata)\n{\n\tstruct amd_input_data *in_data = &privdata->in_data;\n\tstruct amdtp_cl_data *cl_data = privdata->cl_data;\n\tstruct amd_mp2_ops *mp2_ops = privdata->mp2_ops;\n\tstruct amd_mp2_sensor_info info;\n\tstruct request_list *req_list;\n\tstruct device *dev;\n\tu32 feature_report_size;\n\tu32 input_report_size;\n\tint rc, i, status;\n\tu8 cl_idx;\n\n\treq_list = &cl_data->req_list;\n\tdev = &privdata->pdev->dev;\n\tamd_sfh_set_desc_ops(mp2_ops);\n\n\tmp2_ops->suspend = amd_sfh_suspend;\n\tmp2_ops->resume = amd_sfh_resume;\n\n\tcl_data->num_hid_devices = amd_mp2_get_sensor_num(privdata, &cl_data->sensor_idx[0]);\n\tif (cl_data->num_hid_devices == 0)\n\t\treturn -ENODEV;\n\n\tINIT_DELAYED_WORK(&cl_data->work, amd_sfh_work);\n\tINIT_DELAYED_WORK(&cl_data->work_buffer, amd_sfh_work_buffer);\n\tINIT_LIST_HEAD(&req_list->list);\n\tcl_data->in_data = in_data;\n\n\tfor (i = 0; i < cl_data->num_hid_devices; i++) {\n\t\tin_data->sensor_virt_addr[i] = dma_alloc_coherent(dev, sizeof(int) * 8,\n\t\t\t\t\t\t\t\t  &cl_data->sensor_dma_addr[i],\n\t\t\t\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!in_data->sensor_virt_addr[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tcl_data->sensor_sts[i] = SENSOR_DISABLED;\n\t\tcl_data->sensor_requested_cnt[i] = 0;\n\t\tcl_data->cur_hid_dev = i;\n\t\tcl_idx = cl_data->sensor_idx[i];\n\t\tcl_data->report_descr_sz[i] = mp2_ops->get_desc_sz(cl_idx, descr_size);\n\t\tif (!cl_data->report_descr_sz[i]) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tfeature_report_size = mp2_ops->get_desc_sz(cl_idx, feature_size);\n\t\tif (!feature_report_size) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tinput_report_size =  mp2_ops->get_desc_sz(cl_idx, input_size);\n\t\tif (!input_report_size) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tcl_data->feature_report[i] = devm_kzalloc(dev, feature_report_size, GFP_KERNEL);\n\t\tif (!cl_data->feature_report[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tin_data->input_report[i] = devm_kzalloc(dev, input_report_size, GFP_KERNEL);\n\t\tif (!in_data->input_report[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tinfo.period = AMD_SFH_IDLE_LOOP;\n\t\tinfo.sensor_idx = cl_idx;\n\t\tinfo.dma_address = cl_data->sensor_dma_addr[i];\n\n\t\tcl_data->report_descr[i] =\n\t\t\tdevm_kzalloc(dev, cl_data->report_descr_sz[i], GFP_KERNEL);\n\t\tif (!cl_data->report_descr[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\trc = mp2_ops->get_rep_desc(cl_idx, cl_data->report_descr[i]);\n\t\tif (rc)\n\t\t\treturn rc;\n\t\tmp2_ops->start(privdata, info);\n\t\tstatus = amd_sfh_wait_for_response\n\t\t\t\t(privdata, cl_data->sensor_idx[i], SENSOR_ENABLED);\n\t\tif (status == SENSOR_ENABLED) {\n\t\t\tcl_data->sensor_sts[i] = SENSOR_ENABLED;\n\t\t\trc = amdtp_hid_probe(cl_data->cur_hid_dev, cl_data);\n\t\t\tif (rc) {\n\t\t\t\tmp2_ops->stop(privdata, cl_data->sensor_idx[i]);\n\t\t\t\tstatus = amd_sfh_wait_for_response\n\t\t\t\t\t(privdata, cl_data->sensor_idx[i], SENSOR_DISABLED);\n\t\t\t\tif (status != SENSOR_ENABLED)\n\t\t\t\t\tcl_data->sensor_sts[i] = SENSOR_DISABLED;\n\t\t\t\tdev_dbg(dev, \"sid 0x%x (%s) status 0x%x\\n\",\n\t\t\t\t\tcl_data->sensor_idx[i],\n\t\t\t\t\tget_sensor_name(cl_data->sensor_idx[i]),\n\t\t\t\t\tcl_data->sensor_sts[i]);\n\t\t\t\tgoto cleanup;\n\t\t\t}\n\t\t}\n\t\tdev_dbg(dev, \"sid 0x%x (%s) status 0x%x\\n\",\n\t\t\tcl_data->sensor_idx[i], get_sensor_name(cl_data->sensor_idx[i]),\n\t\t\tcl_data->sensor_sts[i]);\n\t}\n\tif (mp2_ops->discovery_status && mp2_ops->discovery_status(privdata) == 0) {\n\t\tamd_sfh_hid_client_deinit(privdata);\n\t\tfor (i = 0; i < cl_data->num_hid_devices; i++) {\n\t\t\tdevm_kfree(dev, cl_data->feature_report[i]);\n\t\t\tdevm_kfree(dev, in_data->input_report[i]);\n\t\t\tdevm_kfree(dev, cl_data->report_descr[i]);\n\t\t}\n\t\tdev_warn(dev, \"Failed to discover, sensors not enabled\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\tschedule_delayed_work(&cl_data->work_buffer, msecs_to_jiffies(AMD_SFH_IDLE_LOOP));\n\treturn 0;\n\ncleanup:\n\tfor (i = 0; i < cl_data->num_hid_devices; i++) {\n\t\tif (in_data->sensor_virt_addr[i]) {\n\t\t\tdma_free_coherent(&privdata->pdev->dev, 8 * sizeof(int),\n\t\t\t\t\t  in_data->sensor_virt_addr[i],\n\t\t\t\t\t  cl_data->sensor_dma_addr[i]);\n\t\t}\n\t\tdevm_kfree(dev, cl_data->feature_report[i]);\n\t\tdevm_kfree(dev, in_data->input_report[i]);\n\t\tdevm_kfree(dev, cl_data->report_descr[i]);\n\t}\n\treturn rc;\n}",
                        "cve_id": "CVE-2023-3357"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int if_sdio_probe(struct sdio_func *func,\n\t\tconst struct sdio_device_id *id)\n{\n\tstruct if_sdio_card *card;\n\tstruct lbs_private *priv;\n\tint ret, i;\n\tunsigned int model;\n\tstruct if_sdio_packet *packet;\n\n\tfor (i = 0;i < func->card->num_info;i++) {\n\t\tif (sscanf(func->card->info[i],\n\t\t\t\t\"802.11 SDIO ID: %x\", &model) == 1)\n\t\t\tbreak;\n\t\tif (sscanf(func->card->info[i],\n\t\t\t\t\"ID: %x\", &model) == 1)\n\t\t\tbreak;\n\t\tif (!strcmp(func->card->info[i], \"IBIS Wireless SDIO Card\")) {\n\t\t\tmodel = MODEL_8385;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (i == func->card->num_info) {\n\t\tpr_err(\"unable to identify card model\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tcard = kzalloc(sizeof(struct if_sdio_card), GFP_KERNEL);\n\tif (!card)\n\t\treturn -ENOMEM;\n\n\tcard->func = func;\n\tcard->model = model;\n\n\tswitch (card->model) {\n\tcase MODEL_8385:\n\t\tcard->scratch_reg = IF_SDIO_SCRATCH_OLD;\n\t\tbreak;\n\tcase MODEL_8686:\n\t\tcard->scratch_reg = IF_SDIO_SCRATCH;\n\t\tbreak;\n\tcase MODEL_8688:\n\tdefault: /* for newer chipsets */\n\t\tcard->scratch_reg = IF_SDIO_FW_STATUS;\n\t\tbreak;\n\t}\n\n\tspin_lock_init(&card->lock);\n\tcard->workqueue = alloc_workqueue(\"libertas_sdio\", WQ_MEM_RECLAIM, 0);\n\tINIT_WORK(&card->packet_worker, if_sdio_host_to_card_worker);\n\tinit_waitqueue_head(&card->pwron_waitq);\n\n\t/* Check if we support this card */\n\tfor (i = 0; i < ARRAY_SIZE(fw_table); i++) {\n\t\tif (card->model == fw_table[i].model)\n\t\t\tbreak;\n\t}\n\tif (i == ARRAY_SIZE(fw_table)) {\n\t\tpr_err(\"unknown card model 0x%x\\n\", card->model);\n\t\tret = -ENODEV;\n\t\tgoto free;\n\t}\n\n\tsdio_set_drvdata(func, card);\n\n\tlbs_deb_sdio(\"class = 0x%X, vendor = 0x%X, \"\n\t\t\t\"device = 0x%X, model = 0x%X, ioport = 0x%X\\n\",\n\t\t\tfunc->class, func->vendor, func->device,\n\t\t\tmodel, (unsigned)card->ioport);\n\n\n\tpriv = lbs_add_card(card, &func->dev);\n\tif (IS_ERR(priv)) {\n\t\tret = PTR_ERR(priv);\n\t\tgoto free;\n\t}\n\n\tcard->priv = priv;\n\n\tpriv->card = card;\n\tpriv->hw_host_to_card = if_sdio_host_to_card;\n\tpriv->enter_deep_sleep = if_sdio_enter_deep_sleep;\n\tpriv->exit_deep_sleep = if_sdio_exit_deep_sleep;\n\tpriv->reset_deep_sleep_wakeup = if_sdio_reset_deep_sleep_wakeup;\n\tpriv->reset_card = if_sdio_reset_card;\n\tpriv->power_save = if_sdio_power_save;\n\tpriv->power_restore = if_sdio_power_restore;\n\tpriv->is_polling = !(func->card->host->caps & MMC_CAP_SDIO_IRQ);\n\tret = if_sdio_power_on(card);\n\tif (ret)\n\t\tgoto err_activate_card;\n\nout:\n\treturn ret;\n\nerr_activate_card:\n\tflush_workqueue(card->workqueue);\n\tlbs_remove_card(priv);\nfree:\n\tdestroy_workqueue(card->workqueue);\n\twhile (card->packets) {\n\t\tpacket = card->packets;\n\t\tcard->packets = card->packets->next;\n\t\tkfree(packet);\n\t}\n\n\tkfree(card);\n\n\tgoto out;\n}",
                        "code_after_change": "static int if_sdio_probe(struct sdio_func *func,\n\t\tconst struct sdio_device_id *id)\n{\n\tstruct if_sdio_card *card;\n\tstruct lbs_private *priv;\n\tint ret, i;\n\tunsigned int model;\n\tstruct if_sdio_packet *packet;\n\n\tfor (i = 0;i < func->card->num_info;i++) {\n\t\tif (sscanf(func->card->info[i],\n\t\t\t\t\"802.11 SDIO ID: %x\", &model) == 1)\n\t\t\tbreak;\n\t\tif (sscanf(func->card->info[i],\n\t\t\t\t\"ID: %x\", &model) == 1)\n\t\t\tbreak;\n\t\tif (!strcmp(func->card->info[i], \"IBIS Wireless SDIO Card\")) {\n\t\t\tmodel = MODEL_8385;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (i == func->card->num_info) {\n\t\tpr_err(\"unable to identify card model\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tcard = kzalloc(sizeof(struct if_sdio_card), GFP_KERNEL);\n\tif (!card)\n\t\treturn -ENOMEM;\n\n\tcard->func = func;\n\tcard->model = model;\n\n\tswitch (card->model) {\n\tcase MODEL_8385:\n\t\tcard->scratch_reg = IF_SDIO_SCRATCH_OLD;\n\t\tbreak;\n\tcase MODEL_8686:\n\t\tcard->scratch_reg = IF_SDIO_SCRATCH;\n\t\tbreak;\n\tcase MODEL_8688:\n\tdefault: /* for newer chipsets */\n\t\tcard->scratch_reg = IF_SDIO_FW_STATUS;\n\t\tbreak;\n\t}\n\n\tspin_lock_init(&card->lock);\n\tcard->workqueue = alloc_workqueue(\"libertas_sdio\", WQ_MEM_RECLAIM, 0);\n\tif (unlikely(!card->workqueue)) {\n\t\tret = -ENOMEM;\n\t\tgoto err_queue;\n\t}\n\tINIT_WORK(&card->packet_worker, if_sdio_host_to_card_worker);\n\tinit_waitqueue_head(&card->pwron_waitq);\n\n\t/* Check if we support this card */\n\tfor (i = 0; i < ARRAY_SIZE(fw_table); i++) {\n\t\tif (card->model == fw_table[i].model)\n\t\t\tbreak;\n\t}\n\tif (i == ARRAY_SIZE(fw_table)) {\n\t\tpr_err(\"unknown card model 0x%x\\n\", card->model);\n\t\tret = -ENODEV;\n\t\tgoto free;\n\t}\n\n\tsdio_set_drvdata(func, card);\n\n\tlbs_deb_sdio(\"class = 0x%X, vendor = 0x%X, \"\n\t\t\t\"device = 0x%X, model = 0x%X, ioport = 0x%X\\n\",\n\t\t\tfunc->class, func->vendor, func->device,\n\t\t\tmodel, (unsigned)card->ioport);\n\n\n\tpriv = lbs_add_card(card, &func->dev);\n\tif (IS_ERR(priv)) {\n\t\tret = PTR_ERR(priv);\n\t\tgoto free;\n\t}\n\n\tcard->priv = priv;\n\n\tpriv->card = card;\n\tpriv->hw_host_to_card = if_sdio_host_to_card;\n\tpriv->enter_deep_sleep = if_sdio_enter_deep_sleep;\n\tpriv->exit_deep_sleep = if_sdio_exit_deep_sleep;\n\tpriv->reset_deep_sleep_wakeup = if_sdio_reset_deep_sleep_wakeup;\n\tpriv->reset_card = if_sdio_reset_card;\n\tpriv->power_save = if_sdio_power_save;\n\tpriv->power_restore = if_sdio_power_restore;\n\tpriv->is_polling = !(func->card->host->caps & MMC_CAP_SDIO_IRQ);\n\tret = if_sdio_power_on(card);\n\tif (ret)\n\t\tgoto err_activate_card;\n\nout:\n\treturn ret;\n\nerr_activate_card:\n\tflush_workqueue(card->workqueue);\n\tlbs_remove_card(priv);\nfree:\n\tdestroy_workqueue(card->workqueue);\nerr_queue:\n\twhile (card->packets) {\n\t\tpacket = card->packets;\n\t\tcard->packets = card->packets->next;\n\t\tkfree(packet);\n\t}\n\n\tkfree(card);\n\n\tgoto out;\n}",
                        "cve_id": "CVE-2019-16232"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 2578,
            "cve_id": "CVE-2020-12364",
            "code_snippet": "static void __guc_ads_init(struct intel_guc *guc)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tstruct __guc_ads_blob *blob = guc->ads_blob;\n\tconst u32 skipped_size = LRC_PPHWSP_SZ * PAGE_SIZE + LR_HW_CONTEXT_SIZE;\n\tu32 base;\n\tu8 engine_class;\n\n\t/* GuC scheduling policies */\n\tguc_policies_init(&blob->policies);\n\n\t/*\n\t * GuC expects a per-engine-class context image and size\n\t * (minus hwsp and ring context). The context image will be\n\t * used to reinitialize engines after a reset. It must exist\n\t * and be pinned in the GGTT, so that the address won't change after\n\t * we have told GuC where to find it. The context size will be used\n\t * to validate that the LRC base + size fall within allowed GGTT.\n\t */\n\tfor (engine_class = 0; engine_class <= MAX_ENGINE_CLASS; ++engine_class) {\n\t\tif (engine_class == OTHER_CLASS)\n\t\t\tcontinue;\n\t\t/*\n\t\t * TODO: Set context pointer to default state to allow\n\t\t * GuC to re-init guilty contexts after internal reset.\n\t\t */\n\t\tblob->ads.golden_context_lrca[engine_class] = 0;\n\t\tblob->ads.eng_state_size[engine_class] =\n\t\t\tintel_engine_context_size(guc_to_gt(guc),\n\t\t\t\t\t\t  engine_class) -\n\t\t\tskipped_size;\n\t}\n\n\t/* System info */\n\tblob->system_info.slice_enabled = hweight8(gt->info.sseu.slice_mask);\n\tblob->system_info.rcs_enabled = 1;\n\tblob->system_info.bcs_enabled = 1;\n\n\tblob->system_info.vdbox_enable_mask = VDBOX_MASK(gt);\n\tblob->system_info.vebox_enable_mask = VEBOX_MASK(gt);\n\tblob->system_info.vdbox_sfc_support_mask = gt->info.vdbox_sfc_access;\n\n\tbase = intel_guc_ggtt_offset(guc, guc->ads_vma);\n\n\t/* Clients info  */\n\tguc_ct_pool_entries_init(blob->ct_pool, ARRAY_SIZE(blob->ct_pool));\n\n\tblob->clients_info.clients_num = 1;\n\tblob->clients_info.ct_pool_addr = base + ptr_offset(blob, ct_pool);\n\tblob->clients_info.ct_pool_count = ARRAY_SIZE(blob->ct_pool);\n\n\t/* ADS */\n\tblob->ads.scheduler_policies = base + ptr_offset(blob, policies);\n\tblob->ads.reg_state_buffer = base + ptr_offset(blob, reg_state_buffer);\n\tblob->ads.reg_state_addr = base + ptr_offset(blob, reg_state);\n\tblob->ads.gt_system_info = base + ptr_offset(blob, system_info);\n\tblob->ads.clients_info = base + ptr_offset(blob, clients_info);\n\n\ti915_gem_object_flush_map(guc->ads_vma->obj);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int intel_engine_setup(struct intel_gt *gt, enum intel_engine_id id)\n{\n\tconst struct engine_info *info = &intel_engines[id];\n\tstruct drm_i915_private *i915 = gt->i915;\n\tstruct intel_engine_cs *engine;\n\n\tBUILD_BUG_ON(MAX_ENGINE_CLASS >= BIT(GEN11_ENGINE_CLASS_WIDTH));\n\tBUILD_BUG_ON(MAX_ENGINE_INSTANCE >= BIT(GEN11_ENGINE_INSTANCE_WIDTH));\n\n\tif (GEM_DEBUG_WARN_ON(id >= ARRAY_SIZE(gt->engine)))\n\t\treturn -EINVAL;\n\n\tif (GEM_DEBUG_WARN_ON(info->class > MAX_ENGINE_CLASS))\n\t\treturn -EINVAL;\n\n\tif (GEM_DEBUG_WARN_ON(info->instance > MAX_ENGINE_INSTANCE))\n\t\treturn -EINVAL;\n\n\tif (GEM_DEBUG_WARN_ON(gt->engine_class[info->class][info->instance]))\n\t\treturn -EINVAL;\n\n\tengine = kzalloc(sizeof(*engine), GFP_KERNEL);\n\tif (!engine)\n\t\treturn -ENOMEM;\n\n\tBUILD_BUG_ON(BITS_PER_TYPE(engine->mask) < I915_NUM_ENGINES);\n\n\tengine->id = id;\n\tengine->legacy_idx = INVALID_ENGINE;\n\tengine->mask = BIT(id);\n\tengine->i915 = i915;\n\tengine->gt = gt;\n\tengine->uncore = gt->uncore;\n\tengine->hw_id = engine->guc_id = info->hw_id;\n\tengine->mmio_base = __engine_mmio_base(i915, info->mmio_bases);\n\n\tengine->class = info->class;\n\tengine->instance = info->instance;\n\t__sprint_engine_name(engine);\n\n\tengine->props.heartbeat_interval_ms =\n\t\tCONFIG_DRM_I915_HEARTBEAT_INTERVAL;\n\tengine->props.max_busywait_duration_ns =\n\t\tCONFIG_DRM_I915_MAX_REQUEST_BUSYWAIT;\n\tengine->props.preempt_timeout_ms =\n\t\tCONFIG_DRM_I915_PREEMPT_TIMEOUT;\n\tengine->props.stop_timeout_ms =\n\t\tCONFIG_DRM_I915_STOP_TIMEOUT;\n\tengine->props.timeslice_duration_ms =\n\t\tCONFIG_DRM_I915_TIMESLICE_DURATION;\n\n\t/* Override to uninterruptible for OpenCL workloads. */\n\tif (INTEL_GEN(i915) == 12 && engine->class == RENDER_CLASS)\n\t\tengine->props.preempt_timeout_ms = 0;\n\n\tengine->defaults = engine->props; /* never to change again */\n\n\tengine->context_size = intel_engine_context_size(gt, engine->class);\n\tif (WARN_ON(engine->context_size > BIT(20)))\n\t\tengine->context_size = 0;\n\tif (engine->context_size)\n\t\tDRIVER_CAPS(i915)->has_logical_contexts = true;\n\n\t/* Nothing to do here, execute in order of dependencies */\n\tengine->schedule = NULL;\n\n\tewma__engine_latency_init(&engine->latency);\n\tseqlock_init(&engine->stats.lock);\n\n\tATOMIC_INIT_NOTIFIER_HEAD(&engine->context_status_notifier);\n\n\t/* Scrub mmio state on takeover */\n\tintel_engine_sanitize_mmio(engine);\n\n\tgt->engine_class[info->class][info->instance] = engine;\n\tgt->engine[id] = engine;\n\n\treturn 0;\n}",
                        "code_after_change": "static int intel_engine_setup(struct intel_gt *gt, enum intel_engine_id id)\n{\n\tconst struct engine_info *info = &intel_engines[id];\n\tstruct drm_i915_private *i915 = gt->i915;\n\tstruct intel_engine_cs *engine;\n\n\tBUILD_BUG_ON(MAX_ENGINE_CLASS >= BIT(GEN11_ENGINE_CLASS_WIDTH));\n\tBUILD_BUG_ON(MAX_ENGINE_INSTANCE >= BIT(GEN11_ENGINE_INSTANCE_WIDTH));\n\n\tif (GEM_DEBUG_WARN_ON(id >= ARRAY_SIZE(gt->engine)))\n\t\treturn -EINVAL;\n\n\tif (GEM_DEBUG_WARN_ON(info->class > MAX_ENGINE_CLASS))\n\t\treturn -EINVAL;\n\n\tif (GEM_DEBUG_WARN_ON(info->instance > MAX_ENGINE_INSTANCE))\n\t\treturn -EINVAL;\n\n\tif (GEM_DEBUG_WARN_ON(gt->engine_class[info->class][info->instance]))\n\t\treturn -EINVAL;\n\n\tengine = kzalloc(sizeof(*engine), GFP_KERNEL);\n\tif (!engine)\n\t\treturn -ENOMEM;\n\n\tBUILD_BUG_ON(BITS_PER_TYPE(engine->mask) < I915_NUM_ENGINES);\n\n\tengine->id = id;\n\tengine->legacy_idx = INVALID_ENGINE;\n\tengine->mask = BIT(id);\n\tengine->i915 = i915;\n\tengine->gt = gt;\n\tengine->uncore = gt->uncore;\n\tengine->mmio_base = __engine_mmio_base(i915, info->mmio_bases);\n\tengine->hw_id = info->hw_id;\n\tengine->guc_id = MAKE_GUC_ID(info->class, info->instance);\n\n\tengine->class = info->class;\n\tengine->instance = info->instance;\n\t__sprint_engine_name(engine);\n\n\tengine->props.heartbeat_interval_ms =\n\t\tCONFIG_DRM_I915_HEARTBEAT_INTERVAL;\n\tengine->props.max_busywait_duration_ns =\n\t\tCONFIG_DRM_I915_MAX_REQUEST_BUSYWAIT;\n\tengine->props.preempt_timeout_ms =\n\t\tCONFIG_DRM_I915_PREEMPT_TIMEOUT;\n\tengine->props.stop_timeout_ms =\n\t\tCONFIG_DRM_I915_STOP_TIMEOUT;\n\tengine->props.timeslice_duration_ms =\n\t\tCONFIG_DRM_I915_TIMESLICE_DURATION;\n\n\t/* Override to uninterruptible for OpenCL workloads. */\n\tif (INTEL_GEN(i915) == 12 && engine->class == RENDER_CLASS)\n\t\tengine->props.preempt_timeout_ms = 0;\n\n\tengine->defaults = engine->props; /* never to change again */\n\n\tengine->context_size = intel_engine_context_size(gt, engine->class);\n\tif (WARN_ON(engine->context_size > BIT(20)))\n\t\tengine->context_size = 0;\n\tif (engine->context_size)\n\t\tDRIVER_CAPS(i915)->has_logical_contexts = true;\n\n\t/* Nothing to do here, execute in order of dependencies */\n\tengine->schedule = NULL;\n\n\tewma__engine_latency_init(&engine->latency);\n\tseqlock_init(&engine->stats.lock);\n\n\tATOMIC_INIT_NOTIFIER_HEAD(&engine->context_status_notifier);\n\n\t/* Scrub mmio state on takeover */\n\tintel_engine_sanitize_mmio(engine);\n\n\tgt->engine_class[info->class][info->instance] = engine;\n\tgt->engine[id] = engine;\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2020-12364"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int intel_guc_ads_create(struct intel_guc *guc)\n{\n\tconst u32 size = PAGE_ALIGN(sizeof(struct __guc_ads_blob));\n\tint ret;\n\n\tGEM_BUG_ON(guc->ads_vma);\n\n\tret = intel_guc_allocate_and_map_vma(guc, size, &guc->ads_vma,\n\t\t\t\t\t     (void **)&guc->ads_blob);\n\n\tif (ret)\n\t\treturn ret;\n\n\t__guc_ads_init(guc);\n\n\treturn 0;\n}",
                        "code_after_change": "int intel_guc_ads_create(struct intel_guc *guc)\n{\n\tu32 size;\n\tint ret;\n\n\tGEM_BUG_ON(guc->ads_vma);\n\n\tsize = guc_ads_blob_size(guc);\n\n\tret = intel_guc_allocate_and_map_vma(guc, size, &guc->ads_vma,\n\t\t\t\t\t     (void **)&guc->ads_blob);\n\tif (ret)\n\t\treturn ret;\n\n\t__guc_ads_init(guc);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2020-12364"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int intel_uc_fw_fetch(struct intel_uc_fw *uc_fw)\n{\n\tstruct drm_i915_private *i915 = __uc_fw_to_gt(uc_fw)->i915;\n\tstruct device *dev = i915->drm.dev;\n\tstruct drm_i915_gem_object *obj;\n\tconst struct firmware *fw = NULL;\n\tstruct uc_css_header *css;\n\tsize_t size;\n\tint err;\n\n\tGEM_BUG_ON(!i915->wopcm.size);\n\tGEM_BUG_ON(!intel_uc_fw_is_enabled(uc_fw));\n\n\terr = i915_inject_probe_error(i915, -ENXIO);\n\tif (err)\n\t\tgoto fail;\n\n\t__force_fw_fetch_failures(uc_fw, -EINVAL);\n\t__force_fw_fetch_failures(uc_fw, -ESTALE);\n\n\terr = request_firmware(&fw, uc_fw->path, dev);\n\tif (err)\n\t\tgoto fail;\n\n\t/* Check the size of the blob before examining buffer contents */\n\tif (unlikely(fw->size < sizeof(struct uc_css_header))) {\n\t\tdrm_warn(&i915->drm, \"%s firmware %s: invalid size: %zu < %zu\\n\",\n\t\t\t intel_uc_fw_type_repr(uc_fw->type), uc_fw->path,\n\t\t\t fw->size, sizeof(struct uc_css_header));\n\t\terr = -ENODATA;\n\t\tgoto fail;\n\t}\n\n\tcss = (struct uc_css_header *)fw->data;\n\n\t/* Check integrity of size values inside CSS header */\n\tsize = (css->header_size_dw - css->key_size_dw - css->modulus_size_dw -\n\t\tcss->exponent_size_dw) * sizeof(u32);\n\tif (unlikely(size != sizeof(struct uc_css_header))) {\n\t\tdrm_warn(&i915->drm,\n\t\t\t \"%s firmware %s: unexpected header size: %zu != %zu\\n\",\n\t\t\t intel_uc_fw_type_repr(uc_fw->type), uc_fw->path,\n\t\t\t fw->size, sizeof(struct uc_css_header));\n\t\terr = -EPROTO;\n\t\tgoto fail;\n\t}\n\n\t/* uCode size must calculated from other sizes */\n\tuc_fw->ucode_size = (css->size_dw - css->header_size_dw) * sizeof(u32);\n\n\t/* now RSA */\n\tif (unlikely(css->key_size_dw != UOS_RSA_SCRATCH_COUNT)) {\n\t\tdrm_warn(&i915->drm, \"%s firmware %s: unexpected key size: %u != %u\\n\",\n\t\t\t intel_uc_fw_type_repr(uc_fw->type), uc_fw->path,\n\t\t\t css->key_size_dw, UOS_RSA_SCRATCH_COUNT);\n\t\terr = -EPROTO;\n\t\tgoto fail;\n\t}\n\tuc_fw->rsa_size = css->key_size_dw * sizeof(u32);\n\n\t/* At least, it should have header, uCode and RSA. Size of all three. */\n\tsize = sizeof(struct uc_css_header) + uc_fw->ucode_size + uc_fw->rsa_size;\n\tif (unlikely(fw->size < size)) {\n\t\tdrm_warn(&i915->drm, \"%s firmware %s: invalid size: %zu < %zu\\n\",\n\t\t\t intel_uc_fw_type_repr(uc_fw->type), uc_fw->path,\n\t\t\t fw->size, size);\n\t\terr = -ENOEXEC;\n\t\tgoto fail;\n\t}\n\n\t/* Sanity check whether this fw is not larger than whole WOPCM memory */\n\tsize = __intel_uc_fw_get_upload_size(uc_fw);\n\tif (unlikely(size >= i915->wopcm.size)) {\n\t\tdrm_warn(&i915->drm, \"%s firmware %s: invalid size: %zu > %zu\\n\",\n\t\t\t intel_uc_fw_type_repr(uc_fw->type), uc_fw->path,\n\t\t\t size, (size_t)i915->wopcm.size);\n\t\terr = -E2BIG;\n\t\tgoto fail;\n\t}\n\n\t/* Get version numbers from the CSS header */\n\tuc_fw->major_ver_found = FIELD_GET(CSS_SW_VERSION_UC_MAJOR,\n\t\t\t\t\t   css->sw_version);\n\tuc_fw->minor_ver_found = FIELD_GET(CSS_SW_VERSION_UC_MINOR,\n\t\t\t\t\t   css->sw_version);\n\n\tif (uc_fw->major_ver_found != uc_fw->major_ver_wanted ||\n\t    uc_fw->minor_ver_found < uc_fw->minor_ver_wanted) {\n\t\tdrm_notice(&i915->drm, \"%s firmware %s: unexpected version: %u.%u != %u.%u\\n\",\n\t\t\t   intel_uc_fw_type_repr(uc_fw->type), uc_fw->path,\n\t\t\t   uc_fw->major_ver_found, uc_fw->minor_ver_found,\n\t\t\t   uc_fw->major_ver_wanted, uc_fw->minor_ver_wanted);\n\t\tif (!intel_uc_fw_is_overridden(uc_fw)) {\n\t\t\terr = -ENOEXEC;\n\t\t\tgoto fail;\n\t\t}\n\t}\n\n\tobj = i915_gem_object_create_shmem_from_data(i915, fw->data, fw->size);\n\tif (IS_ERR(obj)) {\n\t\terr = PTR_ERR(obj);\n\t\tgoto fail;\n\t}\n\n\tuc_fw->obj = obj;\n\tuc_fw->size = fw->size;\n\tintel_uc_fw_change_status(uc_fw, INTEL_UC_FIRMWARE_AVAILABLE);\n\n\trelease_firmware(fw);\n\treturn 0;\n\nfail:\n\tintel_uc_fw_change_status(uc_fw, err == -ENOENT ?\n\t\t\t\t  INTEL_UC_FIRMWARE_MISSING :\n\t\t\t\t  INTEL_UC_FIRMWARE_ERROR);\n\n\tdrm_notice(&i915->drm, \"%s firmware %s: fetch failed with error %d\\n\",\n\t\t   intel_uc_fw_type_repr(uc_fw->type), uc_fw->path, err);\n\tdrm_info(&i915->drm, \"%s firmware(s) can be downloaded from %s\\n\",\n\t\t intel_uc_fw_type_repr(uc_fw->type), INTEL_UC_FIRMWARE_URL);\n\n\trelease_firmware(fw);\t\t/* OK even if fw is NULL */\n\treturn err;\n}",
                        "code_after_change": "int intel_uc_fw_fetch(struct intel_uc_fw *uc_fw)\n{\n\tstruct drm_i915_private *i915 = __uc_fw_to_gt(uc_fw)->i915;\n\tstruct device *dev = i915->drm.dev;\n\tstruct drm_i915_gem_object *obj;\n\tconst struct firmware *fw = NULL;\n\tstruct uc_css_header *css;\n\tsize_t size;\n\tint err;\n\n\tGEM_BUG_ON(!i915->wopcm.size);\n\tGEM_BUG_ON(!intel_uc_fw_is_enabled(uc_fw));\n\n\terr = i915_inject_probe_error(i915, -ENXIO);\n\tif (err)\n\t\tgoto fail;\n\n\t__force_fw_fetch_failures(uc_fw, -EINVAL);\n\t__force_fw_fetch_failures(uc_fw, -ESTALE);\n\n\terr = request_firmware(&fw, uc_fw->path, dev);\n\tif (err)\n\t\tgoto fail;\n\n\t/* Check the size of the blob before examining buffer contents */\n\tif (unlikely(fw->size < sizeof(struct uc_css_header))) {\n\t\tdrm_warn(&i915->drm, \"%s firmware %s: invalid size: %zu < %zu\\n\",\n\t\t\t intel_uc_fw_type_repr(uc_fw->type), uc_fw->path,\n\t\t\t fw->size, sizeof(struct uc_css_header));\n\t\terr = -ENODATA;\n\t\tgoto fail;\n\t}\n\n\tcss = (struct uc_css_header *)fw->data;\n\n\t/* Check integrity of size values inside CSS header */\n\tsize = (css->header_size_dw - css->key_size_dw - css->modulus_size_dw -\n\t\tcss->exponent_size_dw) * sizeof(u32);\n\tif (unlikely(size != sizeof(struct uc_css_header))) {\n\t\tdrm_warn(&i915->drm,\n\t\t\t \"%s firmware %s: unexpected header size: %zu != %zu\\n\",\n\t\t\t intel_uc_fw_type_repr(uc_fw->type), uc_fw->path,\n\t\t\t fw->size, sizeof(struct uc_css_header));\n\t\terr = -EPROTO;\n\t\tgoto fail;\n\t}\n\n\t/* uCode size must calculated from other sizes */\n\tuc_fw->ucode_size = (css->size_dw - css->header_size_dw) * sizeof(u32);\n\n\t/* now RSA */\n\tif (unlikely(css->key_size_dw != UOS_RSA_SCRATCH_COUNT)) {\n\t\tdrm_warn(&i915->drm, \"%s firmware %s: unexpected key size: %u != %u\\n\",\n\t\t\t intel_uc_fw_type_repr(uc_fw->type), uc_fw->path,\n\t\t\t css->key_size_dw, UOS_RSA_SCRATCH_COUNT);\n\t\terr = -EPROTO;\n\t\tgoto fail;\n\t}\n\tuc_fw->rsa_size = css->key_size_dw * sizeof(u32);\n\n\t/* At least, it should have header, uCode and RSA. Size of all three. */\n\tsize = sizeof(struct uc_css_header) + uc_fw->ucode_size + uc_fw->rsa_size;\n\tif (unlikely(fw->size < size)) {\n\t\tdrm_warn(&i915->drm, \"%s firmware %s: invalid size: %zu < %zu\\n\",\n\t\t\t intel_uc_fw_type_repr(uc_fw->type), uc_fw->path,\n\t\t\t fw->size, size);\n\t\terr = -ENOEXEC;\n\t\tgoto fail;\n\t}\n\n\t/* Sanity check whether this fw is not larger than whole WOPCM memory */\n\tsize = __intel_uc_fw_get_upload_size(uc_fw);\n\tif (unlikely(size >= i915->wopcm.size)) {\n\t\tdrm_warn(&i915->drm, \"%s firmware %s: invalid size: %zu > %zu\\n\",\n\t\t\t intel_uc_fw_type_repr(uc_fw->type), uc_fw->path,\n\t\t\t size, (size_t)i915->wopcm.size);\n\t\terr = -E2BIG;\n\t\tgoto fail;\n\t}\n\n\t/* Get version numbers from the CSS header */\n\tuc_fw->major_ver_found = FIELD_GET(CSS_SW_VERSION_UC_MAJOR,\n\t\t\t\t\t   css->sw_version);\n\tuc_fw->minor_ver_found = FIELD_GET(CSS_SW_VERSION_UC_MINOR,\n\t\t\t\t\t   css->sw_version);\n\n\tif (uc_fw->major_ver_found != uc_fw->major_ver_wanted ||\n\t    uc_fw->minor_ver_found < uc_fw->minor_ver_wanted) {\n\t\tdrm_notice(&i915->drm, \"%s firmware %s: unexpected version: %u.%u != %u.%u\\n\",\n\t\t\t   intel_uc_fw_type_repr(uc_fw->type), uc_fw->path,\n\t\t\t   uc_fw->major_ver_found, uc_fw->minor_ver_found,\n\t\t\t   uc_fw->major_ver_wanted, uc_fw->minor_ver_wanted);\n\t\tif (!intel_uc_fw_is_overridden(uc_fw)) {\n\t\t\terr = -ENOEXEC;\n\t\t\tgoto fail;\n\t\t}\n\t}\n\n\tif (uc_fw->type == INTEL_UC_FW_TYPE_GUC)\n\t\tuc_fw->private_data_size = css->private_data_size;\n\n\tobj = i915_gem_object_create_shmem_from_data(i915, fw->data, fw->size);\n\tif (IS_ERR(obj)) {\n\t\terr = PTR_ERR(obj);\n\t\tgoto fail;\n\t}\n\n\tuc_fw->obj = obj;\n\tuc_fw->size = fw->size;\n\tintel_uc_fw_change_status(uc_fw, INTEL_UC_FIRMWARE_AVAILABLE);\n\n\trelease_firmware(fw);\n\treturn 0;\n\nfail:\n\tintel_uc_fw_change_status(uc_fw, err == -ENOENT ?\n\t\t\t\t  INTEL_UC_FIRMWARE_MISSING :\n\t\t\t\t  INTEL_UC_FIRMWARE_ERROR);\n\n\tdrm_notice(&i915->drm, \"%s firmware %s: fetch failed with error %d\\n\",\n\t\t   intel_uc_fw_type_repr(uc_fw->type), uc_fw->path, err);\n\tdrm_info(&i915->drm, \"%s firmware(s) can be downloaded from %s\\n\",\n\t\t intel_uc_fw_type_repr(uc_fw->type), INTEL_UC_FIRMWARE_URL);\n\n\trelease_firmware(fw);\t\t/* OK even if fw is NULL */\n\treturn err;\n}",
                        "cve_id": "CVE-2020-12364"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "void intel_guc_ads_reset(struct intel_guc *guc)\n{\n\tif (!guc->ads_vma)\n\t\treturn;\n\t__guc_ads_init(guc);\n}",
                        "code_after_change": "void intel_guc_ads_reset(struct intel_guc *guc)\n{\n\tif (!guc->ads_vma)\n\t\treturn;\n\n\t__guc_ads_init(guc);\n\n\tguc_ads_private_data_reset(guc);\n}",
                        "cve_id": "CVE-2020-12364"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2628,
            "cve_id": "CVE-2020-14356",
            "code_snippet": "void cgroup_sk_free(struct sock_cgroup_data *skcd)\n{\n\tstruct cgroup *cgrp = sock_cgroup_ptr(skcd);\n\n\tcgroup_bpf_put(cgrp);\n\tcgroup_put(cgrp);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "void cgroup_sk_alloc(struct sock_cgroup_data *skcd)\n{\n\tif (cgroup_sk_alloc_disabled)\n\t\treturn;\n\n\t/* Socket clone path */\n\tif (skcd->val) {\n\t\t/*\n\t\t * We might be cloning a socket which is left in an empty\n\t\t * cgroup and the cgroup might have already been rmdir'd.\n\t\t * Don't use cgroup_get_live().\n\t\t */\n\t\tcgroup_get(sock_cgroup_ptr(skcd));\n\t\tcgroup_bpf_get(sock_cgroup_ptr(skcd));\n\t\treturn;\n\t}\n\n\t/* Don't associate the sock with unrelated interrupted task's cgroup. */\n\tif (in_interrupt())\n\t\treturn;\n\n\trcu_read_lock();\n\n\twhile (true) {\n\t\tstruct css_set *cset;\n\n\t\tcset = task_css_set(current);\n\t\tif (likely(cgroup_tryget(cset->dfl_cgrp))) {\n\t\t\tskcd->val = (unsigned long)cset->dfl_cgrp;\n\t\t\tcgroup_bpf_get(cset->dfl_cgrp);\n\t\t\tbreak;\n\t\t}\n\t\tcpu_relax();\n\t}\n\n\trcu_read_unlock();\n}",
                        "code_after_change": "static inline struct cgroup *sock_cgroup_ptr(struct sock_cgroup_data *skcd)\n{\n#if defined(CONFIG_CGROUP_NET_PRIO) || defined(CONFIG_CGROUP_NET_CLASSID)\n\tunsigned long v;\n\n\t/*\n\t * @skcd->val is 64bit but the following is safe on 32bit too as we\n\t * just need the lower ulong to be written and read atomically.\n\t */\n\tv = READ_ONCE(skcd->val);\n\n\tif (v & 3)\n\t\treturn &cgrp_dfl_root.cgrp;\n\n\treturn (struct cgroup *)(unsigned long)v ?: &cgrp_dfl_root.cgrp;\n#else\n\treturn (struct cgroup *)(unsigned long)skcd->val;\n#endif\n}",
                        "cve_id": "CVE-2020-14356"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2699,
            "cve_id": "CVE-2020-25285",
            "code_snippet": "static int hugetlb_sysctl_handler_common(bool obey_mempolicy,\n\t\t\t struct ctl_table *table, int write,\n\t\t\t void *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp = h->max_huge_pages;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\ttable->data = &tmp;\n\ttable->maxlen = sizeof(unsigned long);\n\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write)\n\t\tret = __nr_hugepages_store_common(obey_mempolicy, h,\n\t\t\t\t\t\t  NUMA_NO_NODE, tmp, *length);\nout:\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int hugetlb_overcommit_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\ttmp = h->nr_overcommit_huge_pages;\n\n\tif (write && hstate_is_gigantic(h))\n\t\treturn -EINVAL;\n\n\ttable->data = &tmp;\n\ttable->maxlen = sizeof(unsigned long);\n\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write) {\n\t\tspin_lock(&hugetlb_lock);\n\t\th->nr_overcommit_huge_pages = tmp;\n\t\tspin_unlock(&hugetlb_lock);\n\t}\nout:\n\treturn ret;\n}",
                        "code_after_change": "int hugetlb_overcommit_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\ttmp = h->nr_overcommit_huge_pages;\n\n\tif (write && hstate_is_gigantic(h))\n\t\treturn -EINVAL;\n\n\tret = proc_hugetlb_doulongvec_minmax(table, write, buffer, length, ppos,\n\t\t\t\t\t     &tmp);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write) {\n\t\tspin_lock(&hugetlb_lock);\n\t\th->nr_overcommit_huge_pages = tmp;\n\t\tspin_unlock(&hugetlb_lock);\n\t}\nout:\n\treturn ret;\n}",
                        "cve_id": "CVE-2020-25285"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2734,
            "cve_id": "CVE-2020-27675",
            "code_snippet": "static void clear_evtchn_to_irq_row(unsigned row)\n{\n\tunsigned col;\n\n\tfor (col = 0; col < EVTCHN_PER_ROW; col++)\n\t\tevtchn_to_irq[row][col] = -1;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int set_evtchn_to_irq(evtchn_port_t evtchn, unsigned int irq)\n{\n\tunsigned row;\n\tunsigned col;\n\n\tif (evtchn >= xen_evtchn_max_channels())\n\t\treturn -EINVAL;\n\n\trow = EVTCHN_ROW(evtchn);\n\tcol = EVTCHN_COL(evtchn);\n\n\tif (evtchn_to_irq[row] == NULL) {\n\t\t/* Unallocated irq entries return -1 anyway */\n\t\tif (irq == -1)\n\t\t\treturn 0;\n\n\t\tevtchn_to_irq[row] = (int *)get_zeroed_page(GFP_KERNEL);\n\t\tif (evtchn_to_irq[row] == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\tclear_evtchn_to_irq_row(row);\n\t}\n\n\tevtchn_to_irq[row][col] = irq;\n\treturn 0;\n}",
                        "code_after_change": "static int set_evtchn_to_irq(evtchn_port_t evtchn, unsigned int irq)\n{\n\tunsigned row;\n\tunsigned col;\n\n\tif (evtchn >= xen_evtchn_max_channels())\n\t\treturn -EINVAL;\n\n\trow = EVTCHN_ROW(evtchn);\n\tcol = EVTCHN_COL(evtchn);\n\n\tif (evtchn_to_irq[row] == NULL) {\n\t\t/* Unallocated irq entries return -1 anyway */\n\t\tif (irq == -1)\n\t\t\treturn 0;\n\n\t\tevtchn_to_irq[row] = (int *)get_zeroed_page(GFP_KERNEL);\n\t\tif (evtchn_to_irq[row] == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\tclear_evtchn_to_irq_row(row);\n\t}\n\n\tWRITE_ONCE(evtchn_to_irq[row][col], irq);\n\treturn 0;\n}",
                        "cve_id": "CVE-2020-27675"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2735,
            "cve_id": "CVE-2020-27675",
            "code_snippet": "static void xen_free_irq(unsigned irq)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\n\tif (WARN_ON(!info))\n\t\treturn;\n\n\tlist_del(&info->list);\n\n\tset_info_for_irq(irq, NULL);\n\n\tWARN_ON(info->refcnt > 0);\n\n\tkfree(info);\n\n\t/* Legacy IRQ descriptors are managed by the arch. */\n\tif (irq < nr_legacy_irqs())\n\t\treturn;\n\n\tirq_free_desc(irq);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "evtchn_port_t evtchn_from_irq(unsigned irq)\n{\n\tif (WARN(irq >= nr_irqs, \"Invalid irq %d!\\n\", irq))\n\t\treturn 0;\n\n\treturn info_for_irq(irq)->evtchn;\n}",
                        "code_after_change": "evtchn_port_t evtchn_from_irq(unsigned irq)\n{\n\tconst struct irq_info *info = NULL;\n\n\tif (likely(irq < nr_irqs))\n\t\tinfo = info_for_irq(irq);\n\tif (!info)\n\t\treturn 0;\n\n\treturn info->evtchn;\n}",
                        "cve_id": "CVE-2020-27675"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2752,
            "cve_id": "CVE-2020-27830",
            "code_snippet": "static int spk_ttyio_initialise_ldisc(struct spk_synth *synth)\n{\n\tint ret = 0;\n\tstruct tty_struct *tty;\n\tstruct ktermios tmp_termios;\n\tdev_t dev;\n\n\tret = get_dev_to_use(synth, &dev);\n\tif (ret)\n\t\treturn ret;\n\n\ttty = tty_kopen(dev);\n\tif (IS_ERR(tty))\n\t\treturn PTR_ERR(tty);\n\n\tif (tty->ops->open)\n\t\tret = tty->ops->open(tty, NULL);\n\telse\n\t\tret = -ENODEV;\n\n\tif (ret) {\n\t\ttty_unlock(tty);\n\t\treturn ret;\n\t}\n\n\tclear_bit(TTY_HUPPED, &tty->flags);\n\t/* ensure hardware flow control is enabled */\n\tget_termios(tty, &tmp_termios);\n\tif (!(tmp_termios.c_cflag & CRTSCTS)) {\n\t\ttmp_termios.c_cflag |= CRTSCTS;\n\t\ttty_set_termios(tty, &tmp_termios);\n\t\t/*\n\t\t * check c_cflag to see if it's updated as tty_set_termios\n\t\t * may not return error even when no tty bits are\n\t\t * changed by the request.\n\t\t */\n\t\tget_termios(tty, &tmp_termios);\n\t\tif (!(tmp_termios.c_cflag & CRTSCTS))\n\t\t\tpr_warn(\"speakup: Failed to set hardware flow control\\n\");\n\t}\n\n\ttty_unlock(tty);\n\n\tret = tty_set_ldisc(tty, N_SPEAKUP);\n\tif (ret)\n\t\tpr_err(\"speakup: Failed to set N_SPEAKUP on tty\\n\");\n\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int spk_ttyio_ldisc_open(struct tty_struct *tty)\n{\n\tstruct spk_ldisc_data *ldisc_data;\n\n\tif (!tty->ops->write)\n\t\treturn -EOPNOTSUPP;\n\n\tmutex_lock(&speakup_tty_mutex);\n\tif (speakup_tty) {\n\t\tmutex_unlock(&speakup_tty_mutex);\n\t\treturn -EBUSY;\n\t}\n\tspeakup_tty = tty;\n\n\tldisc_data = kmalloc(sizeof(*ldisc_data), GFP_KERNEL);\n\tif (!ldisc_data) {\n\t\tspeakup_tty = NULL;\n\t\tmutex_unlock(&speakup_tty_mutex);\n\t\treturn -ENOMEM;\n\t}\n\n\tinit_completion(&ldisc_data->completion);\n\tldisc_data->buf_free = true;\n\tspeakup_tty->disc_data = ldisc_data;\n\tmutex_unlock(&speakup_tty_mutex);\n\n\treturn 0;\n}",
                        "code_after_change": "static int spk_ttyio_ldisc_open(struct tty_struct *tty)\n{\n\tstruct spk_ldisc_data *ldisc_data;\n\n\tif (tty != speakup_tty)\n\t\t/* Somebody tried to use this line discipline outside speakup */\n\t\treturn -ENODEV;\n\n\tif (!tty->ops->write)\n\t\treturn -EOPNOTSUPP;\n\n\tldisc_data = kmalloc(sizeof(*ldisc_data), GFP_KERNEL);\n\tif (!ldisc_data)\n\t\treturn -ENOMEM;\n\n\tinit_completion(&ldisc_data->completion);\n\tldisc_data->buf_free = true;\n\ttty->disc_data = ldisc_data;\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2020-27830"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3212,
            "cve_id": "CVE-2021-38206",
            "code_snippet": "netdev_tx_t ieee80211_monitor_start_xmit(struct sk_buff *skb,\n\t\t\t\t\t struct net_device *dev)\n{\n\tstruct ieee80211_local *local = wdev_priv(dev->ieee80211_ptr);\n\tstruct ieee80211_chanctx_conf *chanctx_conf;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_hdr *hdr;\n\tstruct ieee80211_sub_if_data *tmp_sdata, *sdata;\n\tstruct cfg80211_chan_def *chandef;\n\tu16 len_rthdr;\n\tint hdrlen;\n\n\tmemset(info, 0, sizeof(*info));\n\tinfo->flags = IEEE80211_TX_CTL_REQ_TX_STATUS |\n\t\t      IEEE80211_TX_CTL_INJECTED;\n\n\t/* Sanity-check and process the injection radiotap header */\n\tif (!ieee80211_parse_tx_radiotap(skb, dev))\n\t\tgoto fail;\n\n\t/* we now know there is a radiotap header with a length we can use */\n\tlen_rthdr = ieee80211_get_radiotap_len(skb->data);\n\n\t/*\n\t * fix up the pointers accounting for the radiotap\n\t * header still being in there.  We are being given\n\t * a precooked IEEE80211 header so no need for\n\t * normal processing\n\t */\n\tskb_set_mac_header(skb, len_rthdr);\n\t/*\n\t * these are just fixed to the end of the rt area since we\n\t * don't have any better information and at this point, nobody cares\n\t */\n\tskb_set_network_header(skb, len_rthdr);\n\tskb_set_transport_header(skb, len_rthdr);\n\n\tif (skb->len < len_rthdr + 2)\n\t\tgoto fail;\n\n\thdr = (struct ieee80211_hdr *)(skb->data + len_rthdr);\n\thdrlen = ieee80211_hdrlen(hdr->frame_control);\n\n\tif (skb->len < len_rthdr + hdrlen)\n\t\tgoto fail;\n\n\t/*\n\t * Initialize skb->protocol if the injected frame is a data frame\n\t * carrying a rfc1042 header\n\t */\n\tif (ieee80211_is_data(hdr->frame_control) &&\n\t    skb->len >= len_rthdr + hdrlen + sizeof(rfc1042_header) + 2) {\n\t\tu8 *payload = (u8 *)hdr + hdrlen;\n\n\t\tif (ether_addr_equal(payload, rfc1042_header))\n\t\t\tskb->protocol = cpu_to_be16((payload[6] << 8) |\n\t\t\t\t\t\t    payload[7]);\n\t}\n\n\trcu_read_lock();\n\n\t/*\n\t * We process outgoing injected frames that have a local address\n\t * we handle as though they are non-injected frames.\n\t * This code here isn't entirely correct, the local MAC address\n\t * isn't always enough to find the interface to use; for proper\n\t * VLAN support we have an nl80211-based mechanism.\n\t *\n\t * This is necessary, for example, for old hostapd versions that\n\t * don't use nl80211-based management TX/RX.\n\t */\n\tsdata = IEEE80211_DEV_TO_SUB_IF(dev);\n\n\tlist_for_each_entry_rcu(tmp_sdata, &local->interfaces, list) {\n\t\tif (!ieee80211_sdata_running(tmp_sdata))\n\t\t\tcontinue;\n\t\tif (tmp_sdata->vif.type == NL80211_IFTYPE_MONITOR ||\n\t\t    tmp_sdata->vif.type == NL80211_IFTYPE_AP_VLAN)\n\t\t\tcontinue;\n\t\tif (ether_addr_equal(tmp_sdata->vif.addr, hdr->addr2)) {\n\t\t\tsdata = tmp_sdata;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\tif (!chanctx_conf) {\n\t\ttmp_sdata = rcu_dereference(local->monitor_sdata);\n\t\tif (tmp_sdata)\n\t\t\tchanctx_conf =\n\t\t\t\trcu_dereference(tmp_sdata->vif.chanctx_conf);\n\t}\n\n\tif (chanctx_conf)\n\t\tchandef = &chanctx_conf->def;\n\telse if (!local->use_chanctx)\n\t\tchandef = &local->_oper_chandef;\n\telse\n\t\tgoto fail_rcu;\n\n\t/*\n\t * Frame injection is not allowed if beaconing is not allowed\n\t * or if we need radar detection. Beaconing is usually not allowed when\n\t * the mode or operation (Adhoc, AP, Mesh) does not support DFS.\n\t * Passive scan is also used in world regulatory domains where\n\t * your country is not known and as such it should be treated as\n\t * NO TX unless the channel is explicitly allowed in which case\n\t * your current regulatory domain would not have the passive scan\n\t * flag.\n\t *\n\t * Since AP mode uses monitor interfaces to inject/TX management\n\t * frames we can make AP mode the exception to this rule once it\n\t * supports radar detection as its implementation can deal with\n\t * radar detection by itself. We can do that later by adding a\n\t * monitor flag interfaces used for AP support.\n\t */\n\tif (!cfg80211_reg_can_beacon(local->hw.wiphy, chandef,\n\t\t\t\t     sdata->vif.type))\n\t\tgoto fail_rcu;\n\n\tinfo->band = chandef->chan->band;\n\n\t/* Initialize skb->priority according to frame type and TID class,\n\t * with respect to the sub interface that the frame will actually\n\t * be transmitted on. If the DONT_REORDER flag is set, the original\n\t * skb-priority is preserved to assure frames injected with this\n\t * flag are not reordered relative to each other.\n\t */\n\tieee80211_select_queue_80211(sdata, skb, hdr);\n\tskb_set_queue_mapping(skb, ieee80211_ac_from_tid(skb->priority));\n\n\t/* remove the injection radiotap header */\n\tskb_pull(skb, len_rthdr);\n\n\tieee80211_xmit(sdata, NULL, skb);\n\trcu_read_unlock();\n\n\treturn NETDEV_TX_OK;\n\nfail_rcu:\n\trcu_read_unlock();\nfail:\n\tdev_kfree_skb(skb);\n\treturn NETDEV_TX_OK; /* meaning, we dealt with the skb */\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static ieee80211_rx_result debug_noinline\nieee80211_rx_h_decrypt(struct ieee80211_rx_data *rx)\n{\n\tstruct sk_buff *skb = rx->skb;\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(skb);\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)skb->data;\n\tint keyidx;\n\tieee80211_rx_result result = RX_DROP_UNUSABLE;\n\tstruct ieee80211_key *sta_ptk = NULL;\n\tstruct ieee80211_key *ptk_idx = NULL;\n\tint mmie_keyidx = -1;\n\t__le16 fc;\n\n\tif (ieee80211_is_ext(hdr->frame_control))\n\t\treturn RX_CONTINUE;\n\n\t/*\n\t * Key selection 101\n\t *\n\t * There are five types of keys:\n\t *  - GTK (group keys)\n\t *  - IGTK (group keys for management frames)\n\t *  - BIGTK (group keys for Beacon frames)\n\t *  - PTK (pairwise keys)\n\t *  - STK (station-to-station pairwise keys)\n\t *\n\t * When selecting a key, we have to distinguish between multicast\n\t * (including broadcast) and unicast frames, the latter can only\n\t * use PTKs and STKs while the former always use GTKs, IGTKs, and\n\t * BIGTKs. Unless, of course, actual WEP keys (\"pre-RSNA\") are used,\n\t * then unicast frames can also use key indices like GTKs. Hence, if we\n\t * don't have a PTK/STK we check the key index for a WEP key.\n\t *\n\t * Note that in a regular BSS, multicast frames are sent by the\n\t * AP only, associated stations unicast the frame to the AP first\n\t * which then multicasts it on their behalf.\n\t *\n\t * There is also a slight problem in IBSS mode: GTKs are negotiated\n\t * with each station, that is something we don't currently handle.\n\t * The spec seems to expect that one negotiates the same key with\n\t * every station but there's no such requirement; VLANs could be\n\t * possible.\n\t */\n\n\t/* start without a key */\n\trx->key = NULL;\n\tfc = hdr->frame_control;\n\n\tif (rx->sta) {\n\t\tint keyid = rx->sta->ptk_idx;\n\t\tsta_ptk = rcu_dereference(rx->sta->ptk[keyid]);\n\n\t\tif (ieee80211_has_protected(fc) &&\n\t\t    !(status->flag & RX_FLAG_IV_STRIPPED)) {\n\t\t\tkeyid = ieee80211_get_keyid(rx->skb);\n\n\t\t\tif (unlikely(keyid < 0))\n\t\t\t\treturn RX_DROP_UNUSABLE;\n\n\t\t\tptk_idx = rcu_dereference(rx->sta->ptk[keyid]);\n\t\t}\n\t}\n\n\tif (!ieee80211_has_protected(fc))\n\t\tmmie_keyidx = ieee80211_get_mmie_keyidx(rx->skb);\n\n\tif (!is_multicast_ether_addr(hdr->addr1) && sta_ptk) {\n\t\trx->key = ptk_idx ? ptk_idx : sta_ptk;\n\t\tif ((status->flag & RX_FLAG_DECRYPTED) &&\n\t\t    (status->flag & RX_FLAG_IV_STRIPPED))\n\t\t\treturn RX_CONTINUE;\n\t\t/* Skip decryption if the frame is not protected. */\n\t\tif (!ieee80211_has_protected(fc))\n\t\t\treturn RX_CONTINUE;\n\t} else if (mmie_keyidx >= 0 && ieee80211_is_beacon(fc)) {\n\t\t/* Broadcast/multicast robust management frame / BIP */\n\t\tif ((status->flag & RX_FLAG_DECRYPTED) &&\n\t\t    (status->flag & RX_FLAG_IV_STRIPPED))\n\t\t\treturn RX_CONTINUE;\n\n\t\tif (mmie_keyidx < NUM_DEFAULT_KEYS + NUM_DEFAULT_MGMT_KEYS ||\n\t\t    mmie_keyidx >= NUM_DEFAULT_KEYS + NUM_DEFAULT_MGMT_KEYS +\n\t\t    NUM_DEFAULT_BEACON_KEYS) {\n\t\t\tcfg80211_rx_unprot_mlme_mgmt(rx->sdata->dev,\n\t\t\t\t\t\t     skb->data,\n\t\t\t\t\t\t     skb->len);\n\t\t\treturn RX_DROP_MONITOR; /* unexpected BIP keyidx */\n\t\t}\n\n\t\trx->key = ieee80211_rx_get_bigtk(rx, mmie_keyidx);\n\t\tif (!rx->key)\n\t\t\treturn RX_CONTINUE; /* Beacon protection not in use */\n\t} else if (mmie_keyidx >= 0) {\n\t\t/* Broadcast/multicast robust management frame / BIP */\n\t\tif ((status->flag & RX_FLAG_DECRYPTED) &&\n\t\t    (status->flag & RX_FLAG_IV_STRIPPED))\n\t\t\treturn RX_CONTINUE;\n\n\t\tif (mmie_keyidx < NUM_DEFAULT_KEYS ||\n\t\t    mmie_keyidx >= NUM_DEFAULT_KEYS + NUM_DEFAULT_MGMT_KEYS)\n\t\t\treturn RX_DROP_MONITOR; /* unexpected BIP keyidx */\n\t\tif (rx->link_sta) {\n\t\t\tif (ieee80211_is_group_privacy_action(skb) &&\n\t\t\t    test_sta_flag(rx->sta, WLAN_STA_MFP))\n\t\t\t\treturn RX_DROP_MONITOR;\n\n\t\t\trx->key = rcu_dereference(rx->link_sta->gtk[mmie_keyidx]);\n\t\t}\n\t\tif (!rx->key)\n\t\t\trx->key = rcu_dereference(rx->link->gtk[mmie_keyidx]);\n\t} else if (!ieee80211_has_protected(fc)) {\n\t\t/*\n\t\t * The frame was not protected, so skip decryption. However, we\n\t\t * need to set rx->key if there is a key that could have been\n\t\t * used so that the frame may be dropped if encryption would\n\t\t * have been expected.\n\t\t */\n\t\tstruct ieee80211_key *key = NULL;\n\t\tint i;\n\n\t\tif (ieee80211_is_beacon(fc)) {\n\t\t\tkey = ieee80211_rx_get_bigtk(rx, -1);\n\t\t} else if (ieee80211_is_mgmt(fc) &&\n\t\t\t   is_multicast_ether_addr(hdr->addr1)) {\n\t\t\tkey = rcu_dereference(rx->link->default_mgmt_key);\n\t\t} else {\n\t\t\tif (rx->link_sta) {\n\t\t\t\tfor (i = 0; i < NUM_DEFAULT_KEYS; i++) {\n\t\t\t\t\tkey = rcu_dereference(rx->link_sta->gtk[i]);\n\t\t\t\t\tif (key)\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (!key) {\n\t\t\t\tfor (i = 0; i < NUM_DEFAULT_KEYS; i++) {\n\t\t\t\t\tkey = rcu_dereference(rx->link->gtk[i]);\n\t\t\t\t\tif (key)\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif (key)\n\t\t\trx->key = key;\n\t\treturn RX_CONTINUE;\n\t} else {\n\t\t/*\n\t\t * The device doesn't give us the IV so we won't be\n\t\t * able to look up the key. That's ok though, we\n\t\t * don't need to decrypt the frame, we just won't\n\t\t * be able to keep statistics accurate.\n\t\t * Except for key threshold notifications, should\n\t\t * we somehow allow the driver to tell us which key\n\t\t * the hardware used if this flag is set?\n\t\t */\n\t\tif ((status->flag & RX_FLAG_DECRYPTED) &&\n\t\t    (status->flag & RX_FLAG_IV_STRIPPED))\n\t\t\treturn RX_CONTINUE;\n\n\t\tkeyidx = ieee80211_get_keyid(rx->skb);\n\n\t\tif (unlikely(keyidx < 0))\n\t\t\treturn RX_DROP_UNUSABLE;\n\n\t\t/* check per-station GTK first, if multicast packet */\n\t\tif (is_multicast_ether_addr(hdr->addr1) && rx->link_sta)\n\t\t\trx->key = rcu_dereference(rx->link_sta->gtk[keyidx]);\n\n\t\t/* if not found, try default key */\n\t\tif (!rx->key) {\n\t\t\tif (is_multicast_ether_addr(hdr->addr1))\n\t\t\t\trx->key = rcu_dereference(rx->link->gtk[keyidx]);\n\t\t\tif (!rx->key)\n\t\t\t\trx->key = rcu_dereference(rx->sdata->keys[keyidx]);\n\n\t\t\t/*\n\t\t\t * RSNA-protected unicast frames should always be\n\t\t\t * sent with pairwise or station-to-station keys,\n\t\t\t * but for WEP we allow using a key index as well.\n\t\t\t */\n\t\t\tif (rx->key &&\n\t\t\t    rx->key->conf.cipher != WLAN_CIPHER_SUITE_WEP40 &&\n\t\t\t    rx->key->conf.cipher != WLAN_CIPHER_SUITE_WEP104 &&\n\t\t\t    !is_multicast_ether_addr(hdr->addr1))\n\t\t\t\trx->key = NULL;\n\t\t}\n\t}\n\n\tif (rx->key) {\n\t\tif (unlikely(rx->key->flags & KEY_FLAG_TAINTED))\n\t\t\treturn RX_DROP_MONITOR;\n\n\t\t/* TODO: add threshold stuff again */\n\t} else {\n\t\treturn RX_DROP_MONITOR;\n\t}\n\n\tswitch (rx->key->conf.cipher) {\n\tcase WLAN_CIPHER_SUITE_WEP40:\n\tcase WLAN_CIPHER_SUITE_WEP104:\n\t\tresult = ieee80211_crypto_wep_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_TKIP:\n\t\tresult = ieee80211_crypto_tkip_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_CCMP:\n\t\tresult = ieee80211_crypto_ccmp_decrypt(\n\t\t\trx, IEEE80211_CCMP_MIC_LEN);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_CCMP_256:\n\t\tresult = ieee80211_crypto_ccmp_decrypt(\n\t\t\trx, IEEE80211_CCMP_256_MIC_LEN);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_AES_CMAC:\n\t\tresult = ieee80211_crypto_aes_cmac_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_BIP_CMAC_256:\n\t\tresult = ieee80211_crypto_aes_cmac_256_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_BIP_GMAC_128:\n\tcase WLAN_CIPHER_SUITE_BIP_GMAC_256:\n\t\tresult = ieee80211_crypto_aes_gmac_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_GCMP:\n\tcase WLAN_CIPHER_SUITE_GCMP_256:\n\t\tresult = ieee80211_crypto_gcmp_decrypt(rx);\n\t\tbreak;\n\tdefault:\n\t\tresult = RX_DROP_UNUSABLE;\n\t}\n\n\t/* the hdr variable is invalid after the decrypt handlers */\n\n\t/* either the frame has been decrypted or will be dropped */\n\tstatus->flag |= RX_FLAG_DECRYPTED;\n\n\tif (unlikely(ieee80211_is_beacon(fc) && result == RX_DROP_UNUSABLE))\n\t\tcfg80211_rx_unprot_mlme_mgmt(rx->sdata->dev,\n\t\t\t\t\t     skb->data, skb->len);\n\n\treturn result;\n}",
                        "code_after_change": "static ieee80211_rx_result debug_noinline\nieee80211_rx_h_decrypt(struct ieee80211_rx_data *rx)\n{\n\tstruct sk_buff *skb = rx->skb;\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(skb);\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)skb->data;\n\tint keyidx;\n\tieee80211_rx_result result = RX_DROP_UNUSABLE;\n\tstruct ieee80211_key *sta_ptk = NULL;\n\tstruct ieee80211_key *ptk_idx = NULL;\n\tint mmie_keyidx = -1;\n\t__le16 fc;\n\n\tif (ieee80211_is_ext(hdr->frame_control))\n\t\treturn RX_CONTINUE;\n\n\t/*\n\t * Key selection 101\n\t *\n\t * There are five types of keys:\n\t *  - GTK (group keys)\n\t *  - IGTK (group keys for management frames)\n\t *  - BIGTK (group keys for Beacon frames)\n\t *  - PTK (pairwise keys)\n\t *  - STK (station-to-station pairwise keys)\n\t *\n\t * When selecting a key, we have to distinguish between multicast\n\t * (including broadcast) and unicast frames, the latter can only\n\t * use PTKs and STKs while the former always use GTKs, IGTKs, and\n\t * BIGTKs. Unless, of course, actual WEP keys (\"pre-RSNA\") are used,\n\t * then unicast frames can also use key indices like GTKs. Hence, if we\n\t * don't have a PTK/STK we check the key index for a WEP key.\n\t *\n\t * Note that in a regular BSS, multicast frames are sent by the\n\t * AP only, associated stations unicast the frame to the AP first\n\t * which then multicasts it on their behalf.\n\t *\n\t * There is also a slight problem in IBSS mode: GTKs are negotiated\n\t * with each station, that is something we don't currently handle.\n\t * The spec seems to expect that one negotiates the same key with\n\t * every station but there's no such requirement; VLANs could be\n\t * possible.\n\t */\n\n\t/* start without a key */\n\trx->key = NULL;\n\tfc = hdr->frame_control;\n\n\tif (rx->sta) {\n\t\tint keyid = rx->sta->ptk_idx;\n\t\tsta_ptk = rcu_dereference(rx->sta->ptk[keyid]);\n\n\t\tif (ieee80211_has_protected(fc) &&\n\t\t    !(status->flag & RX_FLAG_IV_STRIPPED)) {\n\t\t\tkeyid = ieee80211_get_keyid(rx->skb);\n\n\t\t\tif (unlikely(keyid < 0))\n\t\t\t\treturn RX_DROP_UNUSABLE;\n\n\t\t\tptk_idx = rcu_dereference(rx->sta->ptk[keyid]);\n\t\t}\n\t}\n\n\tif (!ieee80211_has_protected(fc))\n\t\tmmie_keyidx = ieee80211_get_mmie_keyidx(rx->skb);\n\n\tif (!is_multicast_ether_addr(hdr->addr1) && sta_ptk) {\n\t\trx->key = ptk_idx ? ptk_idx : sta_ptk;\n\t\tif ((status->flag & RX_FLAG_DECRYPTED) &&\n\t\t    (status->flag & RX_FLAG_IV_STRIPPED))\n\t\t\treturn RX_CONTINUE;\n\t\t/* Skip decryption if the frame is not protected. */\n\t\tif (!ieee80211_has_protected(fc))\n\t\t\treturn RX_CONTINUE;\n\t} else if (mmie_keyidx >= 0 && ieee80211_is_beacon(fc)) {\n\t\t/* Broadcast/multicast robust management frame / BIP */\n\t\tif ((status->flag & RX_FLAG_DECRYPTED) &&\n\t\t    (status->flag & RX_FLAG_IV_STRIPPED))\n\t\t\treturn RX_CONTINUE;\n\n\t\tif (mmie_keyidx < NUM_DEFAULT_KEYS + NUM_DEFAULT_MGMT_KEYS ||\n\t\t    mmie_keyidx >= NUM_DEFAULT_KEYS + NUM_DEFAULT_MGMT_KEYS +\n\t\t\t\t   NUM_DEFAULT_BEACON_KEYS) {\n\t\t\tif (rx->sdata->dev)\n\t\t\t\tcfg80211_rx_unprot_mlme_mgmt(rx->sdata->dev,\n\t\t\t\t\t\t\t     skb->data,\n\t\t\t\t\t\t\t     skb->len);\n\t\t\treturn RX_DROP_MONITOR; /* unexpected BIP keyidx */\n\t\t}\n\n\t\trx->key = ieee80211_rx_get_bigtk(rx, mmie_keyidx);\n\t\tif (!rx->key)\n\t\t\treturn RX_CONTINUE; /* Beacon protection not in use */\n\t} else if (mmie_keyidx >= 0) {\n\t\t/* Broadcast/multicast robust management frame / BIP */\n\t\tif ((status->flag & RX_FLAG_DECRYPTED) &&\n\t\t    (status->flag & RX_FLAG_IV_STRIPPED))\n\t\t\treturn RX_CONTINUE;\n\n\t\tif (mmie_keyidx < NUM_DEFAULT_KEYS ||\n\t\t    mmie_keyidx >= NUM_DEFAULT_KEYS + NUM_DEFAULT_MGMT_KEYS)\n\t\t\treturn RX_DROP_MONITOR; /* unexpected BIP keyidx */\n\t\tif (rx->link_sta) {\n\t\t\tif (ieee80211_is_group_privacy_action(skb) &&\n\t\t\t    test_sta_flag(rx->sta, WLAN_STA_MFP))\n\t\t\t\treturn RX_DROP_MONITOR;\n\n\t\t\trx->key = rcu_dereference(rx->link_sta->gtk[mmie_keyidx]);\n\t\t}\n\t\tif (!rx->key)\n\t\t\trx->key = rcu_dereference(rx->link->gtk[mmie_keyidx]);\n\t} else if (!ieee80211_has_protected(fc)) {\n\t\t/*\n\t\t * The frame was not protected, so skip decryption. However, we\n\t\t * need to set rx->key if there is a key that could have been\n\t\t * used so that the frame may be dropped if encryption would\n\t\t * have been expected.\n\t\t */\n\t\tstruct ieee80211_key *key = NULL;\n\t\tint i;\n\n\t\tif (ieee80211_is_beacon(fc)) {\n\t\t\tkey = ieee80211_rx_get_bigtk(rx, -1);\n\t\t} else if (ieee80211_is_mgmt(fc) &&\n\t\t\t   is_multicast_ether_addr(hdr->addr1)) {\n\t\t\tkey = rcu_dereference(rx->link->default_mgmt_key);\n\t\t} else {\n\t\t\tif (rx->link_sta) {\n\t\t\t\tfor (i = 0; i < NUM_DEFAULT_KEYS; i++) {\n\t\t\t\t\tkey = rcu_dereference(rx->link_sta->gtk[i]);\n\t\t\t\t\tif (key)\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (!key) {\n\t\t\t\tfor (i = 0; i < NUM_DEFAULT_KEYS; i++) {\n\t\t\t\t\tkey = rcu_dereference(rx->link->gtk[i]);\n\t\t\t\t\tif (key)\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif (key)\n\t\t\trx->key = key;\n\t\treturn RX_CONTINUE;\n\t} else {\n\t\t/*\n\t\t * The device doesn't give us the IV so we won't be\n\t\t * able to look up the key. That's ok though, we\n\t\t * don't need to decrypt the frame, we just won't\n\t\t * be able to keep statistics accurate.\n\t\t * Except for key threshold notifications, should\n\t\t * we somehow allow the driver to tell us which key\n\t\t * the hardware used if this flag is set?\n\t\t */\n\t\tif ((status->flag & RX_FLAG_DECRYPTED) &&\n\t\t    (status->flag & RX_FLAG_IV_STRIPPED))\n\t\t\treturn RX_CONTINUE;\n\n\t\tkeyidx = ieee80211_get_keyid(rx->skb);\n\n\t\tif (unlikely(keyidx < 0))\n\t\t\treturn RX_DROP_UNUSABLE;\n\n\t\t/* check per-station GTK first, if multicast packet */\n\t\tif (is_multicast_ether_addr(hdr->addr1) && rx->link_sta)\n\t\t\trx->key = rcu_dereference(rx->link_sta->gtk[keyidx]);\n\n\t\t/* if not found, try default key */\n\t\tif (!rx->key) {\n\t\t\tif (is_multicast_ether_addr(hdr->addr1))\n\t\t\t\trx->key = rcu_dereference(rx->link->gtk[keyidx]);\n\t\t\tif (!rx->key)\n\t\t\t\trx->key = rcu_dereference(rx->sdata->keys[keyidx]);\n\n\t\t\t/*\n\t\t\t * RSNA-protected unicast frames should always be\n\t\t\t * sent with pairwise or station-to-station keys,\n\t\t\t * but for WEP we allow using a key index as well.\n\t\t\t */\n\t\t\tif (rx->key &&\n\t\t\t    rx->key->conf.cipher != WLAN_CIPHER_SUITE_WEP40 &&\n\t\t\t    rx->key->conf.cipher != WLAN_CIPHER_SUITE_WEP104 &&\n\t\t\t    !is_multicast_ether_addr(hdr->addr1))\n\t\t\t\trx->key = NULL;\n\t\t}\n\t}\n\n\tif (rx->key) {\n\t\tif (unlikely(rx->key->flags & KEY_FLAG_TAINTED))\n\t\t\treturn RX_DROP_MONITOR;\n\n\t\t/* TODO: add threshold stuff again */\n\t} else {\n\t\treturn RX_DROP_MONITOR;\n\t}\n\n\tswitch (rx->key->conf.cipher) {\n\tcase WLAN_CIPHER_SUITE_WEP40:\n\tcase WLAN_CIPHER_SUITE_WEP104:\n\t\tresult = ieee80211_crypto_wep_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_TKIP:\n\t\tresult = ieee80211_crypto_tkip_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_CCMP:\n\t\tresult = ieee80211_crypto_ccmp_decrypt(\n\t\t\trx, IEEE80211_CCMP_MIC_LEN);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_CCMP_256:\n\t\tresult = ieee80211_crypto_ccmp_decrypt(\n\t\t\trx, IEEE80211_CCMP_256_MIC_LEN);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_AES_CMAC:\n\t\tresult = ieee80211_crypto_aes_cmac_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_BIP_CMAC_256:\n\t\tresult = ieee80211_crypto_aes_cmac_256_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_BIP_GMAC_128:\n\tcase WLAN_CIPHER_SUITE_BIP_GMAC_256:\n\t\tresult = ieee80211_crypto_aes_gmac_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_GCMP:\n\tcase WLAN_CIPHER_SUITE_GCMP_256:\n\t\tresult = ieee80211_crypto_gcmp_decrypt(rx);\n\t\tbreak;\n\tdefault:\n\t\tresult = RX_DROP_UNUSABLE;\n\t}\n\n\t/* the hdr variable is invalid after the decrypt handlers */\n\n\t/* either the frame has been decrypted or will be dropped */\n\tstatus->flag |= RX_FLAG_DECRYPTED;\n\n\tif (unlikely(ieee80211_is_beacon(fc) && result == RX_DROP_UNUSABLE &&\n\t\t     rx->sdata->dev))\n\t\tcfg80211_rx_unprot_mlme_mgmt(rx->sdata->dev,\n\t\t\t\t\t     skb->data, skb->len);\n\n\treturn result;\n}",
                        "cve_id": "CVE-2022-42722"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3262,
            "cve_id": "CVE-2021-4095",
            "code_snippet": "static int kvm_xen_shared_info_init(struct kvm *kvm, gfn_t gfn)\n{\n\tstruct gfn_to_pfn_cache *gpc = &kvm->arch.xen.shinfo_cache;\n\tgpa_t gpa = gfn_to_gpa(gfn);\n\tint wc_ofs, sec_hi_ofs;\n\tint ret = 0;\n\tint idx = srcu_read_lock(&kvm->srcu);\n\n\tif (gfn == GPA_INVALID) {\n\t\tkvm_gfn_to_pfn_cache_destroy(kvm, gpc);\n\t\tgoto out;\n\t}\n\n\tret = kvm_gfn_to_pfn_cache_init(kvm, gpc, NULL, false, true, gpa,\n\t\t\t\t\tPAGE_SIZE, false);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Paranoia checks on the 32-bit struct layout */\n\tBUILD_BUG_ON(offsetof(struct compat_shared_info, wc) != 0x900);\n\tBUILD_BUG_ON(offsetof(struct compat_shared_info, arch.wc_sec_hi) != 0x924);\n\tBUILD_BUG_ON(offsetof(struct pvclock_vcpu_time_info, version) != 0);\n\n\t/* 32-bit location by default */\n\twc_ofs = offsetof(struct compat_shared_info, wc);\n\tsec_hi_ofs = offsetof(struct compat_shared_info, arch.wc_sec_hi);\n\n#ifdef CONFIG_X86_64\n\t/* Paranoia checks on the 64-bit struct layout */\n\tBUILD_BUG_ON(offsetof(struct shared_info, wc) != 0xc00);\n\tBUILD_BUG_ON(offsetof(struct shared_info, wc_sec_hi) != 0xc0c);\n\n\tif (kvm->arch.xen.long_mode) {\n\t\twc_ofs = offsetof(struct shared_info, wc);\n\t\tsec_hi_ofs = offsetof(struct shared_info, wc_sec_hi);\n\t}\n#endif\n\n\tkvm_write_wall_clock(kvm, gpa + wc_ofs, sec_hi_ofs - wc_ofs);\n\tkvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);\n\nout:\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_ofs)\n{\n\tint version;\n\tint r;\n\tstruct pvclock_wall_clock wc;\n\tu32 wc_sec_hi;\n\tu64 wall_nsec;\n\n\tif (!wall_clock)\n\t\treturn;\n\n\tr = kvm_read_guest(kvm, wall_clock, &version, sizeof(version));\n\tif (r)\n\t\treturn;\n\n\tif (version & 1)\n\t\t++version;  /* first time write, random junk */\n\n\t++version;\n\n\tif (kvm_write_guest(kvm, wall_clock, &version, sizeof(version)))\n\t\treturn;\n\n\t/*\n\t * The guest calculates current wall clock time by adding\n\t * system time (updated by kvm_guest_time_update below) to the\n\t * wall clock specified here.  We do the reverse here.\n\t */\n\twall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);\n\n\twc.nsec = do_div(wall_nsec, 1000000000);\n\twc.sec = (u32)wall_nsec; /* overflow in 2106 guest time */\n\twc.version = version;\n\n\tkvm_write_guest(kvm, wall_clock, &wc, sizeof(wc));\n\n\tif (sec_hi_ofs) {\n\t\twc_sec_hi = wall_nsec >> 32;\n\t\tkvm_write_guest(kvm, wall_clock + sec_hi_ofs,\n\t\t\t\t&wc_sec_hi, sizeof(wc_sec_hi));\n\t}\n\n\tversion++;\n\tkvm_write_guest(kvm, wall_clock, &version, sizeof(version));\n}",
                        "code_after_change": "static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_ofs)\n{\n\tint version;\n\tint r;\n\tstruct pvclock_wall_clock wc;\n\tu32 wc_sec_hi;\n\tu64 wall_nsec;\n\n\tif (!wall_clock)\n\t\treturn;\n\n\tr = kvm_read_guest(kvm, wall_clock, &version, sizeof(version));\n\tif (r)\n\t\treturn;\n\n\tif (version & 1)\n\t\t++version;  /* first time write, random junk */\n\n\t++version;\n\n\tif (kvm_write_guest(kvm, wall_clock, &version, sizeof(version)))\n\t\treturn;\n\n\t/*\n\t * The guest calculates current wall clock time by adding\n\t * system time (updated by kvm_guest_time_update below) to the\n\t * wall clock specified here.  We do the reverse here.\n\t */\n\twall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);\n\n\twc.nsec = do_div(wall_nsec, 1000000000);\n\twc.sec = (u32)wall_nsec; /* overflow in 2106 guest time */\n\twc.version = version;\n\n\tkvm_write_guest(kvm, wall_clock, &wc, sizeof(wc));\n\n\tif (sec_hi_ofs) {\n\t\twc_sec_hi = wall_nsec >> 32;\n\t\tkvm_write_guest(kvm, wall_clock + sec_hi_ofs,\n\t\t\t\t&wc_sec_hi, sizeof(wc_sec_hi));\n\t}\n\n\tversion++;\n\tkvm_write_guest(kvm, wall_clock, &version, sizeof(version));\n}",
                        "cve_id": "CVE-2021-4095"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3389,
            "cve_id": "CVE-2022-1205",
            "code_snippet": "void ax25_disconnect(ax25_cb *ax25, int reason)\n{\n\tax25_clear_queues(ax25);\n\n\tif (!ax25->sk || !sock_flag(ax25->sk, SOCK_DESTROY))\n\t\tax25_stop_heartbeat(ax25);\n\tax25_stop_t1timer(ax25);\n\tax25_stop_t2timer(ax25);\n\tax25_stop_t3timer(ax25);\n\tax25_stop_idletimer(ax25);\n\n\tax25->state = AX25_STATE_0;\n\n\tax25_link_failed(ax25, reason);\n\n\tif (ax25->sk != NULL) {\n\t\tlocal_bh_disable();\n\t\tbh_lock_sock(ax25->sk);\n\t\tax25->sk->sk_state     = TCP_CLOSE;\n\t\tax25->sk->sk_err       = reason;\n\t\tax25->sk->sk_shutdown |= SEND_SHUTDOWN;\n\t\tif (!sock_flag(ax25->sk, SOCK_DEAD)) {\n\t\t\tax25->sk->sk_state_change(ax25->sk);\n\t\t\tsock_set_flag(ax25->sk, SOCK_DEAD);\n\t\t}\n\t\tbh_unlock_sock(ax25->sk);\n\t\tlocal_bh_enable();\n\t}\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static void ax25_kill_by_device(struct net_device *dev)\n{\n\tax25_dev *ax25_dev;\n\tax25_cb *s;\n\tstruct sock *sk;\n\n\tif ((ax25_dev = ax25_dev_ax25dev(dev)) == NULL)\n\t\treturn;\n\n\tspin_lock_bh(&ax25_list_lock);\nagain:\n\tax25_for_each(s, &ax25_list) {\n\t\tif (s->ax25_dev == ax25_dev) {\n\t\t\tsk = s->sk;\n\t\t\tif (!sk) {\n\t\t\t\tspin_unlock_bh(&ax25_list_lock);\n\t\t\t\ts->ax25_dev = NULL;\n\t\t\t\tax25_disconnect(s, ENETUNREACH);\n\t\t\t\tspin_lock_bh(&ax25_list_lock);\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t\tsock_hold(sk);\n\t\t\tspin_unlock_bh(&ax25_list_lock);\n\t\t\tlock_sock(sk);\n\t\t\ts->ax25_dev = NULL;\n\t\t\tif (sk->sk_socket) {\n\t\t\t\tdev_put_track(ax25_dev->dev, &ax25_dev->dev_tracker);\n\t\t\t\tax25_dev_put(ax25_dev);\n\t\t\t}\n\t\t\tax25_disconnect(s, ENETUNREACH);\n\t\t\trelease_sock(sk);\n\t\t\tspin_lock_bh(&ax25_list_lock);\n\t\t\tsock_put(sk);\n\t\t\t/* The entry could have been deleted from the\n\t\t\t * list meanwhile and thus the next pointer is\n\t\t\t * no longer valid.  Play it safe and restart\n\t\t\t * the scan.  Forward progress is ensured\n\t\t\t * because we set s->ax25_dev to NULL and we\n\t\t\t * are never passed a NULL 'dev' argument.\n\t\t\t */\n\t\t\tgoto again;\n\t\t}\n\t}\n\tspin_unlock_bh(&ax25_list_lock);\n}",
                        "code_after_change": "static void ax25_kill_by_device(struct net_device *dev)\n{\n\tax25_dev *ax25_dev;\n\tax25_cb *s;\n\tstruct sock *sk;\n\n\tif ((ax25_dev = ax25_dev_ax25dev(dev)) == NULL)\n\t\treturn;\n\n\tspin_lock_bh(&ax25_list_lock);\nagain:\n\tax25_for_each(s, &ax25_list) {\n\t\tif (s->ax25_dev == ax25_dev) {\n\t\t\tsk = s->sk;\n\t\t\tif (!sk) {\n\t\t\t\tspin_unlock_bh(&ax25_list_lock);\n\t\t\t\tax25_disconnect(s, ENETUNREACH);\n\t\t\t\ts->ax25_dev = NULL;\n\t\t\t\tspin_lock_bh(&ax25_list_lock);\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t\tsock_hold(sk);\n\t\t\tspin_unlock_bh(&ax25_list_lock);\n\t\t\tlock_sock(sk);\n\t\t\tax25_disconnect(s, ENETUNREACH);\n\t\t\ts->ax25_dev = NULL;\n\t\t\tif (sk->sk_socket) {\n\t\t\t\tdev_put_track(ax25_dev->dev, &ax25_dev->dev_tracker);\n\t\t\t\tax25_dev_put(ax25_dev);\n\t\t\t}\n\t\t\trelease_sock(sk);\n\t\t\tspin_lock_bh(&ax25_list_lock);\n\t\t\tsock_put(sk);\n\t\t\t/* The entry could have been deleted from the\n\t\t\t * list meanwhile and thus the next pointer is\n\t\t\t * no longer valid.  Play it safe and restart\n\t\t\t * the scan.  Forward progress is ensured\n\t\t\t * because we set s->ax25_dev to NULL and we\n\t\t\t * are never passed a NULL 'dev' argument.\n\t\t\t */\n\t\t\tgoto again;\n\t\t}\n\t}\n\tspin_unlock_bh(&ax25_list_lock);\n}",
                        "cve_id": "CVE-2022-1205"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3405,
            "cve_id": "CVE-2022-1671",
            "code_snippet": "static int rxrpc_preparse_s(struct key_preparsed_payload *prep)\n{\n\tconst struct rxrpc_security *sec;\n\tunsigned int service, sec_class;\n\tint n;\n\n\t_enter(\"%zu\", prep->datalen);\n\n\tif (!prep->orig_description)\n\t\treturn -EINVAL;\n\n\tif (sscanf(prep->orig_description, \"%u:%u%n\", &service, &sec_class, &n) != 2)\n\t\treturn -EINVAL;\n\n\tsec = rxrpc_security_lookup(sec_class);\n\tif (!sec)\n\t\treturn -ENOPKG;\n\n\tprep->payload.data[1] = (struct rxrpc_security *)sec;\n\n\treturn sec->preparse_server_key(prep);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static void rxrpc_free_preparse_s(struct key_preparsed_payload *prep)\n{\n\tconst struct rxrpc_security *sec = prep->payload.data[1];\n\n\tif (sec)\n\t\tsec->free_preparse_server_key(prep);\n}",
                        "code_after_change": "static void rxrpc_free_preparse_s(struct key_preparsed_payload *prep)\n{\n\tconst struct rxrpc_security *sec = prep->payload.data[1];\n\n\tif (sec && sec->free_preparse_server_key)\n\t\tsec->free_preparse_server_key(prep);\n}",
                        "cve_id": "CVE-2022-1671"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3428,
            "cve_id": "CVE-2022-1852",
            "code_snippet": "int x86_decode_emulated_instruction(struct kvm_vcpu *vcpu, int emulation_type,\n\t\t\t\t    void *insn, int insn_len)\n{\n\tint r = EMULATION_OK;\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\n\tinit_emulate_ctxt(vcpu);\n\n\t/*\n\t * We will reenter on the same instruction since we do not set\n\t * complete_userspace_io. This does not handle watchpoints yet,\n\t * those would be handled in the emulate_ops.\n\t */\n\tif (!(emulation_type & EMULTYPE_SKIP) &&\n\t    kvm_vcpu_check_breakpoint(vcpu, &r))\n\t\treturn r;\n\n\tr = x86_decode_insn(ctxt, insn, insn_len, emulation_type);\n\n\ttrace_kvm_emulate_insn_start(vcpu);\n\t++vcpu->stat.insn_emulation;\n\n\treturn r;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,\n\t\t\t    int emulation_type, void *insn, int insn_len)\n{\n\tint r;\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tbool writeback = true;\n\tbool write_fault_to_spt;\n\n\tif (unlikely(!kvm_can_emulate_insn(vcpu, emulation_type, insn, insn_len)))\n\t\treturn 1;\n\n\tvcpu->arch.l1tf_flush_l1d = true;\n\n\t/*\n\t * Clear write_fault_to_shadow_pgtable here to ensure it is\n\t * never reused.\n\t */\n\twrite_fault_to_spt = vcpu->arch.write_fault_to_shadow_pgtable;\n\tvcpu->arch.write_fault_to_shadow_pgtable = false;\n\n\tif (!(emulation_type & EMULTYPE_NO_DECODE)) {\n\t\tkvm_clear_exception_queue(vcpu);\n\n\t\tr = x86_decode_emulated_instruction(vcpu, emulation_type,\n\t\t\t\t\t\t    insn, insn_len);\n\t\tif (r != EMULATION_OK)  {\n\t\t\tif ((emulation_type & EMULTYPE_TRAP_UD) ||\n\t\t\t    (emulation_type & EMULTYPE_TRAP_UD_FORCED)) {\n\t\t\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\tif (reexecute_instruction(vcpu, cr2_or_gpa,\n\t\t\t\t\t\t  write_fault_to_spt,\n\t\t\t\t\t\t  emulation_type))\n\t\t\t\treturn 1;\n\t\t\tif (ctxt->have_exception) {\n\t\t\t\t/*\n\t\t\t\t * #UD should result in just EMULATION_FAILED, and trap-like\n\t\t\t\t * exception should not be encountered during decode.\n\t\t\t\t */\n\t\t\t\tWARN_ON_ONCE(ctxt->exception.vector == UD_VECTOR ||\n\t\t\t\t\t     exception_type(ctxt->exception.vector) == EXCPT_TRAP);\n\t\t\t\tinject_emulated_exception(vcpu);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t\t}\n\t}\n\n\tif ((emulation_type & EMULTYPE_VMWARE_GP) &&\n\t    !is_vmware_backdoor_opcode(ctxt)) {\n\t\tkvm_queue_exception_e(vcpu, GP_VECTOR, 0);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * EMULTYPE_SKIP without EMULTYPE_COMPLETE_USER_EXIT is intended for\n\t * use *only* by vendor callbacks for kvm_skip_emulated_instruction().\n\t * The caller is responsible for updating interruptibility state and\n\t * injecting single-step #DBs.\n\t */\n\tif (emulation_type & EMULTYPE_SKIP) {\n\t\tif (ctxt->mode != X86EMUL_MODE_PROT64)\n\t\t\tctxt->eip = (u32)ctxt->_eip;\n\t\telse\n\t\t\tctxt->eip = ctxt->_eip;\n\n\t\tif (emulation_type & EMULTYPE_COMPLETE_USER_EXIT) {\n\t\t\tr = 1;\n\t\t\tgoto writeback;\n\t\t}\n\n\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\tif (ctxt->eflags & X86_EFLAGS_RF)\n\t\t\tkvm_set_rflags(vcpu, ctxt->eflags & ~X86_EFLAGS_RF);\n\t\treturn 1;\n\t}\n\n\tif (retry_instruction(ctxt, cr2_or_gpa, emulation_type))\n\t\treturn 1;\n\n\t/* this is needed for vmware backdoor interface to work since it\n\t   changes registers values  during IO operation */\n\tif (vcpu->arch.emulate_regs_need_sync_from_vcpu) {\n\t\tvcpu->arch.emulate_regs_need_sync_from_vcpu = false;\n\t\temulator_invalidate_register_cache(ctxt);\n\t}\n\nrestart:\n\tif (emulation_type & EMULTYPE_PF) {\n\t\t/* Save the faulting GPA (cr2) in the address field */\n\t\tctxt->exception.address = cr2_or_gpa;\n\n\t\t/* With shadow page tables, cr2 contains a GVA or nGPA. */\n\t\tif (vcpu->arch.mmu->root_role.direct) {\n\t\t\tctxt->gpa_available = true;\n\t\t\tctxt->gpa_val = cr2_or_gpa;\n\t\t}\n\t} else {\n\t\t/* Sanitize the address out of an abundance of paranoia. */\n\t\tctxt->exception.address = 0;\n\t}\n\n\tr = x86_emulate_insn(ctxt);\n\n\tif (r == EMULATION_INTERCEPTED)\n\t\treturn 1;\n\n\tif (r == EMULATION_FAILED) {\n\t\tif (reexecute_instruction(vcpu, cr2_or_gpa, write_fault_to_spt,\n\t\t\t\t\temulation_type))\n\t\t\treturn 1;\n\n\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t}\n\n\tif (ctxt->have_exception) {\n\t\tr = 1;\n\t\tif (inject_emulated_exception(vcpu))\n\t\t\treturn r;\n\t} else if (vcpu->arch.pio.count) {\n\t\tif (!vcpu->arch.pio.in) {\n\t\t\t/* FIXME: return into emulator if single-stepping.  */\n\t\t\tvcpu->arch.pio.count = 0;\n\t\t} else {\n\t\t\twriteback = false;\n\t\t\tvcpu->arch.complete_userspace_io = complete_emulated_pio;\n\t\t}\n\t\tr = 0;\n\t} else if (vcpu->mmio_needed) {\n\t\t++vcpu->stat.mmio_exits;\n\n\t\tif (!vcpu->mmio_is_write)\n\t\t\twriteback = false;\n\t\tr = 0;\n\t\tvcpu->arch.complete_userspace_io = complete_emulated_mmio;\n\t} else if (vcpu->arch.complete_userspace_io) {\n\t\twriteback = false;\n\t\tr = 0;\n\t} else if (r == EMULATION_RESTART)\n\t\tgoto restart;\n\telse\n\t\tr = 1;\n\nwriteback:\n\tif (writeback) {\n\t\tunsigned long rflags = static_call(kvm_x86_get_rflags)(vcpu);\n\t\ttoggle_interruptibility(vcpu, ctxt->interruptibility);\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = false;\n\t\tif (!ctxt->have_exception ||\n\t\t    exception_type(ctxt->exception.vector) == EXCPT_TRAP) {\n\t\t\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);\n\t\t\tif (ctxt->is_branch)\n\t\t\t\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);\n\t\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\t\tif (r && (ctxt->tf || (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)))\n\t\t\t\tr = kvm_vcpu_do_singlestep(vcpu);\n\t\t\tstatic_call_cond(kvm_x86_update_emulated_instruction)(vcpu);\n\t\t\t__kvm_set_rflags(vcpu, ctxt->eflags);\n\t\t}\n\n\t\t/*\n\t\t * For STI, interrupts are shadowed; so KVM_REQ_EVENT will\n\t\t * do nothing, and it will be requested again as soon as\n\t\t * the shadow expires.  But we still need to check here,\n\t\t * because POPF has no interrupt shadow.\n\t\t */\n\t\tif (unlikely((ctxt->eflags & ~rflags) & X86_EFLAGS_IF))\n\t\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t} else\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = true;\n\n\treturn r;\n}",
                        "code_after_change": "int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,\n\t\t\t    int emulation_type, void *insn, int insn_len)\n{\n\tint r;\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tbool writeback = true;\n\tbool write_fault_to_spt;\n\n\tif (unlikely(!kvm_can_emulate_insn(vcpu, emulation_type, insn, insn_len)))\n\t\treturn 1;\n\n\tvcpu->arch.l1tf_flush_l1d = true;\n\n\t/*\n\t * Clear write_fault_to_shadow_pgtable here to ensure it is\n\t * never reused.\n\t */\n\twrite_fault_to_spt = vcpu->arch.write_fault_to_shadow_pgtable;\n\tvcpu->arch.write_fault_to_shadow_pgtable = false;\n\n\tif (!(emulation_type & EMULTYPE_NO_DECODE)) {\n\t\tkvm_clear_exception_queue(vcpu);\n\n\t\t/*\n\t\t * Return immediately if RIP hits a code breakpoint, such #DBs\n\t\t * are fault-like and are higher priority than any faults on\n\t\t * the code fetch itself.\n\t\t */\n\t\tif (!(emulation_type & EMULTYPE_SKIP) &&\n\t\t    kvm_vcpu_check_code_breakpoint(vcpu, &r))\n\t\t\treturn r;\n\n\t\tr = x86_decode_emulated_instruction(vcpu, emulation_type,\n\t\t\t\t\t\t    insn, insn_len);\n\t\tif (r != EMULATION_OK)  {\n\t\t\tif ((emulation_type & EMULTYPE_TRAP_UD) ||\n\t\t\t    (emulation_type & EMULTYPE_TRAP_UD_FORCED)) {\n\t\t\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\tif (reexecute_instruction(vcpu, cr2_or_gpa,\n\t\t\t\t\t\t  write_fault_to_spt,\n\t\t\t\t\t\t  emulation_type))\n\t\t\t\treturn 1;\n\t\t\tif (ctxt->have_exception) {\n\t\t\t\t/*\n\t\t\t\t * #UD should result in just EMULATION_FAILED, and trap-like\n\t\t\t\t * exception should not be encountered during decode.\n\t\t\t\t */\n\t\t\t\tWARN_ON_ONCE(ctxt->exception.vector == UD_VECTOR ||\n\t\t\t\t\t     exception_type(ctxt->exception.vector) == EXCPT_TRAP);\n\t\t\t\tinject_emulated_exception(vcpu);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t\t}\n\t}\n\n\tif ((emulation_type & EMULTYPE_VMWARE_GP) &&\n\t    !is_vmware_backdoor_opcode(ctxt)) {\n\t\tkvm_queue_exception_e(vcpu, GP_VECTOR, 0);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * EMULTYPE_SKIP without EMULTYPE_COMPLETE_USER_EXIT is intended for\n\t * use *only* by vendor callbacks for kvm_skip_emulated_instruction().\n\t * The caller is responsible for updating interruptibility state and\n\t * injecting single-step #DBs.\n\t */\n\tif (emulation_type & EMULTYPE_SKIP) {\n\t\tif (ctxt->mode != X86EMUL_MODE_PROT64)\n\t\t\tctxt->eip = (u32)ctxt->_eip;\n\t\telse\n\t\t\tctxt->eip = ctxt->_eip;\n\n\t\tif (emulation_type & EMULTYPE_COMPLETE_USER_EXIT) {\n\t\t\tr = 1;\n\t\t\tgoto writeback;\n\t\t}\n\n\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\tif (ctxt->eflags & X86_EFLAGS_RF)\n\t\t\tkvm_set_rflags(vcpu, ctxt->eflags & ~X86_EFLAGS_RF);\n\t\treturn 1;\n\t}\n\n\tif (retry_instruction(ctxt, cr2_or_gpa, emulation_type))\n\t\treturn 1;\n\n\t/* this is needed for vmware backdoor interface to work since it\n\t   changes registers values  during IO operation */\n\tif (vcpu->arch.emulate_regs_need_sync_from_vcpu) {\n\t\tvcpu->arch.emulate_regs_need_sync_from_vcpu = false;\n\t\temulator_invalidate_register_cache(ctxt);\n\t}\n\nrestart:\n\tif (emulation_type & EMULTYPE_PF) {\n\t\t/* Save the faulting GPA (cr2) in the address field */\n\t\tctxt->exception.address = cr2_or_gpa;\n\n\t\t/* With shadow page tables, cr2 contains a GVA or nGPA. */\n\t\tif (vcpu->arch.mmu->root_role.direct) {\n\t\t\tctxt->gpa_available = true;\n\t\t\tctxt->gpa_val = cr2_or_gpa;\n\t\t}\n\t} else {\n\t\t/* Sanitize the address out of an abundance of paranoia. */\n\t\tctxt->exception.address = 0;\n\t}\n\n\tr = x86_emulate_insn(ctxt);\n\n\tif (r == EMULATION_INTERCEPTED)\n\t\treturn 1;\n\n\tif (r == EMULATION_FAILED) {\n\t\tif (reexecute_instruction(vcpu, cr2_or_gpa, write_fault_to_spt,\n\t\t\t\t\temulation_type))\n\t\t\treturn 1;\n\n\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t}\n\n\tif (ctxt->have_exception) {\n\t\tr = 1;\n\t\tif (inject_emulated_exception(vcpu))\n\t\t\treturn r;\n\t} else if (vcpu->arch.pio.count) {\n\t\tif (!vcpu->arch.pio.in) {\n\t\t\t/* FIXME: return into emulator if single-stepping.  */\n\t\t\tvcpu->arch.pio.count = 0;\n\t\t} else {\n\t\t\twriteback = false;\n\t\t\tvcpu->arch.complete_userspace_io = complete_emulated_pio;\n\t\t}\n\t\tr = 0;\n\t} else if (vcpu->mmio_needed) {\n\t\t++vcpu->stat.mmio_exits;\n\n\t\tif (!vcpu->mmio_is_write)\n\t\t\twriteback = false;\n\t\tr = 0;\n\t\tvcpu->arch.complete_userspace_io = complete_emulated_mmio;\n\t} else if (vcpu->arch.complete_userspace_io) {\n\t\twriteback = false;\n\t\tr = 0;\n\t} else if (r == EMULATION_RESTART)\n\t\tgoto restart;\n\telse\n\t\tr = 1;\n\nwriteback:\n\tif (writeback) {\n\t\tunsigned long rflags = static_call(kvm_x86_get_rflags)(vcpu);\n\t\ttoggle_interruptibility(vcpu, ctxt->interruptibility);\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = false;\n\t\tif (!ctxt->have_exception ||\n\t\t    exception_type(ctxt->exception.vector) == EXCPT_TRAP) {\n\t\t\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);\n\t\t\tif (ctxt->is_branch)\n\t\t\t\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);\n\t\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\t\tif (r && (ctxt->tf || (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)))\n\t\t\t\tr = kvm_vcpu_do_singlestep(vcpu);\n\t\t\tstatic_call_cond(kvm_x86_update_emulated_instruction)(vcpu);\n\t\t\t__kvm_set_rflags(vcpu, ctxt->eflags);\n\t\t}\n\n\t\t/*\n\t\t * For STI, interrupts are shadowed; so KVM_REQ_EVENT will\n\t\t * do nothing, and it will be requested again as soon as\n\t\t * the shadow expires.  But we still need to check here,\n\t\t * because POPF has no interrupt shadow.\n\t\t */\n\t\tif (unlikely((ctxt->eflags & ~rflags) & X86_EFLAGS_IF))\n\t\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t} else\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = true;\n\n\treturn r;\n}",
                        "cve_id": "CVE-2022-1852"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "bool kvm_irq_delivery_to_apic_fast(struct kvm *kvm, struct kvm_lapic *src,\n\t\tstruct kvm_lapic_irq *irq, int *r, struct dest_map *dest_map)\n{\n\tstruct kvm_apic_map *map;\n\tunsigned long bitmap;\n\tstruct kvm_lapic **dst = NULL;\n\tint i;\n\tbool ret;\n\n\t*r = -1;\n\n\tif (irq->shorthand == APIC_DEST_SELF) {\n\t\t*r = kvm_apic_set_irq(src->vcpu, irq, dest_map);\n\t\treturn true;\n\t}\n\n\trcu_read_lock();\n\tmap = rcu_dereference(kvm->arch.apic_map);\n\n\tret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dst, &bitmap);\n\tif (ret) {\n\t\t*r = 0;\n\t\tfor_each_set_bit(i, &bitmap, 16) {\n\t\t\tif (!dst[i])\n\t\t\t\tcontinue;\n\t\t\t*r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);\n\t\t}\n\t}\n\n\trcu_read_unlock();\n\treturn ret;\n}",
                        "code_after_change": "bool kvm_irq_delivery_to_apic_fast(struct kvm *kvm, struct kvm_lapic *src,\n\t\tstruct kvm_lapic_irq *irq, int *r, struct dest_map *dest_map)\n{\n\tstruct kvm_apic_map *map;\n\tunsigned long bitmap;\n\tstruct kvm_lapic **dst = NULL;\n\tint i;\n\tbool ret;\n\n\t*r = -1;\n\n\tif (irq->shorthand == APIC_DEST_SELF) {\n\t\tif (KVM_BUG_ON(!src, kvm)) {\n\t\t\t*r = 0;\n\t\t\treturn true;\n\t\t}\n\t\t*r = kvm_apic_set_irq(src->vcpu, irq, dest_map);\n\t\treturn true;\n\t}\n\n\trcu_read_lock();\n\tmap = rcu_dereference(kvm->arch.apic_map);\n\n\tret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dst, &bitmap);\n\tif (ret) {\n\t\t*r = 0;\n\t\tfor_each_set_bit(i, &bitmap, 16) {\n\t\t\tif (!dst[i])\n\t\t\t\tcontinue;\n\t\t\t*r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);\n\t\t}\n\t}\n\n\trcu_read_unlock();\n\treturn ret;\n}",
                        "cve_id": "CVE-2022-2153"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "void kvm_mmu_invpcid_gva(struct kvm_vcpu *vcpu, gva_t gva, unsigned long pcid)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.mmu;\n\tbool tlb_flush = false;\n\tuint i;\n\n\tif (pcid == kvm_get_active_pcid(vcpu)) {\n\t\tmmu->invlpg(vcpu, gva, mmu->root.hpa);\n\t\ttlb_flush = true;\n\t}\n\n\tfor (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++) {\n\t\tif (VALID_PAGE(mmu->prev_roots[i].hpa) &&\n\t\t    pcid == kvm_get_pcid(vcpu, mmu->prev_roots[i].pgd)) {\n\t\t\tmmu->invlpg(vcpu, gva, mmu->prev_roots[i].hpa);\n\t\t\ttlb_flush = true;\n\t\t}\n\t}\n\n\tif (tlb_flush)\n\t\tstatic_call(kvm_x86_flush_tlb_gva)(vcpu, gva);\n\n\t++vcpu->stat.invlpg;\n\n\t/*\n\t * Mappings not reachable via the current cr3 or the prev_roots will be\n\t * synced when switching to that cr3, so nothing needs to be done here\n\t * for them.\n\t */\n}",
                        "code_after_change": "void kvm_mmu_invpcid_gva(struct kvm_vcpu *vcpu, gva_t gva, unsigned long pcid)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.mmu;\n\tbool tlb_flush = false;\n\tuint i;\n\n\tif (pcid == kvm_get_active_pcid(vcpu)) {\n\t\tif (mmu->invlpg)\n\t\t\tmmu->invlpg(vcpu, gva, mmu->root.hpa);\n\t\ttlb_flush = true;\n\t}\n\n\tfor (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++) {\n\t\tif (VALID_PAGE(mmu->prev_roots[i].hpa) &&\n\t\t    pcid == kvm_get_pcid(vcpu, mmu->prev_roots[i].pgd)) {\n\t\t\tif (mmu->invlpg)\n\t\t\t\tmmu->invlpg(vcpu, gva, mmu->prev_roots[i].hpa);\n\t\t\ttlb_flush = true;\n\t\t}\n\t}\n\n\tif (tlb_flush)\n\t\tstatic_call(kvm_x86_flush_tlb_gva)(vcpu, gva);\n\n\t++vcpu->stat.invlpg;\n\n\t/*\n\t * Mappings not reachable via the current cr3 or the prev_roots will be\n\t * synced when switching to that cr3, so nothing needs to be done here\n\t * for them.\n\t */\n}",
                        "cve_id": "CVE-2022-1789"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "code_after_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tif (ptr_reg->type & PTR_MAYBE_NULL) {\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\t}\n\n\tswitch (base_type(ptr_reg->type)) {\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void kvm_vcpu_destroy(struct kvm_vcpu *vcpu)\n{\n\tkvm_dirty_ring_free(&vcpu->dirty_ring);\n\tkvm_arch_vcpu_destroy(vcpu);\n\n\t/*\n\t * No need for rcu_read_lock as VCPU_RUN is the only place that changes\n\t * the vcpu->pid pointer, and at destruction time all file descriptors\n\t * are already gone.\n\t */\n\tput_pid(rcu_dereference_protected(vcpu->pid, 1));\n\n\tfree_page((unsigned long)vcpu->run);\n\tkmem_cache_free(kvm_vcpu_cache, vcpu);\n}",
                        "code_after_change": "static void kvm_vcpu_destroy(struct kvm_vcpu *vcpu)\n{\n\tkvm_arch_vcpu_destroy(vcpu);\n\tkvm_dirty_ring_free(&vcpu->dirty_ring);\n\n\t/*\n\t * No need for rcu_read_lock as VCPU_RUN is the only place that changes\n\t * the vcpu->pid pointer, and at destruction time all file descriptors\n\t * are already gone.\n\t */\n\tput_pid(rcu_dereference_protected(vcpu->pid, 1));\n\n\tfree_page((unsigned long)vcpu->run);\n\tkmem_cache_free(kvm_vcpu_cache, vcpu);\n}",
                        "cve_id": "CVE-2022-1263"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 3575,
            "cve_id": "CVE-2022-23222",
            "code_snippet": "static int check_cond_jmp_op(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_insn *insn, int *insn_idx)\n{\n\tstruct bpf_verifier_state *this_branch = env->cur_state;\n\tstruct bpf_verifier_state *other_branch;\n\tstruct bpf_reg_state *regs = this_branch->frame[this_branch->curframe]->regs;\n\tstruct bpf_reg_state *dst_reg, *other_branch_regs, *src_reg = NULL;\n\tu8 opcode = BPF_OP(insn->code);\n\tbool is_jmp32;\n\tint pred = -1;\n\tint err;\n\n\t/* Only conditional jumps are expected to reach here. */\n\tif (opcode == BPF_JA || opcode > BPF_JSLE) {\n\t\tverbose(env, \"invalid BPF_JMP/JMP32 opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (insn->imm != 0) {\n\t\t\tverbose(env, \"BPF_JMP/JMP32 uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* check src1 operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\t\tinsn->src_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tsrc_reg = &regs[insn->src_reg];\n\t} else {\n\t\tif (insn->src_reg != BPF_REG_0) {\n\t\t\tverbose(env, \"BPF_JMP/JMP32 uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tdst_reg = &regs[insn->dst_reg];\n\tis_jmp32 = BPF_CLASS(insn->code) == BPF_JMP32;\n\n\tif (BPF_SRC(insn->code) == BPF_K) {\n\t\tpred = is_branch_taken(dst_reg, insn->imm, opcode, is_jmp32);\n\t} else if (src_reg->type == SCALAR_VALUE &&\n\t\t   is_jmp32 && tnum_is_const(tnum_subreg(src_reg->var_off))) {\n\t\tpred = is_branch_taken(dst_reg,\n\t\t\t\t       tnum_subreg(src_reg->var_off).value,\n\t\t\t\t       opcode,\n\t\t\t\t       is_jmp32);\n\t} else if (src_reg->type == SCALAR_VALUE &&\n\t\t   !is_jmp32 && tnum_is_const(src_reg->var_off)) {\n\t\tpred = is_branch_taken(dst_reg,\n\t\t\t\t       src_reg->var_off.value,\n\t\t\t\t       opcode,\n\t\t\t\t       is_jmp32);\n\t} else if (reg_is_pkt_pointer_any(dst_reg) &&\n\t\t   reg_is_pkt_pointer_any(src_reg) &&\n\t\t   !is_jmp32) {\n\t\tpred = is_pkt_ptr_branch_taken(dst_reg, src_reg, opcode);\n\t}\n\n\tif (pred >= 0) {\n\t\t/* If we get here with a dst_reg pointer type it is because\n\t\t * above is_branch_taken() special cased the 0 comparison.\n\t\t */\n\t\tif (!__is_pointer_value(false, dst_reg))\n\t\t\terr = mark_chain_precision(env, insn->dst_reg);\n\t\tif (BPF_SRC(insn->code) == BPF_X && !err &&\n\t\t    !__is_pointer_value(false, src_reg))\n\t\t\terr = mark_chain_precision(env, insn->src_reg);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (pred == 1) {\n\t\t/* Only follow the goto, ignore fall-through. If needed, push\n\t\t * the fall-through branch for simulation under speculative\n\t\t * execution.\n\t\t */\n\t\tif (!env->bypass_spec_v1 &&\n\t\t    !sanitize_speculative_path(env, insn, *insn_idx + 1,\n\t\t\t\t\t       *insn_idx))\n\t\t\treturn -EFAULT;\n\t\t*insn_idx += insn->off;\n\t\treturn 0;\n\t} else if (pred == 0) {\n\t\t/* Only follow the fall-through branch, since that's where the\n\t\t * program will go. If needed, push the goto branch for\n\t\t * simulation under speculative execution.\n\t\t */\n\t\tif (!env->bypass_spec_v1 &&\n\t\t    !sanitize_speculative_path(env, insn,\n\t\t\t\t\t       *insn_idx + insn->off + 1,\n\t\t\t\t\t       *insn_idx))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\n\tother_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx,\n\t\t\t\t  false);\n\tif (!other_branch)\n\t\treturn -EFAULT;\n\tother_branch_regs = other_branch->frame[other_branch->curframe]->regs;\n\n\t/* detect if we are comparing against a constant value so we can adjust\n\t * our min/max values for our dst register.\n\t * this is only legit if both are scalars (or pointers to the same\n\t * object, I suppose, but we don't support that right now), because\n\t * otherwise the different base pointers mean the offsets aren't\n\t * comparable.\n\t */\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tstruct bpf_reg_state *src_reg = &regs[insn->src_reg];\n\n\t\tif (dst_reg->type == SCALAR_VALUE &&\n\t\t    src_reg->type == SCALAR_VALUE) {\n\t\t\tif (tnum_is_const(src_reg->var_off) ||\n\t\t\t    (is_jmp32 &&\n\t\t\t     tnum_is_const(tnum_subreg(src_reg->var_off))))\n\t\t\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\tdst_reg,\n\t\t\t\t\t\tsrc_reg->var_off.value,\n\t\t\t\t\t\ttnum_subreg(src_reg->var_off).value,\n\t\t\t\t\t\topcode, is_jmp32);\n\t\t\telse if (tnum_is_const(dst_reg->var_off) ||\n\t\t\t\t (is_jmp32 &&\n\t\t\t\t  tnum_is_const(tnum_subreg(dst_reg->var_off))))\n\t\t\t\treg_set_min_max_inv(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    src_reg,\n\t\t\t\t\t\t    dst_reg->var_off.value,\n\t\t\t\t\t\t    tnum_subreg(dst_reg->var_off).value,\n\t\t\t\t\t\t    opcode, is_jmp32);\n\t\t\telse if (!is_jmp32 &&\n\t\t\t\t (opcode == BPF_JEQ || opcode == BPF_JNE))\n\t\t\t\t/* Comparing for equality, we can combine knowledge */\n\t\t\t\treg_combine_min_max(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\t    src_reg, dst_reg, opcode);\n\t\t\tif (src_reg->id &&\n\t\t\t    !WARN_ON_ONCE(src_reg->id != other_branch_regs[insn->src_reg].id)) {\n\t\t\t\tfind_equal_scalars(this_branch, src_reg);\n\t\t\t\tfind_equal_scalars(other_branch, &other_branch_regs[insn->src_reg]);\n\t\t\t}\n\n\t\t}\n\t} else if (dst_reg->type == SCALAR_VALUE) {\n\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\tdst_reg, insn->imm, (u32)insn->imm,\n\t\t\t\t\topcode, is_jmp32);\n\t}\n\n\tif (dst_reg->type == SCALAR_VALUE && dst_reg->id &&\n\t    !WARN_ON_ONCE(dst_reg->id != other_branch_regs[insn->dst_reg].id)) {\n\t\tfind_equal_scalars(this_branch, dst_reg);\n\t\tfind_equal_scalars(other_branch, &other_branch_regs[insn->dst_reg]);\n\t}\n\n\t/* detect if R == 0 where R is returned from bpf_map_lookup_elem().\n\t * NOTE: these optimizations below are related with pointer comparison\n\t *       which will never be JMP32.\n\t */\n\tif (!is_jmp32 && BPF_SRC(insn->code) == BPF_K &&\n\t    insn->imm == 0 && (opcode == BPF_JEQ || opcode == BPF_JNE) &&\n\t    reg_type_may_be_null(dst_reg->type)) {\n\t\t/* Mark all identical registers in each branch as either\n\t\t * safe or unknown depending R == 0 or R != 0 conditional.\n\t\t */\n\t\tmark_ptr_or_null_regs(this_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JNE);\n\t\tmark_ptr_or_null_regs(other_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JEQ);\n\t} else if (!try_match_pkt_pointers(insn, dst_reg, &regs[insn->src_reg],\n\t\t\t\t\t   this_branch, other_branch) &&\n\t\t   is_pointer_value(env, insn->dst_reg)) {\n\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\tinsn->dst_reg);\n\t\treturn -EACCES;\n\t}\n\tif (env->log.level & BPF_LOG_LEVEL)\n\t\tprint_insn_state(env, this_branch->frame[this_branch->curframe]);\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "code_after_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tif (ptr_reg->type & PTR_MAYBE_NULL) {\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\t}\n\n\tswitch (base_type(ptr_reg->type)) {\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3576,
            "cve_id": "CVE-2022-23222",
            "code_snippet": "static bool is_spillable_regtype(enum bpf_reg_type type)\n{\n\tswitch (type) {\n\tcase PTR_TO_MAP_VALUE:\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\tcase PTR_TO_STACK:\n\tcase PTR_TO_CTX:\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\tcase PTR_TO_BTF_ID:\n\tcase PTR_TO_BTF_ID_OR_NULL:\n\tcase PTR_TO_RDONLY_BUF:\n\tcase PTR_TO_RDONLY_BUF_OR_NULL:\n\tcase PTR_TO_RDWR_BUF:\n\tcase PTR_TO_RDWR_BUF_OR_NULL:\n\tcase PTR_TO_PERCPU_BTF_ID:\n\tcase PTR_TO_MEM:\n\tcase PTR_TO_MEM_OR_NULL:\n\tcase PTR_TO_FUNC:\n\tcase PTR_TO_MAP_KEY:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static void mark_ptr_not_null_reg(struct bpf_reg_state *reg)\n{\n\tswitch (reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL: {\n\t\tconst struct bpf_map *map = reg->map_ptr;\n\n\t\tif (map->inner_map_meta) {\n\t\t\treg->type = CONST_PTR_TO_MAP;\n\t\t\treg->map_ptr = map->inner_map_meta;\n\t\t\t/* transfer reg's id which is unique for every map_lookup_elem\n\t\t\t * as UID of the inner map.\n\t\t\t */\n\t\t\tif (map_value_has_timer(map->inner_map_meta))\n\t\t\t\treg->map_uid = reg->id;\n\t\t} else if (map->map_type == BPF_MAP_TYPE_XSKMAP) {\n\t\t\treg->type = PTR_TO_XDP_SOCK;\n\t\t} else if (map->map_type == BPF_MAP_TYPE_SOCKMAP ||\n\t\t\t   map->map_type == BPF_MAP_TYPE_SOCKHASH) {\n\t\t\treg->type = PTR_TO_SOCKET;\n\t\t} else {\n\t\t\treg->type = PTR_TO_MAP_VALUE;\n\t\t}\n\t\tbreak;\n\t}\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\treg->type = PTR_TO_SOCKET;\n\t\tbreak;\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\t\treg->type = PTR_TO_SOCK_COMMON;\n\t\tbreak;\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\t\treg->type = PTR_TO_TCP_SOCK;\n\t\tbreak;\n\tcase PTR_TO_BTF_ID_OR_NULL:\n\t\treg->type = PTR_TO_BTF_ID;\n\t\tbreak;\n\tcase PTR_TO_MEM_OR_NULL:\n\t\treg->type = PTR_TO_MEM;\n\t\tbreak;\n\tcase PTR_TO_RDONLY_BUF_OR_NULL:\n\t\treg->type = PTR_TO_RDONLY_BUF;\n\t\tbreak;\n\tcase PTR_TO_RDWR_BUF_OR_NULL:\n\t\treg->type = PTR_TO_RDWR_BUF;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"unknown nullable register type\");\n\t}\n}",
                        "code_after_change": "static bool reg_type_mismatch_ok(enum bpf_reg_type type)\n{\n\tswitch (base_type(type)) {\n\tcase PTR_TO_CTX:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\tcase PTR_TO_BTF_ID:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static bool reg_type_mismatch_ok(enum bpf_reg_type type)\n{\n\tswitch (type) {\n\tcase PTR_TO_CTX:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\tcase PTR_TO_BTF_ID:\n\tcase PTR_TO_BTF_ID_OR_NULL:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}",
                        "code_after_change": "static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_KEY:\n\t\treturn check_mem_region_access(env, regno, reg->off, access_size,\n\t\t\t\t\t       reg->map_ptr->key_size, false);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tcase PTR_TO_STACK:\n\t\treturn check_stack_range_initialized(\n\t\t\t\tenv,\n\t\t\t\tregno, reg->off, access_size,\n\t\t\t\tzero_size_allowed, ACCESS_HELPER, meta);\n\tdefault: /* scalar_value or invalid ptr */\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s \", regno,\n\t\t\treg_type_str(env, reg->type));\n\t\tverbose(env, \"expected=%s\\n\", reg_type_str(env, PTR_TO_STACK));\n\t\treturn -EACCES;\n\t}\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static bool regsafe(struct bpf_verifier_env *env, struct bpf_reg_state *rold,\n\t\t    struct bpf_reg_state *rcur, struct bpf_id_pair *idmap)\n{\n\tbool equal;\n\n\tif (!(rold->live & REG_LIVE_READ))\n\t\t/* explored state didn't use this */\n\t\treturn true;\n\n\tequal = memcmp(rold, rcur, offsetof(struct bpf_reg_state, parent)) == 0;\n\n\tif (rold->type == PTR_TO_STACK)\n\t\t/* two stack pointers are equal only if they're pointing to\n\t\t * the same stack frame, since fp-8 in foo != fp-8 in bar\n\t\t */\n\t\treturn equal && rold->frameno == rcur->frameno;\n\n\tif (equal)\n\t\treturn true;\n\n\tif (rold->type == NOT_INIT)\n\t\t/* explored state can't have used this */\n\t\treturn true;\n\tif (rcur->type == NOT_INIT)\n\t\treturn false;\n\tswitch (rold->type) {\n\tcase SCALAR_VALUE:\n\t\tif (env->explore_alu_limits)\n\t\t\treturn false;\n\t\tif (rcur->type == SCALAR_VALUE) {\n\t\t\tif (!rold->precise && !rcur->precise)\n\t\t\t\treturn true;\n\t\t\t/* new val must satisfy old val knowledge */\n\t\t\treturn range_within(rold, rcur) &&\n\t\t\t       tnum_in(rold->var_off, rcur->var_off);\n\t\t} else {\n\t\t\t/* We're trying to use a pointer in place of a scalar.\n\t\t\t * Even if the scalar was unbounded, this could lead to\n\t\t\t * pointer leaks because scalars are allowed to leak\n\t\t\t * while pointers are not. We could make this safe in\n\t\t\t * special cases if root is calling us, but it's\n\t\t\t * probably not worth the hassle.\n\t\t\t */\n\t\t\treturn false;\n\t\t}\n\tcase PTR_TO_MAP_KEY:\n\tcase PTR_TO_MAP_VALUE:\n\t\t/* If the new min/max/var_off satisfy the old ones and\n\t\t * everything else matches, we are OK.\n\t\t * 'id' is not compared, since it's only used for maps with\n\t\t * bpf_spin_lock inside map element and in such cases if\n\t\t * the rest of the prog is valid for one map element then\n\t\t * it's valid for all map elements regardless of the key\n\t\t * used in bpf_map_lookup()\n\t\t */\n\t\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&\n\t\t       range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\t/* a PTR_TO_MAP_VALUE could be safe to use as a\n\t\t * PTR_TO_MAP_VALUE_OR_NULL into the same map.\n\t\t * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-\n\t\t * checked, doing so could have affected others with the same\n\t\t * id, and we can't check for that because we lost the id when\n\t\t * we converted to a PTR_TO_MAP_VALUE.\n\t\t */\n\t\tif (rcur->type != PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\treturn false;\n\t\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)))\n\t\t\treturn false;\n\t\t/* Check our ids match any regs they're supposed to */\n\t\treturn check_ids(rold->id, rcur->id, idmap);\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET:\n\t\tif (rcur->type != rold->type)\n\t\t\treturn false;\n\t\t/* We must have at least as much range as the old ptr\n\t\t * did, so that any accesses which were safe before are\n\t\t * still safe.  This is true even if old range < old off,\n\t\t * since someone could have accessed through (ptr - k), or\n\t\t * even done ptr -= k in a register, to get a safe access.\n\t\t */\n\t\tif (rold->range > rcur->range)\n\t\t\treturn false;\n\t\t/* If the offsets don't match, we can't trust our alignment;\n\t\t * nor can we be sure that we won't fall out of range.\n\t\t */\n\t\tif (rold->off != rcur->off)\n\t\t\treturn false;\n\t\t/* id relations must be preserved */\n\t\tif (rold->id && !check_ids(rold->id, rcur->id, idmap))\n\t\t\treturn false;\n\t\t/* new val must satisfy old val knowledge */\n\t\treturn range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_CTX:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\t/* Only valid matches are exact, which memcmp() above\n\t\t * would have accepted\n\t\t */\n\tdefault:\n\t\t/* Don't know what's going on, just say it's not safe */\n\t\treturn false;\n\t}\n\n\t/* Shouldn't get here; if we do, say it's not safe */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}",
                        "code_after_change": "static bool regsafe(struct bpf_verifier_env *env, struct bpf_reg_state *rold,\n\t\t    struct bpf_reg_state *rcur, struct bpf_id_pair *idmap)\n{\n\tbool equal;\n\n\tif (!(rold->live & REG_LIVE_READ))\n\t\t/* explored state didn't use this */\n\t\treturn true;\n\n\tequal = memcmp(rold, rcur, offsetof(struct bpf_reg_state, parent)) == 0;\n\n\tif (rold->type == PTR_TO_STACK)\n\t\t/* two stack pointers are equal only if they're pointing to\n\t\t * the same stack frame, since fp-8 in foo != fp-8 in bar\n\t\t */\n\t\treturn equal && rold->frameno == rcur->frameno;\n\n\tif (equal)\n\t\treturn true;\n\n\tif (rold->type == NOT_INIT)\n\t\t/* explored state can't have used this */\n\t\treturn true;\n\tif (rcur->type == NOT_INIT)\n\t\treturn false;\n\tswitch (base_type(rold->type)) {\n\tcase SCALAR_VALUE:\n\t\tif (env->explore_alu_limits)\n\t\t\treturn false;\n\t\tif (rcur->type == SCALAR_VALUE) {\n\t\t\tif (!rold->precise && !rcur->precise)\n\t\t\t\treturn true;\n\t\t\t/* new val must satisfy old val knowledge */\n\t\t\treturn range_within(rold, rcur) &&\n\t\t\t       tnum_in(rold->var_off, rcur->var_off);\n\t\t} else {\n\t\t\t/* We're trying to use a pointer in place of a scalar.\n\t\t\t * Even if the scalar was unbounded, this could lead to\n\t\t\t * pointer leaks because scalars are allowed to leak\n\t\t\t * while pointers are not. We could make this safe in\n\t\t\t * special cases if root is calling us, but it's\n\t\t\t * probably not worth the hassle.\n\t\t\t */\n\t\t\treturn false;\n\t\t}\n\tcase PTR_TO_MAP_KEY:\n\tcase PTR_TO_MAP_VALUE:\n\t\t/* a PTR_TO_MAP_VALUE could be safe to use as a\n\t\t * PTR_TO_MAP_VALUE_OR_NULL into the same map.\n\t\t * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-\n\t\t * checked, doing so could have affected others with the same\n\t\t * id, and we can't check for that because we lost the id when\n\t\t * we converted to a PTR_TO_MAP_VALUE.\n\t\t */\n\t\tif (type_may_be_null(rold->type)) {\n\t\t\tif (!type_may_be_null(rcur->type))\n\t\t\t\treturn false;\n\t\t\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)))\n\t\t\t\treturn false;\n\t\t\t/* Check our ids match any regs they're supposed to */\n\t\t\treturn check_ids(rold->id, rcur->id, idmap);\n\t\t}\n\n\t\t/* If the new min/max/var_off satisfy the old ones and\n\t\t * everything else matches, we are OK.\n\t\t * 'id' is not compared, since it's only used for maps with\n\t\t * bpf_spin_lock inside map element and in such cases if\n\t\t * the rest of the prog is valid for one map element then\n\t\t * it's valid for all map elements regardless of the key\n\t\t * used in bpf_map_lookup()\n\t\t */\n\t\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&\n\t\t       range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET:\n\t\tif (rcur->type != rold->type)\n\t\t\treturn false;\n\t\t/* We must have at least as much range as the old ptr\n\t\t * did, so that any accesses which were safe before are\n\t\t * still safe.  This is true even if old range < old off,\n\t\t * since someone could have accessed through (ptr - k), or\n\t\t * even done ptr -= k in a register, to get a safe access.\n\t\t */\n\t\tif (rold->range > rcur->range)\n\t\t\treturn false;\n\t\t/* If the offsets don't match, we can't trust our alignment;\n\t\t * nor can we be sure that we won't fall out of range.\n\t\t */\n\t\tif (rold->off != rcur->off)\n\t\t\treturn false;\n\t\t/* id relations must be preserved */\n\t\tif (rold->id && !check_ids(rold->id, rcur->id, idmap))\n\t\t\treturn false;\n\t\t/* new val must satisfy old val knowledge */\n\t\treturn range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_CTX:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\t\t/* Only valid matches are exact, which memcmp() above\n\t\t * would have accepted\n\t\t */\n\tdefault:\n\t\t/* Don't know what's going on, just say it's not safe */\n\t\treturn false;\n\t}\n\n\t/* Shouldn't get here; if we do, say it's not safe */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_KEY:\n\t\treturn check_mem_region_access(env, regno, reg->off, access_size,\n\t\t\t\t\t       reg->map_ptr->key_size, false);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tcase PTR_TO_STACK:\n\t\treturn check_stack_range_initialized(\n\t\t\t\tenv,\n\t\t\t\tregno, reg->off, access_size,\n\t\t\t\tzero_size_allowed, ACCESS_HELPER, meta);\n\tdefault: /* scalar_value or invalid ptr */\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s expected=%s\\n\", regno,\n\t\t\treg_type_str[reg->type],\n\t\t\treg_type_str[PTR_TO_STACK]);\n\t\treturn -EACCES;\n\t}\n}",
                        "code_after_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tif (ptr_reg->type & PTR_MAYBE_NULL) {\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\t}\n\n\tswitch (base_type(ptr_reg->type)) {\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "code_after_change": "static int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct tnum enforce_attach_type_range = tnum_unknown;\n\tconst struct bpf_prog *prog = env->prog;\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\tenum bpf_prog_type prog_type = resolve_prog_type(env->prog);\n\tint err;\n\tstruct bpf_func_state *frame = env->cur_state->frame[0];\n\tconst bool is_subprog = frame->subprogno;\n\n\t/* LSM and struct_ops func-ptr's return type could be \"void\" */\n\tif (!is_subprog &&\n\t    (prog_type == BPF_PROG_TYPE_STRUCT_OPS ||\n\t     prog_type == BPF_PROG_TYPE_LSM) &&\n\t    !prog->aux->attach_func_proto->type)\n\t\treturn 0;\n\n\t/* eBPF calling convention is such that R0 is used\n\t * to return the value from eBPF program.\n\t * Make sure that it's readable at this time\n\t * of bpf_exit, which means that program wrote\n\t * something into it earlier\n\t */\n\terr = check_reg_arg(env, BPF_REG_0, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\tverbose(env, \"R0 leaks addr as return value\\n\");\n\t\treturn -EACCES;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\n\tif (frame->in_async_callback_fn) {\n\t\t/* enforce return zero from async callbacks like timer */\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"In async callback the register R0 is not a known value (%s)\\n\",\n\t\t\t\treg_type_str(env, reg->type));\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!tnum_in(tnum_const(0), reg->var_off)) {\n\t\t\tverbose_invalid_scalar(env, reg, &range, \"async callback\", \"R0\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (is_subprog) {\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"At subprogram exit the register R0 is not a scalar value (%s)\\n\",\n\t\t\t\treg_type_str(env, reg->type));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tswitch (prog_type) {\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_UDP4_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_UDP6_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETSOCKNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETSOCKNAME)\n\t\t\trange = tnum_range(1, 1);\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET4_BIND ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_BIND)\n\t\t\trange = tnum_range(0, 3);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET_EGRESS) {\n\t\t\trange = tnum_range(0, 3);\n\t\t\tenforce_attach_type_range = tnum_range(2, 3);\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\tcase BPF_PROG_TYPE_CGROUP_SYSCTL:\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\t\tbreak;\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT:\n\t\tif (!env->prog->aux->attach_btf_id)\n\t\t\treturn 0;\n\t\trange = tnum_const(0);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_TRACING:\n\t\tswitch (env->prog->expected_attach_type) {\n\t\tcase BPF_TRACE_FENTRY:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\trange = tnum_const(0);\n\t\t\tbreak;\n\t\tcase BPF_TRACE_RAW_TP:\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\treturn 0;\n\t\tcase BPF_TRACE_ITER:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -ENOTSUPP;\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_SK_LOOKUP:\n\t\trange = tnum_range(SK_DROP, SK_PASS);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_EXT:\n\t\t/* freplace program can return anything as its return value\n\t\t * depends on the to-be-replaced kernel func or bpf program.\n\t\t */\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str(env, reg->type));\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose_invalid_scalar(env, reg, &range, \"program exit\", \"R0\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_is_unknown(enforce_attach_type_range) &&\n\t    tnum_in(enforce_attach_type_range, reg->var_off))\n\t\tenv->prog->enforce_expected_attach_type = 1;\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 3577,
            "cve_id": "CVE-2022-23222",
            "code_snippet": "static int check_helper_call(struct bpf_verifier_env *env, struct bpf_insn *insn,\n\t\t\t     int *insn_idx_p)\n{\n\tconst struct bpf_func_proto *fn = NULL;\n\tenum bpf_return_type ret_type;\n\tstruct bpf_reg_state *regs;\n\tstruct bpf_call_arg_meta meta;\n\tint insn_idx = *insn_idx_p;\n\tbool changes_data;\n\tint i, err, func_id;\n\n\t/* find function prototype */\n\tfunc_id = insn->imm;\n\tif (func_id < 0 || func_id >= __BPF_FUNC_MAX_ID) {\n\t\tverbose(env, \"invalid func %s#%d\\n\", func_id_name(func_id),\n\t\t\tfunc_id);\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->ops->get_func_proto)\n\t\tfn = env->ops->get_func_proto(func_id, env->prog);\n\tif (!fn) {\n\t\tverbose(env, \"unknown func %s#%d\\n\", func_id_name(func_id),\n\t\t\tfunc_id);\n\t\treturn -EINVAL;\n\t}\n\n\t/* eBPF programs must be GPL compatible to use GPL-ed functions */\n\tif (!env->prog->gpl_compatible && fn->gpl_only) {\n\t\tverbose(env, \"cannot call GPL-restricted function from non-GPL compatible program\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (fn->allowed && !fn->allowed(env->prog)) {\n\t\tverbose(env, \"helper call is not allowed in probe\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* With LD_ABS/IND some JITs save/restore skb from r1. */\n\tchanges_data = bpf_helper_changes_pkt_data(fn->func);\n\tif (changes_data && fn->arg1_type != ARG_PTR_TO_CTX) {\n\t\tverbose(env, \"kernel subsystem misconfigured func %s#%d: r1 != ctx\\n\",\n\t\t\tfunc_id_name(func_id), func_id);\n\t\treturn -EINVAL;\n\t}\n\n\tmemset(&meta, 0, sizeof(meta));\n\tmeta.pkt_access = fn->pkt_access;\n\n\terr = check_func_proto(fn, func_id);\n\tif (err) {\n\t\tverbose(env, \"kernel subsystem misconfigured func %s#%d\\n\",\n\t\t\tfunc_id_name(func_id), func_id);\n\t\treturn err;\n\t}\n\n\tmeta.func_id = func_id;\n\t/* check args */\n\tfor (i = 0; i < MAX_BPF_FUNC_REG_ARGS; i++) {\n\t\terr = check_func_arg(env, i, &meta, fn);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = record_func_map(env, &meta, func_id, insn_idx);\n\tif (err)\n\t\treturn err;\n\n\terr = record_func_key(env, &meta, func_id, insn_idx);\n\tif (err)\n\t\treturn err;\n\n\t/* Mark slots with STACK_MISC in case of raw mode, stack offset\n\t * is inferred from register state.\n\t */\n\tfor (i = 0; i < meta.access_size; i++) {\n\t\terr = check_mem_access(env, insn_idx, meta.regno, i, BPF_B,\n\t\t\t\t       BPF_WRITE, -1, false);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (is_release_function(func_id)) {\n\t\terr = release_reference(env, meta.ref_obj_id);\n\t\tif (err) {\n\t\t\tverbose(env, \"func %s#%d reference has not been acquired before\\n\",\n\t\t\t\tfunc_id_name(func_id), func_id);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tregs = cur_regs(env);\n\n\tswitch (func_id) {\n\tcase BPF_FUNC_tail_call:\n\t\terr = check_reference_leak(env);\n\t\tif (err) {\n\t\t\tverbose(env, \"tail_call would lead to reference leak\\n\");\n\t\t\treturn err;\n\t\t}\n\t\tbreak;\n\tcase BPF_FUNC_get_local_storage:\n\t\t/* check that flags argument in get_local_storage(map, flags) is 0,\n\t\t * this is required because get_local_storage() can't return an error.\n\t\t */\n\t\tif (!register_is_null(&regs[BPF_REG_2])) {\n\t\t\tverbose(env, \"get_local_storage() doesn't support non-zero flags\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tcase BPF_FUNC_for_each_map_elem:\n\t\terr = __check_func_call(env, insn, insn_idx_p, meta.subprogno,\n\t\t\t\t\tset_map_elem_callback_state);\n\t\tbreak;\n\tcase BPF_FUNC_timer_set_callback:\n\t\terr = __check_func_call(env, insn, insn_idx_p, meta.subprogno,\n\t\t\t\t\tset_timer_callback_state);\n\t\tbreak;\n\tcase BPF_FUNC_find_vma:\n\t\terr = __check_func_call(env, insn, insn_idx_p, meta.subprogno,\n\t\t\t\t\tset_find_vma_callback_state);\n\t\tbreak;\n\tcase BPF_FUNC_snprintf:\n\t\terr = check_bpf_snprintf_call(env, regs);\n\t\tbreak;\n\tcase BPF_FUNC_loop:\n\t\terr = __check_func_call(env, insn, insn_idx_p, meta.subprogno,\n\t\t\t\t\tset_loop_callback_state);\n\t\tbreak;\n\t}\n\n\tif (err)\n\t\treturn err;\n\n\t/* reset caller saved regs */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* helper call returns 64-bit value. */\n\tregs[BPF_REG_0].subreg_def = DEF_NOT_SUBREG;\n\n\t/* update return register (already marked as written above) */\n\tret_type = fn->ret_type;\n\tif (ret_type == RET_INTEGER) {\n\t\t/* sets type to SCALAR_VALUE */\n\t\tmark_reg_unknown(env, regs, BPF_REG_0);\n\t} else if (ret_type == RET_VOID) {\n\t\tregs[BPF_REG_0].type = NOT_INIT;\n\t} else if (base_type(ret_type) == RET_PTR_TO_MAP_VALUE) {\n\t\t/* There is no offset yet applied, variable or fixed */\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\t/* remember map_ptr, so that check_map_access()\n\t\t * can check 'value_size' boundary of memory access\n\t\t * to map element returned from bpf_map_lookup_elem()\n\t\t */\n\t\tif (meta.map_ptr == NULL) {\n\t\t\tverbose(env,\n\t\t\t\t\"kernel subsystem misconfigured verifier\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tregs[BPF_REG_0].map_ptr = meta.map_ptr;\n\t\tregs[BPF_REG_0].map_uid = meta.map_uid;\n\t\tif (type_may_be_null(ret_type)) {\n\t\t\tregs[BPF_REG_0].type = PTR_TO_MAP_VALUE_OR_NULL;\n\t\t} else {\n\t\t\tregs[BPF_REG_0].type = PTR_TO_MAP_VALUE;\n\t\t\tif (map_value_has_spin_lock(meta.map_ptr))\n\t\t\t\tregs[BPF_REG_0].id = ++env->id_gen;\n\t\t}\n\t} else if (base_type(ret_type) == RET_PTR_TO_SOCKET) {\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_SOCKET_OR_NULL;\n\t} else if (base_type(ret_type) == RET_PTR_TO_SOCK_COMMON) {\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_SOCK_COMMON_OR_NULL;\n\t} else if (base_type(ret_type) == RET_PTR_TO_TCP_SOCK) {\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_TCP_SOCK_OR_NULL;\n\t} else if (base_type(ret_type) == RET_PTR_TO_ALLOC_MEM) {\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_MEM_OR_NULL;\n\t\tregs[BPF_REG_0].mem_size = meta.mem_size;\n\t} else if (base_type(ret_type) == RET_PTR_TO_MEM_OR_BTF_ID) {\n\t\tconst struct btf_type *t;\n\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tt = btf_type_skip_modifiers(meta.ret_btf, meta.ret_btf_id, NULL);\n\t\tif (!btf_type_is_struct(t)) {\n\t\t\tu32 tsize;\n\t\t\tconst struct btf_type *ret;\n\t\t\tconst char *tname;\n\n\t\t\t/* resolve the type size of ksym. */\n\t\t\tret = btf_resolve_size(meta.ret_btf, t, &tsize);\n\t\t\tif (IS_ERR(ret)) {\n\t\t\t\ttname = btf_name_by_offset(meta.ret_btf, t->name_off);\n\t\t\t\tverbose(env, \"unable to resolve the size of type '%s': %ld\\n\",\n\t\t\t\t\ttname, PTR_ERR(ret));\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tregs[BPF_REG_0].type =\n\t\t\t\t(ret_type & PTR_MAYBE_NULL) ?\n\t\t\t\tPTR_TO_MEM_OR_NULL : PTR_TO_MEM;\n\t\t\tregs[BPF_REG_0].mem_size = tsize;\n\t\t} else {\n\t\t\tregs[BPF_REG_0].type =\n\t\t\t\t(ret_type & PTR_MAYBE_NULL) ?\n\t\t\t\tPTR_TO_BTF_ID_OR_NULL : PTR_TO_BTF_ID;\n\t\t\tregs[BPF_REG_0].btf = meta.ret_btf;\n\t\t\tregs[BPF_REG_0].btf_id = meta.ret_btf_id;\n\t\t}\n\t} else if (base_type(ret_type) == RET_PTR_TO_BTF_ID) {\n\t\tint ret_btf_id;\n\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = (ret_type & PTR_MAYBE_NULL) ?\n\t\t\t\t\t\t     PTR_TO_BTF_ID_OR_NULL :\n\t\t\t\t\t\t     PTR_TO_BTF_ID;\n\t\tret_btf_id = *fn->ret_btf_id;\n\t\tif (ret_btf_id == 0) {\n\t\t\tverbose(env, \"invalid return type %u of func %s#%d\\n\",\n\t\t\t\tbase_type(ret_type), func_id_name(func_id),\n\t\t\t\tfunc_id);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t/* current BPF helper definitions are only coming from\n\t\t * built-in code with type IDs from  vmlinux BTF\n\t\t */\n\t\tregs[BPF_REG_0].btf = btf_vmlinux;\n\t\tregs[BPF_REG_0].btf_id = ret_btf_id;\n\t} else {\n\t\tverbose(env, \"unknown return type %u of func %s#%d\\n\",\n\t\t\tbase_type(ret_type), func_id_name(func_id), func_id);\n\t\treturn -EINVAL;\n\t}\n\n\tif (reg_type_may_be_null(regs[BPF_REG_0].type))\n\t\tregs[BPF_REG_0].id = ++env->id_gen;\n\n\tif (is_ptr_cast_function(func_id)) {\n\t\t/* For release_reference() */\n\t\tregs[BPF_REG_0].ref_obj_id = meta.ref_obj_id;\n\t} else if (is_acquire_function(func_id, meta.map_ptr)) {\n\t\tint id = acquire_reference_state(env, insn_idx);\n\n\t\tif (id < 0)\n\t\t\treturn id;\n\t\t/* For mark_ptr_or_null_reg() */\n\t\tregs[BPF_REG_0].id = id;\n\t\t/* For release_reference() */\n\t\tregs[BPF_REG_0].ref_obj_id = id;\n\t}\n\n\tdo_refine_retval_range(regs, fn->ret_type, func_id, &meta);\n\n\terr = check_map_func_compatibility(env, meta.map_ptr, func_id);\n\tif (err)\n\t\treturn err;\n\n\tif ((func_id == BPF_FUNC_get_stack ||\n\t     func_id == BPF_FUNC_get_task_stack) &&\n\t    !env->prog->has_callchain_buf) {\n\t\tconst char *err_str;\n\n#ifdef CONFIG_PERF_EVENTS\n\t\terr = get_callchain_buffers(sysctl_perf_event_max_stack);\n\t\terr_str = \"cannot get callchain buffer for func %s#%d\\n\";\n#else\n\t\terr = -ENOTSUPP;\n\t\terr_str = \"func %s#%d not supported without CONFIG_PERF_EVENTS\\n\";\n#endif\n\t\tif (err) {\n\t\t\tverbose(env, err_str, func_id_name(func_id), func_id);\n\t\t\treturn err;\n\t\t}\n\n\t\tenv->prog->has_callchain_buf = true;\n\t}\n\n\tif (func_id == BPF_FUNC_get_stackid || func_id == BPF_FUNC_get_stack)\n\t\tenv->prog->call_get_stack = true;\n\n\tif (func_id == BPF_FUNC_get_func_ip) {\n\t\tif (check_get_func_ip(env))\n\t\t\treturn -ENOTSUPP;\n\t\tenv->prog->call_get_func_ip = true;\n\t}\n\n\tif (changes_data)\n\t\tclear_all_pkt_pointers(env);\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct tnum enforce_attach_type_range = tnum_unknown;\n\tconst struct bpf_prog *prog = env->prog;\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\tenum bpf_prog_type prog_type = resolve_prog_type(env->prog);\n\tint err;\n\tstruct bpf_func_state *frame = env->cur_state->frame[0];\n\tconst bool is_subprog = frame->subprogno;\n\n\t/* LSM and struct_ops func-ptr's return type could be \"void\" */\n\tif (!is_subprog &&\n\t    (prog_type == BPF_PROG_TYPE_STRUCT_OPS ||\n\t     prog_type == BPF_PROG_TYPE_LSM) &&\n\t    !prog->aux->attach_func_proto->type)\n\t\treturn 0;\n\n\t/* eBPF calling convention is such that R0 is used\n\t * to return the value from eBPF program.\n\t * Make sure that it's readable at this time\n\t * of bpf_exit, which means that program wrote\n\t * something into it earlier\n\t */\n\terr = check_reg_arg(env, BPF_REG_0, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\tverbose(env, \"R0 leaks addr as return value\\n\");\n\t\treturn -EACCES;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\n\tif (frame->in_async_callback_fn) {\n\t\t/* enforce return zero from async callbacks like timer */\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"In async callback the register R0 is not a known value (%s)\\n\",\n\t\t\t\treg_type_str[reg->type]);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!tnum_in(tnum_const(0), reg->var_off)) {\n\t\t\tverbose_invalid_scalar(env, reg, &range, \"async callback\", \"R0\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (is_subprog) {\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"At subprogram exit the register R0 is not a scalar value (%s)\\n\",\n\t\t\t\treg_type_str[reg->type]);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tswitch (prog_type) {\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_UDP4_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_UDP6_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETSOCKNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETSOCKNAME)\n\t\t\trange = tnum_range(1, 1);\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET4_BIND ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_BIND)\n\t\t\trange = tnum_range(0, 3);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET_EGRESS) {\n\t\t\trange = tnum_range(0, 3);\n\t\t\tenforce_attach_type_range = tnum_range(2, 3);\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\tcase BPF_PROG_TYPE_CGROUP_SYSCTL:\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\t\tbreak;\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT:\n\t\tif (!env->prog->aux->attach_btf_id)\n\t\t\treturn 0;\n\t\trange = tnum_const(0);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_TRACING:\n\t\tswitch (env->prog->expected_attach_type) {\n\t\tcase BPF_TRACE_FENTRY:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\trange = tnum_const(0);\n\t\t\tbreak;\n\t\tcase BPF_TRACE_RAW_TP:\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\treturn 0;\n\t\tcase BPF_TRACE_ITER:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -ENOTSUPP;\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_SK_LOOKUP:\n\t\trange = tnum_range(SK_DROP, SK_PASS);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_EXT:\n\t\t/* freplace program can return anything as its return value\n\t\t * depends on the to-be-replaced kernel func or bpf program.\n\t\t */\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose_invalid_scalar(env, reg, &range, \"program exit\", \"R0\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_is_unknown(enforce_attach_type_range) &&\n\t    tnum_in(enforce_attach_type_range, reg->var_off))\n\t\tenv->prog->enforce_expected_attach_type = 1;\n\treturn 0;\n}",
                        "code_after_change": "static int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct tnum enforce_attach_type_range = tnum_unknown;\n\tconst struct bpf_prog *prog = env->prog;\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\tenum bpf_prog_type prog_type = resolve_prog_type(env->prog);\n\tint err;\n\tstruct bpf_func_state *frame = env->cur_state->frame[0];\n\tconst bool is_subprog = frame->subprogno;\n\n\t/* LSM and struct_ops func-ptr's return type could be \"void\" */\n\tif (!is_subprog &&\n\t    (prog_type == BPF_PROG_TYPE_STRUCT_OPS ||\n\t     prog_type == BPF_PROG_TYPE_LSM) &&\n\t    !prog->aux->attach_func_proto->type)\n\t\treturn 0;\n\n\t/* eBPF calling convention is such that R0 is used\n\t * to return the value from eBPF program.\n\t * Make sure that it's readable at this time\n\t * of bpf_exit, which means that program wrote\n\t * something into it earlier\n\t */\n\terr = check_reg_arg(env, BPF_REG_0, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\tverbose(env, \"R0 leaks addr as return value\\n\");\n\t\treturn -EACCES;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\n\tif (frame->in_async_callback_fn) {\n\t\t/* enforce return zero from async callbacks like timer */\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"In async callback the register R0 is not a known value (%s)\\n\",\n\t\t\t\treg_type_str(env, reg->type));\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!tnum_in(tnum_const(0), reg->var_off)) {\n\t\t\tverbose_invalid_scalar(env, reg, &range, \"async callback\", \"R0\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (is_subprog) {\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"At subprogram exit the register R0 is not a scalar value (%s)\\n\",\n\t\t\t\treg_type_str(env, reg->type));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tswitch (prog_type) {\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_UDP4_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_UDP6_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETSOCKNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETSOCKNAME)\n\t\t\trange = tnum_range(1, 1);\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET4_BIND ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_BIND)\n\t\t\trange = tnum_range(0, 3);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET_EGRESS) {\n\t\t\trange = tnum_range(0, 3);\n\t\t\tenforce_attach_type_range = tnum_range(2, 3);\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\tcase BPF_PROG_TYPE_CGROUP_SYSCTL:\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\t\tbreak;\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT:\n\t\tif (!env->prog->aux->attach_btf_id)\n\t\t\treturn 0;\n\t\trange = tnum_const(0);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_TRACING:\n\t\tswitch (env->prog->expected_attach_type) {\n\t\tcase BPF_TRACE_FENTRY:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\trange = tnum_const(0);\n\t\t\tbreak;\n\t\tcase BPF_TRACE_RAW_TP:\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\treturn 0;\n\t\tcase BPF_TRACE_ITER:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -ENOTSUPP;\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_SK_LOOKUP:\n\t\trange = tnum_range(SK_DROP, SK_PASS);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_EXT:\n\t\t/* freplace program can return anything as its return value\n\t\t * depends on the to-be-replaced kernel func or bpf program.\n\t\t */\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str(env, reg->type));\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose_invalid_scalar(env, reg, &range, \"program exit\", \"R0\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_is_unknown(enforce_attach_type_range) &&\n\t    tnum_in(enforce_attach_type_range, reg->var_off))\n\t\tenv->prog->enforce_expected_attach_type = 1;\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_KEY:\n\t\treturn check_mem_region_access(env, regno, reg->off, access_size,\n\t\t\t\t\t       reg->map_ptr->key_size, false);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tcase PTR_TO_STACK:\n\t\treturn check_stack_range_initialized(\n\t\t\t\tenv,\n\t\t\t\tregno, reg->off, access_size,\n\t\t\t\tzero_size_allowed, ACCESS_HELPER, meta);\n\tdefault: /* scalar_value or invalid ptr */\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s expected=%s\\n\", regno,\n\t\t\treg_type_str[reg->type],\n\t\t\treg_type_str[PTR_TO_STACK]);\n\t\treturn -EACCES;\n\t}\n}",
                        "code_after_change": "static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_KEY:\n\t\treturn check_mem_region_access(env, regno, reg->off, access_size,\n\t\t\t\t\t       reg->map_ptr->key_size, false);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tcase PTR_TO_STACK:\n\t\treturn check_stack_range_initialized(\n\t\t\t\tenv,\n\t\t\t\tregno, reg->off, access_size,\n\t\t\t\tzero_size_allowed, ACCESS_HELPER, meta);\n\tdefault: /* scalar_value or invalid ptr */\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s \", regno,\n\t\t\treg_type_str(env, reg->type));\n\t\tverbose(env, \"expected=%s\\n\", reg_type_str(env, PTR_TO_STACK));\n\t\treturn -EACCES;\n\t}\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "code_after_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tif (ptr_reg->type & PTR_MAYBE_NULL) {\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\t}\n\n\tswitch (base_type(ptr_reg->type)) {\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state,\n\t\t\t\t bool print_all)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tif (!print_all && !reg_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\tverbose(env, \"P\");\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tif (t == PTR_TO_BTF_ID ||\n\t\t\t    t == PTR_TO_BTF_ID_OR_NULL ||\n\t\t\t    t == PTR_TO_PERCPU_BTF_ID)\n\t\t\t\tverbose(env, \"%s\", kernel_type_name(reg->btf, reg->btf_id));\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (reg_type_may_be_refcounted_or_null(t))\n\t\t\t\tverbose(env, \",ref_obj_id=%d\", reg->ref_obj_id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (t == CONST_PTR_TO_MAP ||\n\t\t\t\t t == PTR_TO_MAP_KEY ||\n\t\t\t\t t == PTR_TO_MAP_VALUE ||\n\t\t\t\t t == PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t\tif (reg->s32_min_value != reg->smin_value &&\n\t\t\t\t    reg->s32_min_value != S32_MIN)\n\t\t\t\t\tverbose(env, \",s32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_min_value));\n\t\t\t\tif (reg->s32_max_value != reg->smax_value &&\n\t\t\t\t    reg->s32_max_value != S32_MAX)\n\t\t\t\t\tverbose(env, \",s32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_max_value));\n\t\t\t\tif (reg->u32_min_value != reg->umin_value &&\n\t\t\t\t    reg->u32_min_value != U32_MIN)\n\t\t\t\t\tverbose(env, \",u32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_min_value));\n\t\t\t\tif (reg->u32_max_value != reg->umax_value &&\n\t\t\t\t    reg->u32_max_value != U32_MAX)\n\t\t\t\t\tverbose(env, \",u32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_max_value));\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tif (!print_all && !stack_slot_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (is_spilled_reg(&state->stack[i])) {\n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tt = reg->type;\n\t\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\t\tverbose(env, \"P\");\n\t\t\tif (t == SCALAR_VALUE && tnum_is_const(reg->var_off))\n\t\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t\t}\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tif (state->in_callback_fn)\n\t\tverbose(env, \" cb\");\n\tif (state->in_async_callback_fn)\n\t\tverbose(env, \" async_cb\");\n\tverbose(env, \"\\n\");\n\tmark_verifier_state_clean(env);\n}",
                        "code_after_change": "static void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state,\n\t\t\t\t bool print_all)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tif (!print_all && !reg_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str(env, t));\n\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\tverbose(env, \"P\");\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tif (base_type(t) == PTR_TO_BTF_ID ||\n\t\t\t    base_type(t) == PTR_TO_PERCPU_BTF_ID)\n\t\t\t\tverbose(env, \"%s\", kernel_type_name(reg->btf, reg->btf_id));\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (reg_type_may_be_refcounted_or_null(t))\n\t\t\t\tverbose(env, \",ref_obj_id=%d\", reg->ref_obj_id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (base_type(t) == CONST_PTR_TO_MAP ||\n\t\t\t\t base_type(t) == PTR_TO_MAP_KEY ||\n\t\t\t\t base_type(t) == PTR_TO_MAP_VALUE)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t\tif (reg->s32_min_value != reg->smin_value &&\n\t\t\t\t    reg->s32_min_value != S32_MIN)\n\t\t\t\t\tverbose(env, \",s32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_min_value));\n\t\t\t\tif (reg->s32_max_value != reg->smax_value &&\n\t\t\t\t    reg->s32_max_value != S32_MAX)\n\t\t\t\t\tverbose(env, \",s32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_max_value));\n\t\t\t\tif (reg->u32_min_value != reg->umin_value &&\n\t\t\t\t    reg->u32_min_value != U32_MIN)\n\t\t\t\t\tverbose(env, \",u32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_min_value));\n\t\t\t\tif (reg->u32_max_value != reg->umax_value &&\n\t\t\t\t    reg->u32_max_value != U32_MAX)\n\t\t\t\t\tverbose(env, \",u32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_max_value));\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tif (!print_all && !stack_slot_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (is_spilled_reg(&state->stack[i])) {\n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tt = reg->type;\n\t\t\tverbose(env, \"=%s\", reg_type_str(env, t));\n\t\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\t\tverbose(env, \"P\");\n\t\t\tif (t == SCALAR_VALUE && tnum_is_const(reg->var_off))\n\t\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t\t}\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tif (state->in_callback_fn)\n\t\tverbose(env, \" cb\");\n\tif (state->in_async_callback_fn)\n\t\tverbose(env, \" async_cb\");\n\tverbose(env, \"\\n\");\n\tmark_verifier_state_clean(env);\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "bool btf_ctx_access(int off, int size, enum bpf_access_type type,\n\t\t    const struct bpf_prog *prog,\n\t\t    struct bpf_insn_access_aux *info)\n{\n\tconst struct btf_type *t = prog->aux->attach_func_proto;\n\tstruct bpf_prog *tgt_prog = prog->aux->dst_prog;\n\tstruct btf *btf = bpf_prog_get_target_btf(prog);\n\tconst char *tname = prog->aux->attach_func_name;\n\tstruct bpf_verifier_log *log = info->log;\n\tconst struct btf_param *args;\n\tu32 nr_args, arg;\n\tint i, ret;\n\n\tif (off % 8) {\n\t\tbpf_log(log, \"func '%s' offset %d is not multiple of 8\\n\",\n\t\t\ttname, off);\n\t\treturn false;\n\t}\n\targ = off / 8;\n\targs = (const struct btf_param *)(t + 1);\n\t/* if (t == NULL) Fall back to default BPF prog with\n\t * MAX_BPF_FUNC_REG_ARGS u64 arguments.\n\t */\n\tnr_args = t ? btf_type_vlen(t) : MAX_BPF_FUNC_REG_ARGS;\n\tif (prog->aux->attach_btf_trace) {\n\t\t/* skip first 'void *__data' argument in btf_trace_##name typedef */\n\t\targs++;\n\t\tnr_args--;\n\t}\n\n\tif (arg > nr_args) {\n\t\tbpf_log(log, \"func '%s' doesn't have %d-th argument\\n\",\n\t\t\ttname, arg + 1);\n\t\treturn false;\n\t}\n\n\tif (arg == nr_args) {\n\t\tswitch (prog->expected_attach_type) {\n\t\tcase BPF_LSM_MAC:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\t/* When LSM programs are attached to void LSM hooks\n\t\t\t * they use FEXIT trampolines and when attached to\n\t\t\t * int LSM hooks, they use MODIFY_RETURN trampolines.\n\t\t\t *\n\t\t\t * While the LSM programs are BPF_MODIFY_RETURN-like\n\t\t\t * the check:\n\t\t\t *\n\t\t\t *\tif (ret_type != 'int')\n\t\t\t *\t\treturn -EINVAL;\n\t\t\t *\n\t\t\t * is _not_ done here. This is still safe as LSM hooks\n\t\t\t * have only void and int return types.\n\t\t\t */\n\t\t\tif (!t)\n\t\t\t\treturn true;\n\t\t\tt = btf_type_by_id(btf, t->type);\n\t\t\tbreak;\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\t/* For now the BPF_MODIFY_RETURN can only be attached to\n\t\t\t * functions that return an int.\n\t\t\t */\n\t\t\tif (!t)\n\t\t\t\treturn false;\n\n\t\t\tt = btf_type_skip_modifiers(btf, t->type, NULL);\n\t\t\tif (!btf_type_is_small_int(t)) {\n\t\t\t\tbpf_log(log,\n\t\t\t\t\t\"ret type %s not allowed for fmod_ret\\n\",\n\t\t\t\t\tbtf_kind_str[BTF_INFO_KIND(t->info)]);\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbpf_log(log, \"func '%s' doesn't have %d-th argument\\n\",\n\t\t\t\ttname, arg + 1);\n\t\t\treturn false;\n\t\t}\n\t} else {\n\t\tif (!t)\n\t\t\t/* Default prog with MAX_BPF_FUNC_REG_ARGS args */\n\t\t\treturn true;\n\t\tt = btf_type_by_id(btf, args[arg].type);\n\t}\n\n\t/* skip modifiers */\n\twhile (btf_type_is_modifier(t))\n\t\tt = btf_type_by_id(btf, t->type);\n\tif (btf_type_is_small_int(t) || btf_type_is_enum(t))\n\t\t/* accessing a scalar */\n\t\treturn true;\n\tif (!btf_type_is_ptr(t)) {\n\t\tbpf_log(log,\n\t\t\t\"func '%s' arg%d '%s' has type %s. Only pointer access is allowed\\n\",\n\t\t\ttname, arg,\n\t\t\t__btf_name_by_offset(btf, t->name_off),\n\t\t\tbtf_kind_str[BTF_INFO_KIND(t->info)]);\n\t\treturn false;\n\t}\n\n\t/* check for PTR_TO_RDONLY_BUF_OR_NULL or PTR_TO_RDWR_BUF_OR_NULL */\n\tfor (i = 0; i < prog->aux->ctx_arg_info_size; i++) {\n\t\tconst struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i];\n\n\t\tif (ctx_arg_info->offset == off &&\n\t\t    (ctx_arg_info->reg_type == PTR_TO_RDONLY_BUF_OR_NULL ||\n\t\t     ctx_arg_info->reg_type == PTR_TO_RDWR_BUF_OR_NULL)) {\n\t\t\tinfo->reg_type = ctx_arg_info->reg_type;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tif (t->type == 0)\n\t\t/* This is a pointer to void.\n\t\t * It is the same as scalar from the verifier safety pov.\n\t\t * No further pointer walking is allowed.\n\t\t */\n\t\treturn true;\n\n\tif (is_int_ptr(btf, t))\n\t\treturn true;\n\n\t/* this is a pointer to another type */\n\tfor (i = 0; i < prog->aux->ctx_arg_info_size; i++) {\n\t\tconst struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i];\n\n\t\tif (ctx_arg_info->offset == off) {\n\t\t\tif (!ctx_arg_info->btf_id) {\n\t\t\t\tbpf_log(log,\"invalid btf_id for context argument offset %u\\n\", off);\n\t\t\t\treturn false;\n\t\t\t}\n\n\t\t\tinfo->reg_type = ctx_arg_info->reg_type;\n\t\t\tinfo->btf = btf_vmlinux;\n\t\t\tinfo->btf_id = ctx_arg_info->btf_id;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tinfo->reg_type = PTR_TO_BTF_ID;\n\tif (tgt_prog) {\n\t\tenum bpf_prog_type tgt_type;\n\n\t\tif (tgt_prog->type == BPF_PROG_TYPE_EXT)\n\t\t\ttgt_type = tgt_prog->aux->saved_dst_prog_type;\n\t\telse\n\t\t\ttgt_type = tgt_prog->type;\n\n\t\tret = btf_translate_to_vmlinux(log, btf, t, tgt_type, arg);\n\t\tif (ret > 0) {\n\t\t\tinfo->btf = btf_vmlinux;\n\t\t\tinfo->btf_id = ret;\n\t\t\treturn true;\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tinfo->btf = btf;\n\tinfo->btf_id = t->type;\n\tt = btf_type_by_id(btf, t->type);\n\t/* skip modifiers */\n\twhile (btf_type_is_modifier(t)) {\n\t\tinfo->btf_id = t->type;\n\t\tt = btf_type_by_id(btf, t->type);\n\t}\n\tif (!btf_type_is_struct(t)) {\n\t\tbpf_log(log,\n\t\t\t\"func '%s' arg%d type %s is not a struct\\n\",\n\t\t\ttname, arg, btf_kind_str[BTF_INFO_KIND(t->info)]);\n\t\treturn false;\n\t}\n\tbpf_log(log, \"func '%s' arg%d has btf_id %d type %s '%s'\\n\",\n\t\ttname, arg, info->btf_id, btf_kind_str[BTF_INFO_KIND(t->info)],\n\t\t__btf_name_by_offset(btf, t->name_off));\n\treturn true;\n}",
                        "code_after_change": "bool btf_ctx_access(int off, int size, enum bpf_access_type type,\n\t\t    const struct bpf_prog *prog,\n\t\t    struct bpf_insn_access_aux *info)\n{\n\tconst struct btf_type *t = prog->aux->attach_func_proto;\n\tstruct bpf_prog *tgt_prog = prog->aux->dst_prog;\n\tstruct btf *btf = bpf_prog_get_target_btf(prog);\n\tconst char *tname = prog->aux->attach_func_name;\n\tstruct bpf_verifier_log *log = info->log;\n\tconst struct btf_param *args;\n\tu32 nr_args, arg;\n\tint i, ret;\n\n\tif (off % 8) {\n\t\tbpf_log(log, \"func '%s' offset %d is not multiple of 8\\n\",\n\t\t\ttname, off);\n\t\treturn false;\n\t}\n\targ = off / 8;\n\targs = (const struct btf_param *)(t + 1);\n\t/* if (t == NULL) Fall back to default BPF prog with\n\t * MAX_BPF_FUNC_REG_ARGS u64 arguments.\n\t */\n\tnr_args = t ? btf_type_vlen(t) : MAX_BPF_FUNC_REG_ARGS;\n\tif (prog->aux->attach_btf_trace) {\n\t\t/* skip first 'void *__data' argument in btf_trace_##name typedef */\n\t\targs++;\n\t\tnr_args--;\n\t}\n\n\tif (arg > nr_args) {\n\t\tbpf_log(log, \"func '%s' doesn't have %d-th argument\\n\",\n\t\t\ttname, arg + 1);\n\t\treturn false;\n\t}\n\n\tif (arg == nr_args) {\n\t\tswitch (prog->expected_attach_type) {\n\t\tcase BPF_LSM_MAC:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\t/* When LSM programs are attached to void LSM hooks\n\t\t\t * they use FEXIT trampolines and when attached to\n\t\t\t * int LSM hooks, they use MODIFY_RETURN trampolines.\n\t\t\t *\n\t\t\t * While the LSM programs are BPF_MODIFY_RETURN-like\n\t\t\t * the check:\n\t\t\t *\n\t\t\t *\tif (ret_type != 'int')\n\t\t\t *\t\treturn -EINVAL;\n\t\t\t *\n\t\t\t * is _not_ done here. This is still safe as LSM hooks\n\t\t\t * have only void and int return types.\n\t\t\t */\n\t\t\tif (!t)\n\t\t\t\treturn true;\n\t\t\tt = btf_type_by_id(btf, t->type);\n\t\t\tbreak;\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\t/* For now the BPF_MODIFY_RETURN can only be attached to\n\t\t\t * functions that return an int.\n\t\t\t */\n\t\t\tif (!t)\n\t\t\t\treturn false;\n\n\t\t\tt = btf_type_skip_modifiers(btf, t->type, NULL);\n\t\t\tif (!btf_type_is_small_int(t)) {\n\t\t\t\tbpf_log(log,\n\t\t\t\t\t\"ret type %s not allowed for fmod_ret\\n\",\n\t\t\t\t\tbtf_kind_str[BTF_INFO_KIND(t->info)]);\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbpf_log(log, \"func '%s' doesn't have %d-th argument\\n\",\n\t\t\t\ttname, arg + 1);\n\t\t\treturn false;\n\t\t}\n\t} else {\n\t\tif (!t)\n\t\t\t/* Default prog with MAX_BPF_FUNC_REG_ARGS args */\n\t\t\treturn true;\n\t\tt = btf_type_by_id(btf, args[arg].type);\n\t}\n\n\t/* skip modifiers */\n\twhile (btf_type_is_modifier(t))\n\t\tt = btf_type_by_id(btf, t->type);\n\tif (btf_type_is_small_int(t) || btf_type_is_enum(t))\n\t\t/* accessing a scalar */\n\t\treturn true;\n\tif (!btf_type_is_ptr(t)) {\n\t\tbpf_log(log,\n\t\t\t\"func '%s' arg%d '%s' has type %s. Only pointer access is allowed\\n\",\n\t\t\ttname, arg,\n\t\t\t__btf_name_by_offset(btf, t->name_off),\n\t\t\tbtf_kind_str[BTF_INFO_KIND(t->info)]);\n\t\treturn false;\n\t}\n\n\t/* check for PTR_TO_RDONLY_BUF_OR_NULL or PTR_TO_RDWR_BUF_OR_NULL */\n\tfor (i = 0; i < prog->aux->ctx_arg_info_size; i++) {\n\t\tconst struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i];\n\t\tu32 type, flag;\n\n\t\ttype = base_type(ctx_arg_info->reg_type);\n\t\tflag = type_flag(ctx_arg_info->reg_type);\n\t\tif (ctx_arg_info->offset == off &&\n\t\t    (type == PTR_TO_RDWR_BUF || type == PTR_TO_RDONLY_BUF) &&\n\t\t    (flag & PTR_MAYBE_NULL)) {\n\t\t\tinfo->reg_type = ctx_arg_info->reg_type;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tif (t->type == 0)\n\t\t/* This is a pointer to void.\n\t\t * It is the same as scalar from the verifier safety pov.\n\t\t * No further pointer walking is allowed.\n\t\t */\n\t\treturn true;\n\n\tif (is_int_ptr(btf, t))\n\t\treturn true;\n\n\t/* this is a pointer to another type */\n\tfor (i = 0; i < prog->aux->ctx_arg_info_size; i++) {\n\t\tconst struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i];\n\n\t\tif (ctx_arg_info->offset == off) {\n\t\t\tif (!ctx_arg_info->btf_id) {\n\t\t\t\tbpf_log(log,\"invalid btf_id for context argument offset %u\\n\", off);\n\t\t\t\treturn false;\n\t\t\t}\n\n\t\t\tinfo->reg_type = ctx_arg_info->reg_type;\n\t\t\tinfo->btf = btf_vmlinux;\n\t\t\tinfo->btf_id = ctx_arg_info->btf_id;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tinfo->reg_type = PTR_TO_BTF_ID;\n\tif (tgt_prog) {\n\t\tenum bpf_prog_type tgt_type;\n\n\t\tif (tgt_prog->type == BPF_PROG_TYPE_EXT)\n\t\t\ttgt_type = tgt_prog->aux->saved_dst_prog_type;\n\t\telse\n\t\t\ttgt_type = tgt_prog->type;\n\n\t\tret = btf_translate_to_vmlinux(log, btf, t, tgt_type, arg);\n\t\tif (ret > 0) {\n\t\t\tinfo->btf = btf_vmlinux;\n\t\t\tinfo->btf_id = ret;\n\t\t\treturn true;\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tinfo->btf = btf;\n\tinfo->btf_id = t->type;\n\tt = btf_type_by_id(btf, t->type);\n\t/* skip modifiers */\n\twhile (btf_type_is_modifier(t)) {\n\t\tinfo->btf_id = t->type;\n\t\tt = btf_type_by_id(btf, t->type);\n\t}\n\tif (!btf_type_is_struct(t)) {\n\t\tbpf_log(log,\n\t\t\t\"func '%s' arg%d type %s is not a struct\\n\",\n\t\t\ttname, arg, btf_kind_str[BTF_INFO_KIND(t->info)]);\n\t\treturn false;\n\t}\n\tbpf_log(log, \"func '%s' arg%d has btf_id %d type %s '%s'\\n\",\n\t\ttname, arg, info->btf_id, btf_kind_str[BTF_INFO_KIND(t->info)],\n\t\t__btf_name_by_offset(btf, t->name_off));\n\treturn true;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 3578,
            "cve_id": "CVE-2022-23222",
            "code_snippet": "static int mark_reg_read(struct bpf_verifier_env *env,\n\t\t\t const struct bpf_reg_state *state,\n\t\t\t struct bpf_reg_state *parent, u8 flag)\n{\n\tbool writes = parent == state->parent; /* Observe write marks */\n\tint cnt = 0;\n\n\twhile (parent) {\n\t\t/* if read wasn't screened by an earlier write ... */\n\t\tif (writes && state->live & REG_LIVE_WRITTEN)\n\t\t\tbreak;\n\t\tif (parent->live & REG_LIVE_DONE) {\n\t\t\tverbose(env, \"verifier BUG type %s var_off %lld off %d\\n\",\n\t\t\t\treg_type_str[parent->type],\n\t\t\t\tparent->var_off.value, parent->off);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\t/* The first condition is more likely to be true than the\n\t\t * second, checked it first.\n\t\t */\n\t\tif ((parent->live & REG_LIVE_READ) == flag ||\n\t\t    parent->live & REG_LIVE_READ64)\n\t\t\t/* The parentage chain never changes and\n\t\t\t * this parent was already marked as LIVE_READ.\n\t\t\t * There is no need to keep walking the chain again and\n\t\t\t * keep re-marking all parents as LIVE_READ.\n\t\t\t * This case happens when the same register is read\n\t\t\t * multiple times without writes into it in-between.\n\t\t\t * Also, if parent has the stronger REG_LIVE_READ64 set,\n\t\t\t * then no need to set the weak REG_LIVE_READ32.\n\t\t\t */\n\t\t\tbreak;\n\t\t/* ... then we depend on parent's value */\n\t\tparent->live |= flag;\n\t\t/* REG_LIVE_READ64 overrides REG_LIVE_READ32. */\n\t\tif (flag == REG_LIVE_READ64)\n\t\t\tparent->live &= ~REG_LIVE_READ32;\n\t\tstate = parent;\n\t\tparent = state->parent;\n\t\twrites = true;\n\t\tcnt++;\n\t}\n\n\tif (env->longest_mark_read_walk < cnt)\n\t\tenv->longest_mark_read_walk = cnt;\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static bool regsafe(struct bpf_verifier_env *env, struct bpf_reg_state *rold,\n\t\t    struct bpf_reg_state *rcur, struct bpf_id_pair *idmap)\n{\n\tbool equal;\n\n\tif (!(rold->live & REG_LIVE_READ))\n\t\t/* explored state didn't use this */\n\t\treturn true;\n\n\tequal = memcmp(rold, rcur, offsetof(struct bpf_reg_state, parent)) == 0;\n\n\tif (rold->type == PTR_TO_STACK)\n\t\t/* two stack pointers are equal only if they're pointing to\n\t\t * the same stack frame, since fp-8 in foo != fp-8 in bar\n\t\t */\n\t\treturn equal && rold->frameno == rcur->frameno;\n\n\tif (equal)\n\t\treturn true;\n\n\tif (rold->type == NOT_INIT)\n\t\t/* explored state can't have used this */\n\t\treturn true;\n\tif (rcur->type == NOT_INIT)\n\t\treturn false;\n\tswitch (rold->type) {\n\tcase SCALAR_VALUE:\n\t\tif (env->explore_alu_limits)\n\t\t\treturn false;\n\t\tif (rcur->type == SCALAR_VALUE) {\n\t\t\tif (!rold->precise && !rcur->precise)\n\t\t\t\treturn true;\n\t\t\t/* new val must satisfy old val knowledge */\n\t\t\treturn range_within(rold, rcur) &&\n\t\t\t       tnum_in(rold->var_off, rcur->var_off);\n\t\t} else {\n\t\t\t/* We're trying to use a pointer in place of a scalar.\n\t\t\t * Even if the scalar was unbounded, this could lead to\n\t\t\t * pointer leaks because scalars are allowed to leak\n\t\t\t * while pointers are not. We could make this safe in\n\t\t\t * special cases if root is calling us, but it's\n\t\t\t * probably not worth the hassle.\n\t\t\t */\n\t\t\treturn false;\n\t\t}\n\tcase PTR_TO_MAP_KEY:\n\tcase PTR_TO_MAP_VALUE:\n\t\t/* If the new min/max/var_off satisfy the old ones and\n\t\t * everything else matches, we are OK.\n\t\t * 'id' is not compared, since it's only used for maps with\n\t\t * bpf_spin_lock inside map element and in such cases if\n\t\t * the rest of the prog is valid for one map element then\n\t\t * it's valid for all map elements regardless of the key\n\t\t * used in bpf_map_lookup()\n\t\t */\n\t\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&\n\t\t       range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\t/* a PTR_TO_MAP_VALUE could be safe to use as a\n\t\t * PTR_TO_MAP_VALUE_OR_NULL into the same map.\n\t\t * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-\n\t\t * checked, doing so could have affected others with the same\n\t\t * id, and we can't check for that because we lost the id when\n\t\t * we converted to a PTR_TO_MAP_VALUE.\n\t\t */\n\t\tif (rcur->type != PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\treturn false;\n\t\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)))\n\t\t\treturn false;\n\t\t/* Check our ids match any regs they're supposed to */\n\t\treturn check_ids(rold->id, rcur->id, idmap);\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET:\n\t\tif (rcur->type != rold->type)\n\t\t\treturn false;\n\t\t/* We must have at least as much range as the old ptr\n\t\t * did, so that any accesses which were safe before are\n\t\t * still safe.  This is true even if old range < old off,\n\t\t * since someone could have accessed through (ptr - k), or\n\t\t * even done ptr -= k in a register, to get a safe access.\n\t\t */\n\t\tif (rold->range > rcur->range)\n\t\t\treturn false;\n\t\t/* If the offsets don't match, we can't trust our alignment;\n\t\t * nor can we be sure that we won't fall out of range.\n\t\t */\n\t\tif (rold->off != rcur->off)\n\t\t\treturn false;\n\t\t/* id relations must be preserved */\n\t\tif (rold->id && !check_ids(rold->id, rcur->id, idmap))\n\t\t\treturn false;\n\t\t/* new val must satisfy old val knowledge */\n\t\treturn range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_CTX:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\t/* Only valid matches are exact, which memcmp() above\n\t\t * would have accepted\n\t\t */\n\tdefault:\n\t\t/* Don't know what's going on, just say it's not safe */\n\t\treturn false;\n\t}\n\n\t/* Shouldn't get here; if we do, say it's not safe */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}",
                        "code_after_change": "static bool regsafe(struct bpf_verifier_env *env, struct bpf_reg_state *rold,\n\t\t    struct bpf_reg_state *rcur, struct bpf_id_pair *idmap)\n{\n\tbool equal;\n\n\tif (!(rold->live & REG_LIVE_READ))\n\t\t/* explored state didn't use this */\n\t\treturn true;\n\n\tequal = memcmp(rold, rcur, offsetof(struct bpf_reg_state, parent)) == 0;\n\n\tif (rold->type == PTR_TO_STACK)\n\t\t/* two stack pointers are equal only if they're pointing to\n\t\t * the same stack frame, since fp-8 in foo != fp-8 in bar\n\t\t */\n\t\treturn equal && rold->frameno == rcur->frameno;\n\n\tif (equal)\n\t\treturn true;\n\n\tif (rold->type == NOT_INIT)\n\t\t/* explored state can't have used this */\n\t\treturn true;\n\tif (rcur->type == NOT_INIT)\n\t\treturn false;\n\tswitch (base_type(rold->type)) {\n\tcase SCALAR_VALUE:\n\t\tif (env->explore_alu_limits)\n\t\t\treturn false;\n\t\tif (rcur->type == SCALAR_VALUE) {\n\t\t\tif (!rold->precise && !rcur->precise)\n\t\t\t\treturn true;\n\t\t\t/* new val must satisfy old val knowledge */\n\t\t\treturn range_within(rold, rcur) &&\n\t\t\t       tnum_in(rold->var_off, rcur->var_off);\n\t\t} else {\n\t\t\t/* We're trying to use a pointer in place of a scalar.\n\t\t\t * Even if the scalar was unbounded, this could lead to\n\t\t\t * pointer leaks because scalars are allowed to leak\n\t\t\t * while pointers are not. We could make this safe in\n\t\t\t * special cases if root is calling us, but it's\n\t\t\t * probably not worth the hassle.\n\t\t\t */\n\t\t\treturn false;\n\t\t}\n\tcase PTR_TO_MAP_KEY:\n\tcase PTR_TO_MAP_VALUE:\n\t\t/* a PTR_TO_MAP_VALUE could be safe to use as a\n\t\t * PTR_TO_MAP_VALUE_OR_NULL into the same map.\n\t\t * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-\n\t\t * checked, doing so could have affected others with the same\n\t\t * id, and we can't check for that because we lost the id when\n\t\t * we converted to a PTR_TO_MAP_VALUE.\n\t\t */\n\t\tif (type_may_be_null(rold->type)) {\n\t\t\tif (!type_may_be_null(rcur->type))\n\t\t\t\treturn false;\n\t\t\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)))\n\t\t\t\treturn false;\n\t\t\t/* Check our ids match any regs they're supposed to */\n\t\t\treturn check_ids(rold->id, rcur->id, idmap);\n\t\t}\n\n\t\t/* If the new min/max/var_off satisfy the old ones and\n\t\t * everything else matches, we are OK.\n\t\t * 'id' is not compared, since it's only used for maps with\n\t\t * bpf_spin_lock inside map element and in such cases if\n\t\t * the rest of the prog is valid for one map element then\n\t\t * it's valid for all map elements regardless of the key\n\t\t * used in bpf_map_lookup()\n\t\t */\n\t\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&\n\t\t       range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET:\n\t\tif (rcur->type != rold->type)\n\t\t\treturn false;\n\t\t/* We must have at least as much range as the old ptr\n\t\t * did, so that any accesses which were safe before are\n\t\t * still safe.  This is true even if old range < old off,\n\t\t * since someone could have accessed through (ptr - k), or\n\t\t * even done ptr -= k in a register, to get a safe access.\n\t\t */\n\t\tif (rold->range > rcur->range)\n\t\t\treturn false;\n\t\t/* If the offsets don't match, we can't trust our alignment;\n\t\t * nor can we be sure that we won't fall out of range.\n\t\t */\n\t\tif (rold->off != rcur->off)\n\t\t\treturn false;\n\t\t/* id relations must be preserved */\n\t\tif (rold->id && !check_ids(rold->id, rcur->id, idmap))\n\t\t\treturn false;\n\t\t/* new val must satisfy old val knowledge */\n\t\treturn range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_CTX:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\t\t/* Only valid matches are exact, which memcmp() above\n\t\t * would have accepted\n\t\t */\n\tdefault:\n\t\t/* Don't know what's going on, just say it's not safe */\n\t\treturn false;\n\t}\n\n\t/* Shouldn't get here; if we do, say it's not safe */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "code_after_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tif (ptr_reg->type & PTR_MAYBE_NULL) {\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\t}\n\n\tswitch (base_type(ptr_reg->type)) {\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state,\n\t\t\t\t bool print_all)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tif (!print_all && !reg_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\tverbose(env, \"P\");\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tif (t == PTR_TO_BTF_ID ||\n\t\t\t    t == PTR_TO_BTF_ID_OR_NULL ||\n\t\t\t    t == PTR_TO_PERCPU_BTF_ID)\n\t\t\t\tverbose(env, \"%s\", kernel_type_name(reg->btf, reg->btf_id));\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (reg_type_may_be_refcounted_or_null(t))\n\t\t\t\tverbose(env, \",ref_obj_id=%d\", reg->ref_obj_id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (t == CONST_PTR_TO_MAP ||\n\t\t\t\t t == PTR_TO_MAP_KEY ||\n\t\t\t\t t == PTR_TO_MAP_VALUE ||\n\t\t\t\t t == PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t\tif (reg->s32_min_value != reg->smin_value &&\n\t\t\t\t    reg->s32_min_value != S32_MIN)\n\t\t\t\t\tverbose(env, \",s32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_min_value));\n\t\t\t\tif (reg->s32_max_value != reg->smax_value &&\n\t\t\t\t    reg->s32_max_value != S32_MAX)\n\t\t\t\t\tverbose(env, \",s32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_max_value));\n\t\t\t\tif (reg->u32_min_value != reg->umin_value &&\n\t\t\t\t    reg->u32_min_value != U32_MIN)\n\t\t\t\t\tverbose(env, \",u32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_min_value));\n\t\t\t\tif (reg->u32_max_value != reg->umax_value &&\n\t\t\t\t    reg->u32_max_value != U32_MAX)\n\t\t\t\t\tverbose(env, \",u32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_max_value));\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tif (!print_all && !stack_slot_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (is_spilled_reg(&state->stack[i])) {\n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tt = reg->type;\n\t\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\t\tverbose(env, \"P\");\n\t\t\tif (t == SCALAR_VALUE && tnum_is_const(reg->var_off))\n\t\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t\t}\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tif (state->in_callback_fn)\n\t\tverbose(env, \" cb\");\n\tif (state->in_async_callback_fn)\n\t\tverbose(env, \" async_cb\");\n\tverbose(env, \"\\n\");\n\tmark_verifier_state_clean(env);\n}",
                        "code_after_change": "static void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state,\n\t\t\t\t bool print_all)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tif (!print_all && !reg_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str(env, t));\n\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\tverbose(env, \"P\");\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tif (base_type(t) == PTR_TO_BTF_ID ||\n\t\t\t    base_type(t) == PTR_TO_PERCPU_BTF_ID)\n\t\t\t\tverbose(env, \"%s\", kernel_type_name(reg->btf, reg->btf_id));\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (reg_type_may_be_refcounted_or_null(t))\n\t\t\t\tverbose(env, \",ref_obj_id=%d\", reg->ref_obj_id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (base_type(t) == CONST_PTR_TO_MAP ||\n\t\t\t\t base_type(t) == PTR_TO_MAP_KEY ||\n\t\t\t\t base_type(t) == PTR_TO_MAP_VALUE)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t\tif (reg->s32_min_value != reg->smin_value &&\n\t\t\t\t    reg->s32_min_value != S32_MIN)\n\t\t\t\t\tverbose(env, \",s32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_min_value));\n\t\t\t\tif (reg->s32_max_value != reg->smax_value &&\n\t\t\t\t    reg->s32_max_value != S32_MAX)\n\t\t\t\t\tverbose(env, \",s32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_max_value));\n\t\t\t\tif (reg->u32_min_value != reg->umin_value &&\n\t\t\t\t    reg->u32_min_value != U32_MIN)\n\t\t\t\t\tverbose(env, \",u32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_min_value));\n\t\t\t\tif (reg->u32_max_value != reg->umax_value &&\n\t\t\t\t    reg->u32_max_value != U32_MAX)\n\t\t\t\t\tverbose(env, \",u32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_max_value));\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tif (!print_all && !stack_slot_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (is_spilled_reg(&state->stack[i])) {\n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tt = reg->type;\n\t\t\tverbose(env, \"=%s\", reg_type_str(env, t));\n\t\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\t\tverbose(env, \"P\");\n\t\t\tif (t == SCALAR_VALUE && tnum_is_const(reg->var_off))\n\t\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t\t}\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tif (state->in_callback_fn)\n\t\tverbose(env, \" cb\");\n\tif (state->in_async_callback_fn)\n\t\tverbose(env, \" async_cb\");\n\tverbose(env, \"\\n\");\n\tmark_verifier_state_clean(env);\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}",
                        "code_after_change": "static bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str(env, type), val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str(env, type), reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str(env, type));\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str(env, type));\n\t\treturn false;\n\t}\n\n\treturn true;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3579,
            "cve_id": "CVE-2022-23222",
            "code_snippet": "static bool reg_type_may_be_refcounted_or_null(enum bpf_reg_type type)\n{\n\treturn type == PTR_TO_SOCKET ||\n\t\ttype == PTR_TO_SOCKET_OR_NULL ||\n\t\ttype == PTR_TO_TCP_SOCK ||\n\t\ttype == PTR_TO_TCP_SOCK_OR_NULL ||\n\t\ttype == PTR_TO_MEM ||\n\t\ttype == PTR_TO_MEM_OR_NULL;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static bool reg_type_mismatch_ok(enum bpf_reg_type type)\n{\n\tswitch (type) {\n\tcase PTR_TO_CTX:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\tcase PTR_TO_BTF_ID:\n\tcase PTR_TO_BTF_ID_OR_NULL:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}",
                        "code_after_change": "static bool reg_type_mismatch_ok(enum bpf_reg_type type)\n{\n\tswitch (base_type(type)) {\n\tcase PTR_TO_CTX:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\tcase PTR_TO_BTF_ID:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void mark_ptr_not_null_reg(struct bpf_reg_state *reg)\n{\n\tswitch (reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL: {\n\t\tconst struct bpf_map *map = reg->map_ptr;\n\n\t\tif (map->inner_map_meta) {\n\t\t\treg->type = CONST_PTR_TO_MAP;\n\t\t\treg->map_ptr = map->inner_map_meta;\n\t\t\t/* transfer reg's id which is unique for every map_lookup_elem\n\t\t\t * as UID of the inner map.\n\t\t\t */\n\t\t\tif (map_value_has_timer(map->inner_map_meta))\n\t\t\t\treg->map_uid = reg->id;\n\t\t} else if (map->map_type == BPF_MAP_TYPE_XSKMAP) {\n\t\t\treg->type = PTR_TO_XDP_SOCK;\n\t\t} else if (map->map_type == BPF_MAP_TYPE_SOCKMAP ||\n\t\t\t   map->map_type == BPF_MAP_TYPE_SOCKHASH) {\n\t\t\treg->type = PTR_TO_SOCKET;\n\t\t} else {\n\t\t\treg->type = PTR_TO_MAP_VALUE;\n\t\t}\n\t\tbreak;\n\t}\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\treg->type = PTR_TO_SOCKET;\n\t\tbreak;\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\t\treg->type = PTR_TO_SOCK_COMMON;\n\t\tbreak;\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\t\treg->type = PTR_TO_TCP_SOCK;\n\t\tbreak;\n\tcase PTR_TO_BTF_ID_OR_NULL:\n\t\treg->type = PTR_TO_BTF_ID;\n\t\tbreak;\n\tcase PTR_TO_MEM_OR_NULL:\n\t\treg->type = PTR_TO_MEM;\n\t\tbreak;\n\tcase PTR_TO_RDONLY_BUF_OR_NULL:\n\t\treg->type = PTR_TO_RDONLY_BUF;\n\t\tbreak;\n\tcase PTR_TO_RDWR_BUF_OR_NULL:\n\t\treg->type = PTR_TO_RDWR_BUF;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"unknown nullable register type\");\n\t}\n}",
                        "code_after_change": "static bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str(env, type), val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str(env, type), reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str(env, type));\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str(env, type));\n\t\treturn false;\n\t}\n\n\treturn true;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}",
                        "code_after_change": "static void mark_ptr_not_null_reg(struct bpf_reg_state *reg)\n{\n\tif (base_type(reg->type) == PTR_TO_MAP_VALUE) {\n\t\tconst struct bpf_map *map = reg->map_ptr;\n\n\t\tif (map->inner_map_meta) {\n\t\t\treg->type = CONST_PTR_TO_MAP;\n\t\t\treg->map_ptr = map->inner_map_meta;\n\t\t\t/* transfer reg's id which is unique for every map_lookup_elem\n\t\t\t * as UID of the inner map.\n\t\t\t */\n\t\t\tif (map_value_has_timer(map->inner_map_meta))\n\t\t\t\treg->map_uid = reg->id;\n\t\t} else if (map->map_type == BPF_MAP_TYPE_XSKMAP) {\n\t\t\treg->type = PTR_TO_XDP_SOCK;\n\t\t} else if (map->map_type == BPF_MAP_TYPE_SOCKMAP ||\n\t\t\t   map->map_type == BPF_MAP_TYPE_SOCKHASH) {\n\t\t\treg->type = PTR_TO_SOCKET;\n\t\t} else {\n\t\t\treg->type = PTR_TO_MAP_VALUE;\n\t\t}\n\t\treturn;\n\t}\n\n\treg->type &= ~PTR_MAYBE_NULL;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static bool regsafe(struct bpf_verifier_env *env, struct bpf_reg_state *rold,\n\t\t    struct bpf_reg_state *rcur, struct bpf_id_pair *idmap)\n{\n\tbool equal;\n\n\tif (!(rold->live & REG_LIVE_READ))\n\t\t/* explored state didn't use this */\n\t\treturn true;\n\n\tequal = memcmp(rold, rcur, offsetof(struct bpf_reg_state, parent)) == 0;\n\n\tif (rold->type == PTR_TO_STACK)\n\t\t/* two stack pointers are equal only if they're pointing to\n\t\t * the same stack frame, since fp-8 in foo != fp-8 in bar\n\t\t */\n\t\treturn equal && rold->frameno == rcur->frameno;\n\n\tif (equal)\n\t\treturn true;\n\n\tif (rold->type == NOT_INIT)\n\t\t/* explored state can't have used this */\n\t\treturn true;\n\tif (rcur->type == NOT_INIT)\n\t\treturn false;\n\tswitch (rold->type) {\n\tcase SCALAR_VALUE:\n\t\tif (env->explore_alu_limits)\n\t\t\treturn false;\n\t\tif (rcur->type == SCALAR_VALUE) {\n\t\t\tif (!rold->precise && !rcur->precise)\n\t\t\t\treturn true;\n\t\t\t/* new val must satisfy old val knowledge */\n\t\t\treturn range_within(rold, rcur) &&\n\t\t\t       tnum_in(rold->var_off, rcur->var_off);\n\t\t} else {\n\t\t\t/* We're trying to use a pointer in place of a scalar.\n\t\t\t * Even if the scalar was unbounded, this could lead to\n\t\t\t * pointer leaks because scalars are allowed to leak\n\t\t\t * while pointers are not. We could make this safe in\n\t\t\t * special cases if root is calling us, but it's\n\t\t\t * probably not worth the hassle.\n\t\t\t */\n\t\t\treturn false;\n\t\t}\n\tcase PTR_TO_MAP_KEY:\n\tcase PTR_TO_MAP_VALUE:\n\t\t/* If the new min/max/var_off satisfy the old ones and\n\t\t * everything else matches, we are OK.\n\t\t * 'id' is not compared, since it's only used for maps with\n\t\t * bpf_spin_lock inside map element and in such cases if\n\t\t * the rest of the prog is valid for one map element then\n\t\t * it's valid for all map elements regardless of the key\n\t\t * used in bpf_map_lookup()\n\t\t */\n\t\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&\n\t\t       range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\t/* a PTR_TO_MAP_VALUE could be safe to use as a\n\t\t * PTR_TO_MAP_VALUE_OR_NULL into the same map.\n\t\t * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-\n\t\t * checked, doing so could have affected others with the same\n\t\t * id, and we can't check for that because we lost the id when\n\t\t * we converted to a PTR_TO_MAP_VALUE.\n\t\t */\n\t\tif (rcur->type != PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\treturn false;\n\t\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)))\n\t\t\treturn false;\n\t\t/* Check our ids match any regs they're supposed to */\n\t\treturn check_ids(rold->id, rcur->id, idmap);\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET:\n\t\tif (rcur->type != rold->type)\n\t\t\treturn false;\n\t\t/* We must have at least as much range as the old ptr\n\t\t * did, so that any accesses which were safe before are\n\t\t * still safe.  This is true even if old range < old off,\n\t\t * since someone could have accessed through (ptr - k), or\n\t\t * even done ptr -= k in a register, to get a safe access.\n\t\t */\n\t\tif (rold->range > rcur->range)\n\t\t\treturn false;\n\t\t/* If the offsets don't match, we can't trust our alignment;\n\t\t * nor can we be sure that we won't fall out of range.\n\t\t */\n\t\tif (rold->off != rcur->off)\n\t\t\treturn false;\n\t\t/* id relations must be preserved */\n\t\tif (rold->id && !check_ids(rold->id, rcur->id, idmap))\n\t\t\treturn false;\n\t\t/* new val must satisfy old val knowledge */\n\t\treturn range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_CTX:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\t/* Only valid matches are exact, which memcmp() above\n\t\t * would have accepted\n\t\t */\n\tdefault:\n\t\t/* Don't know what's going on, just say it's not safe */\n\t\treturn false;\n\t}\n\n\t/* Shouldn't get here; if we do, say it's not safe */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}",
                        "code_after_change": "static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_KEY:\n\t\treturn check_mem_region_access(env, regno, reg->off, access_size,\n\t\t\t\t\t       reg->map_ptr->key_size, false);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tcase PTR_TO_STACK:\n\t\treturn check_stack_range_initialized(\n\t\t\t\tenv,\n\t\t\t\tregno, reg->off, access_size,\n\t\t\t\tzero_size_allowed, ACCESS_HELPER, meta);\n\tdefault: /* scalar_value or invalid ptr */\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s \", regno,\n\t\t\treg_type_str(env, reg->type));\n\t\tverbose(env, \"expected=%s\\n\", reg_type_str(env, PTR_TO_STACK));\n\t\treturn -EACCES;\n\t}\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_KEY:\n\t\treturn check_mem_region_access(env, regno, reg->off, access_size,\n\t\t\t\t\t       reg->map_ptr->key_size, false);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tcase PTR_TO_STACK:\n\t\treturn check_stack_range_initialized(\n\t\t\t\tenv,\n\t\t\t\tregno, reg->off, access_size,\n\t\t\t\tzero_size_allowed, ACCESS_HELPER, meta);\n\tdefault: /* scalar_value or invalid ptr */\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s expected=%s\\n\", regno,\n\t\t\treg_type_str[reg->type],\n\t\t\treg_type_str[PTR_TO_STACK]);\n\t\treturn -EACCES;\n\t}\n}",
                        "code_after_change": "static bool regsafe(struct bpf_verifier_env *env, struct bpf_reg_state *rold,\n\t\t    struct bpf_reg_state *rcur, struct bpf_id_pair *idmap)\n{\n\tbool equal;\n\n\tif (!(rold->live & REG_LIVE_READ))\n\t\t/* explored state didn't use this */\n\t\treturn true;\n\n\tequal = memcmp(rold, rcur, offsetof(struct bpf_reg_state, parent)) == 0;\n\n\tif (rold->type == PTR_TO_STACK)\n\t\t/* two stack pointers are equal only if they're pointing to\n\t\t * the same stack frame, since fp-8 in foo != fp-8 in bar\n\t\t */\n\t\treturn equal && rold->frameno == rcur->frameno;\n\n\tif (equal)\n\t\treturn true;\n\n\tif (rold->type == NOT_INIT)\n\t\t/* explored state can't have used this */\n\t\treturn true;\n\tif (rcur->type == NOT_INIT)\n\t\treturn false;\n\tswitch (base_type(rold->type)) {\n\tcase SCALAR_VALUE:\n\t\tif (env->explore_alu_limits)\n\t\t\treturn false;\n\t\tif (rcur->type == SCALAR_VALUE) {\n\t\t\tif (!rold->precise && !rcur->precise)\n\t\t\t\treturn true;\n\t\t\t/* new val must satisfy old val knowledge */\n\t\t\treturn range_within(rold, rcur) &&\n\t\t\t       tnum_in(rold->var_off, rcur->var_off);\n\t\t} else {\n\t\t\t/* We're trying to use a pointer in place of a scalar.\n\t\t\t * Even if the scalar was unbounded, this could lead to\n\t\t\t * pointer leaks because scalars are allowed to leak\n\t\t\t * while pointers are not. We could make this safe in\n\t\t\t * special cases if root is calling us, but it's\n\t\t\t * probably not worth the hassle.\n\t\t\t */\n\t\t\treturn false;\n\t\t}\n\tcase PTR_TO_MAP_KEY:\n\tcase PTR_TO_MAP_VALUE:\n\t\t/* a PTR_TO_MAP_VALUE could be safe to use as a\n\t\t * PTR_TO_MAP_VALUE_OR_NULL into the same map.\n\t\t * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-\n\t\t * checked, doing so could have affected others with the same\n\t\t * id, and we can't check for that because we lost the id when\n\t\t * we converted to a PTR_TO_MAP_VALUE.\n\t\t */\n\t\tif (type_may_be_null(rold->type)) {\n\t\t\tif (!type_may_be_null(rcur->type))\n\t\t\t\treturn false;\n\t\t\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)))\n\t\t\t\treturn false;\n\t\t\t/* Check our ids match any regs they're supposed to */\n\t\t\treturn check_ids(rold->id, rcur->id, idmap);\n\t\t}\n\n\t\t/* If the new min/max/var_off satisfy the old ones and\n\t\t * everything else matches, we are OK.\n\t\t * 'id' is not compared, since it's only used for maps with\n\t\t * bpf_spin_lock inside map element and in such cases if\n\t\t * the rest of the prog is valid for one map element then\n\t\t * it's valid for all map elements regardless of the key\n\t\t * used in bpf_map_lookup()\n\t\t */\n\t\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&\n\t\t       range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET:\n\t\tif (rcur->type != rold->type)\n\t\t\treturn false;\n\t\t/* We must have at least as much range as the old ptr\n\t\t * did, so that any accesses which were safe before are\n\t\t * still safe.  This is true even if old range < old off,\n\t\t * since someone could have accessed through (ptr - k), or\n\t\t * even done ptr -= k in a register, to get a safe access.\n\t\t */\n\t\tif (rold->range > rcur->range)\n\t\t\treturn false;\n\t\t/* If the offsets don't match, we can't trust our alignment;\n\t\t * nor can we be sure that we won't fall out of range.\n\t\t */\n\t\tif (rold->off != rcur->off)\n\t\t\treturn false;\n\t\t/* id relations must be preserved */\n\t\tif (rold->id && !check_ids(rold->id, rcur->id, idmap))\n\t\t\treturn false;\n\t\t/* new val must satisfy old val knowledge */\n\t\treturn range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_CTX:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\t\t/* Only valid matches are exact, which memcmp() above\n\t\t * would have accepted\n\t\t */\n\tdefault:\n\t\t/* Don't know what's going on, just say it's not safe */\n\t\treturn false;\n\t}\n\n\t/* Shouldn't get here; if we do, say it's not safe */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 3580,
            "cve_id": "CVE-2022-23222",
            "code_snippet": "static int check_sock_access(struct bpf_verifier_env *env, int insn_idx,\n\t\t\t     u32 regno, int off, int size,\n\t\t\t     enum bpf_access_type t)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = &regs[regno];\n\tstruct bpf_insn_access_aux info = {};\n\tbool valid;\n\n\tif (reg->smin_value < 0) {\n\t\tverbose(env, \"R%d min value is negative, either use unsigned index or do a if (index >=0) check.\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (reg->type) {\n\tcase PTR_TO_SOCK_COMMON:\n\t\tvalid = bpf_sock_common_is_valid_access(off, size, t, &info);\n\t\tbreak;\n\tcase PTR_TO_SOCKET:\n\t\tvalid = bpf_sock_is_valid_access(off, size, t, &info);\n\t\tbreak;\n\tcase PTR_TO_TCP_SOCK:\n\t\tvalid = bpf_tcp_sock_is_valid_access(off, size, t, &info);\n\t\tbreak;\n\tcase PTR_TO_XDP_SOCK:\n\t\tvalid = bpf_xdp_sock_is_valid_access(off, size, t, &info);\n\t\tbreak;\n\tdefault:\n\t\tvalid = false;\n\t}\n\n\n\tif (valid) {\n\t\tenv->insn_aux_data[insn_idx].ctx_field_size =\n\t\t\tinfo.ctx_field_size;\n\t\treturn 0;\n\t}\n\n\tverbose(env, \"R%d invalid %s access off=%d size=%d\\n\",\n\t\tregno, reg_type_str[reg->type], off, size);\n\n\treturn -EACCES;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_KEY:\n\t\treturn check_mem_region_access(env, regno, reg->off, access_size,\n\t\t\t\t\t       reg->map_ptr->key_size, false);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tcase PTR_TO_STACK:\n\t\treturn check_stack_range_initialized(\n\t\t\t\tenv,\n\t\t\t\tregno, reg->off, access_size,\n\t\t\t\tzero_size_allowed, ACCESS_HELPER, meta);\n\tdefault: /* scalar_value or invalid ptr */\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s expected=%s\\n\", regno,\n\t\t\treg_type_str[reg->type],\n\t\t\treg_type_str[PTR_TO_STACK]);\n\t\treturn -EACCES;\n\t}\n}",
                        "code_after_change": "static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_KEY:\n\t\treturn check_mem_region_access(env, regno, reg->off, access_size,\n\t\t\t\t\t       reg->map_ptr->key_size, false);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tcase PTR_TO_STACK:\n\t\treturn check_stack_range_initialized(\n\t\t\t\tenv,\n\t\t\t\tregno, reg->off, access_size,\n\t\t\t\tzero_size_allowed, ACCESS_HELPER, meta);\n\tdefault: /* scalar_value or invalid ptr */\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s \", regno,\n\t\t\treg_type_str(env, reg->type));\n\t\tverbose(env, \"expected=%s\\n\", reg_type_str(env, PTR_TO_STACK));\n\t\treturn -EACCES;\n\t}\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3581,
            "cve_id": "CVE-2022-23222",
            "code_snippet": "static int do_check(struct bpf_verifier_env *env)\n{\n\tbool pop_log = !(env->log.level & BPF_LOG_LEVEL2);\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tstruct bpf_reg_state *regs;\n\tint insn_cnt = env->prog->len;\n\tbool do_print_state = false;\n\tint prev_insn_idx = -1;\n\n\tfor (;;) {\n\t\tstruct bpf_insn *insn;\n\t\tu8 class;\n\t\tint err;\n\n\t\tenv->prev_insn_idx = prev_insn_idx;\n\t\tif (env->insn_idx >= insn_cnt) {\n\t\t\tverbose(env, \"invalid insn idx %d insn_cnt %d\\n\",\n\t\t\t\tenv->insn_idx, insn_cnt);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tinsn = &insns[env->insn_idx];\n\t\tclass = BPF_CLASS(insn->code);\n\n\t\tif (++env->insn_processed > BPF_COMPLEXITY_LIMIT_INSNS) {\n\t\t\tverbose(env,\n\t\t\t\t\"BPF program is too large. Processed %d insn\\n\",\n\t\t\t\tenv->insn_processed);\n\t\t\treturn -E2BIG;\n\t\t}\n\n\t\terr = is_state_visited(env, env->insn_idx);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tif (err == 1) {\n\t\t\t/* found equivalent state, can prune the search */\n\t\t\tif (env->log.level & BPF_LOG_LEVEL) {\n\t\t\t\tif (do_print_state)\n\t\t\t\t\tverbose(env, \"\\nfrom %d to %d%s: safe\\n\",\n\t\t\t\t\t\tenv->prev_insn_idx, env->insn_idx,\n\t\t\t\t\t\tenv->cur_state->speculative ?\n\t\t\t\t\t\t\" (speculative execution)\" : \"\");\n\t\t\t\telse\n\t\t\t\t\tverbose(env, \"%d: safe\\n\", env->insn_idx);\n\t\t\t}\n\t\t\tgoto process_bpf_exit;\n\t\t}\n\n\t\tif (signal_pending(current))\n\t\t\treturn -EAGAIN;\n\n\t\tif (need_resched())\n\t\t\tcond_resched();\n\n\t\tif (env->log.level & BPF_LOG_LEVEL2 && do_print_state) {\n\t\t\tverbose(env, \"\\nfrom %d to %d%s:\",\n\t\t\t\tenv->prev_insn_idx, env->insn_idx,\n\t\t\t\tenv->cur_state->speculative ?\n\t\t\t\t\" (speculative execution)\" : \"\");\n\t\t\tprint_verifier_state(env, state->frame[state->curframe], true);\n\t\t\tdo_print_state = false;\n\t\t}\n\n\t\tif (env->log.level & BPF_LOG_LEVEL) {\n\t\t\tconst struct bpf_insn_cbs cbs = {\n\t\t\t\t.cb_call\t= disasm_kfunc_name,\n\t\t\t\t.cb_print\t= verbose,\n\t\t\t\t.private_data\t= env,\n\t\t\t};\n\n\t\t\tif (verifier_state_scratched(env))\n\t\t\t\tprint_insn_state(env, state->frame[state->curframe]);\n\n\t\t\tverbose_linfo(env, env->insn_idx, \"; \");\n\t\t\tenv->prev_log_len = env->log.len_used;\n\t\t\tverbose(env, \"%d: \", env->insn_idx);\n\t\t\tprint_bpf_insn(&cbs, insn, env->allow_ptr_leaks);\n\t\t\tenv->prev_insn_print_len = env->log.len_used - env->prev_log_len;\n\t\t\tenv->prev_log_len = env->log.len_used;\n\t\t}\n\n\t\tif (bpf_prog_is_dev_bound(env->prog->aux)) {\n\t\t\terr = bpf_prog_offload_verify_insn(env, env->insn_idx,\n\t\t\t\t\t\t\t   env->prev_insn_idx);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tregs = cur_regs(env);\n\t\tsanitize_mark_insn_seen(env);\n\t\tprev_insn_idx = env->insn_idx;\n\n\t\tif (class == BPF_ALU || class == BPF_ALU64) {\n\t\t\terr = check_alu_op(env, insn);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_LDX) {\n\t\t\tenum bpf_reg_type *prev_src_type, src_reg_type;\n\n\t\t\t/* check for reserved fields is already done */\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tsrc_reg_type = regs[insn->src_reg].type;\n\n\t\t\t/* check that memory (src_reg + off) is readable,\n\t\t\t * the state of dst_reg will be updated by this func\n\t\t\t */\n\t\t\terr = check_mem_access(env, env->insn_idx, insn->src_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_READ, insn->dst_reg, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_src_type = &env->insn_aux_data[env->insn_idx].ptr_type;\n\n\t\t\tif (*prev_src_type == NOT_INIT) {\n\t\t\t\t/* saw a valid insn\n\t\t\t\t * dst_reg = *(u32 *)(src_reg + off)\n\t\t\t\t * save type to validate intersecting paths\n\t\t\t\t */\n\t\t\t\t*prev_src_type = src_reg_type;\n\n\t\t\t} else if (reg_type_mismatch(src_reg_type, *prev_src_type)) {\n\t\t\t\t/* ABuser program is trying to use the same insn\n\t\t\t\t * dst_reg = *(u32*) (src_reg + off)\n\t\t\t\t * with different pointer types:\n\t\t\t\t * src_reg == ctx in one branch and\n\t\t\t\t * src_reg == stack|map in some other branch.\n\t\t\t\t * Reject it.\n\t\t\t\t */\n\t\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_STX) {\n\t\t\tenum bpf_reg_type *prev_dst_type, dst_reg_type;\n\n\t\t\tif (BPF_MODE(insn->code) == BPF_ATOMIC) {\n\t\t\t\terr = check_atomic(env, env->insn_idx, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\tenv->insn_idx++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (BPF_MODE(insn->code) != BPF_MEM || insn->imm != 0) {\n\t\t\t\tverbose(env, \"BPF_STX uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\t/* check src2 operand */\n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tdst_reg_type = regs[insn->dst_reg].type;\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, env->insn_idx, insn->dst_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_WRITE, insn->src_reg, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_dst_type = &env->insn_aux_data[env->insn_idx].ptr_type;\n\n\t\t\tif (*prev_dst_type == NOT_INIT) {\n\t\t\t\t*prev_dst_type = dst_reg_type;\n\t\t\t} else if (reg_type_mismatch(dst_reg_type, *prev_dst_type)) {\n\t\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_ST) {\n\t\t\tif (BPF_MODE(insn->code) != BPF_MEM ||\n\t\t\t    insn->src_reg != BPF_REG_0) {\n\t\t\t\tverbose(env, \"BPF_ST uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tif (is_ctx_reg(env, insn->dst_reg)) {\n\t\t\t\tverbose(env, \"BPF_ST stores into R%d %s is not allowed\\n\",\n\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\treg_type_str[reg_state(env, insn->dst_reg)->type]);\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, env->insn_idx, insn->dst_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_WRITE, -1, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_JMP || class == BPF_JMP32) {\n\t\t\tu8 opcode = BPF_OP(insn->code);\n\n\t\t\tenv->jmps_processed++;\n\t\t\tif (opcode == BPF_CALL) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    (insn->src_reg != BPF_PSEUDO_KFUNC_CALL\n\t\t\t\t     && insn->off != 0) ||\n\t\t\t\t    (insn->src_reg != BPF_REG_0 &&\n\t\t\t\t     insn->src_reg != BPF_PSEUDO_CALL &&\n\t\t\t\t     insn->src_reg != BPF_PSEUDO_KFUNC_CALL) ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0 ||\n\t\t\t\t    class == BPF_JMP32) {\n\t\t\t\t\tverbose(env, \"BPF_CALL uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (env->cur_state->active_spin_lock &&\n\t\t\t\t    (insn->src_reg == BPF_PSEUDO_CALL ||\n\t\t\t\t     insn->imm != BPF_FUNC_spin_unlock)) {\n\t\t\t\t\tverbose(env, \"function calls are not allowed while holding a lock\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tif (insn->src_reg == BPF_PSEUDO_CALL)\n\t\t\t\t\terr = check_func_call(env, insn, &env->insn_idx);\n\t\t\t\telse if (insn->src_reg == BPF_PSEUDO_KFUNC_CALL)\n\t\t\t\t\terr = check_kfunc_call(env, insn);\n\t\t\t\telse\n\t\t\t\t\terr = check_helper_call(env, insn, &env->insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t} else if (opcode == BPF_JA) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0 ||\n\t\t\t\t    class == BPF_JMP32) {\n\t\t\t\t\tverbose(env, \"BPF_JA uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tenv->insn_idx += insn->off + 1;\n\t\t\t\tcontinue;\n\n\t\t\t} else if (opcode == BPF_EXIT) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0 ||\n\t\t\t\t    class == BPF_JMP32) {\n\t\t\t\t\tverbose(env, \"BPF_EXIT uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (env->cur_state->active_spin_lock) {\n\t\t\t\t\tverbose(env, \"bpf_spin_unlock is missing\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (state->curframe) {\n\t\t\t\t\t/* exit from nested function */\n\t\t\t\t\terr = prepare_func_exit(env, &env->insn_idx);\n\t\t\t\t\tif (err)\n\t\t\t\t\t\treturn err;\n\t\t\t\t\tdo_print_state = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\terr = check_reference_leak(env);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\terr = check_return_code(env);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\nprocess_bpf_exit:\n\t\t\t\tmark_verifier_state_scratched(env);\n\t\t\t\tupdate_branch_counts(env, env->cur_state);\n\t\t\t\terr = pop_stack(env, &prev_insn_idx,\n\t\t\t\t\t\t&env->insn_idx, pop_log);\n\t\t\t\tif (err < 0) {\n\t\t\t\t\tif (err != -ENOENT)\n\t\t\t\t\t\treturn err;\n\t\t\t\t\tbreak;\n\t\t\t\t} else {\n\t\t\t\t\tdo_print_state = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terr = check_cond_jmp_op(env, insn, &env->insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t} else if (class == BPF_LD) {\n\t\t\tu8 mode = BPF_MODE(insn->code);\n\n\t\t\tif (mode == BPF_ABS || mode == BPF_IND) {\n\t\t\t\terr = check_ld_abs(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (mode == BPF_IMM) {\n\t\t\t\terr = check_ld_imm(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tenv->insn_idx++;\n\t\t\t\tsanitize_mark_insn_seen(env);\n\t\t\t} else {\n\t\t\t\tverbose(env, \"invalid BPF_LD mode\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tverbose(env, \"unknown insn class %d\\n\", class);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tenv->insn_idx++;\n\t}\n\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "code_after_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tif (ptr_reg->type & PTR_MAYBE_NULL) {\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\t}\n\n\tswitch (base_type(ptr_reg->type)) {\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct tnum enforce_attach_type_range = tnum_unknown;\n\tconst struct bpf_prog *prog = env->prog;\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\tenum bpf_prog_type prog_type = resolve_prog_type(env->prog);\n\tint err;\n\tstruct bpf_func_state *frame = env->cur_state->frame[0];\n\tconst bool is_subprog = frame->subprogno;\n\n\t/* LSM and struct_ops func-ptr's return type could be \"void\" */\n\tif (!is_subprog &&\n\t    (prog_type == BPF_PROG_TYPE_STRUCT_OPS ||\n\t     prog_type == BPF_PROG_TYPE_LSM) &&\n\t    !prog->aux->attach_func_proto->type)\n\t\treturn 0;\n\n\t/* eBPF calling convention is such that R0 is used\n\t * to return the value from eBPF program.\n\t * Make sure that it's readable at this time\n\t * of bpf_exit, which means that program wrote\n\t * something into it earlier\n\t */\n\terr = check_reg_arg(env, BPF_REG_0, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\tverbose(env, \"R0 leaks addr as return value\\n\");\n\t\treturn -EACCES;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\n\tif (frame->in_async_callback_fn) {\n\t\t/* enforce return zero from async callbacks like timer */\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"In async callback the register R0 is not a known value (%s)\\n\",\n\t\t\t\treg_type_str[reg->type]);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!tnum_in(tnum_const(0), reg->var_off)) {\n\t\t\tverbose_invalid_scalar(env, reg, &range, \"async callback\", \"R0\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (is_subprog) {\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"At subprogram exit the register R0 is not a scalar value (%s)\\n\",\n\t\t\t\treg_type_str[reg->type]);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tswitch (prog_type) {\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_UDP4_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_UDP6_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETSOCKNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETSOCKNAME)\n\t\t\trange = tnum_range(1, 1);\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET4_BIND ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_BIND)\n\t\t\trange = tnum_range(0, 3);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET_EGRESS) {\n\t\t\trange = tnum_range(0, 3);\n\t\t\tenforce_attach_type_range = tnum_range(2, 3);\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\tcase BPF_PROG_TYPE_CGROUP_SYSCTL:\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\t\tbreak;\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT:\n\t\tif (!env->prog->aux->attach_btf_id)\n\t\t\treturn 0;\n\t\trange = tnum_const(0);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_TRACING:\n\t\tswitch (env->prog->expected_attach_type) {\n\t\tcase BPF_TRACE_FENTRY:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\trange = tnum_const(0);\n\t\t\tbreak;\n\t\tcase BPF_TRACE_RAW_TP:\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\treturn 0;\n\t\tcase BPF_TRACE_ITER:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -ENOTSUPP;\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_SK_LOOKUP:\n\t\trange = tnum_range(SK_DROP, SK_PASS);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_EXT:\n\t\t/* freplace program can return anything as its return value\n\t\t * depends on the to-be-replaced kernel func or bpf program.\n\t\t */\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose_invalid_scalar(env, reg, &range, \"program exit\", \"R0\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_is_unknown(enforce_attach_type_range) &&\n\t    tnum_in(enforce_attach_type_range, reg->var_off))\n\t\tenv->prog->enforce_expected_attach_type = 1;\n\treturn 0;\n}",
                        "code_after_change": "static int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct tnum enforce_attach_type_range = tnum_unknown;\n\tconst struct bpf_prog *prog = env->prog;\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\tenum bpf_prog_type prog_type = resolve_prog_type(env->prog);\n\tint err;\n\tstruct bpf_func_state *frame = env->cur_state->frame[0];\n\tconst bool is_subprog = frame->subprogno;\n\n\t/* LSM and struct_ops func-ptr's return type could be \"void\" */\n\tif (!is_subprog &&\n\t    (prog_type == BPF_PROG_TYPE_STRUCT_OPS ||\n\t     prog_type == BPF_PROG_TYPE_LSM) &&\n\t    !prog->aux->attach_func_proto->type)\n\t\treturn 0;\n\n\t/* eBPF calling convention is such that R0 is used\n\t * to return the value from eBPF program.\n\t * Make sure that it's readable at this time\n\t * of bpf_exit, which means that program wrote\n\t * something into it earlier\n\t */\n\terr = check_reg_arg(env, BPF_REG_0, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\tverbose(env, \"R0 leaks addr as return value\\n\");\n\t\treturn -EACCES;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\n\tif (frame->in_async_callback_fn) {\n\t\t/* enforce return zero from async callbacks like timer */\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"In async callback the register R0 is not a known value (%s)\\n\",\n\t\t\t\treg_type_str(env, reg->type));\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!tnum_in(tnum_const(0), reg->var_off)) {\n\t\t\tverbose_invalid_scalar(env, reg, &range, \"async callback\", \"R0\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (is_subprog) {\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"At subprogram exit the register R0 is not a scalar value (%s)\\n\",\n\t\t\t\treg_type_str(env, reg->type));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tswitch (prog_type) {\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_UDP4_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_UDP6_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETSOCKNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETSOCKNAME)\n\t\t\trange = tnum_range(1, 1);\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET4_BIND ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_BIND)\n\t\t\trange = tnum_range(0, 3);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET_EGRESS) {\n\t\t\trange = tnum_range(0, 3);\n\t\t\tenforce_attach_type_range = tnum_range(2, 3);\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\tcase BPF_PROG_TYPE_CGROUP_SYSCTL:\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\t\tbreak;\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT:\n\t\tif (!env->prog->aux->attach_btf_id)\n\t\t\treturn 0;\n\t\trange = tnum_const(0);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_TRACING:\n\t\tswitch (env->prog->expected_attach_type) {\n\t\tcase BPF_TRACE_FENTRY:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\trange = tnum_const(0);\n\t\t\tbreak;\n\t\tcase BPF_TRACE_RAW_TP:\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\treturn 0;\n\t\tcase BPF_TRACE_ITER:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -ENOTSUPP;\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_SK_LOOKUP:\n\t\trange = tnum_range(SK_DROP, SK_PASS);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_EXT:\n\t\t/* freplace program can return anything as its return value\n\t\t * depends on the to-be-replaced kernel func or bpf program.\n\t\t */\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str(env, reg->type));\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose_invalid_scalar(env, reg, &range, \"program exit\", \"R0\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_is_unknown(enforce_attach_type_range) &&\n\t    tnum_in(enforce_attach_type_range, reg->var_off))\n\t\tenv->prog->enforce_expected_attach_type = 1;\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state,\n\t\t\t\t bool print_all)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tif (!print_all && !reg_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\tverbose(env, \"P\");\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tif (t == PTR_TO_BTF_ID ||\n\t\t\t    t == PTR_TO_BTF_ID_OR_NULL ||\n\t\t\t    t == PTR_TO_PERCPU_BTF_ID)\n\t\t\t\tverbose(env, \"%s\", kernel_type_name(reg->btf, reg->btf_id));\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (reg_type_may_be_refcounted_or_null(t))\n\t\t\t\tverbose(env, \",ref_obj_id=%d\", reg->ref_obj_id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (t == CONST_PTR_TO_MAP ||\n\t\t\t\t t == PTR_TO_MAP_KEY ||\n\t\t\t\t t == PTR_TO_MAP_VALUE ||\n\t\t\t\t t == PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t\tif (reg->s32_min_value != reg->smin_value &&\n\t\t\t\t    reg->s32_min_value != S32_MIN)\n\t\t\t\t\tverbose(env, \",s32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_min_value));\n\t\t\t\tif (reg->s32_max_value != reg->smax_value &&\n\t\t\t\t    reg->s32_max_value != S32_MAX)\n\t\t\t\t\tverbose(env, \",s32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_max_value));\n\t\t\t\tif (reg->u32_min_value != reg->umin_value &&\n\t\t\t\t    reg->u32_min_value != U32_MIN)\n\t\t\t\t\tverbose(env, \",u32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_min_value));\n\t\t\t\tif (reg->u32_max_value != reg->umax_value &&\n\t\t\t\t    reg->u32_max_value != U32_MAX)\n\t\t\t\t\tverbose(env, \",u32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_max_value));\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tif (!print_all && !stack_slot_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (is_spilled_reg(&state->stack[i])) {\n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tt = reg->type;\n\t\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\t\tverbose(env, \"P\");\n\t\t\tif (t == SCALAR_VALUE && tnum_is_const(reg->var_off))\n\t\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t\t}\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tif (state->in_callback_fn)\n\t\tverbose(env, \" cb\");\n\tif (state->in_async_callback_fn)\n\t\tverbose(env, \" async_cb\");\n\tverbose(env, \"\\n\");\n\tmark_verifier_state_clean(env);\n}",
                        "code_after_change": "static void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state,\n\t\t\t\t bool print_all)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tif (!print_all && !reg_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str(env, t));\n\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\tverbose(env, \"P\");\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tif (base_type(t) == PTR_TO_BTF_ID ||\n\t\t\t    base_type(t) == PTR_TO_PERCPU_BTF_ID)\n\t\t\t\tverbose(env, \"%s\", kernel_type_name(reg->btf, reg->btf_id));\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (reg_type_may_be_refcounted_or_null(t))\n\t\t\t\tverbose(env, \",ref_obj_id=%d\", reg->ref_obj_id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (base_type(t) == CONST_PTR_TO_MAP ||\n\t\t\t\t base_type(t) == PTR_TO_MAP_KEY ||\n\t\t\t\t base_type(t) == PTR_TO_MAP_VALUE)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t\tif (reg->s32_min_value != reg->smin_value &&\n\t\t\t\t    reg->s32_min_value != S32_MIN)\n\t\t\t\t\tverbose(env, \",s32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_min_value));\n\t\t\t\tif (reg->s32_max_value != reg->smax_value &&\n\t\t\t\t    reg->s32_max_value != S32_MAX)\n\t\t\t\t\tverbose(env, \",s32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_max_value));\n\t\t\t\tif (reg->u32_min_value != reg->umin_value &&\n\t\t\t\t    reg->u32_min_value != U32_MIN)\n\t\t\t\t\tverbose(env, \",u32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_min_value));\n\t\t\t\tif (reg->u32_max_value != reg->umax_value &&\n\t\t\t\t    reg->u32_max_value != U32_MAX)\n\t\t\t\t\tverbose(env, \",u32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_max_value));\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tif (!print_all && !stack_slot_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (is_spilled_reg(&state->stack[i])) {\n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tt = reg->type;\n\t\t\tverbose(env, \"=%s\", reg_type_str(env, t));\n\t\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\t\tverbose(env, \"P\");\n\t\t\tif (t == SCALAR_VALUE && tnum_is_const(reg->var_off))\n\t\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t\t}\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tif (state->in_callback_fn)\n\t\tverbose(env, \" cb\");\n\tif (state->in_async_callback_fn)\n\t\tverbose(env, \" async_cb\");\n\tverbose(env, \"\\n\");\n\tmark_verifier_state_clean(env);\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3582,
            "cve_id": "CVE-2022-23222",
            "code_snippet": "static int check_mem_access(struct bpf_verifier_env *env, int insn_idx, u32 regno,\n\t\t\t    int off, int bpf_size, enum bpf_access_type t,\n\t\t\t    int value_regno, bool strict_alignment_once)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = regs + regno;\n\tstruct bpf_func_state *state;\n\tint size, err = 0;\n\n\tsize = bpf_size_to_bytes(bpf_size);\n\tif (size < 0)\n\t\treturn size;\n\n\t/* alignment checks will add in reg->off themselves */\n\terr = check_ptr_alignment(env, reg, off, size, strict_alignment_once);\n\tif (err)\n\t\treturn err;\n\n\t/* for access checks, reg->off is just part of off */\n\toff += reg->off;\n\n\tif (reg->type == PTR_TO_MAP_KEY) {\n\t\tif (t == BPF_WRITE) {\n\t\t\tverbose(env, \"write to change key R%d not allowed\\n\", regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\terr = check_mem_region_access(env, regno, off, size,\n\t\t\t\t\t      reg->map_ptr->key_size, false);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_MAP_VALUE) {\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into map\\n\", value_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_map_access_type(env, regno, off, size, t);\n\t\tif (err)\n\t\t\treturn err;\n\t\terr = check_map_access(env, regno, off, size, false);\n\t\tif (!err && t == BPF_READ && value_regno >= 0) {\n\t\t\tstruct bpf_map *map = reg->map_ptr;\n\n\t\t\t/* if map is read-only, track its contents as scalars */\n\t\t\tif (tnum_is_const(reg->var_off) &&\n\t\t\t    bpf_map_is_rdonly(map) &&\n\t\t\t    map->ops->map_direct_value_addr) {\n\t\t\t\tint map_off = off + reg->var_off.value;\n\t\t\t\tu64 val = 0;\n\n\t\t\t\terr = bpf_map_direct_read(map, map_off, size,\n\t\t\t\t\t\t\t  &val);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tregs[value_regno].type = SCALAR_VALUE;\n\t\t\t\t__mark_reg_known(&regs[value_regno], val);\n\t\t\t} else {\n\t\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t\t\t}\n\t\t}\n\t} else if (reg->type == PTR_TO_MEM) {\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into mem\\n\", value_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_mem_region_access(env, regno, off, size,\n\t\t\t\t\t      reg->mem_size, false);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_CTX) {\n\t\tenum bpf_reg_type reg_type = SCALAR_VALUE;\n\t\tstruct btf *btf = NULL;\n\t\tu32 btf_id = 0;\n\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into ctx\\n\", value_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\terr = check_ctx_reg(env, reg, regno);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\terr = check_ctx_access(env, insn_idx, off, size, t, &reg_type, &btf, &btf_id);\n\t\tif (err)\n\t\t\tverbose_linfo(env, insn_idx, \"; \");\n\t\tif (!err && t == BPF_READ && value_regno >= 0) {\n\t\t\t/* ctx access returns either a scalar, or a\n\t\t\t * PTR_TO_PACKET[_META,_END]. In the latter\n\t\t\t * case, we know the offset is zero.\n\t\t\t */\n\t\t\tif (reg_type == SCALAR_VALUE) {\n\t\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t\t\t} else {\n\t\t\t\tmark_reg_known_zero(env, regs,\n\t\t\t\t\t\t    value_regno);\n\t\t\t\tif (reg_type_may_be_null(reg_type))\n\t\t\t\t\tregs[value_regno].id = ++env->id_gen;\n\t\t\t\t/* A load of ctx field could have different\n\t\t\t\t * actual load size with the one encoded in the\n\t\t\t\t * insn. When the dst is PTR, it is for sure not\n\t\t\t\t * a sub-register.\n\t\t\t\t */\n\t\t\t\tregs[value_regno].subreg_def = DEF_NOT_SUBREG;\n\t\t\t\tif (reg_type == PTR_TO_BTF_ID ||\n\t\t\t\t    reg_type == PTR_TO_BTF_ID_OR_NULL) {\n\t\t\t\t\tregs[value_regno].btf = btf;\n\t\t\t\t\tregs[value_regno].btf_id = btf_id;\n\t\t\t\t}\n\t\t\t}\n\t\t\tregs[value_regno].type = reg_type;\n\t\t}\n\n\t} else if (reg->type == PTR_TO_STACK) {\n\t\t/* Basic bounds checks. */\n\t\terr = check_stack_access_within_bounds(env, regno, off, size, ACCESS_DIRECT, t);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tstate = func(env, reg);\n\t\terr = update_stack_depth(env, state, off);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (t == BPF_READ)\n\t\t\terr = check_stack_read(env, regno, off, size,\n\t\t\t\t\t       value_regno);\n\t\telse\n\t\t\terr = check_stack_write(env, regno, off, size,\n\t\t\t\t\t\tvalue_regno, insn_idx);\n\t} else if (reg_is_pkt_pointer(reg)) {\n\t\tif (t == BPF_WRITE && !may_access_direct_pkt_data(env, NULL, t)) {\n\t\t\tverbose(env, \"cannot write into packet\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into packet\\n\",\n\t\t\t\tvalue_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_packet_access(env, regno, off, size, false);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_FLOW_KEYS) {\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into flow keys\\n\",\n\t\t\t\tvalue_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\terr = check_flow_keys_access(env, off, size);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (type_is_sk_pointer(reg->type)) {\n\t\tif (t == BPF_WRITE) {\n\t\t\tverbose(env, \"R%d cannot write into %s\\n\",\n\t\t\t\tregno, reg_type_str[reg->type]);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_sock_access(env, insn_idx, regno, off, size, t);\n\t\tif (!err && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_TP_BUFFER) {\n\t\terr = check_tp_buffer_access(env, reg, regno, off, size);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_BTF_ID) {\n\t\terr = check_ptr_to_btf_access(env, regs, regno, off, size, t,\n\t\t\t\t\t      value_regno);\n\t} else if (reg->type == CONST_PTR_TO_MAP) {\n\t\terr = check_ptr_to_map_access(env, regs, regno, off, size, t,\n\t\t\t\t\t      value_regno);\n\t} else if (reg->type == PTR_TO_RDONLY_BUF) {\n\t\tif (t == BPF_WRITE) {\n\t\t\tverbose(env, \"R%d cannot write into %s\\n\",\n\t\t\t\tregno, reg_type_str[reg->type]);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_buffer_access(env, reg, regno, off, size, false,\n\t\t\t\t\t  \"rdonly\",\n\t\t\t\t\t  &env->prog->aux->max_rdonly_access);\n\t\tif (!err && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_RDWR_BUF) {\n\t\terr = check_buffer_access(env, reg, regno, off, size, false,\n\t\t\t\t\t  \"rdwr\",\n\t\t\t\t\t  &env->prog->aux->max_rdwr_access);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else {\n\t\tverbose(env, \"R%d invalid mem access '%s'\\n\", regno,\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!err && size < BPF_REG_SIZE && value_regno >= 0 && t == BPF_READ &&\n\t    regs[value_regno].type == SCALAR_VALUE) {\n\t\t/* b/h/w load zero-extends, mark upper bits as known 0 */\n\t\tcoerce_reg_to_size(&regs[value_regno], size);\n\t}\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_KEY:\n\t\treturn check_mem_region_access(env, regno, reg->off, access_size,\n\t\t\t\t\t       reg->map_ptr->key_size, false);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tcase PTR_TO_STACK:\n\t\treturn check_stack_range_initialized(\n\t\t\t\tenv,\n\t\t\t\tregno, reg->off, access_size,\n\t\t\t\tzero_size_allowed, ACCESS_HELPER, meta);\n\tdefault: /* scalar_value or invalid ptr */\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s expected=%s\\n\", regno,\n\t\t\treg_type_str[reg->type],\n\t\t\treg_type_str[PTR_TO_STACK]);\n\t\treturn -EACCES;\n\t}\n}",
                        "code_after_change": "static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_KEY:\n\t\treturn check_mem_region_access(env, regno, reg->off, access_size,\n\t\t\t\t\t       reg->map_ptr->key_size, false);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tcase PTR_TO_STACK:\n\t\treturn check_stack_range_initialized(\n\t\t\t\tenv,\n\t\t\t\tregno, reg->off, access_size,\n\t\t\t\tzero_size_allowed, ACCESS_HELPER, meta);\n\tdefault: /* scalar_value or invalid ptr */\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s \", regno,\n\t\t\treg_type_str(env, reg->type));\n\t\tverbose(env, \"expected=%s\\n\", reg_type_str(env, PTR_TO_STACK));\n\t\treturn -EACCES;\n\t}\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state,\n\t\t\t\t bool print_all)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tif (!print_all && !reg_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\tverbose(env, \"P\");\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tif (t == PTR_TO_BTF_ID ||\n\t\t\t    t == PTR_TO_BTF_ID_OR_NULL ||\n\t\t\t    t == PTR_TO_PERCPU_BTF_ID)\n\t\t\t\tverbose(env, \"%s\", kernel_type_name(reg->btf, reg->btf_id));\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (reg_type_may_be_refcounted_or_null(t))\n\t\t\t\tverbose(env, \",ref_obj_id=%d\", reg->ref_obj_id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (t == CONST_PTR_TO_MAP ||\n\t\t\t\t t == PTR_TO_MAP_KEY ||\n\t\t\t\t t == PTR_TO_MAP_VALUE ||\n\t\t\t\t t == PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t\tif (reg->s32_min_value != reg->smin_value &&\n\t\t\t\t    reg->s32_min_value != S32_MIN)\n\t\t\t\t\tverbose(env, \",s32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_min_value));\n\t\t\t\tif (reg->s32_max_value != reg->smax_value &&\n\t\t\t\t    reg->s32_max_value != S32_MAX)\n\t\t\t\t\tverbose(env, \",s32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_max_value));\n\t\t\t\tif (reg->u32_min_value != reg->umin_value &&\n\t\t\t\t    reg->u32_min_value != U32_MIN)\n\t\t\t\t\tverbose(env, \",u32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_min_value));\n\t\t\t\tif (reg->u32_max_value != reg->umax_value &&\n\t\t\t\t    reg->u32_max_value != U32_MAX)\n\t\t\t\t\tverbose(env, \",u32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_max_value));\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tif (!print_all && !stack_slot_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (is_spilled_reg(&state->stack[i])) {\n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tt = reg->type;\n\t\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\t\tverbose(env, \"P\");\n\t\t\tif (t == SCALAR_VALUE && tnum_is_const(reg->var_off))\n\t\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t\t}\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tif (state->in_callback_fn)\n\t\tverbose(env, \" cb\");\n\tif (state->in_async_callback_fn)\n\t\tverbose(env, \" async_cb\");\n\tverbose(env, \"\\n\");\n\tmark_verifier_state_clean(env);\n}",
                        "code_after_change": "static void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state,\n\t\t\t\t bool print_all)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tif (!print_all && !reg_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str(env, t));\n\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\tverbose(env, \"P\");\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tif (base_type(t) == PTR_TO_BTF_ID ||\n\t\t\t    base_type(t) == PTR_TO_PERCPU_BTF_ID)\n\t\t\t\tverbose(env, \"%s\", kernel_type_name(reg->btf, reg->btf_id));\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (reg_type_may_be_refcounted_or_null(t))\n\t\t\t\tverbose(env, \",ref_obj_id=%d\", reg->ref_obj_id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (base_type(t) == CONST_PTR_TO_MAP ||\n\t\t\t\t base_type(t) == PTR_TO_MAP_KEY ||\n\t\t\t\t base_type(t) == PTR_TO_MAP_VALUE)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t\tif (reg->s32_min_value != reg->smin_value &&\n\t\t\t\t    reg->s32_min_value != S32_MIN)\n\t\t\t\t\tverbose(env, \",s32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_min_value));\n\t\t\t\tif (reg->s32_max_value != reg->smax_value &&\n\t\t\t\t    reg->s32_max_value != S32_MAX)\n\t\t\t\t\tverbose(env, \",s32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_max_value));\n\t\t\t\tif (reg->u32_min_value != reg->umin_value &&\n\t\t\t\t    reg->u32_min_value != U32_MIN)\n\t\t\t\t\tverbose(env, \",u32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_min_value));\n\t\t\t\tif (reg->u32_max_value != reg->umax_value &&\n\t\t\t\t    reg->u32_max_value != U32_MAX)\n\t\t\t\t\tverbose(env, \",u32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_max_value));\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tif (!print_all && !stack_slot_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (is_spilled_reg(&state->stack[i])) {\n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tt = reg->type;\n\t\t\tverbose(env, \"=%s\", reg_type_str(env, t));\n\t\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\t\tverbose(env, \"P\");\n\t\t\tif (t == SCALAR_VALUE && tnum_is_const(reg->var_off))\n\t\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t\t}\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tif (state->in_callback_fn)\n\t\tverbose(env, \" cb\");\n\tif (state->in_async_callback_fn)\n\t\tverbose(env, \" async_cb\");\n\tverbose(env, \"\\n\");\n\tmark_verifier_state_clean(env);\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct tnum enforce_attach_type_range = tnum_unknown;\n\tconst struct bpf_prog *prog = env->prog;\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\tenum bpf_prog_type prog_type = resolve_prog_type(env->prog);\n\tint err;\n\tstruct bpf_func_state *frame = env->cur_state->frame[0];\n\tconst bool is_subprog = frame->subprogno;\n\n\t/* LSM and struct_ops func-ptr's return type could be \"void\" */\n\tif (!is_subprog &&\n\t    (prog_type == BPF_PROG_TYPE_STRUCT_OPS ||\n\t     prog_type == BPF_PROG_TYPE_LSM) &&\n\t    !prog->aux->attach_func_proto->type)\n\t\treturn 0;\n\n\t/* eBPF calling convention is such that R0 is used\n\t * to return the value from eBPF program.\n\t * Make sure that it's readable at this time\n\t * of bpf_exit, which means that program wrote\n\t * something into it earlier\n\t */\n\terr = check_reg_arg(env, BPF_REG_0, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\tverbose(env, \"R0 leaks addr as return value\\n\");\n\t\treturn -EACCES;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\n\tif (frame->in_async_callback_fn) {\n\t\t/* enforce return zero from async callbacks like timer */\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"In async callback the register R0 is not a known value (%s)\\n\",\n\t\t\t\treg_type_str[reg->type]);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!tnum_in(tnum_const(0), reg->var_off)) {\n\t\t\tverbose_invalid_scalar(env, reg, &range, \"async callback\", \"R0\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (is_subprog) {\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"At subprogram exit the register R0 is not a scalar value (%s)\\n\",\n\t\t\t\treg_type_str[reg->type]);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tswitch (prog_type) {\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_UDP4_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_UDP6_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETSOCKNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETSOCKNAME)\n\t\t\trange = tnum_range(1, 1);\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET4_BIND ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_BIND)\n\t\t\trange = tnum_range(0, 3);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET_EGRESS) {\n\t\t\trange = tnum_range(0, 3);\n\t\t\tenforce_attach_type_range = tnum_range(2, 3);\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\tcase BPF_PROG_TYPE_CGROUP_SYSCTL:\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\t\tbreak;\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT:\n\t\tif (!env->prog->aux->attach_btf_id)\n\t\t\treturn 0;\n\t\trange = tnum_const(0);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_TRACING:\n\t\tswitch (env->prog->expected_attach_type) {\n\t\tcase BPF_TRACE_FENTRY:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\trange = tnum_const(0);\n\t\t\tbreak;\n\t\tcase BPF_TRACE_RAW_TP:\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\treturn 0;\n\t\tcase BPF_TRACE_ITER:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -ENOTSUPP;\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_SK_LOOKUP:\n\t\trange = tnum_range(SK_DROP, SK_PASS);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_EXT:\n\t\t/* freplace program can return anything as its return value\n\t\t * depends on the to-be-replaced kernel func or bpf program.\n\t\t */\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose_invalid_scalar(env, reg, &range, \"program exit\", \"R0\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_is_unknown(enforce_attach_type_range) &&\n\t    tnum_in(enforce_attach_type_range, reg->var_off))\n\t\tenv->prog->enforce_expected_attach_type = 1;\n\treturn 0;\n}",
                        "code_after_change": "static int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct tnum enforce_attach_type_range = tnum_unknown;\n\tconst struct bpf_prog *prog = env->prog;\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\tenum bpf_prog_type prog_type = resolve_prog_type(env->prog);\n\tint err;\n\tstruct bpf_func_state *frame = env->cur_state->frame[0];\n\tconst bool is_subprog = frame->subprogno;\n\n\t/* LSM and struct_ops func-ptr's return type could be \"void\" */\n\tif (!is_subprog &&\n\t    (prog_type == BPF_PROG_TYPE_STRUCT_OPS ||\n\t     prog_type == BPF_PROG_TYPE_LSM) &&\n\t    !prog->aux->attach_func_proto->type)\n\t\treturn 0;\n\n\t/* eBPF calling convention is such that R0 is used\n\t * to return the value from eBPF program.\n\t * Make sure that it's readable at this time\n\t * of bpf_exit, which means that program wrote\n\t * something into it earlier\n\t */\n\terr = check_reg_arg(env, BPF_REG_0, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\tverbose(env, \"R0 leaks addr as return value\\n\");\n\t\treturn -EACCES;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\n\tif (frame->in_async_callback_fn) {\n\t\t/* enforce return zero from async callbacks like timer */\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"In async callback the register R0 is not a known value (%s)\\n\",\n\t\t\t\treg_type_str(env, reg->type));\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!tnum_in(tnum_const(0), reg->var_off)) {\n\t\t\tverbose_invalid_scalar(env, reg, &range, \"async callback\", \"R0\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (is_subprog) {\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"At subprogram exit the register R0 is not a scalar value (%s)\\n\",\n\t\t\t\treg_type_str(env, reg->type));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tswitch (prog_type) {\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_UDP4_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_UDP6_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETSOCKNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETSOCKNAME)\n\t\t\trange = tnum_range(1, 1);\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET4_BIND ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_BIND)\n\t\t\trange = tnum_range(0, 3);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET_EGRESS) {\n\t\t\trange = tnum_range(0, 3);\n\t\t\tenforce_attach_type_range = tnum_range(2, 3);\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\tcase BPF_PROG_TYPE_CGROUP_SYSCTL:\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\t\tbreak;\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT:\n\t\tif (!env->prog->aux->attach_btf_id)\n\t\t\treturn 0;\n\t\trange = tnum_const(0);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_TRACING:\n\t\tswitch (env->prog->expected_attach_type) {\n\t\tcase BPF_TRACE_FENTRY:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\trange = tnum_const(0);\n\t\t\tbreak;\n\t\tcase BPF_TRACE_RAW_TP:\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\treturn 0;\n\t\tcase BPF_TRACE_ITER:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -ENOTSUPP;\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_SK_LOOKUP:\n\t\trange = tnum_range(SK_DROP, SK_PASS);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_EXT:\n\t\t/* freplace program can return anything as its return value\n\t\t * depends on the to-be-replaced kernel func or bpf program.\n\t\t */\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str(env, reg->type));\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose_invalid_scalar(env, reg, &range, \"program exit\", \"R0\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_is_unknown(enforce_attach_type_range) &&\n\t    tnum_in(enforce_attach_type_range, reg->var_off))\n\t\tenv->prog->enforce_expected_attach_type = 1;\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "code_after_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tif (ptr_reg->type & PTR_MAYBE_NULL) {\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\t}\n\n\tswitch (base_type(ptr_reg->type)) {\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}",
                        "code_after_change": "static bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str(env, type), val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str(env, type), reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str(env, type));\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str(env, type));\n\t\treturn false;\n\t}\n\n\treturn true;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3583,
            "cve_id": "CVE-2022-23222",
            "code_snippet": "static int check_ctx_access(struct bpf_verifier_env *env, int insn_idx, int off, int size,\n\t\t\t    enum bpf_access_type t, enum bpf_reg_type *reg_type,\n\t\t\t    struct btf **btf, u32 *btf_id)\n{\n\tstruct bpf_insn_access_aux info = {\n\t\t.reg_type = *reg_type,\n\t\t.log = &env->log,\n\t};\n\n\tif (env->ops->is_valid_access &&\n\t    env->ops->is_valid_access(off, size, t, env->prog, &info)) {\n\t\t/* A non zero info.ctx_field_size indicates that this field is a\n\t\t * candidate for later verifier transformation to load the whole\n\t\t * field and then apply a mask when accessed with a narrower\n\t\t * access than actual ctx access size. A zero info.ctx_field_size\n\t\t * will only allow for whole field access and rejects any other\n\t\t * type of narrower access.\n\t\t */\n\t\t*reg_type = info.reg_type;\n\n\t\tif (*reg_type == PTR_TO_BTF_ID || *reg_type == PTR_TO_BTF_ID_OR_NULL) {\n\t\t\t*btf = info.btf;\n\t\t\t*btf_id = info.btf_id;\n\t\t} else {\n\t\t\tenv->insn_aux_data[insn_idx].ctx_field_size = info.ctx_field_size;\n\t\t}\n\t\t/* remember the offset of last byte accessed in ctx */\n\t\tif (env->prog->aux->max_ctx_offset < off + size)\n\t\t\tenv->prog->aux->max_ctx_offset = off + size;\n\t\treturn 0;\n\t}\n\n\tverbose(env, \"invalid bpf_context access off=%d size=%d\\n\", off, size);\n\treturn -EACCES;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "bool btf_ctx_access(int off, int size, enum bpf_access_type type,\n\t\t    const struct bpf_prog *prog,\n\t\t    struct bpf_insn_access_aux *info)\n{\n\tconst struct btf_type *t = prog->aux->attach_func_proto;\n\tstruct bpf_prog *tgt_prog = prog->aux->dst_prog;\n\tstruct btf *btf = bpf_prog_get_target_btf(prog);\n\tconst char *tname = prog->aux->attach_func_name;\n\tstruct bpf_verifier_log *log = info->log;\n\tconst struct btf_param *args;\n\tu32 nr_args, arg;\n\tint i, ret;\n\n\tif (off % 8) {\n\t\tbpf_log(log, \"func '%s' offset %d is not multiple of 8\\n\",\n\t\t\ttname, off);\n\t\treturn false;\n\t}\n\targ = off / 8;\n\targs = (const struct btf_param *)(t + 1);\n\t/* if (t == NULL) Fall back to default BPF prog with\n\t * MAX_BPF_FUNC_REG_ARGS u64 arguments.\n\t */\n\tnr_args = t ? btf_type_vlen(t) : MAX_BPF_FUNC_REG_ARGS;\n\tif (prog->aux->attach_btf_trace) {\n\t\t/* skip first 'void *__data' argument in btf_trace_##name typedef */\n\t\targs++;\n\t\tnr_args--;\n\t}\n\n\tif (arg > nr_args) {\n\t\tbpf_log(log, \"func '%s' doesn't have %d-th argument\\n\",\n\t\t\ttname, arg + 1);\n\t\treturn false;\n\t}\n\n\tif (arg == nr_args) {\n\t\tswitch (prog->expected_attach_type) {\n\t\tcase BPF_LSM_MAC:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\t/* When LSM programs are attached to void LSM hooks\n\t\t\t * they use FEXIT trampolines and when attached to\n\t\t\t * int LSM hooks, they use MODIFY_RETURN trampolines.\n\t\t\t *\n\t\t\t * While the LSM programs are BPF_MODIFY_RETURN-like\n\t\t\t * the check:\n\t\t\t *\n\t\t\t *\tif (ret_type != 'int')\n\t\t\t *\t\treturn -EINVAL;\n\t\t\t *\n\t\t\t * is _not_ done here. This is still safe as LSM hooks\n\t\t\t * have only void and int return types.\n\t\t\t */\n\t\t\tif (!t)\n\t\t\t\treturn true;\n\t\t\tt = btf_type_by_id(btf, t->type);\n\t\t\tbreak;\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\t/* For now the BPF_MODIFY_RETURN can only be attached to\n\t\t\t * functions that return an int.\n\t\t\t */\n\t\t\tif (!t)\n\t\t\t\treturn false;\n\n\t\t\tt = btf_type_skip_modifiers(btf, t->type, NULL);\n\t\t\tif (!btf_type_is_small_int(t)) {\n\t\t\t\tbpf_log(log,\n\t\t\t\t\t\"ret type %s not allowed for fmod_ret\\n\",\n\t\t\t\t\tbtf_kind_str[BTF_INFO_KIND(t->info)]);\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbpf_log(log, \"func '%s' doesn't have %d-th argument\\n\",\n\t\t\t\ttname, arg + 1);\n\t\t\treturn false;\n\t\t}\n\t} else {\n\t\tif (!t)\n\t\t\t/* Default prog with MAX_BPF_FUNC_REG_ARGS args */\n\t\t\treturn true;\n\t\tt = btf_type_by_id(btf, args[arg].type);\n\t}\n\n\t/* skip modifiers */\n\twhile (btf_type_is_modifier(t))\n\t\tt = btf_type_by_id(btf, t->type);\n\tif (btf_type_is_small_int(t) || btf_type_is_enum(t))\n\t\t/* accessing a scalar */\n\t\treturn true;\n\tif (!btf_type_is_ptr(t)) {\n\t\tbpf_log(log,\n\t\t\t\"func '%s' arg%d '%s' has type %s. Only pointer access is allowed\\n\",\n\t\t\ttname, arg,\n\t\t\t__btf_name_by_offset(btf, t->name_off),\n\t\t\tbtf_kind_str[BTF_INFO_KIND(t->info)]);\n\t\treturn false;\n\t}\n\n\t/* check for PTR_TO_RDONLY_BUF_OR_NULL or PTR_TO_RDWR_BUF_OR_NULL */\n\tfor (i = 0; i < prog->aux->ctx_arg_info_size; i++) {\n\t\tconst struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i];\n\n\t\tif (ctx_arg_info->offset == off &&\n\t\t    (ctx_arg_info->reg_type == PTR_TO_RDONLY_BUF_OR_NULL ||\n\t\t     ctx_arg_info->reg_type == PTR_TO_RDWR_BUF_OR_NULL)) {\n\t\t\tinfo->reg_type = ctx_arg_info->reg_type;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tif (t->type == 0)\n\t\t/* This is a pointer to void.\n\t\t * It is the same as scalar from the verifier safety pov.\n\t\t * No further pointer walking is allowed.\n\t\t */\n\t\treturn true;\n\n\tif (is_int_ptr(btf, t))\n\t\treturn true;\n\n\t/* this is a pointer to another type */\n\tfor (i = 0; i < prog->aux->ctx_arg_info_size; i++) {\n\t\tconst struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i];\n\n\t\tif (ctx_arg_info->offset == off) {\n\t\t\tif (!ctx_arg_info->btf_id) {\n\t\t\t\tbpf_log(log,\"invalid btf_id for context argument offset %u\\n\", off);\n\t\t\t\treturn false;\n\t\t\t}\n\n\t\t\tinfo->reg_type = ctx_arg_info->reg_type;\n\t\t\tinfo->btf = btf_vmlinux;\n\t\t\tinfo->btf_id = ctx_arg_info->btf_id;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tinfo->reg_type = PTR_TO_BTF_ID;\n\tif (tgt_prog) {\n\t\tenum bpf_prog_type tgt_type;\n\n\t\tif (tgt_prog->type == BPF_PROG_TYPE_EXT)\n\t\t\ttgt_type = tgt_prog->aux->saved_dst_prog_type;\n\t\telse\n\t\t\ttgt_type = tgt_prog->type;\n\n\t\tret = btf_translate_to_vmlinux(log, btf, t, tgt_type, arg);\n\t\tif (ret > 0) {\n\t\t\tinfo->btf = btf_vmlinux;\n\t\t\tinfo->btf_id = ret;\n\t\t\treturn true;\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tinfo->btf = btf;\n\tinfo->btf_id = t->type;\n\tt = btf_type_by_id(btf, t->type);\n\t/* skip modifiers */\n\twhile (btf_type_is_modifier(t)) {\n\t\tinfo->btf_id = t->type;\n\t\tt = btf_type_by_id(btf, t->type);\n\t}\n\tif (!btf_type_is_struct(t)) {\n\t\tbpf_log(log,\n\t\t\t\"func '%s' arg%d type %s is not a struct\\n\",\n\t\t\ttname, arg, btf_kind_str[BTF_INFO_KIND(t->info)]);\n\t\treturn false;\n\t}\n\tbpf_log(log, \"func '%s' arg%d has btf_id %d type %s '%s'\\n\",\n\t\ttname, arg, info->btf_id, btf_kind_str[BTF_INFO_KIND(t->info)],\n\t\t__btf_name_by_offset(btf, t->name_off));\n\treturn true;\n}",
                        "code_after_change": "bool btf_ctx_access(int off, int size, enum bpf_access_type type,\n\t\t    const struct bpf_prog *prog,\n\t\t    struct bpf_insn_access_aux *info)\n{\n\tconst struct btf_type *t = prog->aux->attach_func_proto;\n\tstruct bpf_prog *tgt_prog = prog->aux->dst_prog;\n\tstruct btf *btf = bpf_prog_get_target_btf(prog);\n\tconst char *tname = prog->aux->attach_func_name;\n\tstruct bpf_verifier_log *log = info->log;\n\tconst struct btf_param *args;\n\tu32 nr_args, arg;\n\tint i, ret;\n\n\tif (off % 8) {\n\t\tbpf_log(log, \"func '%s' offset %d is not multiple of 8\\n\",\n\t\t\ttname, off);\n\t\treturn false;\n\t}\n\targ = off / 8;\n\targs = (const struct btf_param *)(t + 1);\n\t/* if (t == NULL) Fall back to default BPF prog with\n\t * MAX_BPF_FUNC_REG_ARGS u64 arguments.\n\t */\n\tnr_args = t ? btf_type_vlen(t) : MAX_BPF_FUNC_REG_ARGS;\n\tif (prog->aux->attach_btf_trace) {\n\t\t/* skip first 'void *__data' argument in btf_trace_##name typedef */\n\t\targs++;\n\t\tnr_args--;\n\t}\n\n\tif (arg > nr_args) {\n\t\tbpf_log(log, \"func '%s' doesn't have %d-th argument\\n\",\n\t\t\ttname, arg + 1);\n\t\treturn false;\n\t}\n\n\tif (arg == nr_args) {\n\t\tswitch (prog->expected_attach_type) {\n\t\tcase BPF_LSM_MAC:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\t/* When LSM programs are attached to void LSM hooks\n\t\t\t * they use FEXIT trampolines and when attached to\n\t\t\t * int LSM hooks, they use MODIFY_RETURN trampolines.\n\t\t\t *\n\t\t\t * While the LSM programs are BPF_MODIFY_RETURN-like\n\t\t\t * the check:\n\t\t\t *\n\t\t\t *\tif (ret_type != 'int')\n\t\t\t *\t\treturn -EINVAL;\n\t\t\t *\n\t\t\t * is _not_ done here. This is still safe as LSM hooks\n\t\t\t * have only void and int return types.\n\t\t\t */\n\t\t\tif (!t)\n\t\t\t\treturn true;\n\t\t\tt = btf_type_by_id(btf, t->type);\n\t\t\tbreak;\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\t/* For now the BPF_MODIFY_RETURN can only be attached to\n\t\t\t * functions that return an int.\n\t\t\t */\n\t\t\tif (!t)\n\t\t\t\treturn false;\n\n\t\t\tt = btf_type_skip_modifiers(btf, t->type, NULL);\n\t\t\tif (!btf_type_is_small_int(t)) {\n\t\t\t\tbpf_log(log,\n\t\t\t\t\t\"ret type %s not allowed for fmod_ret\\n\",\n\t\t\t\t\tbtf_kind_str[BTF_INFO_KIND(t->info)]);\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbpf_log(log, \"func '%s' doesn't have %d-th argument\\n\",\n\t\t\t\ttname, arg + 1);\n\t\t\treturn false;\n\t\t}\n\t} else {\n\t\tif (!t)\n\t\t\t/* Default prog with MAX_BPF_FUNC_REG_ARGS args */\n\t\t\treturn true;\n\t\tt = btf_type_by_id(btf, args[arg].type);\n\t}\n\n\t/* skip modifiers */\n\twhile (btf_type_is_modifier(t))\n\t\tt = btf_type_by_id(btf, t->type);\n\tif (btf_type_is_small_int(t) || btf_type_is_enum(t))\n\t\t/* accessing a scalar */\n\t\treturn true;\n\tif (!btf_type_is_ptr(t)) {\n\t\tbpf_log(log,\n\t\t\t\"func '%s' arg%d '%s' has type %s. Only pointer access is allowed\\n\",\n\t\t\ttname, arg,\n\t\t\t__btf_name_by_offset(btf, t->name_off),\n\t\t\tbtf_kind_str[BTF_INFO_KIND(t->info)]);\n\t\treturn false;\n\t}\n\n\t/* check for PTR_TO_RDONLY_BUF_OR_NULL or PTR_TO_RDWR_BUF_OR_NULL */\n\tfor (i = 0; i < prog->aux->ctx_arg_info_size; i++) {\n\t\tconst struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i];\n\t\tu32 type, flag;\n\n\t\ttype = base_type(ctx_arg_info->reg_type);\n\t\tflag = type_flag(ctx_arg_info->reg_type);\n\t\tif (ctx_arg_info->offset == off &&\n\t\t    (type == PTR_TO_RDWR_BUF || type == PTR_TO_RDONLY_BUF) &&\n\t\t    (flag & PTR_MAYBE_NULL)) {\n\t\t\tinfo->reg_type = ctx_arg_info->reg_type;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tif (t->type == 0)\n\t\t/* This is a pointer to void.\n\t\t * It is the same as scalar from the verifier safety pov.\n\t\t * No further pointer walking is allowed.\n\t\t */\n\t\treturn true;\n\n\tif (is_int_ptr(btf, t))\n\t\treturn true;\n\n\t/* this is a pointer to another type */\n\tfor (i = 0; i < prog->aux->ctx_arg_info_size; i++) {\n\t\tconst struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i];\n\n\t\tif (ctx_arg_info->offset == off) {\n\t\t\tif (!ctx_arg_info->btf_id) {\n\t\t\t\tbpf_log(log,\"invalid btf_id for context argument offset %u\\n\", off);\n\t\t\t\treturn false;\n\t\t\t}\n\n\t\t\tinfo->reg_type = ctx_arg_info->reg_type;\n\t\t\tinfo->btf = btf_vmlinux;\n\t\t\tinfo->btf_id = ctx_arg_info->btf_id;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tinfo->reg_type = PTR_TO_BTF_ID;\n\tif (tgt_prog) {\n\t\tenum bpf_prog_type tgt_type;\n\n\t\tif (tgt_prog->type == BPF_PROG_TYPE_EXT)\n\t\t\ttgt_type = tgt_prog->aux->saved_dst_prog_type;\n\t\telse\n\t\t\ttgt_type = tgt_prog->type;\n\n\t\tret = btf_translate_to_vmlinux(log, btf, t, tgt_type, arg);\n\t\tif (ret > 0) {\n\t\t\tinfo->btf = btf_vmlinux;\n\t\t\tinfo->btf_id = ret;\n\t\t\treturn true;\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tinfo->btf = btf;\n\tinfo->btf_id = t->type;\n\tt = btf_type_by_id(btf, t->type);\n\t/* skip modifiers */\n\twhile (btf_type_is_modifier(t)) {\n\t\tinfo->btf_id = t->type;\n\t\tt = btf_type_by_id(btf, t->type);\n\t}\n\tif (!btf_type_is_struct(t)) {\n\t\tbpf_log(log,\n\t\t\t\"func '%s' arg%d type %s is not a struct\\n\",\n\t\t\ttname, arg, btf_kind_str[BTF_INFO_KIND(t->info)]);\n\t\treturn false;\n\t}\n\tbpf_log(log, \"func '%s' arg%d has btf_id %d type %s '%s'\\n\",\n\t\ttname, arg, info->btf_id, btf_kind_str[BTF_INFO_KIND(t->info)],\n\t\t__btf_name_by_offset(btf, t->name_off));\n\treturn true;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3584,
            "cve_id": "CVE-2022-23222",
            "code_snippet": "static int check_reg_type(struct bpf_verifier_env *env, u32 regno,\n\t\t\t  enum bpf_arg_type arg_type,\n\t\t\t  const u32 *arg_btf_id)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\tenum bpf_reg_type expected, type = reg->type;\n\tconst struct bpf_reg_types *compatible;\n\tint i, j;\n\n\tcompatible = compatible_reg_types[base_type(arg_type)];\n\tif (!compatible) {\n\t\tverbose(env, \"verifier internal error: unsupported arg type %d\\n\", arg_type);\n\t\treturn -EFAULT;\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(compatible->types); i++) {\n\t\texpected = compatible->types[i];\n\t\tif (expected == NOT_INIT)\n\t\t\tbreak;\n\n\t\tif (type == expected)\n\t\t\tgoto found;\n\t}\n\n\tverbose(env, \"R%d type=%s expected=\", regno, reg_type_str[type]);\n\tfor (j = 0; j + 1 < i; j++)\n\t\tverbose(env, \"%s, \", reg_type_str[compatible->types[j]]);\n\tverbose(env, \"%s\\n\", reg_type_str[compatible->types[j]]);\n\treturn -EACCES;\n\nfound:\n\tif (type == PTR_TO_BTF_ID) {\n\t\tif (!arg_btf_id) {\n\t\t\tif (!compatible->btf_id) {\n\t\t\t\tverbose(env, \"verifier internal error: missing arg compatible BTF ID\\n\");\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t\targ_btf_id = compatible->btf_id;\n\t\t}\n\n\t\tif (!btf_struct_ids_match(&env->log, reg->btf, reg->btf_id, reg->off,\n\t\t\t\t\t  btf_vmlinux, *arg_btf_id)) {\n\t\t\tverbose(env, \"R%d is of type %s but %s is expected\\n\",\n\t\t\t\tregno, kernel_type_name(reg->btf, reg->btf_id),\n\t\t\t\tkernel_type_name(btf_vmlinux, *arg_btf_id));\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tif (!tnum_is_const(reg->var_off) || reg->var_off.value) {\n\t\t\tverbose(env, \"R%d is a pointer to in-kernel struct with non-zero offset\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state,\n\t\t\t\t bool print_all)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tif (!print_all && !reg_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\tverbose(env, \"P\");\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tif (t == PTR_TO_BTF_ID ||\n\t\t\t    t == PTR_TO_BTF_ID_OR_NULL ||\n\t\t\t    t == PTR_TO_PERCPU_BTF_ID)\n\t\t\t\tverbose(env, \"%s\", kernel_type_name(reg->btf, reg->btf_id));\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (reg_type_may_be_refcounted_or_null(t))\n\t\t\t\tverbose(env, \",ref_obj_id=%d\", reg->ref_obj_id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (t == CONST_PTR_TO_MAP ||\n\t\t\t\t t == PTR_TO_MAP_KEY ||\n\t\t\t\t t == PTR_TO_MAP_VALUE ||\n\t\t\t\t t == PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t\tif (reg->s32_min_value != reg->smin_value &&\n\t\t\t\t    reg->s32_min_value != S32_MIN)\n\t\t\t\t\tverbose(env, \",s32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_min_value));\n\t\t\t\tif (reg->s32_max_value != reg->smax_value &&\n\t\t\t\t    reg->s32_max_value != S32_MAX)\n\t\t\t\t\tverbose(env, \",s32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_max_value));\n\t\t\t\tif (reg->u32_min_value != reg->umin_value &&\n\t\t\t\t    reg->u32_min_value != U32_MIN)\n\t\t\t\t\tverbose(env, \",u32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_min_value));\n\t\t\t\tif (reg->u32_max_value != reg->umax_value &&\n\t\t\t\t    reg->u32_max_value != U32_MAX)\n\t\t\t\t\tverbose(env, \",u32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_max_value));\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tif (!print_all && !stack_slot_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (is_spilled_reg(&state->stack[i])) {\n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tt = reg->type;\n\t\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\t\tverbose(env, \"P\");\n\t\t\tif (t == SCALAR_VALUE && tnum_is_const(reg->var_off))\n\t\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t\t}\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tif (state->in_callback_fn)\n\t\tverbose(env, \" cb\");\n\tif (state->in_async_callback_fn)\n\t\tverbose(env, \" async_cb\");\n\tverbose(env, \"\\n\");\n\tmark_verifier_state_clean(env);\n}",
                        "code_after_change": "static void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state,\n\t\t\t\t bool print_all)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tif (!print_all && !reg_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str(env, t));\n\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\tverbose(env, \"P\");\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tif (base_type(t) == PTR_TO_BTF_ID ||\n\t\t\t    base_type(t) == PTR_TO_PERCPU_BTF_ID)\n\t\t\t\tverbose(env, \"%s\", kernel_type_name(reg->btf, reg->btf_id));\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (reg_type_may_be_refcounted_or_null(t))\n\t\t\t\tverbose(env, \",ref_obj_id=%d\", reg->ref_obj_id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (base_type(t) == CONST_PTR_TO_MAP ||\n\t\t\t\t base_type(t) == PTR_TO_MAP_KEY ||\n\t\t\t\t base_type(t) == PTR_TO_MAP_VALUE)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t\tif (reg->s32_min_value != reg->smin_value &&\n\t\t\t\t    reg->s32_min_value != S32_MIN)\n\t\t\t\t\tverbose(env, \",s32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_min_value));\n\t\t\t\tif (reg->s32_max_value != reg->smax_value &&\n\t\t\t\t    reg->s32_max_value != S32_MAX)\n\t\t\t\t\tverbose(env, \",s32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_max_value));\n\t\t\t\tif (reg->u32_min_value != reg->umin_value &&\n\t\t\t\t    reg->u32_min_value != U32_MIN)\n\t\t\t\t\tverbose(env, \",u32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_min_value));\n\t\t\t\tif (reg->u32_max_value != reg->umax_value &&\n\t\t\t\t    reg->u32_max_value != U32_MAX)\n\t\t\t\t\tverbose(env, \",u32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_max_value));\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tif (!print_all && !stack_slot_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (is_spilled_reg(&state->stack[i])) {\n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tt = reg->type;\n\t\t\tverbose(env, \"=%s\", reg_type_str(env, t));\n\t\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\t\tverbose(env, \"P\");\n\t\t\tif (t == SCALAR_VALUE && tnum_is_const(reg->var_off))\n\t\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t\t}\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tif (state->in_callback_fn)\n\t\tverbose(env, \" cb\");\n\tif (state->in_async_callback_fn)\n\t\tverbose(env, \" async_cb\");\n\tverbose(env, \"\\n\");\n\tmark_verifier_state_clean(env);\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3689,
            "cve_id": "CVE-2022-3110",
            "code_snippet": "s32\t_rtw_init_xmit_priv(struct xmit_priv *pxmitpriv, struct adapter *padapter)\n{\n\tint i;\n\tstruct xmit_buf *pxmitbuf;\n\tstruct xmit_frame *pxframe;\n\tint\tres = _SUCCESS;\n\tu32 max_xmit_extbuf_size = MAX_XMIT_EXTBUF_SZ;\n\tu32 num_xmit_extbuf = NR_XMIT_EXTBUFF;\n\n\t/*  We don't need to memset padapter->XXX to zero, because adapter is allocated by vzalloc(). */\n\n\tspin_lock_init(&pxmitpriv->lock);\n\tsema_init(&pxmitpriv->terminate_xmitthread_sema, 0);\n\n\t/*\n\t * Please insert all the queue initializaiton using rtw_init_queue below\n\t */\n\n\tpxmitpriv->adapter = padapter;\n\n\trtw_init_queue(&pxmitpriv->be_pending);\n\trtw_init_queue(&pxmitpriv->bk_pending);\n\trtw_init_queue(&pxmitpriv->vi_pending);\n\trtw_init_queue(&pxmitpriv->vo_pending);\n\trtw_init_queue(&pxmitpriv->bm_pending);\n\n\trtw_init_queue(&pxmitpriv->free_xmit_queue);\n\n\t/*\n\t * Please allocate memory with the sz = (struct xmit_frame) * NR_XMITFRAME,\n\t * and initialize free_xmit_frame below.\n\t * Please also apply  free_txobj to link_up all the xmit_frames...\n\t */\n\n\tpxmitpriv->pallocated_frame_buf = vzalloc(NR_XMITFRAME * sizeof(struct xmit_frame) + 4);\n\n\tif (!pxmitpriv->pallocated_frame_buf) {\n\t\tpxmitpriv->pxmit_frame_buf = NULL;\n\t\tres = _FAIL;\n\t\tgoto exit;\n\t}\n\tpxmitpriv->pxmit_frame_buf = (u8 *)N_BYTE_ALIGMENT((size_t)(pxmitpriv->pallocated_frame_buf), 4);\n\t/* pxmitpriv->pxmit_frame_buf = pxmitpriv->pallocated_frame_buf + 4 - */\n\t/* \t\t\t\t\t\t((size_t) (pxmitpriv->pallocated_frame_buf) &3); */\n\n\tpxframe = (struct xmit_frame *)pxmitpriv->pxmit_frame_buf;\n\n\tfor (i = 0; i < NR_XMITFRAME; i++) {\n\t\tINIT_LIST_HEAD(&pxframe->list);\n\n\t\tpxframe->padapter = padapter;\n\t\tpxframe->frame_tag = NULL_FRAMETAG;\n\n\t\tpxframe->pkt = NULL;\n\n\t\tpxframe->buf_addr = NULL;\n\t\tpxframe->pxmitbuf = NULL;\n\n\t\tlist_add_tail(&pxframe->list, &pxmitpriv->free_xmit_queue.queue);\n\n\t\tpxframe++;\n\t}\n\n\tpxmitpriv->free_xmitframe_cnt = NR_XMITFRAME;\n\n\tpxmitpriv->frag_len = MAX_FRAG_THRESHOLD;\n\n\t/* init xmit_buf */\n\trtw_init_queue(&pxmitpriv->free_xmitbuf_queue);\n\trtw_init_queue(&pxmitpriv->pending_xmitbuf_queue);\n\n\tpxmitpriv->pallocated_xmitbuf = vzalloc(NR_XMITBUFF * sizeof(struct xmit_buf) + 4);\n\n\tif (!pxmitpriv->pallocated_xmitbuf) {\n\t\tres = _FAIL;\n\t\tgoto exit;\n\t}\n\n\tpxmitpriv->pxmitbuf = (u8 *)N_BYTE_ALIGMENT((size_t)(pxmitpriv->pallocated_xmitbuf), 4);\n\t/* pxmitpriv->pxmitbuf = pxmitpriv->pallocated_xmitbuf + 4 - */\n\t/* \t\t\t\t\t\t((size_t) (pxmitpriv->pallocated_xmitbuf) &3); */\n\n\tpxmitbuf = (struct xmit_buf *)pxmitpriv->pxmitbuf;\n\n\tfor (i = 0; i < NR_XMITBUFF; i++) {\n\t\tINIT_LIST_HEAD(&pxmitbuf->list);\n\n\t\tpxmitbuf->priv_data = NULL;\n\t\tpxmitbuf->padapter = padapter;\n\t\tpxmitbuf->ext_tag = false;\n\n\t\t/* Tx buf allocation may fail sometimes, so sleep and retry. */\n\t\tres = rtw_os_xmit_resource_alloc(padapter, pxmitbuf, (MAX_XMITBUF_SZ + XMITBUF_ALIGN_SZ));\n\t\tif (res == _FAIL) {\n\t\t\tmsleep(10);\n\t\t\tres = rtw_os_xmit_resource_alloc(padapter, pxmitbuf, (MAX_XMITBUF_SZ + XMITBUF_ALIGN_SZ));\n\t\t\tif (res == _FAIL)\n\t\t\t\tgoto exit;\n\t\t}\n\n\t\tpxmitbuf->flags = XMIT_VO_QUEUE;\n\n\t\tlist_add_tail(&pxmitbuf->list, &pxmitpriv->free_xmitbuf_queue.queue);\n\t\tpxmitbuf++;\n\t}\n\n\tpxmitpriv->free_xmitbuf_cnt = NR_XMITBUFF;\n\n\t/*  Init xmit extension buff */\n\trtw_init_queue(&pxmitpriv->free_xmit_extbuf_queue);\n\n\tpxmitpriv->pallocated_xmit_extbuf = vzalloc(num_xmit_extbuf * sizeof(struct xmit_buf) + 4);\n\n\tif (!pxmitpriv->pallocated_xmit_extbuf) {\n\t\tres = _FAIL;\n\t\tgoto exit;\n\t}\n\n\tpxmitpriv->pxmit_extbuf = (u8 *)N_BYTE_ALIGMENT((size_t)(pxmitpriv->pallocated_xmit_extbuf), 4);\n\n\tpxmitbuf = (struct xmit_buf *)pxmitpriv->pxmit_extbuf;\n\n\tfor (i = 0; i < num_xmit_extbuf; i++) {\n\t\tINIT_LIST_HEAD(&pxmitbuf->list);\n\n\t\tpxmitbuf->priv_data = NULL;\n\t\tpxmitbuf->padapter = padapter;\n\t\tpxmitbuf->ext_tag = true;\n\n\t\tres = rtw_os_xmit_resource_alloc(padapter, pxmitbuf, max_xmit_extbuf_size + XMITBUF_ALIGN_SZ);\n\t\tif (res == _FAIL) {\n\t\t\tres = _FAIL;\n\t\t\tgoto exit;\n\t\t}\n\n\t\tlist_add_tail(&pxmitbuf->list, &pxmitpriv->free_xmit_extbuf_queue.queue);\n\t\tpxmitbuf++;\n\t}\n\n\tpxmitpriv->free_xmit_extbuf_cnt = num_xmit_extbuf;\n\n\trtw_alloc_hwxmits(padapter);\n\trtw_init_hwxmits(pxmitpriv->hwxmits, pxmitpriv->hwxmit_entry);\n\n\tfor (i = 0; i < 4; i++)\n\t\tpxmitpriv->wmm_para_seq[i] = i;\n\n\tpxmitpriv->txirp_cnt = 1;\n\n\tsema_init(&pxmitpriv->tx_retevt, 0);\n\n\t/* per AC pending irp */\n\tpxmitpriv->beq_cnt = 0;\n\tpxmitpriv->bkq_cnt = 0;\n\tpxmitpriv->viq_cnt = 0;\n\tpxmitpriv->voq_cnt = 0;\n\n\tpxmitpriv->ack_tx = false;\n\tmutex_init(&pxmitpriv->ack_tx_mutex);\n\trtw_sctx_init(&pxmitpriv->ack_tx_ops, 0);\n\n\trtl8188eu_init_xmit_priv(padapter);\n\nexit:\n\n\treturn res;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "void rtw_alloc_hwxmits(struct adapter *padapter)\n{\n\tstruct hw_xmit *hwxmits;\n\tstruct xmit_priv *pxmitpriv = &padapter->xmitpriv;\n\n\tpxmitpriv->hwxmit_entry = HWXMIT_ENTRY;\n\n\tpxmitpriv->hwxmits = kzalloc(sizeof(struct hw_xmit) * pxmitpriv->hwxmit_entry, GFP_KERNEL);\n\n\thwxmits = pxmitpriv->hwxmits;\n\n\tif (pxmitpriv->hwxmit_entry == 5) {\n\t\thwxmits[0] .sta_queue = &pxmitpriv->bm_pending;\n\t\thwxmits[1] .sta_queue = &pxmitpriv->vo_pending;\n\t\thwxmits[2] .sta_queue = &pxmitpriv->vi_pending;\n\t\thwxmits[3] .sta_queue = &pxmitpriv->bk_pending;\n\t\thwxmits[4] .sta_queue = &pxmitpriv->be_pending;\n\t} else if (pxmitpriv->hwxmit_entry == 4) {\n\t\thwxmits[0] .sta_queue = &pxmitpriv->vo_pending;\n\t\thwxmits[1] .sta_queue = &pxmitpriv->vi_pending;\n\t\thwxmits[2] .sta_queue = &pxmitpriv->be_pending;\n\t\thwxmits[3] .sta_queue = &pxmitpriv->bk_pending;\n\t} else {\n\t}\n}",
                        "code_after_change": "int rtw_alloc_hwxmits(struct adapter *padapter)\n{\n\tstruct hw_xmit *hwxmits;\n\tstruct xmit_priv *pxmitpriv = &padapter->xmitpriv;\n\n\tpxmitpriv->hwxmit_entry = HWXMIT_ENTRY;\n\n\tpxmitpriv->hwxmits = kzalloc(sizeof(struct hw_xmit) * pxmitpriv->hwxmit_entry, GFP_KERNEL);\n\tif (!pxmitpriv->hwxmits)\n\t\treturn -ENOMEM;\n\n\thwxmits = pxmitpriv->hwxmits;\n\n\tif (pxmitpriv->hwxmit_entry == 5) {\n\t\thwxmits[0] .sta_queue = &pxmitpriv->bm_pending;\n\t\thwxmits[1] .sta_queue = &pxmitpriv->vo_pending;\n\t\thwxmits[2] .sta_queue = &pxmitpriv->vi_pending;\n\t\thwxmits[3] .sta_queue = &pxmitpriv->bk_pending;\n\t\thwxmits[4] .sta_queue = &pxmitpriv->be_pending;\n\t} else if (pxmitpriv->hwxmit_entry == 4) {\n\t\thwxmits[0] .sta_queue = &pxmitpriv->vo_pending;\n\t\thwxmits[1] .sta_queue = &pxmitpriv->vi_pending;\n\t\thwxmits[2] .sta_queue = &pxmitpriv->be_pending;\n\t\thwxmits[3] .sta_queue = &pxmitpriv->bk_pending;\n\t} else {\n\t}\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-3110"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3692,
            "cve_id": "CVE-2022-3112",
            "code_snippet": "void amvdec_add_ts(struct amvdec_session *sess, u64 ts,\n\t\t   struct v4l2_timecode tc, u32 offset, u32 vbuf_flags)\n{\n\tstruct amvdec_timestamp *new_ts;\n\tunsigned long flags;\n\n\tnew_ts = kzalloc(sizeof(*new_ts), GFP_KERNEL);\n\tnew_ts->ts = ts;\n\tnew_ts->tc = tc;\n\tnew_ts->offset = offset;\n\tnew_ts->flags = vbuf_flags;\n\n\tspin_lock_irqsave(&sess->ts_spinlock, flags);\n\tlist_add_tail(&new_ts->list, &sess->timestamps);\n\tspin_unlock_irqrestore(&sess->ts_spinlock, flags);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int\nesparser_queue(struct amvdec_session *sess, struct vb2_v4l2_buffer *vbuf)\n{\n\tint ret;\n\tstruct vb2_buffer *vb = &vbuf->vb2_buf;\n\tstruct amvdec_core *core = sess->core;\n\tstruct amvdec_codec_ops *codec_ops = sess->fmt_out->codec_ops;\n\tu32 payload_size = vb2_get_plane_payload(vb, 0);\n\tdma_addr_t phy = vb2_dma_contig_plane_dma_addr(vb, 0);\n\tu32 num_dst_bufs = 0;\n\tu32 offset;\n\tu32 pad_size;\n\n\t/*\n\t * When max ref frame is held by VP9, this should be -= 3 to prevent a\n\t * shortage of CAPTURE buffers on the decoder side.\n\t * For the future, a good enhancement of the way this is handled could\n\t * be to notify new capture buffers to the decoding modules, so that\n\t * they could pause when there is no capture buffer available and\n\t * resume on this notification.\n\t */\n\tif (sess->fmt_out->pixfmt == V4L2_PIX_FMT_VP9) {\n\t\tif (codec_ops->num_pending_bufs)\n\t\t\tnum_dst_bufs = codec_ops->num_pending_bufs(sess);\n\n\t\tnum_dst_bufs += v4l2_m2m_num_dst_bufs_ready(sess->m2m_ctx);\n\t\tif (sess->fmt_out->pixfmt == V4L2_PIX_FMT_VP9)\n\t\t\tnum_dst_bufs -= 3;\n\n\t\tif (esparser_vififo_get_free_space(sess) < payload_size ||\n\t\t    atomic_read(&sess->esparser_queued_bufs) >= num_dst_bufs)\n\t\t\treturn -EAGAIN;\n\t} else if (esparser_vififo_get_free_space(sess) < payload_size) {\n\t\treturn -EAGAIN;\n\t}\n\n\tv4l2_m2m_src_buf_remove_by_buf(sess->m2m_ctx, vbuf);\n\n\toffset = esparser_get_offset(sess);\n\n\tamvdec_add_ts(sess, vb->timestamp, vbuf->timecode, offset, vbuf->flags);\n\tdev_dbg(core->dev, \"esparser: ts = %llu pld_size = %u offset = %08X flags = %08X\\n\",\n\t\tvb->timestamp, payload_size, offset, vbuf->flags);\n\n\tvbuf->flags = 0;\n\tvbuf->field = V4L2_FIELD_NONE;\n\tvbuf->sequence = sess->sequence_out++;\n\n\tif (sess->fmt_out->pixfmt == V4L2_PIX_FMT_VP9) {\n\t\tpayload_size = vp9_update_header(core, vb);\n\n\t\t/* If unable to alter buffer to add headers */\n\t\tif (payload_size == 0) {\n\t\t\tamvdec_remove_ts(sess, vb->timestamp);\n\t\t\tv4l2_m2m_buf_done(vbuf, VB2_BUF_STATE_ERROR);\n\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tpad_size = esparser_pad_start_code(core, vb, payload_size);\n\tret = esparser_write_data(core, phy, payload_size + pad_size);\n\n\tif (ret <= 0) {\n\t\tdev_warn(core->dev, \"esparser: input parsing error\\n\");\n\t\tamvdec_remove_ts(sess, vb->timestamp);\n\t\tv4l2_m2m_buf_done(vbuf, VB2_BUF_STATE_ERROR);\n\t\tamvdec_write_parser(core, PARSER_FETCH_CMD, 0);\n\n\t\treturn 0;\n\t}\n\n\tatomic_inc(&sess->esparser_queued_bufs);\n\tv4l2_m2m_buf_done(vbuf, VB2_BUF_STATE_DONE);\n\n\treturn 0;\n}",
                        "code_after_change": "static int\nesparser_queue(struct amvdec_session *sess, struct vb2_v4l2_buffer *vbuf)\n{\n\tint ret;\n\tstruct vb2_buffer *vb = &vbuf->vb2_buf;\n\tstruct amvdec_core *core = sess->core;\n\tstruct amvdec_codec_ops *codec_ops = sess->fmt_out->codec_ops;\n\tu32 payload_size = vb2_get_plane_payload(vb, 0);\n\tdma_addr_t phy = vb2_dma_contig_plane_dma_addr(vb, 0);\n\tu32 num_dst_bufs = 0;\n\tu32 offset;\n\tu32 pad_size;\n\n\t/*\n\t * When max ref frame is held by VP9, this should be -= 3 to prevent a\n\t * shortage of CAPTURE buffers on the decoder side.\n\t * For the future, a good enhancement of the way this is handled could\n\t * be to notify new capture buffers to the decoding modules, so that\n\t * they could pause when there is no capture buffer available and\n\t * resume on this notification.\n\t */\n\tif (sess->fmt_out->pixfmt == V4L2_PIX_FMT_VP9) {\n\t\tif (codec_ops->num_pending_bufs)\n\t\t\tnum_dst_bufs = codec_ops->num_pending_bufs(sess);\n\n\t\tnum_dst_bufs += v4l2_m2m_num_dst_bufs_ready(sess->m2m_ctx);\n\t\tif (sess->fmt_out->pixfmt == V4L2_PIX_FMT_VP9)\n\t\t\tnum_dst_bufs -= 3;\n\n\t\tif (esparser_vififo_get_free_space(sess) < payload_size ||\n\t\t    atomic_read(&sess->esparser_queued_bufs) >= num_dst_bufs)\n\t\t\treturn -EAGAIN;\n\t} else if (esparser_vififo_get_free_space(sess) < payload_size) {\n\t\treturn -EAGAIN;\n\t}\n\n\tv4l2_m2m_src_buf_remove_by_buf(sess->m2m_ctx, vbuf);\n\n\toffset = esparser_get_offset(sess);\n\n\tret = amvdec_add_ts(sess, vb->timestamp, vbuf->timecode, offset, vbuf->flags);\n\tif (ret) {\n\t\tv4l2_m2m_buf_done(vbuf, VB2_BUF_STATE_ERROR);\n\t\treturn ret;\n\t}\n\n\tdev_dbg(core->dev, \"esparser: ts = %llu pld_size = %u offset = %08X flags = %08X\\n\",\n\t\tvb->timestamp, payload_size, offset, vbuf->flags);\n\n\tvbuf->flags = 0;\n\tvbuf->field = V4L2_FIELD_NONE;\n\tvbuf->sequence = sess->sequence_out++;\n\n\tif (sess->fmt_out->pixfmt == V4L2_PIX_FMT_VP9) {\n\t\tpayload_size = vp9_update_header(core, vb);\n\n\t\t/* If unable to alter buffer to add headers */\n\t\tif (payload_size == 0) {\n\t\t\tamvdec_remove_ts(sess, vb->timestamp);\n\t\t\tv4l2_m2m_buf_done(vbuf, VB2_BUF_STATE_ERROR);\n\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tpad_size = esparser_pad_start_code(core, vb, payload_size);\n\tret = esparser_write_data(core, phy, payload_size + pad_size);\n\n\tif (ret <= 0) {\n\t\tdev_warn(core->dev, \"esparser: input parsing error\\n\");\n\t\tamvdec_remove_ts(sess, vb->timestamp);\n\t\tv4l2_m2m_buf_done(vbuf, VB2_BUF_STATE_ERROR);\n\t\tamvdec_write_parser(core, PARSER_FETCH_CMD, 0);\n\n\t\treturn 0;\n\t}\n\n\tatomic_inc(&sess->esparser_queued_bufs);\n\tv4l2_m2m_buf_done(vbuf, VB2_BUF_STATE_DONE);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-3112"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 4014,
            "cve_id": "CVE-2023-1382",
            "code_snippet": "static void tipc_topsrv_accept(struct work_struct *work)\n{\n\tstruct tipc_topsrv *srv = container_of(work, struct tipc_topsrv, awork);\n\tstruct socket *newsock, *lsock;\n\tstruct tipc_conn *con;\n\tstruct sock *newsk;\n\tint ret;\n\n\tspin_lock_bh(&srv->idr_lock);\n\tif (!srv->listener) {\n\t\tspin_unlock_bh(&srv->idr_lock);\n\t\treturn;\n\t}\n\tlsock = srv->listener;\n\tspin_unlock_bh(&srv->idr_lock);\n\n\twhile (1) {\n\t\tret = kernel_accept(lsock, &newsock, O_NONBLOCK);\n\t\tif (ret < 0)\n\t\t\treturn;\n\t\tcon = tipc_conn_alloc(srv);\n\t\tif (IS_ERR(con)) {\n\t\t\tret = PTR_ERR(con);\n\t\t\tsock_release(newsock);\n\t\t\treturn;\n\t\t}\n\t\t/* Register callbacks */\n\t\tnewsk = newsock->sk;\n\t\twrite_lock_bh(&newsk->sk_callback_lock);\n\t\tnewsk->sk_data_ready = tipc_conn_data_ready;\n\t\tnewsk->sk_write_space = tipc_conn_write_space;\n\t\tnewsk->sk_user_data = con;\n\t\tcon->sock = newsock;\n\t\twrite_unlock_bh(&newsk->sk_callback_lock);\n\n\t\t/* Wake up receive process in case of 'SYN+' message */\n\t\tnewsk->sk_data_ready(newsk);\n\t}\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "struct sock *sk_clone_lock(const struct sock *sk, const gfp_t priority)\n{\n\tstruct proto *prot = READ_ONCE(sk->sk_prot);\n\tstruct sock *newsk;\n\tbool is_charged = true;\n\n\tnewsk = sk_prot_alloc(prot, priority, sk->sk_family);\n\tif (newsk != NULL) {\n\t\tstruct sk_filter *filter;\n\n\t\tsock_copy(newsk, sk);\n\n\t\tnewsk->sk_prot_creator = prot;\n\n\t\t/* SANITY */\n\t\tif (likely(newsk->sk_net_refcnt))\n\t\t\tget_net(sock_net(newsk));\n\t\tsk_node_init(&newsk->sk_node);\n\t\tsock_lock_init(newsk);\n\t\tbh_lock_sock(newsk);\n\t\tnewsk->sk_backlog.head\t= newsk->sk_backlog.tail = NULL;\n\t\tnewsk->sk_backlog.len = 0;\n\n\t\tatomic_set(&newsk->sk_rmem_alloc, 0);\n\t\t/*\n\t\t * sk_wmem_alloc set to one (see sk_free() and sock_wfree())\n\t\t */\n\t\trefcount_set(&newsk->sk_wmem_alloc, 1);\n\t\tatomic_set(&newsk->sk_omem_alloc, 0);\n\t\tsk_init_common(newsk);\n\n\t\tnewsk->sk_dst_cache\t= NULL;\n\t\tnewsk->sk_dst_pending_confirm = 0;\n\t\tnewsk->sk_wmem_queued\t= 0;\n\t\tnewsk->sk_forward_alloc = 0;\n\t\tatomic_set(&newsk->sk_drops, 0);\n\t\tnewsk->sk_send_head\t= NULL;\n\t\tnewsk->sk_userlocks\t= sk->sk_userlocks & ~SOCK_BINDPORT_LOCK;\n\t\tatomic_set(&newsk->sk_zckey, 0);\n\n\t\tsock_reset_flag(newsk, SOCK_DONE);\n\n\t\t/* sk->sk_memcg will be populated at accept() time */\n\t\tnewsk->sk_memcg = NULL;\n\n\t\tcgroup_sk_alloc(&newsk->sk_cgrp_data);\n\n\t\trcu_read_lock();\n\t\tfilter = rcu_dereference(sk->sk_filter);\n\t\tif (filter != NULL)\n\t\t\t/* though it's an empty new sock, the charging may fail\n\t\t\t * if sysctl_optmem_max was changed between creation of\n\t\t\t * original socket and cloning\n\t\t\t */\n\t\t\tis_charged = sk_filter_charge(newsk, filter);\n\t\tRCU_INIT_POINTER(newsk->sk_filter, filter);\n\t\trcu_read_unlock();\n\n\t\tif (unlikely(!is_charged || xfrm_sk_clone_policy(newsk, sk))) {\n\t\t\t/* We need to make sure that we don't uncharge the new\n\t\t\t * socket if we couldn't charge it in the first place\n\t\t\t * as otherwise we uncharge the parent's filter.\n\t\t\t */\n\t\t\tif (!is_charged)\n\t\t\t\tRCU_INIT_POINTER(newsk->sk_filter, NULL);\n\t\t\tsk_free_unlock_clone(newsk);\n\t\t\tnewsk = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tRCU_INIT_POINTER(newsk->sk_reuseport_cb, NULL);\n\n\t\tif (bpf_sk_storage_clone(sk, newsk)) {\n\t\t\tsk_free_unlock_clone(newsk);\n\t\t\tnewsk = NULL;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/* Clear sk_user_data if parent had the pointer tagged\n\t\t * as not suitable for copying when cloning.\n\t\t */\n\t\tif (sk_user_data_is_nocopy(newsk))\n\t\t\tnewsk->sk_user_data = NULL;\n\n\t\tnewsk->sk_err\t   = 0;\n\t\tnewsk->sk_err_soft = 0;\n\t\tnewsk->sk_priority = 0;\n\t\tnewsk->sk_incoming_cpu = raw_smp_processor_id();\n\t\tif (likely(newsk->sk_net_refcnt))\n\t\t\tsock_inuse_add(sock_net(newsk), 1);\n\n\t\t/*\n\t\t * Before updating sk_refcnt, we must commit prior changes to memory\n\t\t * (Documentation/RCU/rculist_nulls.txt for details)\n\t\t */\n\t\tsmp_wmb();\n\t\trefcount_set(&newsk->sk_refcnt, 2);\n\n\t\t/*\n\t\t * Increment the counter in the same struct proto as the master\n\t\t * sock (sk_refcnt_debug_inc uses newsk->sk_prot->socks, that\n\t\t * is the same as sk->sk_prot->socks, as this field was copied\n\t\t * with memcpy).\n\t\t *\n\t\t * This _changes_ the previous behaviour, where\n\t\t * tcp_create_openreq_child always was incrementing the\n\t\t * equivalent to tcp_prot->socks (inet_sock_nr), so this have\n\t\t * to be taken into account in all callers. -acme\n\t\t */\n\t\tsk_refcnt_debug_inc(newsk);\n\t\tsk_set_socket(newsk, NULL);\n\t\tsk_tx_queue_clear(newsk);\n\t\tRCU_INIT_POINTER(newsk->sk_wq, NULL);\n\n\t\tif (newsk->sk_prot->sockets_allocated)\n\t\t\tsk_sockets_allocated_inc(newsk);\n\n\t\tif (sock_needs_netstamp(sk) &&\n\t\t    newsk->sk_flags & SK_FLAGS_TIMESTAMP)\n\t\t\tnet_enable_timestamp();\n\t}\nout:\n\treturn newsk;\n}",
                        "code_after_change": "struct sock *sk_clone_lock(const struct sock *sk, const gfp_t priority)\n{\n\tstruct proto *prot = READ_ONCE(sk->sk_prot);\n\tstruct sock *newsk;\n\tbool is_charged = true;\n\n\tnewsk = sk_prot_alloc(prot, priority, sk->sk_family);\n\tif (newsk != NULL) {\n\t\tstruct sk_filter *filter;\n\n\t\tsock_copy(newsk, sk);\n\n\t\tnewsk->sk_prot_creator = prot;\n\n\t\t/* SANITY */\n\t\tif (likely(newsk->sk_net_refcnt))\n\t\t\tget_net(sock_net(newsk));\n\t\tsk_node_init(&newsk->sk_node);\n\t\tsock_lock_init(newsk);\n\t\tbh_lock_sock(newsk);\n\t\tnewsk->sk_backlog.head\t= newsk->sk_backlog.tail = NULL;\n\t\tnewsk->sk_backlog.len = 0;\n\n\t\tatomic_set(&newsk->sk_rmem_alloc, 0);\n\t\t/*\n\t\t * sk_wmem_alloc set to one (see sk_free() and sock_wfree())\n\t\t */\n\t\trefcount_set(&newsk->sk_wmem_alloc, 1);\n\t\tatomic_set(&newsk->sk_omem_alloc, 0);\n\t\tsk_init_common(newsk);\n\n\t\tnewsk->sk_dst_cache\t= NULL;\n\t\tnewsk->sk_dst_pending_confirm = 0;\n\t\tnewsk->sk_wmem_queued\t= 0;\n\t\tnewsk->sk_forward_alloc = 0;\n\t\tatomic_set(&newsk->sk_drops, 0);\n\t\tnewsk->sk_send_head\t= NULL;\n\t\tnewsk->sk_userlocks\t= sk->sk_userlocks & ~SOCK_BINDPORT_LOCK;\n\t\tatomic_set(&newsk->sk_zckey, 0);\n\n\t\tsock_reset_flag(newsk, SOCK_DONE);\n\n\t\t/* sk->sk_memcg will be populated at accept() time */\n\t\tnewsk->sk_memcg = NULL;\n\n\t\tcgroup_sk_clone(&newsk->sk_cgrp_data);\n\n\t\trcu_read_lock();\n\t\tfilter = rcu_dereference(sk->sk_filter);\n\t\tif (filter != NULL)\n\t\t\t/* though it's an empty new sock, the charging may fail\n\t\t\t * if sysctl_optmem_max was changed between creation of\n\t\t\t * original socket and cloning\n\t\t\t */\n\t\t\tis_charged = sk_filter_charge(newsk, filter);\n\t\tRCU_INIT_POINTER(newsk->sk_filter, filter);\n\t\trcu_read_unlock();\n\n\t\tif (unlikely(!is_charged || xfrm_sk_clone_policy(newsk, sk))) {\n\t\t\t/* We need to make sure that we don't uncharge the new\n\t\t\t * socket if we couldn't charge it in the first place\n\t\t\t * as otherwise we uncharge the parent's filter.\n\t\t\t */\n\t\t\tif (!is_charged)\n\t\t\t\tRCU_INIT_POINTER(newsk->sk_filter, NULL);\n\t\t\tsk_free_unlock_clone(newsk);\n\t\t\tnewsk = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tRCU_INIT_POINTER(newsk->sk_reuseport_cb, NULL);\n\n\t\tif (bpf_sk_storage_clone(sk, newsk)) {\n\t\t\tsk_free_unlock_clone(newsk);\n\t\t\tnewsk = NULL;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/* Clear sk_user_data if parent had the pointer tagged\n\t\t * as not suitable for copying when cloning.\n\t\t */\n\t\tif (sk_user_data_is_nocopy(newsk))\n\t\t\tnewsk->sk_user_data = NULL;\n\n\t\tnewsk->sk_err\t   = 0;\n\t\tnewsk->sk_err_soft = 0;\n\t\tnewsk->sk_priority = 0;\n\t\tnewsk->sk_incoming_cpu = raw_smp_processor_id();\n\t\tif (likely(newsk->sk_net_refcnt))\n\t\t\tsock_inuse_add(sock_net(newsk), 1);\n\n\t\t/*\n\t\t * Before updating sk_refcnt, we must commit prior changes to memory\n\t\t * (Documentation/RCU/rculist_nulls.txt for details)\n\t\t */\n\t\tsmp_wmb();\n\t\trefcount_set(&newsk->sk_refcnt, 2);\n\n\t\t/*\n\t\t * Increment the counter in the same struct proto as the master\n\t\t * sock (sk_refcnt_debug_inc uses newsk->sk_prot->socks, that\n\t\t * is the same as sk->sk_prot->socks, as this field was copied\n\t\t * with memcpy).\n\t\t *\n\t\t * This _changes_ the previous behaviour, where\n\t\t * tcp_create_openreq_child always was incrementing the\n\t\t * equivalent to tcp_prot->socks (inet_sock_nr), so this have\n\t\t * to be taken into account in all callers. -acme\n\t\t */\n\t\tsk_refcnt_debug_inc(newsk);\n\t\tsk_set_socket(newsk, NULL);\n\t\tsk_tx_queue_clear(newsk);\n\t\tRCU_INIT_POINTER(newsk->sk_wq, NULL);\n\n\t\tif (newsk->sk_prot->sockets_allocated)\n\t\t\tsk_sockets_allocated_inc(newsk);\n\n\t\tif (sock_needs_netstamp(sk) &&\n\t\t    newsk->sk_flags & SK_FLAGS_TIMESTAMP)\n\t\t\tnet_enable_timestamp();\n\t}\nout:\n\treturn newsk;\n}",
                        "cve_id": "CVE-2020-14356"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 4021,
            "cve_id": "CVE-2023-1583",
            "code_snippet": "void __io_sqe_files_unregister(struct io_ring_ctx *ctx)\n{\n\tint i;\n\n\tfor (i = 0; i < ctx->nr_user_files; i++) {\n\t\tstruct file *file = io_file_from_index(&ctx->file_table, i);\n\n\t\t/* skip scm accounted files, they'll be freed by ->ring_sock */\n\t\tif (!file || io_file_need_scm(file))\n\t\t\tcontinue;\n\t\tio_file_bitmap_clear(&ctx->file_table, i);\n\t\tfput(file);\n\t}\n\n#if defined(CONFIG_UNIX)\n\tif (ctx->ring_sock) {\n\t\tstruct sock *sock = ctx->ring_sock->sk;\n\t\tstruct sk_buff *skb;\n\n\t\twhile ((skb = skb_dequeue(&sock->sk_receive_queue)) != NULL)\n\t\t\tkfree_skb(skb);\n\t}\n#endif\n\tio_free_file_tables(&ctx->file_table);\n\tio_rsrc_data_free(ctx->file_data);\n\tctx->file_data = NULL;\n\tctx->nr_user_files = 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "__cold void io_uring_show_fdinfo(struct seq_file *m, struct file *f)\n{\n\tstruct io_ring_ctx *ctx = f->private_data;\n\tstruct io_sq_data *sq = NULL;\n\tstruct io_overflow_cqe *ocqe;\n\tstruct io_rings *r = ctx->rings;\n\tunsigned int sq_mask = ctx->sq_entries - 1, cq_mask = ctx->cq_entries - 1;\n\tunsigned int sq_head = READ_ONCE(r->sq.head);\n\tunsigned int sq_tail = READ_ONCE(r->sq.tail);\n\tunsigned int cq_head = READ_ONCE(r->cq.head);\n\tunsigned int cq_tail = READ_ONCE(r->cq.tail);\n\tunsigned int cq_shift = 0;\n\tunsigned int sq_shift = 0;\n\tunsigned int sq_entries, cq_entries;\n\tbool has_lock;\n\tunsigned int i;\n\n\tif (ctx->flags & IORING_SETUP_CQE32)\n\t\tcq_shift = 1;\n\tif (ctx->flags & IORING_SETUP_SQE128)\n\t\tsq_shift = 1;\n\n\t/*\n\t * we may get imprecise sqe and cqe info if uring is actively running\n\t * since we get cached_sq_head and cached_cq_tail without uring_lock\n\t * and sq_tail and cq_head are changed by userspace. But it's ok since\n\t * we usually use these info when it is stuck.\n\t */\n\tseq_printf(m, \"SqMask:\\t0x%x\\n\", sq_mask);\n\tseq_printf(m, \"SqHead:\\t%u\\n\", sq_head);\n\tseq_printf(m, \"SqTail:\\t%u\\n\", sq_tail);\n\tseq_printf(m, \"CachedSqHead:\\t%u\\n\", ctx->cached_sq_head);\n\tseq_printf(m, \"CqMask:\\t0x%x\\n\", cq_mask);\n\tseq_printf(m, \"CqHead:\\t%u\\n\", cq_head);\n\tseq_printf(m, \"CqTail:\\t%u\\n\", cq_tail);\n\tseq_printf(m, \"CachedCqTail:\\t%u\\n\", ctx->cached_cq_tail);\n\tseq_printf(m, \"SQEs:\\t%u\\n\", sq_tail - sq_head);\n\tsq_entries = min(sq_tail - sq_head, ctx->sq_entries);\n\tfor (i = 0; i < sq_entries; i++) {\n\t\tunsigned int entry = i + sq_head;\n\t\tstruct io_uring_sqe *sqe;\n\t\tunsigned int sq_idx;\n\n\t\tif (ctx->flags & IORING_SETUP_NO_SQARRAY)\n\t\t\tbreak;\n\t\tsq_idx = READ_ONCE(ctx->sq_array[entry & sq_mask]);\n\t\tif (sq_idx > sq_mask)\n\t\t\tcontinue;\n\t\tsqe = &ctx->sq_sqes[sq_idx << sq_shift];\n\t\tseq_printf(m, \"%5u: opcode:%s, fd:%d, flags:%x, off:%llu, \"\n\t\t\t      \"addr:0x%llx, rw_flags:0x%x, buf_index:%d \"\n\t\t\t      \"user_data:%llu\",\n\t\t\t   sq_idx, io_uring_get_opcode(sqe->opcode), sqe->fd,\n\t\t\t   sqe->flags, (unsigned long long) sqe->off,\n\t\t\t   (unsigned long long) sqe->addr, sqe->rw_flags,\n\t\t\t   sqe->buf_index, sqe->user_data);\n\t\tif (sq_shift) {\n\t\t\tu64 *sqeb = (void *) (sqe + 1);\n\t\t\tint size = sizeof(struct io_uring_sqe) / sizeof(u64);\n\t\t\tint j;\n\n\t\t\tfor (j = 0; j < size; j++) {\n\t\t\t\tseq_printf(m, \", e%d:0x%llx\", j,\n\t\t\t\t\t\t(unsigned long long) *sqeb);\n\t\t\t\tsqeb++;\n\t\t\t}\n\t\t}\n\t\tseq_printf(m, \"\\n\");\n\t}\n\tseq_printf(m, \"CQEs:\\t%u\\n\", cq_tail - cq_head);\n\tcq_entries = min(cq_tail - cq_head, ctx->cq_entries);\n\tfor (i = 0; i < cq_entries; i++) {\n\t\tunsigned int entry = i + cq_head;\n\t\tstruct io_uring_cqe *cqe = &r->cqes[(entry & cq_mask) << cq_shift];\n\n\t\tseq_printf(m, \"%5u: user_data:%llu, res:%d, flag:%x\",\n\t\t\t   entry & cq_mask, cqe->user_data, cqe->res,\n\t\t\t   cqe->flags);\n\t\tif (cq_shift)\n\t\t\tseq_printf(m, \", extra1:%llu, extra2:%llu\\n\",\n\t\t\t\t\tcqe->big_cqe[0], cqe->big_cqe[1]);\n\t\tseq_printf(m, \"\\n\");\n\t}\n\n\t/*\n\t * Avoid ABBA deadlock between the seq lock and the io_uring mutex,\n\t * since fdinfo case grabs it in the opposite direction of normal use\n\t * cases. If we fail to get the lock, we just don't iterate any\n\t * structures that could be going away outside the io_uring mutex.\n\t */\n\thas_lock = mutex_trylock(&ctx->uring_lock);\n\n\tif (has_lock && (ctx->flags & IORING_SETUP_SQPOLL)) {\n\t\tsq = ctx->sq_data;\n\t\tif (!sq->thread)\n\t\t\tsq = NULL;\n\t}\n\n\tseq_printf(m, \"SqThread:\\t%d\\n\", sq ? task_pid_nr(sq->thread) : -1);\n\tseq_printf(m, \"SqThreadCpu:\\t%d\\n\", sq ? task_cpu(sq->thread) : -1);\n\tseq_printf(m, \"UserFiles:\\t%u\\n\", ctx->nr_user_files);\n\tfor (i = 0; has_lock && i < ctx->nr_user_files; i++) {\n\t\tstruct file *f = io_file_from_index(&ctx->file_table, i);\n\n\t\tif (f)\n\t\t\tseq_printf(m, \"%5u: %s\\n\", i, file_dentry(f)->d_iname);\n\t\telse\n\t\t\tseq_printf(m, \"%5u: <none>\\n\", i);\n\t}\n\tseq_printf(m, \"UserBufs:\\t%u\\n\", ctx->nr_user_bufs);\n\tfor (i = 0; has_lock && i < ctx->nr_user_bufs; i++) {\n\t\tstruct io_mapped_ubuf *buf = ctx->user_bufs[i];\n\t\tunsigned int len = buf->ubuf_end - buf->ubuf;\n\n\t\tseq_printf(m, \"%5u: 0x%llx/%u\\n\", i, buf->ubuf, len);\n\t}\n\tif (has_lock && !xa_empty(&ctx->personalities)) {\n\t\tunsigned long index;\n\t\tconst struct cred *cred;\n\n\t\tseq_printf(m, \"Personalities:\\n\");\n\t\txa_for_each(&ctx->personalities, index, cred)\n\t\t\tio_uring_show_cred(m, index, cred);\n\t}\n\n\tseq_puts(m, \"PollList:\\n\");\n\tfor (i = 0; i < (1U << ctx->cancel_table.hash_bits); i++) {\n\t\tstruct io_hash_bucket *hb = &ctx->cancel_table.hbs[i];\n\t\tstruct io_hash_bucket *hbl = &ctx->cancel_table_locked.hbs[i];\n\t\tstruct io_kiocb *req;\n\n\t\tspin_lock(&hb->lock);\n\t\thlist_for_each_entry(req, &hb->list, hash_node)\n\t\t\tseq_printf(m, \"  op=%d, task_works=%d\\n\", req->opcode,\n\t\t\t\t\ttask_work_pending(req->task));\n\t\tspin_unlock(&hb->lock);\n\n\t\tif (!has_lock)\n\t\t\tcontinue;\n\t\thlist_for_each_entry(req, &hbl->list, hash_node)\n\t\t\tseq_printf(m, \"  op=%d, task_works=%d\\n\", req->opcode,\n\t\t\t\t\ttask_work_pending(req->task));\n\t}\n\n\tif (has_lock)\n\t\tmutex_unlock(&ctx->uring_lock);\n\n\tseq_puts(m, \"CqOverflowList:\\n\");\n\tspin_lock(&ctx->completion_lock);\n\tlist_for_each_entry(ocqe, &ctx->cq_overflow_list, list) {\n\t\tstruct io_uring_cqe *cqe = &ocqe->cqe;\n\n\t\tseq_printf(m, \"  user_data=%llu, res=%d, flags=%x\\n\",\n\t\t\t   cqe->user_data, cqe->res, cqe->flags);\n\n\t}\n\n\tspin_unlock(&ctx->completion_lock);\n}",
                        "code_after_change": "__cold void io_uring_show_fdinfo(struct seq_file *m, struct file *f)\n{\n\tstruct io_ring_ctx *ctx = f->private_data;\n\tstruct io_overflow_cqe *ocqe;\n\tstruct io_rings *r = ctx->rings;\n\tunsigned int sq_mask = ctx->sq_entries - 1, cq_mask = ctx->cq_entries - 1;\n\tunsigned int sq_head = READ_ONCE(r->sq.head);\n\tunsigned int sq_tail = READ_ONCE(r->sq.tail);\n\tunsigned int cq_head = READ_ONCE(r->cq.head);\n\tunsigned int cq_tail = READ_ONCE(r->cq.tail);\n\tunsigned int cq_shift = 0;\n\tunsigned int sq_shift = 0;\n\tunsigned int sq_entries, cq_entries;\n\tint sq_pid = -1, sq_cpu = -1;\n\tbool has_lock;\n\tunsigned int i;\n\n\tif (ctx->flags & IORING_SETUP_CQE32)\n\t\tcq_shift = 1;\n\tif (ctx->flags & IORING_SETUP_SQE128)\n\t\tsq_shift = 1;\n\n\t/*\n\t * we may get imprecise sqe and cqe info if uring is actively running\n\t * since we get cached_sq_head and cached_cq_tail without uring_lock\n\t * and sq_tail and cq_head are changed by userspace. But it's ok since\n\t * we usually use these info when it is stuck.\n\t */\n\tseq_printf(m, \"SqMask:\\t0x%x\\n\", sq_mask);\n\tseq_printf(m, \"SqHead:\\t%u\\n\", sq_head);\n\tseq_printf(m, \"SqTail:\\t%u\\n\", sq_tail);\n\tseq_printf(m, \"CachedSqHead:\\t%u\\n\", ctx->cached_sq_head);\n\tseq_printf(m, \"CqMask:\\t0x%x\\n\", cq_mask);\n\tseq_printf(m, \"CqHead:\\t%u\\n\", cq_head);\n\tseq_printf(m, \"CqTail:\\t%u\\n\", cq_tail);\n\tseq_printf(m, \"CachedCqTail:\\t%u\\n\", ctx->cached_cq_tail);\n\tseq_printf(m, \"SQEs:\\t%u\\n\", sq_tail - sq_head);\n\tsq_entries = min(sq_tail - sq_head, ctx->sq_entries);\n\tfor (i = 0; i < sq_entries; i++) {\n\t\tunsigned int entry = i + sq_head;\n\t\tstruct io_uring_sqe *sqe;\n\t\tunsigned int sq_idx;\n\n\t\tif (ctx->flags & IORING_SETUP_NO_SQARRAY)\n\t\t\tbreak;\n\t\tsq_idx = READ_ONCE(ctx->sq_array[entry & sq_mask]);\n\t\tif (sq_idx > sq_mask)\n\t\t\tcontinue;\n\t\tsqe = &ctx->sq_sqes[sq_idx << sq_shift];\n\t\tseq_printf(m, \"%5u: opcode:%s, fd:%d, flags:%x, off:%llu, \"\n\t\t\t      \"addr:0x%llx, rw_flags:0x%x, buf_index:%d \"\n\t\t\t      \"user_data:%llu\",\n\t\t\t   sq_idx, io_uring_get_opcode(sqe->opcode), sqe->fd,\n\t\t\t   sqe->flags, (unsigned long long) sqe->off,\n\t\t\t   (unsigned long long) sqe->addr, sqe->rw_flags,\n\t\t\t   sqe->buf_index, sqe->user_data);\n\t\tif (sq_shift) {\n\t\t\tu64 *sqeb = (void *) (sqe + 1);\n\t\t\tint size = sizeof(struct io_uring_sqe) / sizeof(u64);\n\t\t\tint j;\n\n\t\t\tfor (j = 0; j < size; j++) {\n\t\t\t\tseq_printf(m, \", e%d:0x%llx\", j,\n\t\t\t\t\t\t(unsigned long long) *sqeb);\n\t\t\t\tsqeb++;\n\t\t\t}\n\t\t}\n\t\tseq_printf(m, \"\\n\");\n\t}\n\tseq_printf(m, \"CQEs:\\t%u\\n\", cq_tail - cq_head);\n\tcq_entries = min(cq_tail - cq_head, ctx->cq_entries);\n\tfor (i = 0; i < cq_entries; i++) {\n\t\tunsigned int entry = i + cq_head;\n\t\tstruct io_uring_cqe *cqe = &r->cqes[(entry & cq_mask) << cq_shift];\n\n\t\tseq_printf(m, \"%5u: user_data:%llu, res:%d, flag:%x\",\n\t\t\t   entry & cq_mask, cqe->user_data, cqe->res,\n\t\t\t   cqe->flags);\n\t\tif (cq_shift)\n\t\t\tseq_printf(m, \", extra1:%llu, extra2:%llu\\n\",\n\t\t\t\t\tcqe->big_cqe[0], cqe->big_cqe[1]);\n\t\tseq_printf(m, \"\\n\");\n\t}\n\n\t/*\n\t * Avoid ABBA deadlock between the seq lock and the io_uring mutex,\n\t * since fdinfo case grabs it in the opposite direction of normal use\n\t * cases. If we fail to get the lock, we just don't iterate any\n\t * structures that could be going away outside the io_uring mutex.\n\t */\n\thas_lock = mutex_trylock(&ctx->uring_lock);\n\n\tif (has_lock && (ctx->flags & IORING_SETUP_SQPOLL)) {\n\t\tstruct io_sq_data *sq = ctx->sq_data;\n\n\t\tif (mutex_trylock(&sq->lock)) {\n\t\t\tif (sq->thread) {\n\t\t\t\tsq_pid = task_pid_nr(sq->thread);\n\t\t\t\tsq_cpu = task_cpu(sq->thread);\n\t\t\t}\n\t\t\tmutex_unlock(&sq->lock);\n\t\t}\n\t}\n\n\tseq_printf(m, \"SqThread:\\t%d\\n\", sq_pid);\n\tseq_printf(m, \"SqThreadCpu:\\t%d\\n\", sq_cpu);\n\tseq_printf(m, \"UserFiles:\\t%u\\n\", ctx->nr_user_files);\n\tfor (i = 0; has_lock && i < ctx->nr_user_files; i++) {\n\t\tstruct file *f = io_file_from_index(&ctx->file_table, i);\n\n\t\tif (f)\n\t\t\tseq_printf(m, \"%5u: %s\\n\", i, file_dentry(f)->d_iname);\n\t\telse\n\t\t\tseq_printf(m, \"%5u: <none>\\n\", i);\n\t}\n\tseq_printf(m, \"UserBufs:\\t%u\\n\", ctx->nr_user_bufs);\n\tfor (i = 0; has_lock && i < ctx->nr_user_bufs; i++) {\n\t\tstruct io_mapped_ubuf *buf = ctx->user_bufs[i];\n\t\tunsigned int len = buf->ubuf_end - buf->ubuf;\n\n\t\tseq_printf(m, \"%5u: 0x%llx/%u\\n\", i, buf->ubuf, len);\n\t}\n\tif (has_lock && !xa_empty(&ctx->personalities)) {\n\t\tunsigned long index;\n\t\tconst struct cred *cred;\n\n\t\tseq_printf(m, \"Personalities:\\n\");\n\t\txa_for_each(&ctx->personalities, index, cred)\n\t\t\tio_uring_show_cred(m, index, cred);\n\t}\n\n\tseq_puts(m, \"PollList:\\n\");\n\tfor (i = 0; i < (1U << ctx->cancel_table.hash_bits); i++) {\n\t\tstruct io_hash_bucket *hb = &ctx->cancel_table.hbs[i];\n\t\tstruct io_hash_bucket *hbl = &ctx->cancel_table_locked.hbs[i];\n\t\tstruct io_kiocb *req;\n\n\t\tspin_lock(&hb->lock);\n\t\thlist_for_each_entry(req, &hb->list, hash_node)\n\t\t\tseq_printf(m, \"  op=%d, task_works=%d\\n\", req->opcode,\n\t\t\t\t\ttask_work_pending(req->task));\n\t\tspin_unlock(&hb->lock);\n\n\t\tif (!has_lock)\n\t\t\tcontinue;\n\t\thlist_for_each_entry(req, &hbl->list, hash_node)\n\t\t\tseq_printf(m, \"  op=%d, task_works=%d\\n\", req->opcode,\n\t\t\t\t\ttask_work_pending(req->task));\n\t}\n\n\tif (has_lock)\n\t\tmutex_unlock(&ctx->uring_lock);\n\n\tseq_puts(m, \"CqOverflowList:\\n\");\n\tspin_lock(&ctx->completion_lock);\n\tlist_for_each_entry(ocqe, &ctx->cq_overflow_list, list) {\n\t\tstruct io_uring_cqe *cqe = &ocqe->cqe;\n\n\t\tseq_printf(m, \"  user_data=%llu, res=%d, flags=%x\\n\",\n\t\t\t   cqe->user_data, cqe->res, cqe->flags);\n\n\t}\n\n\tspin_unlock(&ctx->completion_lock);\n}",
                        "cve_id": "CVE-2023-46862"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int io_file_bitmap_get(struct io_ring_ctx *ctx)\n{\n\tstruct io_file_table *table = &ctx->file_table;\n\tunsigned long nr = ctx->file_alloc_end;\n\tint ret;\n\n\tdo {\n\t\tret = find_next_zero_bit(table->bitmap, nr, table->alloc_hint);\n\t\tif (ret != nr)\n\t\t\treturn ret;\n\n\t\tif (table->alloc_hint == ctx->file_alloc_start)\n\t\t\tbreak;\n\t\tnr = table->alloc_hint;\n\t\ttable->alloc_hint = ctx->file_alloc_start;\n\t} while (1);\n\n\treturn -ENFILE;\n}",
                        "code_after_change": "static int io_file_bitmap_get(struct io_ring_ctx *ctx)\n{\n\tstruct io_file_table *table = &ctx->file_table;\n\tunsigned long nr = ctx->file_alloc_end;\n\tint ret;\n\n\tif (!table->bitmap)\n\t\treturn -ENFILE;\n\n\tdo {\n\t\tret = find_next_zero_bit(table->bitmap, nr, table->alloc_hint);\n\t\tif (ret != nr)\n\t\t\treturn ret;\n\n\t\tif (table->alloc_hint == ctx->file_alloc_start)\n\t\t\tbreak;\n\t\tnr = table->alloc_hint;\n\t\ttable->alloc_hint = ctx->file_alloc_start;\n\t} while (1);\n\n\treturn -ENFILE;\n}",
                        "cve_id": "CVE-2023-1583"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int skcipher_accept_parent(void *private, struct sock *sk)\n{\n\tstruct skcipher_ctx *ctx;\n\tstruct alg_sock *ask = alg_sk(sk);\n\tunsigned int len = sizeof(*ctx) + crypto_skcipher_reqsize(private);\n\n\tctx = sock_kmalloc(sk, len, GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\tctx->iv = sock_kmalloc(sk, crypto_skcipher_ivsize(private),\n\t\t\t       GFP_KERNEL);\n\tif (!ctx->iv) {\n\t\tsock_kfree_s(sk, ctx, len);\n\t\treturn -ENOMEM;\n\t}\n\n\tmemset(ctx->iv, 0, crypto_skcipher_ivsize(private));\n\n\tINIT_LIST_HEAD(&ctx->tsgl);\n\tctx->len = len;\n\tctx->used = 0;\n\tctx->more = 0;\n\tctx->merge = 0;\n\tctx->enc = 0;\n\tatomic_set(&ctx->inflight, 0);\n\taf_alg_init_completion(&ctx->completion);\n\n\task->private = ctx;\n\n\tskcipher_request_set_tfm(&ctx->req, private);\n\tskcipher_request_set_callback(&ctx->req, CRYPTO_TFM_REQ_MAY_BACKLOG,\n\t\t\t\t      af_alg_complete, &ctx->completion);\n\n\tsk->sk_destruct = skcipher_sock_destruct;\n\n\treturn 0;\n}",
                        "code_after_change": "static int io_files_update_with_index_alloc(struct io_kiocb *req,\n\t\t\t\t\t    unsigned int issue_flags)\n{\n\t__s32 __user *fds = u64_to_user_ptr(req->rsrc_update.arg);\n\tunsigned int done;\n\tstruct file *file;\n\tint ret, fd;\n\n\tif (!req->ctx->file_data)\n\t\treturn -ENXIO;\n\n\tfor (done = 0; done < req->rsrc_update.nr_args; done++) {\n\t\tif (copy_from_user(&fd, &fds[done], sizeof(fd))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tfile = fget(fd);\n\t\tif (!file) {\n\t\t\tret = -EBADF;\n\t\t\tbreak;\n\t\t}\n\t\tret = io_fixed_fd_install(req, issue_flags, file,\n\t\t\t\t\t  IORING_FILE_INDEX_ALLOC);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\tif (copy_to_user(&fds[done], &ret, sizeof(ret))) {\n\t\t\t__io_close_fixed(req, issue_flags, ret);\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (done)\n\t\treturn done;\n\treturn ret;\n}",
                        "cve_id": "CVE-2015-8970"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int nft_dynset_init(const struct nft_ctx *ctx,\n\t\t\t   const struct nft_expr *expr,\n\t\t\t   const struct nlattr * const tb[])\n{\n\tstruct nftables_pernet *nft_net = nft_pernet(ctx->net);\n\tstruct nft_dynset *priv = nft_expr_priv(expr);\n\tu8 genmask = nft_genmask_next(ctx->net);\n\tstruct nft_set *set;\n\tu64 timeout;\n\tint err, i;\n\n\tlockdep_assert_held(&nft_net->commit_mutex);\n\n\tif (tb[NFTA_DYNSET_SET_NAME] == NULL ||\n\t    tb[NFTA_DYNSET_OP] == NULL ||\n\t    tb[NFTA_DYNSET_SREG_KEY] == NULL)\n\t\treturn -EINVAL;\n\n\tif (tb[NFTA_DYNSET_FLAGS]) {\n\t\tu32 flags = ntohl(nla_get_be32(tb[NFTA_DYNSET_FLAGS]));\n\t\tif (flags & ~(NFT_DYNSET_F_INV | NFT_DYNSET_F_EXPR))\n\t\t\treturn -EOPNOTSUPP;\n\t\tif (flags & NFT_DYNSET_F_INV)\n\t\t\tpriv->invert = true;\n\t\tif (flags & NFT_DYNSET_F_EXPR)\n\t\t\tpriv->expr = true;\n\t}\n\n\tset = nft_set_lookup_global(ctx->net, ctx->table,\n\t\t\t\t    tb[NFTA_DYNSET_SET_NAME],\n\t\t\t\t    tb[NFTA_DYNSET_SET_ID], genmask);\n\tif (IS_ERR(set))\n\t\treturn PTR_ERR(set);\n\n\tif (set->flags & NFT_SET_OBJECT)\n\t\treturn -EOPNOTSUPP;\n\n\tif (set->ops->update == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (set->flags & NFT_SET_CONSTANT)\n\t\treturn -EBUSY;\n\n\tpriv->op = ntohl(nla_get_be32(tb[NFTA_DYNSET_OP]));\n\tif (priv->op > NFT_DYNSET_OP_DELETE)\n\t\treturn -EOPNOTSUPP;\n\n\ttimeout = 0;\n\tif (tb[NFTA_DYNSET_TIMEOUT] != NULL) {\n\t\tif (!(set->flags & NFT_SET_TIMEOUT))\n\t\t\treturn -EOPNOTSUPP;\n\n\t\terr = nf_msecs_to_jiffies64(tb[NFTA_DYNSET_TIMEOUT], &timeout);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = nft_parse_register_load(tb[NFTA_DYNSET_SREG_KEY], &priv->sreg_key,\n\t\t\t\t      set->klen);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[NFTA_DYNSET_SREG_DATA] != NULL) {\n\t\tif (!(set->flags & NFT_SET_MAP))\n\t\t\treturn -EOPNOTSUPP;\n\t\tif (set->dtype == NFT_DATA_VERDICT)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\terr = nft_parse_register_load(tb[NFTA_DYNSET_SREG_DATA],\n\t\t\t\t\t      &priv->sreg_data, set->dlen);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else if (set->flags & NFT_SET_MAP)\n\t\treturn -EINVAL;\n\n\tif ((tb[NFTA_DYNSET_EXPR] || tb[NFTA_DYNSET_EXPRESSIONS]) &&\n\t    !(set->flags & NFT_SET_EVAL))\n\t\treturn -EINVAL;\n\n\tif (tb[NFTA_DYNSET_EXPR]) {\n\t\tstruct nft_expr *dynset_expr;\n\n\t\tdynset_expr = nft_dynset_expr_alloc(ctx, set,\n\t\t\t\t\t\t    tb[NFTA_DYNSET_EXPR], 0);\n\t\tif (IS_ERR(dynset_expr))\n\t\t\treturn PTR_ERR(dynset_expr);\n\n\t\tpriv->num_exprs++;\n\t\tpriv->expr_array[0] = dynset_expr;\n\n\t\tif (set->num_exprs > 1 ||\n\t\t    (set->num_exprs == 1 &&\n\t\t     dynset_expr->ops != set->exprs[0]->ops)) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto err_expr_free;\n\t\t}\n\t} else if (tb[NFTA_DYNSET_EXPRESSIONS]) {\n\t\tstruct nft_expr *dynset_expr;\n\t\tstruct nlattr *tmp;\n\t\tint left;\n\n\t\tif (!priv->expr)\n\t\t\treturn -EINVAL;\n\n\t\ti = 0;\n\t\tnla_for_each_nested(tmp, tb[NFTA_DYNSET_EXPRESSIONS], left) {\n\t\t\tif (i == NFT_SET_EXPR_MAX) {\n\t\t\t\terr = -E2BIG;\n\t\t\t\tgoto err_expr_free;\n\t\t\t}\n\t\t\tif (nla_type(tmp) != NFTA_LIST_ELEM) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_expr_free;\n\t\t\t}\n\t\t\tdynset_expr = nft_dynset_expr_alloc(ctx, set, tmp, i);\n\t\t\tif (IS_ERR(dynset_expr)) {\n\t\t\t\terr = PTR_ERR(dynset_expr);\n\t\t\t\tgoto err_expr_free;\n\t\t\t}\n\t\t\tpriv->expr_array[i] = dynset_expr;\n\t\t\tpriv->num_exprs++;\n\n\t\t\tif (set->num_exprs &&\n\t\t\t    dynset_expr->ops != set->exprs[i]->ops) {\n\t\t\t\terr = -EOPNOTSUPP;\n\t\t\t\tgoto err_expr_free;\n\t\t\t}\n\t\t\ti++;\n\t\t}\n\t\tif (set->num_exprs && set->num_exprs != i) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto err_expr_free;\n\t\t}\n\t} else if (set->num_exprs > 0) {\n\t\terr = nft_set_elem_expr_clone(ctx, set, priv->expr_array);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\tpriv->num_exprs = set->num_exprs;\n\t}\n\n\tnft_set_ext_prepare(&priv->tmpl);\n\tnft_set_ext_add_length(&priv->tmpl, NFT_SET_EXT_KEY, set->klen);\n\tif (set->flags & NFT_SET_MAP)\n\t\tnft_set_ext_add_length(&priv->tmpl, NFT_SET_EXT_DATA, set->dlen);\n\n\tif (priv->num_exprs)\n\t\tnft_dynset_ext_add_expr(priv);\n\n\tif (set->flags & NFT_SET_TIMEOUT) {\n\t\tif (timeout || set->timeout) {\n\t\t\tnft_set_ext_add(&priv->tmpl, NFT_SET_EXT_TIMEOUT);\n\t\t\tnft_set_ext_add(&priv->tmpl, NFT_SET_EXT_EXPIRATION);\n\t\t}\n\t}\n\n\tpriv->timeout = timeout;\n\n\terr = nf_tables_bind_set(ctx, set, &priv->binding);\n\tif (err < 0)\n\t\tgoto err_expr_free;\n\n\tif (set->size == 0)\n\t\tset->size = 0xffff;\n\n\tpriv->set = set;\n\treturn 0;\n\nerr_expr_free:\n\tfor (i = 0; i < priv->num_exprs; i++)\n\t\tnft_expr_destroy(ctx, priv->expr_array[i]);\n\treturn err;\n}",
                        "code_after_change": "static int skcipher_accept_parent(void *private, struct sock *sk)\n{\n\tstruct skcipher_ctx *ctx;\n\tstruct alg_sock *ask = alg_sk(sk);\n\tstruct skcipher_tfm *tfm = private;\n\tstruct crypto_skcipher *skcipher = tfm->skcipher;\n\tunsigned int len = sizeof(*ctx) + crypto_skcipher_reqsize(skcipher);\n\n\tif (!tfm->has_key)\n\t\treturn -ENOKEY;\n\n\tctx = sock_kmalloc(sk, len, GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\tctx->iv = sock_kmalloc(sk, crypto_skcipher_ivsize(skcipher),\n\t\t\t       GFP_KERNEL);\n\tif (!ctx->iv) {\n\t\tsock_kfree_s(sk, ctx, len);\n\t\treturn -ENOMEM;\n\t}\n\n\tmemset(ctx->iv, 0, crypto_skcipher_ivsize(skcipher));\n\n\tINIT_LIST_HEAD(&ctx->tsgl);\n\tctx->len = len;\n\tctx->used = 0;\n\tctx->more = 0;\n\tctx->merge = 0;\n\tctx->enc = 0;\n\tatomic_set(&ctx->inflight, 0);\n\taf_alg_init_completion(&ctx->completion);\n\n\task->private = ctx;\n\n\tskcipher_request_set_tfm(&ctx->req, skcipher);\n\tskcipher_request_set_callback(&ctx->req, CRYPTO_TFM_REQ_MAY_BACKLOG,\n\t\t\t\t      af_alg_complete, &ctx->completion);\n\n\tsk->sk_destruct = skcipher_sock_destruct;\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2023-6622"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int do_tls_getsockopt_no_pad(struct sock *sk, char __user *optval,\n\t\t\t\t    int __user *optlen)\n{\n\tstruct tls_context *ctx = tls_get_ctx(sk);\n\tint value, len;\n\n\tif (ctx->prot_info.version != TLS_1_3_VERSION)\n\t\treturn -EINVAL;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (len < sizeof(value))\n\t\treturn -EINVAL;\n\n\tlock_sock(sk);\n\tvalue = -EINVAL;\n\tif (ctx->rx_conf == TLS_SW || ctx->rx_conf == TLS_HW)\n\t\tvalue = ctx->rx_no_pad;\n\trelease_sock(sk);\n\tif (value < 0)\n\t\treturn value;\n\n\tif (put_user(sizeof(value), optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &value, sizeof(value)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}",
                        "code_after_change": "static int nft_dynset_init(const struct nft_ctx *ctx,\n\t\t\t   const struct nft_expr *expr,\n\t\t\t   const struct nlattr * const tb[])\n{\n\tstruct nftables_pernet *nft_net = nft_pernet(ctx->net);\n\tstruct nft_dynset *priv = nft_expr_priv(expr);\n\tu8 genmask = nft_genmask_next(ctx->net);\n\tstruct nft_set *set;\n\tu64 timeout;\n\tint err, i;\n\n\tlockdep_assert_held(&nft_net->commit_mutex);\n\n\tif (tb[NFTA_DYNSET_SET_NAME] == NULL ||\n\t    tb[NFTA_DYNSET_OP] == NULL ||\n\t    tb[NFTA_DYNSET_SREG_KEY] == NULL)\n\t\treturn -EINVAL;\n\n\tif (tb[NFTA_DYNSET_FLAGS]) {\n\t\tu32 flags = ntohl(nla_get_be32(tb[NFTA_DYNSET_FLAGS]));\n\t\tif (flags & ~(NFT_DYNSET_F_INV | NFT_DYNSET_F_EXPR))\n\t\t\treturn -EOPNOTSUPP;\n\t\tif (flags & NFT_DYNSET_F_INV)\n\t\t\tpriv->invert = true;\n\t\tif (flags & NFT_DYNSET_F_EXPR)\n\t\t\tpriv->expr = true;\n\t}\n\n\tset = nft_set_lookup_global(ctx->net, ctx->table,\n\t\t\t\t    tb[NFTA_DYNSET_SET_NAME],\n\t\t\t\t    tb[NFTA_DYNSET_SET_ID], genmask);\n\tif (IS_ERR(set))\n\t\treturn PTR_ERR(set);\n\n\tif (set->flags & NFT_SET_OBJECT)\n\t\treturn -EOPNOTSUPP;\n\n\tif (set->ops->update == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (set->flags & NFT_SET_CONSTANT)\n\t\treturn -EBUSY;\n\n\tpriv->op = ntohl(nla_get_be32(tb[NFTA_DYNSET_OP]));\n\tif (priv->op > NFT_DYNSET_OP_DELETE)\n\t\treturn -EOPNOTSUPP;\n\n\ttimeout = 0;\n\tif (tb[NFTA_DYNSET_TIMEOUT] != NULL) {\n\t\tif (!(set->flags & NFT_SET_TIMEOUT))\n\t\t\treturn -EOPNOTSUPP;\n\n\t\terr = nf_msecs_to_jiffies64(tb[NFTA_DYNSET_TIMEOUT], &timeout);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = nft_parse_register_load(tb[NFTA_DYNSET_SREG_KEY], &priv->sreg_key,\n\t\t\t\t      set->klen);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[NFTA_DYNSET_SREG_DATA] != NULL) {\n\t\tif (!(set->flags & NFT_SET_MAP))\n\t\t\treturn -EOPNOTSUPP;\n\t\tif (set->dtype == NFT_DATA_VERDICT)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\terr = nft_parse_register_load(tb[NFTA_DYNSET_SREG_DATA],\n\t\t\t\t\t      &priv->sreg_data, set->dlen);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else if (set->flags & NFT_SET_MAP)\n\t\treturn -EINVAL;\n\n\tif ((tb[NFTA_DYNSET_EXPR] || tb[NFTA_DYNSET_EXPRESSIONS]) &&\n\t    !(set->flags & NFT_SET_EVAL))\n\t\treturn -EINVAL;\n\n\tif (tb[NFTA_DYNSET_EXPR]) {\n\t\tstruct nft_expr *dynset_expr;\n\n\t\tdynset_expr = nft_dynset_expr_alloc(ctx, set,\n\t\t\t\t\t\t    tb[NFTA_DYNSET_EXPR], 0);\n\t\tif (IS_ERR(dynset_expr))\n\t\t\treturn PTR_ERR(dynset_expr);\n\n\t\tpriv->num_exprs++;\n\t\tpriv->expr_array[0] = dynset_expr;\n\n\t\tif (set->num_exprs > 1 ||\n\t\t    (set->num_exprs == 1 &&\n\t\t     dynset_expr->ops != set->exprs[0]->ops)) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto err_expr_free;\n\t\t}\n\t} else if (tb[NFTA_DYNSET_EXPRESSIONS]) {\n\t\tstruct nft_expr *dynset_expr;\n\t\tstruct nlattr *tmp;\n\t\tint left;\n\n\t\tif (!priv->expr)\n\t\t\treturn -EINVAL;\n\n\t\ti = 0;\n\t\tnla_for_each_nested(tmp, tb[NFTA_DYNSET_EXPRESSIONS], left) {\n\t\t\tif (i == NFT_SET_EXPR_MAX) {\n\t\t\t\terr = -E2BIG;\n\t\t\t\tgoto err_expr_free;\n\t\t\t}\n\t\t\tif (nla_type(tmp) != NFTA_LIST_ELEM) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_expr_free;\n\t\t\t}\n\t\t\tdynset_expr = nft_dynset_expr_alloc(ctx, set, tmp, i);\n\t\t\tif (IS_ERR(dynset_expr)) {\n\t\t\t\terr = PTR_ERR(dynset_expr);\n\t\t\t\tgoto err_expr_free;\n\t\t\t}\n\t\t\tpriv->expr_array[i] = dynset_expr;\n\t\t\tpriv->num_exprs++;\n\n\t\t\tif (set->num_exprs) {\n\t\t\t\tif (i >= set->num_exprs) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tgoto err_expr_free;\n\t\t\t\t}\n\t\t\t\tif (dynset_expr->ops != set->exprs[i]->ops) {\n\t\t\t\t\terr = -EOPNOTSUPP;\n\t\t\t\t\tgoto err_expr_free;\n\t\t\t\t}\n\t\t\t}\n\t\t\ti++;\n\t\t}\n\t\tif (set->num_exprs && set->num_exprs != i) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto err_expr_free;\n\t\t}\n\t} else if (set->num_exprs > 0) {\n\t\terr = nft_set_elem_expr_clone(ctx, set, priv->expr_array);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\tpriv->num_exprs = set->num_exprs;\n\t}\n\n\tnft_set_ext_prepare(&priv->tmpl);\n\tnft_set_ext_add_length(&priv->tmpl, NFT_SET_EXT_KEY, set->klen);\n\tif (set->flags & NFT_SET_MAP)\n\t\tnft_set_ext_add_length(&priv->tmpl, NFT_SET_EXT_DATA, set->dlen);\n\n\tif (priv->num_exprs)\n\t\tnft_dynset_ext_add_expr(priv);\n\n\tif (set->flags & NFT_SET_TIMEOUT) {\n\t\tif (timeout || set->timeout) {\n\t\t\tnft_set_ext_add(&priv->tmpl, NFT_SET_EXT_TIMEOUT);\n\t\t\tnft_set_ext_add(&priv->tmpl, NFT_SET_EXT_EXPIRATION);\n\t\t}\n\t}\n\n\tpriv->timeout = timeout;\n\n\terr = nf_tables_bind_set(ctx, set, &priv->binding);\n\tif (err < 0)\n\t\tgoto err_expr_free;\n\n\tif (set->size == 0)\n\t\tset->size = 0xffff;\n\n\tpriv->set = set;\n\treturn 0;\n\nerr_expr_free:\n\tfor (i = 0; i < priv->num_exprs; i++)\n\t\tnft_expr_destroy(ctx, priv->expr_array[i]);\n\treturn err;\n}",
                        "cve_id": "CVE-2023-28466"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 4075,
            "cve_id": "CVE-2023-2166",
            "code_snippet": "static int canfd_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t     struct packet_type *pt, struct net_device *orig_dev)\n{\n\tif (unlikely(dev->type != ARPHRD_CAN || (!can_is_canfd_skb(skb)))) {\n\t\tpr_warn_once(\"PF_CAN: dropped non conform CAN FD skbuff: dev type %d, len %d\\n\",\n\t\t\t     dev->type, skb->len);\n\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\n\tcan_receive(skb, dev);\n\treturn NET_RX_SUCCESS;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int can_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t   struct packet_type *pt, struct net_device *orig_dev)\n{\n\tif (unlikely(dev->type != ARPHRD_CAN || (!can_is_can_skb(skb)))) {\n\t\tpr_warn_once(\"PF_CAN: dropped non conform CAN skbuff: dev type %d, len %d\\n\",\n\t\t\t     dev->type, skb->len);\n\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\n\tcan_receive(skb, dev);\n\treturn NET_RX_SUCCESS;\n}",
                        "code_after_change": "static int can_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t   struct packet_type *pt, struct net_device *orig_dev)\n{\n\tif (unlikely(dev->type != ARPHRD_CAN || !can_get_ml_priv(dev) || !can_is_can_skb(skb))) {\n\t\tpr_warn_once(\"PF_CAN: dropped non conform CAN skbuff: dev type %d, len %d\\n\",\n\t\t\t     dev->type, skb->len);\n\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\n\tcan_receive(skb, dev);\n\treturn NET_RX_SUCCESS;\n}",
                        "cve_id": "CVE-2023-2166"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 4081,
            "cve_id": "CVE-2023-2177",
            "code_snippet": "int sctp_stream_init(struct sctp_stream *stream, __u16 outcnt, __u16 incnt,\n\t\t     gfp_t gfp)\n{\n\tstruct sctp_sched_ops *sched = sctp_sched_ops_from_stream(stream);\n\tint i, ret = 0;\n\n\tgfp |= __GFP_NOWARN;\n\n\t/* Initial stream->out size may be very big, so free it and alloc\n\t * a new one with new outcnt to save memory if needed.\n\t */\n\tif (outcnt == stream->outcnt)\n\t\tgoto handle_in;\n\n\t/* Filter out chunks queued on streams that won't exist anymore */\n\tsched->unsched_all(stream);\n\tsctp_stream_outq_migrate(stream, NULL, outcnt);\n\tsched->sched_all(stream);\n\n\tret = sctp_stream_alloc_out(stream, outcnt, gfp);\n\tif (ret)\n\t\tgoto out_err;\n\n\tfor (i = 0; i < stream->outcnt; i++)\n\t\tSCTP_SO(stream, i)->state = SCTP_STREAM_OPEN;\n\nhandle_in:\n\tsctp_stream_interleave_init(stream);\n\tif (!incnt)\n\t\tgoto out;\n\n\tret = sctp_stream_alloc_in(stream, incnt, gfp);\n\tif (ret)\n\t\tgoto in_err;\n\n\tgoto out;\n\nin_err:\n\tsched->free(stream);\n\tgenradix_free(&stream->in);\nout_err:\n\tgenradix_free(&stream->out);\n\tstream->outcnt = 0;\nout:\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static struct sctp_association *sctp_association_init(\n\t\t\t\t\tstruct sctp_association *asoc,\n\t\t\t\t\tconst struct sctp_endpoint *ep,\n\t\t\t\t\tconst struct sock *sk,\n\t\t\t\t\tenum sctp_scope scope, gfp_t gfp)\n{\n\tstruct sctp_sock *sp;\n\tstruct sctp_paramhdr *p;\n\tint i;\n\n\t/* Retrieve the SCTP per socket area.  */\n\tsp = sctp_sk((struct sock *)sk);\n\n\t/* Discarding const is appropriate here.  */\n\tasoc->ep = (struct sctp_endpoint *)ep;\n\tasoc->base.sk = (struct sock *)sk;\n\tasoc->base.net = sock_net(sk);\n\n\tsctp_endpoint_hold(asoc->ep);\n\tsock_hold(asoc->base.sk);\n\n\t/* Initialize the common base substructure.  */\n\tasoc->base.type = SCTP_EP_TYPE_ASSOCIATION;\n\n\t/* Initialize the object handling fields.  */\n\trefcount_set(&asoc->base.refcnt, 1);\n\n\t/* Initialize the bind addr area.  */\n\tsctp_bind_addr_init(&asoc->base.bind_addr, ep->base.bind_addr.port);\n\n\tasoc->state = SCTP_STATE_CLOSED;\n\tasoc->cookie_life = ms_to_ktime(sp->assocparams.sasoc_cookie_life);\n\tasoc->user_frag = sp->user_frag;\n\n\t/* Set the association max_retrans and RTO values from the\n\t * socket values.\n\t */\n\tasoc->max_retrans = sp->assocparams.sasoc_asocmaxrxt;\n\tasoc->pf_retrans  = sp->pf_retrans;\n\tasoc->ps_retrans  = sp->ps_retrans;\n\tasoc->pf_expose   = sp->pf_expose;\n\n\tasoc->rto_initial = msecs_to_jiffies(sp->rtoinfo.srto_initial);\n\tasoc->rto_max = msecs_to_jiffies(sp->rtoinfo.srto_max);\n\tasoc->rto_min = msecs_to_jiffies(sp->rtoinfo.srto_min);\n\n\t/* Initialize the association's heartbeat interval based on the\n\t * sock configured value.\n\t */\n\tasoc->hbinterval = msecs_to_jiffies(sp->hbinterval);\n\tasoc->probe_interval = msecs_to_jiffies(sp->probe_interval);\n\n\tasoc->encap_port = sp->encap_port;\n\n\t/* Initialize path max retrans value. */\n\tasoc->pathmaxrxt = sp->pathmaxrxt;\n\n\tasoc->flowlabel = sp->flowlabel;\n\tasoc->dscp = sp->dscp;\n\n\t/* Set association default SACK delay */\n\tasoc->sackdelay = msecs_to_jiffies(sp->sackdelay);\n\tasoc->sackfreq = sp->sackfreq;\n\n\t/* Set the association default flags controlling\n\t * Heartbeat, SACK delay, and Path MTU Discovery.\n\t */\n\tasoc->param_flags = sp->param_flags;\n\n\t/* Initialize the maximum number of new data packets that can be sent\n\t * in a burst.\n\t */\n\tasoc->max_burst = sp->max_burst;\n\n\tasoc->subscribe = sp->subscribe;\n\n\t/* initialize association timers */\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_T1_COOKIE] = asoc->rto_initial;\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_T1_INIT] = asoc->rto_initial;\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_T2_SHUTDOWN] = asoc->rto_initial;\n\n\t/* sctpimpguide Section 2.12.2\n\t * If the 'T5-shutdown-guard' timer is used, it SHOULD be set to the\n\t * recommended value of 5 times 'RTO.Max'.\n\t */\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_T5_SHUTDOWN_GUARD]\n\t\t= 5 * asoc->rto_max;\n\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_SACK] = asoc->sackdelay;\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_AUTOCLOSE] = sp->autoclose * HZ;\n\n\t/* Initializes the timers */\n\tfor (i = SCTP_EVENT_TIMEOUT_NONE; i < SCTP_NUM_TIMEOUT_TYPES; ++i)\n\t\ttimer_setup(&asoc->timers[i], sctp_timer_events[i], 0);\n\n\t/* Pull default initialization values from the sock options.\n\t * Note: This assumes that the values have already been\n\t * validated in the sock.\n\t */\n\tasoc->c.sinit_max_instreams = sp->initmsg.sinit_max_instreams;\n\tasoc->c.sinit_num_ostreams  = sp->initmsg.sinit_num_ostreams;\n\tasoc->max_init_attempts\t= sp->initmsg.sinit_max_attempts;\n\n\tasoc->max_init_timeo =\n\t\t msecs_to_jiffies(sp->initmsg.sinit_max_init_timeo);\n\n\t/* Set the local window size for receive.\n\t * This is also the rcvbuf space per association.\n\t * RFC 6 - A SCTP receiver MUST be able to receive a minimum of\n\t * 1500 bytes in one SCTP packet.\n\t */\n\tif ((sk->sk_rcvbuf/2) < SCTP_DEFAULT_MINWINDOW)\n\t\tasoc->rwnd = SCTP_DEFAULT_MINWINDOW;\n\telse\n\t\tasoc->rwnd = sk->sk_rcvbuf/2;\n\n\tasoc->a_rwnd = asoc->rwnd;\n\n\t/* Use my own max window until I learn something better.  */\n\tasoc->peer.rwnd = SCTP_DEFAULT_MAXWINDOW;\n\n\t/* Initialize the receive memory counter */\n\tatomic_set(&asoc->rmem_alloc, 0);\n\n\tinit_waitqueue_head(&asoc->wait);\n\n\tasoc->c.my_vtag = sctp_generate_tag(ep);\n\tasoc->c.my_port = ep->base.bind_addr.port;\n\n\tasoc->c.initial_tsn = sctp_generate_tsn(ep);\n\n\tasoc->next_tsn = asoc->c.initial_tsn;\n\n\tasoc->ctsn_ack_point = asoc->next_tsn - 1;\n\tasoc->adv_peer_ack_point = asoc->ctsn_ack_point;\n\tasoc->highest_sacked = asoc->ctsn_ack_point;\n\tasoc->last_cwr_tsn = asoc->ctsn_ack_point;\n\n\t/* ADDIP Section 4.1 Asconf Chunk Procedures\n\t *\n\t * When an endpoint has an ASCONF signaled change to be sent to the\n\t * remote endpoint it should do the following:\n\t * ...\n\t * A2) a serial number should be assigned to the chunk. The serial\n\t * number SHOULD be a monotonically increasing number. The serial\n\t * numbers SHOULD be initialized at the start of the\n\t * association to the same value as the initial TSN.\n\t */\n\tasoc->addip_serial = asoc->c.initial_tsn;\n\tasoc->strreset_outseq = asoc->c.initial_tsn;\n\n\tINIT_LIST_HEAD(&asoc->addip_chunk_list);\n\tINIT_LIST_HEAD(&asoc->asconf_ack_list);\n\n\t/* Make an empty list of remote transport addresses.  */\n\tINIT_LIST_HEAD(&asoc->peer.transport_addr_list);\n\n\t/* RFC 2960 5.1 Normal Establishment of an Association\n\t *\n\t * After the reception of the first data chunk in an\n\t * association the endpoint must immediately respond with a\n\t * sack to acknowledge the data chunk.  Subsequent\n\t * acknowledgements should be done as described in Section\n\t * 6.2.\n\t *\n\t * [We implement this by telling a new association that it\n\t * already received one packet.]\n\t */\n\tasoc->peer.sack_needed = 1;\n\tasoc->peer.sack_generation = 1;\n\n\t/* Create an input queue.  */\n\tsctp_inq_init(&asoc->base.inqueue);\n\tsctp_inq_set_th_handler(&asoc->base.inqueue, sctp_assoc_bh_rcv);\n\n\t/* Create an output queue.  */\n\tsctp_outq_init(asoc, &asoc->outqueue);\n\n\tif (!sctp_ulpq_init(&asoc->ulpq, asoc))\n\t\tgoto fail_init;\n\n\tif (sctp_stream_init(&asoc->stream, asoc->c.sinit_num_ostreams,\n\t\t\t     0, gfp))\n\t\tgoto fail_init;\n\n\t/* Initialize default path MTU. */\n\tasoc->pathmtu = sp->pathmtu;\n\tsctp_assoc_update_frag_point(asoc);\n\n\t/* Assume that peer would support both address types unless we are\n\t * told otherwise.\n\t */\n\tasoc->peer.ipv4_address = 1;\n\tif (asoc->base.sk->sk_family == PF_INET6)\n\t\tasoc->peer.ipv6_address = 1;\n\tINIT_LIST_HEAD(&asoc->asocs);\n\n\tasoc->default_stream = sp->default_stream;\n\tasoc->default_ppid = sp->default_ppid;\n\tasoc->default_flags = sp->default_flags;\n\tasoc->default_context = sp->default_context;\n\tasoc->default_timetolive = sp->default_timetolive;\n\tasoc->default_rcv_context = sp->default_rcv_context;\n\n\t/* AUTH related initializations */\n\tINIT_LIST_HEAD(&asoc->endpoint_shared_keys);\n\tif (sctp_auth_asoc_copy_shkeys(ep, asoc, gfp))\n\t\tgoto stream_free;\n\n\tasoc->active_key_id = ep->active_key_id;\n\tasoc->strreset_enable = ep->strreset_enable;\n\n\t/* Save the hmacs and chunks list into this association */\n\tif (ep->auth_hmacs_list)\n\t\tmemcpy(asoc->c.auth_hmacs, ep->auth_hmacs_list,\n\t\t\tntohs(ep->auth_hmacs_list->param_hdr.length));\n\tif (ep->auth_chunk_list)\n\t\tmemcpy(asoc->c.auth_chunks, ep->auth_chunk_list,\n\t\t\tntohs(ep->auth_chunk_list->param_hdr.length));\n\n\t/* Get the AUTH random number for this association */\n\tp = (struct sctp_paramhdr *)asoc->c.auth_random;\n\tp->type = SCTP_PARAM_RANDOM;\n\tp->length = htons(sizeof(*p) + SCTP_AUTH_RANDOM_LENGTH);\n\tget_random_bytes(p+1, SCTP_AUTH_RANDOM_LENGTH);\n\n\treturn asoc;\n\nstream_free:\n\tsctp_stream_free(&asoc->stream);\nfail_init:\n\tsock_put(asoc->base.sk);\n\tsctp_endpoint_put(asoc->ep);\n\treturn NULL;\n}",
                        "code_after_change": "static struct sctp_association *sctp_association_init(\n\t\t\t\t\tstruct sctp_association *asoc,\n\t\t\t\t\tconst struct sctp_endpoint *ep,\n\t\t\t\t\tconst struct sock *sk,\n\t\t\t\t\tenum sctp_scope scope, gfp_t gfp)\n{\n\tstruct sctp_sock *sp;\n\tstruct sctp_paramhdr *p;\n\tint i;\n\n\t/* Retrieve the SCTP per socket area.  */\n\tsp = sctp_sk((struct sock *)sk);\n\n\t/* Discarding const is appropriate here.  */\n\tasoc->ep = (struct sctp_endpoint *)ep;\n\tasoc->base.sk = (struct sock *)sk;\n\tasoc->base.net = sock_net(sk);\n\n\tsctp_endpoint_hold(asoc->ep);\n\tsock_hold(asoc->base.sk);\n\n\t/* Initialize the common base substructure.  */\n\tasoc->base.type = SCTP_EP_TYPE_ASSOCIATION;\n\n\t/* Initialize the object handling fields.  */\n\trefcount_set(&asoc->base.refcnt, 1);\n\n\t/* Initialize the bind addr area.  */\n\tsctp_bind_addr_init(&asoc->base.bind_addr, ep->base.bind_addr.port);\n\n\tasoc->state = SCTP_STATE_CLOSED;\n\tasoc->cookie_life = ms_to_ktime(sp->assocparams.sasoc_cookie_life);\n\tasoc->user_frag = sp->user_frag;\n\n\t/* Set the association max_retrans and RTO values from the\n\t * socket values.\n\t */\n\tasoc->max_retrans = sp->assocparams.sasoc_asocmaxrxt;\n\tasoc->pf_retrans  = sp->pf_retrans;\n\tasoc->ps_retrans  = sp->ps_retrans;\n\tasoc->pf_expose   = sp->pf_expose;\n\n\tasoc->rto_initial = msecs_to_jiffies(sp->rtoinfo.srto_initial);\n\tasoc->rto_max = msecs_to_jiffies(sp->rtoinfo.srto_max);\n\tasoc->rto_min = msecs_to_jiffies(sp->rtoinfo.srto_min);\n\n\t/* Initialize the association's heartbeat interval based on the\n\t * sock configured value.\n\t */\n\tasoc->hbinterval = msecs_to_jiffies(sp->hbinterval);\n\tasoc->probe_interval = msecs_to_jiffies(sp->probe_interval);\n\n\tasoc->encap_port = sp->encap_port;\n\n\t/* Initialize path max retrans value. */\n\tasoc->pathmaxrxt = sp->pathmaxrxt;\n\n\tasoc->flowlabel = sp->flowlabel;\n\tasoc->dscp = sp->dscp;\n\n\t/* Set association default SACK delay */\n\tasoc->sackdelay = msecs_to_jiffies(sp->sackdelay);\n\tasoc->sackfreq = sp->sackfreq;\n\n\t/* Set the association default flags controlling\n\t * Heartbeat, SACK delay, and Path MTU Discovery.\n\t */\n\tasoc->param_flags = sp->param_flags;\n\n\t/* Initialize the maximum number of new data packets that can be sent\n\t * in a burst.\n\t */\n\tasoc->max_burst = sp->max_burst;\n\n\tasoc->subscribe = sp->subscribe;\n\n\t/* initialize association timers */\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_T1_COOKIE] = asoc->rto_initial;\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_T1_INIT] = asoc->rto_initial;\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_T2_SHUTDOWN] = asoc->rto_initial;\n\n\t/* sctpimpguide Section 2.12.2\n\t * If the 'T5-shutdown-guard' timer is used, it SHOULD be set to the\n\t * recommended value of 5 times 'RTO.Max'.\n\t */\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_T5_SHUTDOWN_GUARD]\n\t\t= 5 * asoc->rto_max;\n\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_SACK] = asoc->sackdelay;\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_AUTOCLOSE] = sp->autoclose * HZ;\n\n\t/* Initializes the timers */\n\tfor (i = SCTP_EVENT_TIMEOUT_NONE; i < SCTP_NUM_TIMEOUT_TYPES; ++i)\n\t\ttimer_setup(&asoc->timers[i], sctp_timer_events[i], 0);\n\n\t/* Pull default initialization values from the sock options.\n\t * Note: This assumes that the values have already been\n\t * validated in the sock.\n\t */\n\tasoc->c.sinit_max_instreams = sp->initmsg.sinit_max_instreams;\n\tasoc->c.sinit_num_ostreams  = sp->initmsg.sinit_num_ostreams;\n\tasoc->max_init_attempts\t= sp->initmsg.sinit_max_attempts;\n\n\tasoc->max_init_timeo =\n\t\t msecs_to_jiffies(sp->initmsg.sinit_max_init_timeo);\n\n\t/* Set the local window size for receive.\n\t * This is also the rcvbuf space per association.\n\t * RFC 6 - A SCTP receiver MUST be able to receive a minimum of\n\t * 1500 bytes in one SCTP packet.\n\t */\n\tif ((sk->sk_rcvbuf/2) < SCTP_DEFAULT_MINWINDOW)\n\t\tasoc->rwnd = SCTP_DEFAULT_MINWINDOW;\n\telse\n\t\tasoc->rwnd = sk->sk_rcvbuf/2;\n\n\tasoc->a_rwnd = asoc->rwnd;\n\n\t/* Use my own max window until I learn something better.  */\n\tasoc->peer.rwnd = SCTP_DEFAULT_MAXWINDOW;\n\n\t/* Initialize the receive memory counter */\n\tatomic_set(&asoc->rmem_alloc, 0);\n\n\tinit_waitqueue_head(&asoc->wait);\n\n\tasoc->c.my_vtag = sctp_generate_tag(ep);\n\tasoc->c.my_port = ep->base.bind_addr.port;\n\n\tasoc->c.initial_tsn = sctp_generate_tsn(ep);\n\n\tasoc->next_tsn = asoc->c.initial_tsn;\n\n\tasoc->ctsn_ack_point = asoc->next_tsn - 1;\n\tasoc->adv_peer_ack_point = asoc->ctsn_ack_point;\n\tasoc->highest_sacked = asoc->ctsn_ack_point;\n\tasoc->last_cwr_tsn = asoc->ctsn_ack_point;\n\n\t/* ADDIP Section 4.1 Asconf Chunk Procedures\n\t *\n\t * When an endpoint has an ASCONF signaled change to be sent to the\n\t * remote endpoint it should do the following:\n\t * ...\n\t * A2) a serial number should be assigned to the chunk. The serial\n\t * number SHOULD be a monotonically increasing number. The serial\n\t * numbers SHOULD be initialized at the start of the\n\t * association to the same value as the initial TSN.\n\t */\n\tasoc->addip_serial = asoc->c.initial_tsn;\n\tasoc->strreset_outseq = asoc->c.initial_tsn;\n\n\tINIT_LIST_HEAD(&asoc->addip_chunk_list);\n\tINIT_LIST_HEAD(&asoc->asconf_ack_list);\n\n\t/* Make an empty list of remote transport addresses.  */\n\tINIT_LIST_HEAD(&asoc->peer.transport_addr_list);\n\n\t/* RFC 2960 5.1 Normal Establishment of an Association\n\t *\n\t * After the reception of the first data chunk in an\n\t * association the endpoint must immediately respond with a\n\t * sack to acknowledge the data chunk.  Subsequent\n\t * acknowledgements should be done as described in Section\n\t * 6.2.\n\t *\n\t * [We implement this by telling a new association that it\n\t * already received one packet.]\n\t */\n\tasoc->peer.sack_needed = 1;\n\tasoc->peer.sack_generation = 1;\n\n\t/* Create an input queue.  */\n\tsctp_inq_init(&asoc->base.inqueue);\n\tsctp_inq_set_th_handler(&asoc->base.inqueue, sctp_assoc_bh_rcv);\n\n\t/* Create an output queue.  */\n\tsctp_outq_init(asoc, &asoc->outqueue);\n\n\tif (!sctp_ulpq_init(&asoc->ulpq, asoc))\n\t\tgoto fail_init;\n\n\tif (sctp_stream_init(&asoc->stream, asoc->c.sinit_num_ostreams, 0, gfp))\n\t\tgoto stream_free;\n\n\t/* Initialize default path MTU. */\n\tasoc->pathmtu = sp->pathmtu;\n\tsctp_assoc_update_frag_point(asoc);\n\n\t/* Assume that peer would support both address types unless we are\n\t * told otherwise.\n\t */\n\tasoc->peer.ipv4_address = 1;\n\tif (asoc->base.sk->sk_family == PF_INET6)\n\t\tasoc->peer.ipv6_address = 1;\n\tINIT_LIST_HEAD(&asoc->asocs);\n\n\tasoc->default_stream = sp->default_stream;\n\tasoc->default_ppid = sp->default_ppid;\n\tasoc->default_flags = sp->default_flags;\n\tasoc->default_context = sp->default_context;\n\tasoc->default_timetolive = sp->default_timetolive;\n\tasoc->default_rcv_context = sp->default_rcv_context;\n\n\t/* AUTH related initializations */\n\tINIT_LIST_HEAD(&asoc->endpoint_shared_keys);\n\tif (sctp_auth_asoc_copy_shkeys(ep, asoc, gfp))\n\t\tgoto stream_free;\n\n\tasoc->active_key_id = ep->active_key_id;\n\tasoc->strreset_enable = ep->strreset_enable;\n\n\t/* Save the hmacs and chunks list into this association */\n\tif (ep->auth_hmacs_list)\n\t\tmemcpy(asoc->c.auth_hmacs, ep->auth_hmacs_list,\n\t\t\tntohs(ep->auth_hmacs_list->param_hdr.length));\n\tif (ep->auth_chunk_list)\n\t\tmemcpy(asoc->c.auth_chunks, ep->auth_chunk_list,\n\t\t\tntohs(ep->auth_chunk_list->param_hdr.length));\n\n\t/* Get the AUTH random number for this association */\n\tp = (struct sctp_paramhdr *)asoc->c.auth_random;\n\tp->type = SCTP_PARAM_RANDOM;\n\tp->length = htons(sizeof(*p) + SCTP_AUTH_RANDOM_LENGTH);\n\tget_random_bytes(p+1, SCTP_AUTH_RANDOM_LENGTH);\n\n\treturn asoc;\n\nstream_free:\n\tsctp_stream_free(&asoc->stream);\nfail_init:\n\tsock_put(asoc->base.sk);\n\tsctp_endpoint_put(asoc->ep);\n\treturn NULL;\n}",
                        "cve_id": "CVE-2023-2177"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 4089,
            "cve_id": "CVE-2023-22997",
            "code_snippet": "static ssize_t module_xz_decompress(struct load_info *info,\n\t\t\t\t    const void *buf, size_t size)\n{\n\tstatic const u8 signature[] = { 0xfd, '7', 'z', 'X', 'Z', 0 };\n\tstruct xz_dec *xz_dec;\n\tstruct xz_buf xz_buf;\n\tenum xz_ret xz_ret;\n\tsize_t new_size = 0;\n\tssize_t retval;\n\n\tif (size < sizeof(signature) ||\n\t    memcmp(buf, signature, sizeof(signature))) {\n\t\tpr_err(\"not an xz compressed module\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\txz_dec = xz_dec_init(XZ_DYNALLOC, (u32)-1);\n\tif (!xz_dec)\n\t\treturn -ENOMEM;\n\n\txz_buf.in_size = size;\n\txz_buf.in = buf;\n\txz_buf.in_pos = 0;\n\n\tdo {\n\t\tstruct page *page = module_get_next_page(info);\n\n\t\tif (!page) {\n\t\t\tretval = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\txz_buf.out = kmap_local_page(page);\n\t\txz_buf.out_pos = 0;\n\t\txz_buf.out_size = PAGE_SIZE;\n\t\txz_ret = xz_dec_run(xz_dec, &xz_buf);\n\t\tkunmap_local(xz_buf.out);\n\n\t\tnew_size += xz_buf.out_pos;\n\t} while (xz_buf.out_pos == PAGE_SIZE && xz_ret == XZ_OK);\n\n\tif (xz_ret != XZ_STREAM_END) {\n\t\tpr_err(\"decompression failed with status %d\\n\", xz_ret);\n\t\tretval = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tretval = new_size;\n\n out:\n\txz_dec_end(xz_dec);\n\treturn retval;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static ssize_t module_gzip_decompress(struct load_info *info,\n\t\t\t\t      const void *buf, size_t size)\n{\n\tstruct z_stream_s s = { 0 };\n\tsize_t new_size = 0;\n\tsize_t gzip_hdr_len;\n\tssize_t retval;\n\tint rc;\n\n\tgzip_hdr_len = module_gzip_header_len(buf, size);\n\tif (!gzip_hdr_len) {\n\t\tpr_err(\"not a gzip compressed module\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\ts.next_in = buf + gzip_hdr_len;\n\ts.avail_in = size - gzip_hdr_len;\n\n\ts.workspace = kmalloc(zlib_inflate_workspacesize(), GFP_KERNEL);\n\tif (!s.workspace)\n\t\treturn -ENOMEM;\n\n\trc = zlib_inflateInit2(&s, -MAX_WBITS);\n\tif (rc != Z_OK) {\n\t\tpr_err(\"failed to initialize decompressor: %d\\n\", rc);\n\t\tretval = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tdo {\n\t\tstruct page *page = module_get_next_page(info);\n\n\t\tif (!page) {\n\t\t\tretval = -ENOMEM;\n\t\t\tgoto out_inflate_end;\n\t\t}\n\n\t\ts.next_out = kmap_local_page(page);\n\t\ts.avail_out = PAGE_SIZE;\n\t\trc = zlib_inflate(&s, 0);\n\t\tkunmap_local(s.next_out);\n\n\t\tnew_size += PAGE_SIZE - s.avail_out;\n\t} while (rc == Z_OK);\n\n\tif (rc != Z_STREAM_END) {\n\t\tpr_err(\"decompression failed with status %d\\n\", rc);\n\t\tretval = -EINVAL;\n\t\tgoto out_inflate_end;\n\t}\n\n\tretval = new_size;\n\nout_inflate_end:\n\tzlib_inflateEnd(&s);\nout:\n\tkfree(s.workspace);\n\treturn retval;\n}",
                        "code_after_change": "static ssize_t module_gzip_decompress(struct load_info *info,\n\t\t\t\t      const void *buf, size_t size)\n{\n\tstruct z_stream_s s = { 0 };\n\tsize_t new_size = 0;\n\tsize_t gzip_hdr_len;\n\tssize_t retval;\n\tint rc;\n\n\tgzip_hdr_len = module_gzip_header_len(buf, size);\n\tif (!gzip_hdr_len) {\n\t\tpr_err(\"not a gzip compressed module\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\ts.next_in = buf + gzip_hdr_len;\n\ts.avail_in = size - gzip_hdr_len;\n\n\ts.workspace = kmalloc(zlib_inflate_workspacesize(), GFP_KERNEL);\n\tif (!s.workspace)\n\t\treturn -ENOMEM;\n\n\trc = zlib_inflateInit2(&s, -MAX_WBITS);\n\tif (rc != Z_OK) {\n\t\tpr_err(\"failed to initialize decompressor: %d\\n\", rc);\n\t\tretval = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tdo {\n\t\tstruct page *page = module_get_next_page(info);\n\n\t\tif (IS_ERR(page)) {\n\t\t\tretval = PTR_ERR(page);\n\t\t\tgoto out_inflate_end;\n\t\t}\n\n\t\ts.next_out = kmap_local_page(page);\n\t\ts.avail_out = PAGE_SIZE;\n\t\trc = zlib_inflate(&s, 0);\n\t\tkunmap_local(s.next_out);\n\n\t\tnew_size += PAGE_SIZE - s.avail_out;\n\t} while (rc == Z_OK);\n\n\tif (rc != Z_STREAM_END) {\n\t\tpr_err(\"decompression failed with status %d\\n\", rc);\n\t\tretval = -EINVAL;\n\t\tgoto out_inflate_end;\n\t}\n\n\tretval = new_size;\n\nout_inflate_end:\n\tzlib_inflateEnd(&s);\nout:\n\tkfree(s.workspace);\n\treturn retval;\n}",
                        "cve_id": "CVE-2023-22997"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 4138,
            "cve_id": "CVE-2023-28327",
            "code_snippet": "static int unix_diag_get_exact(struct sk_buff *in_skb,\n\t\t\t       const struct nlmsghdr *nlh,\n\t\t\t       struct unix_diag_req *req)\n{\n\tstruct net *net = sock_net(in_skb->sk);\n\tunsigned int extra_len;\n\tstruct sk_buff *rep;\n\tstruct sock *sk;\n\tint err;\n\n\terr = -EINVAL;\n\tif (req->udiag_ino == 0)\n\t\tgoto out_nosk;\n\n\tsk = unix_lookup_by_ino(net, req->udiag_ino);\n\terr = -ENOENT;\n\tif (sk == NULL)\n\t\tgoto out_nosk;\n\n\terr = sock_diag_check_cookie(sk, req->udiag_cookie);\n\tif (err)\n\t\tgoto out;\n\n\textra_len = 256;\nagain:\n\terr = -ENOMEM;\n\trep = nlmsg_new(sizeof(struct unix_diag_msg) + extra_len, GFP_KERNEL);\n\tif (!rep)\n\t\tgoto out;\n\n\terr = sk_diag_fill(sk, rep, req, NETLINK_CB(in_skb).portid,\n\t\t\t   nlh->nlmsg_seq, 0, req->udiag_ino);\n\tif (err < 0) {\n\t\tnlmsg_free(rep);\n\t\textra_len += 256;\n\t\tif (extra_len >= PAGE_SIZE)\n\t\t\tgoto out;\n\n\t\tgoto again;\n\t}\n\terr = nlmsg_unicast(net->diag_nlsk, rep, NETLINK_CB(in_skb).portid);\n\nout:\n\tif (sk)\n\t\tsock_put(sk);\nout_nosk:\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int inet_rtm_getroute(struct sk_buff *in_skb, struct nlmsghdr *nlh,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(in_skb->sk);\n\tstruct rtmsg *rtm;\n\tstruct nlattr *tb[RTA_MAX+1];\n\tstruct fib_result res = {};\n\tstruct rtable *rt = NULL;\n\tstruct flowi4 fl4;\n\t__be32 dst = 0;\n\t__be32 src = 0;\n\tu32 iif;\n\tint err;\n\tint mark;\n\tstruct sk_buff *skb;\n\tu32 table_id = RT_TABLE_MAIN;\n\tkuid_t uid;\n\n\terr = nlmsg_parse(nlh, sizeof(*rtm), tb, RTA_MAX, rtm_ipv4_policy,\n\t\t\t  extack);\n\tif (err < 0)\n\t\tgoto errout;\n\n\trtm = nlmsg_data(nlh);\n\n\tskb = alloc_skb(NLMSG_GOODSIZE, GFP_KERNEL);\n\tif (!skb) {\n\t\terr = -ENOBUFS;\n\t\tgoto errout;\n\t}\n\n\t/* Reserve room for dummy headers, this skb can pass\n\t   through good chunk of routing engine.\n\t */\n\tskb_reset_mac_header(skb);\n\tskb_reset_network_header(skb);\n\n\tsrc = tb[RTA_SRC] ? nla_get_in_addr(tb[RTA_SRC]) : 0;\n\tdst = tb[RTA_DST] ? nla_get_in_addr(tb[RTA_DST]) : 0;\n\tiif = tb[RTA_IIF] ? nla_get_u32(tb[RTA_IIF]) : 0;\n\tmark = tb[RTA_MARK] ? nla_get_u32(tb[RTA_MARK]) : 0;\n\tif (tb[RTA_UID])\n\t\tuid = make_kuid(current_user_ns(), nla_get_u32(tb[RTA_UID]));\n\telse\n\t\tuid = (iif ? INVALID_UID : current_uid());\n\n\t/* Bugfix: need to give ip_route_input enough of an IP header to\n\t * not gag.\n\t */\n\tip_hdr(skb)->protocol = IPPROTO_UDP;\n\tip_hdr(skb)->saddr = src;\n\tip_hdr(skb)->daddr = dst;\n\n\tskb_reserve(skb, MAX_HEADER + sizeof(struct iphdr));\n\n\tmemset(&fl4, 0, sizeof(fl4));\n\tfl4.daddr = dst;\n\tfl4.saddr = src;\n\tfl4.flowi4_tos = rtm->rtm_tos;\n\tfl4.flowi4_oif = tb[RTA_OIF] ? nla_get_u32(tb[RTA_OIF]) : 0;\n\tfl4.flowi4_mark = mark;\n\tfl4.flowi4_uid = uid;\n\n\trcu_read_lock();\n\n\tif (iif) {\n\t\tstruct net_device *dev;\n\n\t\tdev = dev_get_by_index_rcu(net, iif);\n\t\tif (!dev) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto errout_free;\n\t\t}\n\n\t\tskb->protocol\t= htons(ETH_P_IP);\n\t\tskb->dev\t= dev;\n\t\tskb->mark\t= mark;\n\t\terr = ip_route_input_rcu(skb, dst, src, rtm->rtm_tos,\n\t\t\t\t\t dev, &res);\n\n\t\trt = skb_rtable(skb);\n\t\tif (err == 0 && rt->dst.error)\n\t\t\terr = -rt->dst.error;\n\t} else {\n\t\trt = ip_route_output_key_hash_rcu(net, &fl4, &res, skb);\n\t\terr = 0;\n\t\tif (IS_ERR(rt))\n\t\t\terr = PTR_ERR(rt);\n\t\telse\n\t\t\tskb_dst_set(skb, &rt->dst);\n\t}\n\n\tif (err)\n\t\tgoto errout_free;\n\n\tif (rtm->rtm_flags & RTM_F_NOTIFY)\n\t\trt->rt_flags |= RTCF_NOTIFY;\n\n\tif (rtm->rtm_flags & RTM_F_LOOKUP_TABLE)\n\t\ttable_id = rt->rt_table_id;\n\n\tif (rtm->rtm_flags & RTM_F_FIB_MATCH)\n\t\terr = fib_dump_info(skb, NETLINK_CB(in_skb).portid,\n\t\t\t\t    nlh->nlmsg_seq, RTM_NEWROUTE, table_id,\n\t\t\t\t    rt->rt_type, res.prefix, res.prefixlen,\n\t\t\t\t    fl4.flowi4_tos, res.fi, 0);\n\telse\n\t\terr = rt_fill_info(net, dst, src, table_id, &fl4, skb,\n\t\t\t\t   NETLINK_CB(in_skb).portid, nlh->nlmsg_seq);\n\tif (err < 0)\n\t\tgoto errout_free;\n\n\trcu_read_unlock();\n\n\terr = rtnl_unicast(skb, net, NETLINK_CB(in_skb).portid);\nerrout:\n\treturn err;\n\nerrout_free:\n\trcu_read_unlock();\n\tkfree_skb(skb);\n\tgoto errout;\n}",
                        "code_after_change": "static int rtnl_getlink(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct net *tgt_net = net;\n\tstruct ifinfomsg *ifm;\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct net_device *dev = NULL;\n\tstruct sk_buff *nskb;\n\tint netnsid = -1;\n\tint err;\n\tu32 ext_filter_mask = 0;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IF_NETNSID]) {\n\t\tnetnsid = nla_get_s32(tb[IFLA_IF_NETNSID]);\n\t\ttgt_net = get_target_net(NETLINK_CB(skb).sk, netnsid);\n\t\tif (IS_ERR(tgt_net))\n\t\t\treturn PTR_ERR(tgt_net);\n\t}\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\n\tif (tb[IFLA_EXT_MASK])\n\t\text_filter_mask = nla_get_u32(tb[IFLA_EXT_MASK]);\n\n\terr = -EINVAL;\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(tgt_net, ifm->ifi_index);\n\telse if (tb[IFLA_IFNAME])\n\t\tdev = __dev_get_by_name(tgt_net, ifname);\n\telse\n\t\tgoto out;\n\n\terr = -ENODEV;\n\tif (dev == NULL)\n\t\tgoto out;\n\n\terr = -ENOBUFS;\n\tnskb = nlmsg_new(if_nlmsg_size(dev, ext_filter_mask), GFP_KERNEL);\n\tif (nskb == NULL)\n\t\tgoto out;\n\n\terr = rtnl_fill_ifinfo(nskb, dev, net,\n\t\t\t       RTM_NEWLINK, NETLINK_CB(skb).portid,\n\t\t\t       nlh->nlmsg_seq, 0, 0, ext_filter_mask,\n\t\t\t       0, NULL, netnsid);\n\tif (err < 0) {\n\t\t/* -EMSGSIZE implies BUG in if_nlmsg_size */\n\t\tWARN_ON(err == -EMSGSIZE);\n\t\tkfree_skb(nskb);\n\t} else\n\t\terr = rtnl_unicast(nskb, net, NETLINK_CB(skb).portid);\nout:\n\tif (netnsid >= 0)\n\t\tput_net(tgt_net);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2017-13686"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int rtnl_getlink(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct net *tgt_net = net;\n\tstruct ifinfomsg *ifm;\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct net_device *dev = NULL;\n\tstruct sk_buff *nskb;\n\tint netnsid = -1;\n\tint err;\n\tu32 ext_filter_mask = 0;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IF_NETNSID]) {\n\t\tnetnsid = nla_get_s32(tb[IFLA_IF_NETNSID]);\n\t\ttgt_net = get_target_net(skb, netnsid);\n\t\tif (IS_ERR(tgt_net))\n\t\t\treturn PTR_ERR(tgt_net);\n\t}\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\n\tif (tb[IFLA_EXT_MASK])\n\t\text_filter_mask = nla_get_u32(tb[IFLA_EXT_MASK]);\n\n\terr = -EINVAL;\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(tgt_net, ifm->ifi_index);\n\telse if (tb[IFLA_IFNAME])\n\t\tdev = __dev_get_by_name(tgt_net, ifname);\n\telse\n\t\tgoto out;\n\n\terr = -ENODEV;\n\tif (dev == NULL)\n\t\tgoto out;\n\n\terr = -ENOBUFS;\n\tnskb = nlmsg_new(if_nlmsg_size(dev, ext_filter_mask), GFP_KERNEL);\n\tif (nskb == NULL)\n\t\tgoto out;\n\n\terr = rtnl_fill_ifinfo(nskb, dev, net,\n\t\t\t       RTM_NEWLINK, NETLINK_CB(skb).portid,\n\t\t\t       nlh->nlmsg_seq, 0, 0, ext_filter_mask,\n\t\t\t       0, NULL, netnsid);\n\tif (err < 0) {\n\t\t/* -EMSGSIZE implies BUG in if_nlmsg_size */\n\t\tWARN_ON(err == -EMSGSIZE);\n\t\tkfree_skb(nskb);\n\t} else\n\t\terr = rtnl_unicast(nskb, net, NETLINK_CB(skb).portid);\nout:\n\tif (netnsid >= 0)\n\t\tput_net(tgt_net);\n\n\treturn err;\n}",
                        "code_after_change": "static int inet_rtm_getroute(struct sk_buff *in_skb, struct nlmsghdr *nlh,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(in_skb->sk);\n\tstruct rtmsg *rtm;\n\tstruct nlattr *tb[RTA_MAX+1];\n\tstruct fib_result res = {};\n\tstruct rtable *rt = NULL;\n\tstruct flowi4 fl4;\n\t__be32 dst = 0;\n\t__be32 src = 0;\n\tu32 iif;\n\tint err;\n\tint mark;\n\tstruct sk_buff *skb;\n\tu32 table_id = RT_TABLE_MAIN;\n\tkuid_t uid;\n\n\terr = nlmsg_parse(nlh, sizeof(*rtm), tb, RTA_MAX, rtm_ipv4_policy,\n\t\t\t  extack);\n\tif (err < 0)\n\t\tgoto errout;\n\n\trtm = nlmsg_data(nlh);\n\n\tskb = alloc_skb(NLMSG_GOODSIZE, GFP_KERNEL);\n\tif (!skb) {\n\t\terr = -ENOBUFS;\n\t\tgoto errout;\n\t}\n\n\t/* Reserve room for dummy headers, this skb can pass\n\t   through good chunk of routing engine.\n\t */\n\tskb_reset_mac_header(skb);\n\tskb_reset_network_header(skb);\n\n\tsrc = tb[RTA_SRC] ? nla_get_in_addr(tb[RTA_SRC]) : 0;\n\tdst = tb[RTA_DST] ? nla_get_in_addr(tb[RTA_DST]) : 0;\n\tiif = tb[RTA_IIF] ? nla_get_u32(tb[RTA_IIF]) : 0;\n\tmark = tb[RTA_MARK] ? nla_get_u32(tb[RTA_MARK]) : 0;\n\tif (tb[RTA_UID])\n\t\tuid = make_kuid(current_user_ns(), nla_get_u32(tb[RTA_UID]));\n\telse\n\t\tuid = (iif ? INVALID_UID : current_uid());\n\n\t/* Bugfix: need to give ip_route_input enough of an IP header to\n\t * not gag.\n\t */\n\tip_hdr(skb)->protocol = IPPROTO_UDP;\n\tip_hdr(skb)->saddr = src;\n\tip_hdr(skb)->daddr = dst;\n\n\tskb_reserve(skb, MAX_HEADER + sizeof(struct iphdr));\n\n\tmemset(&fl4, 0, sizeof(fl4));\n\tfl4.daddr = dst;\n\tfl4.saddr = src;\n\tfl4.flowi4_tos = rtm->rtm_tos;\n\tfl4.flowi4_oif = tb[RTA_OIF] ? nla_get_u32(tb[RTA_OIF]) : 0;\n\tfl4.flowi4_mark = mark;\n\tfl4.flowi4_uid = uid;\n\n\trcu_read_lock();\n\n\tif (iif) {\n\t\tstruct net_device *dev;\n\n\t\tdev = dev_get_by_index_rcu(net, iif);\n\t\tif (!dev) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto errout_free;\n\t\t}\n\n\t\tskb->protocol\t= htons(ETH_P_IP);\n\t\tskb->dev\t= dev;\n\t\tskb->mark\t= mark;\n\t\terr = ip_route_input_rcu(skb, dst, src, rtm->rtm_tos,\n\t\t\t\t\t dev, &res);\n\n\t\trt = skb_rtable(skb);\n\t\tif (err == 0 && rt->dst.error)\n\t\t\terr = -rt->dst.error;\n\t} else {\n\t\trt = ip_route_output_key_hash_rcu(net, &fl4, &res, skb);\n\t\terr = 0;\n\t\tif (IS_ERR(rt))\n\t\t\terr = PTR_ERR(rt);\n\t\telse\n\t\t\tskb_dst_set(skb, &rt->dst);\n\t}\n\n\tif (err)\n\t\tgoto errout_free;\n\n\tif (rtm->rtm_flags & RTM_F_NOTIFY)\n\t\trt->rt_flags |= RTCF_NOTIFY;\n\n\tif (rtm->rtm_flags & RTM_F_LOOKUP_TABLE)\n\t\ttable_id = rt->rt_table_id;\n\n\tif (rtm->rtm_flags & RTM_F_FIB_MATCH) {\n\t\tif (!res.fi) {\n\t\t\terr = fib_props[res.type].error;\n\t\t\tif (!err)\n\t\t\t\terr = -EHOSTUNREACH;\n\t\t\tgoto errout_free;\n\t\t}\n\t\terr = fib_dump_info(skb, NETLINK_CB(in_skb).portid,\n\t\t\t\t    nlh->nlmsg_seq, RTM_NEWROUTE, table_id,\n\t\t\t\t    rt->rt_type, res.prefix, res.prefixlen,\n\t\t\t\t    fl4.flowi4_tos, res.fi, 0);\n\t} else {\n\t\terr = rt_fill_info(net, dst, src, table_id, &fl4, skb,\n\t\t\t\t   NETLINK_CB(in_skb).portid, nlh->nlmsg_seq);\n\t}\n\tif (err < 0)\n\t\tgoto errout_free;\n\n\trcu_read_unlock();\n\n\terr = rtnl_unicast(skb, net, NETLINK_CB(in_skb).portid);\nerrout:\n\treturn err;\n\nerrout_free:\n\trcu_read_unlock();\n\tkfree_skb(skb);\n\tgoto errout;\n}",
                        "cve_id": "CVE-2018-14646"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int unix_diag_dump(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tint num, s_num, slot, s_slot;\n\tstruct unix_diag_req *req;\n\n\treq = nlmsg_data(cb->nlh);\n\n\ts_slot = cb->args[0];\n\tnum = s_num = cb->args[1];\n\n\tfor (slot = s_slot; slot < UNIX_HASH_SIZE; s_num = 0, slot++) {\n\t\tstruct sock *sk;\n\n\t\tnum = 0;\n\t\tspin_lock(&net->unx.table.locks[slot]);\n\t\tsk_for_each(sk, &net->unx.table.buckets[slot]) {\n\t\t\tif (num < s_num)\n\t\t\t\tgoto next;\n\t\t\tif (!(req->udiag_states & (1 << sk->sk_state)))\n\t\t\t\tgoto next;\n\t\t\tif (sk_diag_dump(sk, skb, req,\n\t\t\t\t\t NETLINK_CB(cb->skb).portid,\n\t\t\t\t\t cb->nlh->nlmsg_seq,\n\t\t\t\t\t NLM_F_MULTI) < 0) {\n\t\t\t\tspin_unlock(&net->unx.table.locks[slot]);\n\t\t\t\tgoto done;\n\t\t\t}\nnext:\n\t\t\tnum++;\n\t\t}\n\t\tspin_unlock(&net->unx.table.locks[slot]);\n\t}\ndone:\n\tcb->args[0] = slot;\n\tcb->args[1] = num;\n\n\treturn skb->len;\n}",
                        "code_after_change": "static int unix_diag_dump(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tint num, s_num, slot, s_slot;\n\tstruct unix_diag_req *req;\n\n\treq = nlmsg_data(cb->nlh);\n\n\ts_slot = cb->args[0];\n\tnum = s_num = cb->args[1];\n\n\tfor (slot = s_slot; slot < UNIX_HASH_SIZE; s_num = 0, slot++) {\n\t\tstruct sock *sk;\n\n\t\tnum = 0;\n\t\tspin_lock(&net->unx.table.locks[slot]);\n\t\tsk_for_each(sk, &net->unx.table.buckets[slot]) {\n\t\t\tif (num < s_num)\n\t\t\t\tgoto next;\n\t\t\tif (!(req->udiag_states & (1 << sk->sk_state)))\n\t\t\t\tgoto next;\n\t\t\tif (sk_diag_dump(sk, skb, req, sk_user_ns(skb->sk),\n\t\t\t\t\t NETLINK_CB(cb->skb).portid,\n\t\t\t\t\t cb->nlh->nlmsg_seq,\n\t\t\t\t\t NLM_F_MULTI) < 0) {\n\t\t\t\tspin_unlock(&net->unx.table.locks[slot]);\n\t\t\t\tgoto done;\n\t\t\t}\nnext:\n\t\t\tnum++;\n\t\t}\n\t\tspin_unlock(&net->unx.table.locks[slot]);\n\t}\ndone:\n\tcb->args[0] = slot;\n\tcb->args[1] = num;\n\n\treturn skb->len;\n}",
                        "cve_id": "CVE-2023-28327"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 4144,
            "cve_id": "CVE-2023-28466",
            "code_snippet": "static int do_tls_getsockopt(struct sock *sk, int optname,\n\t\t\t     char __user *optval, int __user *optlen)\n{\n\tint rc = 0;\n\n\tswitch (optname) {\n\tcase TLS_TX:\n\tcase TLS_RX:\n\t\trc = do_tls_getsockopt_conf(sk, optval, optlen,\n\t\t\t\t\t    optname == TLS_TX);\n\t\tbreak;\n\tcase TLS_TX_ZEROCOPY_RO:\n\t\trc = do_tls_getsockopt_tx_zc(sk, optval, optlen);\n\t\tbreak;\n\tcase TLS_RX_EXPECT_NO_PAD:\n\t\trc = do_tls_getsockopt_no_pad(sk, optval, optlen);\n\t\tbreak;\n\tdefault:\n\t\trc = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\treturn rc;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int sco_sock_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t       char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tint len, err = 0;\n\tstruct bt_voice voice;\n\tu32 phys;\n\tint pkt_status;\n\n\tBT_DBG(\"sk %p\", sk);\n\n\tif (level == SOL_SCO)\n\t\treturn sco_sock_getsockopt_old(sock, optname, optval, optlen);\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\n\tcase BT_DEFER_SETUP:\n\t\tif (sk->sk_state != BT_BOUND && sk->sk_state != BT_LISTEN) {\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (put_user(test_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags),\n\t\t\t     (u32 __user *)optval))\n\t\t\terr = -EFAULT;\n\n\t\tbreak;\n\n\tcase BT_VOICE:\n\t\tvoice.setting = sco_pi(sk)->setting;\n\n\t\tlen = min_t(unsigned int, len, sizeof(voice));\n\t\tif (copy_to_user(optval, (char *)&voice, len))\n\t\t\terr = -EFAULT;\n\n\t\tbreak;\n\n\tcase BT_PHY:\n\t\tif (sk->sk_state != BT_CONNECTED) {\n\t\t\terr = -ENOTCONN;\n\t\t\tbreak;\n\t\t}\n\n\t\tphys = hci_conn_get_phy(sco_pi(sk)->conn->hcon);\n\n\t\tif (put_user(phys, (u32 __user *) optval))\n\t\t\terr = -EFAULT;\n\t\tbreak;\n\n\tcase BT_PKT_STATUS:\n\t\tpkt_status = (sco_pi(sk)->cmsg_mask & SCO_CMSG_PKT_STATUS);\n\n\t\tif (put_user(pkt_status, (int __user *)optval))\n\t\t\terr = -EFAULT;\n\t\tbreak;\n\n\tcase BT_SNDMTU:\n\tcase BT_RCVMTU:\n\t\tif (put_user(sco_pi(sk)->conn->mtu, (u32 __user *)optval))\n\t\t\terr = -EFAULT;\n\t\tbreak;\n\n\tdefault:\n\t\terr = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\treturn err;\n}",
                        "code_after_change": "static int sco_sock_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t       char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tint len, err = 0;\n\tstruct bt_voice voice;\n\tu32 phys;\n\tint pkt_status;\n\n\tBT_DBG(\"sk %p\", sk);\n\n\tif (level == SOL_SCO)\n\t\treturn sco_sock_getsockopt_old(sock, optname, optval, optlen);\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\n\tcase BT_DEFER_SETUP:\n\t\tif (sk->sk_state != BT_BOUND && sk->sk_state != BT_LISTEN) {\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (put_user(test_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags),\n\t\t\t     (u32 __user *)optval))\n\t\t\terr = -EFAULT;\n\n\t\tbreak;\n\n\tcase BT_VOICE:\n\t\tvoice.setting = sco_pi(sk)->setting;\n\n\t\tlen = min_t(unsigned int, len, sizeof(voice));\n\t\tif (copy_to_user(optval, (char *)&voice, len))\n\t\t\terr = -EFAULT;\n\n\t\tbreak;\n\n\tcase BT_PHY:\n\t\tif (sk->sk_state != BT_CONNECTED) {\n\t\t\terr = -ENOTCONN;\n\t\t\tbreak;\n\t\t}\n\n\t\tphys = hci_conn_get_phy(sco_pi(sk)->conn->hcon);\n\n\t\tif (put_user(phys, (u32 __user *) optval))\n\t\t\terr = -EFAULT;\n\t\tbreak;\n\n\tcase BT_PKT_STATUS:\n\t\tpkt_status = (sco_pi(sk)->cmsg_mask & SCO_CMSG_PKT_STATUS);\n\n\t\tif (put_user(pkt_status, (int __user *)optval))\n\t\t\terr = -EFAULT;\n\t\tbreak;\n\n\tcase BT_SNDMTU:\n\tcase BT_RCVMTU:\n\t\tif (sk->sk_state != BT_CONNECTED) {\n\t\t\terr = -ENOTCONN;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (put_user(sco_pi(sk)->conn->mtu, (u32 __user *)optval))\n\t\t\terr = -EFAULT;\n\t\tbreak;\n\n\tdefault:\n\t\terr = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\treturn err;\n}",
                        "cve_id": "CVE-2020-35499"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 4155,
            "cve_id": "CVE-2023-3106",
            "code_snippet": "static int xfrm_dump_sa(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct xfrm_state_walk *walk = (struct xfrm_state_walk *) &cb->args[1];\n\tstruct xfrm_dump_info info;\n\n\tBUILD_BUG_ON(sizeof(struct xfrm_state_walk) >\n\t\t     sizeof(cb->args) - sizeof(cb->args[0]));\n\n\tinfo.in_skb = cb->skb;\n\tinfo.out_skb = skb;\n\tinfo.nlmsg_seq = cb->nlh->nlmsg_seq;\n\tinfo.nlmsg_flags = NLM_F_MULTI;\n\n\tif (!cb->args[0]) {\n\t\tstruct nlattr *attrs[XFRMA_MAX+1];\n\t\tstruct xfrm_address_filter *filter = NULL;\n\t\tu8 proto = 0;\n\t\tint err;\n\n\t\tcb->args[0] = 1;\n\n\t\terr = nlmsg_parse(cb->nlh, 0, attrs, XFRMA_MAX,\n\t\t\t\t  xfrma_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\tif (attrs[XFRMA_ADDRESS_FILTER]) {\n\t\t\tfilter = kmemdup(nla_data(attrs[XFRMA_ADDRESS_FILTER]),\n\t\t\t\t\t sizeof(*filter), GFP_KERNEL);\n\t\t\tif (filter == NULL)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tif (attrs[XFRMA_PROTO])\n\t\t\tproto = nla_get_u8(attrs[XFRMA_PROTO]);\n\n\t\txfrm_state_walk_init(walk, proto, filter);\n\t}\n\n\t(void) xfrm_state_walk(net, walk, dump_one_state, &info);\n\n\treturn skb->len;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int xfrm_dump_sa_done(struct netlink_callback *cb)\n{\n\tstruct xfrm_state_walk *walk = (struct xfrm_state_walk *) &cb->args[1];\n\tstruct sock *sk = cb->skb->sk;\n\tstruct net *net = sock_net(sk);\n\n\txfrm_state_walk_done(walk, net);\n\treturn 0;\n}",
                        "code_after_change": "static int xfrm_dump_sa_done(struct netlink_callback *cb)\n{\n\tstruct xfrm_state_walk *walk = (struct xfrm_state_walk *) &cb->args[1];\n\tstruct sock *sk = cb->skb->sk;\n\tstruct net *net = sock_net(sk);\n\n\tif (cb->args[0])\n\t\txfrm_state_walk_done(walk, net);\n\treturn 0;\n}",
                        "cve_id": "CVE-2023-3106"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 4189,
            "cve_id": "CVE-2023-32252",
            "code_snippet": "static void ksmbd_conn_lock(struct ksmbd_conn *conn)\n{\n\tmutex_lock(&conn->srv_mutex);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static void ksmbd_conn_unlock(struct ksmbd_conn *conn)\n{\n\tmutex_unlock(&conn->srv_mutex);\n}",
                        "code_after_change": "void ksmbd_conn_unlock(struct ksmbd_conn *conn)\n{\n\tmutex_unlock(&conn->srv_mutex);\n}",
                        "cve_id": "CVE-2023-32252"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void destroy_previous_session(struct ksmbd_conn *conn,\n\t\t\t\t     struct ksmbd_user *user, u64 id)\n{\n\tstruct ksmbd_session *prev_sess = ksmbd_session_lookup_slowpath(id);\n\tstruct ksmbd_user *prev_user;\n\tstruct channel *chann;\n\tlong index;\n\n\tif (!prev_sess)\n\t\treturn;\n\n\tprev_user = prev_sess->user;\n\n\tif (!prev_user ||\n\t    strcmp(user->name, prev_user->name) ||\n\t    user->passkey_sz != prev_user->passkey_sz ||\n\t    memcmp(user->passkey, prev_user->passkey, user->passkey_sz))\n\t\treturn;\n\n\tprev_sess->state = SMB2_SESSION_EXPIRED;\n\txa_for_each(&prev_sess->ksmbd_chann_list, index, chann)\n\t\tchann->conn->status = KSMBD_SESS_EXITING;\n}",
                        "code_after_change": "static void destroy_previous_session(struct ksmbd_conn *conn,\n\t\t\t\t     struct ksmbd_user *user, u64 id)\n{\n\tstruct ksmbd_session *prev_sess = ksmbd_session_lookup_slowpath(id);\n\tstruct ksmbd_user *prev_user;\n\tstruct channel *chann;\n\tlong index;\n\n\tif (!prev_sess)\n\t\treturn;\n\n\tprev_user = prev_sess->user;\n\n\tif (!prev_user ||\n\t    strcmp(user->name, prev_user->name) ||\n\t    user->passkey_sz != prev_user->passkey_sz ||\n\t    memcmp(user->passkey, prev_user->passkey, user->passkey_sz))\n\t\treturn;\n\n\tprev_sess->state = SMB2_SESSION_EXPIRED;\n\txa_for_each(&prev_sess->ksmbd_chann_list, index, chann)\n\t\tksmbd_conn_set_exiting(chann->conn);\n}",
                        "cve_id": "CVE-2023-32252"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int init_smb2_neg_rsp(struct ksmbd_work *work)\n{\n\tstruct smb2_hdr *rsp_hdr;\n\tstruct smb2_negotiate_rsp *rsp;\n\tstruct ksmbd_conn *conn = work->conn;\n\n\t*(__be32 *)work->response_buf =\n\t\tcpu_to_be32(conn->vals->header_size);\n\n\trsp_hdr = smb2_get_msg(work->response_buf);\n\tmemset(rsp_hdr, 0, sizeof(struct smb2_hdr) + 2);\n\trsp_hdr->ProtocolId = SMB2_PROTO_NUMBER;\n\trsp_hdr->StructureSize = SMB2_HEADER_STRUCTURE_SIZE;\n\trsp_hdr->CreditRequest = cpu_to_le16(2);\n\trsp_hdr->Command = SMB2_NEGOTIATE;\n\trsp_hdr->Flags = (SMB2_FLAGS_SERVER_TO_REDIR);\n\trsp_hdr->NextCommand = 0;\n\trsp_hdr->MessageId = 0;\n\trsp_hdr->Id.SyncId.ProcessId = 0;\n\trsp_hdr->Id.SyncId.TreeId = 0;\n\trsp_hdr->SessionId = 0;\n\tmemset(rsp_hdr->Signature, 0, 16);\n\n\trsp = smb2_get_msg(work->response_buf);\n\n\tWARN_ON(ksmbd_conn_good(work));\n\n\trsp->StructureSize = cpu_to_le16(65);\n\tksmbd_debug(SMB, \"conn->dialect 0x%x\\n\", conn->dialect);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying connection\n\t */\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\t/* Default Max Message Size till SMB2.0, 64K*/\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\tle16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf,\n\t\t\tsizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tif (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY)\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\tconn->use_spnego = true;\n\n\tksmbd_conn_set_need_negotiate(work);\n\treturn 0;\n}",
                        "code_after_change": "int init_smb2_neg_rsp(struct ksmbd_work *work)\n{\n\tstruct smb2_hdr *rsp_hdr;\n\tstruct smb2_negotiate_rsp *rsp;\n\tstruct ksmbd_conn *conn = work->conn;\n\n\t*(__be32 *)work->response_buf =\n\t\tcpu_to_be32(conn->vals->header_size);\n\n\trsp_hdr = smb2_get_msg(work->response_buf);\n\tmemset(rsp_hdr, 0, sizeof(struct smb2_hdr) + 2);\n\trsp_hdr->ProtocolId = SMB2_PROTO_NUMBER;\n\trsp_hdr->StructureSize = SMB2_HEADER_STRUCTURE_SIZE;\n\trsp_hdr->CreditRequest = cpu_to_le16(2);\n\trsp_hdr->Command = SMB2_NEGOTIATE;\n\trsp_hdr->Flags = (SMB2_FLAGS_SERVER_TO_REDIR);\n\trsp_hdr->NextCommand = 0;\n\trsp_hdr->MessageId = 0;\n\trsp_hdr->Id.SyncId.ProcessId = 0;\n\trsp_hdr->Id.SyncId.TreeId = 0;\n\trsp_hdr->SessionId = 0;\n\tmemset(rsp_hdr->Signature, 0, 16);\n\n\trsp = smb2_get_msg(work->response_buf);\n\n\tWARN_ON(ksmbd_conn_good(conn));\n\n\trsp->StructureSize = cpu_to_le16(65);\n\tksmbd_debug(SMB, \"conn->dialect 0x%x\\n\", conn->dialect);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying connection\n\t */\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\t/* Default Max Message Size till SMB2.0, 64K*/\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\tle16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf,\n\t\t\tsizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tif (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY)\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\tconn->use_spnego = true;\n\n\tksmbd_conn_set_need_negotiate(conn);\n\treturn 0;\n}",
                        "cve_id": "CVE-2023-32252"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 4190,
            "cve_id": "CVE-2023-32252",
            "code_snippet": "bool ksmbd_conn_alive(struct ksmbd_conn *conn)\n{\n\tif (!ksmbd_server_running())\n\t\treturn false;\n\n\tif (conn->status == KSMBD_SESS_EXITING)\n\t\treturn false;\n\n\tif (kthread_should_stop())\n\t\treturn false;\n\n\tif (atomic_read(&conn->stats.open_files_count) > 0)\n\t\treturn true;\n\n\t/*\n\t * Stop current session if the time that get last request from client\n\t * is bigger than deadtime user configured and opening file count is\n\t * zero.\n\t */\n\tif (server_conf.deadtime > 0 &&\n\t    time_after(jiffies, conn->last_active + server_conf.deadtime)) {\n\t\tksmbd_debug(CONN, \"No response from client in %lu minutes\\n\",\n\t\t\t    server_conf.deadtime / SMB_ECHO_INTERVAL);\n\t\treturn false;\n\t}\n\treturn true;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int smb2_handle_negotiate(struct ksmbd_work *work)\n{\n\tstruct ksmbd_conn *conn = work->conn;\n\tstruct smb2_negotiate_req *req = smb2_get_msg(work->request_buf);\n\tstruct smb2_negotiate_rsp *rsp = smb2_get_msg(work->response_buf);\n\tint rc = 0;\n\tunsigned int smb2_buf_len, smb2_neg_size;\n\t__le32 status;\n\n\tksmbd_debug(SMB, \"Received negotiate request\\n\");\n\tconn->need_neg = false;\n\tif (ksmbd_conn_good(work)) {\n\t\tpr_err(\"conn->tcp_status is already in CifsGood State\\n\");\n\t\twork->send_no_response = 1;\n\t\treturn rc;\n\t}\n\n\tif (req->DialectCount == 0) {\n\t\tpr_err(\"malformed packet\\n\");\n\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tsmb2_buf_len = get_rfc1002_len(work->request_buf);\n\tsmb2_neg_size = offsetof(struct smb2_negotiate_req, Dialects);\n\tif (smb2_neg_size > smb2_buf_len) {\n\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tif (conn->dialect == SMB311_PROT_ID) {\n\t\tunsigned int nego_ctxt_off = le32_to_cpu(req->NegotiateContextOffset);\n\n\t\tif (smb2_buf_len < nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (smb2_neg_size > nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (smb2_neg_size + le16_to_cpu(req->DialectCount) * sizeof(__le16) >\n\t\t    nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t} else {\n\t\tif (smb2_neg_size + le16_to_cpu(req->DialectCount) * sizeof(__le16) >\n\t\t    smb2_buf_len) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\tconn->cli_cap = le32_to_cpu(req->Capabilities);\n\tswitch (conn->dialect) {\n\tcase SMB311_PROT_ID:\n\t\tconn->preauth_info =\n\t\t\tkzalloc(sizeof(struct preauth_integrity_info),\n\t\t\t\tGFP_KERNEL);\n\t\tif (!conn->preauth_info) {\n\t\t\trc = -ENOMEM;\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tstatus = deassemble_neg_contexts(conn, req,\n\t\t\t\t\t\t get_rfc1002_len(work->request_buf));\n\t\tif (status != STATUS_SUCCESS) {\n\t\t\tpr_err(\"deassemble_neg_contexts error(0x%x)\\n\",\n\t\t\t       status);\n\t\t\trsp->hdr.Status = status;\n\t\t\trc = -EINVAL;\n\t\t\tkfree(conn->preauth_info);\n\t\t\tconn->preauth_info = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\trc = init_smb3_11_server(conn);\n\t\tif (rc < 0) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\tkfree(conn->preauth_info);\n\t\t\tconn->preauth_info = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tksmbd_gen_preauth_integrity_hash(conn,\n\t\t\t\t\t\t work->request_buf,\n\t\t\t\t\t\t conn->preauth_info->Preauth_HashValue);\n\t\trsp->NegotiateContextOffset =\n\t\t\t\tcpu_to_le32(OFFSET_OF_NEG_CONTEXT);\n\t\tassemble_neg_contexts(conn, rsp, work->response_buf);\n\t\tbreak;\n\tcase SMB302_PROT_ID:\n\t\tinit_smb3_02_server(conn);\n\t\tbreak;\n\tcase SMB30_PROT_ID:\n\t\tinit_smb3_0_server(conn);\n\t\tbreak;\n\tcase SMB21_PROT_ID:\n\t\tinit_smb2_1_server(conn);\n\t\tbreak;\n\tcase SMB2X_PROT_ID:\n\tcase BAD_PROT_ID:\n\tdefault:\n\t\tksmbd_debug(SMB, \"Server dialect :0x%x not supported\\n\",\n\t\t\t    conn->dialect);\n\t\trsp->hdr.Status = STATUS_NOT_SUPPORTED;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\n\t/* For stats */\n\tconn->connection_type = conn->dialect;\n\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\tmemcpy(conn->ClientGUID, req->ClientGUID,\n\t\t\tSMB2_CLIENT_GUID_SIZE);\n\tconn->cli_sec_mode = le16_to_cpu(req->SecurityMode);\n\n\trsp->StructureSize = cpu_to_le16(65);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying server\n\t */\n\tmemset(rsp->ServerGUID, 0, SMB2_CLIENT_GUID_SIZE);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\tksmbd_debug(SMB, \"negotiate context offset %d, count %d\\n\",\n\t\t    le32_to_cpu(rsp->NegotiateContextOffset),\n\t\t    le16_to_cpu(rsp->NegotiateContextCount));\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\t\t\t  le16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf, sizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tconn->use_spnego = true;\n\n\tif ((server_conf.signing == KSMBD_CONFIG_OPT_AUTO ||\n\t     server_conf.signing == KSMBD_CONFIG_OPT_DISABLED) &&\n\t    req->SecurityMode & SMB2_NEGOTIATE_SIGNING_REQUIRED_LE)\n\t\tconn->sign = true;\n\telse if (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY) {\n\t\tserver_conf.enforced_signing = true;\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\t\tconn->sign = true;\n\t}\n\n\tconn->srv_sec_mode = le16_to_cpu(rsp->SecurityMode);\n\tksmbd_conn_set_need_negotiate(work);\n\nerr_out:\n\tif (rc < 0)\n\t\tsmb2_set_err_rsp(work);\n\n\treturn rc;\n}",
                        "code_after_change": "int smb2_handle_negotiate(struct ksmbd_work *work)\n{\n\tstruct ksmbd_conn *conn = work->conn;\n\tstruct smb2_negotiate_req *req = smb2_get_msg(work->request_buf);\n\tstruct smb2_negotiate_rsp *rsp = smb2_get_msg(work->response_buf);\n\tint rc = 0;\n\tunsigned int smb2_buf_len, smb2_neg_size;\n\t__le32 status;\n\n\tksmbd_debug(SMB, \"Received negotiate request\\n\");\n\tconn->need_neg = false;\n\tif (ksmbd_conn_good(conn)) {\n\t\tpr_err(\"conn->tcp_status is already in CifsGood State\\n\");\n\t\twork->send_no_response = 1;\n\t\treturn rc;\n\t}\n\n\tif (req->DialectCount == 0) {\n\t\tpr_err(\"malformed packet\\n\");\n\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tsmb2_buf_len = get_rfc1002_len(work->request_buf);\n\tsmb2_neg_size = offsetof(struct smb2_negotiate_req, Dialects);\n\tif (smb2_neg_size > smb2_buf_len) {\n\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tif (conn->dialect == SMB311_PROT_ID) {\n\t\tunsigned int nego_ctxt_off = le32_to_cpu(req->NegotiateContextOffset);\n\n\t\tif (smb2_buf_len < nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (smb2_neg_size > nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (smb2_neg_size + le16_to_cpu(req->DialectCount) * sizeof(__le16) >\n\t\t    nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t} else {\n\t\tif (smb2_neg_size + le16_to_cpu(req->DialectCount) * sizeof(__le16) >\n\t\t    smb2_buf_len) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\tconn->cli_cap = le32_to_cpu(req->Capabilities);\n\tswitch (conn->dialect) {\n\tcase SMB311_PROT_ID:\n\t\tconn->preauth_info =\n\t\t\tkzalloc(sizeof(struct preauth_integrity_info),\n\t\t\t\tGFP_KERNEL);\n\t\tif (!conn->preauth_info) {\n\t\t\trc = -ENOMEM;\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tstatus = deassemble_neg_contexts(conn, req,\n\t\t\t\t\t\t get_rfc1002_len(work->request_buf));\n\t\tif (status != STATUS_SUCCESS) {\n\t\t\tpr_err(\"deassemble_neg_contexts error(0x%x)\\n\",\n\t\t\t       status);\n\t\t\trsp->hdr.Status = status;\n\t\t\trc = -EINVAL;\n\t\t\tkfree(conn->preauth_info);\n\t\t\tconn->preauth_info = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\trc = init_smb3_11_server(conn);\n\t\tif (rc < 0) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\tkfree(conn->preauth_info);\n\t\t\tconn->preauth_info = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tksmbd_gen_preauth_integrity_hash(conn,\n\t\t\t\t\t\t work->request_buf,\n\t\t\t\t\t\t conn->preauth_info->Preauth_HashValue);\n\t\trsp->NegotiateContextOffset =\n\t\t\t\tcpu_to_le32(OFFSET_OF_NEG_CONTEXT);\n\t\tassemble_neg_contexts(conn, rsp, work->response_buf);\n\t\tbreak;\n\tcase SMB302_PROT_ID:\n\t\tinit_smb3_02_server(conn);\n\t\tbreak;\n\tcase SMB30_PROT_ID:\n\t\tinit_smb3_0_server(conn);\n\t\tbreak;\n\tcase SMB21_PROT_ID:\n\t\tinit_smb2_1_server(conn);\n\t\tbreak;\n\tcase SMB2X_PROT_ID:\n\tcase BAD_PROT_ID:\n\tdefault:\n\t\tksmbd_debug(SMB, \"Server dialect :0x%x not supported\\n\",\n\t\t\t    conn->dialect);\n\t\trsp->hdr.Status = STATUS_NOT_SUPPORTED;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\n\t/* For stats */\n\tconn->connection_type = conn->dialect;\n\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\tmemcpy(conn->ClientGUID, req->ClientGUID,\n\t\t\tSMB2_CLIENT_GUID_SIZE);\n\tconn->cli_sec_mode = le16_to_cpu(req->SecurityMode);\n\n\trsp->StructureSize = cpu_to_le16(65);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying server\n\t */\n\tmemset(rsp->ServerGUID, 0, SMB2_CLIENT_GUID_SIZE);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\tksmbd_debug(SMB, \"negotiate context offset %d, count %d\\n\",\n\t\t    le32_to_cpu(rsp->NegotiateContextOffset),\n\t\t    le16_to_cpu(rsp->NegotiateContextCount));\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\t\t\t  le16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf, sizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tconn->use_spnego = true;\n\n\tif ((server_conf.signing == KSMBD_CONFIG_OPT_AUTO ||\n\t     server_conf.signing == KSMBD_CONFIG_OPT_DISABLED) &&\n\t    req->SecurityMode & SMB2_NEGOTIATE_SIGNING_REQUIRED_LE)\n\t\tconn->sign = true;\n\telse if (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY) {\n\t\tserver_conf.enforced_signing = true;\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\t\tconn->sign = true;\n\t}\n\n\tconn->srv_sec_mode = le16_to_cpu(rsp->SecurityMode);\n\tksmbd_conn_set_need_negotiate(conn);\n\nerr_out:\n\tif (rc < 0)\n\t\tsmb2_set_err_rsp(work);\n\n\treturn rc;\n}",
                        "cve_id": "CVE-2023-32252"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int init_smb2_neg_rsp(struct ksmbd_work *work)\n{\n\tstruct smb2_hdr *rsp_hdr;\n\tstruct smb2_negotiate_rsp *rsp;\n\tstruct ksmbd_conn *conn = work->conn;\n\n\t*(__be32 *)work->response_buf =\n\t\tcpu_to_be32(conn->vals->header_size);\n\n\trsp_hdr = smb2_get_msg(work->response_buf);\n\tmemset(rsp_hdr, 0, sizeof(struct smb2_hdr) + 2);\n\trsp_hdr->ProtocolId = SMB2_PROTO_NUMBER;\n\trsp_hdr->StructureSize = SMB2_HEADER_STRUCTURE_SIZE;\n\trsp_hdr->CreditRequest = cpu_to_le16(2);\n\trsp_hdr->Command = SMB2_NEGOTIATE;\n\trsp_hdr->Flags = (SMB2_FLAGS_SERVER_TO_REDIR);\n\trsp_hdr->NextCommand = 0;\n\trsp_hdr->MessageId = 0;\n\trsp_hdr->Id.SyncId.ProcessId = 0;\n\trsp_hdr->Id.SyncId.TreeId = 0;\n\trsp_hdr->SessionId = 0;\n\tmemset(rsp_hdr->Signature, 0, 16);\n\n\trsp = smb2_get_msg(work->response_buf);\n\n\tWARN_ON(ksmbd_conn_good(work));\n\n\trsp->StructureSize = cpu_to_le16(65);\n\tksmbd_debug(SMB, \"conn->dialect 0x%x\\n\", conn->dialect);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying connection\n\t */\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\t/* Default Max Message Size till SMB2.0, 64K*/\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\tle16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf,\n\t\t\tsizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tif (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY)\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\tconn->use_spnego = true;\n\n\tksmbd_conn_set_need_negotiate(work);\n\treturn 0;\n}",
                        "code_after_change": "int init_smb2_neg_rsp(struct ksmbd_work *work)\n{\n\tstruct smb2_hdr *rsp_hdr;\n\tstruct smb2_negotiate_rsp *rsp;\n\tstruct ksmbd_conn *conn = work->conn;\n\n\t*(__be32 *)work->response_buf =\n\t\tcpu_to_be32(conn->vals->header_size);\n\n\trsp_hdr = smb2_get_msg(work->response_buf);\n\tmemset(rsp_hdr, 0, sizeof(struct smb2_hdr) + 2);\n\trsp_hdr->ProtocolId = SMB2_PROTO_NUMBER;\n\trsp_hdr->StructureSize = SMB2_HEADER_STRUCTURE_SIZE;\n\trsp_hdr->CreditRequest = cpu_to_le16(2);\n\trsp_hdr->Command = SMB2_NEGOTIATE;\n\trsp_hdr->Flags = (SMB2_FLAGS_SERVER_TO_REDIR);\n\trsp_hdr->NextCommand = 0;\n\trsp_hdr->MessageId = 0;\n\trsp_hdr->Id.SyncId.ProcessId = 0;\n\trsp_hdr->Id.SyncId.TreeId = 0;\n\trsp_hdr->SessionId = 0;\n\tmemset(rsp_hdr->Signature, 0, 16);\n\n\trsp = smb2_get_msg(work->response_buf);\n\n\tWARN_ON(ksmbd_conn_good(conn));\n\n\trsp->StructureSize = cpu_to_le16(65);\n\tksmbd_debug(SMB, \"conn->dialect 0x%x\\n\", conn->dialect);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying connection\n\t */\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\t/* Default Max Message Size till SMB2.0, 64K*/\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\tle16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf,\n\t\t\tsizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tif (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY)\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\tconn->use_spnego = true;\n\n\tksmbd_conn_set_need_negotiate(conn);\n\treturn 0;\n}",
                        "cve_id": "CVE-2023-32252"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void destroy_previous_session(struct ksmbd_conn *conn,\n\t\t\t\t     struct ksmbd_user *user, u64 id)\n{\n\tstruct ksmbd_session *prev_sess = ksmbd_session_lookup_slowpath(id);\n\tstruct ksmbd_user *prev_user;\n\tstruct channel *chann;\n\tlong index;\n\n\tif (!prev_sess)\n\t\treturn;\n\n\tprev_user = prev_sess->user;\n\n\tif (!prev_user ||\n\t    strcmp(user->name, prev_user->name) ||\n\t    user->passkey_sz != prev_user->passkey_sz ||\n\t    memcmp(user->passkey, prev_user->passkey, user->passkey_sz))\n\t\treturn;\n\n\tprev_sess->state = SMB2_SESSION_EXPIRED;\n\txa_for_each(&prev_sess->ksmbd_chann_list, index, chann)\n\t\tchann->conn->status = KSMBD_SESS_EXITING;\n}",
                        "code_after_change": "static int ksmbd_tcp_readv(struct tcp_transport *t, struct kvec *iov_orig,\n\t\t\t   unsigned int nr_segs, unsigned int to_read,\n\t\t\t   int max_retries)\n{\n\tint length = 0;\n\tint total_read;\n\tunsigned int segs;\n\tstruct msghdr ksmbd_msg;\n\tstruct kvec *iov;\n\tstruct ksmbd_conn *conn = KSMBD_TRANS(t)->conn;\n\n\tiov = get_conn_iovec(t, nr_segs);\n\tif (!iov)\n\t\treturn -ENOMEM;\n\n\tksmbd_msg.msg_control = NULL;\n\tksmbd_msg.msg_controllen = 0;\n\n\tfor (total_read = 0; to_read; total_read += length, to_read -= length) {\n\t\ttry_to_freeze();\n\n\t\tif (!ksmbd_conn_alive(conn)) {\n\t\t\ttotal_read = -ESHUTDOWN;\n\t\t\tbreak;\n\t\t}\n\t\tsegs = kvec_array_init(iov, iov_orig, nr_segs, total_read);\n\n\t\tlength = kernel_recvmsg(t->sock, &ksmbd_msg,\n\t\t\t\t\tiov, segs, to_read, 0);\n\n\t\tif (length == -EINTR) {\n\t\t\ttotal_read = -ESHUTDOWN;\n\t\t\tbreak;\n\t\t} else if (ksmbd_conn_need_reconnect(conn)) {\n\t\t\ttotal_read = -EAGAIN;\n\t\t\tbreak;\n\t\t} else if (length == -ERESTARTSYS || length == -EAGAIN) {\n\t\t\t/*\n\t\t\t * If max_retries is negative, Allow unlimited\n\t\t\t * retries to keep connection with inactive sessions.\n\t\t\t */\n\t\t\tif (max_retries == 0) {\n\t\t\t\ttotal_read = length;\n\t\t\t\tbreak;\n\t\t\t} else if (max_retries > 0) {\n\t\t\t\tmax_retries--;\n\t\t\t}\n\n\t\t\tusleep_range(1000, 2000);\n\t\t\tlength = 0;\n\t\t\tcontinue;\n\t\t} else if (length <= 0) {\n\t\t\ttotal_read = length;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn total_read;\n}",
                        "cve_id": "CVE-2023-32252"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int ksmbd_tcp_readv(struct tcp_transport *t, struct kvec *iov_orig,\n\t\t\t   unsigned int nr_segs, unsigned int to_read,\n\t\t\t   int max_retries)\n{\n\tint length = 0;\n\tint total_read;\n\tunsigned int segs;\n\tstruct msghdr ksmbd_msg;\n\tstruct kvec *iov;\n\tstruct ksmbd_conn *conn = KSMBD_TRANS(t)->conn;\n\n\tiov = get_conn_iovec(t, nr_segs);\n\tif (!iov)\n\t\treturn -ENOMEM;\n\n\tksmbd_msg.msg_control = NULL;\n\tksmbd_msg.msg_controllen = 0;\n\n\tfor (total_read = 0; to_read; total_read += length, to_read -= length) {\n\t\ttry_to_freeze();\n\n\t\tif (!ksmbd_conn_alive(conn)) {\n\t\t\ttotal_read = -ESHUTDOWN;\n\t\t\tbreak;\n\t\t}\n\t\tsegs = kvec_array_init(iov, iov_orig, nr_segs, total_read);\n\n\t\tlength = kernel_recvmsg(t->sock, &ksmbd_msg,\n\t\t\t\t\tiov, segs, to_read, 0);\n\n\t\tif (length == -EINTR) {\n\t\t\ttotal_read = -ESHUTDOWN;\n\t\t\tbreak;\n\t\t} else if (conn->status == KSMBD_SESS_NEED_RECONNECT) {\n\t\t\ttotal_read = -EAGAIN;\n\t\t\tbreak;\n\t\t} else if (length == -ERESTARTSYS || length == -EAGAIN) {\n\t\t\t/*\n\t\t\t * If max_retries is negative, Allow unlimited\n\t\t\t * retries to keep connection with inactive sessions.\n\t\t\t */\n\t\t\tif (max_retries == 0) {\n\t\t\t\ttotal_read = length;\n\t\t\t\tbreak;\n\t\t\t} else if (max_retries > 0) {\n\t\t\t\tmax_retries--;\n\t\t\t}\n\n\t\t\tusleep_range(1000, 2000);\n\t\t\tlength = 0;\n\t\t\tcontinue;\n\t\t} else if (length <= 0) {\n\t\t\ttotal_read = length;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn total_read;\n}",
                        "code_after_change": "static void destroy_previous_session(struct ksmbd_conn *conn,\n\t\t\t\t     struct ksmbd_user *user, u64 id)\n{\n\tstruct ksmbd_session *prev_sess = ksmbd_session_lookup_slowpath(id);\n\tstruct ksmbd_user *prev_user;\n\tstruct channel *chann;\n\tlong index;\n\n\tif (!prev_sess)\n\t\treturn;\n\n\tprev_user = prev_sess->user;\n\n\tif (!prev_user ||\n\t    strcmp(user->name, prev_user->name) ||\n\t    user->passkey_sz != prev_user->passkey_sz ||\n\t    memcmp(user->passkey, prev_user->passkey, user->passkey_sz))\n\t\treturn;\n\n\tprev_sess->state = SMB2_SESSION_EXPIRED;\n\txa_for_each(&prev_sess->ksmbd_chann_list, index, chann)\n\t\tksmbd_conn_set_exiting(chann->conn);\n}",
                        "cve_id": "CVE-2023-32252"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int smb2_get_info_filesystem(struct ksmbd_work *work,\n\t\t\t\t    struct smb2_query_info_req *req,\n\t\t\t\t    struct smb2_query_info_rsp *rsp)\n{\n\tstruct ksmbd_session *sess = work->sess;\n\tstruct ksmbd_conn *conn = work->conn;\n\tstruct ksmbd_share_config *share = work->tcon->share_conf;\n\tint fsinfoclass = 0;\n\tstruct kstatfs stfs;\n\tstruct path path;\n\tint rc = 0, len;\n\tint fs_infoclass_size = 0;\n\n\trc = kern_path(share->path, LOOKUP_NO_SYMLINKS, &path);\n\tif (rc) {\n\t\tpr_err(\"cannot create vfs path\\n\");\n\t\treturn -EIO;\n\t}\n\n\trc = vfs_statfs(&path, &stfs);\n\tif (rc) {\n\t\tpr_err(\"cannot do stat of path %s\\n\", share->path);\n\t\tpath_put(&path);\n\t\treturn -EIO;\n\t}\n\n\tfsinfoclass = req->FileInfoClass;\n\n\tswitch (fsinfoclass) {\n\tcase FS_DEVICE_INFORMATION:\n\t{\n\t\tstruct filesystem_device_info *info;\n\n\t\tinfo = (struct filesystem_device_info *)rsp->Buffer;\n\n\t\tinfo->DeviceType = cpu_to_le32(stfs.f_type);\n\t\tinfo->DeviceCharacteristics = cpu_to_le32(0x00000020);\n\t\trsp->OutputBufferLength = cpu_to_le32(8);\n\t\tinc_rfc1001_len(work->response_buf, 8);\n\t\tfs_infoclass_size = FS_DEVICE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_ATTRIBUTE_INFORMATION:\n\t{\n\t\tstruct filesystem_attribute_info *info;\n\t\tsize_t sz;\n\n\t\tinfo = (struct filesystem_attribute_info *)rsp->Buffer;\n\t\tinfo->Attributes = cpu_to_le32(FILE_SUPPORTS_OBJECT_IDS |\n\t\t\t\t\t       FILE_PERSISTENT_ACLS |\n\t\t\t\t\t       FILE_UNICODE_ON_DISK |\n\t\t\t\t\t       FILE_CASE_PRESERVED_NAMES |\n\t\t\t\t\t       FILE_CASE_SENSITIVE_SEARCH |\n\t\t\t\t\t       FILE_SUPPORTS_BLOCK_REFCOUNTING);\n\n\t\tinfo->Attributes |= cpu_to_le32(server_conf.share_fake_fscaps);\n\n\t\tif (test_share_config_flag(work->tcon->share_conf,\n\t\t    KSMBD_SHARE_FLAG_STREAMS))\n\t\t\tinfo->Attributes |= cpu_to_le32(FILE_NAMED_STREAMS);\n\n\t\tinfo->MaxPathNameComponentLength = cpu_to_le32(stfs.f_namelen);\n\t\tlen = smbConvertToUTF16((__le16 *)info->FileSystemName,\n\t\t\t\t\t\"NTFS\", PATH_MAX, conn->local_nls, 0);\n\t\tlen = len * 2;\n\t\tinfo->FileSystemNameLen = cpu_to_le32(len);\n\t\tsz = sizeof(struct filesystem_attribute_info) - 2 + len;\n\t\trsp->OutputBufferLength = cpu_to_le32(sz);\n\t\tinc_rfc1001_len(work->response_buf, sz);\n\t\tfs_infoclass_size = FS_ATTRIBUTE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_VOLUME_INFORMATION:\n\t{\n\t\tstruct filesystem_vol_info *info;\n\t\tsize_t sz;\n\t\tunsigned int serial_crc = 0;\n\n\t\tinfo = (struct filesystem_vol_info *)(rsp->Buffer);\n\t\tinfo->VolumeCreationTime = 0;\n\t\tserial_crc = crc32_le(serial_crc, share->name,\n\t\t\t\t      strlen(share->name));\n\t\tserial_crc = crc32_le(serial_crc, share->path,\n\t\t\t\t      strlen(share->path));\n\t\tserial_crc = crc32_le(serial_crc, ksmbd_netbios_name(),\n\t\t\t\t      strlen(ksmbd_netbios_name()));\n\t\t/* Taking dummy value of serial number*/\n\t\tinfo->SerialNumber = cpu_to_le32(serial_crc);\n\t\tlen = smbConvertToUTF16((__le16 *)info->VolumeLabel,\n\t\t\t\t\tshare->name, PATH_MAX,\n\t\t\t\t\tconn->local_nls, 0);\n\t\tlen = len * 2;\n\t\tinfo->VolumeLabelSize = cpu_to_le32(len);\n\t\tinfo->Reserved = 0;\n\t\tsz = sizeof(struct filesystem_vol_info) - 2 + len;\n\t\trsp->OutputBufferLength = cpu_to_le32(sz);\n\t\tinc_rfc1001_len(work->response_buf, sz);\n\t\tfs_infoclass_size = FS_VOLUME_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_SIZE_INFORMATION:\n\t{\n\t\tstruct filesystem_info *info;\n\n\t\tinfo = (struct filesystem_info *)(rsp->Buffer);\n\t\tinfo->TotalAllocationUnits = cpu_to_le64(stfs.f_blocks);\n\t\tinfo->FreeAllocationUnits = cpu_to_le64(stfs.f_bfree);\n\t\tinfo->SectorsPerAllocationUnit = cpu_to_le32(1);\n\t\tinfo->BytesPerSector = cpu_to_le32(stfs.f_bsize);\n\t\trsp->OutputBufferLength = cpu_to_le32(24);\n\t\tinc_rfc1001_len(work->response_buf, 24);\n\t\tfs_infoclass_size = FS_SIZE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_FULL_SIZE_INFORMATION:\n\t{\n\t\tstruct smb2_fs_full_size_info *info;\n\n\t\tinfo = (struct smb2_fs_full_size_info *)(rsp->Buffer);\n\t\tinfo->TotalAllocationUnits = cpu_to_le64(stfs.f_blocks);\n\t\tinfo->CallerAvailableAllocationUnits =\n\t\t\t\t\tcpu_to_le64(stfs.f_bavail);\n\t\tinfo->ActualAvailableAllocationUnits =\n\t\t\t\t\tcpu_to_le64(stfs.f_bfree);\n\t\tinfo->SectorsPerAllocationUnit = cpu_to_le32(1);\n\t\tinfo->BytesPerSector = cpu_to_le32(stfs.f_bsize);\n\t\trsp->OutputBufferLength = cpu_to_le32(32);\n\t\tinc_rfc1001_len(work->response_buf, 32);\n\t\tfs_infoclass_size = FS_FULL_SIZE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_OBJECT_ID_INFORMATION:\n\t{\n\t\tstruct object_id_info *info;\n\n\t\tinfo = (struct object_id_info *)(rsp->Buffer);\n\n\t\tif (!user_guest(sess->user))\n\t\t\tmemcpy(info->objid, user_passkey(sess->user), 16);\n\t\telse\n\t\t\tmemset(info->objid, 0, 16);\n\n\t\tinfo->extended_info.magic = cpu_to_le32(EXTENDED_INFO_MAGIC);\n\t\tinfo->extended_info.version = cpu_to_le32(1);\n\t\tinfo->extended_info.release = cpu_to_le32(1);\n\t\tinfo->extended_info.rel_date = 0;\n\t\tmemcpy(info->extended_info.version_string, \"1.1.0\", strlen(\"1.1.0\"));\n\t\trsp->OutputBufferLength = cpu_to_le32(64);\n\t\tinc_rfc1001_len(work->response_buf, 64);\n\t\tfs_infoclass_size = FS_OBJECT_ID_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_SECTOR_SIZE_INFORMATION:\n\t{\n\t\tstruct smb3_fs_ss_info *info;\n\t\tunsigned int sector_size =\n\t\t\tmin_t(unsigned int, path.mnt->mnt_sb->s_blocksize, 4096);\n\n\t\tinfo = (struct smb3_fs_ss_info *)(rsp->Buffer);\n\n\t\tinfo->LogicalBytesPerSector = cpu_to_le32(sector_size);\n\t\tinfo->PhysicalBytesPerSectorForAtomicity =\n\t\t\t\tcpu_to_le32(sector_size);\n\t\tinfo->PhysicalBytesPerSectorForPerf = cpu_to_le32(sector_size);\n\t\tinfo->FSEffPhysicalBytesPerSectorForAtomicity =\n\t\t\t\tcpu_to_le32(sector_size);\n\t\tinfo->Flags = cpu_to_le32(SSINFO_FLAGS_ALIGNED_DEVICE |\n\t\t\t\t    SSINFO_FLAGS_PARTITION_ALIGNED_ON_DEVICE);\n\t\tinfo->ByteOffsetForSectorAlignment = 0;\n\t\tinfo->ByteOffsetForPartitionAlignment = 0;\n\t\trsp->OutputBufferLength = cpu_to_le32(28);\n\t\tinc_rfc1001_len(work->response_buf, 28);\n\t\tfs_infoclass_size = FS_SECTOR_SIZE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_CONTROL_INFORMATION:\n\t{\n\t\t/*\n\t\t * TODO : The current implementation is based on\n\t\t * test result with win7(NTFS) server. It's need to\n\t\t * modify this to get valid Quota values\n\t\t * from Linux kernel\n\t\t */\n\t\tstruct smb2_fs_control_info *info;\n\n\t\tinfo = (struct smb2_fs_control_info *)(rsp->Buffer);\n\t\tinfo->FreeSpaceStartFiltering = 0;\n\t\tinfo->FreeSpaceThreshold = 0;\n\t\tinfo->FreeSpaceStopFiltering = 0;\n\t\tinfo->DefaultQuotaThreshold = cpu_to_le64(SMB2_NO_FID);\n\t\tinfo->DefaultQuotaLimit = cpu_to_le64(SMB2_NO_FID);\n\t\tinfo->Padding = 0;\n\t\trsp->OutputBufferLength = cpu_to_le32(48);\n\t\tinc_rfc1001_len(work->response_buf, 48);\n\t\tfs_infoclass_size = FS_CONTROL_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_POSIX_INFORMATION:\n\t{\n\t\tstruct filesystem_posix_info *info;\n\n\t\tif (!work->tcon->posix_extensions) {\n\t\t\tpr_err(\"client doesn't negotiate with SMB3.1.1 POSIX Extensions\\n\");\n\t\t\trc = -EOPNOTSUPP;\n\t\t} else {\n\t\t\tinfo = (struct filesystem_posix_info *)(rsp->Buffer);\n\t\t\tinfo->OptimalTransferSize = cpu_to_le32(stfs.f_bsize);\n\t\t\tinfo->BlockSize = cpu_to_le32(stfs.f_bsize);\n\t\t\tinfo->TotalBlocks = cpu_to_le64(stfs.f_blocks);\n\t\t\tinfo->BlocksAvail = cpu_to_le64(stfs.f_bfree);\n\t\t\tinfo->UserBlocksAvail = cpu_to_le64(stfs.f_bavail);\n\t\t\tinfo->TotalFileNodes = cpu_to_le64(stfs.f_files);\n\t\t\tinfo->FreeFileNodes = cpu_to_le64(stfs.f_ffree);\n\t\t\trsp->OutputBufferLength = cpu_to_le32(56);\n\t\t\tinc_rfc1001_len(work->response_buf, 56);\n\t\t\tfs_infoclass_size = FS_POSIX_INFORMATION_SIZE;\n\t\t}\n\t\tbreak;\n\t}\n\tdefault:\n\t\tpath_put(&path);\n\t\treturn -EOPNOTSUPP;\n\t}\n\trc = buffer_check_err(le32_to_cpu(req->OutputBufferLength),\n\t\t\t      rsp, work->response_buf,\n\t\t\t      fs_infoclass_size);\n\tpath_put(&path);\n\treturn rc;\n}",
                        "code_after_change": "static int smb2_get_info_filesystem(struct ksmbd_work *work,\n\t\t\t\t    struct smb2_query_info_req *req,\n\t\t\t\t    struct smb2_query_info_rsp *rsp)\n{\n\tstruct ksmbd_session *sess = work->sess;\n\tstruct ksmbd_conn *conn = work->conn;\n\tstruct ksmbd_share_config *share = work->tcon->share_conf;\n\tint fsinfoclass = 0;\n\tstruct kstatfs stfs;\n\tstruct path path;\n\tint rc = 0, len;\n\tint fs_infoclass_size = 0;\n\n\tif (!share->path)\n\t\treturn -EIO;\n\n\trc = kern_path(share->path, LOOKUP_NO_SYMLINKS, &path);\n\tif (rc) {\n\t\tpr_err(\"cannot create vfs path\\n\");\n\t\treturn -EIO;\n\t}\n\n\trc = vfs_statfs(&path, &stfs);\n\tif (rc) {\n\t\tpr_err(\"cannot do stat of path %s\\n\", share->path);\n\t\tpath_put(&path);\n\t\treturn -EIO;\n\t}\n\n\tfsinfoclass = req->FileInfoClass;\n\n\tswitch (fsinfoclass) {\n\tcase FS_DEVICE_INFORMATION:\n\t{\n\t\tstruct filesystem_device_info *info;\n\n\t\tinfo = (struct filesystem_device_info *)rsp->Buffer;\n\n\t\tinfo->DeviceType = cpu_to_le32(stfs.f_type);\n\t\tinfo->DeviceCharacteristics = cpu_to_le32(0x00000020);\n\t\trsp->OutputBufferLength = cpu_to_le32(8);\n\t\tinc_rfc1001_len(work->response_buf, 8);\n\t\tfs_infoclass_size = FS_DEVICE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_ATTRIBUTE_INFORMATION:\n\t{\n\t\tstruct filesystem_attribute_info *info;\n\t\tsize_t sz;\n\n\t\tinfo = (struct filesystem_attribute_info *)rsp->Buffer;\n\t\tinfo->Attributes = cpu_to_le32(FILE_SUPPORTS_OBJECT_IDS |\n\t\t\t\t\t       FILE_PERSISTENT_ACLS |\n\t\t\t\t\t       FILE_UNICODE_ON_DISK |\n\t\t\t\t\t       FILE_CASE_PRESERVED_NAMES |\n\t\t\t\t\t       FILE_CASE_SENSITIVE_SEARCH |\n\t\t\t\t\t       FILE_SUPPORTS_BLOCK_REFCOUNTING);\n\n\t\tinfo->Attributes |= cpu_to_le32(server_conf.share_fake_fscaps);\n\n\t\tif (test_share_config_flag(work->tcon->share_conf,\n\t\t    KSMBD_SHARE_FLAG_STREAMS))\n\t\t\tinfo->Attributes |= cpu_to_le32(FILE_NAMED_STREAMS);\n\n\t\tinfo->MaxPathNameComponentLength = cpu_to_le32(stfs.f_namelen);\n\t\tlen = smbConvertToUTF16((__le16 *)info->FileSystemName,\n\t\t\t\t\t\"NTFS\", PATH_MAX, conn->local_nls, 0);\n\t\tlen = len * 2;\n\t\tinfo->FileSystemNameLen = cpu_to_le32(len);\n\t\tsz = sizeof(struct filesystem_attribute_info) - 2 + len;\n\t\trsp->OutputBufferLength = cpu_to_le32(sz);\n\t\tinc_rfc1001_len(work->response_buf, sz);\n\t\tfs_infoclass_size = FS_ATTRIBUTE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_VOLUME_INFORMATION:\n\t{\n\t\tstruct filesystem_vol_info *info;\n\t\tsize_t sz;\n\t\tunsigned int serial_crc = 0;\n\n\t\tinfo = (struct filesystem_vol_info *)(rsp->Buffer);\n\t\tinfo->VolumeCreationTime = 0;\n\t\tserial_crc = crc32_le(serial_crc, share->name,\n\t\t\t\t      strlen(share->name));\n\t\tserial_crc = crc32_le(serial_crc, share->path,\n\t\t\t\t      strlen(share->path));\n\t\tserial_crc = crc32_le(serial_crc, ksmbd_netbios_name(),\n\t\t\t\t      strlen(ksmbd_netbios_name()));\n\t\t/* Taking dummy value of serial number*/\n\t\tinfo->SerialNumber = cpu_to_le32(serial_crc);\n\t\tlen = smbConvertToUTF16((__le16 *)info->VolumeLabel,\n\t\t\t\t\tshare->name, PATH_MAX,\n\t\t\t\t\tconn->local_nls, 0);\n\t\tlen = len * 2;\n\t\tinfo->VolumeLabelSize = cpu_to_le32(len);\n\t\tinfo->Reserved = 0;\n\t\tsz = sizeof(struct filesystem_vol_info) - 2 + len;\n\t\trsp->OutputBufferLength = cpu_to_le32(sz);\n\t\tinc_rfc1001_len(work->response_buf, sz);\n\t\tfs_infoclass_size = FS_VOLUME_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_SIZE_INFORMATION:\n\t{\n\t\tstruct filesystem_info *info;\n\n\t\tinfo = (struct filesystem_info *)(rsp->Buffer);\n\t\tinfo->TotalAllocationUnits = cpu_to_le64(stfs.f_blocks);\n\t\tinfo->FreeAllocationUnits = cpu_to_le64(stfs.f_bfree);\n\t\tinfo->SectorsPerAllocationUnit = cpu_to_le32(1);\n\t\tinfo->BytesPerSector = cpu_to_le32(stfs.f_bsize);\n\t\trsp->OutputBufferLength = cpu_to_le32(24);\n\t\tinc_rfc1001_len(work->response_buf, 24);\n\t\tfs_infoclass_size = FS_SIZE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_FULL_SIZE_INFORMATION:\n\t{\n\t\tstruct smb2_fs_full_size_info *info;\n\n\t\tinfo = (struct smb2_fs_full_size_info *)(rsp->Buffer);\n\t\tinfo->TotalAllocationUnits = cpu_to_le64(stfs.f_blocks);\n\t\tinfo->CallerAvailableAllocationUnits =\n\t\t\t\t\tcpu_to_le64(stfs.f_bavail);\n\t\tinfo->ActualAvailableAllocationUnits =\n\t\t\t\t\tcpu_to_le64(stfs.f_bfree);\n\t\tinfo->SectorsPerAllocationUnit = cpu_to_le32(1);\n\t\tinfo->BytesPerSector = cpu_to_le32(stfs.f_bsize);\n\t\trsp->OutputBufferLength = cpu_to_le32(32);\n\t\tinc_rfc1001_len(work->response_buf, 32);\n\t\tfs_infoclass_size = FS_FULL_SIZE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_OBJECT_ID_INFORMATION:\n\t{\n\t\tstruct object_id_info *info;\n\n\t\tinfo = (struct object_id_info *)(rsp->Buffer);\n\n\t\tif (!user_guest(sess->user))\n\t\t\tmemcpy(info->objid, user_passkey(sess->user), 16);\n\t\telse\n\t\t\tmemset(info->objid, 0, 16);\n\n\t\tinfo->extended_info.magic = cpu_to_le32(EXTENDED_INFO_MAGIC);\n\t\tinfo->extended_info.version = cpu_to_le32(1);\n\t\tinfo->extended_info.release = cpu_to_le32(1);\n\t\tinfo->extended_info.rel_date = 0;\n\t\tmemcpy(info->extended_info.version_string, \"1.1.0\", strlen(\"1.1.0\"));\n\t\trsp->OutputBufferLength = cpu_to_le32(64);\n\t\tinc_rfc1001_len(work->response_buf, 64);\n\t\tfs_infoclass_size = FS_OBJECT_ID_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_SECTOR_SIZE_INFORMATION:\n\t{\n\t\tstruct smb3_fs_ss_info *info;\n\t\tunsigned int sector_size =\n\t\t\tmin_t(unsigned int, path.mnt->mnt_sb->s_blocksize, 4096);\n\n\t\tinfo = (struct smb3_fs_ss_info *)(rsp->Buffer);\n\n\t\tinfo->LogicalBytesPerSector = cpu_to_le32(sector_size);\n\t\tinfo->PhysicalBytesPerSectorForAtomicity =\n\t\t\t\tcpu_to_le32(sector_size);\n\t\tinfo->PhysicalBytesPerSectorForPerf = cpu_to_le32(sector_size);\n\t\tinfo->FSEffPhysicalBytesPerSectorForAtomicity =\n\t\t\t\tcpu_to_le32(sector_size);\n\t\tinfo->Flags = cpu_to_le32(SSINFO_FLAGS_ALIGNED_DEVICE |\n\t\t\t\t    SSINFO_FLAGS_PARTITION_ALIGNED_ON_DEVICE);\n\t\tinfo->ByteOffsetForSectorAlignment = 0;\n\t\tinfo->ByteOffsetForPartitionAlignment = 0;\n\t\trsp->OutputBufferLength = cpu_to_le32(28);\n\t\tinc_rfc1001_len(work->response_buf, 28);\n\t\tfs_infoclass_size = FS_SECTOR_SIZE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_CONTROL_INFORMATION:\n\t{\n\t\t/*\n\t\t * TODO : The current implementation is based on\n\t\t * test result with win7(NTFS) server. It's need to\n\t\t * modify this to get valid Quota values\n\t\t * from Linux kernel\n\t\t */\n\t\tstruct smb2_fs_control_info *info;\n\n\t\tinfo = (struct smb2_fs_control_info *)(rsp->Buffer);\n\t\tinfo->FreeSpaceStartFiltering = 0;\n\t\tinfo->FreeSpaceThreshold = 0;\n\t\tinfo->FreeSpaceStopFiltering = 0;\n\t\tinfo->DefaultQuotaThreshold = cpu_to_le64(SMB2_NO_FID);\n\t\tinfo->DefaultQuotaLimit = cpu_to_le64(SMB2_NO_FID);\n\t\tinfo->Padding = 0;\n\t\trsp->OutputBufferLength = cpu_to_le32(48);\n\t\tinc_rfc1001_len(work->response_buf, 48);\n\t\tfs_infoclass_size = FS_CONTROL_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_POSIX_INFORMATION:\n\t{\n\t\tstruct filesystem_posix_info *info;\n\n\t\tif (!work->tcon->posix_extensions) {\n\t\t\tpr_err(\"client doesn't negotiate with SMB3.1.1 POSIX Extensions\\n\");\n\t\t\trc = -EOPNOTSUPP;\n\t\t} else {\n\t\t\tinfo = (struct filesystem_posix_info *)(rsp->Buffer);\n\t\t\tinfo->OptimalTransferSize = cpu_to_le32(stfs.f_bsize);\n\t\t\tinfo->BlockSize = cpu_to_le32(stfs.f_bsize);\n\t\t\tinfo->TotalBlocks = cpu_to_le64(stfs.f_blocks);\n\t\t\tinfo->BlocksAvail = cpu_to_le64(stfs.f_bfree);\n\t\t\tinfo->UserBlocksAvail = cpu_to_le64(stfs.f_bavail);\n\t\t\tinfo->TotalFileNodes = cpu_to_le64(stfs.f_files);\n\t\t\tinfo->FreeFileNodes = cpu_to_le64(stfs.f_ffree);\n\t\t\trsp->OutputBufferLength = cpu_to_le32(56);\n\t\t\tinc_rfc1001_len(work->response_buf, 56);\n\t\t\tfs_infoclass_size = FS_POSIX_INFORMATION_SIZE;\n\t\t}\n\t\tbreak;\n\t}\n\tdefault:\n\t\tpath_put(&path);\n\t\treturn -EOPNOTSUPP;\n\t}\n\trc = buffer_check_err(le32_to_cpu(req->OutputBufferLength),\n\t\t\t      rsp, work->response_buf,\n\t\t\t      fs_infoclass_size);\n\tpath_put(&path);\n\treturn rc;\n}",
                        "cve_id": "CVE-2023-32248"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 4191,
            "cve_id": "CVE-2023-32252",
            "code_snippet": "static inline int check_conn_state(struct ksmbd_work *work)\n{\n\tstruct smb_hdr *rsp_hdr;\n\n\tif (ksmbd_conn_exiting(work) || ksmbd_conn_need_reconnect(work)) {\n\t\trsp_hdr = work->response_buf;\n\t\trsp_hdr->Status.CifsError = STATUS_CONNECTION_DISCONNECTED;\n\t\treturn 1;\n\t}\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int init_smb2_neg_rsp(struct ksmbd_work *work)\n{\n\tstruct smb2_hdr *rsp_hdr;\n\tstruct smb2_negotiate_rsp *rsp;\n\tstruct ksmbd_conn *conn = work->conn;\n\n\t*(__be32 *)work->response_buf =\n\t\tcpu_to_be32(conn->vals->header_size);\n\n\trsp_hdr = smb2_get_msg(work->response_buf);\n\tmemset(rsp_hdr, 0, sizeof(struct smb2_hdr) + 2);\n\trsp_hdr->ProtocolId = SMB2_PROTO_NUMBER;\n\trsp_hdr->StructureSize = SMB2_HEADER_STRUCTURE_SIZE;\n\trsp_hdr->CreditRequest = cpu_to_le16(2);\n\trsp_hdr->Command = SMB2_NEGOTIATE;\n\trsp_hdr->Flags = (SMB2_FLAGS_SERVER_TO_REDIR);\n\trsp_hdr->NextCommand = 0;\n\trsp_hdr->MessageId = 0;\n\trsp_hdr->Id.SyncId.ProcessId = 0;\n\trsp_hdr->Id.SyncId.TreeId = 0;\n\trsp_hdr->SessionId = 0;\n\tmemset(rsp_hdr->Signature, 0, 16);\n\n\trsp = smb2_get_msg(work->response_buf);\n\n\tWARN_ON(ksmbd_conn_good(work));\n\n\trsp->StructureSize = cpu_to_le16(65);\n\tksmbd_debug(SMB, \"conn->dialect 0x%x\\n\", conn->dialect);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying connection\n\t */\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\t/* Default Max Message Size till SMB2.0, 64K*/\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\tle16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf,\n\t\t\tsizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tif (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY)\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\tconn->use_spnego = true;\n\n\tksmbd_conn_set_need_negotiate(work);\n\treturn 0;\n}",
                        "code_after_change": "int init_smb2_neg_rsp(struct ksmbd_work *work)\n{\n\tstruct smb2_hdr *rsp_hdr;\n\tstruct smb2_negotiate_rsp *rsp;\n\tstruct ksmbd_conn *conn = work->conn;\n\n\t*(__be32 *)work->response_buf =\n\t\tcpu_to_be32(conn->vals->header_size);\n\n\trsp_hdr = smb2_get_msg(work->response_buf);\n\tmemset(rsp_hdr, 0, sizeof(struct smb2_hdr) + 2);\n\trsp_hdr->ProtocolId = SMB2_PROTO_NUMBER;\n\trsp_hdr->StructureSize = SMB2_HEADER_STRUCTURE_SIZE;\n\trsp_hdr->CreditRequest = cpu_to_le16(2);\n\trsp_hdr->Command = SMB2_NEGOTIATE;\n\trsp_hdr->Flags = (SMB2_FLAGS_SERVER_TO_REDIR);\n\trsp_hdr->NextCommand = 0;\n\trsp_hdr->MessageId = 0;\n\trsp_hdr->Id.SyncId.ProcessId = 0;\n\trsp_hdr->Id.SyncId.TreeId = 0;\n\trsp_hdr->SessionId = 0;\n\tmemset(rsp_hdr->Signature, 0, 16);\n\n\trsp = smb2_get_msg(work->response_buf);\n\n\tWARN_ON(ksmbd_conn_good(conn));\n\n\trsp->StructureSize = cpu_to_le16(65);\n\tksmbd_debug(SMB, \"conn->dialect 0x%x\\n\", conn->dialect);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying connection\n\t */\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\t/* Default Max Message Size till SMB2.0, 64K*/\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\tle16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf,\n\t\t\tsizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tif (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY)\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\tconn->use_spnego = true;\n\n\tksmbd_conn_set_need_negotiate(conn);\n\treturn 0;\n}",
                        "cve_id": "CVE-2023-32252"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int smb2_get_info_filesystem(struct ksmbd_work *work,\n\t\t\t\t    struct smb2_query_info_req *req,\n\t\t\t\t    struct smb2_query_info_rsp *rsp)\n{\n\tstruct ksmbd_session *sess = work->sess;\n\tstruct ksmbd_conn *conn = work->conn;\n\tstruct ksmbd_share_config *share = work->tcon->share_conf;\n\tint fsinfoclass = 0;\n\tstruct kstatfs stfs;\n\tstruct path path;\n\tint rc = 0, len;\n\tint fs_infoclass_size = 0;\n\n\trc = kern_path(share->path, LOOKUP_NO_SYMLINKS, &path);\n\tif (rc) {\n\t\tpr_err(\"cannot create vfs path\\n\");\n\t\treturn -EIO;\n\t}\n\n\trc = vfs_statfs(&path, &stfs);\n\tif (rc) {\n\t\tpr_err(\"cannot do stat of path %s\\n\", share->path);\n\t\tpath_put(&path);\n\t\treturn -EIO;\n\t}\n\n\tfsinfoclass = req->FileInfoClass;\n\n\tswitch (fsinfoclass) {\n\tcase FS_DEVICE_INFORMATION:\n\t{\n\t\tstruct filesystem_device_info *info;\n\n\t\tinfo = (struct filesystem_device_info *)rsp->Buffer;\n\n\t\tinfo->DeviceType = cpu_to_le32(stfs.f_type);\n\t\tinfo->DeviceCharacteristics = cpu_to_le32(0x00000020);\n\t\trsp->OutputBufferLength = cpu_to_le32(8);\n\t\tinc_rfc1001_len(work->response_buf, 8);\n\t\tfs_infoclass_size = FS_DEVICE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_ATTRIBUTE_INFORMATION:\n\t{\n\t\tstruct filesystem_attribute_info *info;\n\t\tsize_t sz;\n\n\t\tinfo = (struct filesystem_attribute_info *)rsp->Buffer;\n\t\tinfo->Attributes = cpu_to_le32(FILE_SUPPORTS_OBJECT_IDS |\n\t\t\t\t\t       FILE_PERSISTENT_ACLS |\n\t\t\t\t\t       FILE_UNICODE_ON_DISK |\n\t\t\t\t\t       FILE_CASE_PRESERVED_NAMES |\n\t\t\t\t\t       FILE_CASE_SENSITIVE_SEARCH |\n\t\t\t\t\t       FILE_SUPPORTS_BLOCK_REFCOUNTING);\n\n\t\tinfo->Attributes |= cpu_to_le32(server_conf.share_fake_fscaps);\n\n\t\tif (test_share_config_flag(work->tcon->share_conf,\n\t\t    KSMBD_SHARE_FLAG_STREAMS))\n\t\t\tinfo->Attributes |= cpu_to_le32(FILE_NAMED_STREAMS);\n\n\t\tinfo->MaxPathNameComponentLength = cpu_to_le32(stfs.f_namelen);\n\t\tlen = smbConvertToUTF16((__le16 *)info->FileSystemName,\n\t\t\t\t\t\"NTFS\", PATH_MAX, conn->local_nls, 0);\n\t\tlen = len * 2;\n\t\tinfo->FileSystemNameLen = cpu_to_le32(len);\n\t\tsz = sizeof(struct filesystem_attribute_info) - 2 + len;\n\t\trsp->OutputBufferLength = cpu_to_le32(sz);\n\t\tinc_rfc1001_len(work->response_buf, sz);\n\t\tfs_infoclass_size = FS_ATTRIBUTE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_VOLUME_INFORMATION:\n\t{\n\t\tstruct filesystem_vol_info *info;\n\t\tsize_t sz;\n\t\tunsigned int serial_crc = 0;\n\n\t\tinfo = (struct filesystem_vol_info *)(rsp->Buffer);\n\t\tinfo->VolumeCreationTime = 0;\n\t\tserial_crc = crc32_le(serial_crc, share->name,\n\t\t\t\t      strlen(share->name));\n\t\tserial_crc = crc32_le(serial_crc, share->path,\n\t\t\t\t      strlen(share->path));\n\t\tserial_crc = crc32_le(serial_crc, ksmbd_netbios_name(),\n\t\t\t\t      strlen(ksmbd_netbios_name()));\n\t\t/* Taking dummy value of serial number*/\n\t\tinfo->SerialNumber = cpu_to_le32(serial_crc);\n\t\tlen = smbConvertToUTF16((__le16 *)info->VolumeLabel,\n\t\t\t\t\tshare->name, PATH_MAX,\n\t\t\t\t\tconn->local_nls, 0);\n\t\tlen = len * 2;\n\t\tinfo->VolumeLabelSize = cpu_to_le32(len);\n\t\tinfo->Reserved = 0;\n\t\tsz = sizeof(struct filesystem_vol_info) - 2 + len;\n\t\trsp->OutputBufferLength = cpu_to_le32(sz);\n\t\tinc_rfc1001_len(work->response_buf, sz);\n\t\tfs_infoclass_size = FS_VOLUME_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_SIZE_INFORMATION:\n\t{\n\t\tstruct filesystem_info *info;\n\n\t\tinfo = (struct filesystem_info *)(rsp->Buffer);\n\t\tinfo->TotalAllocationUnits = cpu_to_le64(stfs.f_blocks);\n\t\tinfo->FreeAllocationUnits = cpu_to_le64(stfs.f_bfree);\n\t\tinfo->SectorsPerAllocationUnit = cpu_to_le32(1);\n\t\tinfo->BytesPerSector = cpu_to_le32(stfs.f_bsize);\n\t\trsp->OutputBufferLength = cpu_to_le32(24);\n\t\tinc_rfc1001_len(work->response_buf, 24);\n\t\tfs_infoclass_size = FS_SIZE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_FULL_SIZE_INFORMATION:\n\t{\n\t\tstruct smb2_fs_full_size_info *info;\n\n\t\tinfo = (struct smb2_fs_full_size_info *)(rsp->Buffer);\n\t\tinfo->TotalAllocationUnits = cpu_to_le64(stfs.f_blocks);\n\t\tinfo->CallerAvailableAllocationUnits =\n\t\t\t\t\tcpu_to_le64(stfs.f_bavail);\n\t\tinfo->ActualAvailableAllocationUnits =\n\t\t\t\t\tcpu_to_le64(stfs.f_bfree);\n\t\tinfo->SectorsPerAllocationUnit = cpu_to_le32(1);\n\t\tinfo->BytesPerSector = cpu_to_le32(stfs.f_bsize);\n\t\trsp->OutputBufferLength = cpu_to_le32(32);\n\t\tinc_rfc1001_len(work->response_buf, 32);\n\t\tfs_infoclass_size = FS_FULL_SIZE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_OBJECT_ID_INFORMATION:\n\t{\n\t\tstruct object_id_info *info;\n\n\t\tinfo = (struct object_id_info *)(rsp->Buffer);\n\n\t\tif (!user_guest(sess->user))\n\t\t\tmemcpy(info->objid, user_passkey(sess->user), 16);\n\t\telse\n\t\t\tmemset(info->objid, 0, 16);\n\n\t\tinfo->extended_info.magic = cpu_to_le32(EXTENDED_INFO_MAGIC);\n\t\tinfo->extended_info.version = cpu_to_le32(1);\n\t\tinfo->extended_info.release = cpu_to_le32(1);\n\t\tinfo->extended_info.rel_date = 0;\n\t\tmemcpy(info->extended_info.version_string, \"1.1.0\", strlen(\"1.1.0\"));\n\t\trsp->OutputBufferLength = cpu_to_le32(64);\n\t\tinc_rfc1001_len(work->response_buf, 64);\n\t\tfs_infoclass_size = FS_OBJECT_ID_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_SECTOR_SIZE_INFORMATION:\n\t{\n\t\tstruct smb3_fs_ss_info *info;\n\t\tunsigned int sector_size =\n\t\t\tmin_t(unsigned int, path.mnt->mnt_sb->s_blocksize, 4096);\n\n\t\tinfo = (struct smb3_fs_ss_info *)(rsp->Buffer);\n\n\t\tinfo->LogicalBytesPerSector = cpu_to_le32(sector_size);\n\t\tinfo->PhysicalBytesPerSectorForAtomicity =\n\t\t\t\tcpu_to_le32(sector_size);\n\t\tinfo->PhysicalBytesPerSectorForPerf = cpu_to_le32(sector_size);\n\t\tinfo->FSEffPhysicalBytesPerSectorForAtomicity =\n\t\t\t\tcpu_to_le32(sector_size);\n\t\tinfo->Flags = cpu_to_le32(SSINFO_FLAGS_ALIGNED_DEVICE |\n\t\t\t\t    SSINFO_FLAGS_PARTITION_ALIGNED_ON_DEVICE);\n\t\tinfo->ByteOffsetForSectorAlignment = 0;\n\t\tinfo->ByteOffsetForPartitionAlignment = 0;\n\t\trsp->OutputBufferLength = cpu_to_le32(28);\n\t\tinc_rfc1001_len(work->response_buf, 28);\n\t\tfs_infoclass_size = FS_SECTOR_SIZE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_CONTROL_INFORMATION:\n\t{\n\t\t/*\n\t\t * TODO : The current implementation is based on\n\t\t * test result with win7(NTFS) server. It's need to\n\t\t * modify this to get valid Quota values\n\t\t * from Linux kernel\n\t\t */\n\t\tstruct smb2_fs_control_info *info;\n\n\t\tinfo = (struct smb2_fs_control_info *)(rsp->Buffer);\n\t\tinfo->FreeSpaceStartFiltering = 0;\n\t\tinfo->FreeSpaceThreshold = 0;\n\t\tinfo->FreeSpaceStopFiltering = 0;\n\t\tinfo->DefaultQuotaThreshold = cpu_to_le64(SMB2_NO_FID);\n\t\tinfo->DefaultQuotaLimit = cpu_to_le64(SMB2_NO_FID);\n\t\tinfo->Padding = 0;\n\t\trsp->OutputBufferLength = cpu_to_le32(48);\n\t\tinc_rfc1001_len(work->response_buf, 48);\n\t\tfs_infoclass_size = FS_CONTROL_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_POSIX_INFORMATION:\n\t{\n\t\tstruct filesystem_posix_info *info;\n\n\t\tif (!work->tcon->posix_extensions) {\n\t\t\tpr_err(\"client doesn't negotiate with SMB3.1.1 POSIX Extensions\\n\");\n\t\t\trc = -EOPNOTSUPP;\n\t\t} else {\n\t\t\tinfo = (struct filesystem_posix_info *)(rsp->Buffer);\n\t\t\tinfo->OptimalTransferSize = cpu_to_le32(stfs.f_bsize);\n\t\t\tinfo->BlockSize = cpu_to_le32(stfs.f_bsize);\n\t\t\tinfo->TotalBlocks = cpu_to_le64(stfs.f_blocks);\n\t\t\tinfo->BlocksAvail = cpu_to_le64(stfs.f_bfree);\n\t\t\tinfo->UserBlocksAvail = cpu_to_le64(stfs.f_bavail);\n\t\t\tinfo->TotalFileNodes = cpu_to_le64(stfs.f_files);\n\t\t\tinfo->FreeFileNodes = cpu_to_le64(stfs.f_ffree);\n\t\t\trsp->OutputBufferLength = cpu_to_le32(56);\n\t\t\tinc_rfc1001_len(work->response_buf, 56);\n\t\t\tfs_infoclass_size = FS_POSIX_INFORMATION_SIZE;\n\t\t}\n\t\tbreak;\n\t}\n\tdefault:\n\t\tpath_put(&path);\n\t\treturn -EOPNOTSUPP;\n\t}\n\trc = buffer_check_err(le32_to_cpu(req->OutputBufferLength),\n\t\t\t      rsp, work->response_buf,\n\t\t\t      fs_infoclass_size);\n\tpath_put(&path);\n\treturn rc;\n}",
                        "code_after_change": "static int smb2_get_info_filesystem(struct ksmbd_work *work,\n\t\t\t\t    struct smb2_query_info_req *req,\n\t\t\t\t    struct smb2_query_info_rsp *rsp)\n{\n\tstruct ksmbd_session *sess = work->sess;\n\tstruct ksmbd_conn *conn = work->conn;\n\tstruct ksmbd_share_config *share = work->tcon->share_conf;\n\tint fsinfoclass = 0;\n\tstruct kstatfs stfs;\n\tstruct path path;\n\tint rc = 0, len;\n\tint fs_infoclass_size = 0;\n\n\tif (!share->path)\n\t\treturn -EIO;\n\n\trc = kern_path(share->path, LOOKUP_NO_SYMLINKS, &path);\n\tif (rc) {\n\t\tpr_err(\"cannot create vfs path\\n\");\n\t\treturn -EIO;\n\t}\n\n\trc = vfs_statfs(&path, &stfs);\n\tif (rc) {\n\t\tpr_err(\"cannot do stat of path %s\\n\", share->path);\n\t\tpath_put(&path);\n\t\treturn -EIO;\n\t}\n\n\tfsinfoclass = req->FileInfoClass;\n\n\tswitch (fsinfoclass) {\n\tcase FS_DEVICE_INFORMATION:\n\t{\n\t\tstruct filesystem_device_info *info;\n\n\t\tinfo = (struct filesystem_device_info *)rsp->Buffer;\n\n\t\tinfo->DeviceType = cpu_to_le32(stfs.f_type);\n\t\tinfo->DeviceCharacteristics = cpu_to_le32(0x00000020);\n\t\trsp->OutputBufferLength = cpu_to_le32(8);\n\t\tinc_rfc1001_len(work->response_buf, 8);\n\t\tfs_infoclass_size = FS_DEVICE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_ATTRIBUTE_INFORMATION:\n\t{\n\t\tstruct filesystem_attribute_info *info;\n\t\tsize_t sz;\n\n\t\tinfo = (struct filesystem_attribute_info *)rsp->Buffer;\n\t\tinfo->Attributes = cpu_to_le32(FILE_SUPPORTS_OBJECT_IDS |\n\t\t\t\t\t       FILE_PERSISTENT_ACLS |\n\t\t\t\t\t       FILE_UNICODE_ON_DISK |\n\t\t\t\t\t       FILE_CASE_PRESERVED_NAMES |\n\t\t\t\t\t       FILE_CASE_SENSITIVE_SEARCH |\n\t\t\t\t\t       FILE_SUPPORTS_BLOCK_REFCOUNTING);\n\n\t\tinfo->Attributes |= cpu_to_le32(server_conf.share_fake_fscaps);\n\n\t\tif (test_share_config_flag(work->tcon->share_conf,\n\t\t    KSMBD_SHARE_FLAG_STREAMS))\n\t\t\tinfo->Attributes |= cpu_to_le32(FILE_NAMED_STREAMS);\n\n\t\tinfo->MaxPathNameComponentLength = cpu_to_le32(stfs.f_namelen);\n\t\tlen = smbConvertToUTF16((__le16 *)info->FileSystemName,\n\t\t\t\t\t\"NTFS\", PATH_MAX, conn->local_nls, 0);\n\t\tlen = len * 2;\n\t\tinfo->FileSystemNameLen = cpu_to_le32(len);\n\t\tsz = sizeof(struct filesystem_attribute_info) - 2 + len;\n\t\trsp->OutputBufferLength = cpu_to_le32(sz);\n\t\tinc_rfc1001_len(work->response_buf, sz);\n\t\tfs_infoclass_size = FS_ATTRIBUTE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_VOLUME_INFORMATION:\n\t{\n\t\tstruct filesystem_vol_info *info;\n\t\tsize_t sz;\n\t\tunsigned int serial_crc = 0;\n\n\t\tinfo = (struct filesystem_vol_info *)(rsp->Buffer);\n\t\tinfo->VolumeCreationTime = 0;\n\t\tserial_crc = crc32_le(serial_crc, share->name,\n\t\t\t\t      strlen(share->name));\n\t\tserial_crc = crc32_le(serial_crc, share->path,\n\t\t\t\t      strlen(share->path));\n\t\tserial_crc = crc32_le(serial_crc, ksmbd_netbios_name(),\n\t\t\t\t      strlen(ksmbd_netbios_name()));\n\t\t/* Taking dummy value of serial number*/\n\t\tinfo->SerialNumber = cpu_to_le32(serial_crc);\n\t\tlen = smbConvertToUTF16((__le16 *)info->VolumeLabel,\n\t\t\t\t\tshare->name, PATH_MAX,\n\t\t\t\t\tconn->local_nls, 0);\n\t\tlen = len * 2;\n\t\tinfo->VolumeLabelSize = cpu_to_le32(len);\n\t\tinfo->Reserved = 0;\n\t\tsz = sizeof(struct filesystem_vol_info) - 2 + len;\n\t\trsp->OutputBufferLength = cpu_to_le32(sz);\n\t\tinc_rfc1001_len(work->response_buf, sz);\n\t\tfs_infoclass_size = FS_VOLUME_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_SIZE_INFORMATION:\n\t{\n\t\tstruct filesystem_info *info;\n\n\t\tinfo = (struct filesystem_info *)(rsp->Buffer);\n\t\tinfo->TotalAllocationUnits = cpu_to_le64(stfs.f_blocks);\n\t\tinfo->FreeAllocationUnits = cpu_to_le64(stfs.f_bfree);\n\t\tinfo->SectorsPerAllocationUnit = cpu_to_le32(1);\n\t\tinfo->BytesPerSector = cpu_to_le32(stfs.f_bsize);\n\t\trsp->OutputBufferLength = cpu_to_le32(24);\n\t\tinc_rfc1001_len(work->response_buf, 24);\n\t\tfs_infoclass_size = FS_SIZE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_FULL_SIZE_INFORMATION:\n\t{\n\t\tstruct smb2_fs_full_size_info *info;\n\n\t\tinfo = (struct smb2_fs_full_size_info *)(rsp->Buffer);\n\t\tinfo->TotalAllocationUnits = cpu_to_le64(stfs.f_blocks);\n\t\tinfo->CallerAvailableAllocationUnits =\n\t\t\t\t\tcpu_to_le64(stfs.f_bavail);\n\t\tinfo->ActualAvailableAllocationUnits =\n\t\t\t\t\tcpu_to_le64(stfs.f_bfree);\n\t\tinfo->SectorsPerAllocationUnit = cpu_to_le32(1);\n\t\tinfo->BytesPerSector = cpu_to_le32(stfs.f_bsize);\n\t\trsp->OutputBufferLength = cpu_to_le32(32);\n\t\tinc_rfc1001_len(work->response_buf, 32);\n\t\tfs_infoclass_size = FS_FULL_SIZE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_OBJECT_ID_INFORMATION:\n\t{\n\t\tstruct object_id_info *info;\n\n\t\tinfo = (struct object_id_info *)(rsp->Buffer);\n\n\t\tif (!user_guest(sess->user))\n\t\t\tmemcpy(info->objid, user_passkey(sess->user), 16);\n\t\telse\n\t\t\tmemset(info->objid, 0, 16);\n\n\t\tinfo->extended_info.magic = cpu_to_le32(EXTENDED_INFO_MAGIC);\n\t\tinfo->extended_info.version = cpu_to_le32(1);\n\t\tinfo->extended_info.release = cpu_to_le32(1);\n\t\tinfo->extended_info.rel_date = 0;\n\t\tmemcpy(info->extended_info.version_string, \"1.1.0\", strlen(\"1.1.0\"));\n\t\trsp->OutputBufferLength = cpu_to_le32(64);\n\t\tinc_rfc1001_len(work->response_buf, 64);\n\t\tfs_infoclass_size = FS_OBJECT_ID_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_SECTOR_SIZE_INFORMATION:\n\t{\n\t\tstruct smb3_fs_ss_info *info;\n\t\tunsigned int sector_size =\n\t\t\tmin_t(unsigned int, path.mnt->mnt_sb->s_blocksize, 4096);\n\n\t\tinfo = (struct smb3_fs_ss_info *)(rsp->Buffer);\n\n\t\tinfo->LogicalBytesPerSector = cpu_to_le32(sector_size);\n\t\tinfo->PhysicalBytesPerSectorForAtomicity =\n\t\t\t\tcpu_to_le32(sector_size);\n\t\tinfo->PhysicalBytesPerSectorForPerf = cpu_to_le32(sector_size);\n\t\tinfo->FSEffPhysicalBytesPerSectorForAtomicity =\n\t\t\t\tcpu_to_le32(sector_size);\n\t\tinfo->Flags = cpu_to_le32(SSINFO_FLAGS_ALIGNED_DEVICE |\n\t\t\t\t    SSINFO_FLAGS_PARTITION_ALIGNED_ON_DEVICE);\n\t\tinfo->ByteOffsetForSectorAlignment = 0;\n\t\tinfo->ByteOffsetForPartitionAlignment = 0;\n\t\trsp->OutputBufferLength = cpu_to_le32(28);\n\t\tinc_rfc1001_len(work->response_buf, 28);\n\t\tfs_infoclass_size = FS_SECTOR_SIZE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_CONTROL_INFORMATION:\n\t{\n\t\t/*\n\t\t * TODO : The current implementation is based on\n\t\t * test result with win7(NTFS) server. It's need to\n\t\t * modify this to get valid Quota values\n\t\t * from Linux kernel\n\t\t */\n\t\tstruct smb2_fs_control_info *info;\n\n\t\tinfo = (struct smb2_fs_control_info *)(rsp->Buffer);\n\t\tinfo->FreeSpaceStartFiltering = 0;\n\t\tinfo->FreeSpaceThreshold = 0;\n\t\tinfo->FreeSpaceStopFiltering = 0;\n\t\tinfo->DefaultQuotaThreshold = cpu_to_le64(SMB2_NO_FID);\n\t\tinfo->DefaultQuotaLimit = cpu_to_le64(SMB2_NO_FID);\n\t\tinfo->Padding = 0;\n\t\trsp->OutputBufferLength = cpu_to_le32(48);\n\t\tinc_rfc1001_len(work->response_buf, 48);\n\t\tfs_infoclass_size = FS_CONTROL_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_POSIX_INFORMATION:\n\t{\n\t\tstruct filesystem_posix_info *info;\n\n\t\tif (!work->tcon->posix_extensions) {\n\t\t\tpr_err(\"client doesn't negotiate with SMB3.1.1 POSIX Extensions\\n\");\n\t\t\trc = -EOPNOTSUPP;\n\t\t} else {\n\t\t\tinfo = (struct filesystem_posix_info *)(rsp->Buffer);\n\t\t\tinfo->OptimalTransferSize = cpu_to_le32(stfs.f_bsize);\n\t\t\tinfo->BlockSize = cpu_to_le32(stfs.f_bsize);\n\t\t\tinfo->TotalBlocks = cpu_to_le64(stfs.f_blocks);\n\t\t\tinfo->BlocksAvail = cpu_to_le64(stfs.f_bfree);\n\t\t\tinfo->UserBlocksAvail = cpu_to_le64(stfs.f_bavail);\n\t\t\tinfo->TotalFileNodes = cpu_to_le64(stfs.f_files);\n\t\t\tinfo->FreeFileNodes = cpu_to_le64(stfs.f_ffree);\n\t\t\trsp->OutputBufferLength = cpu_to_le32(56);\n\t\t\tinc_rfc1001_len(work->response_buf, 56);\n\t\t\tfs_infoclass_size = FS_POSIX_INFORMATION_SIZE;\n\t\t}\n\t\tbreak;\n\t}\n\tdefault:\n\t\tpath_put(&path);\n\t\treturn -EOPNOTSUPP;\n\t}\n\trc = buffer_check_err(le32_to_cpu(req->OutputBufferLength),\n\t\t\t      rsp, work->response_buf,\n\t\t\t      fs_infoclass_size);\n\tpath_put(&path);\n\treturn rc;\n}",
                        "cve_id": "CVE-2023-32248"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int smb2_handle_negotiate(struct ksmbd_work *work)\n{\n\tstruct ksmbd_conn *conn = work->conn;\n\tstruct smb2_negotiate_req *req = smb2_get_msg(work->request_buf);\n\tstruct smb2_negotiate_rsp *rsp = smb2_get_msg(work->response_buf);\n\tint rc = 0;\n\tunsigned int smb2_buf_len, smb2_neg_size;\n\t__le32 status;\n\n\tksmbd_debug(SMB, \"Received negotiate request\\n\");\n\tconn->need_neg = false;\n\tif (ksmbd_conn_good(work)) {\n\t\tpr_err(\"conn->tcp_status is already in CifsGood State\\n\");\n\t\twork->send_no_response = 1;\n\t\treturn rc;\n\t}\n\n\tif (req->DialectCount == 0) {\n\t\tpr_err(\"malformed packet\\n\");\n\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tsmb2_buf_len = get_rfc1002_len(work->request_buf);\n\tsmb2_neg_size = offsetof(struct smb2_negotiate_req, Dialects);\n\tif (smb2_neg_size > smb2_buf_len) {\n\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tif (conn->dialect == SMB311_PROT_ID) {\n\t\tunsigned int nego_ctxt_off = le32_to_cpu(req->NegotiateContextOffset);\n\n\t\tif (smb2_buf_len < nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (smb2_neg_size > nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (smb2_neg_size + le16_to_cpu(req->DialectCount) * sizeof(__le16) >\n\t\t    nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t} else {\n\t\tif (smb2_neg_size + le16_to_cpu(req->DialectCount) * sizeof(__le16) >\n\t\t    smb2_buf_len) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\tconn->cli_cap = le32_to_cpu(req->Capabilities);\n\tswitch (conn->dialect) {\n\tcase SMB311_PROT_ID:\n\t\tconn->preauth_info =\n\t\t\tkzalloc(sizeof(struct preauth_integrity_info),\n\t\t\t\tGFP_KERNEL);\n\t\tif (!conn->preauth_info) {\n\t\t\trc = -ENOMEM;\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tstatus = deassemble_neg_contexts(conn, req,\n\t\t\t\t\t\t get_rfc1002_len(work->request_buf));\n\t\tif (status != STATUS_SUCCESS) {\n\t\t\tpr_err(\"deassemble_neg_contexts error(0x%x)\\n\",\n\t\t\t       status);\n\t\t\trsp->hdr.Status = status;\n\t\t\trc = -EINVAL;\n\t\t\tkfree(conn->preauth_info);\n\t\t\tconn->preauth_info = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\trc = init_smb3_11_server(conn);\n\t\tif (rc < 0) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\tkfree(conn->preauth_info);\n\t\t\tconn->preauth_info = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tksmbd_gen_preauth_integrity_hash(conn,\n\t\t\t\t\t\t work->request_buf,\n\t\t\t\t\t\t conn->preauth_info->Preauth_HashValue);\n\t\trsp->NegotiateContextOffset =\n\t\t\t\tcpu_to_le32(OFFSET_OF_NEG_CONTEXT);\n\t\tassemble_neg_contexts(conn, rsp, work->response_buf);\n\t\tbreak;\n\tcase SMB302_PROT_ID:\n\t\tinit_smb3_02_server(conn);\n\t\tbreak;\n\tcase SMB30_PROT_ID:\n\t\tinit_smb3_0_server(conn);\n\t\tbreak;\n\tcase SMB21_PROT_ID:\n\t\tinit_smb2_1_server(conn);\n\t\tbreak;\n\tcase SMB2X_PROT_ID:\n\tcase BAD_PROT_ID:\n\tdefault:\n\t\tksmbd_debug(SMB, \"Server dialect :0x%x not supported\\n\",\n\t\t\t    conn->dialect);\n\t\trsp->hdr.Status = STATUS_NOT_SUPPORTED;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\n\t/* For stats */\n\tconn->connection_type = conn->dialect;\n\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\tmemcpy(conn->ClientGUID, req->ClientGUID,\n\t\t\tSMB2_CLIENT_GUID_SIZE);\n\tconn->cli_sec_mode = le16_to_cpu(req->SecurityMode);\n\n\trsp->StructureSize = cpu_to_le16(65);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying server\n\t */\n\tmemset(rsp->ServerGUID, 0, SMB2_CLIENT_GUID_SIZE);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\tksmbd_debug(SMB, \"negotiate context offset %d, count %d\\n\",\n\t\t    le32_to_cpu(rsp->NegotiateContextOffset),\n\t\t    le16_to_cpu(rsp->NegotiateContextCount));\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\t\t\t  le16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf, sizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tconn->use_spnego = true;\n\n\tif ((server_conf.signing == KSMBD_CONFIG_OPT_AUTO ||\n\t     server_conf.signing == KSMBD_CONFIG_OPT_DISABLED) &&\n\t    req->SecurityMode & SMB2_NEGOTIATE_SIGNING_REQUIRED_LE)\n\t\tconn->sign = true;\n\telse if (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY) {\n\t\tserver_conf.enforced_signing = true;\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\t\tconn->sign = true;\n\t}\n\n\tconn->srv_sec_mode = le16_to_cpu(rsp->SecurityMode);\n\tksmbd_conn_set_need_negotiate(work);\n\nerr_out:\n\tif (rc < 0)\n\t\tsmb2_set_err_rsp(work);\n\n\treturn rc;\n}",
                        "code_after_change": "int smb2_handle_negotiate(struct ksmbd_work *work)\n{\n\tstruct ksmbd_conn *conn = work->conn;\n\tstruct smb2_negotiate_req *req = smb2_get_msg(work->request_buf);\n\tstruct smb2_negotiate_rsp *rsp = smb2_get_msg(work->response_buf);\n\tint rc = 0;\n\tunsigned int smb2_buf_len, smb2_neg_size;\n\t__le32 status;\n\n\tksmbd_debug(SMB, \"Received negotiate request\\n\");\n\tconn->need_neg = false;\n\tif (ksmbd_conn_good(conn)) {\n\t\tpr_err(\"conn->tcp_status is already in CifsGood State\\n\");\n\t\twork->send_no_response = 1;\n\t\treturn rc;\n\t}\n\n\tif (req->DialectCount == 0) {\n\t\tpr_err(\"malformed packet\\n\");\n\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tsmb2_buf_len = get_rfc1002_len(work->request_buf);\n\tsmb2_neg_size = offsetof(struct smb2_negotiate_req, Dialects);\n\tif (smb2_neg_size > smb2_buf_len) {\n\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tif (conn->dialect == SMB311_PROT_ID) {\n\t\tunsigned int nego_ctxt_off = le32_to_cpu(req->NegotiateContextOffset);\n\n\t\tif (smb2_buf_len < nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (smb2_neg_size > nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (smb2_neg_size + le16_to_cpu(req->DialectCount) * sizeof(__le16) >\n\t\t    nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t} else {\n\t\tif (smb2_neg_size + le16_to_cpu(req->DialectCount) * sizeof(__le16) >\n\t\t    smb2_buf_len) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\tconn->cli_cap = le32_to_cpu(req->Capabilities);\n\tswitch (conn->dialect) {\n\tcase SMB311_PROT_ID:\n\t\tconn->preauth_info =\n\t\t\tkzalloc(sizeof(struct preauth_integrity_info),\n\t\t\t\tGFP_KERNEL);\n\t\tif (!conn->preauth_info) {\n\t\t\trc = -ENOMEM;\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tstatus = deassemble_neg_contexts(conn, req,\n\t\t\t\t\t\t get_rfc1002_len(work->request_buf));\n\t\tif (status != STATUS_SUCCESS) {\n\t\t\tpr_err(\"deassemble_neg_contexts error(0x%x)\\n\",\n\t\t\t       status);\n\t\t\trsp->hdr.Status = status;\n\t\t\trc = -EINVAL;\n\t\t\tkfree(conn->preauth_info);\n\t\t\tconn->preauth_info = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\trc = init_smb3_11_server(conn);\n\t\tif (rc < 0) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\tkfree(conn->preauth_info);\n\t\t\tconn->preauth_info = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tksmbd_gen_preauth_integrity_hash(conn,\n\t\t\t\t\t\t work->request_buf,\n\t\t\t\t\t\t conn->preauth_info->Preauth_HashValue);\n\t\trsp->NegotiateContextOffset =\n\t\t\t\tcpu_to_le32(OFFSET_OF_NEG_CONTEXT);\n\t\tassemble_neg_contexts(conn, rsp, work->response_buf);\n\t\tbreak;\n\tcase SMB302_PROT_ID:\n\t\tinit_smb3_02_server(conn);\n\t\tbreak;\n\tcase SMB30_PROT_ID:\n\t\tinit_smb3_0_server(conn);\n\t\tbreak;\n\tcase SMB21_PROT_ID:\n\t\tinit_smb2_1_server(conn);\n\t\tbreak;\n\tcase SMB2X_PROT_ID:\n\tcase BAD_PROT_ID:\n\tdefault:\n\t\tksmbd_debug(SMB, \"Server dialect :0x%x not supported\\n\",\n\t\t\t    conn->dialect);\n\t\trsp->hdr.Status = STATUS_NOT_SUPPORTED;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\n\t/* For stats */\n\tconn->connection_type = conn->dialect;\n\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\tmemcpy(conn->ClientGUID, req->ClientGUID,\n\t\t\tSMB2_CLIENT_GUID_SIZE);\n\tconn->cli_sec_mode = le16_to_cpu(req->SecurityMode);\n\n\trsp->StructureSize = cpu_to_le16(65);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying server\n\t */\n\tmemset(rsp->ServerGUID, 0, SMB2_CLIENT_GUID_SIZE);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\tksmbd_debug(SMB, \"negotiate context offset %d, count %d\\n\",\n\t\t    le32_to_cpu(rsp->NegotiateContextOffset),\n\t\t    le16_to_cpu(rsp->NegotiateContextCount));\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\t\t\t  le16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf, sizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tconn->use_spnego = true;\n\n\tif ((server_conf.signing == KSMBD_CONFIG_OPT_AUTO ||\n\t     server_conf.signing == KSMBD_CONFIG_OPT_DISABLED) &&\n\t    req->SecurityMode & SMB2_NEGOTIATE_SIGNING_REQUIRED_LE)\n\t\tconn->sign = true;\n\telse if (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY) {\n\t\tserver_conf.enforced_signing = true;\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\t\tconn->sign = true;\n\t}\n\n\tconn->srv_sec_mode = le16_to_cpu(rsp->SecurityMode);\n\tksmbd_conn_set_need_negotiate(conn);\n\nerr_out:\n\tif (rc < 0)\n\t\tsmb2_set_err_rsp(work);\n\n\treturn rc;\n}",
                        "cve_id": "CVE-2023-32252"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,\n\t\t\t    int emulation_type, void *insn, int insn_len)\n{\n\tint r;\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tbool writeback = true;\n\tbool write_fault_to_spt;\n\n\tif (unlikely(!kvm_can_emulate_insn(vcpu, emulation_type, insn, insn_len)))\n\t\treturn 1;\n\n\tvcpu->arch.l1tf_flush_l1d = true;\n\n\t/*\n\t * Clear write_fault_to_shadow_pgtable here to ensure it is\n\t * never reused.\n\t */\n\twrite_fault_to_spt = vcpu->arch.write_fault_to_shadow_pgtable;\n\tvcpu->arch.write_fault_to_shadow_pgtable = false;\n\n\tif (!(emulation_type & EMULTYPE_NO_DECODE)) {\n\t\tkvm_clear_exception_queue(vcpu);\n\n\t\tr = x86_decode_emulated_instruction(vcpu, emulation_type,\n\t\t\t\t\t\t    insn, insn_len);\n\t\tif (r != EMULATION_OK)  {\n\t\t\tif ((emulation_type & EMULTYPE_TRAP_UD) ||\n\t\t\t    (emulation_type & EMULTYPE_TRAP_UD_FORCED)) {\n\t\t\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\tif (reexecute_instruction(vcpu, cr2_or_gpa,\n\t\t\t\t\t\t  write_fault_to_spt,\n\t\t\t\t\t\t  emulation_type))\n\t\t\t\treturn 1;\n\t\t\tif (ctxt->have_exception) {\n\t\t\t\t/*\n\t\t\t\t * #UD should result in just EMULATION_FAILED, and trap-like\n\t\t\t\t * exception should not be encountered during decode.\n\t\t\t\t */\n\t\t\t\tWARN_ON_ONCE(ctxt->exception.vector == UD_VECTOR ||\n\t\t\t\t\t     exception_type(ctxt->exception.vector) == EXCPT_TRAP);\n\t\t\t\tinject_emulated_exception(vcpu);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t\t}\n\t}\n\n\tif ((emulation_type & EMULTYPE_VMWARE_GP) &&\n\t    !is_vmware_backdoor_opcode(ctxt)) {\n\t\tkvm_queue_exception_e(vcpu, GP_VECTOR, 0);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * EMULTYPE_SKIP without EMULTYPE_COMPLETE_USER_EXIT is intended for\n\t * use *only* by vendor callbacks for kvm_skip_emulated_instruction().\n\t * The caller is responsible for updating interruptibility state and\n\t * injecting single-step #DBs.\n\t */\n\tif (emulation_type & EMULTYPE_SKIP) {\n\t\tif (ctxt->mode != X86EMUL_MODE_PROT64)\n\t\t\tctxt->eip = (u32)ctxt->_eip;\n\t\telse\n\t\t\tctxt->eip = ctxt->_eip;\n\n\t\tif (emulation_type & EMULTYPE_COMPLETE_USER_EXIT) {\n\t\t\tr = 1;\n\t\t\tgoto writeback;\n\t\t}\n\n\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\tif (ctxt->eflags & X86_EFLAGS_RF)\n\t\t\tkvm_set_rflags(vcpu, ctxt->eflags & ~X86_EFLAGS_RF);\n\t\treturn 1;\n\t}\n\n\tif (retry_instruction(ctxt, cr2_or_gpa, emulation_type))\n\t\treturn 1;\n\n\t/* this is needed for vmware backdoor interface to work since it\n\t   changes registers values  during IO operation */\n\tif (vcpu->arch.emulate_regs_need_sync_from_vcpu) {\n\t\tvcpu->arch.emulate_regs_need_sync_from_vcpu = false;\n\t\temulator_invalidate_register_cache(ctxt);\n\t}\n\nrestart:\n\tif (emulation_type & EMULTYPE_PF) {\n\t\t/* Save the faulting GPA (cr2) in the address field */\n\t\tctxt->exception.address = cr2_or_gpa;\n\n\t\t/* With shadow page tables, cr2 contains a GVA or nGPA. */\n\t\tif (vcpu->arch.mmu->root_role.direct) {\n\t\t\tctxt->gpa_available = true;\n\t\t\tctxt->gpa_val = cr2_or_gpa;\n\t\t}\n\t} else {\n\t\t/* Sanitize the address out of an abundance of paranoia. */\n\t\tctxt->exception.address = 0;\n\t}\n\n\tr = x86_emulate_insn(ctxt);\n\n\tif (r == EMULATION_INTERCEPTED)\n\t\treturn 1;\n\n\tif (r == EMULATION_FAILED) {\n\t\tif (reexecute_instruction(vcpu, cr2_or_gpa, write_fault_to_spt,\n\t\t\t\t\temulation_type))\n\t\t\treturn 1;\n\n\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t}\n\n\tif (ctxt->have_exception) {\n\t\tr = 1;\n\t\tif (inject_emulated_exception(vcpu))\n\t\t\treturn r;\n\t} else if (vcpu->arch.pio.count) {\n\t\tif (!vcpu->arch.pio.in) {\n\t\t\t/* FIXME: return into emulator if single-stepping.  */\n\t\t\tvcpu->arch.pio.count = 0;\n\t\t} else {\n\t\t\twriteback = false;\n\t\t\tvcpu->arch.complete_userspace_io = complete_emulated_pio;\n\t\t}\n\t\tr = 0;\n\t} else if (vcpu->mmio_needed) {\n\t\t++vcpu->stat.mmio_exits;\n\n\t\tif (!vcpu->mmio_is_write)\n\t\t\twriteback = false;\n\t\tr = 0;\n\t\tvcpu->arch.complete_userspace_io = complete_emulated_mmio;\n\t} else if (vcpu->arch.complete_userspace_io) {\n\t\twriteback = false;\n\t\tr = 0;\n\t} else if (r == EMULATION_RESTART)\n\t\tgoto restart;\n\telse\n\t\tr = 1;\n\nwriteback:\n\tif (writeback) {\n\t\tunsigned long rflags = static_call(kvm_x86_get_rflags)(vcpu);\n\t\ttoggle_interruptibility(vcpu, ctxt->interruptibility);\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = false;\n\t\tif (!ctxt->have_exception ||\n\t\t    exception_type(ctxt->exception.vector) == EXCPT_TRAP) {\n\t\t\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);\n\t\t\tif (ctxt->is_branch)\n\t\t\t\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);\n\t\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\t\tif (r && (ctxt->tf || (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)))\n\t\t\t\tr = kvm_vcpu_do_singlestep(vcpu);\n\t\t\tstatic_call_cond(kvm_x86_update_emulated_instruction)(vcpu);\n\t\t\t__kvm_set_rflags(vcpu, ctxt->eflags);\n\t\t}\n\n\t\t/*\n\t\t * For STI, interrupts are shadowed; so KVM_REQ_EVENT will\n\t\t * do nothing, and it will be requested again as soon as\n\t\t * the shadow expires.  But we still need to check here,\n\t\t * because POPF has no interrupt shadow.\n\t\t */\n\t\tif (unlikely((ctxt->eflags & ~rflags) & X86_EFLAGS_IF))\n\t\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t} else\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = true;\n\n\treturn r;\n}",
                        "code_after_change": "int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,\n\t\t\t    int emulation_type, void *insn, int insn_len)\n{\n\tint r;\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tbool writeback = true;\n\tbool write_fault_to_spt;\n\n\tif (unlikely(!kvm_can_emulate_insn(vcpu, emulation_type, insn, insn_len)))\n\t\treturn 1;\n\n\tvcpu->arch.l1tf_flush_l1d = true;\n\n\t/*\n\t * Clear write_fault_to_shadow_pgtable here to ensure it is\n\t * never reused.\n\t */\n\twrite_fault_to_spt = vcpu->arch.write_fault_to_shadow_pgtable;\n\tvcpu->arch.write_fault_to_shadow_pgtable = false;\n\n\tif (!(emulation_type & EMULTYPE_NO_DECODE)) {\n\t\tkvm_clear_exception_queue(vcpu);\n\n\t\t/*\n\t\t * Return immediately if RIP hits a code breakpoint, such #DBs\n\t\t * are fault-like and are higher priority than any faults on\n\t\t * the code fetch itself.\n\t\t */\n\t\tif (!(emulation_type & EMULTYPE_SKIP) &&\n\t\t    kvm_vcpu_check_code_breakpoint(vcpu, &r))\n\t\t\treturn r;\n\n\t\tr = x86_decode_emulated_instruction(vcpu, emulation_type,\n\t\t\t\t\t\t    insn, insn_len);\n\t\tif (r != EMULATION_OK)  {\n\t\t\tif ((emulation_type & EMULTYPE_TRAP_UD) ||\n\t\t\t    (emulation_type & EMULTYPE_TRAP_UD_FORCED)) {\n\t\t\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\tif (reexecute_instruction(vcpu, cr2_or_gpa,\n\t\t\t\t\t\t  write_fault_to_spt,\n\t\t\t\t\t\t  emulation_type))\n\t\t\t\treturn 1;\n\t\t\tif (ctxt->have_exception) {\n\t\t\t\t/*\n\t\t\t\t * #UD should result in just EMULATION_FAILED, and trap-like\n\t\t\t\t * exception should not be encountered during decode.\n\t\t\t\t */\n\t\t\t\tWARN_ON_ONCE(ctxt->exception.vector == UD_VECTOR ||\n\t\t\t\t\t     exception_type(ctxt->exception.vector) == EXCPT_TRAP);\n\t\t\t\tinject_emulated_exception(vcpu);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t\t}\n\t}\n\n\tif ((emulation_type & EMULTYPE_VMWARE_GP) &&\n\t    !is_vmware_backdoor_opcode(ctxt)) {\n\t\tkvm_queue_exception_e(vcpu, GP_VECTOR, 0);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * EMULTYPE_SKIP without EMULTYPE_COMPLETE_USER_EXIT is intended for\n\t * use *only* by vendor callbacks for kvm_skip_emulated_instruction().\n\t * The caller is responsible for updating interruptibility state and\n\t * injecting single-step #DBs.\n\t */\n\tif (emulation_type & EMULTYPE_SKIP) {\n\t\tif (ctxt->mode != X86EMUL_MODE_PROT64)\n\t\t\tctxt->eip = (u32)ctxt->_eip;\n\t\telse\n\t\t\tctxt->eip = ctxt->_eip;\n\n\t\tif (emulation_type & EMULTYPE_COMPLETE_USER_EXIT) {\n\t\t\tr = 1;\n\t\t\tgoto writeback;\n\t\t}\n\n\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\tif (ctxt->eflags & X86_EFLAGS_RF)\n\t\t\tkvm_set_rflags(vcpu, ctxt->eflags & ~X86_EFLAGS_RF);\n\t\treturn 1;\n\t}\n\n\tif (retry_instruction(ctxt, cr2_or_gpa, emulation_type))\n\t\treturn 1;\n\n\t/* this is needed for vmware backdoor interface to work since it\n\t   changes registers values  during IO operation */\n\tif (vcpu->arch.emulate_regs_need_sync_from_vcpu) {\n\t\tvcpu->arch.emulate_regs_need_sync_from_vcpu = false;\n\t\temulator_invalidate_register_cache(ctxt);\n\t}\n\nrestart:\n\tif (emulation_type & EMULTYPE_PF) {\n\t\t/* Save the faulting GPA (cr2) in the address field */\n\t\tctxt->exception.address = cr2_or_gpa;\n\n\t\t/* With shadow page tables, cr2 contains a GVA or nGPA. */\n\t\tif (vcpu->arch.mmu->root_role.direct) {\n\t\t\tctxt->gpa_available = true;\n\t\t\tctxt->gpa_val = cr2_or_gpa;\n\t\t}\n\t} else {\n\t\t/* Sanitize the address out of an abundance of paranoia. */\n\t\tctxt->exception.address = 0;\n\t}\n\n\tr = x86_emulate_insn(ctxt);\n\n\tif (r == EMULATION_INTERCEPTED)\n\t\treturn 1;\n\n\tif (r == EMULATION_FAILED) {\n\t\tif (reexecute_instruction(vcpu, cr2_or_gpa, write_fault_to_spt,\n\t\t\t\t\temulation_type))\n\t\t\treturn 1;\n\n\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t}\n\n\tif (ctxt->have_exception) {\n\t\tr = 1;\n\t\tif (inject_emulated_exception(vcpu))\n\t\t\treturn r;\n\t} else if (vcpu->arch.pio.count) {\n\t\tif (!vcpu->arch.pio.in) {\n\t\t\t/* FIXME: return into emulator if single-stepping.  */\n\t\t\tvcpu->arch.pio.count = 0;\n\t\t} else {\n\t\t\twriteback = false;\n\t\t\tvcpu->arch.complete_userspace_io = complete_emulated_pio;\n\t\t}\n\t\tr = 0;\n\t} else if (vcpu->mmio_needed) {\n\t\t++vcpu->stat.mmio_exits;\n\n\t\tif (!vcpu->mmio_is_write)\n\t\t\twriteback = false;\n\t\tr = 0;\n\t\tvcpu->arch.complete_userspace_io = complete_emulated_mmio;\n\t} else if (vcpu->arch.complete_userspace_io) {\n\t\twriteback = false;\n\t\tr = 0;\n\t} else if (r == EMULATION_RESTART)\n\t\tgoto restart;\n\telse\n\t\tr = 1;\n\nwriteback:\n\tif (writeback) {\n\t\tunsigned long rflags = static_call(kvm_x86_get_rflags)(vcpu);\n\t\ttoggle_interruptibility(vcpu, ctxt->interruptibility);\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = false;\n\t\tif (!ctxt->have_exception ||\n\t\t    exception_type(ctxt->exception.vector) == EXCPT_TRAP) {\n\t\t\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);\n\t\t\tif (ctxt->is_branch)\n\t\t\t\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);\n\t\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\t\tif (r && (ctxt->tf || (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)))\n\t\t\t\tr = kvm_vcpu_do_singlestep(vcpu);\n\t\t\tstatic_call_cond(kvm_x86_update_emulated_instruction)(vcpu);\n\t\t\t__kvm_set_rflags(vcpu, ctxt->eflags);\n\t\t}\n\n\t\t/*\n\t\t * For STI, interrupts are shadowed; so KVM_REQ_EVENT will\n\t\t * do nothing, and it will be requested again as soon as\n\t\t * the shadow expires.  But we still need to check here,\n\t\t * because POPF has no interrupt shadow.\n\t\t */\n\t\tif (unlikely((ctxt->eflags & ~rflags) & X86_EFLAGS_IF))\n\t\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t} else\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = true;\n\n\treturn r;\n}",
                        "cve_id": "CVE-2022-1852"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int amd_sfh_hid_client_init(struct amd_mp2_dev *privdata)\n{\n\tstruct amd_input_data *in_data = &privdata->in_data;\n\tstruct amdtp_cl_data *cl_data = privdata->cl_data;\n\tstruct amd_mp2_ops *mp2_ops = privdata->mp2_ops;\n\tstruct amd_mp2_sensor_info info;\n\tstruct request_list *req_list;\n\tstruct device *dev;\n\tu32 feature_report_size;\n\tu32 input_report_size;\n\tint rc, i, status;\n\tu8 cl_idx;\n\n\treq_list = &cl_data->req_list;\n\tdev = &privdata->pdev->dev;\n\tamd_sfh_set_desc_ops(mp2_ops);\n\n\tmp2_ops->suspend = amd_sfh_suspend;\n\tmp2_ops->resume = amd_sfh_resume;\n\n\tcl_data->num_hid_devices = amd_mp2_get_sensor_num(privdata, &cl_data->sensor_idx[0]);\n\tif (cl_data->num_hid_devices == 0)\n\t\treturn -ENODEV;\n\n\tINIT_DELAYED_WORK(&cl_data->work, amd_sfh_work);\n\tINIT_DELAYED_WORK(&cl_data->work_buffer, amd_sfh_work_buffer);\n\tINIT_LIST_HEAD(&req_list->list);\n\tcl_data->in_data = in_data;\n\n\tfor (i = 0; i < cl_data->num_hid_devices; i++) {\n\t\tin_data->sensor_virt_addr[i] = dma_alloc_coherent(dev, sizeof(int) * 8,\n\t\t\t\t\t\t\t\t  &cl_data->sensor_dma_addr[i],\n\t\t\t\t\t\t\t\t  GFP_KERNEL);\n\t\tcl_data->sensor_sts[i] = SENSOR_DISABLED;\n\t\tcl_data->sensor_requested_cnt[i] = 0;\n\t\tcl_data->cur_hid_dev = i;\n\t\tcl_idx = cl_data->sensor_idx[i];\n\t\tcl_data->report_descr_sz[i] = mp2_ops->get_desc_sz(cl_idx, descr_size);\n\t\tif (!cl_data->report_descr_sz[i]) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tfeature_report_size = mp2_ops->get_desc_sz(cl_idx, feature_size);\n\t\tif (!feature_report_size) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tinput_report_size =  mp2_ops->get_desc_sz(cl_idx, input_size);\n\t\tif (!input_report_size) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tcl_data->feature_report[i] = devm_kzalloc(dev, feature_report_size, GFP_KERNEL);\n\t\tif (!cl_data->feature_report[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tin_data->input_report[i] = devm_kzalloc(dev, input_report_size, GFP_KERNEL);\n\t\tif (!in_data->input_report[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tinfo.period = AMD_SFH_IDLE_LOOP;\n\t\tinfo.sensor_idx = cl_idx;\n\t\tinfo.dma_address = cl_data->sensor_dma_addr[i];\n\n\t\tcl_data->report_descr[i] =\n\t\t\tdevm_kzalloc(dev, cl_data->report_descr_sz[i], GFP_KERNEL);\n\t\tif (!cl_data->report_descr[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\trc = mp2_ops->get_rep_desc(cl_idx, cl_data->report_descr[i]);\n\t\tif (rc)\n\t\t\treturn rc;\n\t\tmp2_ops->start(privdata, info);\n\t\tstatus = amd_sfh_wait_for_response\n\t\t\t\t(privdata, cl_data->sensor_idx[i], SENSOR_ENABLED);\n\t\tif (status == SENSOR_ENABLED) {\n\t\t\tcl_data->sensor_sts[i] = SENSOR_ENABLED;\n\t\t\trc = amdtp_hid_probe(cl_data->cur_hid_dev, cl_data);\n\t\t\tif (rc) {\n\t\t\t\tmp2_ops->stop(privdata, cl_data->sensor_idx[i]);\n\t\t\t\tstatus = amd_sfh_wait_for_response\n\t\t\t\t\t(privdata, cl_data->sensor_idx[i], SENSOR_DISABLED);\n\t\t\t\tif (status != SENSOR_ENABLED)\n\t\t\t\t\tcl_data->sensor_sts[i] = SENSOR_DISABLED;\n\t\t\t\tdev_dbg(dev, \"sid 0x%x (%s) status 0x%x\\n\",\n\t\t\t\t\tcl_data->sensor_idx[i],\n\t\t\t\t\tget_sensor_name(cl_data->sensor_idx[i]),\n\t\t\t\t\tcl_data->sensor_sts[i]);\n\t\t\t\tgoto cleanup;\n\t\t\t}\n\t\t}\n\t\tdev_dbg(dev, \"sid 0x%x (%s) status 0x%x\\n\",\n\t\t\tcl_data->sensor_idx[i], get_sensor_name(cl_data->sensor_idx[i]),\n\t\t\tcl_data->sensor_sts[i]);\n\t}\n\tif (mp2_ops->discovery_status && mp2_ops->discovery_status(privdata) == 0) {\n\t\tamd_sfh_hid_client_deinit(privdata);\n\t\tfor (i = 0; i < cl_data->num_hid_devices; i++) {\n\t\t\tdevm_kfree(dev, cl_data->feature_report[i]);\n\t\t\tdevm_kfree(dev, in_data->input_report[i]);\n\t\t\tdevm_kfree(dev, cl_data->report_descr[i]);\n\t\t}\n\t\tdev_warn(dev, \"Failed to discover, sensors not enabled\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\tschedule_delayed_work(&cl_data->work_buffer, msecs_to_jiffies(AMD_SFH_IDLE_LOOP));\n\treturn 0;\n\ncleanup:\n\tfor (i = 0; i < cl_data->num_hid_devices; i++) {\n\t\tif (in_data->sensor_virt_addr[i]) {\n\t\t\tdma_free_coherent(&privdata->pdev->dev, 8 * sizeof(int),\n\t\t\t\t\t  in_data->sensor_virt_addr[i],\n\t\t\t\t\t  cl_data->sensor_dma_addr[i]);\n\t\t}\n\t\tdevm_kfree(dev, cl_data->feature_report[i]);\n\t\tdevm_kfree(dev, in_data->input_report[i]);\n\t\tdevm_kfree(dev, cl_data->report_descr[i]);\n\t}\n\treturn rc;\n}",
                        "code_after_change": "int amd_sfh_hid_client_init(struct amd_mp2_dev *privdata)\n{\n\tstruct amd_input_data *in_data = &privdata->in_data;\n\tstruct amdtp_cl_data *cl_data = privdata->cl_data;\n\tstruct amd_mp2_ops *mp2_ops = privdata->mp2_ops;\n\tstruct amd_mp2_sensor_info info;\n\tstruct request_list *req_list;\n\tstruct device *dev;\n\tu32 feature_report_size;\n\tu32 input_report_size;\n\tint rc, i, status;\n\tu8 cl_idx;\n\n\treq_list = &cl_data->req_list;\n\tdev = &privdata->pdev->dev;\n\tamd_sfh_set_desc_ops(mp2_ops);\n\n\tmp2_ops->suspend = amd_sfh_suspend;\n\tmp2_ops->resume = amd_sfh_resume;\n\n\tcl_data->num_hid_devices = amd_mp2_get_sensor_num(privdata, &cl_data->sensor_idx[0]);\n\tif (cl_data->num_hid_devices == 0)\n\t\treturn -ENODEV;\n\n\tINIT_DELAYED_WORK(&cl_data->work, amd_sfh_work);\n\tINIT_DELAYED_WORK(&cl_data->work_buffer, amd_sfh_work_buffer);\n\tINIT_LIST_HEAD(&req_list->list);\n\tcl_data->in_data = in_data;\n\n\tfor (i = 0; i < cl_data->num_hid_devices; i++) {\n\t\tin_data->sensor_virt_addr[i] = dma_alloc_coherent(dev, sizeof(int) * 8,\n\t\t\t\t\t\t\t\t  &cl_data->sensor_dma_addr[i],\n\t\t\t\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!in_data->sensor_virt_addr[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tcl_data->sensor_sts[i] = SENSOR_DISABLED;\n\t\tcl_data->sensor_requested_cnt[i] = 0;\n\t\tcl_data->cur_hid_dev = i;\n\t\tcl_idx = cl_data->sensor_idx[i];\n\t\tcl_data->report_descr_sz[i] = mp2_ops->get_desc_sz(cl_idx, descr_size);\n\t\tif (!cl_data->report_descr_sz[i]) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tfeature_report_size = mp2_ops->get_desc_sz(cl_idx, feature_size);\n\t\tif (!feature_report_size) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tinput_report_size =  mp2_ops->get_desc_sz(cl_idx, input_size);\n\t\tif (!input_report_size) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tcl_data->feature_report[i] = devm_kzalloc(dev, feature_report_size, GFP_KERNEL);\n\t\tif (!cl_data->feature_report[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tin_data->input_report[i] = devm_kzalloc(dev, input_report_size, GFP_KERNEL);\n\t\tif (!in_data->input_report[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tinfo.period = AMD_SFH_IDLE_LOOP;\n\t\tinfo.sensor_idx = cl_idx;\n\t\tinfo.dma_address = cl_data->sensor_dma_addr[i];\n\n\t\tcl_data->report_descr[i] =\n\t\t\tdevm_kzalloc(dev, cl_data->report_descr_sz[i], GFP_KERNEL);\n\t\tif (!cl_data->report_descr[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\trc = mp2_ops->get_rep_desc(cl_idx, cl_data->report_descr[i]);\n\t\tif (rc)\n\t\t\treturn rc;\n\t\tmp2_ops->start(privdata, info);\n\t\tstatus = amd_sfh_wait_for_response\n\t\t\t\t(privdata, cl_data->sensor_idx[i], SENSOR_ENABLED);\n\t\tif (status == SENSOR_ENABLED) {\n\t\t\tcl_data->sensor_sts[i] = SENSOR_ENABLED;\n\t\t\trc = amdtp_hid_probe(cl_data->cur_hid_dev, cl_data);\n\t\t\tif (rc) {\n\t\t\t\tmp2_ops->stop(privdata, cl_data->sensor_idx[i]);\n\t\t\t\tstatus = amd_sfh_wait_for_response\n\t\t\t\t\t(privdata, cl_data->sensor_idx[i], SENSOR_DISABLED);\n\t\t\t\tif (status != SENSOR_ENABLED)\n\t\t\t\t\tcl_data->sensor_sts[i] = SENSOR_DISABLED;\n\t\t\t\tdev_dbg(dev, \"sid 0x%x (%s) status 0x%x\\n\",\n\t\t\t\t\tcl_data->sensor_idx[i],\n\t\t\t\t\tget_sensor_name(cl_data->sensor_idx[i]),\n\t\t\t\t\tcl_data->sensor_sts[i]);\n\t\t\t\tgoto cleanup;\n\t\t\t}\n\t\t}\n\t\tdev_dbg(dev, \"sid 0x%x (%s) status 0x%x\\n\",\n\t\t\tcl_data->sensor_idx[i], get_sensor_name(cl_data->sensor_idx[i]),\n\t\t\tcl_data->sensor_sts[i]);\n\t}\n\tif (mp2_ops->discovery_status && mp2_ops->discovery_status(privdata) == 0) {\n\t\tamd_sfh_hid_client_deinit(privdata);\n\t\tfor (i = 0; i < cl_data->num_hid_devices; i++) {\n\t\t\tdevm_kfree(dev, cl_data->feature_report[i]);\n\t\t\tdevm_kfree(dev, in_data->input_report[i]);\n\t\t\tdevm_kfree(dev, cl_data->report_descr[i]);\n\t\t}\n\t\tdev_warn(dev, \"Failed to discover, sensors not enabled\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\tschedule_delayed_work(&cl_data->work_buffer, msecs_to_jiffies(AMD_SFH_IDLE_LOOP));\n\treturn 0;\n\ncleanup:\n\tfor (i = 0; i < cl_data->num_hid_devices; i++) {\n\t\tif (in_data->sensor_virt_addr[i]) {\n\t\t\tdma_free_coherent(&privdata->pdev->dev, 8 * sizeof(int),\n\t\t\t\t\t  in_data->sensor_virt_addr[i],\n\t\t\t\t\t  cl_data->sensor_dma_addr[i]);\n\t\t}\n\t\tdevm_kfree(dev, cl_data->feature_report[i]);\n\t\tdevm_kfree(dev, in_data->input_report[i]);\n\t\tdevm_kfree(dev, cl_data->report_descr[i]);\n\t}\n\treturn rc;\n}",
                        "cve_id": "CVE-2023-3357"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 4226,
            "cve_id": "CVE-2023-3358",
            "code_snippet": "void ishtp_cl_release_dma_acked_mem(struct ishtp_device *dev,\n\t\t\t\t    void *msg_addr,\n\t\t\t\t    uint8_t size)\n{\n\tunsigned long\tflags;\n\tint acked_slots = (size / DMA_SLOT_SIZE)\n\t\t+ 1 * (size % DMA_SLOT_SIZE != 0);\n\tint i, j;\n\n\tif ((msg_addr - dev->ishtp_host_dma_tx_buf) % DMA_SLOT_SIZE) {\n\t\tdev_err(dev->devc, \"Bad DMA Tx ack address\\n\");\n\t\treturn;\n\t}\n\n\ti = (msg_addr - dev->ishtp_host_dma_tx_buf) / DMA_SLOT_SIZE;\n\tspin_lock_irqsave(&dev->ishtp_dma_tx_lock, flags);\n\tfor (j = 0; j < acked_slots; j++) {\n\t\tif ((i + j) >= dev->ishtp_dma_num_slots ||\n\t\t\t\t\t!dev->ishtp_dma_tx_map[i+j]) {\n\t\t\t/* no such slot, or memory is already free */\n\t\t\tspin_unlock_irqrestore(&dev->ishtp_dma_tx_lock, flags);\n\t\t\tdev_err(dev->devc, \"Bad DMA Tx ack address\\n\");\n\t\t\treturn;\n\t\t}\n\t\tdev->ishtp_dma_tx_map[i+j] = 0;\n\t}\n\tspin_unlock_irqrestore(&dev->ishtp_dma_tx_lock, flags);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "void *ishtp_cl_get_dma_send_buf(struct ishtp_device *dev,\n\t\t\t\tuint32_t size)\n{\n\tunsigned long\tflags;\n\tint i, j, free;\n\t/* additional slot is needed if there is rem */\n\tint required_slots = (size / DMA_SLOT_SIZE)\n\t\t+ 1 * (size % DMA_SLOT_SIZE != 0);\n\n\tspin_lock_irqsave(&dev->ishtp_dma_tx_lock, flags);\n\tfor (i = 0; i <= (dev->ishtp_dma_num_slots - required_slots); i++) {\n\t\tfree = 1;\n\t\tfor (j = 0; j < required_slots; j++)\n\t\t\tif (dev->ishtp_dma_tx_map[i+j]) {\n\t\t\t\tfree = 0;\n\t\t\t\ti += j;\n\t\t\t\tbreak;\n\t\t\t}\n\t\tif (free) {\n\t\t\t/* mark memory as \"caught\" */\n\t\t\tfor (j = 0; j < required_slots; j++)\n\t\t\t\tdev->ishtp_dma_tx_map[i+j] = 1;\n\t\t\tspin_unlock_irqrestore(&dev->ishtp_dma_tx_lock, flags);\n\t\t\treturn (i * DMA_SLOT_SIZE) +\n\t\t\t\t(unsigned char *)dev->ishtp_host_dma_tx_buf;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&dev->ishtp_dma_tx_lock, flags);\n\tdev_err(dev->devc, \"No free DMA buffer to send msg\\n\");\n\treturn NULL;\n}",
                        "code_after_change": "void *ishtp_cl_get_dma_send_buf(struct ishtp_device *dev,\n\t\t\t\tuint32_t size)\n{\n\tunsigned long\tflags;\n\tint i, j, free;\n\t/* additional slot is needed if there is rem */\n\tint required_slots = (size / DMA_SLOT_SIZE)\n\t\t+ 1 * (size % DMA_SLOT_SIZE != 0);\n\n\tif (!dev->ishtp_dma_tx_map) {\n\t\tdev_err(dev->devc, \"Fail to allocate Tx map\\n\");\n\t\treturn NULL;\n\t}\n\n\tspin_lock_irqsave(&dev->ishtp_dma_tx_lock, flags);\n\tfor (i = 0; i <= (dev->ishtp_dma_num_slots - required_slots); i++) {\n\t\tfree = 1;\n\t\tfor (j = 0; j < required_slots; j++)\n\t\t\tif (dev->ishtp_dma_tx_map[i+j]) {\n\t\t\t\tfree = 0;\n\t\t\t\ti += j;\n\t\t\t\tbreak;\n\t\t\t}\n\t\tif (free) {\n\t\t\t/* mark memory as \"caught\" */\n\t\t\tfor (j = 0; j < required_slots; j++)\n\t\t\t\tdev->ishtp_dma_tx_map[i+j] = 1;\n\t\t\tspin_unlock_irqrestore(&dev->ishtp_dma_tx_lock, flags);\n\t\t\treturn (i * DMA_SLOT_SIZE) +\n\t\t\t\t(unsigned char *)dev->ishtp_host_dma_tx_buf;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&dev->ishtp_dma_tx_lock, flags);\n\tdev_err(dev->devc, \"No free DMA buffer to send msg\\n\");\n\treturn NULL;\n}",
                        "cve_id": "CVE-2023-3358"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        }
    ],
    "non_vul_data": [
        {
            "id": 661,
            "cve_id": "CVE-2014-7826",
            "code_snippet": "static void perf_syscall_exit(void *ignore, struct pt_regs *regs, long ret)\n{\n\tstruct syscall_metadata *sys_data;\n\tstruct syscall_trace_exit *rec;\n\tstruct hlist_head *head;\n\tint syscall_nr;\n\tint rctx;\n\tint size;\n\n\tsyscall_nr = trace_get_syscall_nr(current, regs);\n\tif (syscall_nr < 0 || syscall_nr >= NR_syscalls)\n\t\treturn;\n\tif (!test_bit(syscall_nr, enabled_perf_exit_syscalls))\n\t\treturn;\n\n\tsys_data = syscall_nr_to_meta(syscall_nr);\n\tif (!sys_data)\n\t\treturn;\n\n\thead = this_cpu_ptr(sys_data->exit_event->perf_events);\n\tif (hlist_empty(head))\n\t\treturn;\n\n\t/* We can probably do that at build time */\n\tsize = ALIGN(sizeof(*rec) + sizeof(u32), sizeof(u64));\n\tsize -= sizeof(u32);\n\n\trec = (struct syscall_trace_exit *)perf_trace_buf_prepare(size,\n\t\t\t\tsys_data->exit_event->event.type, regs, &rctx);\n\tif (!rec)\n\t\treturn;\n\n\trec->nr = syscall_nr;\n\trec->ret = syscall_get_return_value(current, regs);\n\tperf_trace_buf_submit(rec, size, rctx, 0, 1, regs, head, NULL);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static void perf_syscall_enter(void *ignore, struct pt_regs *regs, long id)\n{\n\tstruct syscall_metadata *sys_data;\n\tstruct syscall_trace_enter *rec;\n\tstruct hlist_head *head;\n\tint syscall_nr;\n\tint rctx;\n\tint size;\n\n\tsyscall_nr = trace_get_syscall_nr(current, regs);\n\tif (syscall_nr < 0)\n\t\treturn;\n\tif (!test_bit(syscall_nr, enabled_perf_enter_syscalls))\n\t\treturn;\n\n\tsys_data = syscall_nr_to_meta(syscall_nr);\n\tif (!sys_data)\n\t\treturn;\n\n\thead = this_cpu_ptr(sys_data->enter_event->perf_events);\n\tif (hlist_empty(head))\n\t\treturn;\n\n\t/* get the size after alignment with the u32 buffer size field */\n\tsize = sizeof(unsigned long) * sys_data->nb_args + sizeof(*rec);\n\tsize = ALIGN(size + sizeof(u32), sizeof(u64));\n\tsize -= sizeof(u32);\n\n\trec = (struct syscall_trace_enter *)perf_trace_buf_prepare(size,\n\t\t\t\tsys_data->enter_event->event.type, regs, &rctx);\n\tif (!rec)\n\t\treturn;\n\n\trec->nr = syscall_nr;\n\tsyscall_get_arguments(current, regs, 0, sys_data->nb_args,\n\t\t\t       (unsigned long *)&rec->args);\n\tperf_trace_buf_submit(rec, size, rctx, 0, 1, regs, head, NULL);\n}",
                        "code_after_change": "static void perf_syscall_enter(void *ignore, struct pt_regs *regs, long id)\n{\n\tstruct syscall_metadata *sys_data;\n\tstruct syscall_trace_enter *rec;\n\tstruct hlist_head *head;\n\tint syscall_nr;\n\tint rctx;\n\tint size;\n\n\tsyscall_nr = trace_get_syscall_nr(current, regs);\n\tif (syscall_nr < 0 || syscall_nr >= NR_syscalls)\n\t\treturn;\n\tif (!test_bit(syscall_nr, enabled_perf_enter_syscalls))\n\t\treturn;\n\n\tsys_data = syscall_nr_to_meta(syscall_nr);\n\tif (!sys_data)\n\t\treturn;\n\n\thead = this_cpu_ptr(sys_data->enter_event->perf_events);\n\tif (hlist_empty(head))\n\t\treturn;\n\n\t/* get the size after alignment with the u32 buffer size field */\n\tsize = sizeof(unsigned long) * sys_data->nb_args + sizeof(*rec);\n\tsize = ALIGN(size + sizeof(u32), sizeof(u64));\n\tsize -= sizeof(u32);\n\n\trec = (struct syscall_trace_enter *)perf_trace_buf_prepare(size,\n\t\t\t\tsys_data->enter_event->event.type, regs, &rctx);\n\tif (!rec)\n\t\treturn;\n\n\trec->nr = syscall_nr;\n\tsyscall_get_arguments(current, regs, 0, sys_data->nb_args,\n\t\t\t       (unsigned long *)&rec->args);\n\tperf_trace_buf_submit(rec, size, rctx, 0, 1, regs, head, NULL);\n}",
                        "cve_id": "CVE-2014-7826"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 944,
            "cve_id": "CVE-2015-8970",
            "code_snippet": "static void skcipher_release(void *private)\n{\n\tstruct skcipher_tfm *tfm = private;\n\n\tcrypto_free_skcipher(tfm->skcipher);\n\tkfree(tfm);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int crypto_skcipher_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_skcipher *skcipher = __crypto_skcipher_cast(tfm);\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(skcipher);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_blkcipher_type)\n\t\treturn crypto_init_skcipher_ops_blkcipher(tfm);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_ablkcipher_type ||\n\t    tfm->__crt_alg->cra_type == &crypto_givcipher_type)\n\t\treturn crypto_init_skcipher_ops_ablkcipher(tfm);\n\n\tskcipher->setkey = alg->setkey;\n\tskcipher->encrypt = alg->encrypt;\n\tskcipher->decrypt = alg->decrypt;\n\tskcipher->ivsize = alg->ivsize;\n\tskcipher->keysize = alg->max_keysize;\n\n\tif (alg->exit)\n\t\tskcipher->base.exit = crypto_skcipher_exit_tfm;\n\n\tif (alg->init)\n\t\treturn alg->init(skcipher);\n\n\treturn 0;\n}",
                        "code_after_change": "static int skcipher_setkey(void *private, const u8 *key, unsigned int keylen)\n{\n\tstruct skcipher_tfm *tfm = private;\n\tint err;\n\n\terr = crypto_skcipher_setkey(tfm->skcipher, key, keylen);\n\ttfm->has_key = !err;\n\n\treturn err;\n}",
                        "cve_id": "CVE-2017-9211"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int crypto_rng_reset(struct crypto_rng *tfm, const u8 *seed, unsigned int slen)\n{\n\tu8 *buf = NULL;\n\tint err;\n\n\tif (!seed && slen) {\n\t\tbuf = kmalloc(slen, GFP_KERNEL);\n\t\tif (!buf)\n\t\t\treturn -ENOMEM;\n\n\t\tget_random_bytes(buf, slen);\n\t\tseed = buf;\n\t}\n\n\terr = tfm->seed(tfm, seed, slen);\n\n\tkfree(buf);\n\treturn err;\n}",
                        "code_after_change": "static int skcipher_accept_parent(void *private, struct sock *sk)\n{\n\tstruct skcipher_ctx *ctx;\n\tstruct alg_sock *ask = alg_sk(sk);\n\tstruct skcipher_tfm *tfm = private;\n\tstruct crypto_skcipher *skcipher = tfm->skcipher;\n\tunsigned int len = sizeof(*ctx) + crypto_skcipher_reqsize(skcipher);\n\n\tif (!tfm->has_key)\n\t\treturn -ENOKEY;\n\n\tctx = sock_kmalloc(sk, len, GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\tctx->iv = sock_kmalloc(sk, crypto_skcipher_ivsize(skcipher),\n\t\t\t       GFP_KERNEL);\n\tif (!ctx->iv) {\n\t\tsock_kfree_s(sk, ctx, len);\n\t\treturn -ENOMEM;\n\t}\n\n\tmemset(ctx->iv, 0, crypto_skcipher_ivsize(skcipher));\n\n\tINIT_LIST_HEAD(&ctx->tsgl);\n\tctx->len = len;\n\tctx->used = 0;\n\tctx->more = 0;\n\tctx->merge = 0;\n\tctx->enc = 0;\n\tatomic_set(&ctx->inflight, 0);\n\taf_alg_init_completion(&ctx->completion);\n\n\task->private = ctx;\n\n\tskcipher_request_set_tfm(&ctx->req, skcipher);\n\tskcipher_request_set_callback(&ctx->req, CRYPTO_TFM_REQ_MAY_BACKLOG,\n\t\t\t\t      af_alg_complete, &ctx->completion);\n\n\tsk->sk_destruct = skcipher_sock_destruct;\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2017-15116"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static struct mac802154_llsec_key*\nllsec_key_alloc(const struct ieee802154_llsec_key *template)\n{\n\tconst int authsizes[3] = { 4, 8, 16 };\n\tstruct mac802154_llsec_key *key;\n\tint i;\n\n\tkey = kzalloc(sizeof(*key), GFP_KERNEL);\n\tif (!key)\n\t\treturn NULL;\n\n\tkref_init(&key->ref);\n\tkey->key = *template;\n\n\tBUILD_BUG_ON(ARRAY_SIZE(authsizes) != ARRAY_SIZE(key->tfm));\n\n\tfor (i = 0; i < ARRAY_SIZE(key->tfm); i++) {\n\t\tkey->tfm[i] = crypto_alloc_aead(\"ccm(aes)\", 0,\n\t\t\t\t\t\tCRYPTO_ALG_ASYNC);\n\t\tif (IS_ERR(key->tfm[i]))\n\t\t\tgoto err_tfm;\n\t\tif (crypto_aead_setkey(key->tfm[i], template->key,\n\t\t\t\t       IEEE802154_LLSEC_KEY_SIZE))\n\t\t\tgoto err_tfm;\n\t\tif (crypto_aead_setauthsize(key->tfm[i], authsizes[i]))\n\t\t\tgoto err_tfm;\n\t}\n\n\tkey->tfm0 = crypto_alloc_sync_skcipher(\"ctr(aes)\", 0, 0);\n\tif (IS_ERR(key->tfm0))\n\t\tgoto err_tfm;\n\n\tif (crypto_sync_skcipher_setkey(key->tfm0, template->key,\n\t\t\t\t   IEEE802154_LLSEC_KEY_SIZE))\n\t\tgoto err_tfm0;\n\n\treturn key;\n\nerr_tfm0:\n\tcrypto_free_sync_skcipher(key->tfm0);\nerr_tfm:\n\tfor (i = 0; i < ARRAY_SIZE(key->tfm); i++)\n\t\tif (key->tfm[i])\n\t\t\tcrypto_free_aead(key->tfm[i]);\n\n\tkfree_sensitive(key);\n\treturn NULL;\n}",
                        "code_after_change": "static void *skcipher_bind(const char *name, u32 type, u32 mask)\n{\n\tstruct skcipher_tfm *tfm;\n\tstruct crypto_skcipher *skcipher;\n\n\ttfm = kzalloc(sizeof(*tfm), GFP_KERNEL);\n\tif (!tfm)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tskcipher = crypto_alloc_skcipher(name, type, mask);\n\tif (IS_ERR(skcipher)) {\n\t\tkfree(tfm);\n\t\treturn ERR_CAST(skcipher);\n\t}\n\n\ttfm->skcipher = skcipher;\n\n\treturn tfm;\n}",
                        "cve_id": "CVE-2021-3659"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static inline int crypto_rng_generate(struct crypto_rng *tfm,\n\t\t\t\t      const u8 *src, unsigned int slen,\n\t\t\t\t      u8 *dst, unsigned int dlen)\n{\n\treturn tfm->generate(tfm, src, slen, dst, dlen);\n}",
                        "code_after_change": "static int crypto_skcipher_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_skcipher *skcipher = __crypto_skcipher_cast(tfm);\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(skcipher);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_blkcipher_type)\n\t\treturn crypto_init_skcipher_ops_blkcipher(tfm);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_ablkcipher_type ||\n\t    tfm->__crt_alg->cra_type == &crypto_givcipher_type)\n\t\treturn crypto_init_skcipher_ops_ablkcipher(tfm);\n\n\tskcipher->setkey = skcipher_setkey;\n\tskcipher->encrypt = alg->encrypt;\n\tskcipher->decrypt = alg->decrypt;\n\tskcipher->ivsize = alg->ivsize;\n\tskcipher->keysize = alg->max_keysize;\n\n\tif (alg->exit)\n\t\tskcipher->base.exit = crypto_skcipher_exit_tfm;\n\n\tif (alg->init)\n\t\treturn alg->init(skcipher);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2017-15116"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static inline int crypto_rng_seedsize(struct crypto_rng *tfm)\n{\n\treturn tfm->seedsize;\n}",
                        "code_after_change": "int crypto_rng_reset(struct crypto_rng *tfm, const u8 *seed, unsigned int slen)\n{\n\tu8 *buf = NULL;\n\tint err;\n\n\tif (!seed && slen) {\n\t\tbuf = kmalloc(slen, GFP_KERNEL);\n\t\tif (!buf)\n\t\t\treturn -ENOMEM;\n\n\t\tget_random_bytes(buf, slen);\n\t\tseed = buf;\n\t}\n\n\terr = crypto_rng_alg(tfm)->seed(tfm, seed, slen);\n\n\tkfree(buf);\n\treturn err;\n}",
                        "cve_id": "CVE-2017-15116"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 963,
            "cve_id": "CVE-2016-10147",
            "code_snippet": "static inline bool mcryptd_check_internal(struct rtattr **tb, u32 *type,\n\t\t\t\t\t  u32 *mask)\n{\n\tstruct crypto_attr_type *algt;\n\n\talgt = crypto_get_attr_type(tb);\n\tif (IS_ERR(algt))\n\t\treturn false;\n\n\t*type |= algt->type & CRYPTO_ALG_INTERNAL;\n\t*mask |= algt->mask & CRYPTO_ALG_INTERNAL;\n\n\tif (*type & *mask & CRYPTO_ALG_INTERNAL)\n\t\treturn true;\n\telse\n\t\treturn false;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int mcryptd_create_hash(struct crypto_template *tmpl, struct rtattr **tb,\n\t\t\t      struct mcryptd_queue *queue)\n{\n\tstruct hashd_instance_ctx *ctx;\n\tstruct ahash_instance *inst;\n\tstruct hash_alg_common *halg;\n\tstruct crypto_alg *alg;\n\tu32 type = 0;\n\tu32 mask = 0;\n\tint err;\n\n\tmcryptd_check_internal(tb, &type, &mask);\n\n\thalg = ahash_attr_alg(tb[1], type, mask);\n\tif (IS_ERR(halg))\n\t\treturn PTR_ERR(halg);\n\n\talg = &halg->base;\n\tpr_debug(\"crypto: mcryptd hash alg: %s\\n\", alg->cra_name);\n\tinst = mcryptd_alloc_instance(alg, ahash_instance_headroom(),\n\t\t\t\t\tsizeof(*ctx));\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tctx = ahash_instance_ctx(inst);\n\tctx->queue = queue;\n\n\terr = crypto_init_ahash_spawn(&ctx->spawn, halg,\n\t\t\t\t      ahash_crypto_instance(inst));\n\tif (err)\n\t\tgoto out_free_inst;\n\n\ttype = CRYPTO_ALG_ASYNC;\n\tif (alg->cra_flags & CRYPTO_ALG_INTERNAL)\n\t\ttype |= CRYPTO_ALG_INTERNAL;\n\tinst->alg.halg.base.cra_flags = type;\n\n\tinst->alg.halg.digestsize = halg->digestsize;\n\tinst->alg.halg.statesize = halg->statesize;\n\tinst->alg.halg.base.cra_ctxsize = sizeof(struct mcryptd_hash_ctx);\n\n\tinst->alg.halg.base.cra_init = mcryptd_hash_init_tfm;\n\tinst->alg.halg.base.cra_exit = mcryptd_hash_exit_tfm;\n\n\tinst->alg.init   = mcryptd_hash_init_enqueue;\n\tinst->alg.update = mcryptd_hash_update_enqueue;\n\tinst->alg.final  = mcryptd_hash_final_enqueue;\n\tinst->alg.finup  = mcryptd_hash_finup_enqueue;\n\tinst->alg.export = mcryptd_hash_export;\n\tinst->alg.import = mcryptd_hash_import;\n\tinst->alg.setkey = mcryptd_hash_setkey;\n\tinst->alg.digest = mcryptd_hash_digest_enqueue;\n\n\terr = ahash_register_instance(tmpl, inst);\n\tif (err) {\n\t\tcrypto_drop_ahash(&ctx->spawn);\nout_free_inst:\n\t\tkfree(inst);\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}",
                        "code_after_change": "static int mcryptd_create_hash(struct crypto_template *tmpl, struct rtattr **tb,\n\t\t\t      struct mcryptd_queue *queue)\n{\n\tstruct hashd_instance_ctx *ctx;\n\tstruct ahash_instance *inst;\n\tstruct hash_alg_common *halg;\n\tstruct crypto_alg *alg;\n\tu32 type = 0;\n\tu32 mask = 0;\n\tint err;\n\n\tif (!mcryptd_check_internal(tb, &type, &mask))\n\t\treturn -EINVAL;\n\n\thalg = ahash_attr_alg(tb[1], type, mask);\n\tif (IS_ERR(halg))\n\t\treturn PTR_ERR(halg);\n\n\talg = &halg->base;\n\tpr_debug(\"crypto: mcryptd hash alg: %s\\n\", alg->cra_name);\n\tinst = mcryptd_alloc_instance(alg, ahash_instance_headroom(),\n\t\t\t\t\tsizeof(*ctx));\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tctx = ahash_instance_ctx(inst);\n\tctx->queue = queue;\n\n\terr = crypto_init_ahash_spawn(&ctx->spawn, halg,\n\t\t\t\t      ahash_crypto_instance(inst));\n\tif (err)\n\t\tgoto out_free_inst;\n\n\ttype = CRYPTO_ALG_ASYNC;\n\tif (alg->cra_flags & CRYPTO_ALG_INTERNAL)\n\t\ttype |= CRYPTO_ALG_INTERNAL;\n\tinst->alg.halg.base.cra_flags = type;\n\n\tinst->alg.halg.digestsize = halg->digestsize;\n\tinst->alg.halg.statesize = halg->statesize;\n\tinst->alg.halg.base.cra_ctxsize = sizeof(struct mcryptd_hash_ctx);\n\n\tinst->alg.halg.base.cra_init = mcryptd_hash_init_tfm;\n\tinst->alg.halg.base.cra_exit = mcryptd_hash_exit_tfm;\n\n\tinst->alg.init   = mcryptd_hash_init_enqueue;\n\tinst->alg.update = mcryptd_hash_update_enqueue;\n\tinst->alg.final  = mcryptd_hash_final_enqueue;\n\tinst->alg.finup  = mcryptd_hash_finup_enqueue;\n\tinst->alg.export = mcryptd_hash_export;\n\tinst->alg.import = mcryptd_hash_import;\n\tinst->alg.setkey = mcryptd_hash_setkey;\n\tinst->alg.digest = mcryptd_hash_digest_enqueue;\n\n\terr = ahash_register_instance(tmpl, inst);\n\tif (err) {\n\t\tcrypto_drop_ahash(&ctx->spawn);\nout_free_inst:\n\t\tkfree(inst);\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}",
                        "cve_id": "CVE-2016-10147"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1028,
            "cve_id": "CVE-2016-3070",
            "code_snippet": "void migrate_page_copy(struct page *newpage, struct page *page)\n{\n\tint cpupid;\n\n\tif (PageHuge(page) || PageTransHuge(page))\n\t\tcopy_huge_page(newpage, page);\n\telse\n\t\tcopy_highpage(newpage, page);\n\n\tif (PageError(page))\n\t\tSetPageError(newpage);\n\tif (PageReferenced(page))\n\t\tSetPageReferenced(newpage);\n\tif (PageUptodate(page))\n\t\tSetPageUptodate(newpage);\n\tif (TestClearPageActive(page)) {\n\t\tVM_BUG_ON_PAGE(PageUnevictable(page), page);\n\t\tSetPageActive(newpage);\n\t} else if (TestClearPageUnevictable(page))\n\t\tSetPageUnevictable(newpage);\n\tif (PageChecked(page))\n\t\tSetPageChecked(newpage);\n\tif (PageMappedToDisk(page))\n\t\tSetPageMappedToDisk(newpage);\n\n\t/* Move dirty on pages not done by migrate_page_move_mapping() */\n\tif (PageDirty(page))\n\t\tSetPageDirty(newpage);\n\n\tif (page_is_young(page))\n\t\tset_page_young(newpage);\n\tif (page_is_idle(page))\n\t\tset_page_idle(newpage);\n\n\t/*\n\t * Copy NUMA information to the new page, to prevent over-eager\n\t * future migrations of this same page.\n\t */\n\tcpupid = page_cpupid_xchg_last(page, -1);\n\tpage_cpupid_xchg_last(newpage, cpupid);\n\n\tksm_migrate_page(newpage, page);\n\t/*\n\t * Please do not reorder this without considering how mm/ksm.c's\n\t * get_ksm_page() depends upon ksm_migrate_page() and PageSwapCache().\n\t */\n\tif (PageSwapCache(page))\n\t\tClearPageSwapCache(page);\n\tClearPagePrivate(page);\n\tset_page_private(page, 0);\n\n\t/*\n\t * If any waiters have accumulated on the new page then\n\t * wake them up.\n\t */\n\tif (PageWriteback(newpage))\n\t\tend_page_writeback(newpage);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int migrate_page_move_mapping(struct address_space *mapping,\n\t\tstruct page *newpage, struct page *page,\n\t\tstruct buffer_head *head, enum migrate_mode mode,\n\t\tint extra_count)\n{\n\tint expected_count = 1 + extra_count;\n\tvoid **pslot;\n\n\tif (!mapping) {\n\t\t/* Anonymous page without mapping */\n\t\tif (page_count(page) != expected_count)\n\t\t\treturn -EAGAIN;\n\n\t\t/* No turning back from here */\n\t\tset_page_memcg(newpage, page_memcg(page));\n\t\tnewpage->index = page->index;\n\t\tnewpage->mapping = page->mapping;\n\t\tif (PageSwapBacked(page))\n\t\t\tSetPageSwapBacked(newpage);\n\n\t\treturn MIGRATEPAGE_SUCCESS;\n\t}\n\n\tspin_lock_irq(&mapping->tree_lock);\n\n\tpslot = radix_tree_lookup_slot(&mapping->page_tree,\n \t\t\t\t\tpage_index(page));\n\n\texpected_count += 1 + page_has_private(page);\n\tif (page_count(page) != expected_count ||\n\t\tradix_tree_deref_slot_protected(pslot, &mapping->tree_lock) != page) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\tif (!page_freeze_refs(page, expected_count)) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * In the async migration case of moving a page with buffers, lock the\n\t * buffers using trylock before the mapping is moved. If the mapping\n\t * was moved, we later failed to lock the buffers and could not move\n\t * the mapping back due to an elevated page count, we would have to\n\t * block waiting on other references to be dropped.\n\t */\n\tif (mode == MIGRATE_ASYNC && head &&\n\t\t\t!buffer_migrate_lock_buffers(head, mode)) {\n\t\tpage_unfreeze_refs(page, expected_count);\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * Now we know that no one else is looking at the page:\n\t * no turning back from here.\n\t */\n\tset_page_memcg(newpage, page_memcg(page));\n\tnewpage->index = page->index;\n\tnewpage->mapping = page->mapping;\n\tif (PageSwapBacked(page))\n\t\tSetPageSwapBacked(newpage);\n\n\tget_page(newpage);\t/* add cache reference */\n\tif (PageSwapCache(page)) {\n\t\tSetPageSwapCache(newpage);\n\t\tset_page_private(newpage, page_private(page));\n\t}\n\n\tradix_tree_replace_slot(pslot, newpage);\n\n\t/*\n\t * Drop cache reference from old page by unfreezing\n\t * to one less reference.\n\t * We know this isn't the last reference.\n\t */\n\tpage_unfreeze_refs(page, expected_count - 1);\n\n\t/*\n\t * If moved to a different zone then also account\n\t * the page for that zone. Other VM counters will be\n\t * taken care of when we establish references to the\n\t * new page and drop references to the old page.\n\t *\n\t * Note that anonymous pages are accounted for\n\t * via NR_FILE_PAGES and NR_ANON_PAGES if they\n\t * are mapped to swap space.\n\t */\n\t__dec_zone_page_state(page, NR_FILE_PAGES);\n\t__inc_zone_page_state(newpage, NR_FILE_PAGES);\n\tif (!PageSwapCache(page) && PageSwapBacked(page)) {\n\t\t__dec_zone_page_state(page, NR_SHMEM);\n\t\t__inc_zone_page_state(newpage, NR_SHMEM);\n\t}\n\tspin_unlock_irq(&mapping->tree_lock);\n\n\treturn MIGRATEPAGE_SUCCESS;\n}",
                        "code_after_change": "int migrate_page_move_mapping(struct address_space *mapping,\n\t\tstruct page *newpage, struct page *page,\n\t\tstruct buffer_head *head, enum migrate_mode mode,\n\t\tint extra_count)\n{\n\tstruct zone *oldzone, *newzone;\n\tint dirty;\n\tint expected_count = 1 + extra_count;\n\tvoid **pslot;\n\n\tif (!mapping) {\n\t\t/* Anonymous page without mapping */\n\t\tif (page_count(page) != expected_count)\n\t\t\treturn -EAGAIN;\n\n\t\t/* No turning back from here */\n\t\tset_page_memcg(newpage, page_memcg(page));\n\t\tnewpage->index = page->index;\n\t\tnewpage->mapping = page->mapping;\n\t\tif (PageSwapBacked(page))\n\t\t\tSetPageSwapBacked(newpage);\n\n\t\treturn MIGRATEPAGE_SUCCESS;\n\t}\n\n\toldzone = page_zone(page);\n\tnewzone = page_zone(newpage);\n\n\tspin_lock_irq(&mapping->tree_lock);\n\n\tpslot = radix_tree_lookup_slot(&mapping->page_tree,\n \t\t\t\t\tpage_index(page));\n\n\texpected_count += 1 + page_has_private(page);\n\tif (page_count(page) != expected_count ||\n\t\tradix_tree_deref_slot_protected(pslot, &mapping->tree_lock) != page) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\tif (!page_freeze_refs(page, expected_count)) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * In the async migration case of moving a page with buffers, lock the\n\t * buffers using trylock before the mapping is moved. If the mapping\n\t * was moved, we later failed to lock the buffers and could not move\n\t * the mapping back due to an elevated page count, we would have to\n\t * block waiting on other references to be dropped.\n\t */\n\tif (mode == MIGRATE_ASYNC && head &&\n\t\t\t!buffer_migrate_lock_buffers(head, mode)) {\n\t\tpage_unfreeze_refs(page, expected_count);\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * Now we know that no one else is looking at the page:\n\t * no turning back from here.\n\t */\n\tset_page_memcg(newpage, page_memcg(page));\n\tnewpage->index = page->index;\n\tnewpage->mapping = page->mapping;\n\tif (PageSwapBacked(page))\n\t\tSetPageSwapBacked(newpage);\n\n\tget_page(newpage);\t/* add cache reference */\n\tif (PageSwapCache(page)) {\n\t\tSetPageSwapCache(newpage);\n\t\tset_page_private(newpage, page_private(page));\n\t}\n\n\t/* Move dirty while page refs frozen and newpage not yet exposed */\n\tdirty = PageDirty(page);\n\tif (dirty) {\n\t\tClearPageDirty(page);\n\t\tSetPageDirty(newpage);\n\t}\n\n\tradix_tree_replace_slot(pslot, newpage);\n\n\t/*\n\t * Drop cache reference from old page by unfreezing\n\t * to one less reference.\n\t * We know this isn't the last reference.\n\t */\n\tpage_unfreeze_refs(page, expected_count - 1);\n\n\tspin_unlock(&mapping->tree_lock);\n\t/* Leave irq disabled to prevent preemption while updating stats */\n\n\t/*\n\t * If moved to a different zone then also account\n\t * the page for that zone. Other VM counters will be\n\t * taken care of when we establish references to the\n\t * new page and drop references to the old page.\n\t *\n\t * Note that anonymous pages are accounted for\n\t * via NR_FILE_PAGES and NR_ANON_PAGES if they\n\t * are mapped to swap space.\n\t */\n\tif (newzone != oldzone) {\n\t\t__dec_zone_state(oldzone, NR_FILE_PAGES);\n\t\t__inc_zone_state(newzone, NR_FILE_PAGES);\n\t\tif (PageSwapBacked(page) && !PageSwapCache(page)) {\n\t\t\t__dec_zone_state(oldzone, NR_SHMEM);\n\t\t\t__inc_zone_state(newzone, NR_SHMEM);\n\t\t}\n\t\tif (dirty && mapping_cap_account_dirty(mapping)) {\n\t\t\t__dec_zone_state(oldzone, NR_FILE_DIRTY);\n\t\t\t__inc_zone_state(newzone, NR_FILE_DIRTY);\n\t\t}\n\t}\n\tlocal_irq_enable();\n\n\treturn MIGRATEPAGE_SUCCESS;\n}",
                        "cve_id": "CVE-2016-3070"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1372,
            "cve_id": "CVE-2017-15116",
            "code_snippet": "static int crypto_rng_init_tfm(struct crypto_tfm *tfm)\n{\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int crypto_skcipher_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_skcipher *skcipher = __crypto_skcipher_cast(tfm);\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(skcipher);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_blkcipher_type)\n\t\treturn crypto_init_skcipher_ops_blkcipher(tfm);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_ablkcipher_type ||\n\t    tfm->__crt_alg->cra_type == &crypto_givcipher_type)\n\t\treturn crypto_init_skcipher_ops_ablkcipher(tfm);\n\n\tskcipher->setkey = alg->setkey;\n\tskcipher->encrypt = alg->encrypt;\n\tskcipher->decrypt = alg->decrypt;\n\tskcipher->ivsize = alg->ivsize;\n\tskcipher->keysize = alg->max_keysize;\n\n\tif (alg->exit)\n\t\tskcipher->base.exit = crypto_skcipher_exit_tfm;\n\n\tif (alg->init)\n\t\treturn alg->init(skcipher);\n\n\treturn 0;\n}",
                        "code_after_change": "static int crypto_skcipher_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_skcipher *skcipher = __crypto_skcipher_cast(tfm);\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(skcipher);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_blkcipher_type)\n\t\treturn crypto_init_skcipher_ops_blkcipher(tfm);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_ablkcipher_type ||\n\t    tfm->__crt_alg->cra_type == &crypto_givcipher_type)\n\t\treturn crypto_init_skcipher_ops_ablkcipher(tfm);\n\n\tskcipher->setkey = skcipher_setkey;\n\tskcipher->encrypt = alg->encrypt;\n\tskcipher->decrypt = alg->decrypt;\n\tskcipher->ivsize = alg->ivsize;\n\tskcipher->keysize = alg->max_keysize;\n\n\tif (alg->exit)\n\t\tskcipher->base.exit = crypto_skcipher_exit_tfm;\n\n\tif (alg->init)\n\t\treturn alg->init(skcipher);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2017-9211"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static struct mac802154_llsec_key*\nllsec_key_alloc(const struct ieee802154_llsec_key *template)\n{\n\tconst int authsizes[3] = { 4, 8, 16 };\n\tstruct mac802154_llsec_key *key;\n\tint i;\n\n\tkey = kzalloc(sizeof(*key), GFP_KERNEL);\n\tif (!key)\n\t\treturn NULL;\n\n\tkref_init(&key->ref);\n\tkey->key = *template;\n\n\tBUILD_BUG_ON(ARRAY_SIZE(authsizes) != ARRAY_SIZE(key->tfm));\n\n\tfor (i = 0; i < ARRAY_SIZE(key->tfm); i++) {\n\t\tkey->tfm[i] = crypto_alloc_aead(\"ccm(aes)\", 0,\n\t\t\t\t\t\tCRYPTO_ALG_ASYNC);\n\t\tif (IS_ERR(key->tfm[i]))\n\t\t\tgoto err_tfm;\n\t\tif (crypto_aead_setkey(key->tfm[i], template->key,\n\t\t\t\t       IEEE802154_LLSEC_KEY_SIZE))\n\t\t\tgoto err_tfm;\n\t\tif (crypto_aead_setauthsize(key->tfm[i], authsizes[i]))\n\t\t\tgoto err_tfm;\n\t}\n\n\tkey->tfm0 = crypto_alloc_sync_skcipher(\"ctr(aes)\", 0, 0);\n\tif (IS_ERR(key->tfm0))\n\t\tgoto err_tfm;\n\n\tif (crypto_sync_skcipher_setkey(key->tfm0, template->key,\n\t\t\t\t   IEEE802154_LLSEC_KEY_SIZE))\n\t\tgoto err_tfm0;\n\n\treturn key;\n\nerr_tfm0:\n\tcrypto_free_sync_skcipher(key->tfm0);\nerr_tfm:\n\tfor (i = 0; i < ARRAY_SIZE(key->tfm); i++)\n\t\tif (key->tfm[i])\n\t\t\tcrypto_free_aead(key->tfm[i]);\n\n\tkfree_sensitive(key);\n\treturn NULL;\n}",
                        "code_after_change": "static struct mac802154_llsec_key*\nllsec_key_alloc(const struct ieee802154_llsec_key *template)\n{\n\tconst int authsizes[3] = { 4, 8, 16 };\n\tstruct mac802154_llsec_key *key;\n\tint i;\n\n\tkey = kzalloc(sizeof(*key), GFP_KERNEL);\n\tif (!key)\n\t\treturn NULL;\n\n\tkref_init(&key->ref);\n\tkey->key = *template;\n\n\tBUILD_BUG_ON(ARRAY_SIZE(authsizes) != ARRAY_SIZE(key->tfm));\n\n\tfor (i = 0; i < ARRAY_SIZE(key->tfm); i++) {\n\t\tkey->tfm[i] = crypto_alloc_aead(\"ccm(aes)\", 0,\n\t\t\t\t\t\tCRYPTO_ALG_ASYNC);\n\t\tif (IS_ERR(key->tfm[i]))\n\t\t\tgoto err_tfm;\n\t\tif (crypto_aead_setkey(key->tfm[i], template->key,\n\t\t\t\t       IEEE802154_LLSEC_KEY_SIZE))\n\t\t\tgoto err_tfm;\n\t\tif (crypto_aead_setauthsize(key->tfm[i], authsizes[i]))\n\t\t\tgoto err_tfm;\n\t}\n\n\tkey->tfm0 = crypto_alloc_sync_skcipher(\"ctr(aes)\", 0, 0);\n\tif (IS_ERR(key->tfm0))\n\t\tgoto err_tfm;\n\n\tif (crypto_sync_skcipher_setkey(key->tfm0, template->key,\n\t\t\t\t   IEEE802154_LLSEC_KEY_SIZE))\n\t\tgoto err_tfm0;\n\n\treturn key;\n\nerr_tfm0:\n\tcrypto_free_sync_skcipher(key->tfm0);\nerr_tfm:\n\tfor (i = 0; i < ARRAY_SIZE(key->tfm); i++)\n\t\tif (!IS_ERR_OR_NULL(key->tfm[i]))\n\t\t\tcrypto_free_aead(key->tfm[i]);\n\n\tkfree_sensitive(key);\n\treturn NULL;\n}",
                        "cve_id": "CVE-2021-3659"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static inline int crypto_rng_generate(struct crypto_rng *tfm,\n\t\t\t\t      const u8 *src, unsigned int slen,\n\t\t\t\t      u8 *dst, unsigned int dlen)\n{\n\treturn tfm->generate(tfm, src, slen, dst, dlen);\n}",
                        "code_after_change": "static inline int crypto_rng_generate(struct crypto_rng *tfm,\n\t\t\t\t      const u8 *src, unsigned int slen,\n\t\t\t\t      u8 *dst, unsigned int dlen)\n{\n\treturn crypto_rng_alg(tfm)->generate(tfm, src, slen, dst, dlen);\n}",
                        "cve_id": "CVE-2017-15116"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static inline int crypto_rng_seedsize(struct crypto_rng *tfm)\n{\n\treturn tfm->seedsize;\n}",
                        "code_after_change": "static int skcipher_setkey(void *private, const u8 *key, unsigned int keylen)\n{\n\tstruct skcipher_tfm *tfm = private;\n\tint err;\n\n\terr = crypto_skcipher_setkey(tfm->skcipher, key, keylen);\n\ttfm->has_key = !err;\n\n\treturn err;\n}",
                        "cve_id": "CVE-2017-15116"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int crypto_rng_reset(struct crypto_rng *tfm, const u8 *seed, unsigned int slen)\n{\n\tu8 *buf = NULL;\n\tint err;\n\n\tif (!seed && slen) {\n\t\tbuf = kmalloc(slen, GFP_KERNEL);\n\t\tif (!buf)\n\t\t\treturn -ENOMEM;\n\n\t\tget_random_bytes(buf, slen);\n\t\tseed = buf;\n\t}\n\n\terr = tfm->seed(tfm, seed, slen);\n\n\tkfree(buf);\n\treturn err;\n}",
                        "code_after_change": "static void *skcipher_bind(const char *name, u32 type, u32 mask)\n{\n\tstruct skcipher_tfm *tfm;\n\tstruct crypto_skcipher *skcipher;\n\n\ttfm = kzalloc(sizeof(*tfm), GFP_KERNEL);\n\tif (!tfm)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tskcipher = crypto_alloc_skcipher(name, type, mask);\n\tif (IS_ERR(skcipher)) {\n\t\tkfree(tfm);\n\t\treturn ERR_CAST(skcipher);\n\t}\n\n\ttfm->skcipher = skcipher;\n\n\treturn tfm;\n}",
                        "cve_id": "CVE-2017-15116"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 1373,
            "cve_id": "CVE-2017-15116",
            "code_snippet": "static unsigned int seedsize(struct crypto_alg *alg)\n{\n\tstruct rng_alg *ralg = container_of(alg, struct rng_alg, base);\n\n\treturn ralg->seedsize;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int mcryptd_create_hash(struct crypto_template *tmpl, struct rtattr **tb,\n\t\t\t      struct mcryptd_queue *queue)\n{\n\tstruct hashd_instance_ctx *ctx;\n\tstruct ahash_instance *inst;\n\tstruct hash_alg_common *halg;\n\tstruct crypto_alg *alg;\n\tu32 type = 0;\n\tu32 mask = 0;\n\tint err;\n\n\tmcryptd_check_internal(tb, &type, &mask);\n\n\thalg = ahash_attr_alg(tb[1], type, mask);\n\tif (IS_ERR(halg))\n\t\treturn PTR_ERR(halg);\n\n\talg = &halg->base;\n\tpr_debug(\"crypto: mcryptd hash alg: %s\\n\", alg->cra_name);\n\tinst = mcryptd_alloc_instance(alg, ahash_instance_headroom(),\n\t\t\t\t\tsizeof(*ctx));\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tctx = ahash_instance_ctx(inst);\n\tctx->queue = queue;\n\n\terr = crypto_init_ahash_spawn(&ctx->spawn, halg,\n\t\t\t\t      ahash_crypto_instance(inst));\n\tif (err)\n\t\tgoto out_free_inst;\n\n\ttype = CRYPTO_ALG_ASYNC;\n\tif (alg->cra_flags & CRYPTO_ALG_INTERNAL)\n\t\ttype |= CRYPTO_ALG_INTERNAL;\n\tinst->alg.halg.base.cra_flags = type;\n\n\tinst->alg.halg.digestsize = halg->digestsize;\n\tinst->alg.halg.statesize = halg->statesize;\n\tinst->alg.halg.base.cra_ctxsize = sizeof(struct mcryptd_hash_ctx);\n\n\tinst->alg.halg.base.cra_init = mcryptd_hash_init_tfm;\n\tinst->alg.halg.base.cra_exit = mcryptd_hash_exit_tfm;\n\n\tinst->alg.init   = mcryptd_hash_init_enqueue;\n\tinst->alg.update = mcryptd_hash_update_enqueue;\n\tinst->alg.final  = mcryptd_hash_final_enqueue;\n\tinst->alg.finup  = mcryptd_hash_finup_enqueue;\n\tinst->alg.export = mcryptd_hash_export;\n\tinst->alg.import = mcryptd_hash_import;\n\tinst->alg.setkey = mcryptd_hash_setkey;\n\tinst->alg.digest = mcryptd_hash_digest_enqueue;\n\n\terr = ahash_register_instance(tmpl, inst);\n\tif (err) {\n\t\tcrypto_drop_ahash(&ctx->spawn);\nout_free_inst:\n\t\tkfree(inst);\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}",
                        "code_after_change": "static int mcryptd_create_hash(struct crypto_template *tmpl, struct rtattr **tb,\n\t\t\t      struct mcryptd_queue *queue)\n{\n\tstruct hashd_instance_ctx *ctx;\n\tstruct ahash_instance *inst;\n\tstruct hash_alg_common *halg;\n\tstruct crypto_alg *alg;\n\tu32 type = 0;\n\tu32 mask = 0;\n\tint err;\n\n\tif (!mcryptd_check_internal(tb, &type, &mask))\n\t\treturn -EINVAL;\n\n\thalg = ahash_attr_alg(tb[1], type, mask);\n\tif (IS_ERR(halg))\n\t\treturn PTR_ERR(halg);\n\n\talg = &halg->base;\n\tpr_debug(\"crypto: mcryptd hash alg: %s\\n\", alg->cra_name);\n\tinst = mcryptd_alloc_instance(alg, ahash_instance_headroom(),\n\t\t\t\t\tsizeof(*ctx));\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\tctx = ahash_instance_ctx(inst);\n\tctx->queue = queue;\n\n\terr = crypto_init_ahash_spawn(&ctx->spawn, halg,\n\t\t\t\t      ahash_crypto_instance(inst));\n\tif (err)\n\t\tgoto out_free_inst;\n\n\ttype = CRYPTO_ALG_ASYNC;\n\tif (alg->cra_flags & CRYPTO_ALG_INTERNAL)\n\t\ttype |= CRYPTO_ALG_INTERNAL;\n\tinst->alg.halg.base.cra_flags = type;\n\n\tinst->alg.halg.digestsize = halg->digestsize;\n\tinst->alg.halg.statesize = halg->statesize;\n\tinst->alg.halg.base.cra_ctxsize = sizeof(struct mcryptd_hash_ctx);\n\n\tinst->alg.halg.base.cra_init = mcryptd_hash_init_tfm;\n\tinst->alg.halg.base.cra_exit = mcryptd_hash_exit_tfm;\n\n\tinst->alg.init   = mcryptd_hash_init_enqueue;\n\tinst->alg.update = mcryptd_hash_update_enqueue;\n\tinst->alg.final  = mcryptd_hash_final_enqueue;\n\tinst->alg.finup  = mcryptd_hash_finup_enqueue;\n\tinst->alg.export = mcryptd_hash_export;\n\tinst->alg.import = mcryptd_hash_import;\n\tinst->alg.setkey = mcryptd_hash_setkey;\n\tinst->alg.digest = mcryptd_hash_digest_enqueue;\n\n\terr = ahash_register_instance(tmpl, inst);\n\tif (err) {\n\t\tcrypto_drop_ahash(&ctx->spawn);\nout_free_inst:\n\t\tkfree(inst);\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}",
                        "cve_id": "CVE-2016-10147"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int crypto_skcipher_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_skcipher *skcipher = __crypto_skcipher_cast(tfm);\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(skcipher);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_blkcipher_type)\n\t\treturn crypto_init_skcipher_ops_blkcipher(tfm);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_ablkcipher_type ||\n\t    tfm->__crt_alg->cra_type == &crypto_givcipher_type)\n\t\treturn crypto_init_skcipher_ops_ablkcipher(tfm);\n\n\tskcipher->setkey = alg->setkey;\n\tskcipher->encrypt = alg->encrypt;\n\tskcipher->decrypt = alg->decrypt;\n\tskcipher->ivsize = alg->ivsize;\n\tskcipher->keysize = alg->max_keysize;\n\n\tif (alg->exit)\n\t\tskcipher->base.exit = crypto_skcipher_exit_tfm;\n\n\tif (alg->init)\n\t\treturn alg->init(skcipher);\n\n\treturn 0;\n}",
                        "code_after_change": "static int crypto_skcipher_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_skcipher *skcipher = __crypto_skcipher_cast(tfm);\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(skcipher);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_blkcipher_type)\n\t\treturn crypto_init_skcipher_ops_blkcipher(tfm);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_ablkcipher_type ||\n\t    tfm->__crt_alg->cra_type == &crypto_givcipher_type)\n\t\treturn crypto_init_skcipher_ops_ablkcipher(tfm);\n\n\tskcipher->setkey = skcipher_setkey;\n\tskcipher->encrypt = alg->encrypt;\n\tskcipher->decrypt = alg->decrypt;\n\tskcipher->ivsize = alg->ivsize;\n\tskcipher->keysize = alg->max_keysize;\n\n\tif (alg->exit)\n\t\tskcipher->base.exit = crypto_skcipher_exit_tfm;\n\n\tif (alg->init)\n\t\treturn alg->init(skcipher);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2017-9211"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static inline int crypto_rng_seedsize(struct crypto_rng *tfm)\n{\n\treturn tfm->seedsize;\n}",
                        "code_after_change": "static inline int crypto_rng_seedsize(struct crypto_rng *tfm)\n{\n\treturn crypto_rng_alg(tfm)->seedsize;\n}",
                        "cve_id": "CVE-2017-15116"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int do_tls_getsockopt_conf(struct sock *sk, char __user *optval,\n\t\t\t\t  int __user *optlen, int tx)\n{\n\tint rc = 0;\n\tstruct tls_context *ctx = tls_get_ctx(sk);\n\tstruct tls_crypto_info *crypto_info;\n\tstruct cipher_context *cctx;\n\tint len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tif (!optval || (len < sizeof(*crypto_info))) {\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (!ctx) {\n\t\trc = -EBUSY;\n\t\tgoto out;\n\t}\n\n\t/* get user crypto info */\n\tif (tx) {\n\t\tcrypto_info = &ctx->crypto_send.info;\n\t\tcctx = &ctx->tx;\n\t} else {\n\t\tcrypto_info = &ctx->crypto_recv.info;\n\t\tcctx = &ctx->rx;\n\t}\n\n\tif (!TLS_CRYPTO_INFO_READY(crypto_info)) {\n\t\trc = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tif (len == sizeof(*crypto_info)) {\n\t\tif (copy_to_user(optval, crypto_info, sizeof(*crypto_info)))\n\t\t\trc = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tswitch (crypto_info->cipher_type) {\n\tcase TLS_CIPHER_AES_GCM_128: {\n\t\tstruct tls12_crypto_info_aes_gcm_128 *\n\t\t  crypto_info_aes_gcm_128 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aes_gcm_128,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aes_gcm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(crypto_info_aes_gcm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_GCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_GCM_128_IV_SIZE);\n\t\tmemcpy(crypto_info_aes_gcm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_GCM_128_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aes_gcm_128,\n\t\t\t\t sizeof(*crypto_info_aes_gcm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_AES_GCM_256: {\n\t\tstruct tls12_crypto_info_aes_gcm_256 *\n\t\t  crypto_info_aes_gcm_256 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aes_gcm_256,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aes_gcm_256)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(crypto_info_aes_gcm_256->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_GCM_256_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_GCM_256_IV_SIZE);\n\t\tmemcpy(crypto_info_aes_gcm_256->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_GCM_256_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aes_gcm_256,\n\t\t\t\t sizeof(*crypto_info_aes_gcm_256)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_AES_CCM_128: {\n\t\tstruct tls12_crypto_info_aes_ccm_128 *aes_ccm_128 =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_aes_ccm_128, info);\n\n\t\tif (len != sizeof(*aes_ccm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(aes_ccm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_CCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_CCM_128_IV_SIZE);\n\t\tmemcpy(aes_ccm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_CCM_128_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval, aes_ccm_128, sizeof(*aes_ccm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_CHACHA20_POLY1305: {\n\t\tstruct tls12_crypto_info_chacha20_poly1305 *chacha20_poly1305 =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_chacha20_poly1305,\n\t\t\t\tinfo);\n\n\t\tif (len != sizeof(*chacha20_poly1305)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(chacha20_poly1305->iv,\n\t\t       cctx->iv + TLS_CIPHER_CHACHA20_POLY1305_SALT_SIZE,\n\t\t       TLS_CIPHER_CHACHA20_POLY1305_IV_SIZE);\n\t\tmemcpy(chacha20_poly1305->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_CHACHA20_POLY1305_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval, chacha20_poly1305,\n\t\t\t\tsizeof(*chacha20_poly1305)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_SM4_GCM: {\n\t\tstruct tls12_crypto_info_sm4_gcm *sm4_gcm_info =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_sm4_gcm, info);\n\n\t\tif (len != sizeof(*sm4_gcm_info)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(sm4_gcm_info->iv,\n\t\t       cctx->iv + TLS_CIPHER_SM4_GCM_SALT_SIZE,\n\t\t       TLS_CIPHER_SM4_GCM_IV_SIZE);\n\t\tmemcpy(sm4_gcm_info->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_SM4_GCM_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval, sm4_gcm_info, sizeof(*sm4_gcm_info)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_SM4_CCM: {\n\t\tstruct tls12_crypto_info_sm4_ccm *sm4_ccm_info =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_sm4_ccm, info);\n\n\t\tif (len != sizeof(*sm4_ccm_info)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(sm4_ccm_info->iv,\n\t\t       cctx->iv + TLS_CIPHER_SM4_CCM_SALT_SIZE,\n\t\t       TLS_CIPHER_SM4_CCM_IV_SIZE);\n\t\tmemcpy(sm4_ccm_info->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_SM4_CCM_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval, sm4_ccm_info, sizeof(*sm4_ccm_info)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_ARIA_GCM_128: {\n\t\tstruct tls12_crypto_info_aria_gcm_128 *\n\t\t  crypto_info_aria_gcm_128 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aria_gcm_128,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aria_gcm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(crypto_info_aria_gcm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_ARIA_GCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_ARIA_GCM_128_IV_SIZE);\n\t\tmemcpy(crypto_info_aria_gcm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_ARIA_GCM_128_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aria_gcm_128,\n\t\t\t\t sizeof(*crypto_info_aria_gcm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_ARIA_GCM_256: {\n\t\tstruct tls12_crypto_info_aria_gcm_256 *\n\t\t  crypto_info_aria_gcm_256 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aria_gcm_256,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aria_gcm_256)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(crypto_info_aria_gcm_256->iv,\n\t\t       cctx->iv + TLS_CIPHER_ARIA_GCM_256_SALT_SIZE,\n\t\t       TLS_CIPHER_ARIA_GCM_256_IV_SIZE);\n\t\tmemcpy(crypto_info_aria_gcm_256->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_ARIA_GCM_256_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aria_gcm_256,\n\t\t\t\t sizeof(*crypto_info_aria_gcm_256)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tdefault:\n\t\trc = -EINVAL;\n\t}\n\nout:\n\treturn rc;\n}",
                        "code_after_change": "static int do_tls_getsockopt_conf(struct sock *sk, char __user *optval,\n\t\t\t\t  int __user *optlen, int tx)\n{\n\tint rc = 0;\n\tstruct tls_context *ctx = tls_get_ctx(sk);\n\tstruct tls_crypto_info *crypto_info;\n\tstruct cipher_context *cctx;\n\tint len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tif (!optval || (len < sizeof(*crypto_info))) {\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (!ctx) {\n\t\trc = -EBUSY;\n\t\tgoto out;\n\t}\n\n\t/* get user crypto info */\n\tif (tx) {\n\t\tcrypto_info = &ctx->crypto_send.info;\n\t\tcctx = &ctx->tx;\n\t} else {\n\t\tcrypto_info = &ctx->crypto_recv.info;\n\t\tcctx = &ctx->rx;\n\t}\n\n\tif (!TLS_CRYPTO_INFO_READY(crypto_info)) {\n\t\trc = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tif (len == sizeof(*crypto_info)) {\n\t\tif (copy_to_user(optval, crypto_info, sizeof(*crypto_info)))\n\t\t\trc = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tswitch (crypto_info->cipher_type) {\n\tcase TLS_CIPHER_AES_GCM_128: {\n\t\tstruct tls12_crypto_info_aes_gcm_128 *\n\t\t  crypto_info_aes_gcm_128 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aes_gcm_128,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aes_gcm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(crypto_info_aes_gcm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_GCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_GCM_128_IV_SIZE);\n\t\tmemcpy(crypto_info_aes_gcm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_GCM_128_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aes_gcm_128,\n\t\t\t\t sizeof(*crypto_info_aes_gcm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_AES_GCM_256: {\n\t\tstruct tls12_crypto_info_aes_gcm_256 *\n\t\t  crypto_info_aes_gcm_256 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aes_gcm_256,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aes_gcm_256)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(crypto_info_aes_gcm_256->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_GCM_256_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_GCM_256_IV_SIZE);\n\t\tmemcpy(crypto_info_aes_gcm_256->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_GCM_256_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aes_gcm_256,\n\t\t\t\t sizeof(*crypto_info_aes_gcm_256)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_AES_CCM_128: {\n\t\tstruct tls12_crypto_info_aes_ccm_128 *aes_ccm_128 =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_aes_ccm_128, info);\n\n\t\tif (len != sizeof(*aes_ccm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(aes_ccm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_CCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_CCM_128_IV_SIZE);\n\t\tmemcpy(aes_ccm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_CCM_128_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval, aes_ccm_128, sizeof(*aes_ccm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_CHACHA20_POLY1305: {\n\t\tstruct tls12_crypto_info_chacha20_poly1305 *chacha20_poly1305 =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_chacha20_poly1305,\n\t\t\t\tinfo);\n\n\t\tif (len != sizeof(*chacha20_poly1305)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(chacha20_poly1305->iv,\n\t\t       cctx->iv + TLS_CIPHER_CHACHA20_POLY1305_SALT_SIZE,\n\t\t       TLS_CIPHER_CHACHA20_POLY1305_IV_SIZE);\n\t\tmemcpy(chacha20_poly1305->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_CHACHA20_POLY1305_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval, chacha20_poly1305,\n\t\t\t\tsizeof(*chacha20_poly1305)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_SM4_GCM: {\n\t\tstruct tls12_crypto_info_sm4_gcm *sm4_gcm_info =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_sm4_gcm, info);\n\n\t\tif (len != sizeof(*sm4_gcm_info)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(sm4_gcm_info->iv,\n\t\t       cctx->iv + TLS_CIPHER_SM4_GCM_SALT_SIZE,\n\t\t       TLS_CIPHER_SM4_GCM_IV_SIZE);\n\t\tmemcpy(sm4_gcm_info->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_SM4_GCM_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval, sm4_gcm_info, sizeof(*sm4_gcm_info)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_SM4_CCM: {\n\t\tstruct tls12_crypto_info_sm4_ccm *sm4_ccm_info =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_sm4_ccm, info);\n\n\t\tif (len != sizeof(*sm4_ccm_info)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(sm4_ccm_info->iv,\n\t\t       cctx->iv + TLS_CIPHER_SM4_CCM_SALT_SIZE,\n\t\t       TLS_CIPHER_SM4_CCM_IV_SIZE);\n\t\tmemcpy(sm4_ccm_info->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_SM4_CCM_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval, sm4_ccm_info, sizeof(*sm4_ccm_info)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_ARIA_GCM_128: {\n\t\tstruct tls12_crypto_info_aria_gcm_128 *\n\t\t  crypto_info_aria_gcm_128 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aria_gcm_128,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aria_gcm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(crypto_info_aria_gcm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_ARIA_GCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_ARIA_GCM_128_IV_SIZE);\n\t\tmemcpy(crypto_info_aria_gcm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_ARIA_GCM_128_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aria_gcm_128,\n\t\t\t\t sizeof(*crypto_info_aria_gcm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_ARIA_GCM_256: {\n\t\tstruct tls12_crypto_info_aria_gcm_256 *\n\t\t  crypto_info_aria_gcm_256 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aria_gcm_256,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aria_gcm_256)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(crypto_info_aria_gcm_256->iv,\n\t\t       cctx->iv + TLS_CIPHER_ARIA_GCM_256_SALT_SIZE,\n\t\t       TLS_CIPHER_ARIA_GCM_256_IV_SIZE);\n\t\tmemcpy(crypto_info_aria_gcm_256->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_ARIA_GCM_256_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aria_gcm_256,\n\t\t\t\t sizeof(*crypto_info_aria_gcm_256)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tdefault:\n\t\trc = -EINVAL;\n\t}\n\nout:\n\treturn rc;\n}",
                        "cve_id": "CVE-2023-28466"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int brcm_nvram_parse(struct brcm_nvram *priv)\n{\n\tstruct device *dev = priv->dev;\n\tstruct brcm_nvram_header header;\n\tuint8_t *data;\n\tsize_t len;\n\tint err;\n\n\tmemcpy_fromio(&header, priv->base, sizeof(header));\n\n\tif (memcmp(header.magic, NVRAM_MAGIC, 4)) {\n\t\tdev_err(dev, \"Invalid NVRAM magic\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tlen = le32_to_cpu(header.len);\n\n\tdata = kzalloc(len, GFP_KERNEL);\n\tmemcpy_fromio(data, priv->base, len);\n\tdata[len - 1] = '\\0';\n\n\terr = brcm_nvram_add_cells(priv, data, len);\n\tif (err) {\n\t\tdev_err(dev, \"Failed to add cells: %d\\n\", err);\n\t\treturn err;\n\t}\n\n\tkfree(data);\n\n\treturn 0;\n}",
                        "code_after_change": "static int brcm_nvram_parse(struct brcm_nvram *priv)\n{\n\tstruct device *dev = priv->dev;\n\tstruct brcm_nvram_header header;\n\tuint8_t *data;\n\tsize_t len;\n\tint err;\n\n\tmemcpy_fromio(&header, priv->base, sizeof(header));\n\n\tif (memcmp(header.magic, NVRAM_MAGIC, 4)) {\n\t\tdev_err(dev, \"Invalid NVRAM magic\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tlen = le32_to_cpu(header.len);\n\n\tdata = kzalloc(len, GFP_KERNEL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tmemcpy_fromio(data, priv->base, len);\n\tdata[len - 1] = '\\0';\n\n\terr = brcm_nvram_add_cells(priv, data, len);\n\tif (err) {\n\t\tdev_err(dev, \"Failed to add cells: %d\\n\", err);\n\t\treturn err;\n\t}\n\n\tkfree(data);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2023-3359"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1384,
            "cve_id": "CVE-2017-15274",
            "code_snippet": " */\nSYSCALL_DEFINE5(add_key, const char __user *, _type,\n\t\tconst char __user *, _description,\n\t\tconst void __user *, _payload,\n\t\tsize_t, plen,\n\t\tkey_serial_t, ringid)\n{\n\tkey_ref_t keyring_ref, key_ref;\n\tchar type[32], *description;\n\tvoid *payload;\n\tlong ret;\n\n\tret = -EINVAL;\n\tif (plen > 1024 * 1024 - 1)\n\t\tgoto error;\n\n\t/* draw all the data into kernel space */\n\tret = key_get_type_from_user(type, _type, sizeof(type));\n\tif (ret < 0)\n\t\tgoto error;\n\n\tdescription = NULL;\n\tif (_description) {\n\t\tdescription = strndup_user(_description, KEY_MAX_DESC_SIZE);\n\t\tif (IS_ERR(description)) {\n\t\t\tret = PTR_ERR(description);\n\t\t\tgoto error;\n\t\t}\n\t\tif (!*description) {\n\t\t\tkfree(description);\n\t\t\tdescription = NULL;\n\t\t} else if ((description[0] == '.') &&\n\t\t\t   (strncmp(type, \"keyring\", 7) == 0)) {\n\t\t\tret = -EPERM;\n\t\t\tgoto error2;\n\t\t}\n\t}\n\n\t/* pull the payload in if one was supplied */\n\tpayload = NULL;\n\n\tif (plen) {\n\t\tret = -ENOMEM;\n\t\tpayload = kvmalloc(plen, GFP_KERNEL);\n\t\tif (!payload)\n\t\t\tgoto error2;\n\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(payload, _payload, plen) != 0)\n\t\t\tgoto error3;\n\t}\n\n\t/* find the target keyring (which must be writable) */\n\tkeyring_ref = lookup_user_key(ringid, KEY_LOOKUP_CREATE, KEY_NEED_WRITE);\n\tif (IS_ERR(keyring_ref)) {\n\t\tret = PTR_ERR(keyring_ref);\n\t\tgoto error3;\n\t}\n\n\t/* create or update the requested key and add it to the target\n\t * keyring */\n\tkey_ref = key_create_or_update(keyring_ref, type, description,\n\t\t\t\t       payload, plen, KEY_PERM_UNDEF,\n\t\t\t\t       KEY_ALLOC_IN_QUOTA);\n\tif (!IS_ERR(key_ref)) {\n\t\tret = key_ref_to_ptr(key_ref)->serial;\n\t\tkey_ref_put(key_ref);\n\t}\n\telse {\n\t\tret = PTR_ERR(key_ref);\n\t}\n\n\tkey_ref_put(keyring_ref);\n error3:\n\tkvfree(payload);\n error2:\n\tkfree(description);\n error:\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "long keyctl_update_key(key_serial_t id,\n\t\t       const void __user *_payload,\n\t\t       size_t plen)\n{\n\tkey_ref_t key_ref;\n\tvoid *payload;\n\tlong ret;\n\n\tret = -EINVAL;\n\tif (plen > PAGE_SIZE)\n\t\tgoto error;\n\n\t/* pull the payload in if one was supplied */\n\tpayload = NULL;\n\tif (_payload) {\n\t\tret = -ENOMEM;\n\t\tpayload = kmalloc(plen, GFP_KERNEL);\n\t\tif (!payload)\n\t\t\tgoto error;\n\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(payload, _payload, plen) != 0)\n\t\t\tgoto error2;\n\t}\n\n\t/* find the target key (which must be writable) */\n\tkey_ref = lookup_user_key(id, 0, KEY_NEED_WRITE);\n\tif (IS_ERR(key_ref)) {\n\t\tret = PTR_ERR(key_ref);\n\t\tgoto error2;\n\t}\n\n\t/* update the key */\n\tret = key_update(key_ref, payload, plen);\n\n\tkey_ref_put(key_ref);\nerror2:\n\tkfree(payload);\nerror:\n\treturn ret;\n}",
                        "code_after_change": "long keyctl_update_key(key_serial_t id,\n\t\t       const void __user *_payload,\n\t\t       size_t plen)\n{\n\tkey_ref_t key_ref;\n\tvoid *payload;\n\tlong ret;\n\n\tret = -EINVAL;\n\tif (plen > PAGE_SIZE)\n\t\tgoto error;\n\n\t/* pull the payload in if one was supplied */\n\tpayload = NULL;\n\tif (plen) {\n\t\tret = -ENOMEM;\n\t\tpayload = kmalloc(plen, GFP_KERNEL);\n\t\tif (!payload)\n\t\t\tgoto error;\n\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(payload, _payload, plen) != 0)\n\t\t\tgoto error2;\n\t}\n\n\t/* find the target key (which must be writable) */\n\tkey_ref = lookup_user_key(id, 0, KEY_NEED_WRITE);\n\tif (IS_ERR(key_ref)) {\n\t\tret = PTR_ERR(key_ref);\n\t\tgoto error2;\n\t}\n\n\t/* update the key */\n\tret = key_update(key_ref, payload, plen);\n\n\tkey_ref_put(key_ref);\nerror2:\n\tkfree(payload);\nerror:\n\treturn ret;\n}",
                        "cve_id": "CVE-2017-15274"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1410,
            "cve_id": "CVE-2017-16646",
            "code_snippet": "static int stk7070p_frontend_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct usb_device_descriptor *p = &adap->dev->udev->descriptor;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\tif (p->idVendor  == cpu_to_le16(USB_VID_PINNACLE) &&\n\t    p->idProduct == cpu_to_le16(USB_PID_PINNACLE_PCTV72E))\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 0);\n\telse\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\n\tdib0700_ctrl_clock(adap->dev, 72, 1);\n\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 18,\n\t\t\t\t     &dib7070p_dib7000p_config) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\",\n\t\t    __func__);\n\t\tdvb_detach(state->dib7000p_ops.set_wbd_ref);\n\t\treturn -ENODEV;\n\t}\n\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x80,\n\t\t&dib7070p_dib7000p_config);\n\treturn adap->fe_adap[0].fe == NULL ? -ENODEV : 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int stk7770p_frontend_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct usb_device_descriptor *p = &adap->dev->udev->descriptor;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\tif (p->idVendor  == cpu_to_le16(USB_VID_PINNACLE) &&\n\t    p->idProduct == cpu_to_le16(USB_PID_PINNACLE_PCTV72E))\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 0);\n\telse\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\n\tdib0700_ctrl_clock(adap->dev, 72, 1);\n\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 18,\n\t\t\t\t     &dib7770p_dib7000p_config) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\",\n\t\t    __func__);\n\t\tdvb_detach(&state->dib7000p_ops);\n\t\treturn -ENODEV;\n\t}\n\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x80,\n\t\t&dib7770p_dib7000p_config);\n\treturn adap->fe_adap[0].fe == NULL ? -ENODEV : 0;\n}",
                        "code_after_change": "static int stk7770p_frontend_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct usb_device_descriptor *p = &adap->dev->udev->descriptor;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\tif (p->idVendor  == cpu_to_le16(USB_VID_PINNACLE) &&\n\t    p->idProduct == cpu_to_le16(USB_PID_PINNACLE_PCTV72E))\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 0);\n\telse\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\n\tdib0700_ctrl_clock(adap->dev, 72, 1);\n\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 18,\n\t\t\t\t     &dib7770p_dib7000p_config) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\",\n\t\t    __func__);\n\t\tdvb_detach(state->dib7000p_ops.set_wbd_ref);\n\t\treturn -ENODEV;\n\t}\n\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x80,\n\t\t&dib7770p_dib7000p_config);\n\treturn adap->fe_adap[0].fe == NULL ? -ENODEV : 0;\n}",
                        "cve_id": "CVE-2017-16646"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1411,
            "cve_id": "CVE-2017-16646",
            "code_snippet": "static int pctv340e_frontend_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct dib0700_state *st = adap->dev->priv;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\t/* Power Supply on */\n\tdib0700_set_gpio(adap->dev, GPIO6,  GPIO_OUT, 0);\n\tmsleep(50);\n\tdib0700_set_gpio(adap->dev, GPIO6,  GPIO_OUT, 1);\n\tmsleep(100); /* Allow power supply to settle before probing */\n\n\t/* cx25843 reset */\n\tdib0700_set_gpio(adap->dev, GPIO10,  GPIO_OUT, 0);\n\tmsleep(1); /* cx25843 datasheet say 350us required */\n\tdib0700_set_gpio(adap->dev, GPIO10,  GPIO_OUT, 1);\n\n\t/* LNA off for now */\n\tdib0700_set_gpio(adap->dev, GPIO8,  GPIO_OUT, 1);\n\n\t/* Put the CX25843 to sleep for now since we're in digital mode */\n\tdib0700_set_gpio(adap->dev, GPIO2, GPIO_OUT, 1);\n\n\t/* FIXME: not verified yet */\n\tdib0700_ctrl_clock(adap->dev, 72, 1);\n\n\tmsleep(500);\n\n\tif (state->dib7000p_ops.dib7000pc_detection(&adap->dev->i2c_adap) == 0) {\n\t\t/* Demodulator not found for some reason? */\n\t\tdvb_detach(state->dib7000p_ops.set_wbd_ref);\n\t\treturn -ENODEV;\n\t}\n\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x12,\n\t\t\t      &pctv_340e_config);\n\tst->is_dib7000pc = 1;\n\n\treturn adap->fe_adap[0].fe == NULL ? -ENODEV : 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tfe7090pvr_frontend0_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct dib0700_state *st = adap->dev->priv;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\t/* The TFE7090 requires the dib0700 to not be in master mode */\n\tst->disable_streaming_master_mode = 1;\n\n\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\t/* initialize IC 0 */\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 0x20, &tfe7090pvr_dib7000p_config[0]) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\", __func__);\n\t\tdvb_detach(&state->dib7000p_ops);\n\t\treturn -ENODEV;\n\t}\n\n\tdib0700_set_i2c_speed(adap->dev, 340);\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x90, &tfe7090pvr_dib7000p_config[0]);\n\tif (adap->fe_adap[0].fe == NULL)\n\t\treturn -ENODEV;\n\n\tstate->dib7000p_ops.slave_reset(adap->fe_adap[0].fe);\n\n\treturn 0;\n}",
                        "code_after_change": "static int tfe7090pvr_frontend0_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct dib0700_state *st = adap->dev->priv;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\t/* The TFE7090 requires the dib0700 to not be in master mode */\n\tst->disable_streaming_master_mode = 1;\n\n\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\t/* initialize IC 0 */\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 0x20, &tfe7090pvr_dib7000p_config[0]) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\", __func__);\n\t\tdvb_detach(state->dib7000p_ops.set_wbd_ref);\n\t\treturn -ENODEV;\n\t}\n\n\tdib0700_set_i2c_speed(adap->dev, 340);\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x90, &tfe7090pvr_dib7000p_config[0]);\n\tif (adap->fe_adap[0].fe == NULL)\n\t\treturn -ENODEV;\n\n\tstate->dib7000p_ops.slave_reset(adap->fe_adap[0].fe);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2017-16646"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1412,
            "cve_id": "CVE-2017-16646",
            "code_snippet": "static int tfe7790p_frontend_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct dib0700_state *st = adap->dev->priv;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\t/* The TFE7790P requires the dib0700 to not be in master mode */\n\tst->disable_streaming_master_mode = 1;\n\n\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\tmsleep(20);\n\tdib0700_ctrl_clock(adap->dev, 72, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap,\n\t\t\t\t1, 0x10, &tfe7790p_dib7000p_config) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\",\n\t\t\t\t__func__);\n\t\tdvb_detach(state->dib7000p_ops.set_wbd_ref);\n\t\treturn -ENODEV;\n\t}\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap,\n\t\t\t0x80, &tfe7790p_dib7000p_config);\n\n\treturn adap->fe_adap[0].fe == NULL ?  -ENODEV : 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tfe7090pvr_frontend0_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct dib0700_state *st = adap->dev->priv;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\t/* The TFE7090 requires the dib0700 to not be in master mode */\n\tst->disable_streaming_master_mode = 1;\n\n\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\t/* initialize IC 0 */\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 0x20, &tfe7090pvr_dib7000p_config[0]) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\", __func__);\n\t\tdvb_detach(&state->dib7000p_ops);\n\t\treturn -ENODEV;\n\t}\n\n\tdib0700_set_i2c_speed(adap->dev, 340);\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x90, &tfe7090pvr_dib7000p_config[0]);\n\tif (adap->fe_adap[0].fe == NULL)\n\t\treturn -ENODEV;\n\n\tstate->dib7000p_ops.slave_reset(adap->fe_adap[0].fe);\n\n\treturn 0;\n}",
                        "code_after_change": "static int tfe7090pvr_frontend0_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct dib0700_state *st = adap->dev->priv;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\t/* The TFE7090 requires the dib0700 to not be in master mode */\n\tst->disable_streaming_master_mode = 1;\n\n\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\t/* initialize IC 0 */\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 0x20, &tfe7090pvr_dib7000p_config[0]) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\", __func__);\n\t\tdvb_detach(state->dib7000p_ops.set_wbd_ref);\n\t\treturn -ENODEV;\n\t}\n\n\tdib0700_set_i2c_speed(adap->dev, 340);\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x90, &tfe7090pvr_dib7000p_config[0]);\n\tif (adap->fe_adap[0].fe == NULL)\n\t\treturn -ENODEV;\n\n\tstate->dib7000p_ops.slave_reset(adap->fe_adap[0].fe);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2017-16646"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1413,
            "cve_id": "CVE-2017-16646",
            "code_snippet": "static int stk7700ph_frontend_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct usb_device_descriptor *desc = &adap->dev->udev->descriptor;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\tif (desc->idVendor  == cpu_to_le16(USB_VID_PINNACLE) &&\n\t    desc->idProduct == cpu_to_le16(USB_PID_PINNACLE_EXPRESSCARD_320CX))\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 0);\n\telse\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\tmsleep(10);\n\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 18,\n\t\t\t\t     &stk7700ph_dib7700_xc3028_config) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\",\n\t\t    __func__);\n\t\tdvb_detach(state->dib7000p_ops.set_wbd_ref);\n\t\treturn -ENODEV;\n\t}\n\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x80,\n\t\t&stk7700ph_dib7700_xc3028_config);\n\n\treturn adap->fe_adap[0].fe == NULL ? -ENODEV : 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int stk7770p_frontend_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct usb_device_descriptor *p = &adap->dev->udev->descriptor;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\tif (p->idVendor  == cpu_to_le16(USB_VID_PINNACLE) &&\n\t    p->idProduct == cpu_to_le16(USB_PID_PINNACLE_PCTV72E))\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 0);\n\telse\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\n\tdib0700_ctrl_clock(adap->dev, 72, 1);\n\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 18,\n\t\t\t\t     &dib7770p_dib7000p_config) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\",\n\t\t    __func__);\n\t\tdvb_detach(&state->dib7000p_ops);\n\t\treturn -ENODEV;\n\t}\n\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x80,\n\t\t&dib7770p_dib7000p_config);\n\treturn adap->fe_adap[0].fe == NULL ? -ENODEV : 0;\n}",
                        "code_after_change": "static int stk7770p_frontend_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct usb_device_descriptor *p = &adap->dev->udev->descriptor;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\tif (p->idVendor  == cpu_to_le16(USB_VID_PINNACLE) &&\n\t    p->idProduct == cpu_to_le16(USB_PID_PINNACLE_PCTV72E))\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 0);\n\telse\n\t\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\n\tdib0700_ctrl_clock(adap->dev, 72, 1);\n\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(10);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 18,\n\t\t\t\t     &dib7770p_dib7000p_config) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\",\n\t\t    __func__);\n\t\tdvb_detach(state->dib7000p_ops.set_wbd_ref);\n\t\treturn -ENODEV;\n\t}\n\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x80,\n\t\t&dib7770p_dib7000p_config);\n\treturn adap->fe_adap[0].fe == NULL ? -ENODEV : 0;\n}",
                        "cve_id": "CVE-2017-16646"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1414,
            "cve_id": "CVE-2017-16646",
            "code_snippet": "static int tfe7090pvr_frontend1_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct i2c_adapter *i2c;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (adap->dev->adapter[0].fe_adap[0].fe == NULL) {\n\t\terr(\"the master dib7090 has to be initialized first\");\n\t\treturn -ENODEV; /* the master device has not been initialized */\n\t}\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\ti2c = state->dib7000p_ops.get_i2c_master(adap->dev->adapter[0].fe_adap[0].fe, DIBX000_I2C_INTERFACE_GPIO_6_7, 1);\n\tif (state->dib7000p_ops.i2c_enumeration(i2c, 1, 0x10, &tfe7090pvr_dib7000p_config[1]) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\", __func__);\n\t\tdvb_detach(state->dib7000p_ops.set_wbd_ref);\n\t\treturn -ENODEV;\n\t}\n\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(i2c, 0x92, &tfe7090pvr_dib7000p_config[1]);\n\tdib0700_set_i2c_speed(adap->dev, 200);\n\n\treturn adap->fe_adap[0].fe == NULL ? -ENODEV : 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tfe7090pvr_frontend0_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct dib0700_state *st = adap->dev->priv;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\t/* The TFE7090 requires the dib0700 to not be in master mode */\n\tst->disable_streaming_master_mode = 1;\n\n\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\t/* initialize IC 0 */\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 0x20, &tfe7090pvr_dib7000p_config[0]) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\", __func__);\n\t\tdvb_detach(&state->dib7000p_ops);\n\t\treturn -ENODEV;\n\t}\n\n\tdib0700_set_i2c_speed(adap->dev, 340);\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x90, &tfe7090pvr_dib7000p_config[0]);\n\tif (adap->fe_adap[0].fe == NULL)\n\t\treturn -ENODEV;\n\n\tstate->dib7000p_ops.slave_reset(adap->fe_adap[0].fe);\n\n\treturn 0;\n}",
                        "code_after_change": "static int tfe7090pvr_frontend0_attach(struct dvb_usb_adapter *adap)\n{\n\tstruct dib0700_state *st = adap->dev->priv;\n\tstruct dib0700_adapter_state *state = adap->priv;\n\n\tif (!dvb_attach(dib7000p_attach, &state->dib7000p_ops))\n\t\treturn -ENODEV;\n\n\t/* The TFE7090 requires the dib0700 to not be in master mode */\n\tst->disable_streaming_master_mode = 1;\n\n\tdib0700_set_gpio(adap->dev, GPIO6, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO9, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO4, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO7, GPIO_OUT, 1);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 0);\n\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO10, GPIO_OUT, 1);\n\tmsleep(20);\n\tdib0700_set_gpio(adap->dev, GPIO0, GPIO_OUT, 1);\n\n\t/* initialize IC 0 */\n\tif (state->dib7000p_ops.i2c_enumeration(&adap->dev->i2c_adap, 1, 0x20, &tfe7090pvr_dib7000p_config[0]) != 0) {\n\t\terr(\"%s: state->dib7000p_ops.i2c_enumeration failed.  Cannot continue\\n\", __func__);\n\t\tdvb_detach(state->dib7000p_ops.set_wbd_ref);\n\t\treturn -ENODEV;\n\t}\n\n\tdib0700_set_i2c_speed(adap->dev, 340);\n\tadap->fe_adap[0].fe = state->dib7000p_ops.init(&adap->dev->i2c_adap, 0x90, &tfe7090pvr_dib7000p_config[0]);\n\tif (adap->fe_adap[0].fe == NULL)\n\t\treturn -ENODEV;\n\n\tstate->dib7000p_ops.slave_reset(adap->fe_adap[0].fe);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2017-16646"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1422,
            "cve_id": "CVE-2017-16647",
            "code_snippet": "static int asix_resume(struct usb_interface *intf)\n{\n\tstruct usbnet *dev = usb_get_intfdata(intf);\n\tstruct asix_common_private *priv = dev->driver_priv;\n\n\tif (priv && priv->resume)\n\t\tpriv->resume(dev);\n\n\treturn usbnet_resume(intf);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int asix_suspend(struct usb_interface *intf, pm_message_t message)\n{\n\tstruct usbnet *dev = usb_get_intfdata(intf);\n\tstruct asix_common_private *priv = dev->driver_priv;\n\n\tif (priv->suspend)\n\t\tpriv->suspend(dev);\n\n\treturn usbnet_suspend(intf, message);\n}",
                        "code_after_change": "static int asix_suspend(struct usb_interface *intf, pm_message_t message)\n{\n\tstruct usbnet *dev = usb_get_intfdata(intf);\n\tstruct asix_common_private *priv = dev->driver_priv;\n\n\tif (priv && priv->suspend)\n\t\tpriv->suspend(dev);\n\n\treturn usbnet_suspend(intf, message);\n}",
                        "cve_id": "CVE-2017-16647"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1473,
            "cve_id": "CVE-2017-18079",
            "code_snippet": "static void i8042_stop(struct serio *serio)\n{\n\tstruct i8042_port *port = serio->port_data;\n\n\tspin_lock_irq(&i8042_lock);\n\tport->exists = false;\n\tport->serio = NULL;\n\tspin_unlock_irq(&i8042_lock);\n\n\t/*\n\t * We need to make sure that interrupt handler finishes using\n\t * our serio port before we return from this function.\n\t * We synchronize with both AUX and KBD IRQs because there is\n\t * a (very unlikely) chance that AUX IRQ is raised for KBD port\n\t * and vice versa.\n\t */\n\tsynchronize_irq(I8042_AUX_IRQ);\n\tsynchronize_irq(I8042_KBD_IRQ);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static irqreturn_t i8042_interrupt(int irq, void *dev_id)\n{\n\tstruct i8042_port *port;\n\tstruct serio *serio;\n\tunsigned long flags;\n\tunsigned char str, data;\n\tunsigned int dfl;\n\tunsigned int port_no;\n\tbool filtered;\n\tint ret = 1;\n\n\tspin_lock_irqsave(&i8042_lock, flags);\n\n\tstr = i8042_read_status();\n\tif (unlikely(~str & I8042_STR_OBF)) {\n\t\tspin_unlock_irqrestore(&i8042_lock, flags);\n\t\tif (irq)\n\t\t\tdbg(\"Interrupt %d, without any data\\n\", irq);\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tdata = i8042_read_data();\n\n\tif (i8042_mux_present && (str & I8042_STR_AUXDATA)) {\n\t\tstatic unsigned long last_transmit;\n\t\tstatic unsigned char last_str;\n\n\t\tdfl = 0;\n\t\tif (str & I8042_STR_MUXERR) {\n\t\t\tdbg(\"MUX error, status is %02x, data is %02x\\n\",\n\t\t\t    str, data);\n/*\n * When MUXERR condition is signalled the data register can only contain\n * 0xfd, 0xfe or 0xff if implementation follows the spec. Unfortunately\n * it is not always the case. Some KBCs also report 0xfc when there is\n * nothing connected to the port while others sometimes get confused which\n * port the data came from and signal error leaving the data intact. They\n * _do not_ revert to legacy mode (actually I've never seen KBC reverting\n * to legacy mode yet, when we see one we'll add proper handling).\n * Anyway, we process 0xfc, 0xfd, 0xfe and 0xff as timeouts, and for the\n * rest assume that the data came from the same serio last byte\n * was transmitted (if transmission happened not too long ago).\n */\n\n\t\t\tswitch (data) {\n\t\t\t\tdefault:\n\t\t\t\t\tif (time_before(jiffies, last_transmit + HZ/10)) {\n\t\t\t\t\t\tstr = last_str;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\t/* fall through - report timeout */\n\t\t\t\tcase 0xfc:\n\t\t\t\tcase 0xfd:\n\t\t\t\tcase 0xfe: dfl = SERIO_TIMEOUT; data = 0xfe; break;\n\t\t\t\tcase 0xff: dfl = SERIO_PARITY;  data = 0xfe; break;\n\t\t\t}\n\t\t}\n\n\t\tport_no = I8042_MUX_PORT_NO + ((str >> 6) & 3);\n\t\tlast_str = str;\n\t\tlast_transmit = jiffies;\n\t} else {\n\n\t\tdfl = ((str & I8042_STR_PARITY) ? SERIO_PARITY : 0) |\n\t\t      ((str & I8042_STR_TIMEOUT && !i8042_notimeout) ? SERIO_TIMEOUT : 0);\n\n\t\tport_no = (str & I8042_STR_AUXDATA) ?\n\t\t\t\tI8042_AUX_PORT_NO : I8042_KBD_PORT_NO;\n\t}\n\n\tport = &i8042_ports[port_no];\n\tserio = port->exists ? port->serio : NULL;\n\n\tfilter_dbg(port->driver_bound, data, \"<- i8042 (interrupt, %d, %d%s%s)\\n\",\n\t\t   port_no, irq,\n\t\t   dfl & SERIO_PARITY ? \", bad parity\" : \"\",\n\t\t   dfl & SERIO_TIMEOUT ? \", timeout\" : \"\");\n\n\tfiltered = i8042_filter(data, str, serio);\n\n\tspin_unlock_irqrestore(&i8042_lock, flags);\n\n\tif (likely(port->exists && !filtered))\n\t\tserio_interrupt(serio, data, dfl);\n\n out:\n\treturn IRQ_RETVAL(ret);\n}",
                        "code_after_change": "static irqreturn_t i8042_interrupt(int irq, void *dev_id)\n{\n\tstruct i8042_port *port;\n\tstruct serio *serio;\n\tunsigned long flags;\n\tunsigned char str, data;\n\tunsigned int dfl;\n\tunsigned int port_no;\n\tbool filtered;\n\tint ret = 1;\n\n\tspin_lock_irqsave(&i8042_lock, flags);\n\n\tstr = i8042_read_status();\n\tif (unlikely(~str & I8042_STR_OBF)) {\n\t\tspin_unlock_irqrestore(&i8042_lock, flags);\n\t\tif (irq)\n\t\t\tdbg(\"Interrupt %d, without any data\\n\", irq);\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tdata = i8042_read_data();\n\n\tif (i8042_mux_present && (str & I8042_STR_AUXDATA)) {\n\t\tstatic unsigned long last_transmit;\n\t\tstatic unsigned char last_str;\n\n\t\tdfl = 0;\n\t\tif (str & I8042_STR_MUXERR) {\n\t\t\tdbg(\"MUX error, status is %02x, data is %02x\\n\",\n\t\t\t    str, data);\n/*\n * When MUXERR condition is signalled the data register can only contain\n * 0xfd, 0xfe or 0xff if implementation follows the spec. Unfortunately\n * it is not always the case. Some KBCs also report 0xfc when there is\n * nothing connected to the port while others sometimes get confused which\n * port the data came from and signal error leaving the data intact. They\n * _do not_ revert to legacy mode (actually I've never seen KBC reverting\n * to legacy mode yet, when we see one we'll add proper handling).\n * Anyway, we process 0xfc, 0xfd, 0xfe and 0xff as timeouts, and for the\n * rest assume that the data came from the same serio last byte\n * was transmitted (if transmission happened not too long ago).\n */\n\n\t\t\tswitch (data) {\n\t\t\t\tdefault:\n\t\t\t\t\tif (time_before(jiffies, last_transmit + HZ/10)) {\n\t\t\t\t\t\tstr = last_str;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\t/* fall through - report timeout */\n\t\t\t\tcase 0xfc:\n\t\t\t\tcase 0xfd:\n\t\t\t\tcase 0xfe: dfl = SERIO_TIMEOUT; data = 0xfe; break;\n\t\t\t\tcase 0xff: dfl = SERIO_PARITY;  data = 0xfe; break;\n\t\t\t}\n\t\t}\n\n\t\tport_no = I8042_MUX_PORT_NO + ((str >> 6) & 3);\n\t\tlast_str = str;\n\t\tlast_transmit = jiffies;\n\t} else {\n\n\t\tdfl = ((str & I8042_STR_PARITY) ? SERIO_PARITY : 0) |\n\t\t      ((str & I8042_STR_TIMEOUT && !i8042_notimeout) ? SERIO_TIMEOUT : 0);\n\n\t\tport_no = (str & I8042_STR_AUXDATA) ?\n\t\t\t\tI8042_AUX_PORT_NO : I8042_KBD_PORT_NO;\n\t}\n\n\tport = &i8042_ports[port_no];\n\tserio = port->exists ? port->serio : NULL;\n\n\tfilter_dbg(port->driver_bound, data, \"<- i8042 (interrupt, %d, %d%s%s)\\n\",\n\t\t   port_no, irq,\n\t\t   dfl & SERIO_PARITY ? \", bad parity\" : \"\",\n\t\t   dfl & SERIO_TIMEOUT ? \", timeout\" : \"\");\n\n\tfiltered = i8042_filter(data, str, serio);\n\n\tspin_unlock_irqrestore(&i8042_lock, flags);\n\n\tif (likely(serio && !filtered))\n\t\tserio_interrupt(serio, data, dfl);\n\n out:\n\treturn IRQ_RETVAL(ret);\n}",
                        "cve_id": "CVE-2017-18079"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1484,
            "cve_id": "CVE-2017-18216",
            "code_snippet": "static ssize_t o2nm_node_num_store(struct config_item *item, const char *page,\n\t\t\t\t   size_t count)\n{\n\tstruct o2nm_node *node = to_o2nm_node(item);\n\tstruct o2nm_cluster *cluster;\n\tunsigned long tmp;\n\tchar *p = (char *)page;\n\tint ret = 0;\n\n\ttmp = simple_strtoul(p, &p, 0);\n\tif (!p || (*p && (*p != '\\n')))\n\t\treturn -EINVAL;\n\n\tif (tmp >= O2NM_MAX_NODES)\n\t\treturn -ERANGE;\n\n\t/* once we're in the cl_nodes tree networking can look us up by\n\t * node number and try to use our address and port attributes\n\t * to connect to this node.. make sure that they've been set\n\t * before writing the node attribute? */\n\tif (!test_bit(O2NM_NODE_ATTR_ADDRESS, &node->nd_set_attributes) ||\n\t    !test_bit(O2NM_NODE_ATTR_PORT, &node->nd_set_attributes))\n\t\treturn -EINVAL; /* XXX */\n\n\to2nm_lock_subsystem();\n\tcluster = to_o2nm_cluster_from_node(node);\n\tif (!cluster) {\n\t\to2nm_unlock_subsystem();\n\t\treturn -EINVAL;\n\t}\n\n\twrite_lock(&cluster->cl_nodes_lock);\n\tif (cluster->cl_nodes[tmp])\n\t\tret = -EEXIST;\n\telse if (test_and_set_bit(O2NM_NODE_ATTR_NUM,\n\t\t\t&node->nd_set_attributes))\n\t\tret = -EBUSY;\n\telse  {\n\t\tcluster->cl_nodes[tmp] = node;\n\t\tnode->nd_num = tmp;\n\t\tset_bit(tmp, cluster->cl_nodes_bitmap);\n\t}\n\twrite_unlock(&cluster->cl_nodes_lock);\n\to2nm_unlock_subsystem();\n\n\tif (ret)\n\t\treturn ret;\n\n\treturn count;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static ssize_t o2nm_node_local_store(struct config_item *item, const char *page,\n\t\t\t\t     size_t count)\n{\n\tstruct o2nm_node *node = to_o2nm_node(item);\n\tstruct o2nm_cluster *cluster = to_o2nm_cluster_from_node(node);\n\tunsigned long tmp;\n\tchar *p = (char *)page;\n\tssize_t ret;\n\n\ttmp = simple_strtoul(p, &p, 0);\n\tif (!p || (*p && (*p != '\\n')))\n\t\treturn -EINVAL;\n\n\ttmp = !!tmp; /* boolean of whether this node wants to be local */\n\n\t/* setting local turns on networking rx for now so we require having\n\t * set everything else first */\n\tif (!test_bit(O2NM_NODE_ATTR_ADDRESS, &node->nd_set_attributes) ||\n\t    !test_bit(O2NM_NODE_ATTR_NUM, &node->nd_set_attributes) ||\n\t    !test_bit(O2NM_NODE_ATTR_PORT, &node->nd_set_attributes))\n\t\treturn -EINVAL; /* XXX */\n\n\t/* the only failure case is trying to set a new local node\n\t * when a different one is already set */\n\tif (tmp && tmp == cluster->cl_has_local &&\n\t    cluster->cl_local_node != node->nd_num)\n\t\treturn -EBUSY;\n\n\t/* bring up the rx thread if we're setting the new local node. */\n\tif (tmp && !cluster->cl_has_local) {\n\t\tret = o2net_start_listening(node);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (!tmp && cluster->cl_has_local &&\n\t    cluster->cl_local_node == node->nd_num) {\n\t\to2net_stop_listening(node);\n\t\tcluster->cl_local_node = O2NM_INVALID_NODE_NUM;\n\t}\n\n\tnode->nd_local = tmp;\n\tif (node->nd_local) {\n\t\tcluster->cl_has_local = tmp;\n\t\tcluster->cl_local_node = node->nd_num;\n\t}\n\n\treturn count;\n}",
                        "code_after_change": "static ssize_t o2nm_node_local_store(struct config_item *item, const char *page,\n\t\t\t\t     size_t count)\n{\n\tstruct o2nm_node *node = to_o2nm_node(item);\n\tstruct o2nm_cluster *cluster;\n\tunsigned long tmp;\n\tchar *p = (char *)page;\n\tssize_t ret;\n\n\ttmp = simple_strtoul(p, &p, 0);\n\tif (!p || (*p && (*p != '\\n')))\n\t\treturn -EINVAL;\n\n\ttmp = !!tmp; /* boolean of whether this node wants to be local */\n\n\t/* setting local turns on networking rx for now so we require having\n\t * set everything else first */\n\tif (!test_bit(O2NM_NODE_ATTR_ADDRESS, &node->nd_set_attributes) ||\n\t    !test_bit(O2NM_NODE_ATTR_NUM, &node->nd_set_attributes) ||\n\t    !test_bit(O2NM_NODE_ATTR_PORT, &node->nd_set_attributes))\n\t\treturn -EINVAL; /* XXX */\n\n\to2nm_lock_subsystem();\n\tcluster = to_o2nm_cluster_from_node(node);\n\tif (!cluster) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* the only failure case is trying to set a new local node\n\t * when a different one is already set */\n\tif (tmp && tmp == cluster->cl_has_local &&\n\t    cluster->cl_local_node != node->nd_num) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\t/* bring up the rx thread if we're setting the new local node. */\n\tif (tmp && !cluster->cl_has_local) {\n\t\tret = o2net_start_listening(node);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tif (!tmp && cluster->cl_has_local &&\n\t    cluster->cl_local_node == node->nd_num) {\n\t\to2net_stop_listening(node);\n\t\tcluster->cl_local_node = O2NM_INVALID_NODE_NUM;\n\t}\n\n\tnode->nd_local = tmp;\n\tif (node->nd_local) {\n\t\tcluster->cl_has_local = tmp;\n\t\tcluster->cl_local_node = node->nd_num;\n\t}\n\n\tret = count;\n\nout:\n\to2nm_unlock_subsystem();\n\treturn ret;\n}",
                        "cve_id": "CVE-2017-18216"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1507,
            "cve_id": "CVE-2017-18241",
            "code_snippet": "int build_segment_manager(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_super_block *raw_super = F2FS_RAW_SUPER(sbi);\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tstruct f2fs_sm_info *sm_info;\n\tint err;\n\n\tsm_info = kzalloc(sizeof(struct f2fs_sm_info), GFP_KERNEL);\n\tif (!sm_info)\n\t\treturn -ENOMEM;\n\n\t/* init sm info */\n\tsbi->sm_info = sm_info;\n\tsm_info->seg0_blkaddr = le32_to_cpu(raw_super->segment0_blkaddr);\n\tsm_info->main_blkaddr = le32_to_cpu(raw_super->main_blkaddr);\n\tsm_info->segment_count = le32_to_cpu(raw_super->segment_count);\n\tsm_info->reserved_segments = le32_to_cpu(ckpt->rsvd_segment_count);\n\tsm_info->ovp_segments = le32_to_cpu(ckpt->overprov_segment_count);\n\tsm_info->main_segments = le32_to_cpu(raw_super->segment_count_main);\n\tsm_info->ssa_blkaddr = le32_to_cpu(raw_super->ssa_blkaddr);\n\tsm_info->rec_prefree_segments = sm_info->main_segments *\n\t\t\t\t\tDEF_RECLAIM_PREFREE_SEGMENTS / 100;\n\tif (sm_info->rec_prefree_segments > DEF_MAX_RECLAIM_PREFREE_SEGMENTS)\n\t\tsm_info->rec_prefree_segments = DEF_MAX_RECLAIM_PREFREE_SEGMENTS;\n\n\tif (!test_opt(sbi, LFS))\n\t\tsm_info->ipu_policy = 1 << F2FS_IPU_FSYNC;\n\tsm_info->min_ipu_util = DEF_MIN_IPU_UTIL;\n\tsm_info->min_fsync_blocks = DEF_MIN_FSYNC_BLOCKS;\n\tsm_info->min_hot_blocks = DEF_MIN_HOT_BLOCKS;\n\n\tsm_info->trim_sections = DEF_BATCHED_TRIM_SECTIONS;\n\n\tINIT_LIST_HEAD(&sm_info->sit_entry_set);\n\n\tif (!f2fs_readonly(sbi->sb)) {\n\t\terr = create_flush_cmd_control(sbi);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = create_discard_cmd_control(sbi);\n\tif (err)\n\t\treturn err;\n\n\terr = build_sit_info(sbi);\n\tif (err)\n\t\treturn err;\n\terr = build_free_segmap(sbi);\n\tif (err)\n\t\treturn err;\n\terr = build_curseg(sbi);\n\tif (err)\n\t\treturn err;\n\n\t/* reinit free segmap based on SIT */\n\tbuild_sit_entries(sbi);\n\n\tinit_free_segmap(sbi);\n\terr = build_dirty_segmap(sbi);\n\tif (err)\n\t\treturn err;\n\n\tinit_min_max_mtime(sbi);\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int f2fs_get_valid_checkpoint(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_checkpoint *cp_block;\n\tstruct f2fs_super_block *fsb = sbi->raw_super;\n\tstruct page *cp1, *cp2, *cur_page;\n\tunsigned long blk_size = sbi->blocksize;\n\tunsigned long long cp1_version = 0, cp2_version = 0;\n\tunsigned long long cp_start_blk_no;\n\tunsigned int cp_blks = 1 + __cp_payload(sbi);\n\tblock_t cp_blk_no;\n\tint i;\n\n\tsbi->ckpt = f2fs_kzalloc(sbi, array_size(blk_size, cp_blks),\n\t\t\t\t GFP_KERNEL);\n\tif (!sbi->ckpt)\n\t\treturn -ENOMEM;\n\t/*\n\t * Finding out valid cp block involves read both\n\t * sets( cp pack1 and cp pack 2)\n\t */\n\tcp_start_blk_no = le32_to_cpu(fsb->cp_blkaddr);\n\tcp1 = validate_checkpoint(sbi, cp_start_blk_no, &cp1_version);\n\n\t/* The second checkpoint pack should start at the next segment */\n\tcp_start_blk_no += ((unsigned long long)1) <<\n\t\t\t\tle32_to_cpu(fsb->log_blocks_per_seg);\n\tcp2 = validate_checkpoint(sbi, cp_start_blk_no, &cp2_version);\n\n\tif (cp1 && cp2) {\n\t\tif (ver_after(cp2_version, cp1_version))\n\t\t\tcur_page = cp2;\n\t\telse\n\t\t\tcur_page = cp1;\n\t} else if (cp1) {\n\t\tcur_page = cp1;\n\t} else if (cp2) {\n\t\tcur_page = cp2;\n\t} else {\n\t\tgoto fail_no_cp;\n\t}\n\n\tcp_block = (struct f2fs_checkpoint *)page_address(cur_page);\n\tmemcpy(sbi->ckpt, cp_block, blk_size);\n\n\t/* Sanity checking of checkpoint */\n\tif (f2fs_sanity_check_ckpt(sbi))\n\t\tgoto free_fail_no_cp;\n\n\tif (cur_page == cp1)\n\t\tsbi->cur_cp_pack = 1;\n\telse\n\t\tsbi->cur_cp_pack = 2;\n\n\tif (cp_blks <= 1)\n\t\tgoto done;\n\n\tcp_blk_no = le32_to_cpu(fsb->cp_blkaddr);\n\tif (cur_page == cp2)\n\t\tcp_blk_no += 1 << le32_to_cpu(fsb->log_blocks_per_seg);\n\n\tfor (i = 1; i < cp_blks; i++) {\n\t\tvoid *sit_bitmap_ptr;\n\t\tunsigned char *ckpt = (unsigned char *)sbi->ckpt;\n\n\t\tcur_page = f2fs_get_meta_page(sbi, cp_blk_no + i);\n\t\tif (IS_ERR(cur_page))\n\t\t\tgoto free_fail_no_cp;\n\t\tsit_bitmap_ptr = page_address(cur_page);\n\t\tmemcpy(ckpt + i * blk_size, sit_bitmap_ptr, blk_size);\n\t\tf2fs_put_page(cur_page, 1);\n\t}\ndone:\n\tf2fs_put_page(cp1, 1);\n\tf2fs_put_page(cp2, 1);\n\treturn 0;\n\nfree_fail_no_cp:\n\tf2fs_put_page(cp1, 1);\n\tf2fs_put_page(cp2, 1);\nfail_no_cp:\n\tkfree(sbi->ckpt);\n\treturn -EINVAL;\n}",
                        "code_after_change": "int f2fs_get_valid_checkpoint(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_checkpoint *cp_block;\n\tstruct f2fs_super_block *fsb = sbi->raw_super;\n\tstruct page *cp1, *cp2, *cur_page;\n\tunsigned long blk_size = sbi->blocksize;\n\tunsigned long long cp1_version = 0, cp2_version = 0;\n\tunsigned long long cp_start_blk_no;\n\tunsigned int cp_blks = 1 + __cp_payload(sbi);\n\tblock_t cp_blk_no;\n\tint i;\n\n\tsbi->ckpt = f2fs_kzalloc(sbi, array_size(blk_size, cp_blks),\n\t\t\t\t GFP_KERNEL);\n\tif (!sbi->ckpt)\n\t\treturn -ENOMEM;\n\t/*\n\t * Finding out valid cp block involves read both\n\t * sets( cp pack1 and cp pack 2)\n\t */\n\tcp_start_blk_no = le32_to_cpu(fsb->cp_blkaddr);\n\tcp1 = validate_checkpoint(sbi, cp_start_blk_no, &cp1_version);\n\n\t/* The second checkpoint pack should start at the next segment */\n\tcp_start_blk_no += ((unsigned long long)1) <<\n\t\t\t\tle32_to_cpu(fsb->log_blocks_per_seg);\n\tcp2 = validate_checkpoint(sbi, cp_start_blk_no, &cp2_version);\n\n\tif (cp1 && cp2) {\n\t\tif (ver_after(cp2_version, cp1_version))\n\t\t\tcur_page = cp2;\n\t\telse\n\t\t\tcur_page = cp1;\n\t} else if (cp1) {\n\t\tcur_page = cp1;\n\t} else if (cp2) {\n\t\tcur_page = cp2;\n\t} else {\n\t\tgoto fail_no_cp;\n\t}\n\n\tcp_block = (struct f2fs_checkpoint *)page_address(cur_page);\n\tmemcpy(sbi->ckpt, cp_block, blk_size);\n\n\tif (cur_page == cp1)\n\t\tsbi->cur_cp_pack = 1;\n\telse\n\t\tsbi->cur_cp_pack = 2;\n\n\t/* Sanity checking of checkpoint */\n\tif (f2fs_sanity_check_ckpt(sbi))\n\t\tgoto free_fail_no_cp;\n\n\tif (cp_blks <= 1)\n\t\tgoto done;\n\n\tcp_blk_no = le32_to_cpu(fsb->cp_blkaddr);\n\tif (cur_page == cp2)\n\t\tcp_blk_no += 1 << le32_to_cpu(fsb->log_blocks_per_seg);\n\n\tfor (i = 1; i < cp_blks; i++) {\n\t\tvoid *sit_bitmap_ptr;\n\t\tunsigned char *ckpt = (unsigned char *)sbi->ckpt;\n\n\t\tcur_page = f2fs_get_meta_page(sbi, cp_blk_no + i);\n\t\tif (IS_ERR(cur_page))\n\t\t\tgoto free_fail_no_cp;\n\t\tsit_bitmap_ptr = page_address(cur_page);\n\t\tmemcpy(ckpt + i * blk_size, sit_bitmap_ptr, blk_size);\n\t\tf2fs_put_page(cur_page, 1);\n\t}\ndone:\n\tf2fs_put_page(cp1, 1);\n\tf2fs_put_page(cp2, 1);\n\treturn 0;\n\nfree_fail_no_cp:\n\tf2fs_put_page(cp1, 1);\n\tf2fs_put_page(cp2, 1);\nfail_no_cp:\n\tkfree(sbi->ckpt);\n\treturn -EINVAL;\n}",
                        "cve_id": "CVE-2018-14614"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1536,
            "cve_id": "CVE-2017-2647",
            "code_snippet": "key_ref_t key_create_or_update(key_ref_t keyring_ref,\n\t\t\t       const char *type,\n\t\t\t       const char *description,\n\t\t\t       const void *payload,\n\t\t\t       size_t plen,\n\t\t\t       key_perm_t perm,\n\t\t\t       unsigned long flags)\n{\n\tstruct keyring_index_key index_key = {\n\t\t.description\t= description,\n\t};\n\tstruct key_preparsed_payload prep;\n\tstruct assoc_array_edit *edit;\n\tconst struct cred *cred = current_cred();\n\tstruct key *keyring, *key = NULL;\n\tkey_ref_t key_ref;\n\tint ret;\n\n\t/* look up the key type to see if it's one of the registered kernel\n\t * types */\n\tindex_key.type = key_type_lookup(type);\n\tif (IS_ERR(index_key.type)) {\n\t\tkey_ref = ERR_PTR(-ENODEV);\n\t\tgoto error;\n\t}\n\n\tkey_ref = ERR_PTR(-EINVAL);\n\tif (!index_key.type->instantiate ||\n\t    (!index_key.description && !index_key.type->preparse))\n\t\tgoto error_put_type;\n\n\tkeyring = key_ref_to_ptr(keyring_ref);\n\n\tkey_check(keyring);\n\n\tkey_ref = ERR_PTR(-ENOTDIR);\n\tif (keyring->type != &key_type_keyring)\n\t\tgoto error_put_type;\n\n\tmemset(&prep, 0, sizeof(prep));\n\tprep.data = payload;\n\tprep.datalen = plen;\n\tprep.quotalen = index_key.type->def_datalen;\n\tprep.trusted = flags & KEY_ALLOC_TRUSTED;\n\tprep.expiry = TIME_T_MAX;\n\tif (index_key.type->preparse) {\n\t\tret = index_key.type->preparse(&prep);\n\t\tif (ret < 0) {\n\t\t\tkey_ref = ERR_PTR(ret);\n\t\t\tgoto error_free_prep;\n\t\t}\n\t\tif (!index_key.description)\n\t\t\tindex_key.description = prep.description;\n\t\tkey_ref = ERR_PTR(-EINVAL);\n\t\tif (!index_key.description)\n\t\t\tgoto error_free_prep;\n\t}\n\tindex_key.desc_len = strlen(index_key.description);\n\n\tkey_ref = ERR_PTR(-EPERM);\n\tif (!prep.trusted && test_bit(KEY_FLAG_TRUSTED_ONLY, &keyring->flags))\n\t\tgoto error_free_prep;\n\tflags |= prep.trusted ? KEY_ALLOC_TRUSTED : 0;\n\n\tret = __key_link_begin(keyring, &index_key, &edit);\n\tif (ret < 0) {\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_free_prep;\n\t}\n\n\t/* if we're going to allocate a new key, we're going to have\n\t * to modify the keyring */\n\tret = key_permission(keyring_ref, KEY_NEED_WRITE);\n\tif (ret < 0) {\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_link_end;\n\t}\n\n\t/* if it's possible to update this type of key, search for an existing\n\t * key of the same type and description in the destination keyring and\n\t * update that instead if possible\n\t */\n\tif (index_key.type->update) {\n\t\tkey_ref = find_key_to_update(keyring_ref, &index_key);\n\t\tif (key_ref)\n\t\t\tgoto found_matching_key;\n\t}\n\n\t/* if the client doesn't provide, decide on the permissions we want */\n\tif (perm == KEY_PERM_UNDEF) {\n\t\tperm = KEY_POS_VIEW | KEY_POS_SEARCH | KEY_POS_LINK | KEY_POS_SETATTR;\n\t\tperm |= KEY_USR_VIEW;\n\n\t\tif (index_key.type->read)\n\t\t\tperm |= KEY_POS_READ;\n\n\t\tif (index_key.type == &key_type_keyring ||\n\t\t    index_key.type->update)\n\t\t\tperm |= KEY_POS_WRITE;\n\t}\n\n\t/* allocate a new key */\n\tkey = key_alloc(index_key.type, index_key.description,\n\t\t\tcred->fsuid, cred->fsgid, cred, perm, flags);\n\tif (IS_ERR(key)) {\n\t\tkey_ref = ERR_CAST(key);\n\t\tgoto error_link_end;\n\t}\n\n\t/* instantiate it and link it into the target keyring */\n\tret = __key_instantiate_and_link(key, &prep, keyring, NULL, &edit);\n\tif (ret < 0) {\n\t\tkey_put(key);\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_link_end;\n\t}\n\n\tkey_ref = make_key_ref(key, is_key_possessed(keyring_ref));\n\nerror_link_end:\n\t__key_link_end(keyring, &index_key, edit);\nerror_free_prep:\n\tif (index_key.type->preparse)\n\t\tindex_key.type->free_preparse(&prep);\nerror_put_type:\n\tkey_type_put(index_key.type);\nerror:\n\treturn key_ref;\n\n found_matching_key:\n\t/* we found a matching key, so we're going to try to update it\n\t * - we can drop the locks first as we have the key pinned\n\t */\n\t__key_link_end(keyring, &index_key, edit);\n\n\tkey_ref = __key_update(key_ref, &prep);\n\tgoto error_free_prep;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "key_ref_t key_create_or_update(key_ref_t keyring_ref,\n\t\t\t       const char *type,\n\t\t\t       const char *description,\n\t\t\t       const void *payload,\n\t\t\t       size_t plen,\n\t\t\t       key_perm_t perm,\n\t\t\t       unsigned long flags)\n{\n\tstruct keyring_index_key index_key = {\n\t\t.description\t= description,\n\t};\n\tstruct key_preparsed_payload prep;\n\tstruct assoc_array_edit *edit;\n\tconst struct cred *cred = current_cred();\n\tstruct key *keyring, *key = NULL;\n\tkey_ref_t key_ref;\n\tint ret;\n\tstruct key_restriction *restrict_link = NULL;\n\n\t/* look up the key type to see if it's one of the registered kernel\n\t * types */\n\tindex_key.type = key_type_lookup(type);\n\tif (IS_ERR(index_key.type)) {\n\t\tkey_ref = ERR_PTR(-ENODEV);\n\t\tgoto error;\n\t}\n\n\tkey_ref = ERR_PTR(-EINVAL);\n\tif (!index_key.type->instantiate ||\n\t    (!index_key.description && !index_key.type->preparse))\n\t\tgoto error_put_type;\n\n\tkeyring = key_ref_to_ptr(keyring_ref);\n\n\tkey_check(keyring);\n\n\tkey_ref = ERR_PTR(-EPERM);\n\tif (!(flags & KEY_ALLOC_BYPASS_RESTRICTION))\n\t\trestrict_link = keyring->restrict_link;\n\n\tkey_ref = ERR_PTR(-ENOTDIR);\n\tif (keyring->type != &key_type_keyring)\n\t\tgoto error_put_type;\n\n\tmemset(&prep, 0, sizeof(prep));\n\tprep.data = payload;\n\tprep.datalen = plen;\n\tprep.quotalen = index_key.type->def_datalen;\n\tprep.expiry = TIME_T_MAX;\n\tif (index_key.type->preparse) {\n\t\tret = index_key.type->preparse(&prep);\n\t\tif (ret < 0) {\n\t\t\tkey_ref = ERR_PTR(ret);\n\t\t\tgoto error_free_prep;\n\t\t}\n\t\tif (!index_key.description)\n\t\t\tindex_key.description = prep.description;\n\t\tkey_ref = ERR_PTR(-EINVAL);\n\t\tif (!index_key.description)\n\t\t\tgoto error_free_prep;\n\t}\n\tindex_key.desc_len = strlen(index_key.description);\n\n\tret = __key_link_begin(keyring, &index_key, &edit);\n\tif (ret < 0) {\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_free_prep;\n\t}\n\n\tif (restrict_link && restrict_link->check) {\n\t\tret = restrict_link->check(keyring, index_key.type,\n\t\t\t\t\t   &prep.payload, restrict_link->key);\n\t\tif (ret < 0) {\n\t\t\tkey_ref = ERR_PTR(ret);\n\t\t\tgoto error_link_end;\n\t\t}\n\t}\n\n\t/* if we're going to allocate a new key, we're going to have\n\t * to modify the keyring */\n\tret = key_permission(keyring_ref, KEY_NEED_WRITE);\n\tif (ret < 0) {\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_link_end;\n\t}\n\n\t/* if it's possible to update this type of key, search for an existing\n\t * key of the same type and description in the destination keyring and\n\t * update that instead if possible\n\t */\n\tif (index_key.type->update) {\n\t\tkey_ref = find_key_to_update(keyring_ref, &index_key);\n\t\tif (key_ref)\n\t\t\tgoto found_matching_key;\n\t}\n\n\t/* if the client doesn't provide, decide on the permissions we want */\n\tif (perm == KEY_PERM_UNDEF) {\n\t\tperm = KEY_POS_VIEW | KEY_POS_SEARCH | KEY_POS_LINK | KEY_POS_SETATTR;\n\t\tperm |= KEY_USR_VIEW;\n\n\t\tif (index_key.type->read)\n\t\t\tperm |= KEY_POS_READ;\n\n\t\tif (index_key.type == &key_type_keyring ||\n\t\t    index_key.type->update)\n\t\t\tperm |= KEY_POS_WRITE;\n\t}\n\n\t/* allocate a new key */\n\tkey = key_alloc(index_key.type, index_key.description,\n\t\t\tcred->fsuid, cred->fsgid, cred, perm, flags, NULL);\n\tif (IS_ERR(key)) {\n\t\tkey_ref = ERR_CAST(key);\n\t\tgoto error_link_end;\n\t}\n\n\t/* instantiate it and link it into the target keyring */\n\tret = __key_instantiate_and_link(key, &prep, keyring, NULL, &edit);\n\tif (ret < 0) {\n\t\tkey_put(key);\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_link_end;\n\t}\n\n\tkey_ref = make_key_ref(key, is_key_possessed(keyring_ref));\n\nerror_link_end:\n\t__key_link_end(keyring, &index_key, edit);\nerror_free_prep:\n\tif (index_key.type->preparse)\n\t\tindex_key.type->free_preparse(&prep);\nerror_put_type:\n\tkey_type_put(index_key.type);\nerror:\n\treturn key_ref;\n\n found_matching_key:\n\t/* we found a matching key, so we're going to try to update it\n\t * - we can drop the locks first as we have the key pinned\n\t */\n\t__key_link_end(keyring, &index_key, edit);\n\n\tkey_ref = __key_update(key_ref, &prep);\n\tgoto error_free_prep;\n}",
                        "code_after_change": "key_ref_t key_create_or_update(key_ref_t keyring_ref,\n\t\t\t       const char *type,\n\t\t\t       const char *description,\n\t\t\t       const void *payload,\n\t\t\t       size_t plen,\n\t\t\t       key_perm_t perm,\n\t\t\t       unsigned long flags)\n{\n\tstruct keyring_index_key index_key = {\n\t\t.description\t= description,\n\t};\n\tstruct key_preparsed_payload prep;\n\tstruct assoc_array_edit *edit;\n\tconst struct cred *cred = current_cred();\n\tstruct key *keyring, *key = NULL;\n\tkey_ref_t key_ref;\n\tint ret;\n\tstruct key_restriction *restrict_link = NULL;\n\n\t/* look up the key type to see if it's one of the registered kernel\n\t * types */\n\tindex_key.type = key_type_lookup(type);\n\tif (IS_ERR(index_key.type)) {\n\t\tkey_ref = ERR_PTR(-ENODEV);\n\t\tgoto error;\n\t}\n\n\tkey_ref = ERR_PTR(-EINVAL);\n\tif (!index_key.type->instantiate ||\n\t    (!index_key.description && !index_key.type->preparse))\n\t\tgoto error_put_type;\n\n\tkeyring = key_ref_to_ptr(keyring_ref);\n\n\tkey_check(keyring);\n\n\tkey_ref = ERR_PTR(-EPERM);\n\tif (!(flags & KEY_ALLOC_BYPASS_RESTRICTION))\n\t\trestrict_link = keyring->restrict_link;\n\n\tkey_ref = ERR_PTR(-ENOTDIR);\n\tif (keyring->type != &key_type_keyring)\n\t\tgoto error_put_type;\n\n\tmemset(&prep, 0, sizeof(prep));\n\tprep.data = payload;\n\tprep.datalen = plen;\n\tprep.quotalen = index_key.type->def_datalen;\n\tprep.expiry = TIME_T_MAX;\n\tif (index_key.type->preparse) {\n\t\tret = index_key.type->preparse(&prep);\n\t\tif (ret < 0) {\n\t\t\tkey_ref = ERR_PTR(ret);\n\t\t\tgoto error_free_prep;\n\t\t}\n\t\tif (!index_key.description)\n\t\t\tindex_key.description = prep.description;\n\t\tkey_ref = ERR_PTR(-EINVAL);\n\t\tif (!index_key.description)\n\t\t\tgoto error_free_prep;\n\t}\n\tindex_key.desc_len = strlen(index_key.description);\n\n\tret = __key_link_begin(keyring, &index_key, &edit);\n\tif (ret < 0) {\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_free_prep;\n\t}\n\n\tif (restrict_link && restrict_link->check) {\n\t\tret = restrict_link->check(keyring, index_key.type,\n\t\t\t\t\t   &prep.payload, restrict_link->key);\n\t\tif (ret < 0) {\n\t\t\tkey_ref = ERR_PTR(ret);\n\t\t\tgoto error_link_end;\n\t\t}\n\t}\n\n\t/* if we're going to allocate a new key, we're going to have\n\t * to modify the keyring */\n\tret = key_permission(keyring_ref, KEY_NEED_WRITE);\n\tif (ret < 0) {\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_link_end;\n\t}\n\n\t/* if it's possible to update this type of key, search for an existing\n\t * key of the same type and description in the destination keyring and\n\t * update that instead if possible\n\t */\n\tif (index_key.type->update) {\n\t\tkey_ref = find_key_to_update(keyring_ref, &index_key);\n\t\tif (key_ref)\n\t\t\tgoto found_matching_key;\n\t}\n\n\t/* if the client doesn't provide, decide on the permissions we want */\n\tif (perm == KEY_PERM_UNDEF) {\n\t\tperm = KEY_POS_VIEW | KEY_POS_SEARCH | KEY_POS_LINK | KEY_POS_SETATTR;\n\t\tperm |= KEY_USR_VIEW;\n\n\t\tif (index_key.type->read)\n\t\t\tperm |= KEY_POS_READ;\n\n\t\tif (index_key.type == &key_type_keyring ||\n\t\t    index_key.type->update)\n\t\t\tperm |= KEY_POS_WRITE;\n\t}\n\n\t/* allocate a new key */\n\tkey = key_alloc(index_key.type, index_key.description,\n\t\t\tcred->fsuid, cred->fsgid, cred, perm, flags, NULL);\n\tif (IS_ERR(key)) {\n\t\tkey_ref = ERR_CAST(key);\n\t\tgoto error_link_end;\n\t}\n\n\t/* instantiate it and link it into the target keyring */\n\tret = __key_instantiate_and_link(key, &prep, keyring, NULL, &edit);\n\tif (ret < 0) {\n\t\tkey_put(key);\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_link_end;\n\t}\n\n\tkey_ref = make_key_ref(key, is_key_possessed(keyring_ref));\n\nerror_link_end:\n\t__key_link_end(keyring, &index_key, edit);\nerror_free_prep:\n\tif (index_key.type->preparse)\n\t\tindex_key.type->free_preparse(&prep);\nerror_put_type:\n\tkey_type_put(index_key.type);\nerror:\n\treturn key_ref;\n\n found_matching_key:\n\t/* we found a matching key, so we're going to try to update it\n\t * - we can drop the locks first as we have the key pinned\n\t */\n\t__key_link_end(keyring, &index_key, edit);\n\n\tkey = key_ref_to_ptr(key_ref);\n\tif (test_bit(KEY_FLAG_USER_CONSTRUCT, &key->flags)) {\n\t\tret = wait_for_key_construction(key, true);\n\t\tif (ret < 0) {\n\t\t\tkey_ref_put(key_ref);\n\t\t\tkey_ref = ERR_PTR(ret);\n\t\t\tgoto error_free_prep;\n\t\t}\n\t}\n\n\tkey_ref = __key_update(key_ref, &prep);\n\tgoto error_free_prep;\n}",
                        "cve_id": "CVE-2017-15299"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1537,
            "cve_id": "CVE-2017-2647",
            "code_snippet": "key_ref_t keyring_search(key_ref_t keyring,\n\t\t\t struct key_type *type,\n\t\t\t const char *description)\n{\n\tstruct keyring_search_context ctx = {\n\t\t.index_key.type\t\t= type,\n\t\t.index_key.description\t= description,\n\t\t.cred\t\t\t= current_cred(),\n\t\t.match_data.cmp\t\t= key_default_cmp,\n\t\t.match_data.raw_data\t= description,\n\t\t.match_data.lookup_type\t= KEYRING_SEARCH_LOOKUP_DIRECT,\n\t\t.flags\t\t\t= KEYRING_SEARCH_DO_STATE_CHECK,\n\t};\n\tkey_ref_t key;\n\tint ret;\n\n\tif (type->match_preparse) {\n\t\tret = type->match_preparse(&ctx.match_data);\n\t\tif (ret < 0)\n\t\t\treturn ERR_PTR(ret);\n\t}\n\n\tkey = keyring_search_aux(keyring, &ctx);\n\n\tif (type->match_free)\n\t\ttype->match_free(&ctx.match_data);\n\treturn key;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "struct key *request_key_and_link(struct key_type *type,\n\t\t\t\t const char *description,\n\t\t\t\t const void *callout_info,\n\t\t\t\t size_t callout_len,\n\t\t\t\t void *aux,\n\t\t\t\t struct key *dest_keyring,\n\t\t\t\t unsigned long flags)\n{\n\tstruct keyring_search_context ctx = {\n\t\t.index_key.type\t\t= type,\n\t\t.index_key.description\t= description,\n\t\t.cred\t\t\t= current_cred(),\n\t\t.match_data.cmp\t\t= type->match,\n\t\t.match_data.raw_data\t= description,\n\t\t.match_data.lookup_type\t= KEYRING_SEARCH_LOOKUP_DIRECT,\n\t};\n\tstruct key *key;\n\tkey_ref_t key_ref;\n\tint ret;\n\n\tkenter(\"%s,%s,%p,%zu,%p,%p,%lx\",\n\t       ctx.index_key.type->name, ctx.index_key.description,\n\t       callout_info, callout_len, aux, dest_keyring, flags);\n\n\tif (type->match_preparse) {\n\t\tret = type->match_preparse(&ctx.match_data);\n\t\tif (ret < 0) {\n\t\t\tkey = ERR_PTR(ret);\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\t/* search all the process keyrings for a key */\n\tkey_ref = search_process_keyrings(&ctx);\n\n\tif (!IS_ERR(key_ref)) {\n\t\tkey = key_ref_to_ptr(key_ref);\n\t\tif (dest_keyring) {\n\t\t\tconstruct_get_dest_keyring(&dest_keyring);\n\t\t\tret = key_link(dest_keyring, key);\n\t\t\tkey_put(dest_keyring);\n\t\t\tif (ret < 0) {\n\t\t\t\tkey_put(key);\n\t\t\t\tkey = ERR_PTR(ret);\n\t\t\t\tgoto error_free;\n\t\t\t}\n\t\t}\n\t} else if (PTR_ERR(key_ref) != -EAGAIN) {\n\t\tkey = ERR_CAST(key_ref);\n\t} else  {\n\t\t/* the search failed, but the keyrings were searchable, so we\n\t\t * should consult userspace if we can */\n\t\tkey = ERR_PTR(-ENOKEY);\n\t\tif (!callout_info)\n\t\t\tgoto error_free;\n\n\t\tkey = construct_key_and_link(&ctx, callout_info, callout_len,\n\t\t\t\t\t     aux, dest_keyring, flags);\n\t}\n\nerror_free:\n\tif (type->match_free)\n\t\ttype->match_free(&ctx.match_data);\nerror:\n\tkleave(\" = %p\", key);\n\treturn key;\n}",
                        "code_after_change": "struct key *request_key_and_link(struct key_type *type,\n\t\t\t\t const char *description,\n\t\t\t\t const void *callout_info,\n\t\t\t\t size_t callout_len,\n\t\t\t\t void *aux,\n\t\t\t\t struct key *dest_keyring,\n\t\t\t\t unsigned long flags)\n{\n\tstruct keyring_search_context ctx = {\n\t\t.index_key.type\t\t= type,\n\t\t.index_key.description\t= description,\n\t\t.cred\t\t\t= current_cred(),\n\t\t.match_data.cmp\t\t= key_default_cmp,\n\t\t.match_data.raw_data\t= description,\n\t\t.match_data.lookup_type\t= KEYRING_SEARCH_LOOKUP_DIRECT,\n\t};\n\tstruct key *key;\n\tkey_ref_t key_ref;\n\tint ret;\n\n\tkenter(\"%s,%s,%p,%zu,%p,%p,%lx\",\n\t       ctx.index_key.type->name, ctx.index_key.description,\n\t       callout_info, callout_len, aux, dest_keyring, flags);\n\n\tif (type->match_preparse) {\n\t\tret = type->match_preparse(&ctx.match_data);\n\t\tif (ret < 0) {\n\t\t\tkey = ERR_PTR(ret);\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\t/* search all the process keyrings for a key */\n\tkey_ref = search_process_keyrings(&ctx);\n\n\tif (!IS_ERR(key_ref)) {\n\t\tkey = key_ref_to_ptr(key_ref);\n\t\tif (dest_keyring) {\n\t\t\tconstruct_get_dest_keyring(&dest_keyring);\n\t\t\tret = key_link(dest_keyring, key);\n\t\t\tkey_put(dest_keyring);\n\t\t\tif (ret < 0) {\n\t\t\t\tkey_put(key);\n\t\t\t\tkey = ERR_PTR(ret);\n\t\t\t\tgoto error_free;\n\t\t\t}\n\t\t}\n\t} else if (PTR_ERR(key_ref) != -EAGAIN) {\n\t\tkey = ERR_CAST(key_ref);\n\t} else  {\n\t\t/* the search failed, but the keyrings were searchable, so we\n\t\t * should consult userspace if we can */\n\t\tkey = ERR_PTR(-ENOKEY);\n\t\tif (!callout_info)\n\t\t\tgoto error_free;\n\n\t\tkey = construct_key_and_link(&ctx, callout_info, callout_len,\n\t\t\t\t\t     aux, dest_keyring, flags);\n\t}\n\nerror_free:\n\tif (type->match_free)\n\t\ttype->match_free(&ctx.match_data);\nerror:\n\tkleave(\" = %p\", key);\n\treturn key;\n}",
                        "cve_id": "CVE-2017-2647"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "key_ref_t key_create_or_update(key_ref_t keyring_ref,\n\t\t\t       const char *type,\n\t\t\t       const char *description,\n\t\t\t       const void *payload,\n\t\t\t       size_t plen,\n\t\t\t       key_perm_t perm,\n\t\t\t       unsigned long flags)\n{\n\tstruct keyring_index_key index_key = {\n\t\t.description\t= description,\n\t};\n\tstruct key_preparsed_payload prep;\n\tstruct assoc_array_edit *edit;\n\tconst struct cred *cred = current_cred();\n\tstruct key *keyring, *key = NULL;\n\tkey_ref_t key_ref;\n\tint ret;\n\tstruct key_restriction *restrict_link = NULL;\n\n\t/* look up the key type to see if it's one of the registered kernel\n\t * types */\n\tindex_key.type = key_type_lookup(type);\n\tif (IS_ERR(index_key.type)) {\n\t\tkey_ref = ERR_PTR(-ENODEV);\n\t\tgoto error;\n\t}\n\n\tkey_ref = ERR_PTR(-EINVAL);\n\tif (!index_key.type->instantiate ||\n\t    (!index_key.description && !index_key.type->preparse))\n\t\tgoto error_put_type;\n\n\tkeyring = key_ref_to_ptr(keyring_ref);\n\n\tkey_check(keyring);\n\n\tkey_ref = ERR_PTR(-EPERM);\n\tif (!(flags & KEY_ALLOC_BYPASS_RESTRICTION))\n\t\trestrict_link = keyring->restrict_link;\n\n\tkey_ref = ERR_PTR(-ENOTDIR);\n\tif (keyring->type != &key_type_keyring)\n\t\tgoto error_put_type;\n\n\tmemset(&prep, 0, sizeof(prep));\n\tprep.data = payload;\n\tprep.datalen = plen;\n\tprep.quotalen = index_key.type->def_datalen;\n\tprep.expiry = TIME_T_MAX;\n\tif (index_key.type->preparse) {\n\t\tret = index_key.type->preparse(&prep);\n\t\tif (ret < 0) {\n\t\t\tkey_ref = ERR_PTR(ret);\n\t\t\tgoto error_free_prep;\n\t\t}\n\t\tif (!index_key.description)\n\t\t\tindex_key.description = prep.description;\n\t\tkey_ref = ERR_PTR(-EINVAL);\n\t\tif (!index_key.description)\n\t\t\tgoto error_free_prep;\n\t}\n\tindex_key.desc_len = strlen(index_key.description);\n\n\tret = __key_link_begin(keyring, &index_key, &edit);\n\tif (ret < 0) {\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_free_prep;\n\t}\n\n\tif (restrict_link && restrict_link->check) {\n\t\tret = restrict_link->check(keyring, index_key.type,\n\t\t\t\t\t   &prep.payload, restrict_link->key);\n\t\tif (ret < 0) {\n\t\t\tkey_ref = ERR_PTR(ret);\n\t\t\tgoto error_link_end;\n\t\t}\n\t}\n\n\t/* if we're going to allocate a new key, we're going to have\n\t * to modify the keyring */\n\tret = key_permission(keyring_ref, KEY_NEED_WRITE);\n\tif (ret < 0) {\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_link_end;\n\t}\n\n\t/* if it's possible to update this type of key, search for an existing\n\t * key of the same type and description in the destination keyring and\n\t * update that instead if possible\n\t */\n\tif (index_key.type->update) {\n\t\tkey_ref = find_key_to_update(keyring_ref, &index_key);\n\t\tif (key_ref)\n\t\t\tgoto found_matching_key;\n\t}\n\n\t/* if the client doesn't provide, decide on the permissions we want */\n\tif (perm == KEY_PERM_UNDEF) {\n\t\tperm = KEY_POS_VIEW | KEY_POS_SEARCH | KEY_POS_LINK | KEY_POS_SETATTR;\n\t\tperm |= KEY_USR_VIEW;\n\n\t\tif (index_key.type->read)\n\t\t\tperm |= KEY_POS_READ;\n\n\t\tif (index_key.type == &key_type_keyring ||\n\t\t    index_key.type->update)\n\t\t\tperm |= KEY_POS_WRITE;\n\t}\n\n\t/* allocate a new key */\n\tkey = key_alloc(index_key.type, index_key.description,\n\t\t\tcred->fsuid, cred->fsgid, cred, perm, flags, NULL);\n\tif (IS_ERR(key)) {\n\t\tkey_ref = ERR_CAST(key);\n\t\tgoto error_link_end;\n\t}\n\n\t/* instantiate it and link it into the target keyring */\n\tret = __key_instantiate_and_link(key, &prep, keyring, NULL, &edit);\n\tif (ret < 0) {\n\t\tkey_put(key);\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_link_end;\n\t}\n\n\tkey_ref = make_key_ref(key, is_key_possessed(keyring_ref));\n\nerror_link_end:\n\t__key_link_end(keyring, &index_key, edit);\nerror_free_prep:\n\tif (index_key.type->preparse)\n\t\tindex_key.type->free_preparse(&prep);\nerror_put_type:\n\tkey_type_put(index_key.type);\nerror:\n\treturn key_ref;\n\n found_matching_key:\n\t/* we found a matching key, so we're going to try to update it\n\t * - we can drop the locks first as we have the key pinned\n\t */\n\t__key_link_end(keyring, &index_key, edit);\n\n\tkey_ref = __key_update(key_ref, &prep);\n\tgoto error_free_prep;\n}",
                        "code_after_change": "struct key *key_get_instantiation_authkey(key_serial_t target_id)\n{\n\tchar description[16];\n\tstruct keyring_search_context ctx = {\n\t\t.index_key.type\t\t= &key_type_request_key_auth,\n\t\t.index_key.description\t= description,\n\t\t.cred\t\t\t= current_cred(),\n\t\t.match_data.cmp\t\t= key_default_cmp,\n\t\t.match_data.raw_data\t= description,\n\t\t.match_data.lookup_type\t= KEYRING_SEARCH_LOOKUP_DIRECT,\n\t};\n\tstruct key *authkey;\n\tkey_ref_t authkey_ref;\n\n\tsprintf(description, \"%x\", target_id);\n\n\tauthkey_ref = search_process_keyrings(&ctx);\n\n\tif (IS_ERR(authkey_ref)) {\n\t\tauthkey = ERR_CAST(authkey_ref);\n\t\tif (authkey == ERR_PTR(-EAGAIN))\n\t\t\tauthkey = ERR_PTR(-ENOKEY);\n\t\tgoto error;\n\t}\n\n\tauthkey = key_ref_to_ptr(authkey_ref);\n\tif (test_bit(KEY_FLAG_REVOKED, &authkey->flags)) {\n\t\tkey_put(authkey);\n\t\tauthkey = ERR_PTR(-EKEYREVOKED);\n\t}\n\nerror:\n\treturn authkey;\n}",
                        "cve_id": "CVE-2017-15299"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "struct key *key_get_instantiation_authkey(key_serial_t target_id)\n{\n\tchar description[16];\n\tstruct keyring_search_context ctx = {\n\t\t.index_key.type\t\t= &key_type_request_key_auth,\n\t\t.index_key.description\t= description,\n\t\t.cred\t\t\t= current_cred(),\n\t\t.match_data.cmp\t\t= user_match,\n\t\t.match_data.raw_data\t= description,\n\t\t.match_data.lookup_type\t= KEYRING_SEARCH_LOOKUP_DIRECT,\n\t};\n\tstruct key *authkey;\n\tkey_ref_t authkey_ref;\n\n\tsprintf(description, \"%x\", target_id);\n\n\tauthkey_ref = search_process_keyrings(&ctx);\n\n\tif (IS_ERR(authkey_ref)) {\n\t\tauthkey = ERR_CAST(authkey_ref);\n\t\tif (authkey == ERR_PTR(-EAGAIN))\n\t\t\tauthkey = ERR_PTR(-ENOKEY);\n\t\tgoto error;\n\t}\n\n\tauthkey = key_ref_to_ptr(authkey_ref);\n\tif (test_bit(KEY_FLAG_REVOKED, &authkey->flags)) {\n\t\tkey_put(authkey);\n\t\tauthkey = ERR_PTR(-EKEYREVOKED);\n\t}\n\nerror:\n\treturn authkey;\n}",
                        "code_after_change": "key_ref_t key_create_or_update(key_ref_t keyring_ref,\n\t\t\t       const char *type,\n\t\t\t       const char *description,\n\t\t\t       const void *payload,\n\t\t\t       size_t plen,\n\t\t\t       key_perm_t perm,\n\t\t\t       unsigned long flags)\n{\n\tstruct keyring_index_key index_key = {\n\t\t.description\t= description,\n\t};\n\tstruct key_preparsed_payload prep;\n\tstruct assoc_array_edit *edit;\n\tconst struct cred *cred = current_cred();\n\tstruct key *keyring, *key = NULL;\n\tkey_ref_t key_ref;\n\tint ret;\n\tstruct key_restriction *restrict_link = NULL;\n\n\t/* look up the key type to see if it's one of the registered kernel\n\t * types */\n\tindex_key.type = key_type_lookup(type);\n\tif (IS_ERR(index_key.type)) {\n\t\tkey_ref = ERR_PTR(-ENODEV);\n\t\tgoto error;\n\t}\n\n\tkey_ref = ERR_PTR(-EINVAL);\n\tif (!index_key.type->instantiate ||\n\t    (!index_key.description && !index_key.type->preparse))\n\t\tgoto error_put_type;\n\n\tkeyring = key_ref_to_ptr(keyring_ref);\n\n\tkey_check(keyring);\n\n\tkey_ref = ERR_PTR(-EPERM);\n\tif (!(flags & KEY_ALLOC_BYPASS_RESTRICTION))\n\t\trestrict_link = keyring->restrict_link;\n\n\tkey_ref = ERR_PTR(-ENOTDIR);\n\tif (keyring->type != &key_type_keyring)\n\t\tgoto error_put_type;\n\n\tmemset(&prep, 0, sizeof(prep));\n\tprep.data = payload;\n\tprep.datalen = plen;\n\tprep.quotalen = index_key.type->def_datalen;\n\tprep.expiry = TIME_T_MAX;\n\tif (index_key.type->preparse) {\n\t\tret = index_key.type->preparse(&prep);\n\t\tif (ret < 0) {\n\t\t\tkey_ref = ERR_PTR(ret);\n\t\t\tgoto error_free_prep;\n\t\t}\n\t\tif (!index_key.description)\n\t\t\tindex_key.description = prep.description;\n\t\tkey_ref = ERR_PTR(-EINVAL);\n\t\tif (!index_key.description)\n\t\t\tgoto error_free_prep;\n\t}\n\tindex_key.desc_len = strlen(index_key.description);\n\n\tret = __key_link_begin(keyring, &index_key, &edit);\n\tif (ret < 0) {\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_free_prep;\n\t}\n\n\tif (restrict_link && restrict_link->check) {\n\t\tret = restrict_link->check(keyring, index_key.type,\n\t\t\t\t\t   &prep.payload, restrict_link->key);\n\t\tif (ret < 0) {\n\t\t\tkey_ref = ERR_PTR(ret);\n\t\t\tgoto error_link_end;\n\t\t}\n\t}\n\n\t/* if we're going to allocate a new key, we're going to have\n\t * to modify the keyring */\n\tret = key_permission(keyring_ref, KEY_NEED_WRITE);\n\tif (ret < 0) {\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_link_end;\n\t}\n\n\t/* if it's possible to update this type of key, search for an existing\n\t * key of the same type and description in the destination keyring and\n\t * update that instead if possible\n\t */\n\tif (index_key.type->update) {\n\t\tkey_ref = find_key_to_update(keyring_ref, &index_key);\n\t\tif (key_ref)\n\t\t\tgoto found_matching_key;\n\t}\n\n\t/* if the client doesn't provide, decide on the permissions we want */\n\tif (perm == KEY_PERM_UNDEF) {\n\t\tperm = KEY_POS_VIEW | KEY_POS_SEARCH | KEY_POS_LINK | KEY_POS_SETATTR;\n\t\tperm |= KEY_USR_VIEW;\n\n\t\tif (index_key.type->read)\n\t\t\tperm |= KEY_POS_READ;\n\n\t\tif (index_key.type == &key_type_keyring ||\n\t\t    index_key.type->update)\n\t\t\tperm |= KEY_POS_WRITE;\n\t}\n\n\t/* allocate a new key */\n\tkey = key_alloc(index_key.type, index_key.description,\n\t\t\tcred->fsuid, cred->fsgid, cred, perm, flags, NULL);\n\tif (IS_ERR(key)) {\n\t\tkey_ref = ERR_CAST(key);\n\t\tgoto error_link_end;\n\t}\n\n\t/* instantiate it and link it into the target keyring */\n\tret = __key_instantiate_and_link(key, &prep, keyring, NULL, &edit);\n\tif (ret < 0) {\n\t\tkey_put(key);\n\t\tkey_ref = ERR_PTR(ret);\n\t\tgoto error_link_end;\n\t}\n\n\tkey_ref = make_key_ref(key, is_key_possessed(keyring_ref));\n\nerror_link_end:\n\t__key_link_end(keyring, &index_key, edit);\nerror_free_prep:\n\tif (index_key.type->preparse)\n\t\tindex_key.type->free_preparse(&prep);\nerror_put_type:\n\tkey_type_put(index_key.type);\nerror:\n\treturn key_ref;\n\n found_matching_key:\n\t/* we found a matching key, so we're going to try to update it\n\t * - we can drop the locks first as we have the key pinned\n\t */\n\t__key_link_end(keyring, &index_key, edit);\n\n\tkey = key_ref_to_ptr(key_ref);\n\tif (test_bit(KEY_FLAG_USER_CONSTRUCT, &key->flags)) {\n\t\tret = wait_for_key_construction(key, true);\n\t\tif (ret < 0) {\n\t\t\tkey_ref_put(key_ref);\n\t\t\tkey_ref = ERR_PTR(ret);\n\t\t\tgoto error_free_prep;\n\t\t}\n\t}\n\n\tkey_ref = __key_update(key_ref, &prep);\n\tgoto error_free_prep;\n}",
                        "cve_id": "CVE-2017-2647"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1584,
            "cve_id": "CVE-2017-7374",
            "code_snippet": "int fscrypt_setup_filename(struct inode *dir, const struct qstr *iname,\n\t\t\t      int lookup, struct fscrypt_name *fname)\n{\n\tint ret = 0, bigname = 0;\n\n\tmemset(fname, 0, sizeof(struct fscrypt_name));\n\tfname->usr_fname = iname;\n\n\tif (!dir->i_sb->s_cop->is_encrypted(dir) ||\n\t\t\t\tfscrypt_is_dot_dotdot(iname)) {\n\t\tfname->disk_name.name = (unsigned char *)iname->name;\n\t\tfname->disk_name.len = iname->len;\n\t\treturn 0;\n\t}\n\tret = fscrypt_get_encryption_info(dir);\n\tif (ret && ret != -EOPNOTSUPP)\n\t\treturn ret;\n\n\tif (dir->i_crypt_info) {\n\t\tret = fscrypt_fname_alloc_buffer(dir, iname->len,\n\t\t\t\t\t\t\t&fname->crypto_buf);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = fname_encrypt(dir, iname, &fname->crypto_buf);\n\t\tif (ret)\n\t\t\tgoto errout;\n\t\tfname->disk_name.name = fname->crypto_buf.name;\n\t\tfname->disk_name.len = fname->crypto_buf.len;\n\t\treturn 0;\n\t}\n\tif (!lookup)\n\t\treturn -ENOKEY;\n\n\t/*\n\t * We don't have the key and we are doing a lookup; decode the\n\t * user-supplied name\n\t */\n\tif (iname->name[0] == '_')\n\t\tbigname = 1;\n\tif ((bigname && (iname->len != 33)) || (!bigname && (iname->len > 43)))\n\t\treturn -ENOENT;\n\n\tfname->crypto_buf.name = kmalloc(32, GFP_KERNEL);\n\tif (fname->crypto_buf.name == NULL)\n\t\treturn -ENOMEM;\n\n\tret = digest_decode(iname->name + bigname, iname->len - bigname,\n\t\t\t\tfname->crypto_buf.name);\n\tif (ret < 0) {\n\t\tret = -ENOENT;\n\t\tgoto errout;\n\t}\n\tfname->crypto_buf.len = ret;\n\tif (bigname) {\n\t\tmemcpy(&fname->hash, fname->crypto_buf.name, 4);\n\t\tmemcpy(&fname->minor_hash, fname->crypto_buf.name + 4, 4);\n\t} else {\n\t\tfname->disk_name.name = fname->crypto_buf.name;\n\t\tfname->disk_name.len = fname->crypto_buf.len;\n\t}\n\treturn 0;\n\nerrout:\n\tfscrypt_fname_free_buffer(&fname->crypto_buf);\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static struct dentry *hfsplus_lookup(struct inode *dir, struct dentry *dentry,\n\t\t\t\t     unsigned int flags)\n{\n\tstruct inode *inode = NULL;\n\tstruct hfs_find_data fd;\n\tstruct super_block *sb;\n\thfsplus_cat_entry entry;\n\tint err;\n\tu32 cnid, linkid = 0;\n\tu16 type;\n\n\tsb = dir->i_sb;\n\n\tdentry->d_fsdata = NULL;\n\terr = hfs_find_init(HFSPLUS_SB(sb)->cat_tree, &fd);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\terr = hfsplus_cat_build_key(sb, fd.search_key, dir->i_ino,\n\t\t\t&dentry->d_name);\n\tif (unlikely(err < 0))\n\t\tgoto fail;\nagain:\n\terr = hfs_brec_read(&fd, &entry, sizeof(entry));\n\tif (err) {\n\t\tif (err == -ENOENT) {\n\t\t\thfs_find_exit(&fd);\n\t\t\t/* No such entry */\n\t\t\tinode = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tgoto fail;\n\t}\n\ttype = be16_to_cpu(entry.type);\n\tif (type == HFSPLUS_FOLDER) {\n\t\tif (fd.entrylength < sizeof(struct hfsplus_cat_folder)) {\n\t\t\terr = -EIO;\n\t\t\tgoto fail;\n\t\t}\n\t\tcnid = be32_to_cpu(entry.folder.id);\n\t\tdentry->d_fsdata = (void *)(unsigned long)cnid;\n\t} else if (type == HFSPLUS_FILE) {\n\t\tif (fd.entrylength < sizeof(struct hfsplus_cat_file)) {\n\t\t\terr = -EIO;\n\t\t\tgoto fail;\n\t\t}\n\t\tcnid = be32_to_cpu(entry.file.id);\n\t\tif (entry.file.user_info.fdType ==\n\t\t\t\tcpu_to_be32(HFSP_HARDLINK_TYPE) &&\n\t\t\t\tentry.file.user_info.fdCreator ==\n\t\t\t\tcpu_to_be32(HFSP_HFSPLUS_CREATOR) &&\n\t\t\t\t(entry.file.create_date ==\n\t\t\t\t\tHFSPLUS_I(HFSPLUS_SB(sb)->hidden_dir)->\n\t\t\t\t\t\tcreate_date ||\n\t\t\t\tentry.file.create_date ==\n\t\t\t\t\tHFSPLUS_I(d_inode(sb->s_root))->\n\t\t\t\t\t\tcreate_date) &&\n\t\t\t\tHFSPLUS_SB(sb)->hidden_dir) {\n\t\t\tstruct qstr str;\n\t\t\tchar name[32];\n\n\t\t\tif (dentry->d_fsdata) {\n\t\t\t\t/*\n\t\t\t\t * We found a link pointing to another link,\n\t\t\t\t * so ignore it and treat it as regular file.\n\t\t\t\t */\n\t\t\t\tcnid = (unsigned long)dentry->d_fsdata;\n\t\t\t\tlinkid = 0;\n\t\t\t} else {\n\t\t\t\tdentry->d_fsdata = (void *)(unsigned long)cnid;\n\t\t\t\tlinkid =\n\t\t\t\t\tbe32_to_cpu(entry.file.permissions.dev);\n\t\t\t\tstr.len = sprintf(name, \"iNode%d\", linkid);\n\t\t\t\tstr.name = name;\n\t\t\t\terr = hfsplus_cat_build_key(sb, fd.search_key,\n\t\t\t\t\tHFSPLUS_SB(sb)->hidden_dir->i_ino,\n\t\t\t\t\t&str);\n\t\t\t\tif (unlikely(err < 0))\n\t\t\t\t\tgoto fail;\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t} else if (!dentry->d_fsdata)\n\t\t\tdentry->d_fsdata = (void *)(unsigned long)cnid;\n\t} else {\n\t\tpr_err(\"invalid catalog entry type in lookup\\n\");\n\t\terr = -EIO;\n\t\tgoto fail;\n\t}\n\thfs_find_exit(&fd);\n\tinode = hfsplus_iget(dir->i_sb, cnid);\n\tif (IS_ERR(inode))\n\t\treturn ERR_CAST(inode);\n\tif (S_ISREG(inode->i_mode))\n\t\tHFSPLUS_I(inode)->linkid = linkid;\nout:\n\treturn d_splice_alias(inode, dentry);\nfail:\n\thfs_find_exit(&fd);\n\treturn ERR_PTR(err);\n}",
                        "code_after_change": "static int fscrypt_d_revalidate(struct dentry *dentry, unsigned int flags)\n{\n\tstruct dentry *dir;\n\tint dir_has_key, cached_with_key;\n\n\tif (flags & LOOKUP_RCU)\n\t\treturn -ECHILD;\n\n\tdir = dget_parent(dentry);\n\tif (!d_inode(dir)->i_sb->s_cop->is_encrypted(d_inode(dir))) {\n\t\tdput(dir);\n\t\treturn 0;\n\t}\n\n\t/* this should eventually be an flag in d_flags */\n\tspin_lock(&dentry->d_lock);\n\tcached_with_key = dentry->d_flags & DCACHE_ENCRYPTED_WITH_KEY;\n\tspin_unlock(&dentry->d_lock);\n\tdir_has_key = (d_inode(dir)->i_crypt_info != NULL);\n\tdput(dir);\n\n\t/*\n\t * If the dentry was cached without the key, and it is a\n\t * negative dentry, it might be a valid name.  We can't check\n\t * if the key has since been made available due to locking\n\t * reasons, so we fail the validation so ext4_lookup() can do\n\t * this check.\n\t *\n\t * We also fail the validation if the dentry was created with\n\t * the key present, but we no longer have the key, or vice versa.\n\t */\n\tif ((!cached_with_key && d_is_negative(dentry)) ||\n\t\t\t(!cached_with_key && dir_has_key) ||\n\t\t\t(cached_with_key && !dir_has_key))\n\t\treturn 0;\n\treturn 1;\n}",
                        "cve_id": "CVE-2018-14617"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1585,
            "cve_id": "CVE-2017-7374",
            "code_snippet": "int fscrypt_get_encryption_info(struct inode *inode)\n{\n\tstruct fscrypt_info *crypt_info;\n\tstruct fscrypt_context ctx;\n\tstruct crypto_skcipher *ctfm;\n\tconst char *cipher_str;\n\tint keysize;\n\tu8 *raw_key = NULL;\n\tint res;\n\n\tif (inode->i_crypt_info)\n\t\treturn 0;\n\n\tres = fscrypt_initialize(inode->i_sb->s_cop->flags);\n\tif (res)\n\t\treturn res;\n\n\tif (!inode->i_sb->s_cop->get_context)\n\t\treturn -EOPNOTSUPP;\n\n\tres = inode->i_sb->s_cop->get_context(inode, &ctx, sizeof(ctx));\n\tif (res < 0) {\n\t\tif (!fscrypt_dummy_context_enabled(inode) ||\n\t\t    inode->i_sb->s_cop->is_encrypted(inode))\n\t\t\treturn res;\n\t\t/* Fake up a context for an unencrypted directory */\n\t\tmemset(&ctx, 0, sizeof(ctx));\n\t\tctx.format = FS_ENCRYPTION_CONTEXT_FORMAT_V1;\n\t\tctx.contents_encryption_mode = FS_ENCRYPTION_MODE_AES_256_XTS;\n\t\tctx.filenames_encryption_mode = FS_ENCRYPTION_MODE_AES_256_CTS;\n\t\tmemset(ctx.master_key_descriptor, 0x42, FS_KEY_DESCRIPTOR_SIZE);\n\t} else if (res != sizeof(ctx)) {\n\t\treturn -EINVAL;\n\t}\n\n\tif (ctx.format != FS_ENCRYPTION_CONTEXT_FORMAT_V1)\n\t\treturn -EINVAL;\n\n\tif (ctx.flags & ~FS_POLICY_FLAGS_VALID)\n\t\treturn -EINVAL;\n\n\tcrypt_info = kmem_cache_alloc(fscrypt_info_cachep, GFP_NOFS);\n\tif (!crypt_info)\n\t\treturn -ENOMEM;\n\n\tcrypt_info->ci_flags = ctx.flags;\n\tcrypt_info->ci_data_mode = ctx.contents_encryption_mode;\n\tcrypt_info->ci_filename_mode = ctx.filenames_encryption_mode;\n\tcrypt_info->ci_ctfm = NULL;\n\tmemcpy(crypt_info->ci_master_key, ctx.master_key_descriptor,\n\t\t\t\tsizeof(crypt_info->ci_master_key));\n\n\tres = determine_cipher_type(crypt_info, inode, &cipher_str, &keysize);\n\tif (res)\n\t\tgoto out;\n\n\t/*\n\t * This cannot be a stack buffer because it is passed to the scatterlist\n\t * crypto API as part of key derivation.\n\t */\n\tres = -ENOMEM;\n\traw_key = kmalloc(FS_MAX_KEY_SIZE, GFP_NOFS);\n\tif (!raw_key)\n\t\tgoto out;\n\n\tres = validate_user_key(crypt_info, &ctx, raw_key, FS_KEY_DESC_PREFIX);\n\tif (res && inode->i_sb->s_cop->key_prefix) {\n\t\tint res2 = validate_user_key(crypt_info, &ctx, raw_key,\n\t\t\t\t\t     inode->i_sb->s_cop->key_prefix);\n\t\tif (res2) {\n\t\t\tif (res2 == -ENOKEY)\n\t\t\t\tres = -ENOKEY;\n\t\t\tgoto out;\n\t\t}\n\t} else if (res) {\n\t\tgoto out;\n\t}\n\tctfm = crypto_alloc_skcipher(cipher_str, 0, 0);\n\tif (!ctfm || IS_ERR(ctfm)) {\n\t\tres = ctfm ? PTR_ERR(ctfm) : -ENOMEM;\n\t\tprintk(KERN_DEBUG\n\t\t       \"%s: error %d (inode %u) allocating crypto tfm\\n\",\n\t\t       __func__, res, (unsigned) inode->i_ino);\n\t\tgoto out;\n\t}\n\tcrypt_info->ci_ctfm = ctfm;\n\tcrypto_skcipher_clear_flags(ctfm, ~0);\n\tcrypto_skcipher_set_flags(ctfm, CRYPTO_TFM_REQ_WEAK_KEY);\n\tres = crypto_skcipher_setkey(ctfm, raw_key, keysize);\n\tif (res)\n\t\tgoto out;\n\n\tif (cmpxchg(&inode->i_crypt_info, NULL, crypt_info) == NULL)\n\t\tcrypt_info = NULL;\nout:\n\tif (res == -ENOKEY)\n\t\tres = 0;\n\tput_crypt_info(crypt_info);\n\tkzfree(raw_key);\n\treturn res;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int validate_user_key(struct fscrypt_info *crypt_info,\n\t\t\tstruct fscrypt_context *ctx, u8 *raw_key,\n\t\t\tconst char *prefix)\n{\n\tchar *description;\n\tstruct key *keyring_key;\n\tstruct fscrypt_key *master_key;\n\tconst struct user_key_payload *ukp;\n\tint res;\n\n\tdescription = kasprintf(GFP_NOFS, \"%s%*phN\", prefix,\n\t\t\t\tFS_KEY_DESCRIPTOR_SIZE,\n\t\t\t\tctx->master_key_descriptor);\n\tif (!description)\n\t\treturn -ENOMEM;\n\n\tkeyring_key = request_key(&key_type_logon, description, NULL);\n\tkfree(description);\n\tif (IS_ERR(keyring_key))\n\t\treturn PTR_ERR(keyring_key);\n\n\tif (keyring_key->type != &key_type_logon) {\n\t\tprintk_once(KERN_WARNING\n\t\t\t\t\"%s: key type must be logon\\n\", __func__);\n\t\tres = -ENOKEY;\n\t\tgoto out;\n\t}\n\tdown_read(&keyring_key->sem);\n\tukp = user_key_payload(keyring_key);\n\tif (ukp->datalen != sizeof(struct fscrypt_key)) {\n\t\tres = -EINVAL;\n\t\tup_read(&keyring_key->sem);\n\t\tgoto out;\n\t}\n\tmaster_key = (struct fscrypt_key *)ukp->data;\n\tBUILD_BUG_ON(FS_AES_128_ECB_KEY_SIZE != FS_KEY_DERIVATION_NONCE_SIZE);\n\n\tif (master_key->size != FS_AES_256_XTS_KEY_SIZE) {\n\t\tprintk_once(KERN_WARNING\n\t\t\t\t\"%s: key size incorrect: %d\\n\",\n\t\t\t\t__func__, master_key->size);\n\t\tres = -ENOKEY;\n\t\tup_read(&keyring_key->sem);\n\t\tgoto out;\n\t}\n\tres = derive_key_aes(ctx->nonce, master_key->raw, raw_key);\n\tup_read(&keyring_key->sem);\n\tif (res)\n\t\tgoto out;\n\n\tcrypt_info->ci_keyring_key = keyring_key;\n\treturn 0;\nout:\n\tkey_put(keyring_key);\n\treturn res;\n}",
                        "code_after_change": "static int validate_user_key(struct fscrypt_info *crypt_info,\n\t\t\tstruct fscrypt_context *ctx, u8 *raw_key,\n\t\t\tconst char *prefix)\n{\n\tchar *description;\n\tstruct key *keyring_key;\n\tstruct fscrypt_key *master_key;\n\tconst struct user_key_payload *ukp;\n\tint res;\n\n\tdescription = kasprintf(GFP_NOFS, \"%s%*phN\", prefix,\n\t\t\t\tFS_KEY_DESCRIPTOR_SIZE,\n\t\t\t\tctx->master_key_descriptor);\n\tif (!description)\n\t\treturn -ENOMEM;\n\n\tkeyring_key = request_key(&key_type_logon, description, NULL);\n\tkfree(description);\n\tif (IS_ERR(keyring_key))\n\t\treturn PTR_ERR(keyring_key);\n\tdown_read(&keyring_key->sem);\n\n\tif (keyring_key->type != &key_type_logon) {\n\t\tprintk_once(KERN_WARNING\n\t\t\t\t\"%s: key type must be logon\\n\", __func__);\n\t\tres = -ENOKEY;\n\t\tgoto out;\n\t}\n\tukp = user_key_payload(keyring_key);\n\tif (ukp->datalen != sizeof(struct fscrypt_key)) {\n\t\tres = -EINVAL;\n\t\tgoto out;\n\t}\n\tmaster_key = (struct fscrypt_key *)ukp->data;\n\tBUILD_BUG_ON(FS_AES_128_ECB_KEY_SIZE != FS_KEY_DERIVATION_NONCE_SIZE);\n\n\tif (master_key->size != FS_AES_256_XTS_KEY_SIZE) {\n\t\tprintk_once(KERN_WARNING\n\t\t\t\t\"%s: key size incorrect: %d\\n\",\n\t\t\t\t__func__, master_key->size);\n\t\tres = -ENOKEY;\n\t\tgoto out;\n\t}\n\tres = derive_key_aes(ctx->nonce, master_key->raw, raw_key);\nout:\n\tup_read(&keyring_key->sem);\n\tkey_put(keyring_key);\n\treturn res;\n}",
                        "cve_id": "CVE-2017-7374"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1637,
            "cve_id": "CVE-2017-8106",
            "code_snippet": "static int handle_invept(struct kvm_vcpu *vcpu)\n{\n\tu32 vmx_instruction_info, types;\n\tunsigned long type;\n\tgva_t gva;\n\tstruct x86_exception e;\n\tstruct {\n\t\tu64 eptp, gpa;\n\t} operand;\n\n\tif (!(nested_vmx_secondary_ctls_high & SECONDARY_EXEC_ENABLE_EPT) ||\n\t    !(nested_vmx_ept_caps & VMX_EPT_INVEPT_BIT)) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tif (!kvm_read_cr0_bits(vcpu, X86_CR0_PE)) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tvmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);\n\ttype = kvm_register_read(vcpu, (vmx_instruction_info >> 28) & 0xf);\n\n\ttypes = (nested_vmx_ept_caps >> VMX_EPT_EXTENT_SHIFT) & 6;\n\n\tif (!(types & (1UL << type))) {\n\t\tnested_vmx_failValid(vcpu,\n\t\t\t\tVMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);\n\t\treturn 1;\n\t}\n\n\t/* According to the Intel VMX instruction reference, the memory\n\t * operand is read even if it isn't needed (e.g., for type==global)\n\t */\n\tif (get_vmx_mem_address(vcpu, vmcs_readl(EXIT_QUALIFICATION),\n\t\t\tvmx_instruction_info, &gva))\n\t\treturn 1;\n\tif (kvm_read_guest_virt(&vcpu->arch.emulate_ctxt, gva, &operand,\n\t\t\t\tsizeof(operand), &e)) {\n\t\tkvm_inject_page_fault(vcpu, &e);\n\t\treturn 1;\n\t}\n\n\tswitch (type) {\n\tcase VMX_EPT_EXTENT_GLOBAL:\n\t\tkvm_mmu_sync_roots(vcpu);\n\t\tkvm_mmu_flush_tlb(vcpu);\n\t\tnested_vmx_succeed(vcpu);\n\t\tbreak;\n\tdefault:\n\t\t/* Trap single context invalidation invept calls */\n\t\tBUG_ON(1);\n\t\tbreak;\n\t}\n\n\tskip_emulated_instruction(vcpu);\n\treturn 1;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,\n\t\t\t    int emulation_type, void *insn, int insn_len)\n{\n\tint r;\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tbool writeback = true;\n\tbool write_fault_to_spt;\n\n\tif (unlikely(!kvm_can_emulate_insn(vcpu, emulation_type, insn, insn_len)))\n\t\treturn 1;\n\n\tvcpu->arch.l1tf_flush_l1d = true;\n\n\t/*\n\t * Clear write_fault_to_shadow_pgtable here to ensure it is\n\t * never reused.\n\t */\n\twrite_fault_to_spt = vcpu->arch.write_fault_to_shadow_pgtable;\n\tvcpu->arch.write_fault_to_shadow_pgtable = false;\n\n\tif (!(emulation_type & EMULTYPE_NO_DECODE)) {\n\t\tkvm_clear_exception_queue(vcpu);\n\n\t\tr = x86_decode_emulated_instruction(vcpu, emulation_type,\n\t\t\t\t\t\t    insn, insn_len);\n\t\tif (r != EMULATION_OK)  {\n\t\t\tif ((emulation_type & EMULTYPE_TRAP_UD) ||\n\t\t\t    (emulation_type & EMULTYPE_TRAP_UD_FORCED)) {\n\t\t\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\tif (reexecute_instruction(vcpu, cr2_or_gpa,\n\t\t\t\t\t\t  write_fault_to_spt,\n\t\t\t\t\t\t  emulation_type))\n\t\t\t\treturn 1;\n\t\t\tif (ctxt->have_exception) {\n\t\t\t\t/*\n\t\t\t\t * #UD should result in just EMULATION_FAILED, and trap-like\n\t\t\t\t * exception should not be encountered during decode.\n\t\t\t\t */\n\t\t\t\tWARN_ON_ONCE(ctxt->exception.vector == UD_VECTOR ||\n\t\t\t\t\t     exception_type(ctxt->exception.vector) == EXCPT_TRAP);\n\t\t\t\tinject_emulated_exception(vcpu);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t\t}\n\t}\n\n\tif ((emulation_type & EMULTYPE_VMWARE_GP) &&\n\t    !is_vmware_backdoor_opcode(ctxt)) {\n\t\tkvm_queue_exception_e(vcpu, GP_VECTOR, 0);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * EMULTYPE_SKIP without EMULTYPE_COMPLETE_USER_EXIT is intended for\n\t * use *only* by vendor callbacks for kvm_skip_emulated_instruction().\n\t * The caller is responsible for updating interruptibility state and\n\t * injecting single-step #DBs.\n\t */\n\tif (emulation_type & EMULTYPE_SKIP) {\n\t\tif (ctxt->mode != X86EMUL_MODE_PROT64)\n\t\t\tctxt->eip = (u32)ctxt->_eip;\n\t\telse\n\t\t\tctxt->eip = ctxt->_eip;\n\n\t\tif (emulation_type & EMULTYPE_COMPLETE_USER_EXIT) {\n\t\t\tr = 1;\n\t\t\tgoto writeback;\n\t\t}\n\n\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\tif (ctxt->eflags & X86_EFLAGS_RF)\n\t\t\tkvm_set_rflags(vcpu, ctxt->eflags & ~X86_EFLAGS_RF);\n\t\treturn 1;\n\t}\n\n\tif (retry_instruction(ctxt, cr2_or_gpa, emulation_type))\n\t\treturn 1;\n\n\t/* this is needed for vmware backdoor interface to work since it\n\t   changes registers values  during IO operation */\n\tif (vcpu->arch.emulate_regs_need_sync_from_vcpu) {\n\t\tvcpu->arch.emulate_regs_need_sync_from_vcpu = false;\n\t\temulator_invalidate_register_cache(ctxt);\n\t}\n\nrestart:\n\tif (emulation_type & EMULTYPE_PF) {\n\t\t/* Save the faulting GPA (cr2) in the address field */\n\t\tctxt->exception.address = cr2_or_gpa;\n\n\t\t/* With shadow page tables, cr2 contains a GVA or nGPA. */\n\t\tif (vcpu->arch.mmu->root_role.direct) {\n\t\t\tctxt->gpa_available = true;\n\t\t\tctxt->gpa_val = cr2_or_gpa;\n\t\t}\n\t} else {\n\t\t/* Sanitize the address out of an abundance of paranoia. */\n\t\tctxt->exception.address = 0;\n\t}\n\n\tr = x86_emulate_insn(ctxt);\n\n\tif (r == EMULATION_INTERCEPTED)\n\t\treturn 1;\n\n\tif (r == EMULATION_FAILED) {\n\t\tif (reexecute_instruction(vcpu, cr2_or_gpa, write_fault_to_spt,\n\t\t\t\t\temulation_type))\n\t\t\treturn 1;\n\n\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t}\n\n\tif (ctxt->have_exception) {\n\t\tr = 1;\n\t\tif (inject_emulated_exception(vcpu))\n\t\t\treturn r;\n\t} else if (vcpu->arch.pio.count) {\n\t\tif (!vcpu->arch.pio.in) {\n\t\t\t/* FIXME: return into emulator if single-stepping.  */\n\t\t\tvcpu->arch.pio.count = 0;\n\t\t} else {\n\t\t\twriteback = false;\n\t\t\tvcpu->arch.complete_userspace_io = complete_emulated_pio;\n\t\t}\n\t\tr = 0;\n\t} else if (vcpu->mmio_needed) {\n\t\t++vcpu->stat.mmio_exits;\n\n\t\tif (!vcpu->mmio_is_write)\n\t\t\twriteback = false;\n\t\tr = 0;\n\t\tvcpu->arch.complete_userspace_io = complete_emulated_mmio;\n\t} else if (vcpu->arch.complete_userspace_io) {\n\t\twriteback = false;\n\t\tr = 0;\n\t} else if (r == EMULATION_RESTART)\n\t\tgoto restart;\n\telse\n\t\tr = 1;\n\nwriteback:\n\tif (writeback) {\n\t\tunsigned long rflags = static_call(kvm_x86_get_rflags)(vcpu);\n\t\ttoggle_interruptibility(vcpu, ctxt->interruptibility);\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = false;\n\t\tif (!ctxt->have_exception ||\n\t\t    exception_type(ctxt->exception.vector) == EXCPT_TRAP) {\n\t\t\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);\n\t\t\tif (ctxt->is_branch)\n\t\t\t\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);\n\t\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\t\tif (r && (ctxt->tf || (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)))\n\t\t\t\tr = kvm_vcpu_do_singlestep(vcpu);\n\t\t\tstatic_call_cond(kvm_x86_update_emulated_instruction)(vcpu);\n\t\t\t__kvm_set_rflags(vcpu, ctxt->eflags);\n\t\t}\n\n\t\t/*\n\t\t * For STI, interrupts are shadowed; so KVM_REQ_EVENT will\n\t\t * do nothing, and it will be requested again as soon as\n\t\t * the shadow expires.  But we still need to check here,\n\t\t * because POPF has no interrupt shadow.\n\t\t */\n\t\tif (unlikely((ctxt->eflags & ~rflags) & X86_EFLAGS_IF))\n\t\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t} else\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = true;\n\n\treturn r;\n}",
                        "code_after_change": "int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,\n\t\t\t    int emulation_type, void *insn, int insn_len)\n{\n\tint r;\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tbool writeback = true;\n\tbool write_fault_to_spt;\n\n\tif (unlikely(!kvm_can_emulate_insn(vcpu, emulation_type, insn, insn_len)))\n\t\treturn 1;\n\n\tvcpu->arch.l1tf_flush_l1d = true;\n\n\t/*\n\t * Clear write_fault_to_shadow_pgtable here to ensure it is\n\t * never reused.\n\t */\n\twrite_fault_to_spt = vcpu->arch.write_fault_to_shadow_pgtable;\n\tvcpu->arch.write_fault_to_shadow_pgtable = false;\n\n\tif (!(emulation_type & EMULTYPE_NO_DECODE)) {\n\t\tkvm_clear_exception_queue(vcpu);\n\n\t\t/*\n\t\t * Return immediately if RIP hits a code breakpoint, such #DBs\n\t\t * are fault-like and are higher priority than any faults on\n\t\t * the code fetch itself.\n\t\t */\n\t\tif (!(emulation_type & EMULTYPE_SKIP) &&\n\t\t    kvm_vcpu_check_code_breakpoint(vcpu, &r))\n\t\t\treturn r;\n\n\t\tr = x86_decode_emulated_instruction(vcpu, emulation_type,\n\t\t\t\t\t\t    insn, insn_len);\n\t\tif (r != EMULATION_OK)  {\n\t\t\tif ((emulation_type & EMULTYPE_TRAP_UD) ||\n\t\t\t    (emulation_type & EMULTYPE_TRAP_UD_FORCED)) {\n\t\t\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\tif (reexecute_instruction(vcpu, cr2_or_gpa,\n\t\t\t\t\t\t  write_fault_to_spt,\n\t\t\t\t\t\t  emulation_type))\n\t\t\t\treturn 1;\n\t\t\tif (ctxt->have_exception) {\n\t\t\t\t/*\n\t\t\t\t * #UD should result in just EMULATION_FAILED, and trap-like\n\t\t\t\t * exception should not be encountered during decode.\n\t\t\t\t */\n\t\t\t\tWARN_ON_ONCE(ctxt->exception.vector == UD_VECTOR ||\n\t\t\t\t\t     exception_type(ctxt->exception.vector) == EXCPT_TRAP);\n\t\t\t\tinject_emulated_exception(vcpu);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t\t}\n\t}\n\n\tif ((emulation_type & EMULTYPE_VMWARE_GP) &&\n\t    !is_vmware_backdoor_opcode(ctxt)) {\n\t\tkvm_queue_exception_e(vcpu, GP_VECTOR, 0);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * EMULTYPE_SKIP without EMULTYPE_COMPLETE_USER_EXIT is intended for\n\t * use *only* by vendor callbacks for kvm_skip_emulated_instruction().\n\t * The caller is responsible for updating interruptibility state and\n\t * injecting single-step #DBs.\n\t */\n\tif (emulation_type & EMULTYPE_SKIP) {\n\t\tif (ctxt->mode != X86EMUL_MODE_PROT64)\n\t\t\tctxt->eip = (u32)ctxt->_eip;\n\t\telse\n\t\t\tctxt->eip = ctxt->_eip;\n\n\t\tif (emulation_type & EMULTYPE_COMPLETE_USER_EXIT) {\n\t\t\tr = 1;\n\t\t\tgoto writeback;\n\t\t}\n\n\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\tif (ctxt->eflags & X86_EFLAGS_RF)\n\t\t\tkvm_set_rflags(vcpu, ctxt->eflags & ~X86_EFLAGS_RF);\n\t\treturn 1;\n\t}\n\n\tif (retry_instruction(ctxt, cr2_or_gpa, emulation_type))\n\t\treturn 1;\n\n\t/* this is needed for vmware backdoor interface to work since it\n\t   changes registers values  during IO operation */\n\tif (vcpu->arch.emulate_regs_need_sync_from_vcpu) {\n\t\tvcpu->arch.emulate_regs_need_sync_from_vcpu = false;\n\t\temulator_invalidate_register_cache(ctxt);\n\t}\n\nrestart:\n\tif (emulation_type & EMULTYPE_PF) {\n\t\t/* Save the faulting GPA (cr2) in the address field */\n\t\tctxt->exception.address = cr2_or_gpa;\n\n\t\t/* With shadow page tables, cr2 contains a GVA or nGPA. */\n\t\tif (vcpu->arch.mmu->root_role.direct) {\n\t\t\tctxt->gpa_available = true;\n\t\t\tctxt->gpa_val = cr2_or_gpa;\n\t\t}\n\t} else {\n\t\t/* Sanitize the address out of an abundance of paranoia. */\n\t\tctxt->exception.address = 0;\n\t}\n\n\tr = x86_emulate_insn(ctxt);\n\n\tif (r == EMULATION_INTERCEPTED)\n\t\treturn 1;\n\n\tif (r == EMULATION_FAILED) {\n\t\tif (reexecute_instruction(vcpu, cr2_or_gpa, write_fault_to_spt,\n\t\t\t\t\temulation_type))\n\t\t\treturn 1;\n\n\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t}\n\n\tif (ctxt->have_exception) {\n\t\tr = 1;\n\t\tif (inject_emulated_exception(vcpu))\n\t\t\treturn r;\n\t} else if (vcpu->arch.pio.count) {\n\t\tif (!vcpu->arch.pio.in) {\n\t\t\t/* FIXME: return into emulator if single-stepping.  */\n\t\t\tvcpu->arch.pio.count = 0;\n\t\t} else {\n\t\t\twriteback = false;\n\t\t\tvcpu->arch.complete_userspace_io = complete_emulated_pio;\n\t\t}\n\t\tr = 0;\n\t} else if (vcpu->mmio_needed) {\n\t\t++vcpu->stat.mmio_exits;\n\n\t\tif (!vcpu->mmio_is_write)\n\t\t\twriteback = false;\n\t\tr = 0;\n\t\tvcpu->arch.complete_userspace_io = complete_emulated_mmio;\n\t} else if (vcpu->arch.complete_userspace_io) {\n\t\twriteback = false;\n\t\tr = 0;\n\t} else if (r == EMULATION_RESTART)\n\t\tgoto restart;\n\telse\n\t\tr = 1;\n\nwriteback:\n\tif (writeback) {\n\t\tunsigned long rflags = static_call(kvm_x86_get_rflags)(vcpu);\n\t\ttoggle_interruptibility(vcpu, ctxt->interruptibility);\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = false;\n\t\tif (!ctxt->have_exception ||\n\t\t    exception_type(ctxt->exception.vector) == EXCPT_TRAP) {\n\t\t\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);\n\t\t\tif (ctxt->is_branch)\n\t\t\t\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);\n\t\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\t\tif (r && (ctxt->tf || (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)))\n\t\t\t\tr = kvm_vcpu_do_singlestep(vcpu);\n\t\t\tstatic_call_cond(kvm_x86_update_emulated_instruction)(vcpu);\n\t\t\t__kvm_set_rflags(vcpu, ctxt->eflags);\n\t\t}\n\n\t\t/*\n\t\t * For STI, interrupts are shadowed; so KVM_REQ_EVENT will\n\t\t * do nothing, and it will be requested again as soon as\n\t\t * the shadow expires.  But we still need to check here,\n\t\t * because POPF has no interrupt shadow.\n\t\t */\n\t\tif (unlikely((ctxt->eflags & ~rflags) & X86_EFLAGS_IF))\n\t\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t} else\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = true;\n\n\treturn r;\n}",
                        "cve_id": "CVE-2022-1852"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "void kvm_mmu_invpcid_gva(struct kvm_vcpu *vcpu, gva_t gva, unsigned long pcid)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.mmu;\n\tbool tlb_flush = false;\n\tuint i;\n\n\tif (pcid == kvm_get_active_pcid(vcpu)) {\n\t\tmmu->invlpg(vcpu, gva, mmu->root.hpa);\n\t\ttlb_flush = true;\n\t}\n\n\tfor (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++) {\n\t\tif (VALID_PAGE(mmu->prev_roots[i].hpa) &&\n\t\t    pcid == kvm_get_pcid(vcpu, mmu->prev_roots[i].pgd)) {\n\t\t\tmmu->invlpg(vcpu, gva, mmu->prev_roots[i].hpa);\n\t\t\ttlb_flush = true;\n\t\t}\n\t}\n\n\tif (tlb_flush)\n\t\tstatic_call(kvm_x86_flush_tlb_gva)(vcpu, gva);\n\n\t++vcpu->stat.invlpg;\n\n\t/*\n\t * Mappings not reachable via the current cr3 or the prev_roots will be\n\t * synced when switching to that cr3, so nothing needs to be done here\n\t * for them.\n\t */\n}",
                        "code_after_change": "void kvm_mmu_invpcid_gva(struct kvm_vcpu *vcpu, gva_t gva, unsigned long pcid)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.mmu;\n\tbool tlb_flush = false;\n\tuint i;\n\n\tif (pcid == kvm_get_active_pcid(vcpu)) {\n\t\tif (mmu->invlpg)\n\t\t\tmmu->invlpg(vcpu, gva, mmu->root.hpa);\n\t\ttlb_flush = true;\n\t}\n\n\tfor (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++) {\n\t\tif (VALID_PAGE(mmu->prev_roots[i].hpa) &&\n\t\t    pcid == kvm_get_pcid(vcpu, mmu->prev_roots[i].pgd)) {\n\t\t\tif (mmu->invlpg)\n\t\t\t\tmmu->invlpg(vcpu, gva, mmu->prev_roots[i].hpa);\n\t\t\ttlb_flush = true;\n\t\t}\n\t}\n\n\tif (tlb_flush)\n\t\tstatic_call(kvm_x86_flush_tlb_gva)(vcpu, gva);\n\n\t++vcpu->stat.invlpg;\n\n\t/*\n\t * Mappings not reachable via the current cr3 or the prev_roots will be\n\t * synced when switching to that cr3, so nothing needs to be done here\n\t * for them.\n\t */\n}",
                        "cve_id": "CVE-2022-1789"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1669,
            "cve_id": "CVE-2018-1000200",
            "code_snippet": "static void oom_reap_task(struct task_struct *tsk)\n{\n\tint attempts = 0;\n\tstruct mm_struct *mm = tsk->signal->oom_mm;\n\n\t/* Retry the down_read_trylock(mmap_sem) a few times */\n\twhile (attempts++ < MAX_OOM_REAP_RETRIES && !oom_reap_task_mm(tsk, mm))\n\t\tschedule_timeout_idle(HZ/10);\n\n\tif (attempts <= MAX_OOM_REAP_RETRIES ||\n\t    test_bit(MMF_OOM_SKIP, &mm->flags))\n\t\tgoto done;\n\n\tpr_info(\"oom_reaper: unable to reap pid:%d (%s)\\n\",\n\t\ttask_pid_nr(tsk), tsk->comm);\n\tdebug_show_all_locks();\n\ndone:\n\ttsk->oom_reaper_list = NULL;\n\n\t/*\n\t * Hide this mm from OOM killer because it has been either reaped or\n\t * somebody can't call up_write(mmap_sem).\n\t */\n\tset_bit(MMF_OOM_SKIP, &mm->flags);\n\n\t/* Drop a reference taken by wake_oom_reaper */\n\tput_task_struct(tsk);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "void exit_mmap(struct mm_struct *mm)\n{\n\tstruct mmu_gather tlb;\n\tstruct vm_area_struct *vma;\n\tunsigned long nr_accounted = 0;\n\n\t/* mm's last user has gone, and its about to be pulled down */\n\tmmu_notifier_release(mm);\n\n\tif (mm->locked_vm) {\n\t\tvma = mm->mmap;\n\t\twhile (vma) {\n\t\t\tif (vma->vm_flags & VM_LOCKED)\n\t\t\t\tmunlock_vma_pages_all(vma);\n\t\t\tvma = vma->vm_next;\n\t\t}\n\t}\n\n\tarch_exit_mmap(mm);\n\n\tvma = mm->mmap;\n\tif (!vma)\t/* Can happen if dup_mmap() received an OOM */\n\t\treturn;\n\n\tlru_add_drain();\n\tflush_cache_mm(mm);\n\ttlb_gather_mmu(&tlb, mm, 0, -1);\n\t/* update_hiwater_rss(mm) here? but nobody should be looking */\n\t/* Use -1 here to ensure all VMAs in the mm are unmapped */\n\tunmap_vmas(&tlb, vma, 0, -1);\n\n\tif (unlikely(mm_is_oom_victim(mm))) {\n\t\t/*\n\t\t * Wait for oom_reap_task() to stop working on this\n\t\t * mm. Because MMF_OOM_SKIP is already set before\n\t\t * calling down_read(), oom_reap_task() will not run\n\t\t * on this \"mm\" post up_write().\n\t\t *\n\t\t * mm_is_oom_victim() cannot be set from under us\n\t\t * either because victim->mm is already set to NULL\n\t\t * under task_lock before calling mmput and oom_mm is\n\t\t * set not NULL by the OOM killer only if victim->mm\n\t\t * is found not NULL while holding the task_lock.\n\t\t */\n\t\tset_bit(MMF_OOM_SKIP, &mm->flags);\n\t\tdown_write(&mm->mmap_sem);\n\t\tup_write(&mm->mmap_sem);\n\t}\n\tfree_pgtables(&tlb, vma, FIRST_USER_ADDRESS, USER_PGTABLES_CEILING);\n\ttlb_finish_mmu(&tlb, 0, -1);\n\n\t/*\n\t * Walk the list again, actually closing and freeing it,\n\t * with preemption enabled, without holding any MM locks.\n\t */\n\twhile (vma) {\n\t\tif (vma->vm_flags & VM_ACCOUNT)\n\t\t\tnr_accounted += vma_pages(vma);\n\t\tvma = remove_vma(vma);\n\t}\n\tvm_unacct_memory(nr_accounted);\n}",
                        "code_after_change": "void exit_mmap(struct mm_struct *mm)\n{\n\tstruct mmu_gather tlb;\n\tstruct vm_area_struct *vma;\n\tunsigned long nr_accounted = 0;\n\n\t/* mm's last user has gone, and its about to be pulled down */\n\tmmu_notifier_release(mm);\n\n\tif (unlikely(mm_is_oom_victim(mm))) {\n\t\t/*\n\t\t * Manually reap the mm to free as much memory as possible.\n\t\t * Then, as the oom reaper does, set MMF_OOM_SKIP to disregard\n\t\t * this mm from further consideration.  Taking mm->mmap_sem for\n\t\t * write after setting MMF_OOM_SKIP will guarantee that the oom\n\t\t * reaper will not run on this mm again after mmap_sem is\n\t\t * dropped.\n\t\t *\n\t\t * Nothing can be holding mm->mmap_sem here and the above call\n\t\t * to mmu_notifier_release(mm) ensures mmu notifier callbacks in\n\t\t * __oom_reap_task_mm() will not block.\n\t\t *\n\t\t * This needs to be done before calling munlock_vma_pages_all(),\n\t\t * which clears VM_LOCKED, otherwise the oom reaper cannot\n\t\t * reliably test it.\n\t\t */\n\t\tmutex_lock(&oom_lock);\n\t\t__oom_reap_task_mm(mm);\n\t\tmutex_unlock(&oom_lock);\n\n\t\tset_bit(MMF_OOM_SKIP, &mm->flags);\n\t\tdown_write(&mm->mmap_sem);\n\t\tup_write(&mm->mmap_sem);\n\t}\n\n\tif (mm->locked_vm) {\n\t\tvma = mm->mmap;\n\t\twhile (vma) {\n\t\t\tif (vma->vm_flags & VM_LOCKED)\n\t\t\t\tmunlock_vma_pages_all(vma);\n\t\t\tvma = vma->vm_next;\n\t\t}\n\t}\n\n\tarch_exit_mmap(mm);\n\n\tvma = mm->mmap;\n\tif (!vma)\t/* Can happen if dup_mmap() received an OOM */\n\t\treturn;\n\n\tlru_add_drain();\n\tflush_cache_mm(mm);\n\ttlb_gather_mmu(&tlb, mm, 0, -1);\n\t/* update_hiwater_rss(mm) here? but nobody should be looking */\n\t/* Use -1 here to ensure all VMAs in the mm are unmapped */\n\tunmap_vmas(&tlb, vma, 0, -1);\n\tfree_pgtables(&tlb, vma, FIRST_USER_ADDRESS, USER_PGTABLES_CEILING);\n\ttlb_finish_mmu(&tlb, 0, -1);\n\n\t/*\n\t * Walk the list again, actually closing and freeing it,\n\t * with preemption enabled, without holding any MM locks.\n\t */\n\twhile (vma) {\n\t\tif (vma->vm_flags & VM_ACCOUNT)\n\t\t\tnr_accounted += vma_pages(vma);\n\t\tvma = remove_vma(vma);\n\t}\n\tvm_unacct_memory(nr_accounted);\n}",
                        "cve_id": "CVE-2018-1000200"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int do_prlimit(struct task_struct *tsk, unsigned int resource,\n\t\t      struct rlimit *new_rlim, struct rlimit *old_rlim)\n{\n\tstruct rlimit *rlim;\n\tint retval = 0;\n\n\tif (resource >= RLIM_NLIMITS)\n\t\treturn -EINVAL;\n\tif (new_rlim) {\n\t\tif (new_rlim->rlim_cur > new_rlim->rlim_max)\n\t\t\treturn -EINVAL;\n\t\tif (resource == RLIMIT_NOFILE &&\n\t\t\t\tnew_rlim->rlim_max > sysctl_nr_open)\n\t\t\treturn -EPERM;\n\t}\n\n\t/* Holding a refcount on tsk protects tsk->signal from disappearing. */\n\trlim = tsk->signal->rlim + resource;\n\ttask_lock(tsk->group_leader);\n\tif (new_rlim) {\n\t\t/*\n\t\t * Keep the capable check against init_user_ns until cgroups can\n\t\t * contain all limits.\n\t\t */\n\t\tif (new_rlim->rlim_max > rlim->rlim_max &&\n\t\t\t\t!capable(CAP_SYS_RESOURCE))\n\t\t\tretval = -EPERM;\n\t\tif (!retval)\n\t\t\tretval = security_task_setrlimit(tsk, resource, new_rlim);\n\t}\n\tif (!retval) {\n\t\tif (old_rlim)\n\t\t\t*old_rlim = *rlim;\n\t\tif (new_rlim)\n\t\t\t*rlim = *new_rlim;\n\t}\n\ttask_unlock(tsk->group_leader);\n\n\t/*\n\t * RLIMIT_CPU handling. Arm the posix CPU timer if the limit is not\n\t * infinite. In case of RLIM_INFINITY the posix CPU timer code\n\t * ignores the rlimit.\n\t */\n\tif (!retval && new_rlim && resource == RLIMIT_CPU &&\n\t    new_rlim->rlim_cur != RLIM_INFINITY &&\n\t    IS_ENABLED(CONFIG_POSIX_TIMERS)) {\n\t\t/*\n\t\t * update_rlimit_cpu can fail if the task is exiting, but there\n\t\t * may be other tasks in the thread group that are not exiting,\n\t\t * and they need their cpu timers adjusted.\n\t\t *\n\t\t * The group_leader is the last task to be released, so if we\n\t\t * cannot update_rlimit_cpu on it, then the entire process is\n\t\t * exiting and we do not need to update at all.\n\t\t */\n\t\tupdate_rlimit_cpu(tsk->group_leader, new_rlim->rlim_cur);\n\t}\n\n\treturn retval;\n}",
                        "code_after_change": "static int do_prlimit(struct task_struct *tsk, unsigned int resource,\n\t\t      struct rlimit *new_rlim, struct rlimit *old_rlim)\n{\n\tstruct rlimit *rlim;\n\tint retval = 0;\n\n\tif (resource >= RLIM_NLIMITS)\n\t\treturn -EINVAL;\n\tresource = array_index_nospec(resource, RLIM_NLIMITS);\n\n\tif (new_rlim) {\n\t\tif (new_rlim->rlim_cur > new_rlim->rlim_max)\n\t\t\treturn -EINVAL;\n\t\tif (resource == RLIMIT_NOFILE &&\n\t\t\t\tnew_rlim->rlim_max > sysctl_nr_open)\n\t\t\treturn -EPERM;\n\t}\n\n\t/* Holding a refcount on tsk protects tsk->signal from disappearing. */\n\trlim = tsk->signal->rlim + resource;\n\ttask_lock(tsk->group_leader);\n\tif (new_rlim) {\n\t\t/*\n\t\t * Keep the capable check against init_user_ns until cgroups can\n\t\t * contain all limits.\n\t\t */\n\t\tif (new_rlim->rlim_max > rlim->rlim_max &&\n\t\t\t\t!capable(CAP_SYS_RESOURCE))\n\t\t\tretval = -EPERM;\n\t\tif (!retval)\n\t\t\tretval = security_task_setrlimit(tsk, resource, new_rlim);\n\t}\n\tif (!retval) {\n\t\tif (old_rlim)\n\t\t\t*old_rlim = *rlim;\n\t\tif (new_rlim)\n\t\t\t*rlim = *new_rlim;\n\t}\n\ttask_unlock(tsk->group_leader);\n\n\t/*\n\t * RLIMIT_CPU handling. Arm the posix CPU timer if the limit is not\n\t * infinite. In case of RLIM_INFINITY the posix CPU timer code\n\t * ignores the rlimit.\n\t */\n\tif (!retval && new_rlim && resource == RLIMIT_CPU &&\n\t    new_rlim->rlim_cur != RLIM_INFINITY &&\n\t    IS_ENABLED(CONFIG_POSIX_TIMERS)) {\n\t\t/*\n\t\t * update_rlimit_cpu can fail if the task is exiting, but there\n\t\t * may be other tasks in the thread group that are not exiting,\n\t\t * and they need their cpu timers adjusted.\n\t\t *\n\t\t * The group_leader is the last task to be released, so if we\n\t\t * cannot update_rlimit_cpu on it, then the entire process is\n\t\t * exiting and we do not need to update at all.\n\t\t */\n\t\tupdate_rlimit_cpu(tsk->group_leader, new_rlim->rlim_cur);\n\t}\n\n\treturn retval;\n}",
                        "cve_id": "CVE-2023-0458"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int expand_downwards(struct vm_area_struct *vma,\n\t\t\t\t   unsigned long address)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct vm_area_struct *prev;\n\tint error;\n\n\taddress &= PAGE_MASK;\n\terror = security_mmap_addr(address);\n\tif (error)\n\t\treturn error;\n\n\t/* Enforce stack_guard_gap */\n\tprev = vma->vm_prev;\n\t/* Check that both stack segments have the same anon_vma? */\n\tif (prev && !(prev->vm_flags & VM_GROWSDOWN) &&\n\t\t\t(prev->vm_flags & (VM_WRITE|VM_READ|VM_EXEC))) {\n\t\tif (address - prev->vm_end < stack_guard_gap)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/* We must make sure the anon_vma is allocated. */\n\tif (unlikely(anon_vma_prepare(vma)))\n\t\treturn -ENOMEM;\n\n\t/*\n\t * vma->vm_start/vm_end cannot change under us because the caller\n\t * is required to hold the mmap_sem in read mode.  We need the\n\t * anon_vma lock to serialize against concurrent expand_stacks.\n\t */\n\tanon_vma_lock_write(vma->anon_vma);\n\n\t/* Somebody else might have raced and expanded it already */\n\tif (address < vma->vm_start) {\n\t\tunsigned long size, grow;\n\n\t\tsize = vma->vm_end - address;\n\t\tgrow = (vma->vm_start - address) >> PAGE_SHIFT;\n\n\t\terror = -ENOMEM;\n\t\tif (grow <= vma->vm_pgoff) {\n\t\t\terror = acct_stack_growth(vma, size, grow);\n\t\t\tif (!error) {\n\t\t\t\t/*\n\t\t\t\t * vma_gap_update() doesn't support concurrent\n\t\t\t\t * updates, but we only hold a shared mmap_sem\n\t\t\t\t * lock here, so we need to protect against\n\t\t\t\t * concurrent vma expansions.\n\t\t\t\t * anon_vma_lock_write() doesn't help here, as\n\t\t\t\t * we don't guarantee that all growable vmas\n\t\t\t\t * in a mm share the same root anon vma.\n\t\t\t\t * So, we reuse mm->page_table_lock to guard\n\t\t\t\t * against concurrent vma expansions.\n\t\t\t\t */\n\t\t\t\tspin_lock(&mm->page_table_lock);\n\t\t\t\tif (vma->vm_flags & VM_LOCKED)\n\t\t\t\t\tmm->locked_vm += grow;\n\t\t\t\tvm_stat_account(mm, vma->vm_flags, grow);\n\t\t\t\tanon_vma_interval_tree_pre_update_vma(vma);\n\t\t\t\tvma->vm_start = address;\n\t\t\t\tvma->vm_pgoff -= grow;\n\t\t\t\tanon_vma_interval_tree_post_update_vma(vma);\n\t\t\t\tvma_gap_update(vma);\n\t\t\t\tspin_unlock(&mm->page_table_lock);\n\n\t\t\t\tperf_event_mmap(vma);\n\t\t\t}\n\t\t}\n\t}\n\tanon_vma_unlock_write(vma->anon_vma);\n\tkhugepaged_enter_vma_merge(vma, vma->vm_flags);\n\tvalidate_mm(mm);\n\treturn error;\n}",
                        "code_after_change": "int expand_downwards(struct vm_area_struct *vma,\n\t\t\t\t   unsigned long address)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct vm_area_struct *prev;\n\tint error = 0;\n\n\taddress &= PAGE_MASK;\n\tif (address < mmap_min_addr)\n\t\treturn -EPERM;\n\n\t/* Enforce stack_guard_gap */\n\tprev = vma->vm_prev;\n\t/* Check that both stack segments have the same anon_vma? */\n\tif (prev && !(prev->vm_flags & VM_GROWSDOWN) &&\n\t\t\t(prev->vm_flags & (VM_WRITE|VM_READ|VM_EXEC))) {\n\t\tif (address - prev->vm_end < stack_guard_gap)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/* We must make sure the anon_vma is allocated. */\n\tif (unlikely(anon_vma_prepare(vma)))\n\t\treturn -ENOMEM;\n\n\t/*\n\t * vma->vm_start/vm_end cannot change under us because the caller\n\t * is required to hold the mmap_sem in read mode.  We need the\n\t * anon_vma lock to serialize against concurrent expand_stacks.\n\t */\n\tanon_vma_lock_write(vma->anon_vma);\n\n\t/* Somebody else might have raced and expanded it already */\n\tif (address < vma->vm_start) {\n\t\tunsigned long size, grow;\n\n\t\tsize = vma->vm_end - address;\n\t\tgrow = (vma->vm_start - address) >> PAGE_SHIFT;\n\n\t\terror = -ENOMEM;\n\t\tif (grow <= vma->vm_pgoff) {\n\t\t\terror = acct_stack_growth(vma, size, grow);\n\t\t\tif (!error) {\n\t\t\t\t/*\n\t\t\t\t * vma_gap_update() doesn't support concurrent\n\t\t\t\t * updates, but we only hold a shared mmap_sem\n\t\t\t\t * lock here, so we need to protect against\n\t\t\t\t * concurrent vma expansions.\n\t\t\t\t * anon_vma_lock_write() doesn't help here, as\n\t\t\t\t * we don't guarantee that all growable vmas\n\t\t\t\t * in a mm share the same root anon vma.\n\t\t\t\t * So, we reuse mm->page_table_lock to guard\n\t\t\t\t * against concurrent vma expansions.\n\t\t\t\t */\n\t\t\t\tspin_lock(&mm->page_table_lock);\n\t\t\t\tif (vma->vm_flags & VM_LOCKED)\n\t\t\t\t\tmm->locked_vm += grow;\n\t\t\t\tvm_stat_account(mm, vma->vm_flags, grow);\n\t\t\t\tanon_vma_interval_tree_pre_update_vma(vma);\n\t\t\t\tvma->vm_start = address;\n\t\t\t\tvma->vm_pgoff -= grow;\n\t\t\t\tanon_vma_interval_tree_post_update_vma(vma);\n\t\t\t\tvma_gap_update(vma);\n\t\t\t\tspin_unlock(&mm->page_table_lock);\n\n\t\t\t\tperf_event_mmap(vma);\n\t\t\t}\n\t\t}\n\t}\n\tanon_vma_unlock_write(vma->anon_vma);\n\tkhugepaged_enter_vma_merge(vma, vma->vm_flags);\n\tvalidate_mm(mm);\n\treturn error;\n}",
                        "cve_id": "CVE-2019-9213"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int\ni915_gem_userptr_ioctl(struct drm_device *dev,\n\t\t       void *data,\n\t\t       struct drm_file *file)\n{\n\tstruct drm_i915_private *dev_priv = to_i915(dev);\n\tstruct drm_i915_gem_userptr *args = data;\n\tstruct drm_i915_gem_object *obj;\n\tint ret;\n\tu32 handle;\n\n\tif (!HAS_LLC(dev_priv) && !HAS_SNOOP(dev_priv)) {\n\t\t/* We cannot support coherent userptr objects on hw without\n\t\t * LLC and broken snooping.\n\t\t */\n\t\treturn -ENODEV;\n\t}\n\n\tif (args->flags & ~(I915_USERPTR_READ_ONLY |\n\t\t\t    I915_USERPTR_UNSYNCHRONIZED))\n\t\treturn -EINVAL;\n\n\tif (offset_in_page(args->user_ptr | args->user_size))\n\t\treturn -EINVAL;\n\n\tif (!access_ok(args->flags & I915_USERPTR_READ_ONLY ? VERIFY_READ : VERIFY_WRITE,\n\t\t       (char __user *)(unsigned long)args->user_ptr, args->user_size))\n\t\treturn -EFAULT;\n\n\tif (args->flags & I915_USERPTR_READ_ONLY) {\n\t\t/* On almost all of the current hw, we cannot tell the GPU that a\n\t\t * page is readonly, so this is just a placeholder in the uAPI.\n\t\t */\n\t\treturn -ENODEV;\n\t}\n\n\tobj = i915_gem_object_alloc(dev_priv);\n\tif (obj == NULL)\n\t\treturn -ENOMEM;\n\n\tdrm_gem_private_object_init(dev, &obj->base, args->user_size);\n\ti915_gem_object_init(obj, &i915_gem_userptr_ops);\n\tobj->read_domains = I915_GEM_DOMAIN_CPU;\n\tobj->write_domain = I915_GEM_DOMAIN_CPU;\n\ti915_gem_object_set_cache_coherency(obj, I915_CACHE_LLC);\n\n\tobj->userptr.ptr = args->user_ptr;\n\tobj->userptr.read_only = !!(args->flags & I915_USERPTR_READ_ONLY);\n\n\t/* And keep a pointer to the current->mm for resolving the user pages\n\t * at binding. This means that we need to hook into the mmu_notifier\n\t * in order to detect if the mmu is destroyed.\n\t */\n\tret = i915_gem_userptr_init__mm_struct(obj);\n\tif (ret == 0)\n\t\tret = i915_gem_userptr_init__mmu_notifier(obj, args->flags);\n\tif (ret == 0)\n\t\tret = drm_gem_handle_create(file, &obj->base, &handle);\n\n\t/* drop reference from allocate - handle holds it now */\n\ti915_gem_object_put(obj);\n\tif (ret)\n\t\treturn ret;\n\n\targs->handle = handle;\n\treturn 0;\n}",
                        "code_after_change": "int\ni915_gem_userptr_ioctl(struct drm_device *dev,\n\t\t       void *data,\n\t\t       struct drm_file *file)\n{\n\tstruct drm_i915_private *dev_priv = to_i915(dev);\n\tstruct drm_i915_gem_userptr *args = data;\n\tstruct drm_i915_gem_object *obj;\n\tint ret;\n\tu32 handle;\n\n\tif (!HAS_LLC(dev_priv) && !HAS_SNOOP(dev_priv)) {\n\t\t/* We cannot support coherent userptr objects on hw without\n\t\t * LLC and broken snooping.\n\t\t */\n\t\treturn -ENODEV;\n\t}\n\n\tif (args->flags & ~(I915_USERPTR_READ_ONLY |\n\t\t\t    I915_USERPTR_UNSYNCHRONIZED))\n\t\treturn -EINVAL;\n\n\tif (!args->user_size)\n\t\treturn -EINVAL;\n\n\tif (offset_in_page(args->user_ptr | args->user_size))\n\t\treturn -EINVAL;\n\n\tif (!access_ok(args->flags & I915_USERPTR_READ_ONLY ? VERIFY_READ : VERIFY_WRITE,\n\t\t       (char __user *)(unsigned long)args->user_ptr, args->user_size))\n\t\treturn -EFAULT;\n\n\tif (args->flags & I915_USERPTR_READ_ONLY) {\n\t\t/* On almost all of the current hw, we cannot tell the GPU that a\n\t\t * page is readonly, so this is just a placeholder in the uAPI.\n\t\t */\n\t\treturn -ENODEV;\n\t}\n\n\tobj = i915_gem_object_alloc(dev_priv);\n\tif (obj == NULL)\n\t\treturn -ENOMEM;\n\n\tdrm_gem_private_object_init(dev, &obj->base, args->user_size);\n\ti915_gem_object_init(obj, &i915_gem_userptr_ops);\n\tobj->read_domains = I915_GEM_DOMAIN_CPU;\n\tobj->write_domain = I915_GEM_DOMAIN_CPU;\n\ti915_gem_object_set_cache_coherency(obj, I915_CACHE_LLC);\n\n\tobj->userptr.ptr = args->user_ptr;\n\tobj->userptr.read_only = !!(args->flags & I915_USERPTR_READ_ONLY);\n\n\t/* And keep a pointer to the current->mm for resolving the user pages\n\t * at binding. This means that we need to hook into the mmu_notifier\n\t * in order to detect if the mmu is destroyed.\n\t */\n\tret = i915_gem_userptr_init__mm_struct(obj);\n\tif (ret == 0)\n\t\tret = i915_gem_userptr_init__mmu_notifier(obj, args->flags);\n\tif (ret == 0)\n\t\tret = drm_gem_handle_create(file, &obj->base, &handle);\n\n\t/* drop reference from allocate - handle holds it now */\n\ti915_gem_object_put(obj);\n\tif (ret)\n\t\treturn ret;\n\n\targs->handle = handle;\n\treturn 0;\n}",
                        "cve_id": "CVE-2019-12881"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static noinline struct btrfs_device *device_list_add(const char *path,\n\t\t\t   struct btrfs_super_block *disk_super,\n\t\t\t   bool *new_device_added)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *fs_devices = NULL;\n\tstruct rcu_string *name;\n\tu64 found_transid = btrfs_super_generation(disk_super);\n\tu64 devid = btrfs_stack_device_id(&disk_super->dev_item);\n\tbool has_metadata_uuid = (btrfs_super_incompat_flags(disk_super) &\n\t\tBTRFS_FEATURE_INCOMPAT_METADATA_UUID);\n\tbool fsid_change_in_progress = (btrfs_super_flags(disk_super) &\n\t\t\t\t\tBTRFS_SUPER_FLAG_CHANGING_FSID_V2);\n\n\tif (fsid_change_in_progress) {\n\t\tif (!has_metadata_uuid) {\n\t\t\t/*\n\t\t\t * When we have an image which has CHANGING_FSID_V2 set\n\t\t\t * it might belong to either a filesystem which has\n\t\t\t * disks with completed fsid change or it might belong\n\t\t\t * to fs with no UUID changes in effect, handle both.\n\t\t\t */\n\t\t\tfs_devices = find_fsid_inprogress(disk_super);\n\t\t\tif (!fs_devices)\n\t\t\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t\t} else {\n\t\t\tfs_devices = find_fsid_changed(disk_super);\n\t\t}\n\t} else if (has_metadata_uuid) {\n\t\tfs_devices = find_fsid(disk_super->fsid,\n\t\t\t\t       disk_super->metadata_uuid);\n\t} else {\n\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t}\n\n\n\tif (!fs_devices) {\n\t\tif (has_metadata_uuid)\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid,\n\t\t\t\t\t\t      disk_super->metadata_uuid);\n\t\telse\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid, NULL);\n\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn ERR_CAST(fs_devices);\n\n\t\tfs_devices->fsid_change = fsid_change_in_progress;\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_add(&fs_devices->fs_list, &fs_uuids);\n\n\t\tdevice = NULL;\n\t} else {\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tdevice = find_device(fs_devices, devid,\n\t\t\t\tdisk_super->dev_item.uuid);\n\n\t\t/*\n\t\t * If this disk has been pulled into an fs devices created by\n\t\t * a device which had the CHANGING_FSID_V2 flag then replace the\n\t\t * metadata_uuid/fsid values of the fs_devices.\n\t\t */\n\t\tif (has_metadata_uuid && fs_devices->fsid_change &&\n\t\t    found_transid > fs_devices->latest_generation) {\n\t\t\tmemcpy(fs_devices->fsid, disk_super->fsid,\n\t\t\t\t\tBTRFS_FSID_SIZE);\n\t\t\tmemcpy(fs_devices->metadata_uuid,\n\t\t\t\t\tdisk_super->metadata_uuid, BTRFS_FSID_SIZE);\n\n\t\t\tfs_devices->fsid_change = false;\n\t\t}\n\t}\n\n\tif (!device) {\n\t\tif (fs_devices->opened) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EBUSY);\n\t\t}\n\n\t\tdevice = btrfs_alloc_device(NULL, &devid,\n\t\t\t\t\t    disk_super->dev_item.uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t/* we can safely leave the fs_devices entry around */\n\t\t\treturn device;\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tbtrfs_free_device(device);\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_assign_pointer(device->name, name);\n\n\t\tlist_add_rcu(&device->dev_list, &fs_devices->devices);\n\t\tfs_devices->num_devices++;\n\n\t\tdevice->fs_devices = fs_devices;\n\t\t*new_device_added = true;\n\n\t\tif (disk_super->label[0])\n\t\t\tpr_info(\"BTRFS: device label %s devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->label, devid, found_transid, path);\n\t\telse\n\t\t\tpr_info(\"BTRFS: device fsid %pU devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->fsid, devid, found_transid, path);\n\n\t} else if (!device->name || strcmp(device->name->str, path)) {\n\t\t/*\n\t\t * When FS is already mounted.\n\t\t * 1. If you are here and if the device->name is NULL that\n\t\t *    means this device was missing at time of FS mount.\n\t\t * 2. If you are here and if the device->name is different\n\t\t *    from 'path' that means either\n\t\t *      a. The same device disappeared and reappeared with\n\t\t *         different name. or\n\t\t *      b. The missing-disk-which-was-replaced, has\n\t\t *         reappeared now.\n\t\t *\n\t\t * We must allow 1 and 2a above. But 2b would be a spurious\n\t\t * and unintentional.\n\t\t *\n\t\t * Further in case of 1 and 2a above, the disk at 'path'\n\t\t * would have missed some transaction when it was away and\n\t\t * in case of 2a the stale bdev has to be updated as well.\n\t\t * 2b must not be allowed at all time.\n\t\t */\n\n\t\t/*\n\t\t * For now, we do allow update to btrfs_fs_device through the\n\t\t * btrfs dev scan cli after FS has been mounted.  We're still\n\t\t * tracking a problem where systems fail mount by subvolume id\n\t\t * when we reject replacement on a mounted FS.\n\t\t */\n\t\tif (!fs_devices->opened && found_transid < device->generation) {\n\t\t\t/*\n\t\t\t * That is if the FS is _not_ mounted and if you\n\t\t\t * are here, that means there is more than one\n\t\t\t * disk with same uuid and devid.We keep the one\n\t\t\t * with larger generation number or the last-in if\n\t\t\t * generation are equal.\n\t\t\t */\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EEXIST);\n\t\t}\n\n\t\t/*\n\t\t * We are going to replace the device path for a given devid,\n\t\t * make sure it's the same device if the device is mounted\n\t\t */\n\t\tif (device->bdev) {\n\t\t\tstruct block_device *path_bdev;\n\n\t\t\tpath_bdev = lookup_bdev(path);\n\t\t\tif (IS_ERR(path_bdev)) {\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\treturn ERR_CAST(path_bdev);\n\t\t\t}\n\n\t\t\tif (device->bdev != path_bdev) {\n\t\t\t\tbdput(path_bdev);\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\tbtrfs_warn_in_rcu(device->fs_info,\n\t\t\t\"duplicate device fsid:devid for %pU:%llu old:%s new:%s\",\n\t\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\t\trcu_str_deref(device->name), path);\n\t\t\t\treturn ERR_PTR(-EEXIST);\n\t\t\t}\n\t\t\tbdput(path_bdev);\n\t\t\tbtrfs_info_in_rcu(device->fs_info,\n\t\t\t\t\"device fsid %pU devid %llu moved old:%s new:%s\",\n\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\trcu_str_deref(device->name), path);\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_string_free(device->name);\n\t\trcu_assign_pointer(device->name, name);\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\tfs_devices->missing_devices--;\n\t\t\tclear_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\t}\n\n\t/*\n\t * Unmount does not free the btrfs_device struct but would zero\n\t * generation along with most of the other members. So just update\n\t * it back. We need it to pick the disk with largest generation\n\t * (as above).\n\t */\n\tif (!fs_devices->opened) {\n\t\tdevice->generation = found_transid;\n\t\tfs_devices->latest_generation = max_t(u64, found_transid,\n\t\t\t\t\t\tfs_devices->latest_generation);\n\t}\n\n\tfs_devices->total_devices = btrfs_super_num_devices(disk_super);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\treturn device;\n}",
                        "code_after_change": "static noinline struct btrfs_device *device_list_add(const char *path,\n\t\t\t   struct btrfs_super_block *disk_super,\n\t\t\t   bool *new_device_added)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *fs_devices = NULL;\n\tstruct rcu_string *name;\n\tu64 found_transid = btrfs_super_generation(disk_super);\n\tu64 devid = btrfs_stack_device_id(&disk_super->dev_item);\n\tbool has_metadata_uuid = (btrfs_super_incompat_flags(disk_super) &\n\t\tBTRFS_FEATURE_INCOMPAT_METADATA_UUID);\n\tbool fsid_change_in_progress = (btrfs_super_flags(disk_super) &\n\t\t\t\t\tBTRFS_SUPER_FLAG_CHANGING_FSID_V2);\n\n\tif (fsid_change_in_progress) {\n\t\tif (!has_metadata_uuid) {\n\t\t\t/*\n\t\t\t * When we have an image which has CHANGING_FSID_V2 set\n\t\t\t * it might belong to either a filesystem which has\n\t\t\t * disks with completed fsid change or it might belong\n\t\t\t * to fs with no UUID changes in effect, handle both.\n\t\t\t */\n\t\t\tfs_devices = find_fsid_inprogress(disk_super);\n\t\t\tif (!fs_devices)\n\t\t\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t\t} else {\n\t\t\tfs_devices = find_fsid_changed(disk_super);\n\t\t}\n\t} else if (has_metadata_uuid) {\n\t\tfs_devices = find_fsid(disk_super->fsid,\n\t\t\t\t       disk_super->metadata_uuid);\n\t} else {\n\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t}\n\n\n\tif (!fs_devices) {\n\t\tif (has_metadata_uuid)\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid,\n\t\t\t\t\t\t      disk_super->metadata_uuid);\n\t\telse\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid, NULL);\n\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn ERR_CAST(fs_devices);\n\n\t\tfs_devices->fsid_change = fsid_change_in_progress;\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_add(&fs_devices->fs_list, &fs_uuids);\n\n\t\tdevice = NULL;\n\t} else {\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tdevice = btrfs_find_device(fs_devices, devid,\n\t\t\t\tdisk_super->dev_item.uuid, NULL, false);\n\n\t\t/*\n\t\t * If this disk has been pulled into an fs devices created by\n\t\t * a device which had the CHANGING_FSID_V2 flag then replace the\n\t\t * metadata_uuid/fsid values of the fs_devices.\n\t\t */\n\t\tif (has_metadata_uuid && fs_devices->fsid_change &&\n\t\t    found_transid > fs_devices->latest_generation) {\n\t\t\tmemcpy(fs_devices->fsid, disk_super->fsid,\n\t\t\t\t\tBTRFS_FSID_SIZE);\n\t\t\tmemcpy(fs_devices->metadata_uuid,\n\t\t\t\t\tdisk_super->metadata_uuid, BTRFS_FSID_SIZE);\n\n\t\t\tfs_devices->fsid_change = false;\n\t\t}\n\t}\n\n\tif (!device) {\n\t\tif (fs_devices->opened) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EBUSY);\n\t\t}\n\n\t\tdevice = btrfs_alloc_device(NULL, &devid,\n\t\t\t\t\t    disk_super->dev_item.uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t/* we can safely leave the fs_devices entry around */\n\t\t\treturn device;\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tbtrfs_free_device(device);\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_assign_pointer(device->name, name);\n\n\t\tlist_add_rcu(&device->dev_list, &fs_devices->devices);\n\t\tfs_devices->num_devices++;\n\n\t\tdevice->fs_devices = fs_devices;\n\t\t*new_device_added = true;\n\n\t\tif (disk_super->label[0])\n\t\t\tpr_info(\"BTRFS: device label %s devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->label, devid, found_transid, path);\n\t\telse\n\t\t\tpr_info(\"BTRFS: device fsid %pU devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->fsid, devid, found_transid, path);\n\n\t} else if (!device->name || strcmp(device->name->str, path)) {\n\t\t/*\n\t\t * When FS is already mounted.\n\t\t * 1. If you are here and if the device->name is NULL that\n\t\t *    means this device was missing at time of FS mount.\n\t\t * 2. If you are here and if the device->name is different\n\t\t *    from 'path' that means either\n\t\t *      a. The same device disappeared and reappeared with\n\t\t *         different name. or\n\t\t *      b. The missing-disk-which-was-replaced, has\n\t\t *         reappeared now.\n\t\t *\n\t\t * We must allow 1 and 2a above. But 2b would be a spurious\n\t\t * and unintentional.\n\t\t *\n\t\t * Further in case of 1 and 2a above, the disk at 'path'\n\t\t * would have missed some transaction when it was away and\n\t\t * in case of 2a the stale bdev has to be updated as well.\n\t\t * 2b must not be allowed at all time.\n\t\t */\n\n\t\t/*\n\t\t * For now, we do allow update to btrfs_fs_device through the\n\t\t * btrfs dev scan cli after FS has been mounted.  We're still\n\t\t * tracking a problem where systems fail mount by subvolume id\n\t\t * when we reject replacement on a mounted FS.\n\t\t */\n\t\tif (!fs_devices->opened && found_transid < device->generation) {\n\t\t\t/*\n\t\t\t * That is if the FS is _not_ mounted and if you\n\t\t\t * are here, that means there is more than one\n\t\t\t * disk with same uuid and devid.We keep the one\n\t\t\t * with larger generation number or the last-in if\n\t\t\t * generation are equal.\n\t\t\t */\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EEXIST);\n\t\t}\n\n\t\t/*\n\t\t * We are going to replace the device path for a given devid,\n\t\t * make sure it's the same device if the device is mounted\n\t\t */\n\t\tif (device->bdev) {\n\t\t\tstruct block_device *path_bdev;\n\n\t\t\tpath_bdev = lookup_bdev(path);\n\t\t\tif (IS_ERR(path_bdev)) {\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\treturn ERR_CAST(path_bdev);\n\t\t\t}\n\n\t\t\tif (device->bdev != path_bdev) {\n\t\t\t\tbdput(path_bdev);\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\tbtrfs_warn_in_rcu(device->fs_info,\n\t\t\t\"duplicate device fsid:devid for %pU:%llu old:%s new:%s\",\n\t\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\t\trcu_str_deref(device->name), path);\n\t\t\t\treturn ERR_PTR(-EEXIST);\n\t\t\t}\n\t\t\tbdput(path_bdev);\n\t\t\tbtrfs_info_in_rcu(device->fs_info,\n\t\t\t\t\"device fsid %pU devid %llu moved old:%s new:%s\",\n\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\trcu_str_deref(device->name), path);\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_string_free(device->name);\n\t\trcu_assign_pointer(device->name, name);\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\tfs_devices->missing_devices--;\n\t\t\tclear_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\t}\n\n\t/*\n\t * Unmount does not free the btrfs_device struct but would zero\n\t * generation along with most of the other members. So just update\n\t * it back. We need it to pick the disk with largest generation\n\t * (as above).\n\t */\n\tif (!fs_devices->opened) {\n\t\tdevice->generation = found_transid;\n\t\tfs_devices->latest_generation = max_t(u64, found_transid,\n\t\t\t\t\t\tfs_devices->latest_generation);\n\t}\n\n\tfs_devices->total_devices = btrfs_super_num_devices(disk_super);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\treturn device;\n}",
                        "cve_id": "CVE-2019-18885"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 1681,
            "cve_id": "CVE-2018-1066",
            "code_snippet": "void build_ntlmssp_negotiate_blob(unsigned char *pbuffer,\n\t\t\t\t\t struct cifs_ses *ses)\n{\n\tNEGOTIATE_MESSAGE *sec_blob = (NEGOTIATE_MESSAGE *)pbuffer;\n\t__u32 flags;\n\n\tmemset(pbuffer, 0, sizeof(NEGOTIATE_MESSAGE));\n\tmemcpy(sec_blob->Signature, NTLMSSP_SIGNATURE, 8);\n\tsec_blob->MessageType = NtLmNegotiate;\n\n\t/* BB is NTLMV2 session security format easier to use here? */\n\tflags = NTLMSSP_NEGOTIATE_56 |\tNTLMSSP_REQUEST_TARGET |\n\t\tNTLMSSP_NEGOTIATE_128 | NTLMSSP_NEGOTIATE_UNICODE |\n\t\tNTLMSSP_NEGOTIATE_NTLM | NTLMSSP_NEGOTIATE_EXTENDED_SEC |\n\t\tNTLMSSP_NEGOTIATE_SEAL;\n\tif (ses->server->sign)\n\t\tflags |= NTLMSSP_NEGOTIATE_SIGN;\n\tif (!ses->server->session_estab || ses->ntlmssp->sesskey_per_smbsess)\n\t\tflags |= NTLMSSP_NEGOTIATE_KEY_XCH;\n\n\tsec_blob->NegotiateFlags = cpu_to_le32(flags);\n\n\tsec_blob->WorkstationName.BufferOffset = 0;\n\tsec_blob->WorkstationName.Length = 0;\n\tsec_blob->WorkstationName.MaximumLength = 0;\n\n\t/* Domain name is sent on the Challenge not Negotiate NTLMSSP request */\n\tsec_blob->DomainName.BufferOffset = 0;\n\tsec_blob->DomainName.Length = 0;\n\tsec_blob->DomainName.MaximumLength = 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int build_ntlmssp_auth_blob(unsigned char **pbuffer,\n\t\t\t\t\tu16 *buflen,\n\t\t\t\t   struct cifs_ses *ses,\n\t\t\t\t   const struct nls_table *nls_cp)\n{\n\tint rc;\n\tAUTHENTICATE_MESSAGE *sec_blob;\n\t__u32 flags;\n\tunsigned char *tmp;\n\n\trc = setup_ntlmv2_rsp(ses, nls_cp);\n\tif (rc) {\n\t\tcifs_dbg(VFS, \"Error %d during NTLMSSP authentication\\n\", rc);\n\t\t*buflen = 0;\n\t\tgoto setup_ntlmv2_ret;\n\t}\n\t*pbuffer = kmalloc(size_of_ntlmssp_blob(ses), GFP_KERNEL);\n\tsec_blob = (AUTHENTICATE_MESSAGE *)*pbuffer;\n\n\tmemcpy(sec_blob->Signature, NTLMSSP_SIGNATURE, 8);\n\tsec_blob->MessageType = NtLmAuthenticate;\n\n\tflags = NTLMSSP_NEGOTIATE_56 |\n\t\tNTLMSSP_REQUEST_TARGET | NTLMSSP_NEGOTIATE_TARGET_INFO |\n\t\tNTLMSSP_NEGOTIATE_128 | NTLMSSP_NEGOTIATE_UNICODE |\n\t\tNTLMSSP_NEGOTIATE_NTLM | NTLMSSP_NEGOTIATE_EXTENDED_SEC;\n\tif (ses->server->sign) {\n\t\tflags |= NTLMSSP_NEGOTIATE_SIGN;\n\t\tif (!ses->server->session_estab ||\n\t\t\t\tses->ntlmssp->sesskey_per_smbsess)\n\t\t\tflags |= NTLMSSP_NEGOTIATE_KEY_XCH;\n\t}\n\n\ttmp = *pbuffer + sizeof(AUTHENTICATE_MESSAGE);\n\tsec_blob->NegotiateFlags = cpu_to_le32(flags);\n\n\tsec_blob->LmChallengeResponse.BufferOffset =\n\t\t\t\tcpu_to_le32(sizeof(AUTHENTICATE_MESSAGE));\n\tsec_blob->LmChallengeResponse.Length = 0;\n\tsec_blob->LmChallengeResponse.MaximumLength = 0;\n\n\tsec_blob->NtChallengeResponse.BufferOffset =\n\t\t\t\tcpu_to_le32(tmp - *pbuffer);\n\tif (ses->user_name != NULL) {\n\t\tmemcpy(tmp, ses->auth_key.response + CIFS_SESS_KEY_SIZE,\n\t\t\t\tses->auth_key.len - CIFS_SESS_KEY_SIZE);\n\t\ttmp += ses->auth_key.len - CIFS_SESS_KEY_SIZE;\n\n\t\tsec_blob->NtChallengeResponse.Length =\n\t\t\t\tcpu_to_le16(ses->auth_key.len - CIFS_SESS_KEY_SIZE);\n\t\tsec_blob->NtChallengeResponse.MaximumLength =\n\t\t\t\tcpu_to_le16(ses->auth_key.len - CIFS_SESS_KEY_SIZE);\n\t} else {\n\t\t/*\n\t\t * don't send an NT Response for anonymous access\n\t\t */\n\t\tsec_blob->NtChallengeResponse.Length = 0;\n\t\tsec_blob->NtChallengeResponse.MaximumLength = 0;\n\t}\n\n\tif (ses->domainName == NULL) {\n\t\tsec_blob->DomainName.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\t\tsec_blob->DomainName.Length = 0;\n\t\tsec_blob->DomainName.MaximumLength = 0;\n\t\ttmp += 2;\n\t} else {\n\t\tint len;\n\t\tlen = cifs_strtoUTF16((__le16 *)tmp, ses->domainName,\n\t\t\t\t      CIFS_MAX_DOMAINNAME_LEN, nls_cp);\n\t\tlen *= 2; /* unicode is 2 bytes each */\n\t\tsec_blob->DomainName.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\t\tsec_blob->DomainName.Length = cpu_to_le16(len);\n\t\tsec_blob->DomainName.MaximumLength = cpu_to_le16(len);\n\t\ttmp += len;\n\t}\n\n\tif (ses->user_name == NULL) {\n\t\tsec_blob->UserName.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\t\tsec_blob->UserName.Length = 0;\n\t\tsec_blob->UserName.MaximumLength = 0;\n\t\ttmp += 2;\n\t} else {\n\t\tint len;\n\t\tlen = cifs_strtoUTF16((__le16 *)tmp, ses->user_name,\n\t\t\t\t      CIFS_MAX_USERNAME_LEN, nls_cp);\n\t\tlen *= 2; /* unicode is 2 bytes each */\n\t\tsec_blob->UserName.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\t\tsec_blob->UserName.Length = cpu_to_le16(len);\n\t\tsec_blob->UserName.MaximumLength = cpu_to_le16(len);\n\t\ttmp += len;\n\t}\n\n\tsec_blob->WorkstationName.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\tsec_blob->WorkstationName.Length = 0;\n\tsec_blob->WorkstationName.MaximumLength = 0;\n\ttmp += 2;\n\n\tif (((ses->ntlmssp->server_flags & NTLMSSP_NEGOTIATE_KEY_XCH) ||\n\t\t(ses->ntlmssp->server_flags & NTLMSSP_NEGOTIATE_EXTENDED_SEC))\n\t\t\t&& !calc_seckey(ses)) {\n\t\tmemcpy(tmp, ses->ntlmssp->ciphertext, CIFS_CPHTXT_SIZE);\n\t\tsec_blob->SessionKey.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\t\tsec_blob->SessionKey.Length = cpu_to_le16(CIFS_CPHTXT_SIZE);\n\t\tsec_blob->SessionKey.MaximumLength =\n\t\t\t\tcpu_to_le16(CIFS_CPHTXT_SIZE);\n\t\ttmp += CIFS_CPHTXT_SIZE;\n\t} else {\n\t\tsec_blob->SessionKey.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\t\tsec_blob->SessionKey.Length = 0;\n\t\tsec_blob->SessionKey.MaximumLength = 0;\n\t}\n\n\t*buflen = tmp - *pbuffer;\nsetup_ntlmv2_ret:\n\treturn rc;\n}",
                        "code_after_change": "int build_ntlmssp_auth_blob(unsigned char **pbuffer,\n\t\t\t\t\tu16 *buflen,\n\t\t\t\t   struct cifs_ses *ses,\n\t\t\t\t   const struct nls_table *nls_cp)\n{\n\tint rc;\n\tAUTHENTICATE_MESSAGE *sec_blob;\n\t__u32 flags;\n\tunsigned char *tmp;\n\n\trc = setup_ntlmv2_rsp(ses, nls_cp);\n\tif (rc) {\n\t\tcifs_dbg(VFS, \"Error %d during NTLMSSP authentication\\n\", rc);\n\t\t*buflen = 0;\n\t\tgoto setup_ntlmv2_ret;\n\t}\n\t*pbuffer = kmalloc(size_of_ntlmssp_blob(ses), GFP_KERNEL);\n\tsec_blob = (AUTHENTICATE_MESSAGE *)*pbuffer;\n\n\tmemcpy(sec_blob->Signature, NTLMSSP_SIGNATURE, 8);\n\tsec_blob->MessageType = NtLmAuthenticate;\n\n\tflags = NTLMSSP_NEGOTIATE_56 |\n\t\tNTLMSSP_REQUEST_TARGET | NTLMSSP_NEGOTIATE_TARGET_INFO |\n\t\tNTLMSSP_NEGOTIATE_128 | NTLMSSP_NEGOTIATE_UNICODE |\n\t\tNTLMSSP_NEGOTIATE_NTLM | NTLMSSP_NEGOTIATE_EXTENDED_SEC |\n\t\tNTLMSSP_NEGOTIATE_SEAL;\n\tif (ses->server->sign)\n\t\tflags |= NTLMSSP_NEGOTIATE_SIGN;\n\tif (!ses->server->session_estab || ses->ntlmssp->sesskey_per_smbsess)\n\t\tflags |= NTLMSSP_NEGOTIATE_KEY_XCH;\n\n\ttmp = *pbuffer + sizeof(AUTHENTICATE_MESSAGE);\n\tsec_blob->NegotiateFlags = cpu_to_le32(flags);\n\n\tsec_blob->LmChallengeResponse.BufferOffset =\n\t\t\t\tcpu_to_le32(sizeof(AUTHENTICATE_MESSAGE));\n\tsec_blob->LmChallengeResponse.Length = 0;\n\tsec_blob->LmChallengeResponse.MaximumLength = 0;\n\n\tsec_blob->NtChallengeResponse.BufferOffset =\n\t\t\t\tcpu_to_le32(tmp - *pbuffer);\n\tif (ses->user_name != NULL) {\n\t\tmemcpy(tmp, ses->auth_key.response + CIFS_SESS_KEY_SIZE,\n\t\t\t\tses->auth_key.len - CIFS_SESS_KEY_SIZE);\n\t\ttmp += ses->auth_key.len - CIFS_SESS_KEY_SIZE;\n\n\t\tsec_blob->NtChallengeResponse.Length =\n\t\t\t\tcpu_to_le16(ses->auth_key.len - CIFS_SESS_KEY_SIZE);\n\t\tsec_blob->NtChallengeResponse.MaximumLength =\n\t\t\t\tcpu_to_le16(ses->auth_key.len - CIFS_SESS_KEY_SIZE);\n\t} else {\n\t\t/*\n\t\t * don't send an NT Response for anonymous access\n\t\t */\n\t\tsec_blob->NtChallengeResponse.Length = 0;\n\t\tsec_blob->NtChallengeResponse.MaximumLength = 0;\n\t}\n\n\tif (ses->domainName == NULL) {\n\t\tsec_blob->DomainName.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\t\tsec_blob->DomainName.Length = 0;\n\t\tsec_blob->DomainName.MaximumLength = 0;\n\t\ttmp += 2;\n\t} else {\n\t\tint len;\n\t\tlen = cifs_strtoUTF16((__le16 *)tmp, ses->domainName,\n\t\t\t\t      CIFS_MAX_DOMAINNAME_LEN, nls_cp);\n\t\tlen *= 2; /* unicode is 2 bytes each */\n\t\tsec_blob->DomainName.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\t\tsec_blob->DomainName.Length = cpu_to_le16(len);\n\t\tsec_blob->DomainName.MaximumLength = cpu_to_le16(len);\n\t\ttmp += len;\n\t}\n\n\tif (ses->user_name == NULL) {\n\t\tsec_blob->UserName.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\t\tsec_blob->UserName.Length = 0;\n\t\tsec_blob->UserName.MaximumLength = 0;\n\t\ttmp += 2;\n\t} else {\n\t\tint len;\n\t\tlen = cifs_strtoUTF16((__le16 *)tmp, ses->user_name,\n\t\t\t\t      CIFS_MAX_USERNAME_LEN, nls_cp);\n\t\tlen *= 2; /* unicode is 2 bytes each */\n\t\tsec_blob->UserName.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\t\tsec_blob->UserName.Length = cpu_to_le16(len);\n\t\tsec_blob->UserName.MaximumLength = cpu_to_le16(len);\n\t\ttmp += len;\n\t}\n\n\tsec_blob->WorkstationName.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\tsec_blob->WorkstationName.Length = 0;\n\tsec_blob->WorkstationName.MaximumLength = 0;\n\ttmp += 2;\n\n\tif (((ses->ntlmssp->server_flags & NTLMSSP_NEGOTIATE_KEY_XCH) ||\n\t\t(ses->ntlmssp->server_flags & NTLMSSP_NEGOTIATE_EXTENDED_SEC))\n\t\t\t&& !calc_seckey(ses)) {\n\t\tmemcpy(tmp, ses->ntlmssp->ciphertext, CIFS_CPHTXT_SIZE);\n\t\tsec_blob->SessionKey.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\t\tsec_blob->SessionKey.Length = cpu_to_le16(CIFS_CPHTXT_SIZE);\n\t\tsec_blob->SessionKey.MaximumLength =\n\t\t\t\tcpu_to_le16(CIFS_CPHTXT_SIZE);\n\t\ttmp += CIFS_CPHTXT_SIZE;\n\t} else {\n\t\tsec_blob->SessionKey.BufferOffset = cpu_to_le32(tmp - *pbuffer);\n\t\tsec_blob->SessionKey.Length = 0;\n\t\tsec_blob->SessionKey.MaximumLength = 0;\n\t}\n\n\t*buflen = tmp - *pbuffer;\nsetup_ntlmv2_ret:\n\treturn rc;\n}",
                        "cve_id": "CVE-2018-1066"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int\nSMB2_sess_establish_session(struct SMB2_sess_data *sess_data)\n{\n\tint rc = 0;\n\tstruct cifs_ses *ses = sess_data->ses;\n\n\tmutex_lock(&ses->server->srv_mutex);\n\tif (ses->server->sign && ses->server->ops->generate_signingkey) {\n\t\trc = ses->server->ops->generate_signingkey(ses);\n\t\tkfree(ses->auth_key.response);\n\t\tses->auth_key.response = NULL;\n\t\tif (rc) {\n\t\t\tcifs_dbg(FYI,\n\t\t\t\t\"SMB3 session key generation failed\\n\");\n\t\t\tmutex_unlock(&ses->server->srv_mutex);\n\t\t\tgoto keygen_exit;\n\t\t}\n\t}\n\tif (!ses->server->session_estab) {\n\t\tses->server->sequence_number = 0x2;\n\t\tses->server->session_estab = true;\n\t}\n\tmutex_unlock(&ses->server->srv_mutex);\n\n\tcifs_dbg(FYI, \"SMB2/3 session established successfully\\n\");\n\tspin_lock(&GlobalMid_Lock);\n\tses->status = CifsGood;\n\tses->need_reconnect = false;\n\tspin_unlock(&GlobalMid_Lock);\n\nkeygen_exit:\n\tif (!ses->server->sign) {\n\t\tkfree(ses->auth_key.response);\n\t\tses->auth_key.response = NULL;\n\t}\n\treturn rc;\n}",
                        "code_after_change": "static int\nSMB2_sess_establish_session(struct SMB2_sess_data *sess_data)\n{\n\tint rc = 0;\n\tstruct cifs_ses *ses = sess_data->ses;\n\n\tmutex_lock(&ses->server->srv_mutex);\n\tif (ses->server->ops->generate_signingkey) {\n\t\trc = ses->server->ops->generate_signingkey(ses);\n\t\tif (rc) {\n\t\t\tcifs_dbg(FYI,\n\t\t\t\t\"SMB3 session key generation failed\\n\");\n\t\t\tmutex_unlock(&ses->server->srv_mutex);\n\t\t\treturn rc;\n\t\t}\n\t}\n\tif (!ses->server->session_estab) {\n\t\tses->server->sequence_number = 0x2;\n\t\tses->server->session_estab = true;\n\t}\n\tmutex_unlock(&ses->server->srv_mutex);\n\n\tcifs_dbg(FYI, \"SMB2/3 session established successfully\\n\");\n\tspin_lock(&GlobalMid_Lock);\n\tses->status = CifsGood;\n\tses->need_reconnect = false;\n\tspin_unlock(&GlobalMid_Lock);\n\treturn rc;\n}",
                        "cve_id": "CVE-2018-1066"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1760,
            "cve_id": "CVE-2018-13093",
            "code_snippet": "static int\nxfs_iget_cache_hit(\n\tstruct xfs_perag\t*pag,\n\tstruct xfs_inode\t*ip,\n\txfs_ino_t\t\tino,\n\tint\t\t\tflags,\n\tint\t\t\tlock_flags) __releases(RCU)\n{\n\tstruct inode\t\t*inode = VFS_I(ip);\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tint\t\t\terror;\n\n\t/*\n\t * check for re-use of an inode within an RCU grace period due to the\n\t * radix tree nodes not being updated yet. We monitor for this by\n\t * setting the inode number to zero before freeing the inode structure.\n\t * If the inode has been reallocated and set up, then the inode number\n\t * will not match, so check for that, too.\n\t */\n\tspin_lock(&ip->i_flags_lock);\n\tif (ip->i_ino != ino) {\n\t\ttrace_xfs_iget_skip(ip);\n\t\tXFS_STATS_INC(mp, xs_ig_frecycle);\n\t\terror = -EAGAIN;\n\t\tgoto out_error;\n\t}\n\n\n\t/*\n\t * If we are racing with another cache hit that is currently\n\t * instantiating this inode or currently recycling it out of\n\t * reclaimabe state, wait for the initialisation to complete\n\t * before continuing.\n\t *\n\t * XXX(hch): eventually we should do something equivalent to\n\t *\t     wait_on_inode to wait for these flags to be cleared\n\t *\t     instead of polling for it.\n\t */\n\tif (ip->i_flags & (XFS_INEW|XFS_IRECLAIM)) {\n\t\ttrace_xfs_iget_skip(ip);\n\t\tXFS_STATS_INC(mp, xs_ig_frecycle);\n\t\terror = -EAGAIN;\n\t\tgoto out_error;\n\t}\n\n\t/*\n\t * Check the inode free state is valid. This also detects lookup\n\t * racing with unlinks.\n\t */\n\terror = xfs_iget_check_free_state(ip, flags);\n\tif (error)\n\t\tgoto out_error;\n\n\t/*\n\t * If IRECLAIMABLE is set, we've torn down the VFS inode already.\n\t * Need to carefully get it back into useable state.\n\t */\n\tif (ip->i_flags & XFS_IRECLAIMABLE) {\n\t\ttrace_xfs_iget_reclaim(ip);\n\n\t\tif (flags & XFS_IGET_INCORE) {\n\t\t\terror = -EAGAIN;\n\t\t\tgoto out_error;\n\t\t}\n\n\t\t/*\n\t\t * We need to set XFS_IRECLAIM to prevent xfs_reclaim_inode\n\t\t * from stomping over us while we recycle the inode.  We can't\n\t\t * clear the radix tree reclaimable tag yet as it requires\n\t\t * pag_ici_lock to be held exclusive.\n\t\t */\n\t\tip->i_flags |= XFS_IRECLAIM;\n\n\t\tspin_unlock(&ip->i_flags_lock);\n\t\trcu_read_unlock();\n\n\t\terror = xfs_reinit_inode(mp, inode);\n\t\tif (error) {\n\t\t\tbool wake;\n\t\t\t/*\n\t\t\t * Re-initializing the inode failed, and we are in deep\n\t\t\t * trouble.  Try to re-add it to the reclaim list.\n\t\t\t */\n\t\t\trcu_read_lock();\n\t\t\tspin_lock(&ip->i_flags_lock);\n\t\t\twake = !!__xfs_iflags_test(ip, XFS_INEW);\n\t\t\tip->i_flags &= ~(XFS_INEW | XFS_IRECLAIM);\n\t\t\tif (wake)\n\t\t\t\twake_up_bit(&ip->i_flags, __XFS_INEW_BIT);\n\t\t\tASSERT(ip->i_flags & XFS_IRECLAIMABLE);\n\t\t\ttrace_xfs_iget_reclaim_fail(ip);\n\t\t\tgoto out_error;\n\t\t}\n\n\t\tspin_lock(&pag->pag_ici_lock);\n\t\tspin_lock(&ip->i_flags_lock);\n\n\t\t/*\n\t\t * Clear the per-lifetime state in the inode as we are now\n\t\t * effectively a new inode and need to return to the initial\n\t\t * state before reuse occurs.\n\t\t */\n\t\tip->i_flags &= ~XFS_IRECLAIM_RESET_FLAGS;\n\t\tip->i_flags |= XFS_INEW;\n\t\txfs_inode_clear_reclaim_tag(pag, ip->i_ino);\n\t\tinode->i_state = I_NEW;\n\n\t\tASSERT(!rwsem_is_locked(&inode->i_rwsem));\n\t\tinit_rwsem(&inode->i_rwsem);\n\n\t\tspin_unlock(&ip->i_flags_lock);\n\t\tspin_unlock(&pag->pag_ici_lock);\n\t} else {\n\t\t/* If the VFS inode is being torn down, pause and try again. */\n\t\tif (!igrab(inode)) {\n\t\t\ttrace_xfs_iget_skip(ip);\n\t\t\terror = -EAGAIN;\n\t\t\tgoto out_error;\n\t\t}\n\n\t\t/* We've got a live one. */\n\t\tspin_unlock(&ip->i_flags_lock);\n\t\trcu_read_unlock();\n\t\ttrace_xfs_iget_hit(ip);\n\t}\n\n\tif (lock_flags != 0)\n\t\txfs_ilock(ip, lock_flags);\n\n\tif (!(flags & XFS_IGET_INCORE))\n\t\txfs_iflags_clear(ip, XFS_ISTALE | XFS_IDONTCACHE);\n\tXFS_STATS_INC(mp, xs_ig_found);\n\n\treturn 0;\n\nout_error:\n\tspin_unlock(&ip->i_flags_lock);\n\trcu_read_unlock();\n\treturn error;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int\nxfs_iget_cache_miss(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_perag\t*pag,\n\txfs_trans_t\t\t*tp,\n\txfs_ino_t\t\tino,\n\tstruct xfs_inode\t**ipp,\n\tint\t\t\tflags,\n\tint\t\t\tlock_flags)\n{\n\tstruct xfs_inode\t*ip;\n\tint\t\t\terror;\n\txfs_agino_t\t\tagino = XFS_INO_TO_AGINO(mp, ino);\n\tint\t\t\tiflags;\n\n\tip = xfs_inode_alloc(mp, ino);\n\tif (!ip)\n\t\treturn -ENOMEM;\n\n\terror = xfs_iread(mp, tp, ip, flags);\n\tif (error)\n\t\tgoto out_destroy;\n\n\tif (!xfs_inode_verify_forks(ip)) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto out_destroy;\n\t}\n\n\ttrace_xfs_iget_miss(ip);\n\n\n\t/*\n\t * If we are allocating a new inode, then check what was returned is\n\t * actually a free, empty inode. If we are not allocating an inode,\n\t * the check we didn't find a free inode.\n\t */\n\tif (flags & XFS_IGET_CREATE) {\n\t\tif (VFS_I(ip)->i_mode != 0) {\n\t\t\txfs_warn(mp,\n\"Corruption detected! Free inode 0x%llx not marked free on disk\",\n\t\t\t\tino);\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto out_destroy;\n\t\t}\n\t\tif (ip->i_d.di_nblocks != 0) {\n\t\t\txfs_warn(mp,\n\"Corruption detected! Free inode 0x%llx has blocks allocated!\",\n\t\t\t\tino);\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto out_destroy;\n\t\t}\n\t} else if (VFS_I(ip)->i_mode == 0) {\n\t\terror = -ENOENT;\n\t\tgoto out_destroy;\n\t}\n\n\t/*\n\t * Preload the radix tree so we can insert safely under the\n\t * write spinlock. Note that we cannot sleep inside the preload\n\t * region. Since we can be called from transaction context, don't\n\t * recurse into the file system.\n\t */\n\tif (radix_tree_preload(GFP_NOFS)) {\n\t\terror = -EAGAIN;\n\t\tgoto out_destroy;\n\t}\n\n\t/*\n\t * Because the inode hasn't been added to the radix-tree yet it can't\n\t * be found by another thread, so we can do the non-sleeping lock here.\n\t */\n\tif (lock_flags) {\n\t\tif (!xfs_ilock_nowait(ip, lock_flags))\n\t\t\tBUG();\n\t}\n\n\t/*\n\t * These values must be set before inserting the inode into the radix\n\t * tree as the moment it is inserted a concurrent lookup (allowed by the\n\t * RCU locking mechanism) can find it and that lookup must see that this\n\t * is an inode currently under construction (i.e. that XFS_INEW is set).\n\t * The ip->i_flags_lock that protects the XFS_INEW flag forms the\n\t * memory barrier that ensures this detection works correctly at lookup\n\t * time.\n\t */\n\tiflags = XFS_INEW;\n\tif (flags & XFS_IGET_DONTCACHE)\n\t\tiflags |= XFS_IDONTCACHE;\n\tip->i_udquot = NULL;\n\tip->i_gdquot = NULL;\n\tip->i_pdquot = NULL;\n\txfs_iflags_set(ip, iflags);\n\n\t/* insert the new inode */\n\tspin_lock(&pag->pag_ici_lock);\n\terror = radix_tree_insert(&pag->pag_ici_root, agino, ip);\n\tif (unlikely(error)) {\n\t\tWARN_ON(error != -EEXIST);\n\t\tXFS_STATS_INC(mp, xs_ig_dup);\n\t\terror = -EAGAIN;\n\t\tgoto out_preload_end;\n\t}\n\tspin_unlock(&pag->pag_ici_lock);\n\tradix_tree_preload_end();\n\n\t*ipp = ip;\n\treturn 0;\n\nout_preload_end:\n\tspin_unlock(&pag->pag_ici_lock);\n\tradix_tree_preload_end();\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\nout_destroy:\n\t__destroy_inode(VFS_I(ip));\n\txfs_inode_free(ip);\n\treturn error;\n}",
                        "code_after_change": "static int\nxfs_iget_cache_miss(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_perag\t*pag,\n\txfs_trans_t\t\t*tp,\n\txfs_ino_t\t\tino,\n\tstruct xfs_inode\t**ipp,\n\tint\t\t\tflags,\n\tint\t\t\tlock_flags)\n{\n\tstruct xfs_inode\t*ip;\n\tint\t\t\terror;\n\txfs_agino_t\t\tagino = XFS_INO_TO_AGINO(mp, ino);\n\tint\t\t\tiflags;\n\n\tip = xfs_inode_alloc(mp, ino);\n\tif (!ip)\n\t\treturn -ENOMEM;\n\n\terror = xfs_iread(mp, tp, ip, flags);\n\tif (error)\n\t\tgoto out_destroy;\n\n\tif (!xfs_inode_verify_forks(ip)) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto out_destroy;\n\t}\n\n\ttrace_xfs_iget_miss(ip);\n\n\n\t/*\n\t * Check the inode free state is valid. This also detects lookup\n\t * racing with unlinks.\n\t */\n\terror = xfs_iget_check_free_state(ip, flags);\n\tif (error)\n\t\tgoto out_destroy;\n\n\t/*\n\t * Preload the radix tree so we can insert safely under the\n\t * write spinlock. Note that we cannot sleep inside the preload\n\t * region. Since we can be called from transaction context, don't\n\t * recurse into the file system.\n\t */\n\tif (radix_tree_preload(GFP_NOFS)) {\n\t\terror = -EAGAIN;\n\t\tgoto out_destroy;\n\t}\n\n\t/*\n\t * Because the inode hasn't been added to the radix-tree yet it can't\n\t * be found by another thread, so we can do the non-sleeping lock here.\n\t */\n\tif (lock_flags) {\n\t\tif (!xfs_ilock_nowait(ip, lock_flags))\n\t\t\tBUG();\n\t}\n\n\t/*\n\t * These values must be set before inserting the inode into the radix\n\t * tree as the moment it is inserted a concurrent lookup (allowed by the\n\t * RCU locking mechanism) can find it and that lookup must see that this\n\t * is an inode currently under construction (i.e. that XFS_INEW is set).\n\t * The ip->i_flags_lock that protects the XFS_INEW flag forms the\n\t * memory barrier that ensures this detection works correctly at lookup\n\t * time.\n\t */\n\tiflags = XFS_INEW;\n\tif (flags & XFS_IGET_DONTCACHE)\n\t\tiflags |= XFS_IDONTCACHE;\n\tip->i_udquot = NULL;\n\tip->i_gdquot = NULL;\n\tip->i_pdquot = NULL;\n\txfs_iflags_set(ip, iflags);\n\n\t/* insert the new inode */\n\tspin_lock(&pag->pag_ici_lock);\n\terror = radix_tree_insert(&pag->pag_ici_root, agino, ip);\n\tif (unlikely(error)) {\n\t\tWARN_ON(error != -EEXIST);\n\t\tXFS_STATS_INC(mp, xs_ig_dup);\n\t\terror = -EAGAIN;\n\t\tgoto out_preload_end;\n\t}\n\tspin_unlock(&pag->pag_ici_lock);\n\tradix_tree_preload_end();\n\n\t*ipp = ip;\n\treturn 0;\n\nout_preload_end:\n\tspin_unlock(&pag->pag_ici_lock);\n\tradix_tree_preload_end();\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\nout_destroy:\n\t__destroy_inode(VFS_I(ip));\n\txfs_inode_free(ip);\n\treturn error;\n}",
                        "cve_id": "CVE-2018-13093"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1775,
            "cve_id": "CVE-2018-14613",
            "code_snippet": "static int __btrfs_alloc_chunk(struct btrfs_trans_handle *trans,\n\t\t\t       u64 start, u64 type)\n{\n\tstruct btrfs_fs_info *info = trans->fs_info;\n\tstruct btrfs_fs_devices *fs_devices = info->fs_devices;\n\tstruct btrfs_device *device;\n\tstruct map_lookup *map = NULL;\n\tstruct extent_map_tree *em_tree;\n\tstruct extent_map *em;\n\tstruct btrfs_device_info *devices_info = NULL;\n\tu64 total_avail;\n\tint num_stripes;\t/* total number of stripes to allocate */\n\tint data_stripes;\t/* number of stripes that count for\n\t\t\t\t   block group size */\n\tint sub_stripes;\t/* sub_stripes info for map */\n\tint dev_stripes;\t/* stripes per dev */\n\tint devs_max;\t\t/* max devs to use */\n\tint devs_min;\t\t/* min devs needed */\n\tint devs_increment;\t/* ndevs has to be a multiple of this */\n\tint ncopies;\t\t/* how many copies to data has */\n\tint ret;\n\tu64 max_stripe_size;\n\tu64 max_chunk_size;\n\tu64 stripe_size;\n\tu64 num_bytes;\n\tint ndevs;\n\tint i;\n\tint j;\n\tint index;\n\n\tBUG_ON(!alloc_profile_is_valid(type, 0));\n\n\tif (list_empty(&fs_devices->alloc_list)) {\n\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG))\n\t\t\tbtrfs_debug(info, \"%s: no writable device\", __func__);\n\t\treturn -ENOSPC;\n\t}\n\n\tindex = btrfs_bg_flags_to_raid_index(type);\n\n\tsub_stripes = btrfs_raid_array[index].sub_stripes;\n\tdev_stripes = btrfs_raid_array[index].dev_stripes;\n\tdevs_max = btrfs_raid_array[index].devs_max;\n\tdevs_min = btrfs_raid_array[index].devs_min;\n\tdevs_increment = btrfs_raid_array[index].devs_increment;\n\tncopies = btrfs_raid_array[index].ncopies;\n\n\tif (type & BTRFS_BLOCK_GROUP_DATA) {\n\t\tmax_stripe_size = SZ_1G;\n\t\tmax_chunk_size = BTRFS_MAX_DATA_CHUNK_SIZE;\n\t\tif (!devs_max)\n\t\t\tdevs_max = BTRFS_MAX_DEVS(info);\n\t} else if (type & BTRFS_BLOCK_GROUP_METADATA) {\n\t\t/* for larger filesystems, use larger metadata chunks */\n\t\tif (fs_devices->total_rw_bytes > 50ULL * SZ_1G)\n\t\t\tmax_stripe_size = SZ_1G;\n\t\telse\n\t\t\tmax_stripe_size = SZ_256M;\n\t\tmax_chunk_size = max_stripe_size;\n\t\tif (!devs_max)\n\t\t\tdevs_max = BTRFS_MAX_DEVS(info);\n\t} else if (type & BTRFS_BLOCK_GROUP_SYSTEM) {\n\t\tmax_stripe_size = SZ_32M;\n\t\tmax_chunk_size = 2 * max_stripe_size;\n\t\tif (!devs_max)\n\t\t\tdevs_max = BTRFS_MAX_DEVS_SYS_CHUNK;\n\t} else {\n\t\tbtrfs_err(info, \"invalid chunk type 0x%llx requested\",\n\t\t       type);\n\t\tBUG_ON(1);\n\t}\n\n\t/* we don't want a chunk larger than 10% of writeable space */\n\tmax_chunk_size = min(div_factor(fs_devices->total_rw_bytes, 1),\n\t\t\t     max_chunk_size);\n\n\tdevices_info = kcalloc(fs_devices->rw_devices, sizeof(*devices_info),\n\t\t\t       GFP_NOFS);\n\tif (!devices_info)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * in the first pass through the devices list, we gather information\n\t * about the available holes on each device.\n\t */\n\tndevs = 0;\n\tlist_for_each_entry(device, &fs_devices->alloc_list, dev_alloc_list) {\n\t\tu64 max_avail;\n\t\tu64 dev_offset;\n\n\t\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\t\tWARN(1, KERN_ERR\n\t\t\t       \"BTRFS: read-only device in alloc_list\\n\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!test_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t\t&device->dev_state) ||\n\t\t    test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state))\n\t\t\tcontinue;\n\n\t\tif (device->total_bytes > device->bytes_used)\n\t\t\ttotal_avail = device->total_bytes - device->bytes_used;\n\t\telse\n\t\t\ttotal_avail = 0;\n\n\t\t/* If there is no space on this device, skip it. */\n\t\tif (total_avail == 0)\n\t\t\tcontinue;\n\n\t\tret = find_free_dev_extent(trans, device,\n\t\t\t\t\t   max_stripe_size * dev_stripes,\n\t\t\t\t\t   &dev_offset, &max_avail);\n\t\tif (ret && ret != -ENOSPC)\n\t\t\tgoto error;\n\n\t\tif (ret == 0)\n\t\t\tmax_avail = max_stripe_size * dev_stripes;\n\n\t\tif (max_avail < BTRFS_STRIPE_LEN * dev_stripes) {\n\t\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG))\n\t\t\t\tbtrfs_debug(info,\n\t\t\t\"%s: devid %llu has no free space, have=%llu want=%u\",\n\t\t\t\t\t    __func__, device->devid, max_avail,\n\t\t\t\t\t    BTRFS_STRIPE_LEN * dev_stripes);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (ndevs == fs_devices->rw_devices) {\n\t\t\tWARN(1, \"%s: found more than %llu devices\\n\",\n\t\t\t     __func__, fs_devices->rw_devices);\n\t\t\tbreak;\n\t\t}\n\t\tdevices_info[ndevs].dev_offset = dev_offset;\n\t\tdevices_info[ndevs].max_avail = max_avail;\n\t\tdevices_info[ndevs].total_avail = total_avail;\n\t\tdevices_info[ndevs].dev = device;\n\t\t++ndevs;\n\t}\n\n\t/*\n\t * now sort the devices by hole size / available space\n\t */\n\tsort(devices_info, ndevs, sizeof(struct btrfs_device_info),\n\t     btrfs_cmp_device_info, NULL);\n\n\t/* round down to number of usable stripes */\n\tndevs = round_down(ndevs, devs_increment);\n\n\tif (ndevs < devs_min) {\n\t\tret = -ENOSPC;\n\t\tif (btrfs_test_opt(info, ENOSPC_DEBUG)) {\n\t\t\tbtrfs_debug(info,\n\t\"%s: not enough devices with free space: have=%d minimum required=%d\",\n\t\t\t\t    __func__, ndevs, devs_min);\n\t\t}\n\t\tgoto error;\n\t}\n\n\tndevs = min(ndevs, devs_max);\n\n\t/*\n\t * The primary goal is to maximize the number of stripes, so use as\n\t * many devices as possible, even if the stripes are not maximum sized.\n\t *\n\t * The DUP profile stores more than one stripe per device, the\n\t * max_avail is the total size so we have to adjust.\n\t */\n\tstripe_size = div_u64(devices_info[ndevs - 1].max_avail, dev_stripes);\n\tnum_stripes = ndevs * dev_stripes;\n\n\t/*\n\t * this will have to be fixed for RAID1 and RAID10 over\n\t * more drives\n\t */\n\tdata_stripes = num_stripes / ncopies;\n\n\tif (type & BTRFS_BLOCK_GROUP_RAID5)\n\t\tdata_stripes = num_stripes - 1;\n\n\tif (type & BTRFS_BLOCK_GROUP_RAID6)\n\t\tdata_stripes = num_stripes - 2;\n\n\t/*\n\t * Use the number of data stripes to figure out how big this chunk\n\t * is really going to be in terms of logical address space,\n\t * and compare that answer with the max chunk size\n\t */\n\tif (stripe_size * data_stripes > max_chunk_size) {\n\t\tstripe_size = div_u64(max_chunk_size, data_stripes);\n\n\t\t/* bump the answer up to a 16MB boundary */\n\t\tstripe_size = round_up(stripe_size, SZ_16M);\n\n\t\t/*\n\t\t * But don't go higher than the limits we found while searching\n\t\t * for free extents\n\t\t */\n\t\tstripe_size = min(devices_info[ndevs - 1].max_avail,\n\t\t\t\t  stripe_size);\n\t}\n\n\t/* align to BTRFS_STRIPE_LEN */\n\tstripe_size = round_down(stripe_size, BTRFS_STRIPE_LEN);\n\n\tmap = kmalloc(map_lookup_size(num_stripes), GFP_NOFS);\n\tif (!map) {\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\tmap->num_stripes = num_stripes;\n\n\tfor (i = 0; i < ndevs; ++i) {\n\t\tfor (j = 0; j < dev_stripes; ++j) {\n\t\t\tint s = i * dev_stripes + j;\n\t\t\tmap->stripes[s].dev = devices_info[i].dev;\n\t\t\tmap->stripes[s].physical = devices_info[i].dev_offset +\n\t\t\t\t\t\t   j * stripe_size;\n\t\t}\n\t}\n\tmap->stripe_len = BTRFS_STRIPE_LEN;\n\tmap->io_align = BTRFS_STRIPE_LEN;\n\tmap->io_width = BTRFS_STRIPE_LEN;\n\tmap->type = type;\n\tmap->sub_stripes = sub_stripes;\n\n\tnum_bytes = stripe_size * data_stripes;\n\n\ttrace_btrfs_chunk_alloc(info, map, start, num_bytes);\n\n\tem = alloc_extent_map();\n\tif (!em) {\n\t\tkfree(map);\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\tset_bit(EXTENT_FLAG_FS_MAPPING, &em->flags);\n\tem->map_lookup = map;\n\tem->start = start;\n\tem->len = num_bytes;\n\tem->block_start = 0;\n\tem->block_len = em->len;\n\tem->orig_block_len = stripe_size;\n\n\tem_tree = &info->mapping_tree.map_tree;\n\twrite_lock(&em_tree->lock);\n\tret = add_extent_mapping(em_tree, em, 0);\n\tif (ret) {\n\t\twrite_unlock(&em_tree->lock);\n\t\tfree_extent_map(em);\n\t\tgoto error;\n\t}\n\n\tlist_add_tail(&em->list, &trans->transaction->pending_chunks);\n\trefcount_inc(&em->refs);\n\twrite_unlock(&em_tree->lock);\n\n\tret = btrfs_make_block_group(trans, 0, type, start, num_bytes);\n\tif (ret)\n\t\tgoto error_del_extent;\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tnum_bytes = map->stripes[i].dev->bytes_used + stripe_size;\n\t\tbtrfs_device_set_bytes_used(map->stripes[i].dev, num_bytes);\n\t}\n\n\tatomic64_sub(stripe_size * map->num_stripes, &info->free_chunk_space);\n\n\tfree_extent_map(em);\n\tcheck_raid56_incompat_flag(info, type);\n\n\tkfree(devices_info);\n\treturn 0;\n\nerror_del_extent:\n\twrite_lock(&em_tree->lock);\n\tremove_extent_mapping(em_tree, em);\n\twrite_unlock(&em_tree->lock);\n\n\t/* One for our allocation */\n\tfree_extent_map(em);\n\t/* One for the tree reference */\n\tfree_extent_map(em);\n\t/* One for the pending_chunks list reference */\n\tfree_extent_map(em);\nerror:\n\tkfree(devices_info);\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int verify_one_dev_extent(struct btrfs_fs_info *fs_info,\n\t\t\t\t u64 chunk_offset, u64 devid,\n\t\t\t\t u64 physical_offset, u64 physical_len)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree.map_tree;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_device *dev;\n\tu64 stripe_len;\n\tbool found = false;\n\tint ret = 0;\n\tint i;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, chunk_offset, 1);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu doesn't have corresponding chunk\",\n\t\t\t  physical_offset, devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tmap = em->map_lookup;\n\tstripe_len = calc_stripe_length(map->type, em->len, map->num_stripes);\n\tif (physical_len != stripe_len) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu length doesn't match chunk %llu, have %llu expect %llu\",\n\t\t\t  physical_offset, devid, em->start, physical_len,\n\t\t\t  stripe_len);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (map->stripes[i].dev->devid == devid &&\n\t\t    map->stripes[i].physical == physical_offset) {\n\t\t\tfound = true;\n\t\t\tif (map->verified_stripes >= map->num_stripes) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"too many dev extents for chunk %llu found\",\n\t\t\t\t\t  em->start);\n\t\t\t\tret = -EUCLEAN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tmap->verified_stripes++;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found) {\n\t\tbtrfs_err(fs_info,\n\t\"dev extent physical offset %llu devid %llu has no corresponding chunk\",\n\t\t\tphysical_offset, devid);\n\t\tret = -EUCLEAN;\n\t}\n\n\t/* Make sure no dev extent is beyond device bondary */\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL);\n\tif (!dev) {\n\t\tbtrfs_err(fs_info, \"failed to find devid %llu\", devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\t/* It's possible this device is a dummy for seed device */\n\tif (dev->disk_total_bytes == 0) {\n\t\tdev = find_device(fs_info->fs_devices->seed, devid, NULL);\n\t\tif (!dev) {\n\t\t\tbtrfs_err(fs_info, \"failed to find seed devid %llu\",\n\t\t\t\t  devid);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (physical_offset + physical_len > dev->disk_total_bytes) {\n\t\tbtrfs_err(fs_info,\n\"dev extent devid %llu physical offset %llu len %llu is beyond device boundary %llu\",\n\t\t\t  devid, physical_offset, physical_len,\n\t\t\t  dev->disk_total_bytes);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\nout:\n\tfree_extent_map(em);\n\treturn ret;\n}",
                        "code_after_change": "static int verify_one_dev_extent(struct btrfs_fs_info *fs_info,\n\t\t\t\t u64 chunk_offset, u64 devid,\n\t\t\t\t u64 physical_offset, u64 physical_len)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree.map_tree;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_device *dev;\n\tu64 stripe_len;\n\tbool found = false;\n\tint ret = 0;\n\tint i;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, chunk_offset, 1);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu doesn't have corresponding chunk\",\n\t\t\t  physical_offset, devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tmap = em->map_lookup;\n\tstripe_len = calc_stripe_length(map->type, em->len, map->num_stripes);\n\tif (physical_len != stripe_len) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu length doesn't match chunk %llu, have %llu expect %llu\",\n\t\t\t  physical_offset, devid, em->start, physical_len,\n\t\t\t  stripe_len);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (map->stripes[i].dev->devid == devid &&\n\t\t    map->stripes[i].physical == physical_offset) {\n\t\t\tfound = true;\n\t\t\tif (map->verified_stripes >= map->num_stripes) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"too many dev extents for chunk %llu found\",\n\t\t\t\t\t  em->start);\n\t\t\t\tret = -EUCLEAN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tmap->verified_stripes++;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found) {\n\t\tbtrfs_err(fs_info,\n\t\"dev extent physical offset %llu devid %llu has no corresponding chunk\",\n\t\t\tphysical_offset, devid);\n\t\tret = -EUCLEAN;\n\t}\n\n\t/* Make sure no dev extent is beyond device bondary */\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL, true);\n\tif (!dev) {\n\t\tbtrfs_err(fs_info, \"failed to find devid %llu\", devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\t/* It's possible this device is a dummy for seed device */\n\tif (dev->disk_total_bytes == 0) {\n\t\tdev = btrfs_find_device(fs_info->fs_devices->seed, devid, NULL,\n\t\t\t\t\tNULL, false);\n\t\tif (!dev) {\n\t\t\tbtrfs_err(fs_info, \"failed to find seed devid %llu\",\n\t\t\t\t  devid);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (physical_offset + physical_len > dev->disk_total_bytes) {\n\t\tbtrfs_err(fs_info,\n\"dev extent devid %llu physical offset %llu len %llu is beyond device boundary %llu\",\n\t\t\t  devid, physical_offset, physical_len,\n\t\t\t  dev->disk_total_bytes);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\nout:\n\tfree_extent_map(em);\n\treturn ret;\n}",
                        "cve_id": "CVE-2019-18885"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1777,
            "cve_id": "CVE-2018-14614",
            "code_snippet": "int f2fs_sanity_check_ckpt(struct f2fs_sb_info *sbi)\n{\n\tunsigned int total, fsmeta;\n\tstruct f2fs_super_block *raw_super = F2FS_RAW_SUPER(sbi);\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tunsigned int ovp_segments, reserved_segments;\n\tunsigned int main_segs, blocks_per_seg;\n\tunsigned int sit_segs, nat_segs;\n\tunsigned int sit_bitmap_size, nat_bitmap_size;\n\tunsigned int log_blocks_per_seg;\n\tunsigned int segment_count_main;\n\tunsigned int cp_pack_start_sum, cp_payload;\n\tblock_t user_block_count;\n\tint i;\n\n\ttotal = le32_to_cpu(raw_super->segment_count);\n\tfsmeta = le32_to_cpu(raw_super->segment_count_ckpt);\n\tsit_segs = le32_to_cpu(raw_super->segment_count_sit);\n\tfsmeta += sit_segs;\n\tnat_segs = le32_to_cpu(raw_super->segment_count_nat);\n\tfsmeta += nat_segs;\n\tfsmeta += le32_to_cpu(ckpt->rsvd_segment_count);\n\tfsmeta += le32_to_cpu(raw_super->segment_count_ssa);\n\n\tif (unlikely(fsmeta >= total))\n\t\treturn 1;\n\n\tovp_segments = le32_to_cpu(ckpt->overprov_segment_count);\n\treserved_segments = le32_to_cpu(ckpt->rsvd_segment_count);\n\n\tif (unlikely(fsmeta < F2FS_MIN_SEGMENTS ||\n\t\t\tovp_segments == 0 || reserved_segments == 0)) {\n\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\"Wrong layout: check mkfs.f2fs version\");\n\t\treturn 1;\n\t}\n\n\tuser_block_count = le64_to_cpu(ckpt->user_block_count);\n\tsegment_count_main = le32_to_cpu(raw_super->segment_count_main);\n\tlog_blocks_per_seg = le32_to_cpu(raw_super->log_blocks_per_seg);\n\tif (!user_block_count || user_block_count >=\n\t\t\tsegment_count_main << log_blocks_per_seg) {\n\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\"Wrong user_block_count: %u\", user_block_count);\n\t\treturn 1;\n\t}\n\n\tmain_segs = le32_to_cpu(raw_super->segment_count_main);\n\tblocks_per_seg = sbi->blocks_per_seg;\n\n\tfor (i = 0; i < NR_CURSEG_NODE_TYPE; i++) {\n\t\tif (le32_to_cpu(ckpt->cur_node_segno[i]) >= main_segs ||\n\t\t\tle16_to_cpu(ckpt->cur_node_blkoff[i]) >= blocks_per_seg)\n\t\t\treturn 1;\n\t}\n\tfor (i = 0; i < NR_CURSEG_DATA_TYPE; i++) {\n\t\tif (le32_to_cpu(ckpt->cur_data_segno[i]) >= main_segs ||\n\t\t\tle16_to_cpu(ckpt->cur_data_blkoff[i]) >= blocks_per_seg)\n\t\t\treturn 1;\n\t}\n\n\tsit_bitmap_size = le32_to_cpu(ckpt->sit_ver_bitmap_bytesize);\n\tnat_bitmap_size = le32_to_cpu(ckpt->nat_ver_bitmap_bytesize);\n\n\tif (sit_bitmap_size != ((sit_segs / 2) << log_blocks_per_seg) / 8 ||\n\t\tnat_bitmap_size != ((nat_segs / 2) << log_blocks_per_seg) / 8) {\n\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\"Wrong bitmap size: sit: %u, nat:%u\",\n\t\t\tsit_bitmap_size, nat_bitmap_size);\n\t\treturn 1;\n\t}\n\n\tcp_pack_start_sum = __start_sum_addr(sbi);\n\tcp_payload = __cp_payload(sbi);\n\tif (cp_pack_start_sum < cp_payload + 1 ||\n\t\tcp_pack_start_sum > blocks_per_seg - 1 -\n\t\t\tNR_CURSEG_TYPE) {\n\t\tf2fs_msg(sbi->sb, KERN_ERR,\n\t\t\t\"Wrong cp_pack_start_sum: %u\",\n\t\t\tcp_pack_start_sum);\n\t\treturn 1;\n\t}\n\n\tif (unlikely(f2fs_cp_error(sbi))) {\n\t\tf2fs_msg(sbi->sb, KERN_ERR, \"A bug case: need to run fsck\");\n\t\treturn 1;\n\t}\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int f2fs_get_valid_checkpoint(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_checkpoint *cp_block;\n\tstruct f2fs_super_block *fsb = sbi->raw_super;\n\tstruct page *cp1, *cp2, *cur_page;\n\tunsigned long blk_size = sbi->blocksize;\n\tunsigned long long cp1_version = 0, cp2_version = 0;\n\tunsigned long long cp_start_blk_no;\n\tunsigned int cp_blks = 1 + __cp_payload(sbi);\n\tblock_t cp_blk_no;\n\tint i;\n\n\tsbi->ckpt = f2fs_kzalloc(sbi, array_size(blk_size, cp_blks),\n\t\t\t\t GFP_KERNEL);\n\tif (!sbi->ckpt)\n\t\treturn -ENOMEM;\n\t/*\n\t * Finding out valid cp block involves read both\n\t * sets( cp pack1 and cp pack 2)\n\t */\n\tcp_start_blk_no = le32_to_cpu(fsb->cp_blkaddr);\n\tcp1 = validate_checkpoint(sbi, cp_start_blk_no, &cp1_version);\n\n\t/* The second checkpoint pack should start at the next segment */\n\tcp_start_blk_no += ((unsigned long long)1) <<\n\t\t\t\tle32_to_cpu(fsb->log_blocks_per_seg);\n\tcp2 = validate_checkpoint(sbi, cp_start_blk_no, &cp2_version);\n\n\tif (cp1 && cp2) {\n\t\tif (ver_after(cp2_version, cp1_version))\n\t\t\tcur_page = cp2;\n\t\telse\n\t\t\tcur_page = cp1;\n\t} else if (cp1) {\n\t\tcur_page = cp1;\n\t} else if (cp2) {\n\t\tcur_page = cp2;\n\t} else {\n\t\tgoto fail_no_cp;\n\t}\n\n\tcp_block = (struct f2fs_checkpoint *)page_address(cur_page);\n\tmemcpy(sbi->ckpt, cp_block, blk_size);\n\n\t/* Sanity checking of checkpoint */\n\tif (f2fs_sanity_check_ckpt(sbi))\n\t\tgoto free_fail_no_cp;\n\n\tif (cur_page == cp1)\n\t\tsbi->cur_cp_pack = 1;\n\telse\n\t\tsbi->cur_cp_pack = 2;\n\n\tif (cp_blks <= 1)\n\t\tgoto done;\n\n\tcp_blk_no = le32_to_cpu(fsb->cp_blkaddr);\n\tif (cur_page == cp2)\n\t\tcp_blk_no += 1 << le32_to_cpu(fsb->log_blocks_per_seg);\n\n\tfor (i = 1; i < cp_blks; i++) {\n\t\tvoid *sit_bitmap_ptr;\n\t\tunsigned char *ckpt = (unsigned char *)sbi->ckpt;\n\n\t\tcur_page = f2fs_get_meta_page(sbi, cp_blk_no + i);\n\t\tif (IS_ERR(cur_page))\n\t\t\tgoto free_fail_no_cp;\n\t\tsit_bitmap_ptr = page_address(cur_page);\n\t\tmemcpy(ckpt + i * blk_size, sit_bitmap_ptr, blk_size);\n\t\tf2fs_put_page(cur_page, 1);\n\t}\ndone:\n\tf2fs_put_page(cp1, 1);\n\tf2fs_put_page(cp2, 1);\n\treturn 0;\n\nfree_fail_no_cp:\n\tf2fs_put_page(cp1, 1);\n\tf2fs_put_page(cp2, 1);\nfail_no_cp:\n\tkfree(sbi->ckpt);\n\treturn -EINVAL;\n}",
                        "code_after_change": "int f2fs_get_valid_checkpoint(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_checkpoint *cp_block;\n\tstruct f2fs_super_block *fsb = sbi->raw_super;\n\tstruct page *cp1, *cp2, *cur_page;\n\tunsigned long blk_size = sbi->blocksize;\n\tunsigned long long cp1_version = 0, cp2_version = 0;\n\tunsigned long long cp_start_blk_no;\n\tunsigned int cp_blks = 1 + __cp_payload(sbi);\n\tblock_t cp_blk_no;\n\tint i;\n\n\tsbi->ckpt = f2fs_kzalloc(sbi, array_size(blk_size, cp_blks),\n\t\t\t\t GFP_KERNEL);\n\tif (!sbi->ckpt)\n\t\treturn -ENOMEM;\n\t/*\n\t * Finding out valid cp block involves read both\n\t * sets( cp pack1 and cp pack 2)\n\t */\n\tcp_start_blk_no = le32_to_cpu(fsb->cp_blkaddr);\n\tcp1 = validate_checkpoint(sbi, cp_start_blk_no, &cp1_version);\n\n\t/* The second checkpoint pack should start at the next segment */\n\tcp_start_blk_no += ((unsigned long long)1) <<\n\t\t\t\tle32_to_cpu(fsb->log_blocks_per_seg);\n\tcp2 = validate_checkpoint(sbi, cp_start_blk_no, &cp2_version);\n\n\tif (cp1 && cp2) {\n\t\tif (ver_after(cp2_version, cp1_version))\n\t\t\tcur_page = cp2;\n\t\telse\n\t\t\tcur_page = cp1;\n\t} else if (cp1) {\n\t\tcur_page = cp1;\n\t} else if (cp2) {\n\t\tcur_page = cp2;\n\t} else {\n\t\tgoto fail_no_cp;\n\t}\n\n\tcp_block = (struct f2fs_checkpoint *)page_address(cur_page);\n\tmemcpy(sbi->ckpt, cp_block, blk_size);\n\n\tif (cur_page == cp1)\n\t\tsbi->cur_cp_pack = 1;\n\telse\n\t\tsbi->cur_cp_pack = 2;\n\n\t/* Sanity checking of checkpoint */\n\tif (f2fs_sanity_check_ckpt(sbi))\n\t\tgoto free_fail_no_cp;\n\n\tif (cp_blks <= 1)\n\t\tgoto done;\n\n\tcp_blk_no = le32_to_cpu(fsb->cp_blkaddr);\n\tif (cur_page == cp2)\n\t\tcp_blk_no += 1 << le32_to_cpu(fsb->log_blocks_per_seg);\n\n\tfor (i = 1; i < cp_blks; i++) {\n\t\tvoid *sit_bitmap_ptr;\n\t\tunsigned char *ckpt = (unsigned char *)sbi->ckpt;\n\n\t\tcur_page = f2fs_get_meta_page(sbi, cp_blk_no + i);\n\t\tif (IS_ERR(cur_page))\n\t\t\tgoto free_fail_no_cp;\n\t\tsit_bitmap_ptr = page_address(cur_page);\n\t\tmemcpy(ckpt + i * blk_size, sit_bitmap_ptr, blk_size);\n\t\tf2fs_put_page(cur_page, 1);\n\t}\ndone:\n\tf2fs_put_page(cp1, 1);\n\tf2fs_put_page(cp2, 1);\n\treturn 0;\n\nfree_fail_no_cp:\n\tf2fs_put_page(cp1, 1);\n\tf2fs_put_page(cp2, 1);\nfail_no_cp:\n\tkfree(sbi->ckpt);\n\treturn -EINVAL;\n}",
                        "cve_id": "CVE-2018-14614"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1780,
            "cve_id": "CVE-2018-14616",
            "code_snippet": "static int do_read_inode(struct inode *inode)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\tstruct page *node_page;\n\tstruct f2fs_inode *ri;\n\tprojid_t i_projid;\n\tint err;\n\n\t/* Check if ino is within scope */\n\tif (f2fs_check_nid_range(sbi, inode->i_ino))\n\t\treturn -EINVAL;\n\n\tnode_page = f2fs_get_node_page(sbi, inode->i_ino);\n\tif (IS_ERR(node_page))\n\t\treturn PTR_ERR(node_page);\n\n\tri = F2FS_INODE(node_page);\n\n\tinode->i_mode = le16_to_cpu(ri->i_mode);\n\ti_uid_write(inode, le32_to_cpu(ri->i_uid));\n\ti_gid_write(inode, le32_to_cpu(ri->i_gid));\n\tset_nlink(inode, le32_to_cpu(ri->i_links));\n\tinode->i_size = le64_to_cpu(ri->i_size);\n\tinode->i_blocks = SECTOR_FROM_BLOCK(le64_to_cpu(ri->i_blocks) - 1);\n\n\tinode->i_atime.tv_sec = le64_to_cpu(ri->i_atime);\n\tinode->i_ctime.tv_sec = le64_to_cpu(ri->i_ctime);\n\tinode->i_mtime.tv_sec = le64_to_cpu(ri->i_mtime);\n\tinode->i_atime.tv_nsec = le32_to_cpu(ri->i_atime_nsec);\n\tinode->i_ctime.tv_nsec = le32_to_cpu(ri->i_ctime_nsec);\n\tinode->i_mtime.tv_nsec = le32_to_cpu(ri->i_mtime_nsec);\n\tinode->i_generation = le32_to_cpu(ri->i_generation);\n\tif (S_ISDIR(inode->i_mode))\n\t\tfi->i_current_depth = le32_to_cpu(ri->i_current_depth);\n\telse if (S_ISREG(inode->i_mode))\n\t\tfi->i_gc_failures[GC_FAILURE_PIN] =\n\t\t\t\t\tle16_to_cpu(ri->i_gc_failures);\n\tfi->i_xattr_nid = le32_to_cpu(ri->i_xattr_nid);\n\tfi->i_flags = le32_to_cpu(ri->i_flags);\n\tfi->flags = 0;\n\tfi->i_advise = ri->i_advise;\n\tfi->i_pino = le32_to_cpu(ri->i_pino);\n\tfi->i_dir_level = ri->i_dir_level;\n\n\tif (f2fs_init_extent_tree(inode, &ri->i_ext))\n\t\tset_page_dirty(node_page);\n\n\tget_inline_info(inode, ri);\n\n\tfi->i_extra_isize = f2fs_has_extra_attr(inode) ?\n\t\t\t\t\tle16_to_cpu(ri->i_extra_isize) : 0;\n\n\tif (f2fs_sb_has_flexible_inline_xattr(sbi->sb)) {\n\t\tfi->i_inline_xattr_size = le16_to_cpu(ri->i_inline_xattr_size);\n\t} else if (f2fs_has_inline_xattr(inode) ||\n\t\t\t\tf2fs_has_inline_dentry(inode)) {\n\t\tfi->i_inline_xattr_size = DEFAULT_INLINE_XATTR_ADDRS;\n\t} else {\n\n\t\t/*\n\t\t * Previous inline data or directory always reserved 200 bytes\n\t\t * in inode layout, even if inline_xattr is disabled. In order\n\t\t * to keep inline_dentry's structure for backward compatibility,\n\t\t * we get the space back only from inline_data.\n\t\t */\n\t\tfi->i_inline_xattr_size = 0;\n\t}\n\n\tif (!sanity_check_inode(inode, node_page)) {\n\t\tf2fs_put_page(node_page, 1);\n\t\treturn -EINVAL;\n\t}\n\n\t/* check data exist */\n\tif (f2fs_has_inline_data(inode) && !f2fs_exist_data(inode))\n\t\t__recover_inline_status(inode, node_page);\n\n\t/* get rdev by using inline_info */\n\t__get_inode_rdev(inode, ri);\n\n\terr = __written_first_block(sbi, ri);\n\tif (err < 0) {\n\t\tf2fs_put_page(node_page, 1);\n\t\treturn err;\n\t}\n\tif (!err)\n\t\tset_inode_flag(inode, FI_FIRST_BLOCK_WRITTEN);\n\n\tif (!f2fs_need_inode_block_update(sbi, inode->i_ino))\n\t\tfi->last_disk_size = inode->i_size;\n\n\tif (fi->i_flags & F2FS_PROJINHERIT_FL)\n\t\tset_inode_flag(inode, FI_PROJ_INHERIT);\n\n\tif (f2fs_has_extra_attr(inode) && f2fs_sb_has_project_quota(sbi->sb) &&\n\t\t\tF2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_projid))\n\t\ti_projid = (projid_t)le32_to_cpu(ri->i_projid);\n\telse\n\t\ti_projid = F2FS_DEF_PROJID;\n\tfi->i_projid = make_kprojid(&init_user_ns, i_projid);\n\n\tif (f2fs_has_extra_attr(inode) && f2fs_sb_has_inode_crtime(sbi->sb) &&\n\t\t\tF2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_crtime)) {\n\t\tfi->i_crtime.tv_sec = le64_to_cpu(ri->i_crtime);\n\t\tfi->i_crtime.tv_nsec = le32_to_cpu(ri->i_crtime_nsec);\n\t}\n\n\tF2FS_I(inode)->i_disk_time[0] = inode->i_atime;\n\tF2FS_I(inode)->i_disk_time[1] = inode->i_ctime;\n\tF2FS_I(inode)->i_disk_time[2] = inode->i_mtime;\n\tF2FS_I(inode)->i_disk_time[3] = F2FS_I(inode)->i_crtime;\n\tf2fs_put_page(node_page, 1);\n\n\tstat_inc_inline_xattr(inode);\n\tstat_inc_inline_inode(inode);\n\tstat_inc_inline_dir(inode);\n\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,\n\t\tstruct gc_inode_list *gc_list, unsigned int segno, int gc_type,\n\t\tbool force_migrate)\n{\n\tstruct super_block *sb = sbi->sb;\n\tstruct f2fs_summary *entry;\n\tblock_t start_addr;\n\tint off;\n\tint phase = 0;\n\tint submitted = 0;\n\tunsigned int usable_blks_in_seg = f2fs_usable_blks_in_seg(sbi, segno);\n\n\tstart_addr = START_BLOCK(sbi, segno);\n\nnext_step:\n\tentry = sum;\n\n\tfor (off = 0; off < usable_blks_in_seg; off++, entry++) {\n\t\tstruct page *data_page;\n\t\tstruct inode *inode;\n\t\tstruct node_info dni; /* dnode info for the data */\n\t\tunsigned int ofs_in_node, nofs;\n\t\tblock_t start_bidx;\n\t\tnid_t nid = le32_to_cpu(entry->nid);\n\n\t\t/*\n\t\t * stop BG_GC if there is not enough free sections.\n\t\t * Or, stop GC if the segment becomes fully valid caused by\n\t\t * race condition along with SSR block allocation.\n\t\t */\n\t\tif ((gc_type == BG_GC && has_not_enough_free_secs(sbi, 0, 0)) ||\n\t\t\t(!force_migrate && get_valid_blocks(sbi, segno, true) ==\n\t\t\t\t\t\t\tBLKS_PER_SEC(sbi)))\n\t\t\treturn submitted;\n\n\t\tif (check_valid_map(sbi, segno, off) == 0)\n\t\t\tcontinue;\n\n\t\tif (phase == 0) {\n\t\t\tf2fs_ra_meta_pages(sbi, NAT_BLOCK_OFFSET(nid), 1,\n\t\t\t\t\t\t\tMETA_NAT, true);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (phase == 1) {\n\t\t\tf2fs_ra_node_page(sbi, nid);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Get an inode by ino with checking validity */\n\t\tif (!is_alive(sbi, entry, &dni, start_addr + off, &nofs))\n\t\t\tcontinue;\n\n\t\tif (phase == 2) {\n\t\t\tf2fs_ra_node_page(sbi, dni.ino);\n\t\t\tcontinue;\n\t\t}\n\n\t\tofs_in_node = le16_to_cpu(entry->ofs_in_node);\n\n\t\tif (phase == 3) {\n\t\t\tinode = f2fs_iget(sb, dni.ino);\n\t\t\tif (IS_ERR(inode) || is_bad_inode(inode))\n\t\t\t\tcontinue;\n\n\t\t\tif (!down_write_trylock(\n\t\t\t\t&F2FS_I(inode)->i_gc_rwsem[WRITE])) {\n\t\t\t\tiput(inode);\n\t\t\t\tsbi->skipped_gc_rwsem++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tstart_bidx = f2fs_start_bidx_of_node(nofs, inode) +\n\t\t\t\t\t\t\t\tofs_in_node;\n\n\t\t\tif (f2fs_post_read_required(inode)) {\n\t\t\t\tint err = ra_data_block(inode, start_bidx);\n\n\t\t\t\tup_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);\n\t\t\t\tif (err) {\n\t\t\t\t\tiput(inode);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tadd_gc_inode(gc_list, inode);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tdata_page = f2fs_get_read_data_page(inode,\n\t\t\t\t\t\tstart_bidx, REQ_RAHEAD, true);\n\t\t\tup_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);\n\t\t\tif (IS_ERR(data_page)) {\n\t\t\t\tiput(inode);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tf2fs_put_page(data_page, 0);\n\t\t\tadd_gc_inode(gc_list, inode);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* phase 4 */\n\t\tinode = find_gc_inode(gc_list, dni.ino);\n\t\tif (inode) {\n\t\t\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\t\t\tbool locked = false;\n\t\t\tint err;\n\n\t\t\tif (S_ISREG(inode->i_mode)) {\n\t\t\t\tif (!down_write_trylock(&fi->i_gc_rwsem[READ])) {\n\t\t\t\t\tsbi->skipped_gc_rwsem++;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tif (!down_write_trylock(\n\t\t\t\t\t\t&fi->i_gc_rwsem[WRITE])) {\n\t\t\t\t\tsbi->skipped_gc_rwsem++;\n\t\t\t\t\tup_write(&fi->i_gc_rwsem[READ]);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tlocked = true;\n\n\t\t\t\t/* wait for all inflight aio data */\n\t\t\t\tinode_dio_wait(inode);\n\t\t\t}\n\n\t\t\tstart_bidx = f2fs_start_bidx_of_node(nofs, inode)\n\t\t\t\t\t\t\t\t+ ofs_in_node;\n\t\t\tif (f2fs_post_read_required(inode))\n\t\t\t\terr = move_data_block(inode, start_bidx,\n\t\t\t\t\t\t\tgc_type, segno, off);\n\t\t\telse\n\t\t\t\terr = move_data_page(inode, start_bidx, gc_type,\n\t\t\t\t\t\t\t\tsegno, off);\n\n\t\t\tif (!err && (gc_type == FG_GC ||\n\t\t\t\t\tf2fs_post_read_required(inode)))\n\t\t\t\tsubmitted++;\n\n\t\t\tif (locked) {\n\t\t\t\tup_write(&fi->i_gc_rwsem[WRITE]);\n\t\t\t\tup_write(&fi->i_gc_rwsem[READ]);\n\t\t\t}\n\n\t\t\tstat_inc_data_blk_count(sbi, 1, gc_type);\n\t\t}\n\t}\n\n\tif (++phase < 5)\n\t\tgoto next_step;\n\n\treturn submitted;\n}",
                        "code_after_change": "static int gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,\n\t\tstruct gc_inode_list *gc_list, unsigned int segno, int gc_type,\n\t\tbool force_migrate)\n{\n\tstruct super_block *sb = sbi->sb;\n\tstruct f2fs_summary *entry;\n\tblock_t start_addr;\n\tint off;\n\tint phase = 0;\n\tint submitted = 0;\n\tunsigned int usable_blks_in_seg = f2fs_usable_blks_in_seg(sbi, segno);\n\n\tstart_addr = START_BLOCK(sbi, segno);\n\nnext_step:\n\tentry = sum;\n\n\tfor (off = 0; off < usable_blks_in_seg; off++, entry++) {\n\t\tstruct page *data_page;\n\t\tstruct inode *inode;\n\t\tstruct node_info dni; /* dnode info for the data */\n\t\tunsigned int ofs_in_node, nofs;\n\t\tblock_t start_bidx;\n\t\tnid_t nid = le32_to_cpu(entry->nid);\n\n\t\t/*\n\t\t * stop BG_GC if there is not enough free sections.\n\t\t * Or, stop GC if the segment becomes fully valid caused by\n\t\t * race condition along with SSR block allocation.\n\t\t */\n\t\tif ((gc_type == BG_GC && has_not_enough_free_secs(sbi, 0, 0)) ||\n\t\t\t(!force_migrate && get_valid_blocks(sbi, segno, true) ==\n\t\t\t\t\t\t\tBLKS_PER_SEC(sbi)))\n\t\t\treturn submitted;\n\n\t\tif (check_valid_map(sbi, segno, off) == 0)\n\t\t\tcontinue;\n\n\t\tif (phase == 0) {\n\t\t\tf2fs_ra_meta_pages(sbi, NAT_BLOCK_OFFSET(nid), 1,\n\t\t\t\t\t\t\tMETA_NAT, true);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (phase == 1) {\n\t\t\tf2fs_ra_node_page(sbi, nid);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Get an inode by ino with checking validity */\n\t\tif (!is_alive(sbi, entry, &dni, start_addr + off, &nofs))\n\t\t\tcontinue;\n\n\t\tif (phase == 2) {\n\t\t\tf2fs_ra_node_page(sbi, dni.ino);\n\t\t\tcontinue;\n\t\t}\n\n\t\tofs_in_node = le16_to_cpu(entry->ofs_in_node);\n\n\t\tif (phase == 3) {\n\t\t\tinode = f2fs_iget(sb, dni.ino);\n\t\t\tif (IS_ERR(inode) || is_bad_inode(inode) ||\n\t\t\t\t\tspecial_file(inode->i_mode))\n\t\t\t\tcontinue;\n\n\t\t\tif (!down_write_trylock(\n\t\t\t\t&F2FS_I(inode)->i_gc_rwsem[WRITE])) {\n\t\t\t\tiput(inode);\n\t\t\t\tsbi->skipped_gc_rwsem++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tstart_bidx = f2fs_start_bidx_of_node(nofs, inode) +\n\t\t\t\t\t\t\t\tofs_in_node;\n\n\t\t\tif (f2fs_post_read_required(inode)) {\n\t\t\t\tint err = ra_data_block(inode, start_bidx);\n\n\t\t\t\tup_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);\n\t\t\t\tif (err) {\n\t\t\t\t\tiput(inode);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tadd_gc_inode(gc_list, inode);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tdata_page = f2fs_get_read_data_page(inode,\n\t\t\t\t\t\tstart_bidx, REQ_RAHEAD, true);\n\t\t\tup_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);\n\t\t\tif (IS_ERR(data_page)) {\n\t\t\t\tiput(inode);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tf2fs_put_page(data_page, 0);\n\t\t\tadd_gc_inode(gc_list, inode);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* phase 4 */\n\t\tinode = find_gc_inode(gc_list, dni.ino);\n\t\tif (inode) {\n\t\t\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\t\t\tbool locked = false;\n\t\t\tint err;\n\n\t\t\tif (S_ISREG(inode->i_mode)) {\n\t\t\t\tif (!down_write_trylock(&fi->i_gc_rwsem[READ])) {\n\t\t\t\t\tsbi->skipped_gc_rwsem++;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tif (!down_write_trylock(\n\t\t\t\t\t\t&fi->i_gc_rwsem[WRITE])) {\n\t\t\t\t\tsbi->skipped_gc_rwsem++;\n\t\t\t\t\tup_write(&fi->i_gc_rwsem[READ]);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tlocked = true;\n\n\t\t\t\t/* wait for all inflight aio data */\n\t\t\t\tinode_dio_wait(inode);\n\t\t\t}\n\n\t\t\tstart_bidx = f2fs_start_bidx_of_node(nofs, inode)\n\t\t\t\t\t\t\t\t+ ofs_in_node;\n\t\t\tif (f2fs_post_read_required(inode))\n\t\t\t\terr = move_data_block(inode, start_bidx,\n\t\t\t\t\t\t\tgc_type, segno, off);\n\t\t\telse\n\t\t\t\terr = move_data_page(inode, start_bidx, gc_type,\n\t\t\t\t\t\t\t\tsegno, off);\n\n\t\t\tif (!err && (gc_type == FG_GC ||\n\t\t\t\t\tf2fs_post_read_required(inode)))\n\t\t\t\tsubmitted++;\n\n\t\t\tif (locked) {\n\t\t\t\tup_write(&fi->i_gc_rwsem[WRITE]);\n\t\t\t\tup_write(&fi->i_gc_rwsem[READ]);\n\t\t\t}\n\n\t\t\tstat_inc_data_blk_count(sbi, 1, gc_type);\n\t\t}\n\t}\n\n\tif (++phase < 5)\n\t\tgoto next_step;\n\n\treturn submitted;\n}",
                        "cve_id": "CVE-2021-44879"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "struct inode *ext4_iget(struct super_block *sb, unsigned long ino)\n{\n\tstruct ext4_iloc iloc;\n\tstruct ext4_inode *raw_inode;\n\tstruct ext4_inode_info *ei;\n\tstruct inode *inode;\n\tjournal_t *journal = EXT4_SB(sb)->s_journal;\n\tlong ret;\n\tloff_t size;\n\tint block;\n\tuid_t i_uid;\n\tgid_t i_gid;\n\tprojid_t i_projid;\n\n\tinode = iget_locked(sb, ino);\n\tif (!inode)\n\t\treturn ERR_PTR(-ENOMEM);\n\tif (!(inode->i_state & I_NEW))\n\t\treturn inode;\n\n\tei = EXT4_I(inode);\n\tiloc.bh = NULL;\n\n\tret = __ext4_get_inode_loc(inode, &iloc, 0);\n\tif (ret < 0)\n\t\tgoto bad_inode;\n\traw_inode = ext4_raw_inode(&iloc);\n\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\tei->i_extra_isize = le16_to_cpu(raw_inode->i_extra_isize);\n\t\tif (EXT4_GOOD_OLD_INODE_SIZE + ei->i_extra_isize >\n\t\t\tEXT4_INODE_SIZE(inode->i_sb) ||\n\t\t    (ei->i_extra_isize & 3)) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"bad extra_isize %u (inode size %u)\",\n\t\t\t\t\t ei->i_extra_isize,\n\t\t\t\t\t EXT4_INODE_SIZE(inode->i_sb));\n\t\t\tret = -EFSCORRUPTED;\n\t\t\tgoto bad_inode;\n\t\t}\n\t} else\n\t\tei->i_extra_isize = 0;\n\n\t/* Precompute checksum seed for inode metadata */\n\tif (ext4_has_metadata_csum(sb)) {\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t\t__u32 csum;\n\t\t__le32 inum = cpu_to_le32(inode->i_ino);\n\t\t__le32 gen = raw_inode->i_generation;\n\t\tcsum = ext4_chksum(sbi, sbi->s_csum_seed, (__u8 *)&inum,\n\t\t\t\t   sizeof(inum));\n\t\tei->i_csum_seed = ext4_chksum(sbi, csum, (__u8 *)&gen,\n\t\t\t\t\t      sizeof(gen));\n\t}\n\n\tif (!ext4_inode_csum_verify(inode, raw_inode, ei)) {\n\t\tEXT4_ERROR_INODE(inode, \"checksum invalid\");\n\t\tret = -EFSBADCRC;\n\t\tgoto bad_inode;\n\t}\n\n\tinode->i_mode = le16_to_cpu(raw_inode->i_mode);\n\ti_uid = (uid_t)le16_to_cpu(raw_inode->i_uid_low);\n\ti_gid = (gid_t)le16_to_cpu(raw_inode->i_gid_low);\n\tif (ext4_has_feature_project(sb) &&\n\t    EXT4_INODE_SIZE(sb) > EXT4_GOOD_OLD_INODE_SIZE &&\n\t    EXT4_FITS_IN_INODE(raw_inode, ei, i_projid))\n\t\ti_projid = (projid_t)le32_to_cpu(raw_inode->i_projid);\n\telse\n\t\ti_projid = EXT4_DEF_PROJID;\n\n\tif (!(test_opt(inode->i_sb, NO_UID32))) {\n\t\ti_uid |= le16_to_cpu(raw_inode->i_uid_high) << 16;\n\t\ti_gid |= le16_to_cpu(raw_inode->i_gid_high) << 16;\n\t}\n\ti_uid_write(inode, i_uid);\n\ti_gid_write(inode, i_gid);\n\tei->i_projid = make_kprojid(&init_user_ns, i_projid);\n\tset_nlink(inode, le16_to_cpu(raw_inode->i_links_count));\n\n\text4_clear_state_flags(ei);\t/* Only relevant on 32-bit archs */\n\tei->i_inline_off = 0;\n\tei->i_dir_start_lookup = 0;\n\tei->i_dtime = le32_to_cpu(raw_inode->i_dtime);\n\t/* We now have enough fields to check if the inode was active or not.\n\t * This is needed because nfsd might try to access dead inodes\n\t * the test is that same one that e2fsck uses\n\t * NeilBrown 1999oct15\n\t */\n\tif (inode->i_nlink == 0) {\n\t\tif ((inode->i_mode == 0 ||\n\t\t     !(EXT4_SB(inode->i_sb)->s_mount_state & EXT4_ORPHAN_FS)) &&\n\t\t    ino != EXT4_BOOT_LOADER_INO) {\n\t\t\t/* this inode is deleted */\n\t\t\tret = -ESTALE;\n\t\t\tgoto bad_inode;\n\t\t}\n\t\t/* The only unlinked inodes we let through here have\n\t\t * valid i_mode and are being read by the orphan\n\t\t * recovery code: that's fine, we're about to complete\n\t\t * the process of deleting those.\n\t\t * OR it is the EXT4_BOOT_LOADER_INO which is\n\t\t * not initialized on a new filesystem. */\n\t}\n\tei->i_flags = le32_to_cpu(raw_inode->i_flags);\n\tinode->i_blocks = ext4_inode_blocks(raw_inode, ei);\n\tei->i_file_acl = le32_to_cpu(raw_inode->i_file_acl_lo);\n\tif (ext4_has_feature_64bit(sb))\n\t\tei->i_file_acl |=\n\t\t\t((__u64)le16_to_cpu(raw_inode->i_file_acl_high)) << 32;\n\tinode->i_size = ext4_isize(sb, raw_inode);\n\tif ((size = i_size_read(inode)) < 0) {\n\t\tEXT4_ERROR_INODE(inode, \"bad i_size value: %lld\", size);\n\t\tret = -EFSCORRUPTED;\n\t\tgoto bad_inode;\n\t}\n\tei->i_disksize = inode->i_size;\n#ifdef CONFIG_QUOTA\n\tei->i_reserved_quota = 0;\n#endif\n\tinode->i_generation = le32_to_cpu(raw_inode->i_generation);\n\tei->i_block_group = iloc.block_group;\n\tei->i_last_alloc_group = ~0;\n\t/*\n\t * NOTE! The in-memory inode i_data array is in little-endian order\n\t * even on big-endian machines: we do NOT byteswap the block numbers!\n\t */\n\tfor (block = 0; block < EXT4_N_BLOCKS; block++)\n\t\tei->i_data[block] = raw_inode->i_block[block];\n\tINIT_LIST_HEAD(&ei->i_orphan);\n\n\t/*\n\t * Set transaction id's of transactions that have to be committed\n\t * to finish f[data]sync. We set them to currently running transaction\n\t * as we cannot be sure that the inode or some of its metadata isn't\n\t * part of the transaction - the inode could have been reclaimed and\n\t * now it is reread from disk.\n\t */\n\tif (journal) {\n\t\ttransaction_t *transaction;\n\t\ttid_t tid;\n\n\t\tread_lock(&journal->j_state_lock);\n\t\tif (journal->j_running_transaction)\n\t\t\ttransaction = journal->j_running_transaction;\n\t\telse\n\t\t\ttransaction = journal->j_committing_transaction;\n\t\tif (transaction)\n\t\t\ttid = transaction->t_tid;\n\t\telse\n\t\t\ttid = journal->j_commit_sequence;\n\t\tread_unlock(&journal->j_state_lock);\n\t\tei->i_sync_tid = tid;\n\t\tei->i_datasync_tid = tid;\n\t}\n\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\tif (ei->i_extra_isize == 0) {\n\t\t\t/* The extra space is currently unused. Use it. */\n\t\t\tBUILD_BUG_ON(sizeof(struct ext4_inode) & 3);\n\t\t\tei->i_extra_isize = sizeof(struct ext4_inode) -\n\t\t\t\t\t    EXT4_GOOD_OLD_INODE_SIZE;\n\t\t} else {\n\t\t\text4_iget_extra_inode(inode, raw_inode, ei);\n\t\t}\n\t}\n\n\tEXT4_INODE_GET_XTIME(i_ctime, inode, raw_inode);\n\tEXT4_INODE_GET_XTIME(i_mtime, inode, raw_inode);\n\tEXT4_INODE_GET_XTIME(i_atime, inode, raw_inode);\n\tEXT4_EINODE_GET_XTIME(i_crtime, ei, raw_inode);\n\n\tif (likely(!test_opt2(inode->i_sb, HURD_COMPAT))) {\n\t\tu64 ivers = le32_to_cpu(raw_inode->i_disk_version);\n\n\t\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\t\tif (EXT4_FITS_IN_INODE(raw_inode, ei, i_version_hi))\n\t\t\t\tivers |=\n\t\t    (__u64)(le32_to_cpu(raw_inode->i_version_hi)) << 32;\n\t\t}\n\t\tinode_set_iversion_queried(inode, ivers);\n\t}\n\n\tret = 0;\n\tif (ei->i_file_acl &&\n\t    !ext4_data_block_valid(EXT4_SB(sb), ei->i_file_acl, 1)) {\n\t\tEXT4_ERROR_INODE(inode, \"bad extended attribute block %llu\",\n\t\t\t\t ei->i_file_acl);\n\t\tret = -EFSCORRUPTED;\n\t\tgoto bad_inode;\n\t} else if (!ext4_has_inline_data(inode)) {\n\t\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\t\tif ((S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||\n\t\t\t    (S_ISLNK(inode->i_mode) &&\n\t\t\t     !ext4_inode_is_fast_symlink(inode))))\n\t\t\t\t/* Validate extent which is part of inode */\n\t\t\t\tret = ext4_ext_check_inode(inode);\n\t\t} else if (S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||\n\t\t\t   (S_ISLNK(inode->i_mode) &&\n\t\t\t    !ext4_inode_is_fast_symlink(inode))) {\n\t\t\t/* Validate block references which are part of inode */\n\t\t\tret = ext4_ind_check_inode(inode);\n\t\t}\n\t}\n\tif (ret)\n\t\tgoto bad_inode;\n\n\tif (S_ISREG(inode->i_mode)) {\n\t\tinode->i_op = &ext4_file_inode_operations;\n\t\tinode->i_fop = &ext4_file_operations;\n\t\text4_set_aops(inode);\n\t} else if (S_ISDIR(inode->i_mode)) {\n\t\tinode->i_op = &ext4_dir_inode_operations;\n\t\tinode->i_fop = &ext4_dir_operations;\n\t} else if (S_ISLNK(inode->i_mode)) {\n\t\tif (ext4_encrypted_inode(inode)) {\n\t\t\tinode->i_op = &ext4_encrypted_symlink_inode_operations;\n\t\t\text4_set_aops(inode);\n\t\t} else if (ext4_inode_is_fast_symlink(inode)) {\n\t\t\tinode->i_link = (char *)ei->i_data;\n\t\t\tinode->i_op = &ext4_fast_symlink_inode_operations;\n\t\t\tnd_terminate_link(ei->i_data, inode->i_size,\n\t\t\t\tsizeof(ei->i_data) - 1);\n\t\t} else {\n\t\t\tinode->i_op = &ext4_symlink_inode_operations;\n\t\t\text4_set_aops(inode);\n\t\t}\n\t\tinode_nohighmem(inode);\n\t} else if (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode) ||\n\t      S_ISFIFO(inode->i_mode) || S_ISSOCK(inode->i_mode)) {\n\t\tinode->i_op = &ext4_special_inode_operations;\n\t\tif (raw_inode->i_block[0])\n\t\t\tinit_special_inode(inode, inode->i_mode,\n\t\t\t   old_decode_dev(le32_to_cpu(raw_inode->i_block[0])));\n\t\telse\n\t\t\tinit_special_inode(inode, inode->i_mode,\n\t\t\t   new_decode_dev(le32_to_cpu(raw_inode->i_block[1])));\n\t} else if (ino == EXT4_BOOT_LOADER_INO) {\n\t\tmake_bad_inode(inode);\n\t} else {\n\t\tret = -EFSCORRUPTED;\n\t\tEXT4_ERROR_INODE(inode, \"bogus i_mode (%o)\", inode->i_mode);\n\t\tgoto bad_inode;\n\t}\n\tbrelse(iloc.bh);\n\text4_set_inode_flags(inode);\n\n\tunlock_new_inode(inode);\n\treturn inode;\n\nbad_inode:\n\tbrelse(iloc.bh);\n\tiget_failed(inode);\n\treturn ERR_PTR(ret);\n}",
                        "code_after_change": "struct inode *ext4_iget(struct super_block *sb, unsigned long ino)\n{\n\tstruct ext4_iloc iloc;\n\tstruct ext4_inode *raw_inode;\n\tstruct ext4_inode_info *ei;\n\tstruct inode *inode;\n\tjournal_t *journal = EXT4_SB(sb)->s_journal;\n\tlong ret;\n\tloff_t size;\n\tint block;\n\tuid_t i_uid;\n\tgid_t i_gid;\n\tprojid_t i_projid;\n\n\tinode = iget_locked(sb, ino);\n\tif (!inode)\n\t\treturn ERR_PTR(-ENOMEM);\n\tif (!(inode->i_state & I_NEW))\n\t\treturn inode;\n\n\tei = EXT4_I(inode);\n\tiloc.bh = NULL;\n\n\tret = __ext4_get_inode_loc(inode, &iloc, 0);\n\tif (ret < 0)\n\t\tgoto bad_inode;\n\traw_inode = ext4_raw_inode(&iloc);\n\n\tif ((ino == EXT4_ROOT_INO) && (raw_inode->i_links_count == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"root inode unallocated\");\n\t\tret = -EFSCORRUPTED;\n\t\tgoto bad_inode;\n\t}\n\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\tei->i_extra_isize = le16_to_cpu(raw_inode->i_extra_isize);\n\t\tif (EXT4_GOOD_OLD_INODE_SIZE + ei->i_extra_isize >\n\t\t\tEXT4_INODE_SIZE(inode->i_sb) ||\n\t\t    (ei->i_extra_isize & 3)) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"bad extra_isize %u (inode size %u)\",\n\t\t\t\t\t ei->i_extra_isize,\n\t\t\t\t\t EXT4_INODE_SIZE(inode->i_sb));\n\t\t\tret = -EFSCORRUPTED;\n\t\t\tgoto bad_inode;\n\t\t}\n\t} else\n\t\tei->i_extra_isize = 0;\n\n\t/* Precompute checksum seed for inode metadata */\n\tif (ext4_has_metadata_csum(sb)) {\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t\t__u32 csum;\n\t\t__le32 inum = cpu_to_le32(inode->i_ino);\n\t\t__le32 gen = raw_inode->i_generation;\n\t\tcsum = ext4_chksum(sbi, sbi->s_csum_seed, (__u8 *)&inum,\n\t\t\t\t   sizeof(inum));\n\t\tei->i_csum_seed = ext4_chksum(sbi, csum, (__u8 *)&gen,\n\t\t\t\t\t      sizeof(gen));\n\t}\n\n\tif (!ext4_inode_csum_verify(inode, raw_inode, ei)) {\n\t\tEXT4_ERROR_INODE(inode, \"checksum invalid\");\n\t\tret = -EFSBADCRC;\n\t\tgoto bad_inode;\n\t}\n\n\tinode->i_mode = le16_to_cpu(raw_inode->i_mode);\n\ti_uid = (uid_t)le16_to_cpu(raw_inode->i_uid_low);\n\ti_gid = (gid_t)le16_to_cpu(raw_inode->i_gid_low);\n\tif (ext4_has_feature_project(sb) &&\n\t    EXT4_INODE_SIZE(sb) > EXT4_GOOD_OLD_INODE_SIZE &&\n\t    EXT4_FITS_IN_INODE(raw_inode, ei, i_projid))\n\t\ti_projid = (projid_t)le32_to_cpu(raw_inode->i_projid);\n\telse\n\t\ti_projid = EXT4_DEF_PROJID;\n\n\tif (!(test_opt(inode->i_sb, NO_UID32))) {\n\t\ti_uid |= le16_to_cpu(raw_inode->i_uid_high) << 16;\n\t\ti_gid |= le16_to_cpu(raw_inode->i_gid_high) << 16;\n\t}\n\ti_uid_write(inode, i_uid);\n\ti_gid_write(inode, i_gid);\n\tei->i_projid = make_kprojid(&init_user_ns, i_projid);\n\tset_nlink(inode, le16_to_cpu(raw_inode->i_links_count));\n\n\text4_clear_state_flags(ei);\t/* Only relevant on 32-bit archs */\n\tei->i_inline_off = 0;\n\tei->i_dir_start_lookup = 0;\n\tei->i_dtime = le32_to_cpu(raw_inode->i_dtime);\n\t/* We now have enough fields to check if the inode was active or not.\n\t * This is needed because nfsd might try to access dead inodes\n\t * the test is that same one that e2fsck uses\n\t * NeilBrown 1999oct15\n\t */\n\tif (inode->i_nlink == 0) {\n\t\tif ((inode->i_mode == 0 ||\n\t\t     !(EXT4_SB(inode->i_sb)->s_mount_state & EXT4_ORPHAN_FS)) &&\n\t\t    ino != EXT4_BOOT_LOADER_INO) {\n\t\t\t/* this inode is deleted */\n\t\t\tret = -ESTALE;\n\t\t\tgoto bad_inode;\n\t\t}\n\t\t/* The only unlinked inodes we let through here have\n\t\t * valid i_mode and are being read by the orphan\n\t\t * recovery code: that's fine, we're about to complete\n\t\t * the process of deleting those.\n\t\t * OR it is the EXT4_BOOT_LOADER_INO which is\n\t\t * not initialized on a new filesystem. */\n\t}\n\tei->i_flags = le32_to_cpu(raw_inode->i_flags);\n\tinode->i_blocks = ext4_inode_blocks(raw_inode, ei);\n\tei->i_file_acl = le32_to_cpu(raw_inode->i_file_acl_lo);\n\tif (ext4_has_feature_64bit(sb))\n\t\tei->i_file_acl |=\n\t\t\t((__u64)le16_to_cpu(raw_inode->i_file_acl_high)) << 32;\n\tinode->i_size = ext4_isize(sb, raw_inode);\n\tif ((size = i_size_read(inode)) < 0) {\n\t\tEXT4_ERROR_INODE(inode, \"bad i_size value: %lld\", size);\n\t\tret = -EFSCORRUPTED;\n\t\tgoto bad_inode;\n\t}\n\tei->i_disksize = inode->i_size;\n#ifdef CONFIG_QUOTA\n\tei->i_reserved_quota = 0;\n#endif\n\tinode->i_generation = le32_to_cpu(raw_inode->i_generation);\n\tei->i_block_group = iloc.block_group;\n\tei->i_last_alloc_group = ~0;\n\t/*\n\t * NOTE! The in-memory inode i_data array is in little-endian order\n\t * even on big-endian machines: we do NOT byteswap the block numbers!\n\t */\n\tfor (block = 0; block < EXT4_N_BLOCKS; block++)\n\t\tei->i_data[block] = raw_inode->i_block[block];\n\tINIT_LIST_HEAD(&ei->i_orphan);\n\n\t/*\n\t * Set transaction id's of transactions that have to be committed\n\t * to finish f[data]sync. We set them to currently running transaction\n\t * as we cannot be sure that the inode or some of its metadata isn't\n\t * part of the transaction - the inode could have been reclaimed and\n\t * now it is reread from disk.\n\t */\n\tif (journal) {\n\t\ttransaction_t *transaction;\n\t\ttid_t tid;\n\n\t\tread_lock(&journal->j_state_lock);\n\t\tif (journal->j_running_transaction)\n\t\t\ttransaction = journal->j_running_transaction;\n\t\telse\n\t\t\ttransaction = journal->j_committing_transaction;\n\t\tif (transaction)\n\t\t\ttid = transaction->t_tid;\n\t\telse\n\t\t\ttid = journal->j_commit_sequence;\n\t\tread_unlock(&journal->j_state_lock);\n\t\tei->i_sync_tid = tid;\n\t\tei->i_datasync_tid = tid;\n\t}\n\n\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\tif (ei->i_extra_isize == 0) {\n\t\t\t/* The extra space is currently unused. Use it. */\n\t\t\tBUILD_BUG_ON(sizeof(struct ext4_inode) & 3);\n\t\t\tei->i_extra_isize = sizeof(struct ext4_inode) -\n\t\t\t\t\t    EXT4_GOOD_OLD_INODE_SIZE;\n\t\t} else {\n\t\t\text4_iget_extra_inode(inode, raw_inode, ei);\n\t\t}\n\t}\n\n\tEXT4_INODE_GET_XTIME(i_ctime, inode, raw_inode);\n\tEXT4_INODE_GET_XTIME(i_mtime, inode, raw_inode);\n\tEXT4_INODE_GET_XTIME(i_atime, inode, raw_inode);\n\tEXT4_EINODE_GET_XTIME(i_crtime, ei, raw_inode);\n\n\tif (likely(!test_opt2(inode->i_sb, HURD_COMPAT))) {\n\t\tu64 ivers = le32_to_cpu(raw_inode->i_disk_version);\n\n\t\tif (EXT4_INODE_SIZE(inode->i_sb) > EXT4_GOOD_OLD_INODE_SIZE) {\n\t\t\tif (EXT4_FITS_IN_INODE(raw_inode, ei, i_version_hi))\n\t\t\t\tivers |=\n\t\t    (__u64)(le32_to_cpu(raw_inode->i_version_hi)) << 32;\n\t\t}\n\t\tinode_set_iversion_queried(inode, ivers);\n\t}\n\n\tret = 0;\n\tif (ei->i_file_acl &&\n\t    !ext4_data_block_valid(EXT4_SB(sb), ei->i_file_acl, 1)) {\n\t\tEXT4_ERROR_INODE(inode, \"bad extended attribute block %llu\",\n\t\t\t\t ei->i_file_acl);\n\t\tret = -EFSCORRUPTED;\n\t\tgoto bad_inode;\n\t} else if (!ext4_has_inline_data(inode)) {\n\t\tif (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)) {\n\t\t\tif ((S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||\n\t\t\t    (S_ISLNK(inode->i_mode) &&\n\t\t\t     !ext4_inode_is_fast_symlink(inode))))\n\t\t\t\t/* Validate extent which is part of inode */\n\t\t\t\tret = ext4_ext_check_inode(inode);\n\t\t} else if (S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||\n\t\t\t   (S_ISLNK(inode->i_mode) &&\n\t\t\t    !ext4_inode_is_fast_symlink(inode))) {\n\t\t\t/* Validate block references which are part of inode */\n\t\t\tret = ext4_ind_check_inode(inode);\n\t\t}\n\t}\n\tif (ret)\n\t\tgoto bad_inode;\n\n\tif (S_ISREG(inode->i_mode)) {\n\t\tinode->i_op = &ext4_file_inode_operations;\n\t\tinode->i_fop = &ext4_file_operations;\n\t\text4_set_aops(inode);\n\t} else if (S_ISDIR(inode->i_mode)) {\n\t\tinode->i_op = &ext4_dir_inode_operations;\n\t\tinode->i_fop = &ext4_dir_operations;\n\t} else if (S_ISLNK(inode->i_mode)) {\n\t\tif (ext4_encrypted_inode(inode)) {\n\t\t\tinode->i_op = &ext4_encrypted_symlink_inode_operations;\n\t\t\text4_set_aops(inode);\n\t\t} else if (ext4_inode_is_fast_symlink(inode)) {\n\t\t\tinode->i_link = (char *)ei->i_data;\n\t\t\tinode->i_op = &ext4_fast_symlink_inode_operations;\n\t\t\tnd_terminate_link(ei->i_data, inode->i_size,\n\t\t\t\tsizeof(ei->i_data) - 1);\n\t\t} else {\n\t\t\tinode->i_op = &ext4_symlink_inode_operations;\n\t\t\text4_set_aops(inode);\n\t\t}\n\t\tinode_nohighmem(inode);\n\t} else if (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode) ||\n\t      S_ISFIFO(inode->i_mode) || S_ISSOCK(inode->i_mode)) {\n\t\tinode->i_op = &ext4_special_inode_operations;\n\t\tif (raw_inode->i_block[0])\n\t\t\tinit_special_inode(inode, inode->i_mode,\n\t\t\t   old_decode_dev(le32_to_cpu(raw_inode->i_block[0])));\n\t\telse\n\t\t\tinit_special_inode(inode, inode->i_mode,\n\t\t\t   new_decode_dev(le32_to_cpu(raw_inode->i_block[1])));\n\t} else if (ino == EXT4_BOOT_LOADER_INO) {\n\t\tmake_bad_inode(inode);\n\t} else {\n\t\tret = -EFSCORRUPTED;\n\t\tEXT4_ERROR_INODE(inode, \"bogus i_mode (%o)\", inode->i_mode);\n\t\tgoto bad_inode;\n\t}\n\tbrelse(iloc.bh);\n\text4_set_inode_flags(inode);\n\n\tunlock_new_inode(inode);\n\treturn inode;\n\nbad_inode:\n\tbrelse(iloc.bh);\n\tiget_failed(inode);\n\treturn ERR_PTR(ret);\n}",
                        "cve_id": "CVE-2018-1092"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static bool __written_first_block(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct f2fs_inode *ri)\n{\n\tblock_t addr = le32_to_cpu(ri->i_addr[offset_in_addr(ri)]);\n\n\tif (is_valid_data_blkaddr(sbi, addr))\n\t\treturn true;\n\treturn false;\n}",
                        "code_after_change": "static int __written_first_block(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct f2fs_inode *ri)\n{\n\tblock_t addr = le32_to_cpu(ri->i_addr[offset_in_addr(ri)]);\n\n\tif (!__is_valid_data_blkaddr(addr))\n\t\treturn 1;\n\tif (!f2fs_is_valid_blkaddr(sbi, addr, DATA_GENERIC))\n\t\treturn -EFAULT;\n\treturn 0;\n}",
                        "cve_id": "CVE-2018-14616"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1794,
            "cve_id": "CVE-2018-14646",
            "code_snippet": "static int rtnl_dump_ifinfo(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct net *tgt_net = net;\n\tint h, s_h;\n\tint idx = 0, s_idx;\n\tstruct net_device *dev;\n\tstruct hlist_head *head;\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tu32 ext_filter_mask = 0;\n\tconst struct rtnl_link_ops *kind_ops = NULL;\n\tunsigned int flags = NLM_F_MULTI;\n\tint master_idx = 0;\n\tint netnsid = -1;\n\tint err;\n\tint hdrlen;\n\n\ts_h = cb->args[0];\n\ts_idx = cb->args[1];\n\n\t/* A hack to preserve kernel<->userspace interface.\n\t * The correct header is ifinfomsg. It is consistent with rtnl_getlink.\n\t * However, before Linux v3.9 the code here assumed rtgenmsg and that's\n\t * what iproute2 < v3.9.0 used.\n\t * We can detect the old iproute2. Even including the IFLA_EXT_MASK\n\t * attribute, its netlink message is shorter than struct ifinfomsg.\n\t */\n\thdrlen = nlmsg_len(cb->nlh) < sizeof(struct ifinfomsg) ?\n\t\t sizeof(struct rtgenmsg) : sizeof(struct ifinfomsg);\n\n\tif (nlmsg_parse(cb->nlh, hdrlen, tb, IFLA_MAX,\n\t\t\tifla_policy, NULL) >= 0) {\n\t\tif (tb[IFLA_IF_NETNSID]) {\n\t\t\tnetnsid = nla_get_s32(tb[IFLA_IF_NETNSID]);\n\t\t\ttgt_net = get_target_net(skb->sk, netnsid);\n\t\t\tif (IS_ERR(tgt_net)) {\n\t\t\t\ttgt_net = net;\n\t\t\t\tnetnsid = -1;\n\t\t\t}\n\t\t}\n\n\t\tif (tb[IFLA_EXT_MASK])\n\t\t\text_filter_mask = nla_get_u32(tb[IFLA_EXT_MASK]);\n\n\t\tif (tb[IFLA_MASTER])\n\t\t\tmaster_idx = nla_get_u32(tb[IFLA_MASTER]);\n\n\t\tif (tb[IFLA_LINKINFO])\n\t\t\tkind_ops = linkinfo_to_kind_ops(tb[IFLA_LINKINFO]);\n\n\t\tif (master_idx || kind_ops)\n\t\t\tflags |= NLM_F_DUMP_FILTERED;\n\t}\n\n\tfor (h = s_h; h < NETDEV_HASHENTRIES; h++, s_idx = 0) {\n\t\tidx = 0;\n\t\thead = &tgt_net->dev_index_head[h];\n\t\thlist_for_each_entry(dev, head, index_hlist) {\n\t\t\tif (link_dump_filtered(dev, master_idx, kind_ops))\n\t\t\t\tgoto cont;\n\t\t\tif (idx < s_idx)\n\t\t\t\tgoto cont;\n\t\t\terr = rtnl_fill_ifinfo(skb, dev, net,\n\t\t\t\t\t       RTM_NEWLINK,\n\t\t\t\t\t       NETLINK_CB(cb->skb).portid,\n\t\t\t\t\t       cb->nlh->nlmsg_seq, 0,\n\t\t\t\t\t       flags,\n\t\t\t\t\t       ext_filter_mask, 0, NULL,\n\t\t\t\t\t       netnsid);\n\n\t\t\tif (err < 0) {\n\t\t\t\tif (likely(skb->len))\n\t\t\t\t\tgoto out;\n\n\t\t\t\tgoto out_err;\n\t\t\t}\ncont:\n\t\t\tidx++;\n\t\t}\n\t}\nout:\n\terr = skb->len;\nout_err:\n\tcb->args[1] = idx;\n\tcb->args[0] = h;\n\tcb->seq = net->dev_base_seq;\n\tnl_dump_check_consistent(cb, nlmsg_hdr(skb));\n\tif (netnsid >= 0)\n\t\tput_net(tgt_net);\n\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int rtnl_getlink(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct net *tgt_net = net;\n\tstruct ifinfomsg *ifm;\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct net_device *dev = NULL;\n\tstruct sk_buff *nskb;\n\tint netnsid = -1;\n\tint err;\n\tu32 ext_filter_mask = 0;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IF_NETNSID]) {\n\t\tnetnsid = nla_get_s32(tb[IFLA_IF_NETNSID]);\n\t\ttgt_net = get_target_net(skb, netnsid);\n\t\tif (IS_ERR(tgt_net))\n\t\t\treturn PTR_ERR(tgt_net);\n\t}\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\n\tif (tb[IFLA_EXT_MASK])\n\t\text_filter_mask = nla_get_u32(tb[IFLA_EXT_MASK]);\n\n\terr = -EINVAL;\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(tgt_net, ifm->ifi_index);\n\telse if (tb[IFLA_IFNAME])\n\t\tdev = __dev_get_by_name(tgt_net, ifname);\n\telse\n\t\tgoto out;\n\n\terr = -ENODEV;\n\tif (dev == NULL)\n\t\tgoto out;\n\n\terr = -ENOBUFS;\n\tnskb = nlmsg_new(if_nlmsg_size(dev, ext_filter_mask), GFP_KERNEL);\n\tif (nskb == NULL)\n\t\tgoto out;\n\n\terr = rtnl_fill_ifinfo(nskb, dev, net,\n\t\t\t       RTM_NEWLINK, NETLINK_CB(skb).portid,\n\t\t\t       nlh->nlmsg_seq, 0, 0, ext_filter_mask,\n\t\t\t       0, NULL, netnsid);\n\tif (err < 0) {\n\t\t/* -EMSGSIZE implies BUG in if_nlmsg_size */\n\t\tWARN_ON(err == -EMSGSIZE);\n\t\tkfree_skb(nskb);\n\t} else\n\t\terr = rtnl_unicast(nskb, net, NETLINK_CB(skb).portid);\nout:\n\tif (netnsid >= 0)\n\t\tput_net(tgt_net);\n\n\treturn err;\n}",
                        "code_after_change": "static int rtnl_getlink(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct net *tgt_net = net;\n\tstruct ifinfomsg *ifm;\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct net_device *dev = NULL;\n\tstruct sk_buff *nskb;\n\tint netnsid = -1;\n\tint err;\n\tu32 ext_filter_mask = 0;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IF_NETNSID]) {\n\t\tnetnsid = nla_get_s32(tb[IFLA_IF_NETNSID]);\n\t\ttgt_net = get_target_net(NETLINK_CB(skb).sk, netnsid);\n\t\tif (IS_ERR(tgt_net))\n\t\t\treturn PTR_ERR(tgt_net);\n\t}\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\n\tif (tb[IFLA_EXT_MASK])\n\t\text_filter_mask = nla_get_u32(tb[IFLA_EXT_MASK]);\n\n\terr = -EINVAL;\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(tgt_net, ifm->ifi_index);\n\telse if (tb[IFLA_IFNAME])\n\t\tdev = __dev_get_by_name(tgt_net, ifname);\n\telse\n\t\tgoto out;\n\n\terr = -ENODEV;\n\tif (dev == NULL)\n\t\tgoto out;\n\n\terr = -ENOBUFS;\n\tnskb = nlmsg_new(if_nlmsg_size(dev, ext_filter_mask), GFP_KERNEL);\n\tif (nskb == NULL)\n\t\tgoto out;\n\n\terr = rtnl_fill_ifinfo(nskb, dev, net,\n\t\t\t       RTM_NEWLINK, NETLINK_CB(skb).portid,\n\t\t\t       nlh->nlmsg_seq, 0, 0, ext_filter_mask,\n\t\t\t       0, NULL, netnsid);\n\tif (err < 0) {\n\t\t/* -EMSGSIZE implies BUG in if_nlmsg_size */\n\t\tWARN_ON(err == -EMSGSIZE);\n\t\tkfree_skb(nskb);\n\t} else\n\t\terr = rtnl_unicast(nskb, net, NETLINK_CB(skb).portid);\nout:\n\tif (netnsid >= 0)\n\t\tput_net(tgt_net);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2018-14646"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int unix_diag_dump(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tint num, s_num, slot, s_slot;\n\tstruct unix_diag_req *req;\n\n\treq = nlmsg_data(cb->nlh);\n\n\ts_slot = cb->args[0];\n\tnum = s_num = cb->args[1];\n\n\tfor (slot = s_slot; slot < UNIX_HASH_SIZE; s_num = 0, slot++) {\n\t\tstruct sock *sk;\n\n\t\tnum = 0;\n\t\tspin_lock(&net->unx.table.locks[slot]);\n\t\tsk_for_each(sk, &net->unx.table.buckets[slot]) {\n\t\t\tif (num < s_num)\n\t\t\t\tgoto next;\n\t\t\tif (!(req->udiag_states & (1 << sk->sk_state)))\n\t\t\t\tgoto next;\n\t\t\tif (sk_diag_dump(sk, skb, req,\n\t\t\t\t\t NETLINK_CB(cb->skb).portid,\n\t\t\t\t\t cb->nlh->nlmsg_seq,\n\t\t\t\t\t NLM_F_MULTI) < 0) {\n\t\t\t\tspin_unlock(&net->unx.table.locks[slot]);\n\t\t\t\tgoto done;\n\t\t\t}\nnext:\n\t\t\tnum++;\n\t\t}\n\t\tspin_unlock(&net->unx.table.locks[slot]);\n\t}\ndone:\n\tcb->args[0] = slot;\n\tcb->args[1] = num;\n\n\treturn skb->len;\n}",
                        "code_after_change": "static int unix_diag_dump(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tint num, s_num, slot, s_slot;\n\tstruct unix_diag_req *req;\n\n\treq = nlmsg_data(cb->nlh);\n\n\ts_slot = cb->args[0];\n\tnum = s_num = cb->args[1];\n\n\tfor (slot = s_slot; slot < UNIX_HASH_SIZE; s_num = 0, slot++) {\n\t\tstruct sock *sk;\n\n\t\tnum = 0;\n\t\tspin_lock(&net->unx.table.locks[slot]);\n\t\tsk_for_each(sk, &net->unx.table.buckets[slot]) {\n\t\t\tif (num < s_num)\n\t\t\t\tgoto next;\n\t\t\tif (!(req->udiag_states & (1 << sk->sk_state)))\n\t\t\t\tgoto next;\n\t\t\tif (sk_diag_dump(sk, skb, req, sk_user_ns(skb->sk),\n\t\t\t\t\t NETLINK_CB(cb->skb).portid,\n\t\t\t\t\t cb->nlh->nlmsg_seq,\n\t\t\t\t\t NLM_F_MULTI) < 0) {\n\t\t\t\tspin_unlock(&net->unx.table.locks[slot]);\n\t\t\t\tgoto done;\n\t\t\t}\nnext:\n\t\t\tnum++;\n\t\t}\n\t\tspin_unlock(&net->unx.table.locks[slot]);\n\t}\ndone:\n\tcb->args[0] = slot;\n\tcb->args[1] = num;\n\n\treturn skb->len;\n}",
                        "cve_id": "CVE-2023-28327"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int inet_rtm_getroute(struct sk_buff *in_skb, struct nlmsghdr *nlh,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(in_skb->sk);\n\tstruct rtmsg *rtm;\n\tstruct nlattr *tb[RTA_MAX+1];\n\tstruct fib_result res = {};\n\tstruct rtable *rt = NULL;\n\tstruct flowi4 fl4;\n\t__be32 dst = 0;\n\t__be32 src = 0;\n\tu32 iif;\n\tint err;\n\tint mark;\n\tstruct sk_buff *skb;\n\tu32 table_id = RT_TABLE_MAIN;\n\tkuid_t uid;\n\n\terr = nlmsg_parse(nlh, sizeof(*rtm), tb, RTA_MAX, rtm_ipv4_policy,\n\t\t\t  extack);\n\tif (err < 0)\n\t\tgoto errout;\n\n\trtm = nlmsg_data(nlh);\n\n\tskb = alloc_skb(NLMSG_GOODSIZE, GFP_KERNEL);\n\tif (!skb) {\n\t\terr = -ENOBUFS;\n\t\tgoto errout;\n\t}\n\n\t/* Reserve room for dummy headers, this skb can pass\n\t   through good chunk of routing engine.\n\t */\n\tskb_reset_mac_header(skb);\n\tskb_reset_network_header(skb);\n\n\tsrc = tb[RTA_SRC] ? nla_get_in_addr(tb[RTA_SRC]) : 0;\n\tdst = tb[RTA_DST] ? nla_get_in_addr(tb[RTA_DST]) : 0;\n\tiif = tb[RTA_IIF] ? nla_get_u32(tb[RTA_IIF]) : 0;\n\tmark = tb[RTA_MARK] ? nla_get_u32(tb[RTA_MARK]) : 0;\n\tif (tb[RTA_UID])\n\t\tuid = make_kuid(current_user_ns(), nla_get_u32(tb[RTA_UID]));\n\telse\n\t\tuid = (iif ? INVALID_UID : current_uid());\n\n\t/* Bugfix: need to give ip_route_input enough of an IP header to\n\t * not gag.\n\t */\n\tip_hdr(skb)->protocol = IPPROTO_UDP;\n\tip_hdr(skb)->saddr = src;\n\tip_hdr(skb)->daddr = dst;\n\n\tskb_reserve(skb, MAX_HEADER + sizeof(struct iphdr));\n\n\tmemset(&fl4, 0, sizeof(fl4));\n\tfl4.daddr = dst;\n\tfl4.saddr = src;\n\tfl4.flowi4_tos = rtm->rtm_tos;\n\tfl4.flowi4_oif = tb[RTA_OIF] ? nla_get_u32(tb[RTA_OIF]) : 0;\n\tfl4.flowi4_mark = mark;\n\tfl4.flowi4_uid = uid;\n\n\trcu_read_lock();\n\n\tif (iif) {\n\t\tstruct net_device *dev;\n\n\t\tdev = dev_get_by_index_rcu(net, iif);\n\t\tif (!dev) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto errout_free;\n\t\t}\n\n\t\tskb->protocol\t= htons(ETH_P_IP);\n\t\tskb->dev\t= dev;\n\t\tskb->mark\t= mark;\n\t\terr = ip_route_input_rcu(skb, dst, src, rtm->rtm_tos,\n\t\t\t\t\t dev, &res);\n\n\t\trt = skb_rtable(skb);\n\t\tif (err == 0 && rt->dst.error)\n\t\t\terr = -rt->dst.error;\n\t} else {\n\t\trt = ip_route_output_key_hash_rcu(net, &fl4, &res, skb);\n\t\terr = 0;\n\t\tif (IS_ERR(rt))\n\t\t\terr = PTR_ERR(rt);\n\t\telse\n\t\t\tskb_dst_set(skb, &rt->dst);\n\t}\n\n\tif (err)\n\t\tgoto errout_free;\n\n\tif (rtm->rtm_flags & RTM_F_NOTIFY)\n\t\trt->rt_flags |= RTCF_NOTIFY;\n\n\tif (rtm->rtm_flags & RTM_F_LOOKUP_TABLE)\n\t\ttable_id = rt->rt_table_id;\n\n\tif (rtm->rtm_flags & RTM_F_FIB_MATCH)\n\t\terr = fib_dump_info(skb, NETLINK_CB(in_skb).portid,\n\t\t\t\t    nlh->nlmsg_seq, RTM_NEWROUTE, table_id,\n\t\t\t\t    rt->rt_type, res.prefix, res.prefixlen,\n\t\t\t\t    fl4.flowi4_tos, res.fi, 0);\n\telse\n\t\terr = rt_fill_info(net, dst, src, table_id, &fl4, skb,\n\t\t\t\t   NETLINK_CB(in_skb).portid, nlh->nlmsg_seq);\n\tif (err < 0)\n\t\tgoto errout_free;\n\n\trcu_read_unlock();\n\n\terr = rtnl_unicast(skb, net, NETLINK_CB(in_skb).portid);\nerrout:\n\treturn err;\n\nerrout_free:\n\trcu_read_unlock();\n\tkfree_skb(skb);\n\tgoto errout;\n}",
                        "code_after_change": "static int inet_rtm_getroute(struct sk_buff *in_skb, struct nlmsghdr *nlh,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(in_skb->sk);\n\tstruct rtmsg *rtm;\n\tstruct nlattr *tb[RTA_MAX+1];\n\tstruct fib_result res = {};\n\tstruct rtable *rt = NULL;\n\tstruct flowi4 fl4;\n\t__be32 dst = 0;\n\t__be32 src = 0;\n\tu32 iif;\n\tint err;\n\tint mark;\n\tstruct sk_buff *skb;\n\tu32 table_id = RT_TABLE_MAIN;\n\tkuid_t uid;\n\n\terr = nlmsg_parse(nlh, sizeof(*rtm), tb, RTA_MAX, rtm_ipv4_policy,\n\t\t\t  extack);\n\tif (err < 0)\n\t\tgoto errout;\n\n\trtm = nlmsg_data(nlh);\n\n\tskb = alloc_skb(NLMSG_GOODSIZE, GFP_KERNEL);\n\tif (!skb) {\n\t\terr = -ENOBUFS;\n\t\tgoto errout;\n\t}\n\n\t/* Reserve room for dummy headers, this skb can pass\n\t   through good chunk of routing engine.\n\t */\n\tskb_reset_mac_header(skb);\n\tskb_reset_network_header(skb);\n\n\tsrc = tb[RTA_SRC] ? nla_get_in_addr(tb[RTA_SRC]) : 0;\n\tdst = tb[RTA_DST] ? nla_get_in_addr(tb[RTA_DST]) : 0;\n\tiif = tb[RTA_IIF] ? nla_get_u32(tb[RTA_IIF]) : 0;\n\tmark = tb[RTA_MARK] ? nla_get_u32(tb[RTA_MARK]) : 0;\n\tif (tb[RTA_UID])\n\t\tuid = make_kuid(current_user_ns(), nla_get_u32(tb[RTA_UID]));\n\telse\n\t\tuid = (iif ? INVALID_UID : current_uid());\n\n\t/* Bugfix: need to give ip_route_input enough of an IP header to\n\t * not gag.\n\t */\n\tip_hdr(skb)->protocol = IPPROTO_UDP;\n\tip_hdr(skb)->saddr = src;\n\tip_hdr(skb)->daddr = dst;\n\n\tskb_reserve(skb, MAX_HEADER + sizeof(struct iphdr));\n\n\tmemset(&fl4, 0, sizeof(fl4));\n\tfl4.daddr = dst;\n\tfl4.saddr = src;\n\tfl4.flowi4_tos = rtm->rtm_tos;\n\tfl4.flowi4_oif = tb[RTA_OIF] ? nla_get_u32(tb[RTA_OIF]) : 0;\n\tfl4.flowi4_mark = mark;\n\tfl4.flowi4_uid = uid;\n\n\trcu_read_lock();\n\n\tif (iif) {\n\t\tstruct net_device *dev;\n\n\t\tdev = dev_get_by_index_rcu(net, iif);\n\t\tif (!dev) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto errout_free;\n\t\t}\n\n\t\tskb->protocol\t= htons(ETH_P_IP);\n\t\tskb->dev\t= dev;\n\t\tskb->mark\t= mark;\n\t\terr = ip_route_input_rcu(skb, dst, src, rtm->rtm_tos,\n\t\t\t\t\t dev, &res);\n\n\t\trt = skb_rtable(skb);\n\t\tif (err == 0 && rt->dst.error)\n\t\t\terr = -rt->dst.error;\n\t} else {\n\t\trt = ip_route_output_key_hash_rcu(net, &fl4, &res, skb);\n\t\terr = 0;\n\t\tif (IS_ERR(rt))\n\t\t\terr = PTR_ERR(rt);\n\t\telse\n\t\t\tskb_dst_set(skb, &rt->dst);\n\t}\n\n\tif (err)\n\t\tgoto errout_free;\n\n\tif (rtm->rtm_flags & RTM_F_NOTIFY)\n\t\trt->rt_flags |= RTCF_NOTIFY;\n\n\tif (rtm->rtm_flags & RTM_F_LOOKUP_TABLE)\n\t\ttable_id = rt->rt_table_id;\n\n\tif (rtm->rtm_flags & RTM_F_FIB_MATCH) {\n\t\tif (!res.fi) {\n\t\t\terr = fib_props[res.type].error;\n\t\t\tif (!err)\n\t\t\t\terr = -EHOSTUNREACH;\n\t\t\tgoto errout_free;\n\t\t}\n\t\terr = fib_dump_info(skb, NETLINK_CB(in_skb).portid,\n\t\t\t\t    nlh->nlmsg_seq, RTM_NEWROUTE, table_id,\n\t\t\t\t    rt->rt_type, res.prefix, res.prefixlen,\n\t\t\t\t    fl4.flowi4_tos, res.fi, 0);\n\t} else {\n\t\terr = rt_fill_info(net, dst, src, table_id, &fl4, skb,\n\t\t\t\t   NETLINK_CB(in_skb).portid, nlh->nlmsg_seq);\n\t}\n\tif (err < 0)\n\t\tgoto errout_free;\n\n\trcu_read_unlock();\n\n\terr = rtnl_unicast(skb, net, NETLINK_CB(in_skb).portid);\nerrout:\n\treturn err;\n\nerrout_free:\n\trcu_read_unlock();\n\tkfree_skb(skb);\n\tgoto errout;\n}",
                        "cve_id": "CVE-2017-13686"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int xfrm_dump_sa_done(struct netlink_callback *cb)\n{\n\tstruct xfrm_state_walk *walk = (struct xfrm_state_walk *) &cb->args[1];\n\tstruct sock *sk = cb->skb->sk;\n\tstruct net *net = sock_net(sk);\n\n\txfrm_state_walk_done(walk, net);\n\treturn 0;\n}",
                        "code_after_change": "static int xfrm_dump_sa_done(struct netlink_callback *cb)\n{\n\tstruct xfrm_state_walk *walk = (struct xfrm_state_walk *) &cb->args[1];\n\tstruct sock *sk = cb->skb->sk;\n\tstruct net *net = sock_net(sk);\n\n\tif (cb->args[0])\n\t\txfrm_state_walk_done(walk, net);\n\treturn 0;\n}",
                        "cve_id": "CVE-2023-3106"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1942,
            "cve_id": "CVE-2018-7191",
            "code_snippet": "int dev_get_valid_name(struct net *net, struct net_device *dev,\n\t\t       const char *name)\n{\n\tBUG_ON(!net);\n\n\tif (!dev_valid_name(name))\n\t\treturn -EINVAL;\n\n\tif (strchr(name, '%'))\n\t\treturn dev_alloc_name_ns(net, dev, name);\n\telse if (__dev_get_by_name(net, name))\n\t\treturn -EEXIST;\n\telse if (dev->name != name)\n\t\tstrlcpy(dev->name, name, IFNAMSIZ);\n\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tun_set_iff(struct net *net, struct file *file, struct ifreq *ifr)\n{\n\tstruct tun_struct *tun;\n\tstruct tun_file *tfile = file->private_data;\n\tstruct net_device *dev;\n\tint err;\n\n\tif (tfile->detached)\n\t\treturn -EINVAL;\n\n\tdev = __dev_get_by_name(net, ifr->ifr_name);\n\tif (dev) {\n\t\tif (ifr->ifr_flags & IFF_TUN_EXCL)\n\t\t\treturn -EBUSY;\n\t\tif ((ifr->ifr_flags & IFF_TUN) && dev->netdev_ops == &tun_netdev_ops)\n\t\t\ttun = netdev_priv(dev);\n\t\telse if ((ifr->ifr_flags & IFF_TAP) && dev->netdev_ops == &tap_netdev_ops)\n\t\t\ttun = netdev_priv(dev);\n\t\telse\n\t\t\treturn -EINVAL;\n\n\t\tif (!!(ifr->ifr_flags & IFF_MULTI_QUEUE) !=\n\t\t    !!(tun->flags & IFF_MULTI_QUEUE))\n\t\t\treturn -EINVAL;\n\n\t\tif (tun_not_capable(tun))\n\t\t\treturn -EPERM;\n\t\terr = security_tun_dev_open(tun->security);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\terr = tun_attach(tun, file, ifr->ifr_flags & IFF_NOFILTER);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\tif (tun->flags & IFF_MULTI_QUEUE &&\n\t\t    (tun->numqueues + tun->numdisabled > 1)) {\n\t\t\t/* One or more queue has already been attached, no need\n\t\t\t * to initialize the device again.\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\t}\n\telse {\n\t\tchar *name;\n\t\tunsigned long flags = 0;\n\t\tint queues = ifr->ifr_flags & IFF_MULTI_QUEUE ?\n\t\t\t     MAX_TAP_QUEUES : 1;\n\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\treturn -EPERM;\n\t\terr = security_tun_dev_create();\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\t/* Set dev type */\n\t\tif (ifr->ifr_flags & IFF_TUN) {\n\t\t\t/* TUN device */\n\t\t\tflags |= IFF_TUN;\n\t\t\tname = \"tun%d\";\n\t\t} else if (ifr->ifr_flags & IFF_TAP) {\n\t\t\t/* TAP device */\n\t\t\tflags |= IFF_TAP;\n\t\t\tname = \"tap%d\";\n\t\t} else\n\t\t\treturn -EINVAL;\n\n\t\tif (*ifr->ifr_name)\n\t\t\tname = ifr->ifr_name;\n\n\t\tdev = alloc_netdev_mqs(sizeof(struct tun_struct), name,\n\t\t\t\t       NET_NAME_UNKNOWN, tun_setup, queues,\n\t\t\t\t       queues);\n\n\t\tif (!dev)\n\t\t\treturn -ENOMEM;\n\n\t\tdev_net_set(dev, net);\n\t\tdev->rtnl_link_ops = &tun_link_ops;\n\t\tdev->ifindex = tfile->ifindex;\n\t\tdev->sysfs_groups[0] = &tun_attr_group;\n\n\t\ttun = netdev_priv(dev);\n\t\ttun->dev = dev;\n\t\ttun->flags = flags;\n\t\ttun->txflt.count = 0;\n\t\ttun->vnet_hdr_sz = sizeof(struct virtio_net_hdr);\n\n\t\ttun->align = NET_SKB_PAD;\n\t\ttun->filter_attached = false;\n\t\ttun->sndbuf = tfile->socket.sk->sk_sndbuf;\n\t\ttun->rx_batched = 0;\n\n\t\ttun->pcpu_stats = netdev_alloc_pcpu_stats(struct tun_pcpu_stats);\n\t\tif (!tun->pcpu_stats) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_free_dev;\n\t\t}\n\n\t\tspin_lock_init(&tun->lock);\n\n\t\terr = security_tun_dev_alloc_security(&tun->security);\n\t\tif (err < 0)\n\t\t\tgoto err_free_stat;\n\n\t\ttun_net_init(dev);\n\t\ttun_flow_init(tun);\n\n\t\tdev->hw_features = NETIF_F_SG | NETIF_F_FRAGLIST |\n\t\t\t\t   TUN_USER_FEATURES | NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t   NETIF_F_HW_VLAN_STAG_TX;\n\t\tdev->features = dev->hw_features | NETIF_F_LLTX;\n\t\tdev->vlan_features = dev->features &\n\t\t\t\t     ~(NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t       NETIF_F_HW_VLAN_STAG_TX);\n\n\t\tINIT_LIST_HEAD(&tun->disabled);\n\t\terr = tun_attach(tun, file, false);\n\t\tif (err < 0)\n\t\t\tgoto err_free_flow;\n\n\t\terr = register_netdevice(tun->dev);\n\t\tif (err < 0)\n\t\t\tgoto err_detach;\n\t}\n\n\tnetif_carrier_on(tun->dev);\n\n\ttun_debug(KERN_INFO, tun, \"tun_set_iff\\n\");\n\n\ttun->flags = (tun->flags & ~TUN_FEATURES) |\n\t\t(ifr->ifr_flags & TUN_FEATURES);\n\n\t/* Make sure persistent devices do not get stuck in\n\t * xoff state.\n\t */\n\tif (netif_running(tun->dev))\n\t\tnetif_tx_wake_all_queues(tun->dev);\n\n\tstrcpy(ifr->ifr_name, tun->dev->name);\n\treturn 0;\n\nerr_detach:\n\ttun_detach_all(dev);\n\t/* register_netdevice() already called tun_free_netdev() */\n\tgoto err_free_dev;\n\nerr_free_flow:\n\ttun_flow_uninit(tun);\n\tsecurity_tun_dev_free_security(tun->security);\nerr_free_stat:\n\tfree_percpu(tun->pcpu_stats);\nerr_free_dev:\n\tfree_netdev(dev);\n\treturn err;\n}",
                        "code_after_change": "static int tun_set_iff(struct net *net, struct file *file, struct ifreq *ifr)\n{\n\tstruct tun_struct *tun;\n\tstruct tun_file *tfile = file->private_data;\n\tstruct net_device *dev;\n\tint err;\n\n\tif (tfile->detached)\n\t\treturn -EINVAL;\n\n\tdev = __dev_get_by_name(net, ifr->ifr_name);\n\tif (dev) {\n\t\tif (ifr->ifr_flags & IFF_TUN_EXCL)\n\t\t\treturn -EBUSY;\n\t\tif ((ifr->ifr_flags & IFF_TUN) && dev->netdev_ops == &tun_netdev_ops)\n\t\t\ttun = netdev_priv(dev);\n\t\telse if ((ifr->ifr_flags & IFF_TAP) && dev->netdev_ops == &tap_netdev_ops)\n\t\t\ttun = netdev_priv(dev);\n\t\telse\n\t\t\treturn -EINVAL;\n\n\t\tif (!!(ifr->ifr_flags & IFF_MULTI_QUEUE) !=\n\t\t    !!(tun->flags & IFF_MULTI_QUEUE))\n\t\t\treturn -EINVAL;\n\n\t\tif (tun_not_capable(tun))\n\t\t\treturn -EPERM;\n\t\terr = security_tun_dev_open(tun->security);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\terr = tun_attach(tun, file, ifr->ifr_flags & IFF_NOFILTER);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\tif (tun->flags & IFF_MULTI_QUEUE &&\n\t\t    (tun->numqueues + tun->numdisabled > 1)) {\n\t\t\t/* One or more queue has already been attached, no need\n\t\t\t * to initialize the device again.\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\t}\n\telse {\n\t\tchar *name;\n\t\tunsigned long flags = 0;\n\t\tint queues = ifr->ifr_flags & IFF_MULTI_QUEUE ?\n\t\t\t     MAX_TAP_QUEUES : 1;\n\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\treturn -EPERM;\n\t\terr = security_tun_dev_create();\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\t/* Set dev type */\n\t\tif (ifr->ifr_flags & IFF_TUN) {\n\t\t\t/* TUN device */\n\t\t\tflags |= IFF_TUN;\n\t\t\tname = \"tun%d\";\n\t\t} else if (ifr->ifr_flags & IFF_TAP) {\n\t\t\t/* TAP device */\n\t\t\tflags |= IFF_TAP;\n\t\t\tname = \"tap%d\";\n\t\t} else\n\t\t\treturn -EINVAL;\n\n\t\tif (*ifr->ifr_name)\n\t\t\tname = ifr->ifr_name;\n\n\t\tdev = alloc_netdev_mqs(sizeof(struct tun_struct), name,\n\t\t\t\t       NET_NAME_UNKNOWN, tun_setup, queues,\n\t\t\t\t       queues);\n\n\t\tif (!dev)\n\t\t\treturn -ENOMEM;\n\t\terr = dev_get_valid_name(net, dev, name);\n\t\tif (err)\n\t\t\tgoto err_free_dev;\n\n\t\tdev_net_set(dev, net);\n\t\tdev->rtnl_link_ops = &tun_link_ops;\n\t\tdev->ifindex = tfile->ifindex;\n\t\tdev->sysfs_groups[0] = &tun_attr_group;\n\n\t\ttun = netdev_priv(dev);\n\t\ttun->dev = dev;\n\t\ttun->flags = flags;\n\t\ttun->txflt.count = 0;\n\t\ttun->vnet_hdr_sz = sizeof(struct virtio_net_hdr);\n\n\t\ttun->align = NET_SKB_PAD;\n\t\ttun->filter_attached = false;\n\t\ttun->sndbuf = tfile->socket.sk->sk_sndbuf;\n\t\ttun->rx_batched = 0;\n\n\t\ttun->pcpu_stats = netdev_alloc_pcpu_stats(struct tun_pcpu_stats);\n\t\tif (!tun->pcpu_stats) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_free_dev;\n\t\t}\n\n\t\tspin_lock_init(&tun->lock);\n\n\t\terr = security_tun_dev_alloc_security(&tun->security);\n\t\tif (err < 0)\n\t\t\tgoto err_free_stat;\n\n\t\ttun_net_init(dev);\n\t\ttun_flow_init(tun);\n\n\t\tdev->hw_features = NETIF_F_SG | NETIF_F_FRAGLIST |\n\t\t\t\t   TUN_USER_FEATURES | NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t   NETIF_F_HW_VLAN_STAG_TX;\n\t\tdev->features = dev->hw_features | NETIF_F_LLTX;\n\t\tdev->vlan_features = dev->features &\n\t\t\t\t     ~(NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t       NETIF_F_HW_VLAN_STAG_TX);\n\n\t\tINIT_LIST_HEAD(&tun->disabled);\n\t\terr = tun_attach(tun, file, false);\n\t\tif (err < 0)\n\t\t\tgoto err_free_flow;\n\n\t\terr = register_netdevice(tun->dev);\n\t\tif (err < 0)\n\t\t\tgoto err_detach;\n\t}\n\n\tnetif_carrier_on(tun->dev);\n\n\ttun_debug(KERN_INFO, tun, \"tun_set_iff\\n\");\n\n\ttun->flags = (tun->flags & ~TUN_FEATURES) |\n\t\t(ifr->ifr_flags & TUN_FEATURES);\n\n\t/* Make sure persistent devices do not get stuck in\n\t * xoff state.\n\t */\n\tif (netif_running(tun->dev))\n\t\tnetif_tx_wake_all_queues(tun->dev);\n\n\tstrcpy(ifr->ifr_name, tun->dev->name);\n\treturn 0;\n\nerr_detach:\n\ttun_detach_all(dev);\n\t/* register_netdevice() already called tun_free_netdev() */\n\tgoto err_free_dev;\n\nerr_free_flow:\n\ttun_flow_uninit(tun);\n\tsecurity_tun_dev_free_security(tun->security);\nerr_free_stat:\n\tfree_percpu(tun->pcpu_stats);\nerr_free_dev:\n\tfree_netdev(dev);\n\treturn err;\n}",
                        "cve_id": "CVE-2018-7191"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int rtnl_getlink(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct net *tgt_net = net;\n\tstruct ifinfomsg *ifm;\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct net_device *dev = NULL;\n\tstruct sk_buff *nskb;\n\tint netnsid = -1;\n\tint err;\n\tu32 ext_filter_mask = 0;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IF_NETNSID]) {\n\t\tnetnsid = nla_get_s32(tb[IFLA_IF_NETNSID]);\n\t\ttgt_net = get_target_net(skb, netnsid);\n\t\tif (IS_ERR(tgt_net))\n\t\t\treturn PTR_ERR(tgt_net);\n\t}\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\n\tif (tb[IFLA_EXT_MASK])\n\t\text_filter_mask = nla_get_u32(tb[IFLA_EXT_MASK]);\n\n\terr = -EINVAL;\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(tgt_net, ifm->ifi_index);\n\telse if (tb[IFLA_IFNAME])\n\t\tdev = __dev_get_by_name(tgt_net, ifname);\n\telse\n\t\tgoto out;\n\n\terr = -ENODEV;\n\tif (dev == NULL)\n\t\tgoto out;\n\n\terr = -ENOBUFS;\n\tnskb = nlmsg_new(if_nlmsg_size(dev, ext_filter_mask), GFP_KERNEL);\n\tif (nskb == NULL)\n\t\tgoto out;\n\n\terr = rtnl_fill_ifinfo(nskb, dev, net,\n\t\t\t       RTM_NEWLINK, NETLINK_CB(skb).portid,\n\t\t\t       nlh->nlmsg_seq, 0, 0, ext_filter_mask,\n\t\t\t       0, NULL, netnsid);\n\tif (err < 0) {\n\t\t/* -EMSGSIZE implies BUG in if_nlmsg_size */\n\t\tWARN_ON(err == -EMSGSIZE);\n\t\tkfree_skb(nskb);\n\t} else\n\t\terr = rtnl_unicast(nskb, net, NETLINK_CB(skb).portid);\nout:\n\tif (netnsid >= 0)\n\t\tput_net(tgt_net);\n\n\treturn err;\n}",
                        "code_after_change": "static int rtnl_getlink(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct net *tgt_net = net;\n\tstruct ifinfomsg *ifm;\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct net_device *dev = NULL;\n\tstruct sk_buff *nskb;\n\tint netnsid = -1;\n\tint err;\n\tu32 ext_filter_mask = 0;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IF_NETNSID]) {\n\t\tnetnsid = nla_get_s32(tb[IFLA_IF_NETNSID]);\n\t\ttgt_net = get_target_net(NETLINK_CB(skb).sk, netnsid);\n\t\tif (IS_ERR(tgt_net))\n\t\t\treturn PTR_ERR(tgt_net);\n\t}\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\n\tif (tb[IFLA_EXT_MASK])\n\t\text_filter_mask = nla_get_u32(tb[IFLA_EXT_MASK]);\n\n\terr = -EINVAL;\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(tgt_net, ifm->ifi_index);\n\telse if (tb[IFLA_IFNAME])\n\t\tdev = __dev_get_by_name(tgt_net, ifname);\n\telse\n\t\tgoto out;\n\n\terr = -ENODEV;\n\tif (dev == NULL)\n\t\tgoto out;\n\n\terr = -ENOBUFS;\n\tnskb = nlmsg_new(if_nlmsg_size(dev, ext_filter_mask), GFP_KERNEL);\n\tif (nskb == NULL)\n\t\tgoto out;\n\n\terr = rtnl_fill_ifinfo(nskb, dev, net,\n\t\t\t       RTM_NEWLINK, NETLINK_CB(skb).portid,\n\t\t\t       nlh->nlmsg_seq, 0, 0, ext_filter_mask,\n\t\t\t       0, NULL, netnsid);\n\tif (err < 0) {\n\t\t/* -EMSGSIZE implies BUG in if_nlmsg_size */\n\t\tWARN_ON(err == -EMSGSIZE);\n\t\tkfree_skb(nskb);\n\t} else\n\t\terr = rtnl_unicast(nskb, net, NETLINK_CB(skb).portid);\nout:\n\tif (netnsid >= 0)\n\t\tput_net(tgt_net);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2018-14646"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1998,
            "cve_id": "CVE-2019-10207",
            "code_snippet": "static int bcm_open(struct hci_uart *hu)\n{\n\tstruct bcm_data *bcm;\n\tstruct list_head *p;\n\tint err;\n\n\tbt_dev_dbg(hu->hdev, \"hu %p\", hu);\n\n\tif (!hci_uart_has_flow_control(hu))\n\t\treturn -EOPNOTSUPP;\n\n\tbcm = kzalloc(sizeof(*bcm), GFP_KERNEL);\n\tif (!bcm)\n\t\treturn -ENOMEM;\n\n\tskb_queue_head_init(&bcm->txq);\n\n\thu->priv = bcm;\n\n\tmutex_lock(&bcm_device_lock);\n\n\tif (hu->serdev) {\n\t\tbcm->dev = serdev_device_get_drvdata(hu->serdev);\n\t\tgoto out;\n\t}\n\n\tif (!hu->tty->dev)\n\t\tgoto out;\n\n\tlist_for_each(p, &bcm_device_list) {\n\t\tstruct bcm_device *dev = list_entry(p, struct bcm_device, list);\n\n\t\t/* Retrieve saved bcm_device based on parent of the\n\t\t * platform device (saved during device probe) and\n\t\t * parent of tty device used by hci_uart\n\t\t */\n\t\tif (hu->tty->dev->parent == dev->dev->parent) {\n\t\t\tbcm->dev = dev;\n#ifdef CONFIG_PM\n\t\t\tdev->hu = hu;\n#endif\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\tif (bcm->dev) {\n\t\thu->init_speed = bcm->dev->init_speed;\n\t\thu->oper_speed = bcm->dev->oper_speed;\n\t\terr = bcm_gpio_set_power(bcm->dev, true);\n\t\tif (err)\n\t\t\tgoto err_unset_hu;\n\t}\n\n\tmutex_unlock(&bcm_device_lock);\n\treturn 0;\n\nerr_unset_hu:\n#ifdef CONFIG_PM\n\tif (!hu->serdev)\n\t\tbcm->dev->hu = NULL;\n#endif\n\tmutex_unlock(&bcm_device_lock);\n\thu->priv = NULL;\n\tkfree(bcm);\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int mrvl_open(struct hci_uart *hu)\n{\n\tstruct mrvl_data *mrvl;\n\tint ret;\n\n\tBT_DBG(\"hu %p\", hu);\n\n\tmrvl = kzalloc(sizeof(*mrvl), GFP_KERNEL);\n\tif (!mrvl)\n\t\treturn -ENOMEM;\n\n\tskb_queue_head_init(&mrvl->txq);\n\tskb_queue_head_init(&mrvl->rawq);\n\n\tset_bit(STATE_CHIP_VER_PENDING, &mrvl->flags);\n\n\thu->priv = mrvl;\n\n\tif (hu->serdev) {\n\t\tret = serdev_device_open(hu->serdev);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\nerr:\n\tkfree(mrvl);\n\n\treturn ret;\n}",
                        "code_after_change": "static int mrvl_open(struct hci_uart *hu)\n{\n\tstruct mrvl_data *mrvl;\n\tint ret;\n\n\tBT_DBG(\"hu %p\", hu);\n\n\tif (!hci_uart_has_flow_control(hu))\n\t\treturn -EOPNOTSUPP;\n\n\tmrvl = kzalloc(sizeof(*mrvl), GFP_KERNEL);\n\tif (!mrvl)\n\t\treturn -ENOMEM;\n\n\tskb_queue_head_init(&mrvl->txq);\n\tskb_queue_head_init(&mrvl->rawq);\n\n\tset_bit(STATE_CHIP_VER_PENDING, &mrvl->flags);\n\n\thu->priv = mrvl;\n\n\tif (hu->serdev) {\n\t\tret = serdev_device_open(hu->serdev);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\nerr:\n\tkfree(mrvl);\n\n\treturn ret;\n}",
                        "cve_id": "CVE-2019-10207"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 1999,
            "cve_id": "CVE-2019-10207",
            "code_snippet": "static int intel_open(struct hci_uart *hu)\n{\n\tstruct intel_data *intel;\n\n\tBT_DBG(\"hu %p\", hu);\n\n\tif (!hci_uart_has_flow_control(hu))\n\t\treturn -EOPNOTSUPP;\n\n\tintel = kzalloc(sizeof(*intel), GFP_KERNEL);\n\tif (!intel)\n\t\treturn -ENOMEM;\n\n\tskb_queue_head_init(&intel->txq);\n\tINIT_WORK(&intel->busy_work, intel_busy_work);\n\n\tintel->hu = hu;\n\n\thu->priv = intel;\n\n\tif (!intel_set_power(hu, true))\n\t\tset_bit(STATE_BOOTING, &intel->flags);\n\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int mrvl_open(struct hci_uart *hu)\n{\n\tstruct mrvl_data *mrvl;\n\tint ret;\n\n\tBT_DBG(\"hu %p\", hu);\n\n\tmrvl = kzalloc(sizeof(*mrvl), GFP_KERNEL);\n\tif (!mrvl)\n\t\treturn -ENOMEM;\n\n\tskb_queue_head_init(&mrvl->txq);\n\tskb_queue_head_init(&mrvl->rawq);\n\n\tset_bit(STATE_CHIP_VER_PENDING, &mrvl->flags);\n\n\thu->priv = mrvl;\n\n\tif (hu->serdev) {\n\t\tret = serdev_device_open(hu->serdev);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\nerr:\n\tkfree(mrvl);\n\n\treturn ret;\n}",
                        "code_after_change": "static int mrvl_open(struct hci_uart *hu)\n{\n\tstruct mrvl_data *mrvl;\n\tint ret;\n\n\tBT_DBG(\"hu %p\", hu);\n\n\tif (!hci_uart_has_flow_control(hu))\n\t\treturn -EOPNOTSUPP;\n\n\tmrvl = kzalloc(sizeof(*mrvl), GFP_KERNEL);\n\tif (!mrvl)\n\t\treturn -ENOMEM;\n\n\tskb_queue_head_init(&mrvl->txq);\n\tskb_queue_head_init(&mrvl->rawq);\n\n\tset_bit(STATE_CHIP_VER_PENDING, &mrvl->flags);\n\n\thu->priv = mrvl;\n\n\tif (hu->serdev) {\n\t\tret = serdev_device_open(hu->serdev);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\nerr:\n\tkfree(mrvl);\n\n\treturn ret;\n}",
                        "cve_id": "CVE-2019-10207"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2053,
            "cve_id": "CVE-2019-12818",
            "code_snippet": "int nfc_llcp_send_connect(struct nfc_llcp_sock *sock)\n{\n\tstruct nfc_llcp_local *local;\n\tstruct sk_buff *skb;\n\tu8 *service_name_tlv = NULL, service_name_tlv_length;\n\tu8 *miux_tlv = NULL, miux_tlv_length;\n\tu8 *rw_tlv = NULL, rw_tlv_length, rw;\n\tint err;\n\tu16 size = 0;\n\t__be16 miux;\n\n\tpr_debug(\"Sending CONNECT\\n\");\n\n\tlocal = sock->local;\n\tif (local == NULL)\n\t\treturn -ENODEV;\n\n\tif (sock->service_name != NULL) {\n\t\tservice_name_tlv = nfc_llcp_build_tlv(LLCP_TLV_SN,\n\t\t\t\t\t\t      sock->service_name,\n\t\t\t\t\t\t      sock->service_name_len,\n\t\t\t\t\t\t      &service_name_tlv_length);\n\t\tif (!service_name_tlv) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto error_tlv;\n\t\t}\n\t\tsize += service_name_tlv_length;\n\t}\n\n\t/* If the socket parameters are not set, use the local ones */\n\tmiux = be16_to_cpu(sock->miux) > LLCP_MAX_MIUX ?\n\t\tlocal->miux : sock->miux;\n\trw = sock->rw > LLCP_MAX_RW ? local->rw : sock->rw;\n\n\tmiux_tlv = nfc_llcp_build_tlv(LLCP_TLV_MIUX, (u8 *)&miux, 0,\n\t\t\t\t      &miux_tlv_length);\n\tif (!miux_tlv) {\n\t\terr = -ENOMEM;\n\t\tgoto error_tlv;\n\t}\n\tsize += miux_tlv_length;\n\n\trw_tlv = nfc_llcp_build_tlv(LLCP_TLV_RW, &rw, 0, &rw_tlv_length);\n\tif (!rw_tlv) {\n\t\terr = -ENOMEM;\n\t\tgoto error_tlv;\n\t}\n\tsize += rw_tlv_length;\n\n\tpr_debug(\"SKB size %d SN length %zu\\n\", size, sock->service_name_len);\n\n\tskb = llcp_allocate_pdu(sock, LLCP_PDU_CONNECT, size);\n\tif (skb == NULL) {\n\t\terr = -ENOMEM;\n\t\tgoto error_tlv;\n\t}\n\n\tllcp_add_tlv(skb, service_name_tlv, service_name_tlv_length);\n\tllcp_add_tlv(skb, miux_tlv, miux_tlv_length);\n\tllcp_add_tlv(skb, rw_tlv, rw_tlv_length);\n\n\tskb_queue_tail(&local->tx_queue, skb);\n\n\terr = 0;\n\nerror_tlv:\n\tif (err)\n\t\tpr_err(\"error %d\\n\", err);\n\n\tkfree(service_name_tlv);\n\tkfree(miux_tlv);\n\tkfree(rw_tlv);\n\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int nfc_llcp_send_cc(struct nfc_llcp_sock *sock)\n{\n\tstruct nfc_llcp_local *local;\n\tstruct sk_buff *skb;\n\tu8 *miux_tlv = NULL, miux_tlv_length;\n\tu8 *rw_tlv = NULL, rw_tlv_length, rw;\n\tint err;\n\tu16 size = 0;\n\t__be16 miux;\n\n\tpr_debug(\"Sending CC\\n\");\n\n\tlocal = sock->local;\n\tif (local == NULL)\n\t\treturn -ENODEV;\n\n\t/* If the socket parameters are not set, use the local ones */\n\tmiux = be16_to_cpu(sock->miux) > LLCP_MAX_MIUX ?\n\t\tlocal->miux : sock->miux;\n\trw = sock->rw > LLCP_MAX_RW ? local->rw : sock->rw;\n\n\tmiux_tlv = nfc_llcp_build_tlv(LLCP_TLV_MIUX, (u8 *)&miux, 0,\n\t\t\t\t      &miux_tlv_length);\n\tsize += miux_tlv_length;\n\n\trw_tlv = nfc_llcp_build_tlv(LLCP_TLV_RW, &rw, 0, &rw_tlv_length);\n\tsize += rw_tlv_length;\n\n\tskb = llcp_allocate_pdu(sock, LLCP_PDU_CC, size);\n\tif (skb == NULL) {\n\t\terr = -ENOMEM;\n\t\tgoto error_tlv;\n\t}\n\n\tllcp_add_tlv(skb, miux_tlv, miux_tlv_length);\n\tllcp_add_tlv(skb, rw_tlv, rw_tlv_length);\n\n\tskb_queue_tail(&local->tx_queue, skb);\n\n\terr = 0;\n\nerror_tlv:\n\tif (err)\n\t\tpr_err(\"error %d\\n\", err);\n\n\tkfree(miux_tlv);\n\tkfree(rw_tlv);\n\n\treturn err;\n}",
                        "code_after_change": "int nfc_llcp_send_cc(struct nfc_llcp_sock *sock)\n{\n\tstruct nfc_llcp_local *local;\n\tstruct sk_buff *skb;\n\tu8 *miux_tlv = NULL, miux_tlv_length;\n\tu8 *rw_tlv = NULL, rw_tlv_length, rw;\n\tint err;\n\tu16 size = 0;\n\t__be16 miux;\n\n\tpr_debug(\"Sending CC\\n\");\n\n\tlocal = sock->local;\n\tif (local == NULL)\n\t\treturn -ENODEV;\n\n\t/* If the socket parameters are not set, use the local ones */\n\tmiux = be16_to_cpu(sock->miux) > LLCP_MAX_MIUX ?\n\t\tlocal->miux : sock->miux;\n\trw = sock->rw > LLCP_MAX_RW ? local->rw : sock->rw;\n\n\tmiux_tlv = nfc_llcp_build_tlv(LLCP_TLV_MIUX, (u8 *)&miux, 0,\n\t\t\t\t      &miux_tlv_length);\n\tif (!miux_tlv) {\n\t\terr = -ENOMEM;\n\t\tgoto error_tlv;\n\t}\n\tsize += miux_tlv_length;\n\n\trw_tlv = nfc_llcp_build_tlv(LLCP_TLV_RW, &rw, 0, &rw_tlv_length);\n\tif (!rw_tlv) {\n\t\terr = -ENOMEM;\n\t\tgoto error_tlv;\n\t}\n\tsize += rw_tlv_length;\n\n\tskb = llcp_allocate_pdu(sock, LLCP_PDU_CC, size);\n\tif (skb == NULL) {\n\t\terr = -ENOMEM;\n\t\tgoto error_tlv;\n\t}\n\n\tllcp_add_tlv(skb, miux_tlv, miux_tlv_length);\n\tllcp_add_tlv(skb, rw_tlv, rw_tlv_length);\n\n\tskb_queue_tail(&local->tx_queue, skb);\n\n\terr = 0;\n\nerror_tlv:\n\tif (err)\n\t\tpr_err(\"error %d\\n\", err);\n\n\tkfree(miux_tlv);\n\tkfree(rw_tlv);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2019-12818"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2095,
            "cve_id": "CVE-2019-15098",
            "code_snippet": "static struct ath6kl_urb_context *\nath6kl_usb_alloc_urb_from_pipe(struct ath6kl_usb_pipe *pipe)\n{\n\tstruct ath6kl_urb_context *urb_context = NULL;\n\tunsigned long flags;\n\n\t/* bail if this pipe is not initialized */\n\tif (!pipe->ar_usb)\n\t\treturn NULL;\n\n\tspin_lock_irqsave(&pipe->ar_usb->cs_lock, flags);\n\tif (!list_empty(&pipe->urb_list_head)) {\n\t\turb_context =\n\t\t    list_first_entry(&pipe->urb_list_head,\n\t\t\t\t     struct ath6kl_urb_context, link);\n\t\tlist_del(&urb_context->link);\n\t\tpipe->urb_cnt--;\n\t}\n\tspin_unlock_irqrestore(&pipe->ar_usb->cs_lock, flags);\n\n\treturn urb_context;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static void ath6kl_usb_free_urb_to_pipe(struct ath6kl_usb_pipe *pipe,\n\t\t\t\t\tstruct ath6kl_urb_context *urb_context)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&pipe->ar_usb->cs_lock, flags);\n\tpipe->urb_cnt++;\n\n\tlist_add(&urb_context->link, &pipe->urb_list_head);\n\tspin_unlock_irqrestore(&pipe->ar_usb->cs_lock, flags);\n}",
                        "code_after_change": "static void ath6kl_usb_free_urb_to_pipe(struct ath6kl_usb_pipe *pipe,\n\t\t\t\t\tstruct ath6kl_urb_context *urb_context)\n{\n\tunsigned long flags;\n\n\t/* bail if this pipe is not initialized */\n\tif (!pipe->ar_usb)\n\t\treturn;\n\n\tspin_lock_irqsave(&pipe->ar_usb->cs_lock, flags);\n\tpipe->urb_cnt++;\n\n\tlist_add(&urb_context->link, &pipe->urb_list_head);\n\tspin_unlock_irqrestore(&pipe->ar_usb->cs_lock, flags);\n}",
                        "cve_id": "CVE-2019-15098"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2097,
            "cve_id": "CVE-2019-15099",
            "code_snippet": "static void ath10k_usb_free_urb_to_pipe(struct ath10k_usb_pipe *pipe,\n\t\t\t\t\tstruct ath10k_urb_context *urb_context)\n{\n\tunsigned long flags;\n\n\t/* bail if this pipe is not initialized */\n\tif (!pipe->ar_usb)\n\t\treturn;\n\n\tspin_lock_irqsave(&pipe->ar_usb->cs_lock, flags);\n\n\tpipe->urb_cnt++;\n\tlist_add(&urb_context->link, &pipe->urb_list_head);\n\n\tspin_unlock_irqrestore(&pipe->ar_usb->cs_lock, flags);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static struct ath10k_urb_context *\nath10k_usb_alloc_urb_from_pipe(struct ath10k_usb_pipe *pipe)\n{\n\tstruct ath10k_urb_context *urb_context = NULL;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&pipe->ar_usb->cs_lock, flags);\n\tif (!list_empty(&pipe->urb_list_head)) {\n\t\turb_context = list_first_entry(&pipe->urb_list_head,\n\t\t\t\t\t       struct ath10k_urb_context, link);\n\t\tlist_del(&urb_context->link);\n\t\tpipe->urb_cnt--;\n\t}\n\tspin_unlock_irqrestore(&pipe->ar_usb->cs_lock, flags);\n\n\treturn urb_context;\n}",
                        "code_after_change": "static struct ath10k_urb_context *\nath10k_usb_alloc_urb_from_pipe(struct ath10k_usb_pipe *pipe)\n{\n\tstruct ath10k_urb_context *urb_context = NULL;\n\tunsigned long flags;\n\n\t/* bail if this pipe is not initialized */\n\tif (!pipe->ar_usb)\n\t\treturn NULL;\n\n\tspin_lock_irqsave(&pipe->ar_usb->cs_lock, flags);\n\tif (!list_empty(&pipe->urb_list_head)) {\n\t\turb_context = list_first_entry(&pipe->urb_list_head,\n\t\t\t\t\t       struct ath10k_urb_context, link);\n\t\tlist_del(&urb_context->link);\n\t\tpipe->urb_cnt--;\n\t}\n\tspin_unlock_irqrestore(&pipe->ar_usb->cs_lock, flags);\n\n\treturn urb_context;\n}",
                        "cve_id": "CVE-2019-15099"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2116,
            "cve_id": "CVE-2019-15223",
            "code_snippet": "int line6_probe(struct usb_interface *interface,\n\t\tconst struct usb_device_id *id,\n\t\tconst char *driver_name,\n\t\tconst struct line6_properties *properties,\n\t\tint (*private_init)(struct usb_line6 *, const struct usb_device_id *id),\n\t\tsize_t data_size)\n{\n\tstruct usb_device *usbdev = interface_to_usbdev(interface);\n\tstruct snd_card *card;\n\tstruct usb_line6 *line6;\n\tint interface_number;\n\tint ret;\n\n\tif (WARN_ON(data_size < sizeof(*line6)))\n\t\treturn -EINVAL;\n\n\t/* we don't handle multiple configurations */\n\tif (usbdev->descriptor.bNumConfigurations != 1)\n\t\treturn -ENODEV;\n\n\tret = snd_card_new(&interface->dev,\n\t\t\t   SNDRV_DEFAULT_IDX1, SNDRV_DEFAULT_STR1,\n\t\t\t   THIS_MODULE, data_size, &card);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/* store basic data: */\n\tline6 = card->private_data;\n\tline6->card = card;\n\tline6->properties = properties;\n\tline6->usbdev = usbdev;\n\tline6->ifcdev = &interface->dev;\n\tINIT_DELAYED_WORK(&line6->startup_work, line6_startup_work);\n\n\tstrcpy(card->id, properties->id);\n\tstrcpy(card->driver, driver_name);\n\tstrcpy(card->shortname, properties->name);\n\tsprintf(card->longname, \"Line 6 %s at USB %s\", properties->name,\n\t\tdev_name(line6->ifcdev));\n\tcard->private_free = line6_destruct;\n\n\tusb_set_intfdata(interface, line6);\n\n\t/* increment reference counters: */\n\tusb_get_dev(usbdev);\n\n\t/* initialize device info: */\n\tdev_info(&interface->dev, \"Line 6 %s found\\n\", properties->name);\n\n\t/* query interface number */\n\tinterface_number = interface->cur_altsetting->desc.bInterfaceNumber;\n\n\t/* TODO reserves the bus bandwidth even without actual transfer */\n\tret = usb_set_interface(usbdev, interface_number,\n\t\t\t\tproperties->altsetting);\n\tif (ret < 0) {\n\t\tdev_err(&interface->dev, \"set_interface failed\\n\");\n\t\tgoto error;\n\t}\n\n\tline6_get_usb_properties(line6);\n\n\tif (properties->capabilities & LINE6_CAP_CONTROL) {\n\t\tret = line6_init_cap_control(line6);\n\t\tif (ret < 0)\n\t\t\tgoto error;\n\t}\n\n\t/* initialize device data based on device: */\n\tret = private_init(line6, id);\n\tif (ret < 0)\n\t\tgoto error;\n\n\t/* creation of additional special files should go here */\n\n\tdev_info(&interface->dev, \"Line 6 %s now attached\\n\",\n\t\t properties->name);\n\n\treturn 0;\n\n error:\n\t/* we can call disconnect callback here because no close-sync is\n\t * needed yet at this point\n\t */\n\tline6_disconnect(interface);\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "void line6_disconnect(struct usb_interface *interface)\n{\n\tstruct usb_line6 *line6 = usb_get_intfdata(interface);\n\tstruct usb_device *usbdev = interface_to_usbdev(interface);\n\n\tif (!line6)\n\t\treturn;\n\n\tif (WARN_ON(usbdev != line6->usbdev))\n\t\treturn;\n\n\tif (line6->urb_listen != NULL)\n\t\tline6_stop_listen(line6);\n\n\tsnd_card_disconnect(line6->card);\n\tif (line6->line6pcm)\n\t\tline6_pcm_disconnect(line6->line6pcm);\n\tif (line6->disconnect)\n\t\tline6->disconnect(line6);\n\n\tdev_info(&interface->dev, \"Line 6 %s now disconnected\\n\",\n\t\t line6->properties->name);\n\n\t/* make sure the device isn't destructed twice: */\n\tusb_set_intfdata(interface, NULL);\n\n\tsnd_card_free_when_closed(line6->card);\n}",
                        "code_after_change": "void line6_disconnect(struct usb_interface *interface)\n{\n\tstruct usb_line6 *line6 = usb_get_intfdata(interface);\n\tstruct usb_device *usbdev = interface_to_usbdev(interface);\n\n\tif (!line6)\n\t\treturn;\n\n\tif (WARN_ON(usbdev != line6->usbdev))\n\t\treturn;\n\n\tcancel_delayed_work(&line6->startup_work);\n\n\tif (line6->urb_listen != NULL)\n\t\tline6_stop_listen(line6);\n\n\tsnd_card_disconnect(line6->card);\n\tif (line6->line6pcm)\n\t\tline6_pcm_disconnect(line6->line6pcm);\n\tif (line6->disconnect)\n\t\tline6->disconnect(line6);\n\n\tdev_info(&interface->dev, \"Line 6 %s now disconnected\\n\",\n\t\t line6->properties->name);\n\n\t/* make sure the device isn't destructed twice: */\n\tusb_set_intfdata(interface, NULL);\n\n\tsnd_card_free_when_closed(line6->card);\n}",
                        "cve_id": "CVE-2019-15223"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2117,
            "cve_id": "CVE-2019-15223",
            "code_snippet": "static void line6_toneport_disconnect(struct usb_line6 *line6)\n{\n\tstruct usb_line6_toneport *toneport =\n\t\t(struct usb_line6_toneport *)line6;\n\n\tif (toneport_has_led(toneport))\n\t\ttoneport_remove_leds(toneport);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int toneport_init(struct usb_line6 *line6,\n\t\t\t const struct usb_device_id *id)\n{\n\tint err;\n\tstruct usb_line6_toneport *toneport =  (struct usb_line6_toneport *) line6;\n\n\ttoneport->type = id->driver_info;\n\tINIT_DELAYED_WORK(&toneport->pcm_work, toneport_start_pcm);\n\n\tline6->disconnect = line6_toneport_disconnect;\n\n\t/* initialize PCM subsystem: */\n\terr = line6_init_pcm(line6, &toneport_pcm_properties);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* register monitor control: */\n\terr = snd_ctl_add(line6->card,\n\t\t\t  snd_ctl_new1(&toneport_control_monitor,\n\t\t\t\t       line6->line6pcm));\n\tif (err < 0)\n\t\treturn err;\n\n\t/* register source select control: */\n\tif (toneport_has_source_select(toneport)) {\n\t\terr =\n\t\t    snd_ctl_add(line6->card,\n\t\t\t\tsnd_ctl_new1(&toneport_control_source,\n\t\t\t\t\t     line6->line6pcm));\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\tline6_read_serial_number(line6, &toneport->serial_number);\n\tline6_read_data(line6, 0x80c2, &toneport->firmware_version, 1);\n\n\tif (toneport_has_led(toneport)) {\n\t\terr = toneport_init_leds(toneport);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\terr = toneport_setup(toneport);\n\tif (err)\n\t\treturn err;\n\n\t/* register audio system: */\n\treturn snd_card_register(line6->card);\n}",
                        "code_after_change": "static int toneport_init(struct usb_line6 *line6,\n\t\t\t const struct usb_device_id *id)\n{\n\tint err;\n\tstruct usb_line6_toneport *toneport =  (struct usb_line6_toneport *) line6;\n\n\ttoneport->type = id->driver_info;\n\n\tline6->disconnect = line6_toneport_disconnect;\n\tline6->startup = toneport_startup;\n\n\t/* initialize PCM subsystem: */\n\terr = line6_init_pcm(line6, &toneport_pcm_properties);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* register monitor control: */\n\terr = snd_ctl_add(line6->card,\n\t\t\t  snd_ctl_new1(&toneport_control_monitor,\n\t\t\t\t       line6->line6pcm));\n\tif (err < 0)\n\t\treturn err;\n\n\t/* register source select control: */\n\tif (toneport_has_source_select(toneport)) {\n\t\terr =\n\t\t    snd_ctl_add(line6->card,\n\t\t\t\tsnd_ctl_new1(&toneport_control_source,\n\t\t\t\t\t     line6->line6pcm));\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\tline6_read_serial_number(line6, &toneport->serial_number);\n\tline6_read_data(line6, 0x80c2, &toneport->firmware_version, 1);\n\n\tif (toneport_has_led(toneport)) {\n\t\terr = toneport_init_leds(toneport);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\terr = toneport_setup(toneport);\n\tif (err)\n\t\treturn err;\n\n\t/* register audio system: */\n\treturn snd_card_register(line6->card);\n}",
                        "cve_id": "CVE-2019-15223"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int toneport_setup(struct usb_line6_toneport *toneport)\n{\n\tu32 *ticks;\n\tstruct usb_line6 *line6 = &toneport->line6;\n\tstruct usb_device *usbdev = line6->usbdev;\n\n\tticks = kmalloc(sizeof(*ticks), GFP_KERNEL);\n\tif (!ticks)\n\t\treturn -ENOMEM;\n\n\t/* sync time on device with host: */\n\t/* note: 32-bit timestamps overflow in year 2106 */\n\t*ticks = (u32)ktime_get_real_seconds();\n\tline6_write_data(line6, 0x80c6, ticks, 4);\n\tkfree(ticks);\n\n\t/* enable device: */\n\ttoneport_send_cmd(usbdev, 0x0301, 0x0000);\n\n\t/* initialize source select: */\n\tif (toneport_has_source_select(toneport))\n\t\ttoneport_send_cmd(usbdev,\n\t\t\t\t  toneport_source_info[toneport->source].code,\n\t\t\t\t  0x0000);\n\n\tif (toneport_has_led(toneport))\n\t\ttoneport_update_led(toneport);\n\n\tschedule_delayed_work(&toneport->pcm_work,\n\t\t\t      msecs_to_jiffies(TONEPORT_PCM_DELAY * 1000));\n\treturn 0;\n}",
                        "code_after_change": "static int toneport_setup(struct usb_line6_toneport *toneport)\n{\n\tu32 *ticks;\n\tstruct usb_line6 *line6 = &toneport->line6;\n\tstruct usb_device *usbdev = line6->usbdev;\n\n\tticks = kmalloc(sizeof(*ticks), GFP_KERNEL);\n\tif (!ticks)\n\t\treturn -ENOMEM;\n\n\t/* sync time on device with host: */\n\t/* note: 32-bit timestamps overflow in year 2106 */\n\t*ticks = (u32)ktime_get_real_seconds();\n\tline6_write_data(line6, 0x80c6, ticks, 4);\n\tkfree(ticks);\n\n\t/* enable device: */\n\ttoneport_send_cmd(usbdev, 0x0301, 0x0000);\n\n\t/* initialize source select: */\n\tif (toneport_has_source_select(toneport))\n\t\ttoneport_send_cmd(usbdev,\n\t\t\t\t  toneport_source_info[toneport->source].code,\n\t\t\t\t  0x0000);\n\n\tif (toneport_has_led(toneport))\n\t\ttoneport_update_led(toneport);\n\n\tschedule_delayed_work(&toneport->line6.startup_work,\n\t\t\t      msecs_to_jiffies(TONEPORT_PCM_DELAY * 1000));\n\treturn 0;\n}",
                        "cve_id": "CVE-2019-15223"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "void line6_disconnect(struct usb_interface *interface)\n{\n\tstruct usb_line6 *line6 = usb_get_intfdata(interface);\n\tstruct usb_device *usbdev = interface_to_usbdev(interface);\n\n\tif (!line6)\n\t\treturn;\n\n\tif (WARN_ON(usbdev != line6->usbdev))\n\t\treturn;\n\n\tif (line6->urb_listen != NULL)\n\t\tline6_stop_listen(line6);\n\n\tsnd_card_disconnect(line6->card);\n\tif (line6->line6pcm)\n\t\tline6_pcm_disconnect(line6->line6pcm);\n\tif (line6->disconnect)\n\t\tline6->disconnect(line6);\n\n\tdev_info(&interface->dev, \"Line 6 %s now disconnected\\n\",\n\t\t line6->properties->name);\n\n\t/* make sure the device isn't destructed twice: */\n\tusb_set_intfdata(interface, NULL);\n\n\tsnd_card_free_when_closed(line6->card);\n}",
                        "code_after_change": "void line6_disconnect(struct usb_interface *interface)\n{\n\tstruct usb_line6 *line6 = usb_get_intfdata(interface);\n\tstruct usb_device *usbdev = interface_to_usbdev(interface);\n\n\tif (!line6)\n\t\treturn;\n\n\tif (WARN_ON(usbdev != line6->usbdev))\n\t\treturn;\n\n\tcancel_delayed_work(&line6->startup_work);\n\n\tif (line6->urb_listen != NULL)\n\t\tline6_stop_listen(line6);\n\n\tsnd_card_disconnect(line6->card);\n\tif (line6->line6pcm)\n\t\tline6_pcm_disconnect(line6->line6pcm);\n\tif (line6->disconnect)\n\t\tline6->disconnect(line6);\n\n\tdev_info(&interface->dev, \"Line 6 %s now disconnected\\n\",\n\t\t line6->properties->name);\n\n\t/* make sure the device isn't destructed twice: */\n\tusb_set_intfdata(interface, NULL);\n\n\tsnd_card_free_when_closed(line6->card);\n}",
                        "cve_id": "CVE-2019-15223"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int line6_init_pcm(struct usb_line6 *line6,\n\t\t   struct line6_pcm_properties *properties)\n{\n\tint i, err;\n\tunsigned ep_read = line6->properties->ep_audio_r;\n\tunsigned ep_write = line6->properties->ep_audio_w;\n\tstruct snd_pcm *pcm;\n\tstruct snd_line6_pcm *line6pcm;\n\n\tif (!(line6->properties->capabilities & LINE6_CAP_PCM))\n\t\treturn 0;\t/* skip PCM initialization and report success */\n\n\terr = snd_line6_new_pcm(line6, &pcm);\n\tif (err < 0)\n\t\treturn err;\n\n\tline6pcm = kzalloc(sizeof(*line6pcm), GFP_KERNEL);\n\tif (!line6pcm)\n\t\treturn -ENOMEM;\n\n\tmutex_init(&line6pcm->state_mutex);\n\tline6pcm->pcm = pcm;\n\tline6pcm->properties = properties;\n\tline6pcm->volume_playback[0] = line6pcm->volume_playback[1] = 255;\n\tline6pcm->volume_monitor = 255;\n\tline6pcm->line6 = line6;\n\n\tline6pcm->max_packet_size_in =\n\t\tusb_maxpacket(line6->usbdev,\n\t\t\tusb_rcvisocpipe(line6->usbdev, ep_read), 0);\n\tline6pcm->max_packet_size_out =\n\t\tusb_maxpacket(line6->usbdev,\n\t\t\tusb_sndisocpipe(line6->usbdev, ep_write), 1);\n\n\tspin_lock_init(&line6pcm->out.lock);\n\tspin_lock_init(&line6pcm->in.lock);\n\tline6pcm->impulse_period = LINE6_IMPULSE_DEFAULT_PERIOD;\n\n\tline6->line6pcm = line6pcm;\n\n\tpcm->private_data = line6pcm;\n\tpcm->private_free = line6_cleanup_pcm;\n\n\terr = line6_create_audio_out_urbs(line6pcm);\n\tif (err < 0)\n\t\treturn err;\n\n\terr = line6_create_audio_in_urbs(line6pcm);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* mixer: */\n\tfor (i = 0; i < ARRAY_SIZE(line6_controls); i++) {\n\t\terr = snd_ctl_add(line6->card,\n\t\t\t\t  snd_ctl_new1(&line6_controls[i], line6pcm));\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}",
                        "code_after_change": "int line6_init_pcm(struct usb_line6 *line6,\n\t\t   struct line6_pcm_properties *properties)\n{\n\tint i, err;\n\tunsigned ep_read = line6->properties->ep_audio_r;\n\tunsigned ep_write = line6->properties->ep_audio_w;\n\tstruct snd_pcm *pcm;\n\tstruct snd_line6_pcm *line6pcm;\n\n\tif (!(line6->properties->capabilities & LINE6_CAP_PCM))\n\t\treturn 0;\t/* skip PCM initialization and report success */\n\n\terr = snd_line6_new_pcm(line6, &pcm);\n\tif (err < 0)\n\t\treturn err;\n\n\tline6pcm = kzalloc(sizeof(*line6pcm), GFP_KERNEL);\n\tif (!line6pcm)\n\t\treturn -ENOMEM;\n\n\tmutex_init(&line6pcm->state_mutex);\n\tline6pcm->pcm = pcm;\n\tline6pcm->properties = properties;\n\tline6pcm->volume_playback[0] = line6pcm->volume_playback[1] = 255;\n\tline6pcm->volume_monitor = 255;\n\tline6pcm->line6 = line6;\n\n\tline6pcm->max_packet_size_in =\n\t\tusb_maxpacket(line6->usbdev,\n\t\t\tusb_rcvisocpipe(line6->usbdev, ep_read), 0);\n\tline6pcm->max_packet_size_out =\n\t\tusb_maxpacket(line6->usbdev,\n\t\t\tusb_sndisocpipe(line6->usbdev, ep_write), 1);\n\tif (!line6pcm->max_packet_size_in || !line6pcm->max_packet_size_out) {\n\t\tdev_err(line6pcm->line6->ifcdev,\n\t\t\t\"cannot get proper max packet size\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tspin_lock_init(&line6pcm->out.lock);\n\tspin_lock_init(&line6pcm->in.lock);\n\tline6pcm->impulse_period = LINE6_IMPULSE_DEFAULT_PERIOD;\n\n\tline6->line6pcm = line6pcm;\n\n\tpcm->private_data = line6pcm;\n\tpcm->private_free = line6_cleanup_pcm;\n\n\terr = line6_create_audio_out_urbs(line6pcm);\n\tif (err < 0)\n\t\treturn err;\n\n\terr = line6_create_audio_in_urbs(line6pcm);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* mixer: */\n\tfor (i = 0; i < ARRAY_SIZE(line6_controls); i++) {\n\t\terr = snd_ctl_add(line6->card,\n\t\t\t\t  snd_ctl_new1(&line6_controls[i], line6pcm));\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2019-15221"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int\next4_xattr_check_entries(struct ext4_xattr_entry *entry, void *end,\n\t\t\t void *value_start)\n{\n\tstruct ext4_xattr_entry *e = entry;\n\n\t/* Find the end of the names list */\n\twhile (!IS_LAST_ENTRY(e)) {\n\t\tstruct ext4_xattr_entry *next = EXT4_XATTR_NEXT(e);\n\t\tif ((void *)next >= end)\n\t\t\treturn -EFSCORRUPTED;\n\t\te = next;\n\t}\n\n\t/* Check the values */\n\twhile (!IS_LAST_ENTRY(entry)) {\n\t\tif (entry->e_value_size != 0 &&\n\t\t    entry->e_value_inum == 0) {\n\t\t\tu16 offs = le16_to_cpu(entry->e_value_offs);\n\t\t\tu32 size = le32_to_cpu(entry->e_value_size);\n\t\t\tvoid *value;\n\n\t\t\t/*\n\t\t\t * The value cannot overlap the names, and the value\n\t\t\t * with padding cannot extend beyond 'end'.  Check both\n\t\t\t * the padded and unpadded sizes, since the size may\n\t\t\t * overflow to 0 when adding padding.\n\t\t\t */\n\t\t\tif (offs > end - value_start)\n\t\t\t\treturn -EFSCORRUPTED;\n\t\t\tvalue = value_start + offs;\n\t\t\tif (value < (void *)e + sizeof(u32) ||\n\t\t\t    size > end - value ||\n\t\t\t    EXT4_XATTR_SIZE(size) > end - value)\n\t\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\t\tentry = EXT4_XATTR_NEXT(entry);\n\t}\n\n\treturn 0;\n}",
                        "code_after_change": "static int\next4_xattr_check_entries(struct ext4_xattr_entry *entry, void *end,\n\t\t\t void *value_start)\n{\n\tstruct ext4_xattr_entry *e = entry;\n\n\t/* Find the end of the names list */\n\twhile (!IS_LAST_ENTRY(e)) {\n\t\tstruct ext4_xattr_entry *next = EXT4_XATTR_NEXT(e);\n\t\tif ((void *)next >= end)\n\t\t\treturn -EFSCORRUPTED;\n\t\te = next;\n\t}\n\n\t/* Check the values */\n\twhile (!IS_LAST_ENTRY(entry)) {\n\t\tu32 size = le32_to_cpu(entry->e_value_size);\n\n\t\tif (size > INT_MAX)\n\t\t\treturn -EFSCORRUPTED;\n\n\t\tif (size != 0 && entry->e_value_inum == 0) {\n\t\t\tu16 offs = le16_to_cpu(entry->e_value_offs);\n\t\t\tvoid *value;\n\n\t\t\t/*\n\t\t\t * The value cannot overlap the names, and the value\n\t\t\t * with padding cannot extend beyond 'end'.  Check both\n\t\t\t * the padded and unpadded sizes, since the size may\n\t\t\t * overflow to 0 when adding padding.\n\t\t\t */\n\t\t\tif (offs > end - value_start)\n\t\t\t\treturn -EFSCORRUPTED;\n\t\t\tvalue = value_start + offs;\n\t\t\tif (value < (void *)e + sizeof(u32) ||\n\t\t\t    size > end - value ||\n\t\t\t    EXT4_XATTR_SIZE(size) > end - value)\n\t\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\t\tentry = EXT4_XATTR_NEXT(entry);\n\t}\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2018-1095"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 2137,
            "cve_id": "CVE-2019-15922",
            "code_snippet": "static void __exit pf_exit(void)\n{\n\tstruct pf_unit *pf;\n\tint unit;\n\tunregister_blkdev(major, name);\n\tfor (pf = units, unit = 0; unit < PF_UNITS; pf++, unit++) {\n\t\tif (!pf->disk)\n\t\t\tcontinue;\n\n\t\tif (pf->present)\n\t\t\tdel_gendisk(pf->disk);\n\n\t\tblk_cleanup_queue(pf->disk->queue);\n\t\tblk_mq_free_tag_set(&pf->tag_set);\n\t\tput_disk(pf->disk);\n\n\t\tif (pf->present)\n\t\t\tpi_release(pf->pi);\n\t}\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int pf_detect(void)\n{\n\tstruct pf_unit *pf = units;\n\tint k, unit;\n\n\tprintk(\"%s: %s version %s, major %d, cluster %d, nice %d\\n\",\n\t       name, name, PF_VERSION, major, cluster, nice);\n\n\tpar_drv = pi_register_driver(name);\n\tif (!par_drv) {\n\t\tpr_err(\"failed to register %s driver\\n\", name);\n\t\treturn -1;\n\t}\n\tk = 0;\n\tif (pf_drive_count == 0) {\n\t\tif (pi_init(pf->pi, 1, -1, -1, -1, -1, -1, pf_scratch, PI_PF,\n\t\t\t    verbose, pf->name)) {\n\t\t\tif (!pf_probe(pf) && pf->disk) {\n\t\t\t\tpf->present = 1;\n\t\t\t\tk++;\n\t\t\t} else\n\t\t\t\tpi_release(pf->pi);\n\t\t}\n\n\t} else\n\t\tfor (unit = 0; unit < PF_UNITS; unit++, pf++) {\n\t\t\tint *conf = *drives[unit];\n\t\t\tif (!conf[D_PRT])\n\t\t\t\tcontinue;\n\t\t\tif (pi_init(pf->pi, 0, conf[D_PRT], conf[D_MOD],\n\t\t\t\t    conf[D_UNI], conf[D_PRO], conf[D_DLY],\n\t\t\t\t    pf_scratch, PI_PF, verbose, pf->name)) {\n\t\t\t\tif (pf->disk && !pf_probe(pf)) {\n\t\t\t\t\tpf->present = 1;\n\t\t\t\t\tk++;\n\t\t\t\t} else\n\t\t\t\t\tpi_release(pf->pi);\n\t\t\t}\n\t\t}\n\tif (k)\n\t\treturn 0;\n\n\tprintk(\"%s: No ATAPI disk detected\\n\", name);\n\tfor (pf = units, unit = 0; unit < PF_UNITS; pf++, unit++) {\n\t\tblk_cleanup_queue(pf->disk->queue);\n\t\tpf->disk->queue = NULL;\n\t\tblk_mq_free_tag_set(&pf->tag_set);\n\t\tput_disk(pf->disk);\n\t}\n\tpi_unregister_driver(par_drv);\n\treturn -1;\n}",
                        "code_after_change": "static int pf_detect(void)\n{\n\tstruct pf_unit *pf = units;\n\tint k, unit;\n\n\tprintk(\"%s: %s version %s, major %d, cluster %d, nice %d\\n\",\n\t       name, name, PF_VERSION, major, cluster, nice);\n\n\tpar_drv = pi_register_driver(name);\n\tif (!par_drv) {\n\t\tpr_err(\"failed to register %s driver\\n\", name);\n\t\treturn -1;\n\t}\n\tk = 0;\n\tif (pf_drive_count == 0) {\n\t\tif (pi_init(pf->pi, 1, -1, -1, -1, -1, -1, pf_scratch, PI_PF,\n\t\t\t    verbose, pf->name)) {\n\t\t\tif (!pf_probe(pf) && pf->disk) {\n\t\t\t\tpf->present = 1;\n\t\t\t\tk++;\n\t\t\t} else\n\t\t\t\tpi_release(pf->pi);\n\t\t}\n\n\t} else\n\t\tfor (unit = 0; unit < PF_UNITS; unit++, pf++) {\n\t\t\tint *conf = *drives[unit];\n\t\t\tif (!conf[D_PRT])\n\t\t\t\tcontinue;\n\t\t\tif (pi_init(pf->pi, 0, conf[D_PRT], conf[D_MOD],\n\t\t\t\t    conf[D_UNI], conf[D_PRO], conf[D_DLY],\n\t\t\t\t    pf_scratch, PI_PF, verbose, pf->name)) {\n\t\t\t\tif (pf->disk && !pf_probe(pf)) {\n\t\t\t\t\tpf->present = 1;\n\t\t\t\t\tk++;\n\t\t\t\t} else\n\t\t\t\t\tpi_release(pf->pi);\n\t\t\t}\n\t\t}\n\tif (k)\n\t\treturn 0;\n\n\tprintk(\"%s: No ATAPI disk detected\\n\", name);\n\tfor (pf = units, unit = 0; unit < PF_UNITS; pf++, unit++) {\n\t\tif (!pf->disk)\n\t\t\tcontinue;\n\t\tblk_cleanup_queue(pf->disk->queue);\n\t\tpf->disk->queue = NULL;\n\t\tblk_mq_free_tag_set(&pf->tag_set);\n\t\tput_disk(pf->disk);\n\t}\n\tpi_unregister_driver(par_drv);\n\treturn -1;\n}",
                        "cve_id": "CVE-2019-15922"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2140,
            "cve_id": "CVE-2019-15923",
            "code_snippet": "static void pcd_init_units(void)\n{\n\tstruct pcd_unit *cd;\n\tint unit;\n\n\tpcd_drive_count = 0;\n\tfor (unit = 0, cd = pcd; unit < PCD_UNITS; unit++, cd++) {\n\t\tstruct gendisk *disk = alloc_disk(1);\n\n\t\tif (!disk)\n\t\t\tcontinue;\n\n\t\tdisk->queue = blk_mq_init_sq_queue(&cd->tag_set, &pcd_mq_ops,\n\t\t\t\t\t\t   1, BLK_MQ_F_SHOULD_MERGE);\n\t\tif (IS_ERR(disk->queue)) {\n\t\t\tput_disk(disk);\n\t\t\tdisk->queue = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\tINIT_LIST_HEAD(&cd->rq_list);\n\t\tdisk->queue->queuedata = cd;\n\t\tblk_queue_bounce_limit(disk->queue, BLK_BOUNCE_HIGH);\n\t\tcd->disk = disk;\n\t\tcd->pi = &cd->pia;\n\t\tcd->present = 0;\n\t\tcd->last_sense = 0;\n\t\tcd->changed = 1;\n\t\tcd->drive = (*drives[unit])[D_SLV];\n\t\tif ((*drives[unit])[D_PRT])\n\t\t\tpcd_drive_count++;\n\n\t\tcd->name = &cd->info.name[0];\n\t\tsnprintf(cd->name, sizeof(cd->info.name), \"%s%d\", name, unit);\n\t\tcd->info.ops = &pcd_dops;\n\t\tcd->info.handle = cd;\n\t\tcd->info.speed = 0;\n\t\tcd->info.capacity = 1;\n\t\tcd->info.mask = 0;\n\t\tdisk->major = major;\n\t\tdisk->first_minor = unit;\n\t\tstrcpy(disk->disk_name, cd->name);\t/* umm... */\n\t\tdisk->fops = &pcd_bdops;\n\t\tdisk->flags = GENHD_FL_BLOCK_EVENTS_ON_EXCL_WRITE;\n\t}\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int pcd_detect(void)\n{\n\tchar id[18];\n\tint k, unit;\n\tstruct pcd_unit *cd;\n\n\tprintk(\"%s: %s version %s, major %d, nice %d\\n\",\n\t       name, name, PCD_VERSION, major, nice);\n\n\tpar_drv = pi_register_driver(name);\n\tif (!par_drv) {\n\t\tpr_err(\"failed to register %s driver\\n\", name);\n\t\treturn -1;\n\t}\n\n\tk = 0;\n\tif (pcd_drive_count == 0) { /* nothing spec'd - so autoprobe for 1 */\n\t\tcd = pcd;\n\t\tif (pi_init(cd->pi, 1, -1, -1, -1, -1, -1, pcd_buffer,\n\t\t\t    PI_PCD, verbose, cd->name)) {\n\t\t\tif (!pcd_probe(cd, -1, id) && cd->disk) {\n\t\t\t\tcd->present = 1;\n\t\t\t\tk++;\n\t\t\t} else\n\t\t\t\tpi_release(cd->pi);\n\t\t}\n\t} else {\n\t\tfor (unit = 0, cd = pcd; unit < PCD_UNITS; unit++, cd++) {\n\t\t\tint *conf = *drives[unit];\n\t\t\tif (!conf[D_PRT])\n\t\t\t\tcontinue;\n\t\t\tif (!pi_init(cd->pi, 0, conf[D_PRT], conf[D_MOD],\n\t\t\t\t     conf[D_UNI], conf[D_PRO], conf[D_DLY],\n\t\t\t\t     pcd_buffer, PI_PCD, verbose, cd->name)) \n\t\t\t\tcontinue;\n\t\t\tif (!pcd_probe(cd, conf[D_SLV], id) && cd->disk) {\n\t\t\t\tcd->present = 1;\n\t\t\t\tk++;\n\t\t\t} else\n\t\t\t\tpi_release(cd->pi);\n\t\t}\n\t}\n\tif (k)\n\t\treturn 0;\n\n\tprintk(\"%s: No CD-ROM drive found\\n\", name);\n\tfor (unit = 0, cd = pcd; unit < PCD_UNITS; unit++, cd++) {\n\t\tblk_cleanup_queue(cd->disk->queue);\n\t\tcd->disk->queue = NULL;\n\t\tblk_mq_free_tag_set(&cd->tag_set);\n\t\tput_disk(cd->disk);\n\t}\n\tpi_unregister_driver(par_drv);\n\treturn -1;\n}",
                        "code_after_change": "static int pcd_detect(void)\n{\n\tchar id[18];\n\tint k, unit;\n\tstruct pcd_unit *cd;\n\n\tprintk(\"%s: %s version %s, major %d, nice %d\\n\",\n\t       name, name, PCD_VERSION, major, nice);\n\n\tpar_drv = pi_register_driver(name);\n\tif (!par_drv) {\n\t\tpr_err(\"failed to register %s driver\\n\", name);\n\t\treturn -1;\n\t}\n\n\tk = 0;\n\tif (pcd_drive_count == 0) { /* nothing spec'd - so autoprobe for 1 */\n\t\tcd = pcd;\n\t\tif (pi_init(cd->pi, 1, -1, -1, -1, -1, -1, pcd_buffer,\n\t\t\t    PI_PCD, verbose, cd->name)) {\n\t\t\tif (!pcd_probe(cd, -1, id) && cd->disk) {\n\t\t\t\tcd->present = 1;\n\t\t\t\tk++;\n\t\t\t} else\n\t\t\t\tpi_release(cd->pi);\n\t\t}\n\t} else {\n\t\tfor (unit = 0, cd = pcd; unit < PCD_UNITS; unit++, cd++) {\n\t\t\tint *conf = *drives[unit];\n\t\t\tif (!conf[D_PRT])\n\t\t\t\tcontinue;\n\t\t\tif (!pi_init(cd->pi, 0, conf[D_PRT], conf[D_MOD],\n\t\t\t\t     conf[D_UNI], conf[D_PRO], conf[D_DLY],\n\t\t\t\t     pcd_buffer, PI_PCD, verbose, cd->name)) \n\t\t\t\tcontinue;\n\t\t\tif (!pcd_probe(cd, conf[D_SLV], id) && cd->disk) {\n\t\t\t\tcd->present = 1;\n\t\t\t\tk++;\n\t\t\t} else\n\t\t\t\tpi_release(cd->pi);\n\t\t}\n\t}\n\tif (k)\n\t\treturn 0;\n\n\tprintk(\"%s: No CD-ROM drive found\\n\", name);\n\tfor (unit = 0, cd = pcd; unit < PCD_UNITS; unit++, cd++) {\n\t\tif (!cd->disk)\n\t\t\tcontinue;\n\t\tblk_cleanup_queue(cd->disk->queue);\n\t\tcd->disk->queue = NULL;\n\t\tblk_mq_free_tag_set(&cd->tag_set);\n\t\tput_disk(cd->disk);\n\t}\n\tpi_unregister_driver(par_drv);\n\treturn -1;\n}",
                        "cve_id": "CVE-2019-15923"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2216,
            "cve_id": "CVE-2019-18885",
            "code_snippet": "static noinline int btrfs_ioctl_resize(struct file *file,\n\t\t\t\t\tvoid __user *arg)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);\n\tu64 new_size;\n\tu64 old_size;\n\tu64 devid = 1;\n\tstruct btrfs_root *root = BTRFS_I(inode)->root;\n\tstruct btrfs_ioctl_vol_args *vol_args;\n\tstruct btrfs_trans_handle *trans;\n\tstruct btrfs_device *device = NULL;\n\tchar *sizestr;\n\tchar *retptr;\n\tchar *devstr = NULL;\n\tint ret = 0;\n\tint mod = 0;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tret = mnt_want_write_file(file);\n\tif (ret)\n\t\treturn ret;\n\n\tif (test_and_set_bit(BTRFS_FS_EXCL_OP, &fs_info->flags)) {\n\t\tmnt_drop_write_file(file);\n\t\treturn BTRFS_ERROR_DEV_EXCL_RUN_IN_PROGRESS;\n\t}\n\n\tvol_args = memdup_user(arg, sizeof(*vol_args));\n\tif (IS_ERR(vol_args)) {\n\t\tret = PTR_ERR(vol_args);\n\t\tgoto out;\n\t}\n\n\tvol_args->name[BTRFS_PATH_NAME_MAX] = '\\0';\n\n\tsizestr = vol_args->name;\n\tdevstr = strchr(sizestr, ':');\n\tif (devstr) {\n\t\tsizestr = devstr + 1;\n\t\t*devstr = '\\0';\n\t\tdevstr = vol_args->name;\n\t\tret = kstrtoull(devstr, 10, &devid);\n\t\tif (ret)\n\t\t\tgoto out_free;\n\t\tif (!devid) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tbtrfs_info(fs_info, \"resizing devid %llu\", devid);\n\t}\n\n\tdevice = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL, true);\n\tif (!device) {\n\t\tbtrfs_info(fs_info, \"resizer unable to find device %llu\",\n\t\t\t   devid);\n\t\tret = -ENODEV;\n\t\tgoto out_free;\n\t}\n\n\tif (!test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tbtrfs_info(fs_info,\n\t\t\t   \"resizer unable to apply on readonly device %llu\",\n\t\t       devid);\n\t\tret = -EPERM;\n\t\tgoto out_free;\n\t}\n\n\tif (!strcmp(sizestr, \"max\"))\n\t\tnew_size = device->bdev->bd_inode->i_size;\n\telse {\n\t\tif (sizestr[0] == '-') {\n\t\t\tmod = -1;\n\t\t\tsizestr++;\n\t\t} else if (sizestr[0] == '+') {\n\t\t\tmod = 1;\n\t\t\tsizestr++;\n\t\t}\n\t\tnew_size = memparse(sizestr, &retptr);\n\t\tif (*retptr != '\\0' || new_size == 0) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = -EPERM;\n\t\tgoto out_free;\n\t}\n\n\told_size = btrfs_device_get_total_bytes(device);\n\n\tif (mod < 0) {\n\t\tif (new_size > old_size) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tnew_size = old_size - new_size;\n\t} else if (mod > 0) {\n\t\tif (new_size > ULLONG_MAX - old_size) {\n\t\t\tret = -ERANGE;\n\t\t\tgoto out_free;\n\t\t}\n\t\tnew_size = old_size + new_size;\n\t}\n\n\tif (new_size < SZ_256M) {\n\t\tret = -EINVAL;\n\t\tgoto out_free;\n\t}\n\tif (new_size > device->bdev->bd_inode->i_size) {\n\t\tret = -EFBIG;\n\t\tgoto out_free;\n\t}\n\n\tnew_size = round_down(new_size, fs_info->sectorsize);\n\n\tbtrfs_info_in_rcu(fs_info, \"new size for %s is %llu\",\n\t\t\t  rcu_str_deref(device->name), new_size);\n\n\tif (new_size > old_size) {\n\t\ttrans = btrfs_start_transaction(root, 0);\n\t\tif (IS_ERR(trans)) {\n\t\t\tret = PTR_ERR(trans);\n\t\t\tgoto out_free;\n\t\t}\n\t\tret = btrfs_grow_device(trans, device, new_size);\n\t\tbtrfs_commit_transaction(trans);\n\t} else if (new_size < old_size) {\n\t\tret = btrfs_shrink_device(device, new_size);\n\t} /* equal, nothing need to do */\n\nout_free:\n\tkfree(vol_args);\nout:\n\tclear_bit(BTRFS_FS_EXCL_OP, &fs_info->flags);\n\tmnt_drop_write_file(file);\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int btrfs_rm_device(struct btrfs_fs_info *fs_info, const char *device_path,\n\t\t    u64 devid)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *cur_devices;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 num_devices;\n\tint ret = 0;\n\n\tmutex_lock(&uuid_mutex);\n\n\tnum_devices = btrfs_num_devices(fs_info);\n\n\tret = btrfs_check_raid_min_devices(fs_info, num_devices - 1);\n\tif (ret)\n\t\tgoto out;\n\n\tdevice = btrfs_find_device_by_devspec(fs_info, devid, device_path);\n\n\tif (IS_ERR(device)) {\n\t\tif (PTR_ERR(device) == -ENOENT &&\n\t\t    strcmp(device_path, \"missing\") == 0)\n\t\t\tret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND;\n\t\telse\n\t\t\tret = PTR_ERR(device);\n\t\tgoto out;\n\t}\n\n\tif (btrfs_pinned_by_swapfile(fs_info, device)) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t  \"cannot remove device %s (devid %llu) due to active swapfile\",\n\t\t\t\t  rcu_str_deref(device->name), device->devid);\n\t\tret = -ETXTBSY;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = BTRFS_ERROR_DEV_TGT_REPLACE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    fs_info->fs_devices->rw_devices == 1) {\n\t\tret = BTRFS_ERROR_DEV_ONLY_WRITABLE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tdevice->fs_devices->rw_devices--;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\n\tmutex_unlock(&uuid_mutex);\n\tret = btrfs_shrink_device(device, 0);\n\tif (!ret)\n\t\tbtrfs_reada_remove_dev(device);\n\tmutex_lock(&uuid_mutex);\n\tif (ret)\n\t\tgoto error_undo;\n\n\t/*\n\t * TODO: the superblock still includes this device in its num_devices\n\t * counter although write_all_supers() is not locked out. This\n\t * could give a filesystem state which requires a degraded mount.\n\t */\n\tret = btrfs_rm_dev_item(device);\n\tif (ret)\n\t\tgoto error_undo;\n\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tbtrfs_scrub_cancel_dev(device);\n\n\t/*\n\t * the device list mutex makes sure that we don't change\n\t * the device list while someone else is writing out all\n\t * the device supers. Whoever is writing all supers, should\n\t * lock the device list mutex before getting the number of\n\t * devices in the super block (super_copy). Conversely,\n\t * whoever updates the number of devices in the super block\n\t * (super_copy) should hold the device list mutex.\n\t */\n\n\t/*\n\t * In normal cases the cur_devices == fs_devices. But in case\n\t * of deleting a seed device, the cur_devices should point to\n\t * its own fs_devices listed under the fs_devices->seed.\n\t */\n\tcur_devices = device->fs_devices;\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_del_rcu(&device->dev_list);\n\n\tcur_devices->num_devices--;\n\tcur_devices->total_devices--;\n\t/* Update total_devices of the parent fs_devices if it's seed */\n\tif (cur_devices != fs_devices)\n\t\tfs_devices->total_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tcur_devices->missing_devices--;\n\n\tbtrfs_assign_next_active_device(device, NULL);\n\n\tif (device->bdev) {\n\t\tcur_devices->open_devices--;\n\t\t/* remove sysfs entry */\n\t\tbtrfs_sysfs_remove_device(device);\n\t}\n\n\tnum_devices = btrfs_super_num_devices(fs_info->super_copy) - 1;\n\tbtrfs_set_super_num_devices(fs_info->super_copy, num_devices);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * at this point, the device is zero sized and detached from\n\t * the devices list.  All that's left is to zero out the old\n\t * supers and free the device.\n\t */\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\tbtrfs_scratch_superblocks(fs_info, device->bdev,\n\t\t\t\t\t  device->name->str);\n\n\tbtrfs_close_bdev(device);\n\tsynchronize_rcu();\n\tbtrfs_free_device(device);\n\n\tif (cur_devices->open_devices == 0) {\n\t\tlist_del_init(&cur_devices->seed_list);\n\t\tclose_fs_devices(cur_devices);\n\t\tfree_fs_devices(cur_devices);\n\t}\n\nout:\n\tmutex_unlock(&uuid_mutex);\n\treturn ret;\n\nerror_undo:\n\tbtrfs_reada_undo_remove_dev(device);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_add(&device->dev_alloc_list,\n\t\t\t &fs_devices->alloc_list);\n\t\tdevice->fs_devices->rw_devices++;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\tgoto out;\n}",
                        "code_after_change": "int btrfs_rm_device(struct btrfs_fs_info *fs_info, const char *device_path,\n\t\t    u64 devid)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *cur_devices;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 num_devices;\n\tint ret = 0;\n\n\tmutex_lock(&uuid_mutex);\n\n\tnum_devices = btrfs_num_devices(fs_info);\n\n\tret = btrfs_check_raid_min_devices(fs_info, num_devices - 1);\n\tif (ret)\n\t\tgoto out;\n\n\tdevice = btrfs_find_device_by_devspec(fs_info, devid, device_path);\n\n\tif (IS_ERR(device)) {\n\t\tif (PTR_ERR(device) == -ENOENT &&\n\t\t    device_path && strcmp(device_path, \"missing\") == 0)\n\t\t\tret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND;\n\t\telse\n\t\t\tret = PTR_ERR(device);\n\t\tgoto out;\n\t}\n\n\tif (btrfs_pinned_by_swapfile(fs_info, device)) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t  \"cannot remove device %s (devid %llu) due to active swapfile\",\n\t\t\t\t  rcu_str_deref(device->name), device->devid);\n\t\tret = -ETXTBSY;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = BTRFS_ERROR_DEV_TGT_REPLACE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    fs_info->fs_devices->rw_devices == 1) {\n\t\tret = BTRFS_ERROR_DEV_ONLY_WRITABLE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tdevice->fs_devices->rw_devices--;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\n\tmutex_unlock(&uuid_mutex);\n\tret = btrfs_shrink_device(device, 0);\n\tif (!ret)\n\t\tbtrfs_reada_remove_dev(device);\n\tmutex_lock(&uuid_mutex);\n\tif (ret)\n\t\tgoto error_undo;\n\n\t/*\n\t * TODO: the superblock still includes this device in its num_devices\n\t * counter although write_all_supers() is not locked out. This\n\t * could give a filesystem state which requires a degraded mount.\n\t */\n\tret = btrfs_rm_dev_item(device);\n\tif (ret)\n\t\tgoto error_undo;\n\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tbtrfs_scrub_cancel_dev(device);\n\n\t/*\n\t * the device list mutex makes sure that we don't change\n\t * the device list while someone else is writing out all\n\t * the device supers. Whoever is writing all supers, should\n\t * lock the device list mutex before getting the number of\n\t * devices in the super block (super_copy). Conversely,\n\t * whoever updates the number of devices in the super block\n\t * (super_copy) should hold the device list mutex.\n\t */\n\n\t/*\n\t * In normal cases the cur_devices == fs_devices. But in case\n\t * of deleting a seed device, the cur_devices should point to\n\t * its own fs_devices listed under the fs_devices->seed.\n\t */\n\tcur_devices = device->fs_devices;\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_del_rcu(&device->dev_list);\n\n\tcur_devices->num_devices--;\n\tcur_devices->total_devices--;\n\t/* Update total_devices of the parent fs_devices if it's seed */\n\tif (cur_devices != fs_devices)\n\t\tfs_devices->total_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tcur_devices->missing_devices--;\n\n\tbtrfs_assign_next_active_device(device, NULL);\n\n\tif (device->bdev) {\n\t\tcur_devices->open_devices--;\n\t\t/* remove sysfs entry */\n\t\tbtrfs_sysfs_remove_device(device);\n\t}\n\n\tnum_devices = btrfs_super_num_devices(fs_info->super_copy) - 1;\n\tbtrfs_set_super_num_devices(fs_info->super_copy, num_devices);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * at this point, the device is zero sized and detached from\n\t * the devices list.  All that's left is to zero out the old\n\t * supers and free the device.\n\t */\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\tbtrfs_scratch_superblocks(fs_info, device->bdev,\n\t\t\t\t\t  device->name->str);\n\n\tbtrfs_close_bdev(device);\n\tsynchronize_rcu();\n\tbtrfs_free_device(device);\n\n\tif (cur_devices->open_devices == 0) {\n\t\tlist_del_init(&cur_devices->seed_list);\n\t\tclose_fs_devices(cur_devices);\n\t\tfree_fs_devices(cur_devices);\n\t}\n\nout:\n\tmutex_unlock(&uuid_mutex);\n\treturn ret;\n\nerror_undo:\n\tbtrfs_reada_undo_remove_dev(device);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_add(&device->dev_alloc_list,\n\t\t\t &fs_devices->alloc_list);\n\t\tdevice->fs_devices->rw_devices++;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\tgoto out;\n}",
                        "cve_id": "CVE-2021-3739"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2217,
            "cve_id": "CVE-2019-18885",
            "code_snippet": "static long btrfs_ioctl_dev_info(struct btrfs_fs_info *fs_info,\n\t\t\t\t void __user *arg)\n{\n\tstruct btrfs_ioctl_dev_info_args *di_args;\n\tstruct btrfs_device *dev;\n\tint ret = 0;\n\tchar *s_uuid = NULL;\n\n\tdi_args = memdup_user(arg, sizeof(*di_args));\n\tif (IS_ERR(di_args))\n\t\treturn PTR_ERR(di_args);\n\n\tif (!btrfs_is_empty_uuid(di_args->uuid))\n\t\ts_uuid = di_args->uuid;\n\n\trcu_read_lock();\n\tdev = btrfs_find_device(fs_info->fs_devices, di_args->devid, s_uuid,\n\t\t\t\tNULL, true);\n\n\tif (!dev) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tdi_args->devid = dev->devid;\n\tdi_args->bytes_used = btrfs_device_get_bytes_used(dev);\n\tdi_args->total_bytes = btrfs_device_get_total_bytes(dev);\n\tmemcpy(di_args->uuid, dev->uuid, sizeof(di_args->uuid));\n\tif (dev->name) {\n\t\tstrncpy(di_args->path, rcu_str_deref(dev->name),\n\t\t\t\tsizeof(di_args->path) - 1);\n\t\tdi_args->path[sizeof(di_args->path) - 1] = 0;\n\t} else {\n\t\tdi_args->path[0] = '\\0';\n\t}\n\nout:\n\trcu_read_unlock();\n\tif (ret == 0 && copy_to_user(arg, di_args, sizeof(*di_args)))\n\t\tret = -EFAULT;\n\n\tkfree(di_args);\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int read_one_chunk(struct btrfs_fs_info *fs_info, struct btrfs_key *key,\n\t\t\t  struct extent_buffer *leaf,\n\t\t\t  struct btrfs_chunk *chunk)\n{\n\tstruct btrfs_mapping_tree *map_tree = &fs_info->mapping_tree;\n\tstruct map_lookup *map;\n\tstruct extent_map *em;\n\tu64 logical;\n\tu64 length;\n\tu64 devid;\n\tu8 uuid[BTRFS_UUID_SIZE];\n\tint num_stripes;\n\tint ret;\n\tint i;\n\n\tlogical = key->offset;\n\tlength = btrfs_chunk_length(leaf, chunk);\n\tnum_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\n\tret = btrfs_check_chunk_valid(fs_info, leaf, chunk, logical);\n\tif (ret)\n\t\treturn ret;\n\n\tread_lock(&map_tree->map_tree.lock);\n\tem = lookup_extent_mapping(&map_tree->map_tree, logical, 1);\n\tread_unlock(&map_tree->map_tree.lock);\n\n\t/* already mapped? */\n\tif (em && em->start <= logical && em->start + em->len > logical) {\n\t\tfree_extent_map(em);\n\t\treturn 0;\n\t} else if (em) {\n\t\tfree_extent_map(em);\n\t}\n\n\tem = alloc_extent_map();\n\tif (!em)\n\t\treturn -ENOMEM;\n\tmap = kmalloc(map_lookup_size(num_stripes), GFP_NOFS);\n\tif (!map) {\n\t\tfree_extent_map(em);\n\t\treturn -ENOMEM;\n\t}\n\n\tset_bit(EXTENT_FLAG_FS_MAPPING, &em->flags);\n\tem->map_lookup = map;\n\tem->start = logical;\n\tem->len = length;\n\tem->orig_start = 0;\n\tem->block_start = 0;\n\tem->block_len = em->len;\n\n\tmap->num_stripes = num_stripes;\n\tmap->io_width = btrfs_chunk_io_width(leaf, chunk);\n\tmap->io_align = btrfs_chunk_io_align(leaf, chunk);\n\tmap->stripe_len = btrfs_chunk_stripe_len(leaf, chunk);\n\tmap->type = btrfs_chunk_type(leaf, chunk);\n\tmap->sub_stripes = btrfs_chunk_sub_stripes(leaf, chunk);\n\tmap->verified_stripes = 0;\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tmap->stripes[i].physical =\n\t\t\tbtrfs_stripe_offset_nr(leaf, chunk, i);\n\t\tdevid = btrfs_stripe_devid_nr(leaf, chunk, i);\n\t\tread_extent_buffer(leaf, uuid, (unsigned long)\n\t\t\t\t   btrfs_stripe_dev_uuid_nr(chunk, i),\n\t\t\t\t   BTRFS_UUID_SIZE);\n\t\tmap->stripes[i].dev = btrfs_find_device(fs_info->fs_devices,\n\t\t\t\t\t\t\tdevid, uuid, NULL);\n\t\tif (!map->stripes[i].dev &&\n\t\t    !btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tfree_extent_map(em);\n\t\t\tbtrfs_report_missing_device(fs_info, devid, uuid, true);\n\t\t\treturn -ENOENT;\n\t\t}\n\t\tif (!map->stripes[i].dev) {\n\t\t\tmap->stripes[i].dev =\n\t\t\t\tadd_missing_dev(fs_info->fs_devices, devid,\n\t\t\t\t\t\tuuid);\n\t\t\tif (IS_ERR(map->stripes[i].dev)) {\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\t\"failed to init missing dev %llu: %ld\",\n\t\t\t\t\tdevid, PTR_ERR(map->stripes[i].dev));\n\t\t\t\treturn PTR_ERR(map->stripes[i].dev);\n\t\t\t}\n\t\t\tbtrfs_report_missing_device(fs_info, devid, uuid, false);\n\t\t}\n\t\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t&(map->stripes[i].dev->dev_state));\n\n\t}\n\n\twrite_lock(&map_tree->map_tree.lock);\n\tret = add_extent_mapping(&map_tree->map_tree, em, 0);\n\twrite_unlock(&map_tree->map_tree.lock);\n\tif (ret < 0) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"failed to add chunk map, start=%llu len=%llu: %d\",\n\t\t\t  em->start, em->len, ret);\n\t}\n\tfree_extent_map(em);\n\n\treturn ret;\n}",
                        "code_after_change": "static int read_one_chunk(struct btrfs_fs_info *fs_info, struct btrfs_key *key,\n\t\t\t  struct extent_buffer *leaf,\n\t\t\t  struct btrfs_chunk *chunk)\n{\n\tstruct btrfs_mapping_tree *map_tree = &fs_info->mapping_tree;\n\tstruct map_lookup *map;\n\tstruct extent_map *em;\n\tu64 logical;\n\tu64 length;\n\tu64 devid;\n\tu8 uuid[BTRFS_UUID_SIZE];\n\tint num_stripes;\n\tint ret;\n\tint i;\n\n\tlogical = key->offset;\n\tlength = btrfs_chunk_length(leaf, chunk);\n\tnum_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\n\tret = btrfs_check_chunk_valid(fs_info, leaf, chunk, logical);\n\tif (ret)\n\t\treturn ret;\n\n\tread_lock(&map_tree->map_tree.lock);\n\tem = lookup_extent_mapping(&map_tree->map_tree, logical, 1);\n\tread_unlock(&map_tree->map_tree.lock);\n\n\t/* already mapped? */\n\tif (em && em->start <= logical && em->start + em->len > logical) {\n\t\tfree_extent_map(em);\n\t\treturn 0;\n\t} else if (em) {\n\t\tfree_extent_map(em);\n\t}\n\n\tem = alloc_extent_map();\n\tif (!em)\n\t\treturn -ENOMEM;\n\tmap = kmalloc(map_lookup_size(num_stripes), GFP_NOFS);\n\tif (!map) {\n\t\tfree_extent_map(em);\n\t\treturn -ENOMEM;\n\t}\n\n\tset_bit(EXTENT_FLAG_FS_MAPPING, &em->flags);\n\tem->map_lookup = map;\n\tem->start = logical;\n\tem->len = length;\n\tem->orig_start = 0;\n\tem->block_start = 0;\n\tem->block_len = em->len;\n\n\tmap->num_stripes = num_stripes;\n\tmap->io_width = btrfs_chunk_io_width(leaf, chunk);\n\tmap->io_align = btrfs_chunk_io_align(leaf, chunk);\n\tmap->stripe_len = btrfs_chunk_stripe_len(leaf, chunk);\n\tmap->type = btrfs_chunk_type(leaf, chunk);\n\tmap->sub_stripes = btrfs_chunk_sub_stripes(leaf, chunk);\n\tmap->verified_stripes = 0;\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tmap->stripes[i].physical =\n\t\t\tbtrfs_stripe_offset_nr(leaf, chunk, i);\n\t\tdevid = btrfs_stripe_devid_nr(leaf, chunk, i);\n\t\tread_extent_buffer(leaf, uuid, (unsigned long)\n\t\t\t\t   btrfs_stripe_dev_uuid_nr(chunk, i),\n\t\t\t\t   BTRFS_UUID_SIZE);\n\t\tmap->stripes[i].dev = btrfs_find_device(fs_info->fs_devices,\n\t\t\t\t\t\t\tdevid, uuid, NULL, true);\n\t\tif (!map->stripes[i].dev &&\n\t\t    !btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tfree_extent_map(em);\n\t\t\tbtrfs_report_missing_device(fs_info, devid, uuid, true);\n\t\t\treturn -ENOENT;\n\t\t}\n\t\tif (!map->stripes[i].dev) {\n\t\t\tmap->stripes[i].dev =\n\t\t\t\tadd_missing_dev(fs_info->fs_devices, devid,\n\t\t\t\t\t\tuuid);\n\t\t\tif (IS_ERR(map->stripes[i].dev)) {\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\t\"failed to init missing dev %llu: %ld\",\n\t\t\t\t\tdevid, PTR_ERR(map->stripes[i].dev));\n\t\t\t\treturn PTR_ERR(map->stripes[i].dev);\n\t\t\t}\n\t\t\tbtrfs_report_missing_device(fs_info, devid, uuid, false);\n\t\t}\n\t\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t&(map->stripes[i].dev->dev_state));\n\n\t}\n\n\twrite_lock(&map_tree->map_tree.lock);\n\tret = add_extent_mapping(&map_tree->map_tree, em, 0);\n\twrite_unlock(&map_tree->map_tree.lock);\n\tif (ret < 0) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"failed to add chunk map, start=%llu len=%llu: %d\",\n\t\t\t  em->start, em->len, ret);\n\t}\n\tfree_extent_map(em);\n\n\treturn ret;\n}",
                        "cve_id": "CVE-2019-18885"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2218,
            "cve_id": "CVE-2019-18885",
            "code_snippet": "int btrfs_scrub_dev(struct btrfs_fs_info *fs_info, u64 devid, u64 start,\n\t\t    u64 end, struct btrfs_scrub_progress *progress,\n\t\t    int readonly, int is_dev_replace)\n{\n\tstruct scrub_ctx *sctx;\n\tint ret;\n\tstruct btrfs_device *dev;\n\tunsigned int nofs_flag;\n\n\tif (btrfs_fs_closing(fs_info))\n\t\treturn -EINVAL;\n\n\tif (fs_info->nodesize > BTRFS_STRIPE_LEN) {\n\t\t/*\n\t\t * in this case scrub is unable to calculate the checksum\n\t\t * the way scrub is implemented. Do not handle this\n\t\t * situation at all because it won't ever happen.\n\t\t */\n\t\tbtrfs_err(fs_info,\n\t\t\t   \"scrub: size assumption nodesize <= BTRFS_STRIPE_LEN (%d <= %d) fails\",\n\t\t       fs_info->nodesize,\n\t\t       BTRFS_STRIPE_LEN);\n\t\treturn -EINVAL;\n\t}\n\n\tif (fs_info->sectorsize != PAGE_SIZE) {\n\t\t/* not supported for data w/o checksums */\n\t\tbtrfs_err_rl(fs_info,\n\t\t\t   \"scrub: size assumption sectorsize != PAGE_SIZE (%d != %lu) fails\",\n\t\t       fs_info->sectorsize, PAGE_SIZE);\n\t\treturn -EINVAL;\n\t}\n\n\tif (fs_info->nodesize >\n\t    PAGE_SIZE * SCRUB_MAX_PAGES_PER_BLOCK ||\n\t    fs_info->sectorsize > PAGE_SIZE * SCRUB_MAX_PAGES_PER_BLOCK) {\n\t\t/*\n\t\t * would exhaust the array bounds of pagev member in\n\t\t * struct scrub_block\n\t\t */\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"scrub: size assumption nodesize and sectorsize <= SCRUB_MAX_PAGES_PER_BLOCK (%d <= %d && %d <= %d) fails\",\n\t\t       fs_info->nodesize,\n\t\t       SCRUB_MAX_PAGES_PER_BLOCK,\n\t\t       fs_info->sectorsize,\n\t\t       SCRUB_MAX_PAGES_PER_BLOCK);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Allocate outside of device_list_mutex */\n\tsctx = scrub_setup_ctx(fs_info, is_dev_replace);\n\tif (IS_ERR(sctx))\n\t\treturn PTR_ERR(sctx);\n\n\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL, true);\n\tif (!dev || (test_bit(BTRFS_DEV_STATE_MISSING, &dev->dev_state) &&\n\t\t     !is_dev_replace)) {\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tret = -ENODEV;\n\t\tgoto out_free_ctx;\n\t}\n\n\tif (!is_dev_replace && !readonly &&\n\t    !test_bit(BTRFS_DEV_STATE_WRITEABLE, &dev->dev_state)) {\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tbtrfs_err_in_rcu(fs_info, \"scrub: device %s is not writable\",\n\t\t\t\trcu_str_deref(dev->name));\n\t\tret = -EROFS;\n\t\tgoto out_free_ctx;\n\t}\n\n\tmutex_lock(&fs_info->scrub_lock);\n\tif (!test_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &dev->dev_state) ||\n\t    test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &dev->dev_state)) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tret = -EIO;\n\t\tgoto out_free_ctx;\n\t}\n\n\tdown_read(&fs_info->dev_replace.rwsem);\n\tif (dev->scrub_ctx ||\n\t    (!is_dev_replace &&\n\t     btrfs_dev_replace_is_ongoing(&fs_info->dev_replace))) {\n\t\tup_read(&fs_info->dev_replace.rwsem);\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tret = -EINPROGRESS;\n\t\tgoto out_free_ctx;\n\t}\n\tup_read(&fs_info->dev_replace.rwsem);\n\n\tret = scrub_workers_get(fs_info, is_dev_replace);\n\tif (ret) {\n\t\tmutex_unlock(&fs_info->scrub_lock);\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t\tgoto out_free_ctx;\n\t}\n\n\tsctx->readonly = readonly;\n\tdev->scrub_ctx = sctx;\n\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\n\t/*\n\t * checking @scrub_pause_req here, we can avoid\n\t * race between committing transaction and scrubbing.\n\t */\n\t__scrub_blocked_if_needed(fs_info);\n\tatomic_inc(&fs_info->scrubs_running);\n\tmutex_unlock(&fs_info->scrub_lock);\n\n\t/*\n\t * In order to avoid deadlock with reclaim when there is a transaction\n\t * trying to pause scrub, make sure we use GFP_NOFS for all the\n\t * allocations done at btrfs_scrub_pages() and scrub_pages_for_parity()\n\t * invoked by our callees. The pausing request is done when the\n\t * transaction commit starts, and it blocks the transaction until scrub\n\t * is paused (done at specific points at scrub_stripe() or right above\n\t * before incrementing fs_info->scrubs_running).\n\t */\n\tnofs_flag = memalloc_nofs_save();\n\tif (!is_dev_replace) {\n\t\t/*\n\t\t * by holding device list mutex, we can\n\t\t * kick off writing super in log tree sync.\n\t\t */\n\t\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\t\tret = scrub_supers(sctx, dev);\n\t\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\t}\n\n\tif (!ret)\n\t\tret = scrub_enumerate_chunks(sctx, dev, start, end);\n\tmemalloc_nofs_restore(nofs_flag);\n\n\twait_event(sctx->list_wait, atomic_read(&sctx->bios_in_flight) == 0);\n\tatomic_dec(&fs_info->scrubs_running);\n\twake_up(&fs_info->scrub_pause_wait);\n\n\twait_event(sctx->list_wait, atomic_read(&sctx->workers_pending) == 0);\n\n\tif (progress)\n\t\tmemcpy(progress, &sctx->stat, sizeof(*progress));\n\n\tmutex_lock(&fs_info->scrub_lock);\n\tdev->scrub_ctx = NULL;\n\tscrub_workers_put(fs_info);\n\tmutex_unlock(&fs_info->scrub_lock);\n\n\tscrub_put_ctx(sctx);\n\n\treturn ret;\n\nout_free_ctx:\n\tscrub_free_ctx(sctx);\n\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int btrfs_rm_device(struct btrfs_fs_info *fs_info, const char *device_path,\n\t\t    u64 devid)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *cur_devices;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 num_devices;\n\tint ret = 0;\n\n\tmutex_lock(&uuid_mutex);\n\n\tnum_devices = btrfs_num_devices(fs_info);\n\n\tret = btrfs_check_raid_min_devices(fs_info, num_devices - 1);\n\tif (ret)\n\t\tgoto out;\n\n\tdevice = btrfs_find_device_by_devspec(fs_info, devid, device_path);\n\n\tif (IS_ERR(device)) {\n\t\tif (PTR_ERR(device) == -ENOENT &&\n\t\t    strcmp(device_path, \"missing\") == 0)\n\t\t\tret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND;\n\t\telse\n\t\t\tret = PTR_ERR(device);\n\t\tgoto out;\n\t}\n\n\tif (btrfs_pinned_by_swapfile(fs_info, device)) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t  \"cannot remove device %s (devid %llu) due to active swapfile\",\n\t\t\t\t  rcu_str_deref(device->name), device->devid);\n\t\tret = -ETXTBSY;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = BTRFS_ERROR_DEV_TGT_REPLACE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    fs_info->fs_devices->rw_devices == 1) {\n\t\tret = BTRFS_ERROR_DEV_ONLY_WRITABLE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tdevice->fs_devices->rw_devices--;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\n\tmutex_unlock(&uuid_mutex);\n\tret = btrfs_shrink_device(device, 0);\n\tif (!ret)\n\t\tbtrfs_reada_remove_dev(device);\n\tmutex_lock(&uuid_mutex);\n\tif (ret)\n\t\tgoto error_undo;\n\n\t/*\n\t * TODO: the superblock still includes this device in its num_devices\n\t * counter although write_all_supers() is not locked out. This\n\t * could give a filesystem state which requires a degraded mount.\n\t */\n\tret = btrfs_rm_dev_item(device);\n\tif (ret)\n\t\tgoto error_undo;\n\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tbtrfs_scrub_cancel_dev(device);\n\n\t/*\n\t * the device list mutex makes sure that we don't change\n\t * the device list while someone else is writing out all\n\t * the device supers. Whoever is writing all supers, should\n\t * lock the device list mutex before getting the number of\n\t * devices in the super block (super_copy). Conversely,\n\t * whoever updates the number of devices in the super block\n\t * (super_copy) should hold the device list mutex.\n\t */\n\n\t/*\n\t * In normal cases the cur_devices == fs_devices. But in case\n\t * of deleting a seed device, the cur_devices should point to\n\t * its own fs_devices listed under the fs_devices->seed.\n\t */\n\tcur_devices = device->fs_devices;\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_del_rcu(&device->dev_list);\n\n\tcur_devices->num_devices--;\n\tcur_devices->total_devices--;\n\t/* Update total_devices of the parent fs_devices if it's seed */\n\tif (cur_devices != fs_devices)\n\t\tfs_devices->total_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tcur_devices->missing_devices--;\n\n\tbtrfs_assign_next_active_device(device, NULL);\n\n\tif (device->bdev) {\n\t\tcur_devices->open_devices--;\n\t\t/* remove sysfs entry */\n\t\tbtrfs_sysfs_remove_device(device);\n\t}\n\n\tnum_devices = btrfs_super_num_devices(fs_info->super_copy) - 1;\n\tbtrfs_set_super_num_devices(fs_info->super_copy, num_devices);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * at this point, the device is zero sized and detached from\n\t * the devices list.  All that's left is to zero out the old\n\t * supers and free the device.\n\t */\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\tbtrfs_scratch_superblocks(fs_info, device->bdev,\n\t\t\t\t\t  device->name->str);\n\n\tbtrfs_close_bdev(device);\n\tsynchronize_rcu();\n\tbtrfs_free_device(device);\n\n\tif (cur_devices->open_devices == 0) {\n\t\tlist_del_init(&cur_devices->seed_list);\n\t\tclose_fs_devices(cur_devices);\n\t\tfree_fs_devices(cur_devices);\n\t}\n\nout:\n\tmutex_unlock(&uuid_mutex);\n\treturn ret;\n\nerror_undo:\n\tbtrfs_reada_undo_remove_dev(device);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_add(&device->dev_alloc_list,\n\t\t\t &fs_devices->alloc_list);\n\t\tdevice->fs_devices->rw_devices++;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\tgoto out;\n}",
                        "code_after_change": "int btrfs_rm_device(struct btrfs_fs_info *fs_info, const char *device_path,\n\t\t    u64 devid)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *cur_devices;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 num_devices;\n\tint ret = 0;\n\n\tmutex_lock(&uuid_mutex);\n\n\tnum_devices = btrfs_num_devices(fs_info);\n\n\tret = btrfs_check_raid_min_devices(fs_info, num_devices - 1);\n\tif (ret)\n\t\tgoto out;\n\n\tdevice = btrfs_find_device_by_devspec(fs_info, devid, device_path);\n\n\tif (IS_ERR(device)) {\n\t\tif (PTR_ERR(device) == -ENOENT &&\n\t\t    device_path && strcmp(device_path, \"missing\") == 0)\n\t\t\tret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND;\n\t\telse\n\t\t\tret = PTR_ERR(device);\n\t\tgoto out;\n\t}\n\n\tif (btrfs_pinned_by_swapfile(fs_info, device)) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t  \"cannot remove device %s (devid %llu) due to active swapfile\",\n\t\t\t\t  rcu_str_deref(device->name), device->devid);\n\t\tret = -ETXTBSY;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = BTRFS_ERROR_DEV_TGT_REPLACE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    fs_info->fs_devices->rw_devices == 1) {\n\t\tret = BTRFS_ERROR_DEV_ONLY_WRITABLE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tdevice->fs_devices->rw_devices--;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\n\tmutex_unlock(&uuid_mutex);\n\tret = btrfs_shrink_device(device, 0);\n\tif (!ret)\n\t\tbtrfs_reada_remove_dev(device);\n\tmutex_lock(&uuid_mutex);\n\tif (ret)\n\t\tgoto error_undo;\n\n\t/*\n\t * TODO: the superblock still includes this device in its num_devices\n\t * counter although write_all_supers() is not locked out. This\n\t * could give a filesystem state which requires a degraded mount.\n\t */\n\tret = btrfs_rm_dev_item(device);\n\tif (ret)\n\t\tgoto error_undo;\n\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tbtrfs_scrub_cancel_dev(device);\n\n\t/*\n\t * the device list mutex makes sure that we don't change\n\t * the device list while someone else is writing out all\n\t * the device supers. Whoever is writing all supers, should\n\t * lock the device list mutex before getting the number of\n\t * devices in the super block (super_copy). Conversely,\n\t * whoever updates the number of devices in the super block\n\t * (super_copy) should hold the device list mutex.\n\t */\n\n\t/*\n\t * In normal cases the cur_devices == fs_devices. But in case\n\t * of deleting a seed device, the cur_devices should point to\n\t * its own fs_devices listed under the fs_devices->seed.\n\t */\n\tcur_devices = device->fs_devices;\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_del_rcu(&device->dev_list);\n\n\tcur_devices->num_devices--;\n\tcur_devices->total_devices--;\n\t/* Update total_devices of the parent fs_devices if it's seed */\n\tif (cur_devices != fs_devices)\n\t\tfs_devices->total_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tcur_devices->missing_devices--;\n\n\tbtrfs_assign_next_active_device(device, NULL);\n\n\tif (device->bdev) {\n\t\tcur_devices->open_devices--;\n\t\t/* remove sysfs entry */\n\t\tbtrfs_sysfs_remove_device(device);\n\t}\n\n\tnum_devices = btrfs_super_num_devices(fs_info->super_copy) - 1;\n\tbtrfs_set_super_num_devices(fs_info->super_copy, num_devices);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * at this point, the device is zero sized and detached from\n\t * the devices list.  All that's left is to zero out the old\n\t * supers and free the device.\n\t */\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\tbtrfs_scratch_superblocks(fs_info, device->bdev,\n\t\t\t\t\t  device->name->str);\n\n\tbtrfs_close_bdev(device);\n\tsynchronize_rcu();\n\tbtrfs_free_device(device);\n\n\tif (cur_devices->open_devices == 0) {\n\t\tlist_del_init(&cur_devices->seed_list);\n\t\tclose_fs_devices(cur_devices);\n\t\tfree_fs_devices(cur_devices);\n\t}\n\nout:\n\tmutex_unlock(&uuid_mutex);\n\treturn ret;\n\nerror_undo:\n\tbtrfs_reada_undo_remove_dev(device);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_add(&device->dev_alloc_list,\n\t\t\t &fs_devices->alloc_list);\n\t\tdevice->fs_devices->rw_devices++;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\tgoto out;\n}",
                        "cve_id": "CVE-2021-3739"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2219,
            "cve_id": "CVE-2019-18885",
            "code_snippet": "int btrfs_scrub_progress(struct btrfs_fs_info *fs_info, u64 devid,\n\t\t\t struct btrfs_scrub_progress *progress)\n{\n\tstruct btrfs_device *dev;\n\tstruct scrub_ctx *sctx = NULL;\n\n\tmutex_lock(&fs_info->fs_devices->device_list_mutex);\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL, true);\n\tif (dev)\n\t\tsctx = dev->scrub_ctx;\n\tif (sctx)\n\t\tmemcpy(progress, &sctx->stat, sizeof(*progress));\n\tmutex_unlock(&fs_info->fs_devices->device_list_mutex);\n\n\treturn dev ? (sctx ? 0 : -ENOTCONN) : -ENODEV;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int verify_one_dev_extent(struct btrfs_fs_info *fs_info,\n\t\t\t\t u64 chunk_offset, u64 devid,\n\t\t\t\t u64 physical_offset, u64 physical_len)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree.map_tree;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_device *dev;\n\tu64 stripe_len;\n\tbool found = false;\n\tint ret = 0;\n\tint i;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, chunk_offset, 1);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu doesn't have corresponding chunk\",\n\t\t\t  physical_offset, devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tmap = em->map_lookup;\n\tstripe_len = calc_stripe_length(map->type, em->len, map->num_stripes);\n\tif (physical_len != stripe_len) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu length doesn't match chunk %llu, have %llu expect %llu\",\n\t\t\t  physical_offset, devid, em->start, physical_len,\n\t\t\t  stripe_len);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (map->stripes[i].dev->devid == devid &&\n\t\t    map->stripes[i].physical == physical_offset) {\n\t\t\tfound = true;\n\t\t\tif (map->verified_stripes >= map->num_stripes) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"too many dev extents for chunk %llu found\",\n\t\t\t\t\t  em->start);\n\t\t\t\tret = -EUCLEAN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tmap->verified_stripes++;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found) {\n\t\tbtrfs_err(fs_info,\n\t\"dev extent physical offset %llu devid %llu has no corresponding chunk\",\n\t\t\tphysical_offset, devid);\n\t\tret = -EUCLEAN;\n\t}\n\n\t/* Make sure no dev extent is beyond device bondary */\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL);\n\tif (!dev) {\n\t\tbtrfs_err(fs_info, \"failed to find devid %llu\", devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\t/* It's possible this device is a dummy for seed device */\n\tif (dev->disk_total_bytes == 0) {\n\t\tdev = find_device(fs_info->fs_devices->seed, devid, NULL);\n\t\tif (!dev) {\n\t\t\tbtrfs_err(fs_info, \"failed to find seed devid %llu\",\n\t\t\t\t  devid);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (physical_offset + physical_len > dev->disk_total_bytes) {\n\t\tbtrfs_err(fs_info,\n\"dev extent devid %llu physical offset %llu len %llu is beyond device boundary %llu\",\n\t\t\t  devid, physical_offset, physical_len,\n\t\t\t  dev->disk_total_bytes);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\nout:\n\tfree_extent_map(em);\n\treturn ret;\n}",
                        "code_after_change": "static noinline struct btrfs_device *device_list_add(const char *path,\n\t\t\t   struct btrfs_super_block *disk_super,\n\t\t\t   bool *new_device_added)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *fs_devices = NULL;\n\tstruct rcu_string *name;\n\tu64 found_transid = btrfs_super_generation(disk_super);\n\tu64 devid = btrfs_stack_device_id(&disk_super->dev_item);\n\tbool has_metadata_uuid = (btrfs_super_incompat_flags(disk_super) &\n\t\tBTRFS_FEATURE_INCOMPAT_METADATA_UUID);\n\tbool fsid_change_in_progress = (btrfs_super_flags(disk_super) &\n\t\t\t\t\tBTRFS_SUPER_FLAG_CHANGING_FSID_V2);\n\n\tif (fsid_change_in_progress) {\n\t\tif (!has_metadata_uuid) {\n\t\t\t/*\n\t\t\t * When we have an image which has CHANGING_FSID_V2 set\n\t\t\t * it might belong to either a filesystem which has\n\t\t\t * disks with completed fsid change or it might belong\n\t\t\t * to fs with no UUID changes in effect, handle both.\n\t\t\t */\n\t\t\tfs_devices = find_fsid_inprogress(disk_super);\n\t\t\tif (!fs_devices)\n\t\t\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t\t} else {\n\t\t\tfs_devices = find_fsid_changed(disk_super);\n\t\t}\n\t} else if (has_metadata_uuid) {\n\t\tfs_devices = find_fsid(disk_super->fsid,\n\t\t\t\t       disk_super->metadata_uuid);\n\t} else {\n\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t}\n\n\n\tif (!fs_devices) {\n\t\tif (has_metadata_uuid)\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid,\n\t\t\t\t\t\t      disk_super->metadata_uuid);\n\t\telse\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid, NULL);\n\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn ERR_CAST(fs_devices);\n\n\t\tfs_devices->fsid_change = fsid_change_in_progress;\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_add(&fs_devices->fs_list, &fs_uuids);\n\n\t\tdevice = NULL;\n\t} else {\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tdevice = btrfs_find_device(fs_devices, devid,\n\t\t\t\tdisk_super->dev_item.uuid, NULL, false);\n\n\t\t/*\n\t\t * If this disk has been pulled into an fs devices created by\n\t\t * a device which had the CHANGING_FSID_V2 flag then replace the\n\t\t * metadata_uuid/fsid values of the fs_devices.\n\t\t */\n\t\tif (has_metadata_uuid && fs_devices->fsid_change &&\n\t\t    found_transid > fs_devices->latest_generation) {\n\t\t\tmemcpy(fs_devices->fsid, disk_super->fsid,\n\t\t\t\t\tBTRFS_FSID_SIZE);\n\t\t\tmemcpy(fs_devices->metadata_uuid,\n\t\t\t\t\tdisk_super->metadata_uuid, BTRFS_FSID_SIZE);\n\n\t\t\tfs_devices->fsid_change = false;\n\t\t}\n\t}\n\n\tif (!device) {\n\t\tif (fs_devices->opened) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EBUSY);\n\t\t}\n\n\t\tdevice = btrfs_alloc_device(NULL, &devid,\n\t\t\t\t\t    disk_super->dev_item.uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t/* we can safely leave the fs_devices entry around */\n\t\t\treturn device;\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tbtrfs_free_device(device);\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_assign_pointer(device->name, name);\n\n\t\tlist_add_rcu(&device->dev_list, &fs_devices->devices);\n\t\tfs_devices->num_devices++;\n\n\t\tdevice->fs_devices = fs_devices;\n\t\t*new_device_added = true;\n\n\t\tif (disk_super->label[0])\n\t\t\tpr_info(\"BTRFS: device label %s devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->label, devid, found_transid, path);\n\t\telse\n\t\t\tpr_info(\"BTRFS: device fsid %pU devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->fsid, devid, found_transid, path);\n\n\t} else if (!device->name || strcmp(device->name->str, path)) {\n\t\t/*\n\t\t * When FS is already mounted.\n\t\t * 1. If you are here and if the device->name is NULL that\n\t\t *    means this device was missing at time of FS mount.\n\t\t * 2. If you are here and if the device->name is different\n\t\t *    from 'path' that means either\n\t\t *      a. The same device disappeared and reappeared with\n\t\t *         different name. or\n\t\t *      b. The missing-disk-which-was-replaced, has\n\t\t *         reappeared now.\n\t\t *\n\t\t * We must allow 1 and 2a above. But 2b would be a spurious\n\t\t * and unintentional.\n\t\t *\n\t\t * Further in case of 1 and 2a above, the disk at 'path'\n\t\t * would have missed some transaction when it was away and\n\t\t * in case of 2a the stale bdev has to be updated as well.\n\t\t * 2b must not be allowed at all time.\n\t\t */\n\n\t\t/*\n\t\t * For now, we do allow update to btrfs_fs_device through the\n\t\t * btrfs dev scan cli after FS has been mounted.  We're still\n\t\t * tracking a problem where systems fail mount by subvolume id\n\t\t * when we reject replacement on a mounted FS.\n\t\t */\n\t\tif (!fs_devices->opened && found_transid < device->generation) {\n\t\t\t/*\n\t\t\t * That is if the FS is _not_ mounted and if you\n\t\t\t * are here, that means there is more than one\n\t\t\t * disk with same uuid and devid.We keep the one\n\t\t\t * with larger generation number or the last-in if\n\t\t\t * generation are equal.\n\t\t\t */\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EEXIST);\n\t\t}\n\n\t\t/*\n\t\t * We are going to replace the device path for a given devid,\n\t\t * make sure it's the same device if the device is mounted\n\t\t */\n\t\tif (device->bdev) {\n\t\t\tstruct block_device *path_bdev;\n\n\t\t\tpath_bdev = lookup_bdev(path);\n\t\t\tif (IS_ERR(path_bdev)) {\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\treturn ERR_CAST(path_bdev);\n\t\t\t}\n\n\t\t\tif (device->bdev != path_bdev) {\n\t\t\t\tbdput(path_bdev);\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\tbtrfs_warn_in_rcu(device->fs_info,\n\t\t\t\"duplicate device fsid:devid for %pU:%llu old:%s new:%s\",\n\t\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\t\trcu_str_deref(device->name), path);\n\t\t\t\treturn ERR_PTR(-EEXIST);\n\t\t\t}\n\t\t\tbdput(path_bdev);\n\t\t\tbtrfs_info_in_rcu(device->fs_info,\n\t\t\t\t\"device fsid %pU devid %llu moved old:%s new:%s\",\n\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\trcu_str_deref(device->name), path);\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_string_free(device->name);\n\t\trcu_assign_pointer(device->name, name);\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\tfs_devices->missing_devices--;\n\t\t\tclear_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\t}\n\n\t/*\n\t * Unmount does not free the btrfs_device struct but would zero\n\t * generation along with most of the other members. So just update\n\t * it back. We need it to pick the disk with largest generation\n\t * (as above).\n\t */\n\tif (!fs_devices->opened) {\n\t\tdevice->generation = found_transid;\n\t\tfs_devices->latest_generation = max_t(u64, found_transid,\n\t\t\t\t\t\tfs_devices->latest_generation);\n\t}\n\n\tfs_devices->total_devices = btrfs_super_num_devices(disk_super);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\treturn device;\n}",
                        "cve_id": "CVE-2019-18885"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int btrfs_rm_device(struct btrfs_fs_info *fs_info, const char *device_path,\n\t\t    u64 devid)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *cur_devices;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 num_devices;\n\tint ret = 0;\n\n\tmutex_lock(&uuid_mutex);\n\n\tnum_devices = btrfs_num_devices(fs_info);\n\n\tret = btrfs_check_raid_min_devices(fs_info, num_devices - 1);\n\tif (ret)\n\t\tgoto out;\n\n\tdevice = btrfs_find_device_by_devspec(fs_info, devid, device_path);\n\n\tif (IS_ERR(device)) {\n\t\tif (PTR_ERR(device) == -ENOENT &&\n\t\t    strcmp(device_path, \"missing\") == 0)\n\t\t\tret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND;\n\t\telse\n\t\t\tret = PTR_ERR(device);\n\t\tgoto out;\n\t}\n\n\tif (btrfs_pinned_by_swapfile(fs_info, device)) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t  \"cannot remove device %s (devid %llu) due to active swapfile\",\n\t\t\t\t  rcu_str_deref(device->name), device->devid);\n\t\tret = -ETXTBSY;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = BTRFS_ERROR_DEV_TGT_REPLACE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    fs_info->fs_devices->rw_devices == 1) {\n\t\tret = BTRFS_ERROR_DEV_ONLY_WRITABLE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tdevice->fs_devices->rw_devices--;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\n\tmutex_unlock(&uuid_mutex);\n\tret = btrfs_shrink_device(device, 0);\n\tif (!ret)\n\t\tbtrfs_reada_remove_dev(device);\n\tmutex_lock(&uuid_mutex);\n\tif (ret)\n\t\tgoto error_undo;\n\n\t/*\n\t * TODO: the superblock still includes this device in its num_devices\n\t * counter although write_all_supers() is not locked out. This\n\t * could give a filesystem state which requires a degraded mount.\n\t */\n\tret = btrfs_rm_dev_item(device);\n\tif (ret)\n\t\tgoto error_undo;\n\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tbtrfs_scrub_cancel_dev(device);\n\n\t/*\n\t * the device list mutex makes sure that we don't change\n\t * the device list while someone else is writing out all\n\t * the device supers. Whoever is writing all supers, should\n\t * lock the device list mutex before getting the number of\n\t * devices in the super block (super_copy). Conversely,\n\t * whoever updates the number of devices in the super block\n\t * (super_copy) should hold the device list mutex.\n\t */\n\n\t/*\n\t * In normal cases the cur_devices == fs_devices. But in case\n\t * of deleting a seed device, the cur_devices should point to\n\t * its own fs_devices listed under the fs_devices->seed.\n\t */\n\tcur_devices = device->fs_devices;\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_del_rcu(&device->dev_list);\n\n\tcur_devices->num_devices--;\n\tcur_devices->total_devices--;\n\t/* Update total_devices of the parent fs_devices if it's seed */\n\tif (cur_devices != fs_devices)\n\t\tfs_devices->total_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tcur_devices->missing_devices--;\n\n\tbtrfs_assign_next_active_device(device, NULL);\n\n\tif (device->bdev) {\n\t\tcur_devices->open_devices--;\n\t\t/* remove sysfs entry */\n\t\tbtrfs_sysfs_remove_device(device);\n\t}\n\n\tnum_devices = btrfs_super_num_devices(fs_info->super_copy) - 1;\n\tbtrfs_set_super_num_devices(fs_info->super_copy, num_devices);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * at this point, the device is zero sized and detached from\n\t * the devices list.  All that's left is to zero out the old\n\t * supers and free the device.\n\t */\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\tbtrfs_scratch_superblocks(fs_info, device->bdev,\n\t\t\t\t\t  device->name->str);\n\n\tbtrfs_close_bdev(device);\n\tsynchronize_rcu();\n\tbtrfs_free_device(device);\n\n\tif (cur_devices->open_devices == 0) {\n\t\tlist_del_init(&cur_devices->seed_list);\n\t\tclose_fs_devices(cur_devices);\n\t\tfree_fs_devices(cur_devices);\n\t}\n\nout:\n\tmutex_unlock(&uuid_mutex);\n\treturn ret;\n\nerror_undo:\n\tbtrfs_reada_undo_remove_dev(device);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_add(&device->dev_alloc_list,\n\t\t\t &fs_devices->alloc_list);\n\t\tdevice->fs_devices->rw_devices++;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\tgoto out;\n}",
                        "code_after_change": "static int verify_one_dev_extent(struct btrfs_fs_info *fs_info,\n\t\t\t\t u64 chunk_offset, u64 devid,\n\t\t\t\t u64 physical_offset, u64 physical_len)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree.map_tree;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_device *dev;\n\tu64 stripe_len;\n\tbool found = false;\n\tint ret = 0;\n\tint i;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, chunk_offset, 1);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu doesn't have corresponding chunk\",\n\t\t\t  physical_offset, devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tmap = em->map_lookup;\n\tstripe_len = calc_stripe_length(map->type, em->len, map->num_stripes);\n\tif (physical_len != stripe_len) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu length doesn't match chunk %llu, have %llu expect %llu\",\n\t\t\t  physical_offset, devid, em->start, physical_len,\n\t\t\t  stripe_len);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (map->stripes[i].dev->devid == devid &&\n\t\t    map->stripes[i].physical == physical_offset) {\n\t\t\tfound = true;\n\t\t\tif (map->verified_stripes >= map->num_stripes) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"too many dev extents for chunk %llu found\",\n\t\t\t\t\t  em->start);\n\t\t\t\tret = -EUCLEAN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tmap->verified_stripes++;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found) {\n\t\tbtrfs_err(fs_info,\n\t\"dev extent physical offset %llu devid %llu has no corresponding chunk\",\n\t\t\tphysical_offset, devid);\n\t\tret = -EUCLEAN;\n\t}\n\n\t/* Make sure no dev extent is beyond device bondary */\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL, true);\n\tif (!dev) {\n\t\tbtrfs_err(fs_info, \"failed to find devid %llu\", devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\t/* It's possible this device is a dummy for seed device */\n\tif (dev->disk_total_bytes == 0) {\n\t\tdev = btrfs_find_device(fs_info->fs_devices->seed, devid, NULL,\n\t\t\t\t\tNULL, false);\n\t\tif (!dev) {\n\t\t\tbtrfs_err(fs_info, \"failed to find seed devid %llu\",\n\t\t\t\t  devid);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (physical_offset + physical_len > dev->disk_total_bytes) {\n\t\tbtrfs_err(fs_info,\n\"dev extent devid %llu physical offset %llu len %llu is beyond device boundary %llu\",\n\t\t\t  devid, physical_offset, physical_len,\n\t\t\t  dev->disk_total_bytes);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\nout:\n\tfree_extent_map(em);\n\treturn ret;\n}",
                        "cve_id": "CVE-2021-3739"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static noinline struct btrfs_device *device_list_add(const char *path,\n\t\t\t   struct btrfs_super_block *disk_super,\n\t\t\t   bool *new_device_added)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *fs_devices = NULL;\n\tstruct rcu_string *name;\n\tu64 found_transid = btrfs_super_generation(disk_super);\n\tu64 devid = btrfs_stack_device_id(&disk_super->dev_item);\n\tbool has_metadata_uuid = (btrfs_super_incompat_flags(disk_super) &\n\t\tBTRFS_FEATURE_INCOMPAT_METADATA_UUID);\n\tbool fsid_change_in_progress = (btrfs_super_flags(disk_super) &\n\t\t\t\t\tBTRFS_SUPER_FLAG_CHANGING_FSID_V2);\n\n\tif (fsid_change_in_progress) {\n\t\tif (!has_metadata_uuid) {\n\t\t\t/*\n\t\t\t * When we have an image which has CHANGING_FSID_V2 set\n\t\t\t * it might belong to either a filesystem which has\n\t\t\t * disks with completed fsid change or it might belong\n\t\t\t * to fs with no UUID changes in effect, handle both.\n\t\t\t */\n\t\t\tfs_devices = find_fsid_inprogress(disk_super);\n\t\t\tif (!fs_devices)\n\t\t\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t\t} else {\n\t\t\tfs_devices = find_fsid_changed(disk_super);\n\t\t}\n\t} else if (has_metadata_uuid) {\n\t\tfs_devices = find_fsid(disk_super->fsid,\n\t\t\t\t       disk_super->metadata_uuid);\n\t} else {\n\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t}\n\n\n\tif (!fs_devices) {\n\t\tif (has_metadata_uuid)\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid,\n\t\t\t\t\t\t      disk_super->metadata_uuid);\n\t\telse\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid, NULL);\n\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn ERR_CAST(fs_devices);\n\n\t\tfs_devices->fsid_change = fsid_change_in_progress;\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_add(&fs_devices->fs_list, &fs_uuids);\n\n\t\tdevice = NULL;\n\t} else {\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tdevice = find_device(fs_devices, devid,\n\t\t\t\tdisk_super->dev_item.uuid);\n\n\t\t/*\n\t\t * If this disk has been pulled into an fs devices created by\n\t\t * a device which had the CHANGING_FSID_V2 flag then replace the\n\t\t * metadata_uuid/fsid values of the fs_devices.\n\t\t */\n\t\tif (has_metadata_uuid && fs_devices->fsid_change &&\n\t\t    found_transid > fs_devices->latest_generation) {\n\t\t\tmemcpy(fs_devices->fsid, disk_super->fsid,\n\t\t\t\t\tBTRFS_FSID_SIZE);\n\t\t\tmemcpy(fs_devices->metadata_uuid,\n\t\t\t\t\tdisk_super->metadata_uuid, BTRFS_FSID_SIZE);\n\n\t\t\tfs_devices->fsid_change = false;\n\t\t}\n\t}\n\n\tif (!device) {\n\t\tif (fs_devices->opened) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EBUSY);\n\t\t}\n\n\t\tdevice = btrfs_alloc_device(NULL, &devid,\n\t\t\t\t\t    disk_super->dev_item.uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t/* we can safely leave the fs_devices entry around */\n\t\t\treturn device;\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tbtrfs_free_device(device);\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_assign_pointer(device->name, name);\n\n\t\tlist_add_rcu(&device->dev_list, &fs_devices->devices);\n\t\tfs_devices->num_devices++;\n\n\t\tdevice->fs_devices = fs_devices;\n\t\t*new_device_added = true;\n\n\t\tif (disk_super->label[0])\n\t\t\tpr_info(\"BTRFS: device label %s devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->label, devid, found_transid, path);\n\t\telse\n\t\t\tpr_info(\"BTRFS: device fsid %pU devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->fsid, devid, found_transid, path);\n\n\t} else if (!device->name || strcmp(device->name->str, path)) {\n\t\t/*\n\t\t * When FS is already mounted.\n\t\t * 1. If you are here and if the device->name is NULL that\n\t\t *    means this device was missing at time of FS mount.\n\t\t * 2. If you are here and if the device->name is different\n\t\t *    from 'path' that means either\n\t\t *      a. The same device disappeared and reappeared with\n\t\t *         different name. or\n\t\t *      b. The missing-disk-which-was-replaced, has\n\t\t *         reappeared now.\n\t\t *\n\t\t * We must allow 1 and 2a above. But 2b would be a spurious\n\t\t * and unintentional.\n\t\t *\n\t\t * Further in case of 1 and 2a above, the disk at 'path'\n\t\t * would have missed some transaction when it was away and\n\t\t * in case of 2a the stale bdev has to be updated as well.\n\t\t * 2b must not be allowed at all time.\n\t\t */\n\n\t\t/*\n\t\t * For now, we do allow update to btrfs_fs_device through the\n\t\t * btrfs dev scan cli after FS has been mounted.  We're still\n\t\t * tracking a problem where systems fail mount by subvolume id\n\t\t * when we reject replacement on a mounted FS.\n\t\t */\n\t\tif (!fs_devices->opened && found_transid < device->generation) {\n\t\t\t/*\n\t\t\t * That is if the FS is _not_ mounted and if you\n\t\t\t * are here, that means there is more than one\n\t\t\t * disk with same uuid and devid.We keep the one\n\t\t\t * with larger generation number or the last-in if\n\t\t\t * generation are equal.\n\t\t\t */\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EEXIST);\n\t\t}\n\n\t\t/*\n\t\t * We are going to replace the device path for a given devid,\n\t\t * make sure it's the same device if the device is mounted\n\t\t */\n\t\tif (device->bdev) {\n\t\t\tstruct block_device *path_bdev;\n\n\t\t\tpath_bdev = lookup_bdev(path);\n\t\t\tif (IS_ERR(path_bdev)) {\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\treturn ERR_CAST(path_bdev);\n\t\t\t}\n\n\t\t\tif (device->bdev != path_bdev) {\n\t\t\t\tbdput(path_bdev);\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\tbtrfs_warn_in_rcu(device->fs_info,\n\t\t\t\"duplicate device fsid:devid for %pU:%llu old:%s new:%s\",\n\t\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\t\trcu_str_deref(device->name), path);\n\t\t\t\treturn ERR_PTR(-EEXIST);\n\t\t\t}\n\t\t\tbdput(path_bdev);\n\t\t\tbtrfs_info_in_rcu(device->fs_info,\n\t\t\t\t\"device fsid %pU devid %llu moved old:%s new:%s\",\n\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\trcu_str_deref(device->name), path);\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_string_free(device->name);\n\t\trcu_assign_pointer(device->name, name);\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\tfs_devices->missing_devices--;\n\t\t\tclear_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\t}\n\n\t/*\n\t * Unmount does not free the btrfs_device struct but would zero\n\t * generation along with most of the other members. So just update\n\t * it back. We need it to pick the disk with largest generation\n\t * (as above).\n\t */\n\tif (!fs_devices->opened) {\n\t\tdevice->generation = found_transid;\n\t\tfs_devices->latest_generation = max_t(u64, found_transid,\n\t\t\t\t\t\tfs_devices->latest_generation);\n\t}\n\n\tfs_devices->total_devices = btrfs_super_num_devices(disk_super);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\treturn device;\n}",
                        "code_after_change": "int btrfs_rm_device(struct btrfs_fs_info *fs_info, const char *device_path,\n\t\t    u64 devid)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *cur_devices;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 num_devices;\n\tint ret = 0;\n\n\tmutex_lock(&uuid_mutex);\n\n\tnum_devices = btrfs_num_devices(fs_info);\n\n\tret = btrfs_check_raid_min_devices(fs_info, num_devices - 1);\n\tif (ret)\n\t\tgoto out;\n\n\tdevice = btrfs_find_device_by_devspec(fs_info, devid, device_path);\n\n\tif (IS_ERR(device)) {\n\t\tif (PTR_ERR(device) == -ENOENT &&\n\t\t    device_path && strcmp(device_path, \"missing\") == 0)\n\t\t\tret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND;\n\t\telse\n\t\t\tret = PTR_ERR(device);\n\t\tgoto out;\n\t}\n\n\tif (btrfs_pinned_by_swapfile(fs_info, device)) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t  \"cannot remove device %s (devid %llu) due to active swapfile\",\n\t\t\t\t  rcu_str_deref(device->name), device->devid);\n\t\tret = -ETXTBSY;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = BTRFS_ERROR_DEV_TGT_REPLACE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    fs_info->fs_devices->rw_devices == 1) {\n\t\tret = BTRFS_ERROR_DEV_ONLY_WRITABLE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tdevice->fs_devices->rw_devices--;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\n\tmutex_unlock(&uuid_mutex);\n\tret = btrfs_shrink_device(device, 0);\n\tif (!ret)\n\t\tbtrfs_reada_remove_dev(device);\n\tmutex_lock(&uuid_mutex);\n\tif (ret)\n\t\tgoto error_undo;\n\n\t/*\n\t * TODO: the superblock still includes this device in its num_devices\n\t * counter although write_all_supers() is not locked out. This\n\t * could give a filesystem state which requires a degraded mount.\n\t */\n\tret = btrfs_rm_dev_item(device);\n\tif (ret)\n\t\tgoto error_undo;\n\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tbtrfs_scrub_cancel_dev(device);\n\n\t/*\n\t * the device list mutex makes sure that we don't change\n\t * the device list while someone else is writing out all\n\t * the device supers. Whoever is writing all supers, should\n\t * lock the device list mutex before getting the number of\n\t * devices in the super block (super_copy). Conversely,\n\t * whoever updates the number of devices in the super block\n\t * (super_copy) should hold the device list mutex.\n\t */\n\n\t/*\n\t * In normal cases the cur_devices == fs_devices. But in case\n\t * of deleting a seed device, the cur_devices should point to\n\t * its own fs_devices listed under the fs_devices->seed.\n\t */\n\tcur_devices = device->fs_devices;\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_del_rcu(&device->dev_list);\n\n\tcur_devices->num_devices--;\n\tcur_devices->total_devices--;\n\t/* Update total_devices of the parent fs_devices if it's seed */\n\tif (cur_devices != fs_devices)\n\t\tfs_devices->total_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tcur_devices->missing_devices--;\n\n\tbtrfs_assign_next_active_device(device, NULL);\n\n\tif (device->bdev) {\n\t\tcur_devices->open_devices--;\n\t\t/* remove sysfs entry */\n\t\tbtrfs_sysfs_remove_device(device);\n\t}\n\n\tnum_devices = btrfs_super_num_devices(fs_info->super_copy) - 1;\n\tbtrfs_set_super_num_devices(fs_info->super_copy, num_devices);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * at this point, the device is zero sized and detached from\n\t * the devices list.  All that's left is to zero out the old\n\t * supers and free the device.\n\t */\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\tbtrfs_scratch_superblocks(fs_info, device->bdev,\n\t\t\t\t\t  device->name->str);\n\n\tbtrfs_close_bdev(device);\n\tsynchronize_rcu();\n\tbtrfs_free_device(device);\n\n\tif (cur_devices->open_devices == 0) {\n\t\tlist_del_init(&cur_devices->seed_list);\n\t\tclose_fs_devices(cur_devices);\n\t\tfree_fs_devices(cur_devices);\n\t}\n\nout:\n\tmutex_unlock(&uuid_mutex);\n\treturn ret;\n\nerror_undo:\n\tbtrfs_reada_undo_remove_dev(device);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_add(&device->dev_alloc_list,\n\t\t\t &fs_devices->alloc_list);\n\t\tdevice->fs_devices->rw_devices++;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\tgoto out;\n}",
                        "cve_id": "CVE-2019-18885"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "struct btrfs_device *btrfs_find_device_by_devspec(\n\t\tstruct btrfs_fs_info *fs_info, u64 devid,\n\t\tconst char *device_path)\n{\n\tstruct btrfs_device *device;\n\n\tif (devid) {\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, NULL,\n\t\t\t\t\t   NULL);\n\t\tif (!device)\n\t\t\treturn ERR_PTR(-ENOENT);\n\t\treturn device;\n\t}\n\n\tif (!device_path || !device_path[0])\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (strcmp(device_path, \"missing\") == 0) {\n\t\t/* Find first missing device */\n\t\tlist_for_each_entry(device, &fs_info->fs_devices->devices,\n\t\t\t\t    dev_list) {\n\t\t\tif (test_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t     &device->dev_state) && !device->bdev)\n\t\t\t\treturn device;\n\t\t}\n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\n\treturn btrfs_find_device_by_path(fs_info, device_path);\n}",
                        "code_after_change": "struct btrfs_device *btrfs_find_device_by_devspec(\n\t\tstruct btrfs_fs_info *fs_info, u64 devid,\n\t\tconst char *device_path)\n{\n\tstruct btrfs_device *device;\n\n\tif (devid) {\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, NULL,\n\t\t\t\t\t   NULL, true);\n\t\tif (!device)\n\t\t\treturn ERR_PTR(-ENOENT);\n\t\treturn device;\n\t}\n\n\tif (!device_path || !device_path[0])\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (strcmp(device_path, \"missing\") == 0) {\n\t\t/* Find first missing device */\n\t\tlist_for_each_entry(device, &fs_info->fs_devices->devices,\n\t\t\t\t    dev_list) {\n\t\t\tif (test_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t     &device->dev_state) && !device->bdev)\n\t\t\t\treturn device;\n\t\t}\n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\n\treturn btrfs_find_device_by_path(fs_info, device_path);\n}",
                        "cve_id": "CVE-2019-18885"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static struct btrfs_device *btrfs_find_device_by_path(\n\t\tstruct btrfs_fs_info *fs_info, const char *device_path)\n{\n\tint ret = 0;\n\tstruct btrfs_super_block *disk_super;\n\tu64 devid;\n\tu8 *dev_uuid;\n\tstruct block_device *bdev;\n\tstruct buffer_head *bh;\n\tstruct btrfs_device *device;\n\n\tret = btrfs_get_bdev_and_sb(device_path, FMODE_READ,\n\t\t\t\t    fs_info->bdev_holder, 0, &bdev, &bh);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\tdisk_super = (struct btrfs_super_block *)bh->b_data;\n\tdevid = btrfs_stack_device_id(&disk_super->dev_item);\n\tdev_uuid = disk_super->dev_item.uuid;\n\tif (btrfs_fs_incompat(fs_info, METADATA_UUID))\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t\t   disk_super->metadata_uuid);\n\telse\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t\t   disk_super->fsid);\n\n\tbrelse(bh);\n\tif (!device)\n\t\tdevice = ERR_PTR(-ENOENT);\n\tblkdev_put(bdev, FMODE_READ);\n\treturn device;\n}",
                        "code_after_change": "static struct btrfs_device *btrfs_find_device_by_path(\n\t\tstruct btrfs_fs_info *fs_info, const char *device_path)\n{\n\tint ret = 0;\n\tstruct btrfs_super_block *disk_super;\n\tu64 devid;\n\tu8 *dev_uuid;\n\tstruct block_device *bdev;\n\tstruct buffer_head *bh;\n\tstruct btrfs_device *device;\n\n\tret = btrfs_get_bdev_and_sb(device_path, FMODE_READ,\n\t\t\t\t    fs_info->bdev_holder, 0, &bdev, &bh);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\tdisk_super = (struct btrfs_super_block *)bh->b_data;\n\tdevid = btrfs_stack_device_id(&disk_super->dev_item);\n\tdev_uuid = disk_super->dev_item.uuid;\n\tif (btrfs_fs_incompat(fs_info, METADATA_UUID))\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t\t   disk_super->metadata_uuid, true);\n\telse\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t\t   disk_super->fsid, true);\n\n\tbrelse(bh);\n\tif (!device)\n\t\tdevice = ERR_PTR(-ENOENT);\n\tblkdev_put(bdev, FMODE_READ);\n\treturn device;\n}",
                        "cve_id": "CVE-2019-18885"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2220,
            "cve_id": "CVE-2019-18885",
            "code_snippet": "int btrfs_get_dev_stats(struct btrfs_fs_info *fs_info,\n\t\t\tstruct btrfs_ioctl_get_dev_stats *stats)\n{\n\tstruct btrfs_device *dev;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tint i;\n\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tdev = btrfs_find_device(fs_info->fs_devices, stats->devid, NULL, NULL,\n\t\t\t\ttrue);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\tif (!dev) {\n\t\tbtrfs_warn(fs_info, \"get dev_stats failed, device not found\");\n\t\treturn -ENODEV;\n\t} else if (!dev->dev_stats_valid) {\n\t\tbtrfs_warn(fs_info, \"get dev_stats failed, not yet valid\");\n\t\treturn -ENODEV;\n\t} else if (stats->flags & BTRFS_DEV_STATS_RESET) {\n\t\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++) {\n\t\t\tif (stats->nr_items > i)\n\t\t\t\tstats->values[i] =\n\t\t\t\t\tbtrfs_dev_stat_read_and_reset(dev, i);\n\t\t\telse\n\t\t\t\tbtrfs_dev_stat_reset(dev, i);\n\t\t}\n\t} else {\n\t\tfor (i = 0; i < BTRFS_DEV_STAT_VALUES_MAX; i++)\n\t\t\tif (stats->nr_items > i)\n\t\t\t\tstats->values[i] = btrfs_dev_stat_read(dev, i);\n\t}\n\tif (stats->nr_items > BTRFS_DEV_STAT_VALUES_MAX)\n\t\tstats->nr_items = BTRFS_DEV_STAT_VALUES_MAX;\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int verify_one_dev_extent(struct btrfs_fs_info *fs_info,\n\t\t\t\t u64 chunk_offset, u64 devid,\n\t\t\t\t u64 physical_offset, u64 physical_len)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree.map_tree;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_device *dev;\n\tu64 stripe_len;\n\tbool found = false;\n\tint ret = 0;\n\tint i;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, chunk_offset, 1);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu doesn't have corresponding chunk\",\n\t\t\t  physical_offset, devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tmap = em->map_lookup;\n\tstripe_len = calc_stripe_length(map->type, em->len, map->num_stripes);\n\tif (physical_len != stripe_len) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu length doesn't match chunk %llu, have %llu expect %llu\",\n\t\t\t  physical_offset, devid, em->start, physical_len,\n\t\t\t  stripe_len);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (map->stripes[i].dev->devid == devid &&\n\t\t    map->stripes[i].physical == physical_offset) {\n\t\t\tfound = true;\n\t\t\tif (map->verified_stripes >= map->num_stripes) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"too many dev extents for chunk %llu found\",\n\t\t\t\t\t  em->start);\n\t\t\t\tret = -EUCLEAN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tmap->verified_stripes++;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found) {\n\t\tbtrfs_err(fs_info,\n\t\"dev extent physical offset %llu devid %llu has no corresponding chunk\",\n\t\t\tphysical_offset, devid);\n\t\tret = -EUCLEAN;\n\t}\n\n\t/* Make sure no dev extent is beyond device bondary */\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL);\n\tif (!dev) {\n\t\tbtrfs_err(fs_info, \"failed to find devid %llu\", devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\t/* It's possible this device is a dummy for seed device */\n\tif (dev->disk_total_bytes == 0) {\n\t\tdev = find_device(fs_info->fs_devices->seed, devid, NULL);\n\t\tif (!dev) {\n\t\t\tbtrfs_err(fs_info, \"failed to find seed devid %llu\",\n\t\t\t\t  devid);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (physical_offset + physical_len > dev->disk_total_bytes) {\n\t\tbtrfs_err(fs_info,\n\"dev extent devid %llu physical offset %llu len %llu is beyond device boundary %llu\",\n\t\t\t  devid, physical_offset, physical_len,\n\t\t\t  dev->disk_total_bytes);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\nout:\n\tfree_extent_map(em);\n\treturn ret;\n}",
                        "code_after_change": "static int verify_one_dev_extent(struct btrfs_fs_info *fs_info,\n\t\t\t\t u64 chunk_offset, u64 devid,\n\t\t\t\t u64 physical_offset, u64 physical_len)\n{\n\tstruct extent_map_tree *em_tree = &fs_info->mapping_tree.map_tree;\n\tstruct extent_map *em;\n\tstruct map_lookup *map;\n\tstruct btrfs_device *dev;\n\tu64 stripe_len;\n\tbool found = false;\n\tint ret = 0;\n\tint i;\n\n\tread_lock(&em_tree->lock);\n\tem = lookup_extent_mapping(em_tree, chunk_offset, 1);\n\tread_unlock(&em_tree->lock);\n\n\tif (!em) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu doesn't have corresponding chunk\",\n\t\t\t  physical_offset, devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tmap = em->map_lookup;\n\tstripe_len = calc_stripe_length(map->type, em->len, map->num_stripes);\n\tif (physical_len != stripe_len) {\n\t\tbtrfs_err(fs_info,\n\"dev extent physical offset %llu on devid %llu length doesn't match chunk %llu, have %llu expect %llu\",\n\t\t\t  physical_offset, devid, em->start, physical_len,\n\t\t\t  stripe_len);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < map->num_stripes; i++) {\n\t\tif (map->stripes[i].dev->devid == devid &&\n\t\t    map->stripes[i].physical == physical_offset) {\n\t\t\tfound = true;\n\t\t\tif (map->verified_stripes >= map->num_stripes) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"too many dev extents for chunk %llu found\",\n\t\t\t\t\t  em->start);\n\t\t\t\tret = -EUCLEAN;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tmap->verified_stripes++;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found) {\n\t\tbtrfs_err(fs_info,\n\t\"dev extent physical offset %llu devid %llu has no corresponding chunk\",\n\t\t\tphysical_offset, devid);\n\t\tret = -EUCLEAN;\n\t}\n\n\t/* Make sure no dev extent is beyond device bondary */\n\tdev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL, true);\n\tif (!dev) {\n\t\tbtrfs_err(fs_info, \"failed to find devid %llu\", devid);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\n\t/* It's possible this device is a dummy for seed device */\n\tif (dev->disk_total_bytes == 0) {\n\t\tdev = btrfs_find_device(fs_info->fs_devices->seed, devid, NULL,\n\t\t\t\t\tNULL, false);\n\t\tif (!dev) {\n\t\t\tbtrfs_err(fs_info, \"failed to find seed devid %llu\",\n\t\t\t\t  devid);\n\t\t\tret = -EUCLEAN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (physical_offset + physical_len > dev->disk_total_bytes) {\n\t\tbtrfs_err(fs_info,\n\"dev extent devid %llu physical offset %llu len %llu is beyond device boundary %llu\",\n\t\t\t  devid, physical_offset, physical_len,\n\t\t\t  dev->disk_total_bytes);\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\nout:\n\tfree_extent_map(em);\n\treturn ret;\n}",
                        "cve_id": "CVE-2019-18885"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int read_one_chunk(struct btrfs_fs_info *fs_info, struct btrfs_key *key,\n\t\t\t  struct extent_buffer *leaf,\n\t\t\t  struct btrfs_chunk *chunk)\n{\n\tstruct btrfs_mapping_tree *map_tree = &fs_info->mapping_tree;\n\tstruct map_lookup *map;\n\tstruct extent_map *em;\n\tu64 logical;\n\tu64 length;\n\tu64 devid;\n\tu8 uuid[BTRFS_UUID_SIZE];\n\tint num_stripes;\n\tint ret;\n\tint i;\n\n\tlogical = key->offset;\n\tlength = btrfs_chunk_length(leaf, chunk);\n\tnum_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\n\tret = btrfs_check_chunk_valid(fs_info, leaf, chunk, logical);\n\tif (ret)\n\t\treturn ret;\n\n\tread_lock(&map_tree->map_tree.lock);\n\tem = lookup_extent_mapping(&map_tree->map_tree, logical, 1);\n\tread_unlock(&map_tree->map_tree.lock);\n\n\t/* already mapped? */\n\tif (em && em->start <= logical && em->start + em->len > logical) {\n\t\tfree_extent_map(em);\n\t\treturn 0;\n\t} else if (em) {\n\t\tfree_extent_map(em);\n\t}\n\n\tem = alloc_extent_map();\n\tif (!em)\n\t\treturn -ENOMEM;\n\tmap = kmalloc(map_lookup_size(num_stripes), GFP_NOFS);\n\tif (!map) {\n\t\tfree_extent_map(em);\n\t\treturn -ENOMEM;\n\t}\n\n\tset_bit(EXTENT_FLAG_FS_MAPPING, &em->flags);\n\tem->map_lookup = map;\n\tem->start = logical;\n\tem->len = length;\n\tem->orig_start = 0;\n\tem->block_start = 0;\n\tem->block_len = em->len;\n\n\tmap->num_stripes = num_stripes;\n\tmap->io_width = btrfs_chunk_io_width(leaf, chunk);\n\tmap->io_align = btrfs_chunk_io_align(leaf, chunk);\n\tmap->stripe_len = btrfs_chunk_stripe_len(leaf, chunk);\n\tmap->type = btrfs_chunk_type(leaf, chunk);\n\tmap->sub_stripes = btrfs_chunk_sub_stripes(leaf, chunk);\n\tmap->verified_stripes = 0;\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tmap->stripes[i].physical =\n\t\t\tbtrfs_stripe_offset_nr(leaf, chunk, i);\n\t\tdevid = btrfs_stripe_devid_nr(leaf, chunk, i);\n\t\tread_extent_buffer(leaf, uuid, (unsigned long)\n\t\t\t\t   btrfs_stripe_dev_uuid_nr(chunk, i),\n\t\t\t\t   BTRFS_UUID_SIZE);\n\t\tmap->stripes[i].dev = btrfs_find_device(fs_info->fs_devices,\n\t\t\t\t\t\t\tdevid, uuid, NULL);\n\t\tif (!map->stripes[i].dev &&\n\t\t    !btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tfree_extent_map(em);\n\t\t\tbtrfs_report_missing_device(fs_info, devid, uuid, true);\n\t\t\treturn -ENOENT;\n\t\t}\n\t\tif (!map->stripes[i].dev) {\n\t\t\tmap->stripes[i].dev =\n\t\t\t\tadd_missing_dev(fs_info->fs_devices, devid,\n\t\t\t\t\t\tuuid);\n\t\t\tif (IS_ERR(map->stripes[i].dev)) {\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\t\"failed to init missing dev %llu: %ld\",\n\t\t\t\t\tdevid, PTR_ERR(map->stripes[i].dev));\n\t\t\t\treturn PTR_ERR(map->stripes[i].dev);\n\t\t\t}\n\t\t\tbtrfs_report_missing_device(fs_info, devid, uuid, false);\n\t\t}\n\t\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t&(map->stripes[i].dev->dev_state));\n\n\t}\n\n\twrite_lock(&map_tree->map_tree.lock);\n\tret = add_extent_mapping(&map_tree->map_tree, em, 0);\n\twrite_unlock(&map_tree->map_tree.lock);\n\tif (ret < 0) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"failed to add chunk map, start=%llu len=%llu: %d\",\n\t\t\t  em->start, em->len, ret);\n\t}\n\tfree_extent_map(em);\n\n\treturn ret;\n}",
                        "code_after_change": "static int read_one_chunk(struct btrfs_fs_info *fs_info, struct btrfs_key *key,\n\t\t\t  struct extent_buffer *leaf,\n\t\t\t  struct btrfs_chunk *chunk)\n{\n\tstruct btrfs_mapping_tree *map_tree = &fs_info->mapping_tree;\n\tstruct map_lookup *map;\n\tstruct extent_map *em;\n\tu64 logical;\n\tu64 length;\n\tu64 devid;\n\tu8 uuid[BTRFS_UUID_SIZE];\n\tint num_stripes;\n\tint ret;\n\tint i;\n\n\tlogical = key->offset;\n\tlength = btrfs_chunk_length(leaf, chunk);\n\tnum_stripes = btrfs_chunk_num_stripes(leaf, chunk);\n\n\tret = btrfs_check_chunk_valid(fs_info, leaf, chunk, logical);\n\tif (ret)\n\t\treturn ret;\n\n\tread_lock(&map_tree->map_tree.lock);\n\tem = lookup_extent_mapping(&map_tree->map_tree, logical, 1);\n\tread_unlock(&map_tree->map_tree.lock);\n\n\t/* already mapped? */\n\tif (em && em->start <= logical && em->start + em->len > logical) {\n\t\tfree_extent_map(em);\n\t\treturn 0;\n\t} else if (em) {\n\t\tfree_extent_map(em);\n\t}\n\n\tem = alloc_extent_map();\n\tif (!em)\n\t\treturn -ENOMEM;\n\tmap = kmalloc(map_lookup_size(num_stripes), GFP_NOFS);\n\tif (!map) {\n\t\tfree_extent_map(em);\n\t\treturn -ENOMEM;\n\t}\n\n\tset_bit(EXTENT_FLAG_FS_MAPPING, &em->flags);\n\tem->map_lookup = map;\n\tem->start = logical;\n\tem->len = length;\n\tem->orig_start = 0;\n\tem->block_start = 0;\n\tem->block_len = em->len;\n\n\tmap->num_stripes = num_stripes;\n\tmap->io_width = btrfs_chunk_io_width(leaf, chunk);\n\tmap->io_align = btrfs_chunk_io_align(leaf, chunk);\n\tmap->stripe_len = btrfs_chunk_stripe_len(leaf, chunk);\n\tmap->type = btrfs_chunk_type(leaf, chunk);\n\tmap->sub_stripes = btrfs_chunk_sub_stripes(leaf, chunk);\n\tmap->verified_stripes = 0;\n\tfor (i = 0; i < num_stripes; i++) {\n\t\tmap->stripes[i].physical =\n\t\t\tbtrfs_stripe_offset_nr(leaf, chunk, i);\n\t\tdevid = btrfs_stripe_devid_nr(leaf, chunk, i);\n\t\tread_extent_buffer(leaf, uuid, (unsigned long)\n\t\t\t\t   btrfs_stripe_dev_uuid_nr(chunk, i),\n\t\t\t\t   BTRFS_UUID_SIZE);\n\t\tmap->stripes[i].dev = btrfs_find_device(fs_info->fs_devices,\n\t\t\t\t\t\t\tdevid, uuid, NULL, true);\n\t\tif (!map->stripes[i].dev &&\n\t\t    !btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tfree_extent_map(em);\n\t\t\tbtrfs_report_missing_device(fs_info, devid, uuid, true);\n\t\t\treturn -ENOENT;\n\t\t}\n\t\tif (!map->stripes[i].dev) {\n\t\t\tmap->stripes[i].dev =\n\t\t\t\tadd_missing_dev(fs_info->fs_devices, devid,\n\t\t\t\t\t\tuuid);\n\t\t\tif (IS_ERR(map->stripes[i].dev)) {\n\t\t\t\tfree_extent_map(em);\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\t\"failed to init missing dev %llu: %ld\",\n\t\t\t\t\tdevid, PTR_ERR(map->stripes[i].dev));\n\t\t\t\treturn PTR_ERR(map->stripes[i].dev);\n\t\t\t}\n\t\t\tbtrfs_report_missing_device(fs_info, devid, uuid, false);\n\t\t}\n\t\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA,\n\t\t\t\t&(map->stripes[i].dev->dev_state));\n\n\t}\n\n\twrite_lock(&map_tree->map_tree.lock);\n\tret = add_extent_mapping(&map_tree->map_tree, em, 0);\n\twrite_unlock(&map_tree->map_tree.lock);\n\tif (ret < 0) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"failed to add chunk map, start=%llu len=%llu: %d\",\n\t\t\t  em->start, em->len, ret);\n\t}\n\tfree_extent_map(em);\n\n\treturn ret;\n}",
                        "cve_id": "CVE-2019-18885"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int btrfs_rm_device(struct btrfs_fs_info *fs_info, const char *device_path,\n\t\t    u64 devid)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *cur_devices;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 num_devices;\n\tint ret = 0;\n\n\tmutex_lock(&uuid_mutex);\n\n\tnum_devices = btrfs_num_devices(fs_info);\n\n\tret = btrfs_check_raid_min_devices(fs_info, num_devices - 1);\n\tif (ret)\n\t\tgoto out;\n\n\tdevice = btrfs_find_device_by_devspec(fs_info, devid, device_path);\n\n\tif (IS_ERR(device)) {\n\t\tif (PTR_ERR(device) == -ENOENT &&\n\t\t    strcmp(device_path, \"missing\") == 0)\n\t\t\tret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND;\n\t\telse\n\t\t\tret = PTR_ERR(device);\n\t\tgoto out;\n\t}\n\n\tif (btrfs_pinned_by_swapfile(fs_info, device)) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t  \"cannot remove device %s (devid %llu) due to active swapfile\",\n\t\t\t\t  rcu_str_deref(device->name), device->devid);\n\t\tret = -ETXTBSY;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = BTRFS_ERROR_DEV_TGT_REPLACE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    fs_info->fs_devices->rw_devices == 1) {\n\t\tret = BTRFS_ERROR_DEV_ONLY_WRITABLE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tdevice->fs_devices->rw_devices--;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\n\tmutex_unlock(&uuid_mutex);\n\tret = btrfs_shrink_device(device, 0);\n\tif (!ret)\n\t\tbtrfs_reada_remove_dev(device);\n\tmutex_lock(&uuid_mutex);\n\tif (ret)\n\t\tgoto error_undo;\n\n\t/*\n\t * TODO: the superblock still includes this device in its num_devices\n\t * counter although write_all_supers() is not locked out. This\n\t * could give a filesystem state which requires a degraded mount.\n\t */\n\tret = btrfs_rm_dev_item(device);\n\tif (ret)\n\t\tgoto error_undo;\n\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tbtrfs_scrub_cancel_dev(device);\n\n\t/*\n\t * the device list mutex makes sure that we don't change\n\t * the device list while someone else is writing out all\n\t * the device supers. Whoever is writing all supers, should\n\t * lock the device list mutex before getting the number of\n\t * devices in the super block (super_copy). Conversely,\n\t * whoever updates the number of devices in the super block\n\t * (super_copy) should hold the device list mutex.\n\t */\n\n\t/*\n\t * In normal cases the cur_devices == fs_devices. But in case\n\t * of deleting a seed device, the cur_devices should point to\n\t * its own fs_devices listed under the fs_devices->seed.\n\t */\n\tcur_devices = device->fs_devices;\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_del_rcu(&device->dev_list);\n\n\tcur_devices->num_devices--;\n\tcur_devices->total_devices--;\n\t/* Update total_devices of the parent fs_devices if it's seed */\n\tif (cur_devices != fs_devices)\n\t\tfs_devices->total_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tcur_devices->missing_devices--;\n\n\tbtrfs_assign_next_active_device(device, NULL);\n\n\tif (device->bdev) {\n\t\tcur_devices->open_devices--;\n\t\t/* remove sysfs entry */\n\t\tbtrfs_sysfs_remove_device(device);\n\t}\n\n\tnum_devices = btrfs_super_num_devices(fs_info->super_copy) - 1;\n\tbtrfs_set_super_num_devices(fs_info->super_copy, num_devices);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * at this point, the device is zero sized and detached from\n\t * the devices list.  All that's left is to zero out the old\n\t * supers and free the device.\n\t */\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\tbtrfs_scratch_superblocks(fs_info, device->bdev,\n\t\t\t\t\t  device->name->str);\n\n\tbtrfs_close_bdev(device);\n\tsynchronize_rcu();\n\tbtrfs_free_device(device);\n\n\tif (cur_devices->open_devices == 0) {\n\t\tlist_del_init(&cur_devices->seed_list);\n\t\tclose_fs_devices(cur_devices);\n\t\tfree_fs_devices(cur_devices);\n\t}\n\nout:\n\tmutex_unlock(&uuid_mutex);\n\treturn ret;\n\nerror_undo:\n\tbtrfs_reada_undo_remove_dev(device);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_add(&device->dev_alloc_list,\n\t\t\t &fs_devices->alloc_list);\n\t\tdevice->fs_devices->rw_devices++;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\tgoto out;\n}",
                        "code_after_change": "static noinline struct btrfs_device *device_list_add(const char *path,\n\t\t\t   struct btrfs_super_block *disk_super,\n\t\t\t   bool *new_device_added)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *fs_devices = NULL;\n\tstruct rcu_string *name;\n\tu64 found_transid = btrfs_super_generation(disk_super);\n\tu64 devid = btrfs_stack_device_id(&disk_super->dev_item);\n\tbool has_metadata_uuid = (btrfs_super_incompat_flags(disk_super) &\n\t\tBTRFS_FEATURE_INCOMPAT_METADATA_UUID);\n\tbool fsid_change_in_progress = (btrfs_super_flags(disk_super) &\n\t\t\t\t\tBTRFS_SUPER_FLAG_CHANGING_FSID_V2);\n\n\tif (fsid_change_in_progress) {\n\t\tif (!has_metadata_uuid) {\n\t\t\t/*\n\t\t\t * When we have an image which has CHANGING_FSID_V2 set\n\t\t\t * it might belong to either a filesystem which has\n\t\t\t * disks with completed fsid change or it might belong\n\t\t\t * to fs with no UUID changes in effect, handle both.\n\t\t\t */\n\t\t\tfs_devices = find_fsid_inprogress(disk_super);\n\t\t\tif (!fs_devices)\n\t\t\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t\t} else {\n\t\t\tfs_devices = find_fsid_changed(disk_super);\n\t\t}\n\t} else if (has_metadata_uuid) {\n\t\tfs_devices = find_fsid(disk_super->fsid,\n\t\t\t\t       disk_super->metadata_uuid);\n\t} else {\n\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t}\n\n\n\tif (!fs_devices) {\n\t\tif (has_metadata_uuid)\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid,\n\t\t\t\t\t\t      disk_super->metadata_uuid);\n\t\telse\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid, NULL);\n\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn ERR_CAST(fs_devices);\n\n\t\tfs_devices->fsid_change = fsid_change_in_progress;\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_add(&fs_devices->fs_list, &fs_uuids);\n\n\t\tdevice = NULL;\n\t} else {\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tdevice = btrfs_find_device(fs_devices, devid,\n\t\t\t\tdisk_super->dev_item.uuid, NULL, false);\n\n\t\t/*\n\t\t * If this disk has been pulled into an fs devices created by\n\t\t * a device which had the CHANGING_FSID_V2 flag then replace the\n\t\t * metadata_uuid/fsid values of the fs_devices.\n\t\t */\n\t\tif (has_metadata_uuid && fs_devices->fsid_change &&\n\t\t    found_transid > fs_devices->latest_generation) {\n\t\t\tmemcpy(fs_devices->fsid, disk_super->fsid,\n\t\t\t\t\tBTRFS_FSID_SIZE);\n\t\t\tmemcpy(fs_devices->metadata_uuid,\n\t\t\t\t\tdisk_super->metadata_uuid, BTRFS_FSID_SIZE);\n\n\t\t\tfs_devices->fsid_change = false;\n\t\t}\n\t}\n\n\tif (!device) {\n\t\tif (fs_devices->opened) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EBUSY);\n\t\t}\n\n\t\tdevice = btrfs_alloc_device(NULL, &devid,\n\t\t\t\t\t    disk_super->dev_item.uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t/* we can safely leave the fs_devices entry around */\n\t\t\treturn device;\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tbtrfs_free_device(device);\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_assign_pointer(device->name, name);\n\n\t\tlist_add_rcu(&device->dev_list, &fs_devices->devices);\n\t\tfs_devices->num_devices++;\n\n\t\tdevice->fs_devices = fs_devices;\n\t\t*new_device_added = true;\n\n\t\tif (disk_super->label[0])\n\t\t\tpr_info(\"BTRFS: device label %s devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->label, devid, found_transid, path);\n\t\telse\n\t\t\tpr_info(\"BTRFS: device fsid %pU devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->fsid, devid, found_transid, path);\n\n\t} else if (!device->name || strcmp(device->name->str, path)) {\n\t\t/*\n\t\t * When FS is already mounted.\n\t\t * 1. If you are here and if the device->name is NULL that\n\t\t *    means this device was missing at time of FS mount.\n\t\t * 2. If you are here and if the device->name is different\n\t\t *    from 'path' that means either\n\t\t *      a. The same device disappeared and reappeared with\n\t\t *         different name. or\n\t\t *      b. The missing-disk-which-was-replaced, has\n\t\t *         reappeared now.\n\t\t *\n\t\t * We must allow 1 and 2a above. But 2b would be a spurious\n\t\t * and unintentional.\n\t\t *\n\t\t * Further in case of 1 and 2a above, the disk at 'path'\n\t\t * would have missed some transaction when it was away and\n\t\t * in case of 2a the stale bdev has to be updated as well.\n\t\t * 2b must not be allowed at all time.\n\t\t */\n\n\t\t/*\n\t\t * For now, we do allow update to btrfs_fs_device through the\n\t\t * btrfs dev scan cli after FS has been mounted.  We're still\n\t\t * tracking a problem where systems fail mount by subvolume id\n\t\t * when we reject replacement on a mounted FS.\n\t\t */\n\t\tif (!fs_devices->opened && found_transid < device->generation) {\n\t\t\t/*\n\t\t\t * That is if the FS is _not_ mounted and if you\n\t\t\t * are here, that means there is more than one\n\t\t\t * disk with same uuid and devid.We keep the one\n\t\t\t * with larger generation number or the last-in if\n\t\t\t * generation are equal.\n\t\t\t */\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EEXIST);\n\t\t}\n\n\t\t/*\n\t\t * We are going to replace the device path for a given devid,\n\t\t * make sure it's the same device if the device is mounted\n\t\t */\n\t\tif (device->bdev) {\n\t\t\tstruct block_device *path_bdev;\n\n\t\t\tpath_bdev = lookup_bdev(path);\n\t\t\tif (IS_ERR(path_bdev)) {\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\treturn ERR_CAST(path_bdev);\n\t\t\t}\n\n\t\t\tif (device->bdev != path_bdev) {\n\t\t\t\tbdput(path_bdev);\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\tbtrfs_warn_in_rcu(device->fs_info,\n\t\t\t\"duplicate device fsid:devid for %pU:%llu old:%s new:%s\",\n\t\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\t\trcu_str_deref(device->name), path);\n\t\t\t\treturn ERR_PTR(-EEXIST);\n\t\t\t}\n\t\t\tbdput(path_bdev);\n\t\t\tbtrfs_info_in_rcu(device->fs_info,\n\t\t\t\t\"device fsid %pU devid %llu moved old:%s new:%s\",\n\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\trcu_str_deref(device->name), path);\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_string_free(device->name);\n\t\trcu_assign_pointer(device->name, name);\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\tfs_devices->missing_devices--;\n\t\t\tclear_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\t}\n\n\t/*\n\t * Unmount does not free the btrfs_device struct but would zero\n\t * generation along with most of the other members. So just update\n\t * it back. We need it to pick the disk with largest generation\n\t * (as above).\n\t */\n\tif (!fs_devices->opened) {\n\t\tdevice->generation = found_transid;\n\t\tfs_devices->latest_generation = max_t(u64, found_transid,\n\t\t\t\t\t\tfs_devices->latest_generation);\n\t}\n\n\tfs_devices->total_devices = btrfs_super_num_devices(disk_super);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\treturn device;\n}",
                        "cve_id": "CVE-2021-3739"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static noinline struct btrfs_device *device_list_add(const char *path,\n\t\t\t   struct btrfs_super_block *disk_super,\n\t\t\t   bool *new_device_added)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *fs_devices = NULL;\n\tstruct rcu_string *name;\n\tu64 found_transid = btrfs_super_generation(disk_super);\n\tu64 devid = btrfs_stack_device_id(&disk_super->dev_item);\n\tbool has_metadata_uuid = (btrfs_super_incompat_flags(disk_super) &\n\t\tBTRFS_FEATURE_INCOMPAT_METADATA_UUID);\n\tbool fsid_change_in_progress = (btrfs_super_flags(disk_super) &\n\t\t\t\t\tBTRFS_SUPER_FLAG_CHANGING_FSID_V2);\n\n\tif (fsid_change_in_progress) {\n\t\tif (!has_metadata_uuid) {\n\t\t\t/*\n\t\t\t * When we have an image which has CHANGING_FSID_V2 set\n\t\t\t * it might belong to either a filesystem which has\n\t\t\t * disks with completed fsid change or it might belong\n\t\t\t * to fs with no UUID changes in effect, handle both.\n\t\t\t */\n\t\t\tfs_devices = find_fsid_inprogress(disk_super);\n\t\t\tif (!fs_devices)\n\t\t\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t\t} else {\n\t\t\tfs_devices = find_fsid_changed(disk_super);\n\t\t}\n\t} else if (has_metadata_uuid) {\n\t\tfs_devices = find_fsid(disk_super->fsid,\n\t\t\t\t       disk_super->metadata_uuid);\n\t} else {\n\t\tfs_devices = find_fsid(disk_super->fsid, NULL);\n\t}\n\n\n\tif (!fs_devices) {\n\t\tif (has_metadata_uuid)\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid,\n\t\t\t\t\t\t      disk_super->metadata_uuid);\n\t\telse\n\t\t\tfs_devices = alloc_fs_devices(disk_super->fsid, NULL);\n\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn ERR_CAST(fs_devices);\n\n\t\tfs_devices->fsid_change = fsid_change_in_progress;\n\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tlist_add(&fs_devices->fs_list, &fs_uuids);\n\n\t\tdevice = NULL;\n\t} else {\n\t\tmutex_lock(&fs_devices->device_list_mutex);\n\t\tdevice = find_device(fs_devices, devid,\n\t\t\t\tdisk_super->dev_item.uuid);\n\n\t\t/*\n\t\t * If this disk has been pulled into an fs devices created by\n\t\t * a device which had the CHANGING_FSID_V2 flag then replace the\n\t\t * metadata_uuid/fsid values of the fs_devices.\n\t\t */\n\t\tif (has_metadata_uuid && fs_devices->fsid_change &&\n\t\t    found_transid > fs_devices->latest_generation) {\n\t\t\tmemcpy(fs_devices->fsid, disk_super->fsid,\n\t\t\t\t\tBTRFS_FSID_SIZE);\n\t\t\tmemcpy(fs_devices->metadata_uuid,\n\t\t\t\t\tdisk_super->metadata_uuid, BTRFS_FSID_SIZE);\n\n\t\t\tfs_devices->fsid_change = false;\n\t\t}\n\t}\n\n\tif (!device) {\n\t\tif (fs_devices->opened) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EBUSY);\n\t\t}\n\n\t\tdevice = btrfs_alloc_device(NULL, &devid,\n\t\t\t\t\t    disk_super->dev_item.uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t/* we can safely leave the fs_devices entry around */\n\t\t\treturn device;\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tbtrfs_free_device(device);\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_assign_pointer(device->name, name);\n\n\t\tlist_add_rcu(&device->dev_list, &fs_devices->devices);\n\t\tfs_devices->num_devices++;\n\n\t\tdevice->fs_devices = fs_devices;\n\t\t*new_device_added = true;\n\n\t\tif (disk_super->label[0])\n\t\t\tpr_info(\"BTRFS: device label %s devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->label, devid, found_transid, path);\n\t\telse\n\t\t\tpr_info(\"BTRFS: device fsid %pU devid %llu transid %llu %s\\n\",\n\t\t\t\tdisk_super->fsid, devid, found_transid, path);\n\n\t} else if (!device->name || strcmp(device->name->str, path)) {\n\t\t/*\n\t\t * When FS is already mounted.\n\t\t * 1. If you are here and if the device->name is NULL that\n\t\t *    means this device was missing at time of FS mount.\n\t\t * 2. If you are here and if the device->name is different\n\t\t *    from 'path' that means either\n\t\t *      a. The same device disappeared and reappeared with\n\t\t *         different name. or\n\t\t *      b. The missing-disk-which-was-replaced, has\n\t\t *         reappeared now.\n\t\t *\n\t\t * We must allow 1 and 2a above. But 2b would be a spurious\n\t\t * and unintentional.\n\t\t *\n\t\t * Further in case of 1 and 2a above, the disk at 'path'\n\t\t * would have missed some transaction when it was away and\n\t\t * in case of 2a the stale bdev has to be updated as well.\n\t\t * 2b must not be allowed at all time.\n\t\t */\n\n\t\t/*\n\t\t * For now, we do allow update to btrfs_fs_device through the\n\t\t * btrfs dev scan cli after FS has been mounted.  We're still\n\t\t * tracking a problem where systems fail mount by subvolume id\n\t\t * when we reject replacement on a mounted FS.\n\t\t */\n\t\tif (!fs_devices->opened && found_transid < device->generation) {\n\t\t\t/*\n\t\t\t * That is if the FS is _not_ mounted and if you\n\t\t\t * are here, that means there is more than one\n\t\t\t * disk with same uuid and devid.We keep the one\n\t\t\t * with larger generation number or the last-in if\n\t\t\t * generation are equal.\n\t\t\t */\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-EEXIST);\n\t\t}\n\n\t\t/*\n\t\t * We are going to replace the device path for a given devid,\n\t\t * make sure it's the same device if the device is mounted\n\t\t */\n\t\tif (device->bdev) {\n\t\t\tstruct block_device *path_bdev;\n\n\t\t\tpath_bdev = lookup_bdev(path);\n\t\t\tif (IS_ERR(path_bdev)) {\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\treturn ERR_CAST(path_bdev);\n\t\t\t}\n\n\t\t\tif (device->bdev != path_bdev) {\n\t\t\t\tbdput(path_bdev);\n\t\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\t\tbtrfs_warn_in_rcu(device->fs_info,\n\t\t\t\"duplicate device fsid:devid for %pU:%llu old:%s new:%s\",\n\t\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\t\trcu_str_deref(device->name), path);\n\t\t\t\treturn ERR_PTR(-EEXIST);\n\t\t\t}\n\t\t\tbdput(path_bdev);\n\t\t\tbtrfs_info_in_rcu(device->fs_info,\n\t\t\t\t\"device fsid %pU devid %llu moved old:%s new:%s\",\n\t\t\t\tdisk_super->fsid, devid,\n\t\t\t\trcu_str_deref(device->name), path);\n\t\t}\n\n\t\tname = rcu_string_strdup(path, GFP_NOFS);\n\t\tif (!name) {\n\t\t\tmutex_unlock(&fs_devices->device_list_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trcu_string_free(device->name);\n\t\trcu_assign_pointer(device->name, name);\n\t\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\tfs_devices->missing_devices--;\n\t\t\tclear_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\t}\n\n\t/*\n\t * Unmount does not free the btrfs_device struct but would zero\n\t * generation along with most of the other members. So just update\n\t * it back. We need it to pick the disk with largest generation\n\t * (as above).\n\t */\n\tif (!fs_devices->opened) {\n\t\tdevice->generation = found_transid;\n\t\tfs_devices->latest_generation = max_t(u64, found_transid,\n\t\t\t\t\t\tfs_devices->latest_generation);\n\t}\n\n\tfs_devices->total_devices = btrfs_super_num_devices(disk_super);\n\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\treturn device;\n}",
                        "code_after_change": "int btrfs_rm_device(struct btrfs_fs_info *fs_info, const char *device_path,\n\t\t    u64 devid)\n{\n\tstruct btrfs_device *device;\n\tstruct btrfs_fs_devices *cur_devices;\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tu64 num_devices;\n\tint ret = 0;\n\n\tmutex_lock(&uuid_mutex);\n\n\tnum_devices = btrfs_num_devices(fs_info);\n\n\tret = btrfs_check_raid_min_devices(fs_info, num_devices - 1);\n\tif (ret)\n\t\tgoto out;\n\n\tdevice = btrfs_find_device_by_devspec(fs_info, devid, device_path);\n\n\tif (IS_ERR(device)) {\n\t\tif (PTR_ERR(device) == -ENOENT &&\n\t\t    device_path && strcmp(device_path, \"missing\") == 0)\n\t\t\tret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND;\n\t\telse\n\t\t\tret = PTR_ERR(device);\n\t\tgoto out;\n\t}\n\n\tif (btrfs_pinned_by_swapfile(fs_info, device)) {\n\t\tbtrfs_warn_in_rcu(fs_info,\n\t\t  \"cannot remove device %s (devid %llu) due to active swapfile\",\n\t\t\t\t  rcu_str_deref(device->name), device->devid);\n\t\tret = -ETXTBSY;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tret = BTRFS_ERROR_DEV_TGT_REPLACE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t    fs_info->fs_devices->rw_devices == 1) {\n\t\tret = BTRFS_ERROR_DEV_ONLY_WRITABLE;\n\t\tgoto out;\n\t}\n\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_del_init(&device->dev_alloc_list);\n\t\tdevice->fs_devices->rw_devices--;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\n\tmutex_unlock(&uuid_mutex);\n\tret = btrfs_shrink_device(device, 0);\n\tif (!ret)\n\t\tbtrfs_reada_remove_dev(device);\n\tmutex_lock(&uuid_mutex);\n\tif (ret)\n\t\tgoto error_undo;\n\n\t/*\n\t * TODO: the superblock still includes this device in its num_devices\n\t * counter although write_all_supers() is not locked out. This\n\t * could give a filesystem state which requires a degraded mount.\n\t */\n\tret = btrfs_rm_dev_item(device);\n\tif (ret)\n\t\tgoto error_undo;\n\n\tclear_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tbtrfs_scrub_cancel_dev(device);\n\n\t/*\n\t * the device list mutex makes sure that we don't change\n\t * the device list while someone else is writing out all\n\t * the device supers. Whoever is writing all supers, should\n\t * lock the device list mutex before getting the number of\n\t * devices in the super block (super_copy). Conversely,\n\t * whoever updates the number of devices in the super block\n\t * (super_copy) should hold the device list mutex.\n\t */\n\n\t/*\n\t * In normal cases the cur_devices == fs_devices. But in case\n\t * of deleting a seed device, the cur_devices should point to\n\t * its own fs_devices listed under the fs_devices->seed.\n\t */\n\tcur_devices = device->fs_devices;\n\tmutex_lock(&fs_devices->device_list_mutex);\n\tlist_del_rcu(&device->dev_list);\n\n\tcur_devices->num_devices--;\n\tcur_devices->total_devices--;\n\t/* Update total_devices of the parent fs_devices if it's seed */\n\tif (cur_devices != fs_devices)\n\t\tfs_devices->total_devices--;\n\n\tif (test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state))\n\t\tcur_devices->missing_devices--;\n\n\tbtrfs_assign_next_active_device(device, NULL);\n\n\tif (device->bdev) {\n\t\tcur_devices->open_devices--;\n\t\t/* remove sysfs entry */\n\t\tbtrfs_sysfs_remove_device(device);\n\t}\n\n\tnum_devices = btrfs_super_num_devices(fs_info->super_copy) - 1;\n\tbtrfs_set_super_num_devices(fs_info->super_copy, num_devices);\n\tmutex_unlock(&fs_devices->device_list_mutex);\n\n\t/*\n\t * at this point, the device is zero sized and detached from\n\t * the devices list.  All that's left is to zero out the old\n\t * supers and free the device.\n\t */\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state))\n\t\tbtrfs_scratch_superblocks(fs_info, device->bdev,\n\t\t\t\t\t  device->name->str);\n\n\tbtrfs_close_bdev(device);\n\tsynchronize_rcu();\n\tbtrfs_free_device(device);\n\n\tif (cur_devices->open_devices == 0) {\n\t\tlist_del_init(&cur_devices->seed_list);\n\t\tclose_fs_devices(cur_devices);\n\t\tfree_fs_devices(cur_devices);\n\t}\n\nout:\n\tmutex_unlock(&uuid_mutex);\n\treturn ret;\n\nerror_undo:\n\tbtrfs_reada_undo_remove_dev(device);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state)) {\n\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\tlist_add(&device->dev_alloc_list,\n\t\t\t &fs_devices->alloc_list);\n\t\tdevice->fs_devices->rw_devices++;\n\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t}\n\tgoto out;\n}",
                        "cve_id": "CVE-2019-18885"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2221,
            "cve_id": "CVE-2019-18885",
            "code_snippet": "static int read_one_dev(struct btrfs_fs_info *fs_info,\n\t\t\tstruct extent_buffer *leaf,\n\t\t\tstruct btrfs_dev_item *dev_item)\n{\n\tstruct btrfs_fs_devices *fs_devices = fs_info->fs_devices;\n\tstruct btrfs_device *device;\n\tu64 devid;\n\tint ret;\n\tu8 fs_uuid[BTRFS_FSID_SIZE];\n\tu8 dev_uuid[BTRFS_UUID_SIZE];\n\n\tdevid = btrfs_device_id(leaf, dev_item);\n\tread_extent_buffer(leaf, dev_uuid, btrfs_device_uuid(dev_item),\n\t\t\t   BTRFS_UUID_SIZE);\n\tread_extent_buffer(leaf, fs_uuid, btrfs_device_fsid(dev_item),\n\t\t\t   BTRFS_FSID_SIZE);\n\n\tif (memcmp(fs_uuid, fs_devices->metadata_uuid, BTRFS_FSID_SIZE)) {\n\t\tfs_devices = open_seed_devices(fs_info, fs_uuid);\n\t\tif (IS_ERR(fs_devices))\n\t\t\treturn PTR_ERR(fs_devices);\n\t}\n\n\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t   fs_uuid, true);\n\tif (!device) {\n\t\tif (!btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\tbtrfs_report_missing_device(fs_info, devid,\n\t\t\t\t\t\t\tdev_uuid, true);\n\t\t\treturn -ENOENT;\n\t\t}\n\n\t\tdevice = add_missing_dev(fs_devices, devid, dev_uuid);\n\t\tif (IS_ERR(device)) {\n\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\"failed to add missing dev %llu: %ld\",\n\t\t\t\tdevid, PTR_ERR(device));\n\t\t\treturn PTR_ERR(device);\n\t\t}\n\t\tbtrfs_report_missing_device(fs_info, devid, dev_uuid, false);\n\t} else {\n\t\tif (!device->bdev) {\n\t\t\tif (!btrfs_test_opt(fs_info, DEGRADED)) {\n\t\t\t\tbtrfs_report_missing_device(fs_info,\n\t\t\t\t\t\tdevid, dev_uuid, true);\n\t\t\t\treturn -ENOENT;\n\t\t\t}\n\t\t\tbtrfs_report_missing_device(fs_info, devid,\n\t\t\t\t\t\t\tdev_uuid, false);\n\t\t}\n\n\t\tif (!device->bdev &&\n\t\t    !test_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state)) {\n\t\t\t/*\n\t\t\t * this happens when a device that was properly setup\n\t\t\t * in the device info lists suddenly goes bad.\n\t\t\t * device->bdev is NULL, and so we have to set\n\t\t\t * device->missing to one here\n\t\t\t */\n\t\t\tdevice->fs_devices->missing_devices++;\n\t\t\tset_bit(BTRFS_DEV_STATE_MISSING, &device->dev_state);\n\t\t}\n\n\t\t/* Move the device to its own fs_devices */\n\t\tif (device->fs_devices != fs_devices) {\n\t\t\tASSERT(test_bit(BTRFS_DEV_STATE_MISSING,\n\t\t\t\t\t\t\t&device->dev_state));\n\n\t\t\tlist_move(&device->dev_list, &fs_devices->devices);\n\t\t\tdevice->fs_devices->num_devices--;\n\t\t\tfs_devices->num_devices++;\n\n\t\t\tdevice->fs_devices->missing_devices--;\n\t\t\tfs_devices->missing_devices++;\n\n\t\t\tdevice->fs_devices = fs_devices;\n\t\t}\n\t}\n\n\tif (device->fs_devices != fs_info->fs_devices) {\n\t\tBUG_ON(test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state));\n\t\tif (device->generation !=\n\t\t    btrfs_device_generation(leaf, dev_item))\n\t\t\treturn -EINVAL;\n\t}\n\n\tfill_device_from_item(leaf, dev_item, device);\n\tset_bit(BTRFS_DEV_STATE_IN_FS_METADATA, &device->dev_state);\n\tif (test_bit(BTRFS_DEV_STATE_WRITEABLE, &device->dev_state) &&\n\t   !test_bit(BTRFS_DEV_STATE_REPLACE_TGT, &device->dev_state)) {\n\t\tdevice->fs_devices->total_rw_bytes += device->total_bytes;\n\t\tatomic64_add(device->total_bytes - device->bytes_used,\n\t\t\t\t&fs_info->free_chunk_space);\n\t}\n\tret = 0;\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int btrfs_finish_sprout(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_dev_item *dev_item;\n\tstruct btrfs_device *device;\n\tstruct btrfs_key key;\n\tu8 fs_uuid[BTRFS_FSID_SIZE];\n\tu8 dev_uuid[BTRFS_UUID_SIZE];\n\tu64 devid;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.offset = 0;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\n\twhile (1) {\n\t\tret = btrfs_search_slot(trans, root, &key, path, 0, 1);\n\t\tif (ret < 0)\n\t\t\tgoto error;\n\n\t\tleaf = path->nodes[0];\nnext_slot:\n\t\tif (path->slots[0] >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret > 0)\n\t\t\t\tbreak;\n\t\t\tif (ret < 0)\n\t\t\t\tgoto error;\n\t\t\tleaf = path->nodes[0];\n\t\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\t\tbtrfs_release_path(path);\n\t\t\tcontinue;\n\t\t}\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\tif (key.objectid != BTRFS_DEV_ITEMS_OBJECTID ||\n\t\t    key.type != BTRFS_DEV_ITEM_KEY)\n\t\t\tbreak;\n\n\t\tdev_item = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\t  struct btrfs_dev_item);\n\t\tdevid = btrfs_device_id(leaf, dev_item);\n\t\tread_extent_buffer(leaf, dev_uuid, btrfs_device_uuid(dev_item),\n\t\t\t\t   BTRFS_UUID_SIZE);\n\t\tread_extent_buffer(leaf, fs_uuid, btrfs_device_fsid(dev_item),\n\t\t\t\t   BTRFS_FSID_SIZE);\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t\t   fs_uuid);\n\t\tBUG_ON(!device); /* Logic error */\n\n\t\tif (device->fs_devices->seeding) {\n\t\t\tbtrfs_set_device_generation(leaf, dev_item,\n\t\t\t\t\t\t    device->generation);\n\t\t\tbtrfs_mark_buffer_dirty(leaf);\n\t\t}\n\n\t\tpath->slots[0]++;\n\t\tgoto next_slot;\n\t}\n\tret = 0;\nerror:\n\tbtrfs_free_path(path);\n\treturn ret;\n}",
                        "code_after_change": "static int btrfs_finish_sprout(struct btrfs_trans_handle *trans,\n\t\t\t       struct btrfs_fs_info *fs_info)\n{\n\tstruct btrfs_root *root = fs_info->chunk_root;\n\tstruct btrfs_path *path;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_dev_item *dev_item;\n\tstruct btrfs_device *device;\n\tstruct btrfs_key key;\n\tu8 fs_uuid[BTRFS_FSID_SIZE];\n\tu8 dev_uuid[BTRFS_UUID_SIZE];\n\tu64 devid;\n\tint ret;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tkey.objectid = BTRFS_DEV_ITEMS_OBJECTID;\n\tkey.offset = 0;\n\tkey.type = BTRFS_DEV_ITEM_KEY;\n\n\twhile (1) {\n\t\tret = btrfs_search_slot(trans, root, &key, path, 0, 1);\n\t\tif (ret < 0)\n\t\t\tgoto error;\n\n\t\tleaf = path->nodes[0];\nnext_slot:\n\t\tif (path->slots[0] >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret > 0)\n\t\t\t\tbreak;\n\t\t\tif (ret < 0)\n\t\t\t\tgoto error;\n\t\t\tleaf = path->nodes[0];\n\t\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\t\tbtrfs_release_path(path);\n\t\t\tcontinue;\n\t\t}\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, path->slots[0]);\n\t\tif (key.objectid != BTRFS_DEV_ITEMS_OBJECTID ||\n\t\t    key.type != BTRFS_DEV_ITEM_KEY)\n\t\t\tbreak;\n\n\t\tdev_item = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t\t  struct btrfs_dev_item);\n\t\tdevid = btrfs_device_id(leaf, dev_item);\n\t\tread_extent_buffer(leaf, dev_uuid, btrfs_device_uuid(dev_item),\n\t\t\t\t   BTRFS_UUID_SIZE);\n\t\tread_extent_buffer(leaf, fs_uuid, btrfs_device_fsid(dev_item),\n\t\t\t\t   BTRFS_FSID_SIZE);\n\t\tdevice = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,\n\t\t\t\t\t   fs_uuid, true);\n\t\tBUG_ON(!device); /* Logic error */\n\n\t\tif (device->fs_devices->seeding) {\n\t\t\tbtrfs_set_device_generation(leaf, dev_item,\n\t\t\t\t\t\t    device->generation);\n\t\t\tbtrfs_mark_buffer_dirty(leaf);\n\t\t}\n\n\t\tpath->slots[0]++;\n\t\tgoto next_slot;\n\t}\n\tret = 0;\nerror:\n\tbtrfs_free_path(path);\n\treturn ret;\n}",
                        "cve_id": "CVE-2019-18885"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2229,
            "cve_id": "CVE-2019-19036",
            "code_snippet": "static int check_leaf(struct extent_buffer *leaf, bool check_item_data)\n{\n\tstruct btrfs_fs_info *fs_info = leaf->fs_info;\n\t/* No valid key type is 0, so all key should be larger than this key */\n\tstruct btrfs_key prev_key = {0, 0, 0};\n\tstruct btrfs_key key;\n\tu32 nritems = btrfs_header_nritems(leaf);\n\tint slot;\n\n\tif (btrfs_header_level(leaf) != 0) {\n\t\tgeneric_err(leaf, 0,\n\t\t\t\"invalid level for leaf, have %d expect 0\",\n\t\t\tbtrfs_header_level(leaf));\n\t\treturn -EUCLEAN;\n\t}\n\n\t/*\n\t * Extent buffers from a relocation tree have a owner field that\n\t * corresponds to the subvolume tree they are based on. So just from an\n\t * extent buffer alone we can not find out what is the id of the\n\t * corresponding subvolume tree, so we can not figure out if the extent\n\t * buffer corresponds to the root of the relocation tree or not. So\n\t * skip this check for relocation trees.\n\t */\n\tif (nritems == 0 && !btrfs_header_flag(leaf, BTRFS_HEADER_FLAG_RELOC)) {\n\t\tu64 owner = btrfs_header_owner(leaf);\n\n\t\t/* These trees must never be empty */\n\t\tif (owner == BTRFS_ROOT_TREE_OBJECTID ||\n\t\t    owner == BTRFS_CHUNK_TREE_OBJECTID ||\n\t\t    owner == BTRFS_EXTENT_TREE_OBJECTID ||\n\t\t    owner == BTRFS_DEV_TREE_OBJECTID ||\n\t\t    owner == BTRFS_FS_TREE_OBJECTID ||\n\t\t    owner == BTRFS_DATA_RELOC_TREE_OBJECTID) {\n\t\t\tgeneric_err(leaf, 0,\n\t\t\t\"invalid root, root %llu must never be empty\",\n\t\t\t\t    owner);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\t\t/* Unknown tree */\n\t\tif (owner == 0) {\n\t\t\tgeneric_err(leaf, 0,\n\t\t\t\t\"invalid owner, root 0 is not defined\");\n\t\t\treturn -EUCLEAN;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (nritems == 0)\n\t\treturn 0;\n\n\t/*\n\t * Check the following things to make sure this is a good leaf, and\n\t * leaf users won't need to bother with similar sanity checks:\n\t *\n\t * 1) key ordering\n\t * 2) item offset and size\n\t *    No overlap, no hole, all inside the leaf.\n\t * 3) item content\n\t *    If possible, do comprehensive sanity check.\n\t *    NOTE: All checks must only rely on the item data itself.\n\t */\n\tfor (slot = 0; slot < nritems; slot++) {\n\t\tu32 item_end_expected;\n\t\tint ret;\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\n\t\t/* Make sure the keys are in the right order */\n\t\tif (btrfs_comp_cpu_keys(&prev_key, &key) >= 0) {\n\t\t\tgeneric_err(leaf, slot,\n\t\"bad key order, prev (%llu %u %llu) current (%llu %u %llu)\",\n\t\t\t\tprev_key.objectid, prev_key.type,\n\t\t\t\tprev_key.offset, key.objectid, key.type,\n\t\t\t\tkey.offset);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/*\n\t\t * Make sure the offset and ends are right, remember that the\n\t\t * item data starts at the end of the leaf and grows towards the\n\t\t * front.\n\t\t */\n\t\tif (slot == 0)\n\t\t\titem_end_expected = BTRFS_LEAF_DATA_SIZE(fs_info);\n\t\telse\n\t\t\titem_end_expected = btrfs_item_offset_nr(leaf,\n\t\t\t\t\t\t\t\t slot - 1);\n\t\tif (btrfs_item_end_nr(leaf, slot) != item_end_expected) {\n\t\t\tgeneric_err(leaf, slot,\n\t\t\t\t\"unexpected item end, have %u expect %u\",\n\t\t\t\tbtrfs_item_end_nr(leaf, slot),\n\t\t\t\titem_end_expected);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/*\n\t\t * Check to make sure that we don't point outside of the leaf,\n\t\t * just in case all the items are consistent to each other, but\n\t\t * all point outside of the leaf.\n\t\t */\n\t\tif (btrfs_item_end_nr(leaf, slot) >\n\t\t    BTRFS_LEAF_DATA_SIZE(fs_info)) {\n\t\t\tgeneric_err(leaf, slot,\n\t\t\t\"slot end outside of leaf, have %u expect range [0, %u]\",\n\t\t\t\tbtrfs_item_end_nr(leaf, slot),\n\t\t\t\tBTRFS_LEAF_DATA_SIZE(fs_info));\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/* Also check if the item pointer overlaps with btrfs item. */\n\t\tif (btrfs_item_nr_offset(slot) + sizeof(struct btrfs_item) >\n\t\t    btrfs_item_ptr_offset(leaf, slot)) {\n\t\t\tgeneric_err(leaf, slot,\n\t\t\"slot overlaps with its data, item end %lu data start %lu\",\n\t\t\t\tbtrfs_item_nr_offset(slot) +\n\t\t\t\tsizeof(struct btrfs_item),\n\t\t\t\tbtrfs_item_ptr_offset(leaf, slot));\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\tif (check_item_data) {\n\t\t\t/*\n\t\t\t * Check if the item size and content meet other\n\t\t\t * criteria\n\t\t\t */\n\t\t\tret = check_leaf_item(leaf, &key, slot, &prev_key);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tprev_key.objectid = key.objectid;\n\t\tprev_key.type = key.type;\n\t\tprev_key.offset = key.offset;\n\t}\n\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int check_leaf(struct btrfs_fs_info *fs_info, struct extent_buffer *leaf,\n\t\t      bool check_item_data)\n{\n\t/* No valid key type is 0, so all key should be larger than this key */\n\tstruct btrfs_key prev_key = {0, 0, 0};\n\tstruct btrfs_key key;\n\tu32 nritems = btrfs_header_nritems(leaf);\n\tint slot;\n\n\t/*\n\t * Extent buffers from a relocation tree have a owner field that\n\t * corresponds to the subvolume tree they are based on. So just from an\n\t * extent buffer alone we can not find out what is the id of the\n\t * corresponding subvolume tree, so we can not figure out if the extent\n\t * buffer corresponds to the root of the relocation tree or not. So\n\t * skip this check for relocation trees.\n\t */\n\tif (nritems == 0 && !btrfs_header_flag(leaf, BTRFS_HEADER_FLAG_RELOC)) {\n\t\tstruct btrfs_root *check_root;\n\n\t\tkey.objectid = btrfs_header_owner(leaf);\n\t\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\t\tkey.offset = (u64)-1;\n\n\t\tcheck_root = btrfs_get_fs_root(fs_info, &key, false);\n\t\t/*\n\t\t * The only reason we also check NULL here is that during\n\t\t * open_ctree() some roots has not yet been set up.\n\t\t */\n\t\tif (!IS_ERR_OR_NULL(check_root)) {\n\t\t\tstruct extent_buffer *eb;\n\n\t\t\teb = btrfs_root_node(check_root);\n\t\t\t/* if leaf is the root, then it's fine */\n\t\t\tif (leaf != eb) {\n\t\t\t\tgeneric_err(fs_info, leaf, 0,\n\t\t\"invalid nritems, have %u should not be 0 for non-root leaf\",\n\t\t\t\t\tnritems);\n\t\t\t\tfree_extent_buffer(eb);\n\t\t\t\treturn -EUCLEAN;\n\t\t\t}\n\t\t\tfree_extent_buffer(eb);\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (nritems == 0)\n\t\treturn 0;\n\n\t/*\n\t * Check the following things to make sure this is a good leaf, and\n\t * leaf users won't need to bother with similar sanity checks:\n\t *\n\t * 1) key ordering\n\t * 2) item offset and size\n\t *    No overlap, no hole, all inside the leaf.\n\t * 3) item content\n\t *    If possible, do comprehensive sanity check.\n\t *    NOTE: All checks must only rely on the item data itself.\n\t */\n\tfor (slot = 0; slot < nritems; slot++) {\n\t\tu32 item_end_expected;\n\t\tint ret;\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\n\t\t/* Make sure the keys are in the right order */\n\t\tif (btrfs_comp_cpu_keys(&prev_key, &key) >= 0) {\n\t\t\tgeneric_err(fs_info, leaf, slot,\n\t\"bad key order, prev (%llu %u %llu) current (%llu %u %llu)\",\n\t\t\t\tprev_key.objectid, prev_key.type,\n\t\t\t\tprev_key.offset, key.objectid, key.type,\n\t\t\t\tkey.offset);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/*\n\t\t * Make sure the offset and ends are right, remember that the\n\t\t * item data starts at the end of the leaf and grows towards the\n\t\t * front.\n\t\t */\n\t\tif (slot == 0)\n\t\t\titem_end_expected = BTRFS_LEAF_DATA_SIZE(fs_info);\n\t\telse\n\t\t\titem_end_expected = btrfs_item_offset_nr(leaf,\n\t\t\t\t\t\t\t\t slot - 1);\n\t\tif (btrfs_item_end_nr(leaf, slot) != item_end_expected) {\n\t\t\tgeneric_err(fs_info, leaf, slot,\n\t\t\t\t\"unexpected item end, have %u expect %u\",\n\t\t\t\tbtrfs_item_end_nr(leaf, slot),\n\t\t\t\titem_end_expected);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/*\n\t\t * Check to make sure that we don't point outside of the leaf,\n\t\t * just in case all the items are consistent to each other, but\n\t\t * all point outside of the leaf.\n\t\t */\n\t\tif (btrfs_item_end_nr(leaf, slot) >\n\t\t    BTRFS_LEAF_DATA_SIZE(fs_info)) {\n\t\t\tgeneric_err(fs_info, leaf, slot,\n\t\t\t\"slot end outside of leaf, have %u expect range [0, %u]\",\n\t\t\t\tbtrfs_item_end_nr(leaf, slot),\n\t\t\t\tBTRFS_LEAF_DATA_SIZE(fs_info));\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/* Also check if the item pointer overlaps with btrfs item. */\n\t\tif (btrfs_item_nr_offset(slot) + sizeof(struct btrfs_item) >\n\t\t    btrfs_item_ptr_offset(leaf, slot)) {\n\t\t\tgeneric_err(fs_info, leaf, slot,\n\t\t\"slot overlaps with its data, item end %lu data start %lu\",\n\t\t\t\tbtrfs_item_nr_offset(slot) +\n\t\t\t\tsizeof(struct btrfs_item),\n\t\t\t\tbtrfs_item_ptr_offset(leaf, slot));\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\tif (check_item_data) {\n\t\t\t/*\n\t\t\t * Check if the item size and content meet other\n\t\t\t * criteria\n\t\t\t */\n\t\t\tret = check_leaf_item(fs_info, leaf, &key, slot);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tprev_key.objectid = key.objectid;\n\t\tprev_key.type = key.type;\n\t\tprev_key.offset = key.offset;\n\t}\n\n\treturn 0;\n}",
                        "code_after_change": "static int check_leaf(struct btrfs_fs_info *fs_info, struct extent_buffer *leaf,\n\t\t      bool check_item_data)\n{\n\t/* No valid key type is 0, so all key should be larger than this key */\n\tstruct btrfs_key prev_key = {0, 0, 0};\n\tstruct btrfs_key key;\n\tu32 nritems = btrfs_header_nritems(leaf);\n\tint slot;\n\n\t/*\n\t * Extent buffers from a relocation tree have a owner field that\n\t * corresponds to the subvolume tree they are based on. So just from an\n\t * extent buffer alone we can not find out what is the id of the\n\t * corresponding subvolume tree, so we can not figure out if the extent\n\t * buffer corresponds to the root of the relocation tree or not. So\n\t * skip this check for relocation trees.\n\t */\n\tif (nritems == 0 && !btrfs_header_flag(leaf, BTRFS_HEADER_FLAG_RELOC)) {\n\t\tu64 owner = btrfs_header_owner(leaf);\n\t\tstruct btrfs_root *check_root;\n\n\t\t/* These trees must never be empty */\n\t\tif (owner == BTRFS_ROOT_TREE_OBJECTID ||\n\t\t    owner == BTRFS_CHUNK_TREE_OBJECTID ||\n\t\t    owner == BTRFS_EXTENT_TREE_OBJECTID ||\n\t\t    owner == BTRFS_DEV_TREE_OBJECTID ||\n\t\t    owner == BTRFS_FS_TREE_OBJECTID ||\n\t\t    owner == BTRFS_DATA_RELOC_TREE_OBJECTID) {\n\t\t\tgeneric_err(fs_info, leaf, 0,\n\t\t\t\"invalid root, root %llu must never be empty\",\n\t\t\t\t    owner);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\t\tkey.objectid = owner;\n\t\tkey.type = BTRFS_ROOT_ITEM_KEY;\n\t\tkey.offset = (u64)-1;\n\n\t\tcheck_root = btrfs_get_fs_root(fs_info, &key, false);\n\t\t/*\n\t\t * The only reason we also check NULL here is that during\n\t\t * open_ctree() some roots has not yet been set up.\n\t\t */\n\t\tif (!IS_ERR_OR_NULL(check_root)) {\n\t\t\tstruct extent_buffer *eb;\n\n\t\t\teb = btrfs_root_node(check_root);\n\t\t\t/* if leaf is the root, then it's fine */\n\t\t\tif (leaf != eb) {\n\t\t\t\tgeneric_err(fs_info, leaf, 0,\n\t\t\"invalid nritems, have %u should not be 0 for non-root leaf\",\n\t\t\t\t\tnritems);\n\t\t\t\tfree_extent_buffer(eb);\n\t\t\t\treturn -EUCLEAN;\n\t\t\t}\n\t\t\tfree_extent_buffer(eb);\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (nritems == 0)\n\t\treturn 0;\n\n\t/*\n\t * Check the following things to make sure this is a good leaf, and\n\t * leaf users won't need to bother with similar sanity checks:\n\t *\n\t * 1) key ordering\n\t * 2) item offset and size\n\t *    No overlap, no hole, all inside the leaf.\n\t * 3) item content\n\t *    If possible, do comprehensive sanity check.\n\t *    NOTE: All checks must only rely on the item data itself.\n\t */\n\tfor (slot = 0; slot < nritems; slot++) {\n\t\tu32 item_end_expected;\n\t\tint ret;\n\n\t\tbtrfs_item_key_to_cpu(leaf, &key, slot);\n\n\t\t/* Make sure the keys are in the right order */\n\t\tif (btrfs_comp_cpu_keys(&prev_key, &key) >= 0) {\n\t\t\tgeneric_err(fs_info, leaf, slot,\n\t\"bad key order, prev (%llu %u %llu) current (%llu %u %llu)\",\n\t\t\t\tprev_key.objectid, prev_key.type,\n\t\t\t\tprev_key.offset, key.objectid, key.type,\n\t\t\t\tkey.offset);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/*\n\t\t * Make sure the offset and ends are right, remember that the\n\t\t * item data starts at the end of the leaf and grows towards the\n\t\t * front.\n\t\t */\n\t\tif (slot == 0)\n\t\t\titem_end_expected = BTRFS_LEAF_DATA_SIZE(fs_info);\n\t\telse\n\t\t\titem_end_expected = btrfs_item_offset_nr(leaf,\n\t\t\t\t\t\t\t\t slot - 1);\n\t\tif (btrfs_item_end_nr(leaf, slot) != item_end_expected) {\n\t\t\tgeneric_err(fs_info, leaf, slot,\n\t\t\t\t\"unexpected item end, have %u expect %u\",\n\t\t\t\tbtrfs_item_end_nr(leaf, slot),\n\t\t\t\titem_end_expected);\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/*\n\t\t * Check to make sure that we don't point outside of the leaf,\n\t\t * just in case all the items are consistent to each other, but\n\t\t * all point outside of the leaf.\n\t\t */\n\t\tif (btrfs_item_end_nr(leaf, slot) >\n\t\t    BTRFS_LEAF_DATA_SIZE(fs_info)) {\n\t\t\tgeneric_err(fs_info, leaf, slot,\n\t\t\t\"slot end outside of leaf, have %u expect range [0, %u]\",\n\t\t\t\tbtrfs_item_end_nr(leaf, slot),\n\t\t\t\tBTRFS_LEAF_DATA_SIZE(fs_info));\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\t/* Also check if the item pointer overlaps with btrfs item. */\n\t\tif (btrfs_item_nr_offset(slot) + sizeof(struct btrfs_item) >\n\t\t    btrfs_item_ptr_offset(leaf, slot)) {\n\t\t\tgeneric_err(fs_info, leaf, slot,\n\t\t\"slot overlaps with its data, item end %lu data start %lu\",\n\t\t\t\tbtrfs_item_nr_offset(slot) +\n\t\t\t\tsizeof(struct btrfs_item),\n\t\t\t\tbtrfs_item_ptr_offset(leaf, slot));\n\t\t\treturn -EUCLEAN;\n\t\t}\n\n\t\tif (check_item_data) {\n\t\t\t/*\n\t\t\t * Check if the item size and content meet other\n\t\t\t * criteria\n\t\t\t */\n\t\t\tret = check_leaf_item(fs_info, leaf, &key, slot);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tprev_key.objectid = key.objectid;\n\t\tprev_key.type = key.type;\n\t\tprev_key.offset = key.offset;\n\t}\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2018-14612"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static bool assoc_array_insert_into_terminal_node(struct assoc_array_edit *edit,\n\t\t\t\t\t\t  const struct assoc_array_ops *ops,\n\t\t\t\t\t\t  const void *index_key,\n\t\t\t\t\t\t  struct assoc_array_walk_result *result)\n{\n\tstruct assoc_array_shortcut *shortcut, *new_s0;\n\tstruct assoc_array_node *node, *new_n0, *new_n1, *side;\n\tstruct assoc_array_ptr *ptr;\n\tunsigned long dissimilarity, base_seg, blank;\n\tsize_t keylen;\n\tbool have_meta;\n\tint level, diff;\n\tint slot, next_slot, free_slot, i, j;\n\n\tnode\t= result->terminal_node.node;\n\tlevel\t= result->terminal_node.level;\n\tedit->segment_cache[ASSOC_ARRAY_FAN_OUT] = result->terminal_node.slot;\n\n\tpr_devel(\"-->%s()\\n\", __func__);\n\n\t/* We arrived at a node which doesn't have an onward node or shortcut\n\t * pointer that we have to follow.  This means that (a) the leaf we\n\t * want must go here (either by insertion or replacement) or (b) we\n\t * need to split this node and insert in one of the fragments.\n\t */\n\tfree_slot = -1;\n\n\t/* Firstly, we have to check the leaves in this node to see if there's\n\t * a matching one we should replace in place.\n\t */\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tptr = node->slots[i];\n\t\tif (!ptr) {\n\t\t\tfree_slot = i;\n\t\t\tcontinue;\n\t\t}\n\t\tif (assoc_array_ptr_is_leaf(ptr) &&\n\t\t    ops->compare_object(assoc_array_ptr_to_leaf(ptr),\n\t\t\t\t\tindex_key)) {\n\t\t\tpr_devel(\"replace in slot %d\\n\", i);\n\t\t\tedit->leaf_p = &node->slots[i];\n\t\t\tedit->dead_leaf = node->slots[i];\n\t\t\tpr_devel(\"<--%s() = ok [replace]\\n\", __func__);\n\t\t\treturn true;\n\t\t}\n\t}\n\n\t/* If there is a free slot in this node then we can just insert the\n\t * leaf here.\n\t */\n\tif (free_slot >= 0) {\n\t\tpr_devel(\"insert in free slot %d\\n\", free_slot);\n\t\tedit->leaf_p = &node->slots[free_slot];\n\t\tedit->adjust_count_on = node;\n\t\tpr_devel(\"<--%s() = ok [insert]\\n\", __func__);\n\t\treturn true;\n\t}\n\n\t/* The node has no spare slots - so we're either going to have to split\n\t * it or insert another node before it.\n\t *\n\t * Whatever, we're going to need at least two new nodes - so allocate\n\t * those now.  We may also need a new shortcut, but we deal with that\n\t * when we need it.\n\t */\n\tnew_n0 = kzalloc(sizeof(struct assoc_array_node), GFP_KERNEL);\n\tif (!new_n0)\n\t\treturn false;\n\tedit->new_meta[0] = assoc_array_node_to_ptr(new_n0);\n\tnew_n1 = kzalloc(sizeof(struct assoc_array_node), GFP_KERNEL);\n\tif (!new_n1)\n\t\treturn false;\n\tedit->new_meta[1] = assoc_array_node_to_ptr(new_n1);\n\n\t/* We need to find out how similar the leaves are. */\n\tpr_devel(\"no spare slots\\n\");\n\thave_meta = false;\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tptr = node->slots[i];\n\t\tif (assoc_array_ptr_is_meta(ptr)) {\n\t\t\tedit->segment_cache[i] = 0xff;\n\t\t\thave_meta = true;\n\t\t\tcontinue;\n\t\t}\n\t\tbase_seg = ops->get_object_key_chunk(\n\t\t\tassoc_array_ptr_to_leaf(ptr), level);\n\t\tbase_seg >>= level & ASSOC_ARRAY_KEY_CHUNK_MASK;\n\t\tedit->segment_cache[i] = base_seg & ASSOC_ARRAY_FAN_MASK;\n\t}\n\n\tif (have_meta) {\n\t\tpr_devel(\"have meta\\n\");\n\t\tgoto split_node;\n\t}\n\n\t/* The node contains only leaves */\n\tdissimilarity = 0;\n\tbase_seg = edit->segment_cache[0];\n\tfor (i = 1; i < ASSOC_ARRAY_FAN_OUT; i++)\n\t\tdissimilarity |= edit->segment_cache[i] ^ base_seg;\n\n\tpr_devel(\"only leaves; dissimilarity=%lx\\n\", dissimilarity);\n\n\tif ((dissimilarity & ASSOC_ARRAY_FAN_MASK) == 0) {\n\t\t/* The old leaves all cluster in the same slot.  We will need\n\t\t * to insert a shortcut if the new node wants to cluster with them.\n\t\t */\n\t\tif ((edit->segment_cache[ASSOC_ARRAY_FAN_OUT] ^ base_seg) == 0)\n\t\t\tgoto all_leaves_cluster_together;\n\n\t\t/* Otherwise we can just insert a new node ahead of the old\n\t\t * one.\n\t\t */\n\t\tgoto present_leaves_cluster_but_not_new_leaf;\n\t}\n\nsplit_node:\n\tpr_devel(\"split node\\n\");\n\n\t/* We need to split the current node; we know that the node doesn't\n\t * simply contain a full set of leaves that cluster together (it\n\t * contains meta pointers and/or non-clustering leaves).\n\t *\n\t * We need to expel at least two leaves out of a set consisting of the\n\t * leaves in the node and the new leaf.\n\t *\n\t * We need a new node (n0) to replace the current one and a new node to\n\t * take the expelled nodes (n1).\n\t */\n\tedit->set[0].to = assoc_array_node_to_ptr(new_n0);\n\tnew_n0->back_pointer = node->back_pointer;\n\tnew_n0->parent_slot = node->parent_slot;\n\tnew_n1->back_pointer = assoc_array_node_to_ptr(new_n0);\n\tnew_n1->parent_slot = -1; /* Need to calculate this */\n\ndo_split_node:\n\tpr_devel(\"do_split_node\\n\");\n\n\tnew_n0->nr_leaves_on_branch = node->nr_leaves_on_branch;\n\tnew_n1->nr_leaves_on_branch = 0;\n\n\t/* Begin by finding two matching leaves.  There have to be at least two\n\t * that match - even if there are meta pointers - because any leaf that\n\t * would match a slot with a meta pointer in it must be somewhere\n\t * behind that meta pointer and cannot be here.  Further, given N\n\t * remaining leaf slots, we now have N+1 leaves to go in them.\n\t */\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tslot = edit->segment_cache[i];\n\t\tif (slot != 0xff)\n\t\t\tfor (j = i + 1; j < ASSOC_ARRAY_FAN_OUT + 1; j++)\n\t\t\t\tif (edit->segment_cache[j] == slot)\n\t\t\t\t\tgoto found_slot_for_multiple_occupancy;\n\t}\nfound_slot_for_multiple_occupancy:\n\tpr_devel(\"same slot: %x %x [%02x]\\n\", i, j, slot);\n\tBUG_ON(i >= ASSOC_ARRAY_FAN_OUT);\n\tBUG_ON(j >= ASSOC_ARRAY_FAN_OUT + 1);\n\tBUG_ON(slot >= ASSOC_ARRAY_FAN_OUT);\n\n\tnew_n1->parent_slot = slot;\n\n\t/* Metadata pointers cannot change slot */\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++)\n\t\tif (assoc_array_ptr_is_meta(node->slots[i]))\n\t\t\tnew_n0->slots[i] = node->slots[i];\n\t\telse\n\t\t\tnew_n0->slots[i] = NULL;\n\tBUG_ON(new_n0->slots[slot] != NULL);\n\tnew_n0->slots[slot] = assoc_array_node_to_ptr(new_n1);\n\n\t/* Filter the leaf pointers between the new nodes */\n\tfree_slot = -1;\n\tnext_slot = 0;\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tif (assoc_array_ptr_is_meta(node->slots[i]))\n\t\t\tcontinue;\n\t\tif (edit->segment_cache[i] == slot) {\n\t\t\tnew_n1->slots[next_slot++] = node->slots[i];\n\t\t\tnew_n1->nr_leaves_on_branch++;\n\t\t} else {\n\t\t\tdo {\n\t\t\t\tfree_slot++;\n\t\t\t} while (new_n0->slots[free_slot] != NULL);\n\t\t\tnew_n0->slots[free_slot] = node->slots[i];\n\t\t}\n\t}\n\n\tpr_devel(\"filtered: f=%x n=%x\\n\", free_slot, next_slot);\n\n\tif (edit->segment_cache[ASSOC_ARRAY_FAN_OUT] != slot) {\n\t\tdo {\n\t\t\tfree_slot++;\n\t\t} while (new_n0->slots[free_slot] != NULL);\n\t\tedit->leaf_p = &new_n0->slots[free_slot];\n\t\tedit->adjust_count_on = new_n0;\n\t} else {\n\t\tedit->leaf_p = &new_n1->slots[next_slot++];\n\t\tedit->adjust_count_on = new_n1;\n\t}\n\n\tBUG_ON(next_slot <= 1);\n\n\tedit->set_backpointers_to = assoc_array_node_to_ptr(new_n0);\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tif (edit->segment_cache[i] == 0xff) {\n\t\t\tptr = node->slots[i];\n\t\t\tBUG_ON(assoc_array_ptr_is_leaf(ptr));\n\t\t\tif (assoc_array_ptr_is_node(ptr)) {\n\t\t\t\tside = assoc_array_ptr_to_node(ptr);\n\t\t\t\tedit->set_backpointers[i] = &side->back_pointer;\n\t\t\t} else {\n\t\t\t\tshortcut = assoc_array_ptr_to_shortcut(ptr);\n\t\t\t\tedit->set_backpointers[i] = &shortcut->back_pointer;\n\t\t\t}\n\t\t}\n\t}\n\n\tptr = node->back_pointer;\n\tif (!ptr)\n\t\tedit->set[0].ptr = &edit->array->root;\n\telse if (assoc_array_ptr_is_node(ptr))\n\t\tedit->set[0].ptr = &assoc_array_ptr_to_node(ptr)->slots[node->parent_slot];\n\telse\n\t\tedit->set[0].ptr = &assoc_array_ptr_to_shortcut(ptr)->next_node;\n\tedit->excised_meta[0] = assoc_array_node_to_ptr(node);\n\tpr_devel(\"<--%s() = ok [split node]\\n\", __func__);\n\treturn true;\n\npresent_leaves_cluster_but_not_new_leaf:\n\t/* All the old leaves cluster in the same slot, but the new leaf wants\n\t * to go into a different slot, so we create a new node to hold the new\n\t * leaf and a pointer to a new node holding all the old leaves.\n\t */\n\tpr_devel(\"present leaves cluster but not new leaf\\n\");\n\n\tnew_n0->back_pointer = node->back_pointer;\n\tnew_n0->parent_slot = node->parent_slot;\n\tnew_n0->nr_leaves_on_branch = node->nr_leaves_on_branch;\n\tnew_n1->back_pointer = assoc_array_node_to_ptr(new_n0);\n\tnew_n1->parent_slot = edit->segment_cache[0];\n\tnew_n1->nr_leaves_on_branch = node->nr_leaves_on_branch;\n\tedit->adjust_count_on = new_n0;\n\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++)\n\t\tnew_n1->slots[i] = node->slots[i];\n\n\tnew_n0->slots[edit->segment_cache[0]] = assoc_array_node_to_ptr(new_n0);\n\tedit->leaf_p = &new_n0->slots[edit->segment_cache[ASSOC_ARRAY_FAN_OUT]];\n\n\tedit->set[0].ptr = &assoc_array_ptr_to_node(node->back_pointer)->slots[node->parent_slot];\n\tedit->set[0].to = assoc_array_node_to_ptr(new_n0);\n\tedit->excised_meta[0] = assoc_array_node_to_ptr(node);\n\tpr_devel(\"<--%s() = ok [insert node before]\\n\", __func__);\n\treturn true;\n\nall_leaves_cluster_together:\n\t/* All the leaves, new and old, want to cluster together in this node\n\t * in the same slot, so we have to replace this node with a shortcut to\n\t * skip over the identical parts of the key and then place a pair of\n\t * nodes, one inside the other, at the end of the shortcut and\n\t * distribute the keys between them.\n\t *\n\t * Firstly we need to work out where the leaves start diverging as a\n\t * bit position into their keys so that we know how big the shortcut\n\t * needs to be.\n\t *\n\t * We only need to make a single pass of N of the N+1 leaves because if\n\t * any keys differ between themselves at bit X then at least one of\n\t * them must also differ with the base key at bit X or before.\n\t */\n\tpr_devel(\"all leaves cluster together\\n\");\n\tdiff = INT_MAX;\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tint x = ops->diff_objects(assoc_array_ptr_to_leaf(node->slots[i]),\n\t\t\t\t\t  index_key);\n\t\tif (x < diff) {\n\t\t\tBUG_ON(x < 0);\n\t\t\tdiff = x;\n\t\t}\n\t}\n\tBUG_ON(diff == INT_MAX);\n\tBUG_ON(diff < level + ASSOC_ARRAY_LEVEL_STEP);\n\n\tkeylen = round_up(diff, ASSOC_ARRAY_KEY_CHUNK_SIZE);\n\tkeylen >>= ASSOC_ARRAY_KEY_CHUNK_SHIFT;\n\n\tnew_s0 = kzalloc(sizeof(struct assoc_array_shortcut) +\n\t\t\t keylen * sizeof(unsigned long), GFP_KERNEL);\n\tif (!new_s0)\n\t\treturn false;\n\tedit->new_meta[2] = assoc_array_shortcut_to_ptr(new_s0);\n\n\tedit->set[0].to = assoc_array_shortcut_to_ptr(new_s0);\n\tnew_s0->back_pointer = node->back_pointer;\n\tnew_s0->parent_slot = node->parent_slot;\n\tnew_s0->next_node = assoc_array_node_to_ptr(new_n0);\n\tnew_n0->back_pointer = assoc_array_shortcut_to_ptr(new_s0);\n\tnew_n0->parent_slot = 0;\n\tnew_n1->back_pointer = assoc_array_node_to_ptr(new_n0);\n\tnew_n1->parent_slot = -1; /* Need to calculate this */\n\n\tnew_s0->skip_to_level = level = diff & ~ASSOC_ARRAY_LEVEL_STEP_MASK;\n\tpr_devel(\"skip_to_level = %d [diff %d]\\n\", level, diff);\n\tBUG_ON(level <= 0);\n\n\tfor (i = 0; i < keylen; i++)\n\t\tnew_s0->index_key[i] =\n\t\t\tops->get_key_chunk(index_key, i * ASSOC_ARRAY_KEY_CHUNK_SIZE);\n\n\tblank = ULONG_MAX << (level & ASSOC_ARRAY_KEY_CHUNK_MASK);\n\tpr_devel(\"blank off [%zu] %d: %lx\\n\", keylen - 1, level, blank);\n\tnew_s0->index_key[keylen - 1] &= ~blank;\n\n\t/* This now reduces to a node splitting exercise for which we'll need\n\t * to regenerate the disparity table.\n\t */\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tptr = node->slots[i];\n\t\tbase_seg = ops->get_object_key_chunk(assoc_array_ptr_to_leaf(ptr),\n\t\t\t\t\t\t     level);\n\t\tbase_seg >>= level & ASSOC_ARRAY_KEY_CHUNK_MASK;\n\t\tedit->segment_cache[i] = base_seg & ASSOC_ARRAY_FAN_MASK;\n\t}\n\n\tbase_seg = ops->get_key_chunk(index_key, level);\n\tbase_seg >>= level & ASSOC_ARRAY_KEY_CHUNK_MASK;\n\tedit->segment_cache[ASSOC_ARRAY_FAN_OUT] = base_seg & ASSOC_ARRAY_FAN_MASK;\n\tgoto do_split_node;\n}",
                        "code_after_change": "static bool assoc_array_insert_into_terminal_node(struct assoc_array_edit *edit,\n\t\t\t\t\t\t  const struct assoc_array_ops *ops,\n\t\t\t\t\t\t  const void *index_key,\n\t\t\t\t\t\t  struct assoc_array_walk_result *result)\n{\n\tstruct assoc_array_shortcut *shortcut, *new_s0;\n\tstruct assoc_array_node *node, *new_n0, *new_n1, *side;\n\tstruct assoc_array_ptr *ptr;\n\tunsigned long dissimilarity, base_seg, blank;\n\tsize_t keylen;\n\tbool have_meta;\n\tint level, diff;\n\tint slot, next_slot, free_slot, i, j;\n\n\tnode\t= result->terminal_node.node;\n\tlevel\t= result->terminal_node.level;\n\tedit->segment_cache[ASSOC_ARRAY_FAN_OUT] = result->terminal_node.slot;\n\n\tpr_devel(\"-->%s()\\n\", __func__);\n\n\t/* We arrived at a node which doesn't have an onward node or shortcut\n\t * pointer that we have to follow.  This means that (a) the leaf we\n\t * want must go here (either by insertion or replacement) or (b) we\n\t * need to split this node and insert in one of the fragments.\n\t */\n\tfree_slot = -1;\n\n\t/* Firstly, we have to check the leaves in this node to see if there's\n\t * a matching one we should replace in place.\n\t */\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tptr = node->slots[i];\n\t\tif (!ptr) {\n\t\t\tfree_slot = i;\n\t\t\tcontinue;\n\t\t}\n\t\tif (assoc_array_ptr_is_leaf(ptr) &&\n\t\t    ops->compare_object(assoc_array_ptr_to_leaf(ptr),\n\t\t\t\t\tindex_key)) {\n\t\t\tpr_devel(\"replace in slot %d\\n\", i);\n\t\t\tedit->leaf_p = &node->slots[i];\n\t\t\tedit->dead_leaf = node->slots[i];\n\t\t\tpr_devel(\"<--%s() = ok [replace]\\n\", __func__);\n\t\t\treturn true;\n\t\t}\n\t}\n\n\t/* If there is a free slot in this node then we can just insert the\n\t * leaf here.\n\t */\n\tif (free_slot >= 0) {\n\t\tpr_devel(\"insert in free slot %d\\n\", free_slot);\n\t\tedit->leaf_p = &node->slots[free_slot];\n\t\tedit->adjust_count_on = node;\n\t\tpr_devel(\"<--%s() = ok [insert]\\n\", __func__);\n\t\treturn true;\n\t}\n\n\t/* The node has no spare slots - so we're either going to have to split\n\t * it or insert another node before it.\n\t *\n\t * Whatever, we're going to need at least two new nodes - so allocate\n\t * those now.  We may also need a new shortcut, but we deal with that\n\t * when we need it.\n\t */\n\tnew_n0 = kzalloc(sizeof(struct assoc_array_node), GFP_KERNEL);\n\tif (!new_n0)\n\t\treturn false;\n\tedit->new_meta[0] = assoc_array_node_to_ptr(new_n0);\n\tnew_n1 = kzalloc(sizeof(struct assoc_array_node), GFP_KERNEL);\n\tif (!new_n1)\n\t\treturn false;\n\tedit->new_meta[1] = assoc_array_node_to_ptr(new_n1);\n\n\t/* We need to find out how similar the leaves are. */\n\tpr_devel(\"no spare slots\\n\");\n\thave_meta = false;\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tptr = node->slots[i];\n\t\tif (assoc_array_ptr_is_meta(ptr)) {\n\t\t\tedit->segment_cache[i] = 0xff;\n\t\t\thave_meta = true;\n\t\t\tcontinue;\n\t\t}\n\t\tbase_seg = ops->get_object_key_chunk(\n\t\t\tassoc_array_ptr_to_leaf(ptr), level);\n\t\tbase_seg >>= level & ASSOC_ARRAY_KEY_CHUNK_MASK;\n\t\tedit->segment_cache[i] = base_seg & ASSOC_ARRAY_FAN_MASK;\n\t}\n\n\tif (have_meta) {\n\t\tpr_devel(\"have meta\\n\");\n\t\tgoto split_node;\n\t}\n\n\t/* The node contains only leaves */\n\tdissimilarity = 0;\n\tbase_seg = edit->segment_cache[0];\n\tfor (i = 1; i < ASSOC_ARRAY_FAN_OUT; i++)\n\t\tdissimilarity |= edit->segment_cache[i] ^ base_seg;\n\n\tpr_devel(\"only leaves; dissimilarity=%lx\\n\", dissimilarity);\n\n\tif ((dissimilarity & ASSOC_ARRAY_FAN_MASK) == 0) {\n\t\t/* The old leaves all cluster in the same slot.  We will need\n\t\t * to insert a shortcut if the new node wants to cluster with them.\n\t\t */\n\t\tif ((edit->segment_cache[ASSOC_ARRAY_FAN_OUT] ^ base_seg) == 0)\n\t\t\tgoto all_leaves_cluster_together;\n\n\t\t/* Otherwise all the old leaves cluster in the same slot, but\n\t\t * the new leaf wants to go into a different slot - so we\n\t\t * create a new node (n0) to hold the new leaf and a pointer to\n\t\t * a new node (n1) holding all the old leaves.\n\t\t *\n\t\t * This can be done by falling through to the node splitting\n\t\t * path.\n\t\t */\n\t\tpr_devel(\"present leaves cluster but not new leaf\\n\");\n\t}\n\nsplit_node:\n\tpr_devel(\"split node\\n\");\n\n\t/* We need to split the current node.  The node must contain anything\n\t * from a single leaf (in the one leaf case, this leaf will cluster\n\t * with the new leaf) and the rest meta-pointers, to all leaves, some\n\t * of which may cluster.\n\t *\n\t * It won't contain the case in which all the current leaves plus the\n\t * new leaves want to cluster in the same slot.\n\t *\n\t * We need to expel at least two leaves out of a set consisting of the\n\t * leaves in the node and the new leaf.  The current meta pointers can\n\t * just be copied as they shouldn't cluster with any of the leaves.\n\t *\n\t * We need a new node (n0) to replace the current one and a new node to\n\t * take the expelled nodes (n1).\n\t */\n\tedit->set[0].to = assoc_array_node_to_ptr(new_n0);\n\tnew_n0->back_pointer = node->back_pointer;\n\tnew_n0->parent_slot = node->parent_slot;\n\tnew_n1->back_pointer = assoc_array_node_to_ptr(new_n0);\n\tnew_n1->parent_slot = -1; /* Need to calculate this */\n\ndo_split_node:\n\tpr_devel(\"do_split_node\\n\");\n\n\tnew_n0->nr_leaves_on_branch = node->nr_leaves_on_branch;\n\tnew_n1->nr_leaves_on_branch = 0;\n\n\t/* Begin by finding two matching leaves.  There have to be at least two\n\t * that match - even if there are meta pointers - because any leaf that\n\t * would match a slot with a meta pointer in it must be somewhere\n\t * behind that meta pointer and cannot be here.  Further, given N\n\t * remaining leaf slots, we now have N+1 leaves to go in them.\n\t */\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tslot = edit->segment_cache[i];\n\t\tif (slot != 0xff)\n\t\t\tfor (j = i + 1; j < ASSOC_ARRAY_FAN_OUT + 1; j++)\n\t\t\t\tif (edit->segment_cache[j] == slot)\n\t\t\t\t\tgoto found_slot_for_multiple_occupancy;\n\t}\nfound_slot_for_multiple_occupancy:\n\tpr_devel(\"same slot: %x %x [%02x]\\n\", i, j, slot);\n\tBUG_ON(i >= ASSOC_ARRAY_FAN_OUT);\n\tBUG_ON(j >= ASSOC_ARRAY_FAN_OUT + 1);\n\tBUG_ON(slot >= ASSOC_ARRAY_FAN_OUT);\n\n\tnew_n1->parent_slot = slot;\n\n\t/* Metadata pointers cannot change slot */\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++)\n\t\tif (assoc_array_ptr_is_meta(node->slots[i]))\n\t\t\tnew_n0->slots[i] = node->slots[i];\n\t\telse\n\t\t\tnew_n0->slots[i] = NULL;\n\tBUG_ON(new_n0->slots[slot] != NULL);\n\tnew_n0->slots[slot] = assoc_array_node_to_ptr(new_n1);\n\n\t/* Filter the leaf pointers between the new nodes */\n\tfree_slot = -1;\n\tnext_slot = 0;\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tif (assoc_array_ptr_is_meta(node->slots[i]))\n\t\t\tcontinue;\n\t\tif (edit->segment_cache[i] == slot) {\n\t\t\tnew_n1->slots[next_slot++] = node->slots[i];\n\t\t\tnew_n1->nr_leaves_on_branch++;\n\t\t} else {\n\t\t\tdo {\n\t\t\t\tfree_slot++;\n\t\t\t} while (new_n0->slots[free_slot] != NULL);\n\t\t\tnew_n0->slots[free_slot] = node->slots[i];\n\t\t}\n\t}\n\n\tpr_devel(\"filtered: f=%x n=%x\\n\", free_slot, next_slot);\n\n\tif (edit->segment_cache[ASSOC_ARRAY_FAN_OUT] != slot) {\n\t\tdo {\n\t\t\tfree_slot++;\n\t\t} while (new_n0->slots[free_slot] != NULL);\n\t\tedit->leaf_p = &new_n0->slots[free_slot];\n\t\tedit->adjust_count_on = new_n0;\n\t} else {\n\t\tedit->leaf_p = &new_n1->slots[next_slot++];\n\t\tedit->adjust_count_on = new_n1;\n\t}\n\n\tBUG_ON(next_slot <= 1);\n\n\tedit->set_backpointers_to = assoc_array_node_to_ptr(new_n0);\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tif (edit->segment_cache[i] == 0xff) {\n\t\t\tptr = node->slots[i];\n\t\t\tBUG_ON(assoc_array_ptr_is_leaf(ptr));\n\t\t\tif (assoc_array_ptr_is_node(ptr)) {\n\t\t\t\tside = assoc_array_ptr_to_node(ptr);\n\t\t\t\tedit->set_backpointers[i] = &side->back_pointer;\n\t\t\t} else {\n\t\t\t\tshortcut = assoc_array_ptr_to_shortcut(ptr);\n\t\t\t\tedit->set_backpointers[i] = &shortcut->back_pointer;\n\t\t\t}\n\t\t}\n\t}\n\n\tptr = node->back_pointer;\n\tif (!ptr)\n\t\tedit->set[0].ptr = &edit->array->root;\n\telse if (assoc_array_ptr_is_node(ptr))\n\t\tedit->set[0].ptr = &assoc_array_ptr_to_node(ptr)->slots[node->parent_slot];\n\telse\n\t\tedit->set[0].ptr = &assoc_array_ptr_to_shortcut(ptr)->next_node;\n\tedit->excised_meta[0] = assoc_array_node_to_ptr(node);\n\tpr_devel(\"<--%s() = ok [split node]\\n\", __func__);\n\treturn true;\n\nall_leaves_cluster_together:\n\t/* All the leaves, new and old, want to cluster together in this node\n\t * in the same slot, so we have to replace this node with a shortcut to\n\t * skip over the identical parts of the key and then place a pair of\n\t * nodes, one inside the other, at the end of the shortcut and\n\t * distribute the keys between them.\n\t *\n\t * Firstly we need to work out where the leaves start diverging as a\n\t * bit position into their keys so that we know how big the shortcut\n\t * needs to be.\n\t *\n\t * We only need to make a single pass of N of the N+1 leaves because if\n\t * any keys differ between themselves at bit X then at least one of\n\t * them must also differ with the base key at bit X or before.\n\t */\n\tpr_devel(\"all leaves cluster together\\n\");\n\tdiff = INT_MAX;\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tint x = ops->diff_objects(assoc_array_ptr_to_leaf(node->slots[i]),\n\t\t\t\t\t  index_key);\n\t\tif (x < diff) {\n\t\t\tBUG_ON(x < 0);\n\t\t\tdiff = x;\n\t\t}\n\t}\n\tBUG_ON(diff == INT_MAX);\n\tBUG_ON(diff < level + ASSOC_ARRAY_LEVEL_STEP);\n\n\tkeylen = round_up(diff, ASSOC_ARRAY_KEY_CHUNK_SIZE);\n\tkeylen >>= ASSOC_ARRAY_KEY_CHUNK_SHIFT;\n\n\tnew_s0 = kzalloc(sizeof(struct assoc_array_shortcut) +\n\t\t\t keylen * sizeof(unsigned long), GFP_KERNEL);\n\tif (!new_s0)\n\t\treturn false;\n\tedit->new_meta[2] = assoc_array_shortcut_to_ptr(new_s0);\n\n\tedit->set[0].to = assoc_array_shortcut_to_ptr(new_s0);\n\tnew_s0->back_pointer = node->back_pointer;\n\tnew_s0->parent_slot = node->parent_slot;\n\tnew_s0->next_node = assoc_array_node_to_ptr(new_n0);\n\tnew_n0->back_pointer = assoc_array_shortcut_to_ptr(new_s0);\n\tnew_n0->parent_slot = 0;\n\tnew_n1->back_pointer = assoc_array_node_to_ptr(new_n0);\n\tnew_n1->parent_slot = -1; /* Need to calculate this */\n\n\tnew_s0->skip_to_level = level = diff & ~ASSOC_ARRAY_LEVEL_STEP_MASK;\n\tpr_devel(\"skip_to_level = %d [diff %d]\\n\", level, diff);\n\tBUG_ON(level <= 0);\n\n\tfor (i = 0; i < keylen; i++)\n\t\tnew_s0->index_key[i] =\n\t\t\tops->get_key_chunk(index_key, i * ASSOC_ARRAY_KEY_CHUNK_SIZE);\n\n\tblank = ULONG_MAX << (level & ASSOC_ARRAY_KEY_CHUNK_MASK);\n\tpr_devel(\"blank off [%zu] %d: %lx\\n\", keylen - 1, level, blank);\n\tnew_s0->index_key[keylen - 1] &= ~blank;\n\n\t/* This now reduces to a node splitting exercise for which we'll need\n\t * to regenerate the disparity table.\n\t */\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tptr = node->slots[i];\n\t\tbase_seg = ops->get_object_key_chunk(assoc_array_ptr_to_leaf(ptr),\n\t\t\t\t\t\t     level);\n\t\tbase_seg >>= level & ASSOC_ARRAY_KEY_CHUNK_MASK;\n\t\tedit->segment_cache[i] = base_seg & ASSOC_ARRAY_FAN_MASK;\n\t}\n\n\tbase_seg = ops->get_key_chunk(index_key, level);\n\tbase_seg >>= level & ASSOC_ARRAY_KEY_CHUNK_MASK;\n\tedit->segment_cache[ASSOC_ARRAY_FAN_OUT] = base_seg & ASSOC_ARRAY_FAN_MASK;\n\tgoto do_split_node;\n}",
                        "cve_id": "CVE-2017-12193"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int check_leaf_item(struct btrfs_fs_info *fs_info,\n\t\t\t   struct extent_buffer *leaf,\n\t\t\t   struct btrfs_key *key, int slot)\n{\n\tint ret = 0;\n\n\tswitch (key->type) {\n\tcase BTRFS_EXTENT_DATA_KEY:\n\t\tret = check_extent_data_item(fs_info, leaf, key, slot);\n\t\tbreak;\n\tcase BTRFS_EXTENT_CSUM_KEY:\n\t\tret = check_csum_item(fs_info, leaf, key, slot);\n\t\tbreak;\n\tcase BTRFS_DIR_ITEM_KEY:\n\tcase BTRFS_DIR_INDEX_KEY:\n\tcase BTRFS_XATTR_ITEM_KEY:\n\t\tret = check_dir_item(fs_info, leaf, key, slot);\n\t\tbreak;\n\t}\n\treturn ret;\n}",
                        "code_after_change": "static int check_leaf_item(struct btrfs_fs_info *fs_info,\n\t\t\t   struct extent_buffer *leaf,\n\t\t\t   struct btrfs_key *key, int slot)\n{\n\tint ret = 0;\n\n\tswitch (key->type) {\n\tcase BTRFS_EXTENT_DATA_KEY:\n\t\tret = check_extent_data_item(fs_info, leaf, key, slot);\n\t\tbreak;\n\tcase BTRFS_EXTENT_CSUM_KEY:\n\t\tret = check_csum_item(fs_info, leaf, key, slot);\n\t\tbreak;\n\tcase BTRFS_DIR_ITEM_KEY:\n\tcase BTRFS_DIR_INDEX_KEY:\n\tcase BTRFS_XATTR_ITEM_KEY:\n\t\tret = check_dir_item(fs_info, leaf, key, slot);\n\t\tbreak;\n\tcase BTRFS_BLOCK_GROUP_ITEM_KEY:\n\t\tret = check_block_group_item(fs_info, leaf, key, slot);\n\t\tbreak;\n\t}\n\treturn ret;\n}",
                        "cve_id": "CVE-2018-14613"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2286,
            "cve_id": "CVE-2019-19227",
            "code_snippet": "static int __init atalk_init(void)\n{\n\tint rc;\n\n\trc = proto_register(&ddp_proto, 0);\n\tif (rc)\n\t\tgoto out;\n\n\trc = sock_register(&atalk_family_ops);\n\tif (rc)\n\t\tgoto out_proto;\n\n\tddp_dl = register_snap_client(ddp_snap_id, atalk_rcv);\n\tif (!ddp_dl) {\n\t\tpr_crit(\"Unable to register DDP with SNAP.\\n\");\n\t\tgoto out_sock;\n\t}\n\n\tdev_add_pack(&ltalk_packet_type);\n\tdev_add_pack(&ppptalk_packet_type);\n\n\trc = register_netdevice_notifier(&ddp_notifier);\n\tif (rc)\n\t\tgoto out_snap;\n\n\trc = aarp_proto_init();\n\tif (rc)\n\t\tgoto out_dev;\n\n\trc = atalk_proc_init();\n\tif (rc)\n\t\tgoto out_aarp;\n\n\trc = atalk_register_sysctl();\n\tif (rc)\n\t\tgoto out_proc;\nout:\n\treturn rc;\nout_proc:\n\tatalk_proc_exit();\nout_aarp:\n\taarp_cleanup_module();\nout_dev:\n\tunregister_netdevice_notifier(&ddp_notifier);\nout_snap:\n\tdev_remove_pack(&ppptalk_packet_type);\n\tdev_remove_pack(&ltalk_packet_type);\n\tunregister_snap_client(ddp_dl);\nout_sock:\n\tsock_unregister(PF_APPLETALK);\nout_proto:\n\tproto_unregister(&ddp_proto);\n\tgoto out;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static ssize_t module_gzip_decompress(struct load_info *info,\n\t\t\t\t      const void *buf, size_t size)\n{\n\tstruct z_stream_s s = { 0 };\n\tsize_t new_size = 0;\n\tsize_t gzip_hdr_len;\n\tssize_t retval;\n\tint rc;\n\n\tgzip_hdr_len = module_gzip_header_len(buf, size);\n\tif (!gzip_hdr_len) {\n\t\tpr_err(\"not a gzip compressed module\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\ts.next_in = buf + gzip_hdr_len;\n\ts.avail_in = size - gzip_hdr_len;\n\n\ts.workspace = kmalloc(zlib_inflate_workspacesize(), GFP_KERNEL);\n\tif (!s.workspace)\n\t\treturn -ENOMEM;\n\n\trc = zlib_inflateInit2(&s, -MAX_WBITS);\n\tif (rc != Z_OK) {\n\t\tpr_err(\"failed to initialize decompressor: %d\\n\", rc);\n\t\tretval = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tdo {\n\t\tstruct page *page = module_get_next_page(info);\n\n\t\tif (!page) {\n\t\t\tretval = -ENOMEM;\n\t\t\tgoto out_inflate_end;\n\t\t}\n\n\t\ts.next_out = kmap_local_page(page);\n\t\ts.avail_out = PAGE_SIZE;\n\t\trc = zlib_inflate(&s, 0);\n\t\tkunmap_local(s.next_out);\n\n\t\tnew_size += PAGE_SIZE - s.avail_out;\n\t} while (rc == Z_OK);\n\n\tif (rc != Z_STREAM_END) {\n\t\tpr_err(\"decompression failed with status %d\\n\", rc);\n\t\tretval = -EINVAL;\n\t\tgoto out_inflate_end;\n\t}\n\n\tretval = new_size;\n\nout_inflate_end:\n\tzlib_inflateEnd(&s);\nout:\n\tkfree(s.workspace);\n\treturn retval;\n}",
                        "code_after_change": "int __init aarp_proto_init(void)\n{\n\tint rc;\n\n\taarp_dl = register_snap_client(aarp_snap_id, aarp_rcv);\n\tif (!aarp_dl) {\n\t\tprintk(KERN_CRIT \"Unable to register AARP with SNAP.\\n\");\n\t\treturn -ENOMEM;\n\t}\n\ttimer_setup(&aarp_timer, aarp_expire_timeout, 0);\n\taarp_timer.expires  = jiffies + sysctl_aarp_expiry_time;\n\tadd_timer(&aarp_timer);\n\trc = register_netdevice_notifier(&aarp_notifier);\n\tif (rc) {\n\t\tdel_timer_sync(&aarp_timer);\n\t\tunregister_snap_client(aarp_dl);\n\t}\n\treturn rc;\n}",
                        "cve_id": "CVE-2023-22997"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int do_tls_getsockopt_conf(struct sock *sk, char __user *optval,\n\t\t\t\t  int __user *optlen, int tx)\n{\n\tint rc = 0;\n\tstruct tls_context *ctx = tls_get_ctx(sk);\n\tstruct tls_crypto_info *crypto_info;\n\tstruct cipher_context *cctx;\n\tint len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tif (!optval || (len < sizeof(*crypto_info))) {\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (!ctx) {\n\t\trc = -EBUSY;\n\t\tgoto out;\n\t}\n\n\t/* get user crypto info */\n\tif (tx) {\n\t\tcrypto_info = &ctx->crypto_send.info;\n\t\tcctx = &ctx->tx;\n\t} else {\n\t\tcrypto_info = &ctx->crypto_recv.info;\n\t\tcctx = &ctx->rx;\n\t}\n\n\tif (!TLS_CRYPTO_INFO_READY(crypto_info)) {\n\t\trc = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tif (len == sizeof(*crypto_info)) {\n\t\tif (copy_to_user(optval, crypto_info, sizeof(*crypto_info)))\n\t\t\trc = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tswitch (crypto_info->cipher_type) {\n\tcase TLS_CIPHER_AES_GCM_128: {\n\t\tstruct tls12_crypto_info_aes_gcm_128 *\n\t\t  crypto_info_aes_gcm_128 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aes_gcm_128,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aes_gcm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(crypto_info_aes_gcm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_GCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_GCM_128_IV_SIZE);\n\t\tmemcpy(crypto_info_aes_gcm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_GCM_128_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aes_gcm_128,\n\t\t\t\t sizeof(*crypto_info_aes_gcm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_AES_GCM_256: {\n\t\tstruct tls12_crypto_info_aes_gcm_256 *\n\t\t  crypto_info_aes_gcm_256 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aes_gcm_256,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aes_gcm_256)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(crypto_info_aes_gcm_256->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_GCM_256_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_GCM_256_IV_SIZE);\n\t\tmemcpy(crypto_info_aes_gcm_256->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_GCM_256_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aes_gcm_256,\n\t\t\t\t sizeof(*crypto_info_aes_gcm_256)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_AES_CCM_128: {\n\t\tstruct tls12_crypto_info_aes_ccm_128 *aes_ccm_128 =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_aes_ccm_128, info);\n\n\t\tif (len != sizeof(*aes_ccm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(aes_ccm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_CCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_CCM_128_IV_SIZE);\n\t\tmemcpy(aes_ccm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_CCM_128_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval, aes_ccm_128, sizeof(*aes_ccm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_CHACHA20_POLY1305: {\n\t\tstruct tls12_crypto_info_chacha20_poly1305 *chacha20_poly1305 =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_chacha20_poly1305,\n\t\t\t\tinfo);\n\n\t\tif (len != sizeof(*chacha20_poly1305)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(chacha20_poly1305->iv,\n\t\t       cctx->iv + TLS_CIPHER_CHACHA20_POLY1305_SALT_SIZE,\n\t\t       TLS_CIPHER_CHACHA20_POLY1305_IV_SIZE);\n\t\tmemcpy(chacha20_poly1305->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_CHACHA20_POLY1305_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval, chacha20_poly1305,\n\t\t\t\tsizeof(*chacha20_poly1305)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_SM4_GCM: {\n\t\tstruct tls12_crypto_info_sm4_gcm *sm4_gcm_info =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_sm4_gcm, info);\n\n\t\tif (len != sizeof(*sm4_gcm_info)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(sm4_gcm_info->iv,\n\t\t       cctx->iv + TLS_CIPHER_SM4_GCM_SALT_SIZE,\n\t\t       TLS_CIPHER_SM4_GCM_IV_SIZE);\n\t\tmemcpy(sm4_gcm_info->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_SM4_GCM_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval, sm4_gcm_info, sizeof(*sm4_gcm_info)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_SM4_CCM: {\n\t\tstruct tls12_crypto_info_sm4_ccm *sm4_ccm_info =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_sm4_ccm, info);\n\n\t\tif (len != sizeof(*sm4_ccm_info)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(sm4_ccm_info->iv,\n\t\t       cctx->iv + TLS_CIPHER_SM4_CCM_SALT_SIZE,\n\t\t       TLS_CIPHER_SM4_CCM_IV_SIZE);\n\t\tmemcpy(sm4_ccm_info->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_SM4_CCM_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval, sm4_ccm_info, sizeof(*sm4_ccm_info)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_ARIA_GCM_128: {\n\t\tstruct tls12_crypto_info_aria_gcm_128 *\n\t\t  crypto_info_aria_gcm_128 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aria_gcm_128,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aria_gcm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(crypto_info_aria_gcm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_ARIA_GCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_ARIA_GCM_128_IV_SIZE);\n\t\tmemcpy(crypto_info_aria_gcm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_ARIA_GCM_128_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aria_gcm_128,\n\t\t\t\t sizeof(*crypto_info_aria_gcm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_ARIA_GCM_256: {\n\t\tstruct tls12_crypto_info_aria_gcm_256 *\n\t\t  crypto_info_aria_gcm_256 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aria_gcm_256,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aria_gcm_256)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(crypto_info_aria_gcm_256->iv,\n\t\t       cctx->iv + TLS_CIPHER_ARIA_GCM_256_SALT_SIZE,\n\t\t       TLS_CIPHER_ARIA_GCM_256_IV_SIZE);\n\t\tmemcpy(crypto_info_aria_gcm_256->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_ARIA_GCM_256_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aria_gcm_256,\n\t\t\t\t sizeof(*crypto_info_aria_gcm_256)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tdefault:\n\t\trc = -EINVAL;\n\t}\n\nout:\n\treturn rc;\n}",
                        "code_after_change": "static ssize_t module_gzip_decompress(struct load_info *info,\n\t\t\t\t      const void *buf, size_t size)\n{\n\tstruct z_stream_s s = { 0 };\n\tsize_t new_size = 0;\n\tsize_t gzip_hdr_len;\n\tssize_t retval;\n\tint rc;\n\n\tgzip_hdr_len = module_gzip_header_len(buf, size);\n\tif (!gzip_hdr_len) {\n\t\tpr_err(\"not a gzip compressed module\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\ts.next_in = buf + gzip_hdr_len;\n\ts.avail_in = size - gzip_hdr_len;\n\n\ts.workspace = kmalloc(zlib_inflate_workspacesize(), GFP_KERNEL);\n\tif (!s.workspace)\n\t\treturn -ENOMEM;\n\n\trc = zlib_inflateInit2(&s, -MAX_WBITS);\n\tif (rc != Z_OK) {\n\t\tpr_err(\"failed to initialize decompressor: %d\\n\", rc);\n\t\tretval = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tdo {\n\t\tstruct page *page = module_get_next_page(info);\n\n\t\tif (IS_ERR(page)) {\n\t\t\tretval = PTR_ERR(page);\n\t\t\tgoto out_inflate_end;\n\t\t}\n\n\t\ts.next_out = kmap_local_page(page);\n\t\ts.avail_out = PAGE_SIZE;\n\t\trc = zlib_inflate(&s, 0);\n\t\tkunmap_local(s.next_out);\n\n\t\tnew_size += PAGE_SIZE - s.avail_out;\n\t} while (rc == Z_OK);\n\n\tif (rc != Z_STREAM_END) {\n\t\tpr_err(\"decompression failed with status %d\\n\", rc);\n\t\tretval = -EINVAL;\n\t\tgoto out_inflate_end;\n\t}\n\n\tretval = new_size;\n\nout_inflate_end:\n\tzlib_inflateEnd(&s);\nout:\n\tkfree(s.workspace);\n\treturn retval;\n}",
                        "cve_id": "CVE-2023-28466"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int dccp_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tconst struct dccp_sock *dp = dccp_sk(sk);\n\tconst int flags = msg->msg_flags;\n\tconst int noblock = flags & MSG_DONTWAIT;\n\tstruct sk_buff *skb;\n\tint rc, size;\n\tlong timeo;\n\n\ttrace_dccp_probe(sk, len);\n\n\tif (len > dp->dccps_mss_cache)\n\t\treturn -EMSGSIZE;\n\n\tlock_sock(sk);\n\n\tif (dccp_qpolicy_full(sk)) {\n\t\trc = -EAGAIN;\n\t\tgoto out_release;\n\t}\n\n\ttimeo = sock_sndtimeo(sk, noblock);\n\n\t/*\n\t * We have to use sk_stream_wait_connect here to set sk_write_pending,\n\t * so that the trick in dccp_rcv_request_sent_state_process.\n\t */\n\t/* Wait for a connection to finish. */\n\tif ((1 << sk->sk_state) & ~(DCCPF_OPEN | DCCPF_PARTOPEN))\n\t\tif ((rc = sk_stream_wait_connect(sk, &timeo)) != 0)\n\t\t\tgoto out_release;\n\n\tsize = sk->sk_prot->max_header + len;\n\trelease_sock(sk);\n\tskb = sock_alloc_send_skb(sk, size, noblock, &rc);\n\tlock_sock(sk);\n\tif (skb == NULL)\n\t\tgoto out_release;\n\n\tskb_reserve(skb, sk->sk_prot->max_header);\n\trc = memcpy_from_msg(skb_put(skb, len), msg, len);\n\tif (rc != 0)\n\t\tgoto out_discard;\n\n\trc = dccp_msghdr_parse(msg, skb);\n\tif (rc != 0)\n\t\tgoto out_discard;\n\n\tdccp_qpolicy_push(sk, skb);\n\t/*\n\t * The xmit_timer is set if the TX CCID is rate-based and will expire\n\t * when congestion control permits to release further packets into the\n\t * network. Window-based CCIDs do not use this timer.\n\t */\n\tif (!timer_pending(&dp->dccps_xmit_timer))\n\t\tdccp_write_xmit(sk);\nout_release:\n\trelease_sock(sk);\n\treturn rc ? : len;\nout_discard:\n\tkfree_skb(skb);\n\tgoto out_release;\n}",
                        "code_after_change": "static int do_tls_getsockopt_conf(struct sock *sk, char __user *optval,\n\t\t\t\t  int __user *optlen, int tx)\n{\n\tint rc = 0;\n\tstruct tls_context *ctx = tls_get_ctx(sk);\n\tstruct tls_crypto_info *crypto_info;\n\tstruct cipher_context *cctx;\n\tint len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tif (!optval || (len < sizeof(*crypto_info))) {\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (!ctx) {\n\t\trc = -EBUSY;\n\t\tgoto out;\n\t}\n\n\t/* get user crypto info */\n\tif (tx) {\n\t\tcrypto_info = &ctx->crypto_send.info;\n\t\tcctx = &ctx->tx;\n\t} else {\n\t\tcrypto_info = &ctx->crypto_recv.info;\n\t\tcctx = &ctx->rx;\n\t}\n\n\tif (!TLS_CRYPTO_INFO_READY(crypto_info)) {\n\t\trc = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tif (len == sizeof(*crypto_info)) {\n\t\tif (copy_to_user(optval, crypto_info, sizeof(*crypto_info)))\n\t\t\trc = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tswitch (crypto_info->cipher_type) {\n\tcase TLS_CIPHER_AES_GCM_128: {\n\t\tstruct tls12_crypto_info_aes_gcm_128 *\n\t\t  crypto_info_aes_gcm_128 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aes_gcm_128,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aes_gcm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(crypto_info_aes_gcm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_GCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_GCM_128_IV_SIZE);\n\t\tmemcpy(crypto_info_aes_gcm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_GCM_128_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aes_gcm_128,\n\t\t\t\t sizeof(*crypto_info_aes_gcm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_AES_GCM_256: {\n\t\tstruct tls12_crypto_info_aes_gcm_256 *\n\t\t  crypto_info_aes_gcm_256 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aes_gcm_256,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aes_gcm_256)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(crypto_info_aes_gcm_256->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_GCM_256_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_GCM_256_IV_SIZE);\n\t\tmemcpy(crypto_info_aes_gcm_256->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_GCM_256_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aes_gcm_256,\n\t\t\t\t sizeof(*crypto_info_aes_gcm_256)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_AES_CCM_128: {\n\t\tstruct tls12_crypto_info_aes_ccm_128 *aes_ccm_128 =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_aes_ccm_128, info);\n\n\t\tif (len != sizeof(*aes_ccm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(aes_ccm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_CCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_CCM_128_IV_SIZE);\n\t\tmemcpy(aes_ccm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_CCM_128_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval, aes_ccm_128, sizeof(*aes_ccm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_CHACHA20_POLY1305: {\n\t\tstruct tls12_crypto_info_chacha20_poly1305 *chacha20_poly1305 =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_chacha20_poly1305,\n\t\t\t\tinfo);\n\n\t\tif (len != sizeof(*chacha20_poly1305)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(chacha20_poly1305->iv,\n\t\t       cctx->iv + TLS_CIPHER_CHACHA20_POLY1305_SALT_SIZE,\n\t\t       TLS_CIPHER_CHACHA20_POLY1305_IV_SIZE);\n\t\tmemcpy(chacha20_poly1305->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_CHACHA20_POLY1305_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval, chacha20_poly1305,\n\t\t\t\tsizeof(*chacha20_poly1305)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_SM4_GCM: {\n\t\tstruct tls12_crypto_info_sm4_gcm *sm4_gcm_info =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_sm4_gcm, info);\n\n\t\tif (len != sizeof(*sm4_gcm_info)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(sm4_gcm_info->iv,\n\t\t       cctx->iv + TLS_CIPHER_SM4_GCM_SALT_SIZE,\n\t\t       TLS_CIPHER_SM4_GCM_IV_SIZE);\n\t\tmemcpy(sm4_gcm_info->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_SM4_GCM_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval, sm4_gcm_info, sizeof(*sm4_gcm_info)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_SM4_CCM: {\n\t\tstruct tls12_crypto_info_sm4_ccm *sm4_ccm_info =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_sm4_ccm, info);\n\n\t\tif (len != sizeof(*sm4_ccm_info)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(sm4_ccm_info->iv,\n\t\t       cctx->iv + TLS_CIPHER_SM4_CCM_SALT_SIZE,\n\t\t       TLS_CIPHER_SM4_CCM_IV_SIZE);\n\t\tmemcpy(sm4_ccm_info->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_SM4_CCM_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval, sm4_ccm_info, sizeof(*sm4_ccm_info)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_ARIA_GCM_128: {\n\t\tstruct tls12_crypto_info_aria_gcm_128 *\n\t\t  crypto_info_aria_gcm_128 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aria_gcm_128,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aria_gcm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(crypto_info_aria_gcm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_ARIA_GCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_ARIA_GCM_128_IV_SIZE);\n\t\tmemcpy(crypto_info_aria_gcm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_ARIA_GCM_128_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aria_gcm_128,\n\t\t\t\t sizeof(*crypto_info_aria_gcm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_ARIA_GCM_256: {\n\t\tstruct tls12_crypto_info_aria_gcm_256 *\n\t\t  crypto_info_aria_gcm_256 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aria_gcm_256,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aria_gcm_256)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(crypto_info_aria_gcm_256->iv,\n\t\t       cctx->iv + TLS_CIPHER_ARIA_GCM_256_SALT_SIZE,\n\t\t       TLS_CIPHER_ARIA_GCM_256_IV_SIZE);\n\t\tmemcpy(crypto_info_aria_gcm_256->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_ARIA_GCM_256_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aria_gcm_256,\n\t\t\t\t sizeof(*crypto_info_aria_gcm_256)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tdefault:\n\t\trc = -EINVAL;\n\t}\n\nout:\n\treturn rc;\n}",
                        "cve_id": "CVE-2018-1130"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2357,
            "cve_id": "CVE-2019-19815",
            "code_snippet": "static int f2fs_read_single_page(struct inode *inode, struct page *page,\n\t\t\t\t\tunsigned nr_pages,\n\t\t\t\t\tstruct f2fs_map_blocks *map,\n\t\t\t\t\tstruct bio **bio_ret,\n\t\t\t\t\tsector_t *last_block_in_bio,\n\t\t\t\t\tbool is_readahead)\n{\n\tstruct bio *bio = *bio_ret;\n\tconst unsigned blkbits = inode->i_blkbits;\n\tconst unsigned blocksize = 1 << blkbits;\n\tsector_t block_in_file;\n\tsector_t last_block;\n\tsector_t last_block_in_file;\n\tsector_t block_nr;\n\tint ret = 0;\n\n\tblock_in_file = (sector_t)page_index(page);\n\tlast_block = block_in_file + nr_pages;\n\tlast_block_in_file = (i_size_read(inode) + blocksize - 1) >>\n\t\t\t\t\t\t\tblkbits;\n\tif (last_block > last_block_in_file)\n\t\tlast_block = last_block_in_file;\n\n\t/* just zeroing out page which is beyond EOF */\n\tif (block_in_file >= last_block)\n\t\tgoto zero_out;\n\t/*\n\t * Map blocks using the previous result first.\n\t */\n\tif ((map->m_flags & F2FS_MAP_MAPPED) &&\n\t\t\tblock_in_file > map->m_lblk &&\n\t\t\tblock_in_file < (map->m_lblk + map->m_len))\n\t\tgoto got_it;\n\n\t/*\n\t * Then do more f2fs_map_blocks() calls until we are\n\t * done with this page.\n\t */\n\tmap->m_lblk = block_in_file;\n\tmap->m_len = last_block - block_in_file;\n\n\tret = f2fs_map_blocks(inode, map, 0, F2FS_GET_BLOCK_DEFAULT);\n\tif (ret)\n\t\tgoto out;\ngot_it:\n\tif ((map->m_flags & F2FS_MAP_MAPPED)) {\n\t\tblock_nr = map->m_pblk + block_in_file - map->m_lblk;\n\t\tSetPageMappedToDisk(page);\n\n\t\tif (!PageUptodate(page) && (!PageSwapCache(page) &&\n\t\t\t\t\t!cleancache_get_page(page))) {\n\t\t\tSetPageUptodate(page);\n\t\t\tgoto confused;\n\t\t}\n\n\t\tif (!f2fs_is_valid_blkaddr(F2FS_I_SB(inode), block_nr,\n\t\t\t\t\t\tDATA_GENERIC_ENHANCE_READ)) {\n\t\t\tret = -EFSCORRUPTED;\n\t\t\tgoto out;\n\t\t}\n\t} else {\nzero_out:\n\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\tif (!PageUptodate(page))\n\t\t\tSetPageUptodate(page);\n\t\tunlock_page(page);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * This page will go to BIO.  Do we need to send this\n\t * BIO off first?\n\t */\n\tif (bio && (*last_block_in_bio != block_nr - 1 ||\n\t\t!__same_bdev(F2FS_I_SB(inode), block_nr, bio))) {\nsubmit_and_realloc:\n\t\t__submit_bio(F2FS_I_SB(inode), bio, DATA);\n\t\tbio = NULL;\n\t}\n\tif (bio == NULL) {\n\t\tbio = f2fs_grab_read_bio(inode, block_nr, nr_pages,\n\t\t\t\tis_readahead ? REQ_RAHEAD : 0);\n\t\tif (IS_ERR(bio)) {\n\t\t\tret = PTR_ERR(bio);\n\t\t\tbio = NULL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/*\n\t * If the page is under writeback, we need to wait for\n\t * its completion to see the correct decrypted data.\n\t */\n\tf2fs_wait_on_block_writeback(inode, block_nr);\n\n\tif (bio_add_page(bio, page, blocksize, 0) < blocksize)\n\t\tgoto submit_and_realloc;\n\n\tinc_page_count(F2FS_I_SB(inode), F2FS_RD_DATA);\n\tClearPageError(page);\n\t*last_block_in_bio = block_nr;\n\tgoto out;\nconfused:\n\tif (bio) {\n\t\t__submit_bio(F2FS_I_SB(inode), bio, DATA);\n\t\tbio = NULL;\n\t}\n\tunlock_page(page);\nout:\n\t*bio_ret = bio;\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,\n\t\t\t\t\tunsigned nr_pages, unsigned op_flag)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct bio *bio;\n\tstruct bio_post_read_ctx *ctx;\n\tunsigned int post_read_steps = 0;\n\n\tbio = f2fs_bio_alloc(sbi, min_t(int, nr_pages, BIO_MAX_PAGES), false);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\tf2fs_target_device(sbi, blkaddr, bio);\n\tbio->bi_end_io = f2fs_read_end_io;\n\tbio_set_op_attrs(bio, REQ_OP_READ, op_flag);\n\n\tif (f2fs_encrypted_file(inode))\n\t\tpost_read_steps |= 1 << STEP_DECRYPT;\n\tif (post_read_steps) {\n\t\tctx = mempool_alloc(bio_post_read_ctx_pool, GFP_NOFS);\n\t\tif (!ctx) {\n\t\t\tbio_put(bio);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\tctx->bio = bio;\n\t\tctx->enabled_steps = post_read_steps;\n\t\tbio->bi_private = ctx;\n\n\t\t/* wait the page to be moved by cleaning */\n\t\tf2fs_wait_on_block_writeback(sbi, blkaddr);\n\t}\n\n\treturn bio;\n}",
                        "code_after_change": "static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,\n\t\t\t\t\tunsigned nr_pages, unsigned op_flag)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct bio *bio;\n\tstruct bio_post_read_ctx *ctx;\n\tunsigned int post_read_steps = 0;\n\n\tif (!f2fs_is_valid_blkaddr(sbi, blkaddr, DATA_GENERIC))\n\t\treturn ERR_PTR(-EFAULT);\n\n\tbio = f2fs_bio_alloc(sbi, min_t(int, nr_pages, BIO_MAX_PAGES), false);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\tf2fs_target_device(sbi, blkaddr, bio);\n\tbio->bi_end_io = f2fs_read_end_io;\n\tbio_set_op_attrs(bio, REQ_OP_READ, op_flag);\n\n\tif (f2fs_encrypted_file(inode))\n\t\tpost_read_steps |= 1 << STEP_DECRYPT;\n\tif (post_read_steps) {\n\t\tctx = mempool_alloc(bio_post_read_ctx_pool, GFP_NOFS);\n\t\tif (!ctx) {\n\t\t\tbio_put(bio);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\tctx->bio = bio;\n\t\tctx->enabled_steps = post_read_steps;\n\t\tbio->bi_private = ctx;\n\n\t\t/* wait the page to be moved by cleaning */\n\t\tf2fs_wait_on_block_writeback(sbi, blkaddr);\n\t}\n\n\treturn bio;\n}",
                        "cve_id": "CVE-2018-14616"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2358,
            "cve_id": "CVE-2019-19815",
            "code_snippet": "static int f2fs_set_data_page_dirty(struct page *page)\n{\n\tstruct inode *inode = page_file_mapping(page)->host;\n\n\ttrace_f2fs_set_page_dirty(page, DATA);\n\n\tif (!PageUptodate(page))\n\t\tSetPageUptodate(page);\n\tif (PageSwapCache(page))\n\t\treturn __set_page_dirty_nobuffers(page);\n\n\tif (f2fs_is_atomic_file(inode) && !f2fs_is_commit_atomic_write(inode)) {\n\t\tif (!IS_ATOMIC_WRITTEN_PAGE(page)) {\n\t\t\tf2fs_register_inmem_page(inode, page);\n\t\t\treturn 1;\n\t\t}\n\t\t/*\n\t\t * Previously, this page has been registered, we just\n\t\t * return here.\n\t\t */\n\t\treturn 0;\n\t}\n\n\tif (!PageDirty(page)) {\n\t\t__set_page_dirty_nobuffers(page);\n\t\tf2fs_update_dirty_page(inode, page);\n\t\treturn 1;\n\t}\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int udf_expand_file_adinicb(struct inode *inode)\n{\n\tstruct page *page;\n\tchar *kaddr;\n\tstruct udf_inode_info *iinfo = UDF_I(inode);\n\tint err;\n\tstruct writeback_control udf_wbc = {\n\t\t.sync_mode = WB_SYNC_NONE,\n\t\t.nr_to_write = 1,\n\t};\n\n\tWARN_ON_ONCE(!inode_is_locked(inode));\n\tif (!iinfo->i_lenAlloc) {\n\t\tif (UDF_QUERY_FLAG(inode->i_sb, UDF_FLAG_USE_SHORT_AD))\n\t\t\tiinfo->i_alloc_type = ICBTAG_FLAG_AD_SHORT;\n\t\telse\n\t\t\tiinfo->i_alloc_type = ICBTAG_FLAG_AD_LONG;\n\t\t/* from now on we have normal address_space methods */\n\t\tinode->i_data.a_ops = &udf_aops;\n\t\tup_write(&iinfo->i_data_sem);\n\t\tmark_inode_dirty(inode);\n\t\treturn 0;\n\t}\n\t/*\n\t * Release i_data_sem so that we can lock a page - page lock ranks\n\t * above i_data_sem. i_mutex still protects us against file changes.\n\t */\n\tup_write(&iinfo->i_data_sem);\n\n\tpage = find_or_create_page(inode->i_mapping, 0, GFP_NOFS);\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tif (!PageUptodate(page)) {\n\t\tkaddr = kmap_atomic(page);\n\t\tmemset(kaddr + iinfo->i_lenAlloc, 0x00,\n\t\t       PAGE_SIZE - iinfo->i_lenAlloc);\n\t\tmemcpy(kaddr, iinfo->i_data + iinfo->i_lenEAttr,\n\t\t\tiinfo->i_lenAlloc);\n\t\tflush_dcache_page(page);\n\t\tSetPageUptodate(page);\n\t\tkunmap_atomic(kaddr);\n\t}\n\tdown_write(&iinfo->i_data_sem);\n\tmemset(iinfo->i_data + iinfo->i_lenEAttr, 0x00,\n\t       iinfo->i_lenAlloc);\n\tiinfo->i_lenAlloc = 0;\n\tif (UDF_QUERY_FLAG(inode->i_sb, UDF_FLAG_USE_SHORT_AD))\n\t\tiinfo->i_alloc_type = ICBTAG_FLAG_AD_SHORT;\n\telse\n\t\tiinfo->i_alloc_type = ICBTAG_FLAG_AD_LONG;\n\t/* from now on we have normal address_space methods */\n\tinode->i_data.a_ops = &udf_aops;\n\tup_write(&iinfo->i_data_sem);\n\terr = inode->i_data.a_ops->writepage(page, &udf_wbc);\n\tif (err) {\n\t\t/* Restore everything back so that we don't lose data... */\n\t\tlock_page(page);\n\t\tdown_write(&iinfo->i_data_sem);\n\t\tkaddr = kmap_atomic(page);\n\t\tmemcpy(iinfo->i_data + iinfo->i_lenEAttr, kaddr, inode->i_size);\n\t\tkunmap_atomic(kaddr);\n\t\tunlock_page(page);\n\t\tiinfo->i_alloc_type = ICBTAG_FLAG_AD_IN_ICB;\n\t\tinode->i_data.a_ops = &udf_adinicb_aops;\n\t\tup_write(&iinfo->i_data_sem);\n\t}\n\tput_page(page);\n\tmark_inode_dirty(inode);\n\n\treturn err;\n}",
                        "code_after_change": "static int f2fs_read_data_page(struct file *file, struct page *page)\n{\n\tstruct inode *inode = page_file_mapping(page)->host;\n\tint ret = -EAGAIN;\n\n\ttrace_f2fs_readpage(page, DATA);\n\n\t/* If the file has inline data, try to read it directly */\n\tif (f2fs_has_inline_data(inode))\n\t\tret = f2fs_read_inline_data(inode, page);\n\tif (ret == -EAGAIN)\n\t\tret = f2fs_mpage_readpages(page_file_mapping(page),\n\t\t\t\t\t\tNULL, page, 1, false);\n\treturn ret;\n}",
                        "cve_id": "CVE-2022-0617"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int f2fs_read_data_page(struct file *file, struct page *page)\n{\n\tstruct inode *inode = page->mapping->host;\n\tint ret = -EAGAIN;\n\n\ttrace_f2fs_readpage(page, DATA);\n\n\t/* If the file has inline data, try to read it directly */\n\tif (f2fs_has_inline_data(inode))\n\t\tret = f2fs_read_inline_data(inode, page);\n\tif (ret == -EAGAIN)\n\t\tret = f2fs_mpage_readpages(page->mapping, NULL, page, 1, false);\n\treturn ret;\n}",
                        "code_after_change": "int udf_expand_file_adinicb(struct inode *inode)\n{\n\tstruct page *page;\n\tchar *kaddr;\n\tstruct udf_inode_info *iinfo = UDF_I(inode);\n\tint err;\n\n\tWARN_ON_ONCE(!inode_is_locked(inode));\n\tif (!iinfo->i_lenAlloc) {\n\t\tif (UDF_QUERY_FLAG(inode->i_sb, UDF_FLAG_USE_SHORT_AD))\n\t\t\tiinfo->i_alloc_type = ICBTAG_FLAG_AD_SHORT;\n\t\telse\n\t\t\tiinfo->i_alloc_type = ICBTAG_FLAG_AD_LONG;\n\t\t/* from now on we have normal address_space methods */\n\t\tinode->i_data.a_ops = &udf_aops;\n\t\tup_write(&iinfo->i_data_sem);\n\t\tmark_inode_dirty(inode);\n\t\treturn 0;\n\t}\n\t/*\n\t * Release i_data_sem so that we can lock a page - page lock ranks\n\t * above i_data_sem. i_mutex still protects us against file changes.\n\t */\n\tup_write(&iinfo->i_data_sem);\n\n\tpage = find_or_create_page(inode->i_mapping, 0, GFP_NOFS);\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tif (!PageUptodate(page)) {\n\t\tkaddr = kmap_atomic(page);\n\t\tmemset(kaddr + iinfo->i_lenAlloc, 0x00,\n\t\t       PAGE_SIZE - iinfo->i_lenAlloc);\n\t\tmemcpy(kaddr, iinfo->i_data + iinfo->i_lenEAttr,\n\t\t\tiinfo->i_lenAlloc);\n\t\tflush_dcache_page(page);\n\t\tSetPageUptodate(page);\n\t\tkunmap_atomic(kaddr);\n\t}\n\tdown_write(&iinfo->i_data_sem);\n\tmemset(iinfo->i_data + iinfo->i_lenEAttr, 0x00,\n\t       iinfo->i_lenAlloc);\n\tiinfo->i_lenAlloc = 0;\n\tif (UDF_QUERY_FLAG(inode->i_sb, UDF_FLAG_USE_SHORT_AD))\n\t\tiinfo->i_alloc_type = ICBTAG_FLAG_AD_SHORT;\n\telse\n\t\tiinfo->i_alloc_type = ICBTAG_FLAG_AD_LONG;\n\t/* from now on we have normal address_space methods */\n\tinode->i_data.a_ops = &udf_aops;\n\tset_page_dirty(page);\n\tunlock_page(page);\n\tup_write(&iinfo->i_data_sem);\n\terr = filemap_fdatawrite(inode->i_mapping);\n\tif (err) {\n\t\t/* Restore everything back so that we don't lose data... */\n\t\tlock_page(page);\n\t\tdown_write(&iinfo->i_data_sem);\n\t\tkaddr = kmap_atomic(page);\n\t\tmemcpy(iinfo->i_data + iinfo->i_lenEAttr, kaddr, inode->i_size);\n\t\tkunmap_atomic(kaddr);\n\t\tunlock_page(page);\n\t\tiinfo->i_alloc_type = ICBTAG_FLAG_AD_IN_ICB;\n\t\tinode->i_data.a_ops = &udf_adinicb_aops;\n\t\tup_write(&iinfo->i_data_sem);\n\t}\n\tput_page(page);\n\tmark_inode_dirty(inode);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2019-19815"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static enum count_type __read_io_type(struct page *page)\n{\n\tstruct address_space *mapping = page->mapping;\n\n\tif (mapping) {\n\t\tstruct inode *inode = mapping->host;\n\t\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\t\tif (inode->i_ino == F2FS_META_INO(sbi))\n\t\t\treturn F2FS_RD_META;\n\n\t\tif (inode->i_ino == F2FS_NODE_INO(sbi))\n\t\t\treturn F2FS_RD_NODE;\n\t}\n\treturn F2FS_RD_DATA;\n}",
                        "code_after_change": "static enum count_type __read_io_type(struct page *page)\n{\n\tstruct address_space *mapping = page_file_mapping(page);\n\n\tif (mapping) {\n\t\tstruct inode *inode = mapping->host;\n\t\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\t\tif (inode->i_ino == F2FS_META_INO(sbi))\n\t\t\treturn F2FS_RD_META;\n\n\t\tif (inode->i_ino == F2FS_NODE_INO(sbi))\n\t\t\treturn F2FS_RD_NODE;\n\t}\n\treturn F2FS_RD_DATA;\n}",
                        "cve_id": "CVE-2019-19815"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int migrate_page_move_mapping(struct address_space *mapping,\n\t\tstruct page *newpage, struct page *page,\n\t\tstruct buffer_head *head, enum migrate_mode mode,\n\t\tint extra_count)\n{\n\tint expected_count = 1 + extra_count;\n\tvoid **pslot;\n\n\tif (!mapping) {\n\t\t/* Anonymous page without mapping */\n\t\tif (page_count(page) != expected_count)\n\t\t\treturn -EAGAIN;\n\n\t\t/* No turning back from here */\n\t\tset_page_memcg(newpage, page_memcg(page));\n\t\tnewpage->index = page->index;\n\t\tnewpage->mapping = page->mapping;\n\t\tif (PageSwapBacked(page))\n\t\t\tSetPageSwapBacked(newpage);\n\n\t\treturn MIGRATEPAGE_SUCCESS;\n\t}\n\n\tspin_lock_irq(&mapping->tree_lock);\n\n\tpslot = radix_tree_lookup_slot(&mapping->page_tree,\n \t\t\t\t\tpage_index(page));\n\n\texpected_count += 1 + page_has_private(page);\n\tif (page_count(page) != expected_count ||\n\t\tradix_tree_deref_slot_protected(pslot, &mapping->tree_lock) != page) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\tif (!page_freeze_refs(page, expected_count)) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * In the async migration case of moving a page with buffers, lock the\n\t * buffers using trylock before the mapping is moved. If the mapping\n\t * was moved, we later failed to lock the buffers and could not move\n\t * the mapping back due to an elevated page count, we would have to\n\t * block waiting on other references to be dropped.\n\t */\n\tif (mode == MIGRATE_ASYNC && head &&\n\t\t\t!buffer_migrate_lock_buffers(head, mode)) {\n\t\tpage_unfreeze_refs(page, expected_count);\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * Now we know that no one else is looking at the page:\n\t * no turning back from here.\n\t */\n\tset_page_memcg(newpage, page_memcg(page));\n\tnewpage->index = page->index;\n\tnewpage->mapping = page->mapping;\n\tif (PageSwapBacked(page))\n\t\tSetPageSwapBacked(newpage);\n\n\tget_page(newpage);\t/* add cache reference */\n\tif (PageSwapCache(page)) {\n\t\tSetPageSwapCache(newpage);\n\t\tset_page_private(newpage, page_private(page));\n\t}\n\n\tradix_tree_replace_slot(pslot, newpage);\n\n\t/*\n\t * Drop cache reference from old page by unfreezing\n\t * to one less reference.\n\t * We know this isn't the last reference.\n\t */\n\tpage_unfreeze_refs(page, expected_count - 1);\n\n\t/*\n\t * If moved to a different zone then also account\n\t * the page for that zone. Other VM counters will be\n\t * taken care of when we establish references to the\n\t * new page and drop references to the old page.\n\t *\n\t * Note that anonymous pages are accounted for\n\t * via NR_FILE_PAGES and NR_ANON_PAGES if they\n\t * are mapped to swap space.\n\t */\n\t__dec_zone_page_state(page, NR_FILE_PAGES);\n\t__inc_zone_page_state(newpage, NR_FILE_PAGES);\n\tif (!PageSwapCache(page) && PageSwapBacked(page)) {\n\t\t__dec_zone_page_state(page, NR_SHMEM);\n\t\t__inc_zone_page_state(newpage, NR_SHMEM);\n\t}\n\tspin_unlock_irq(&mapping->tree_lock);\n\n\treturn MIGRATEPAGE_SUCCESS;\n}",
                        "code_after_change": "int migrate_page_move_mapping(struct address_space *mapping,\n\t\tstruct page *newpage, struct page *page,\n\t\tstruct buffer_head *head, enum migrate_mode mode,\n\t\tint extra_count)\n{\n\tstruct zone *oldzone, *newzone;\n\tint dirty;\n\tint expected_count = 1 + extra_count;\n\tvoid **pslot;\n\n\tif (!mapping) {\n\t\t/* Anonymous page without mapping */\n\t\tif (page_count(page) != expected_count)\n\t\t\treturn -EAGAIN;\n\n\t\t/* No turning back from here */\n\t\tset_page_memcg(newpage, page_memcg(page));\n\t\tnewpage->index = page->index;\n\t\tnewpage->mapping = page->mapping;\n\t\tif (PageSwapBacked(page))\n\t\t\tSetPageSwapBacked(newpage);\n\n\t\treturn MIGRATEPAGE_SUCCESS;\n\t}\n\n\toldzone = page_zone(page);\n\tnewzone = page_zone(newpage);\n\n\tspin_lock_irq(&mapping->tree_lock);\n\n\tpslot = radix_tree_lookup_slot(&mapping->page_tree,\n \t\t\t\t\tpage_index(page));\n\n\texpected_count += 1 + page_has_private(page);\n\tif (page_count(page) != expected_count ||\n\t\tradix_tree_deref_slot_protected(pslot, &mapping->tree_lock) != page) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\tif (!page_freeze_refs(page, expected_count)) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * In the async migration case of moving a page with buffers, lock the\n\t * buffers using trylock before the mapping is moved. If the mapping\n\t * was moved, we later failed to lock the buffers and could not move\n\t * the mapping back due to an elevated page count, we would have to\n\t * block waiting on other references to be dropped.\n\t */\n\tif (mode == MIGRATE_ASYNC && head &&\n\t\t\t!buffer_migrate_lock_buffers(head, mode)) {\n\t\tpage_unfreeze_refs(page, expected_count);\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * Now we know that no one else is looking at the page:\n\t * no turning back from here.\n\t */\n\tset_page_memcg(newpage, page_memcg(page));\n\tnewpage->index = page->index;\n\tnewpage->mapping = page->mapping;\n\tif (PageSwapBacked(page))\n\t\tSetPageSwapBacked(newpage);\n\n\tget_page(newpage);\t/* add cache reference */\n\tif (PageSwapCache(page)) {\n\t\tSetPageSwapCache(newpage);\n\t\tset_page_private(newpage, page_private(page));\n\t}\n\n\t/* Move dirty while page refs frozen and newpage not yet exposed */\n\tdirty = PageDirty(page);\n\tif (dirty) {\n\t\tClearPageDirty(page);\n\t\tSetPageDirty(newpage);\n\t}\n\n\tradix_tree_replace_slot(pslot, newpage);\n\n\t/*\n\t * Drop cache reference from old page by unfreezing\n\t * to one less reference.\n\t * We know this isn't the last reference.\n\t */\n\tpage_unfreeze_refs(page, expected_count - 1);\n\n\tspin_unlock(&mapping->tree_lock);\n\t/* Leave irq disabled to prevent preemption while updating stats */\n\n\t/*\n\t * If moved to a different zone then also account\n\t * the page for that zone. Other VM counters will be\n\t * taken care of when we establish references to the\n\t * new page and drop references to the old page.\n\t *\n\t * Note that anonymous pages are accounted for\n\t * via NR_FILE_PAGES and NR_ANON_PAGES if they\n\t * are mapped to swap space.\n\t */\n\tif (newzone != oldzone) {\n\t\t__dec_zone_state(oldzone, NR_FILE_PAGES);\n\t\t__inc_zone_state(newzone, NR_FILE_PAGES);\n\t\tif (PageSwapBacked(page) && !PageSwapCache(page)) {\n\t\t\t__dec_zone_state(oldzone, NR_SHMEM);\n\t\t\t__inc_zone_state(newzone, NR_SHMEM);\n\t\t}\n\t\tif (dirty && mapping_cap_account_dirty(mapping)) {\n\t\t\t__dec_zone_state(oldzone, NR_FILE_DIRTY);\n\t\t\t__inc_zone_state(newzone, NR_FILE_DIRTY);\n\t\t}\n\t}\n\tlocal_irq_enable();\n\n\treturn MIGRATEPAGE_SUCCESS;\n}",
                        "cve_id": "CVE-2016-3070"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,\n\t\t\t\t\tunsigned nr_pages, unsigned op_flag)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct bio *bio;\n\tstruct bio_post_read_ctx *ctx;\n\tunsigned int post_read_steps = 0;\n\n\tbio = f2fs_bio_alloc(sbi, min_t(int, nr_pages, BIO_MAX_PAGES), false);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\tf2fs_target_device(sbi, blkaddr, bio);\n\tbio->bi_end_io = f2fs_read_end_io;\n\tbio_set_op_attrs(bio, REQ_OP_READ, op_flag);\n\n\tif (f2fs_encrypted_file(inode))\n\t\tpost_read_steps |= 1 << STEP_DECRYPT;\n\tif (post_read_steps) {\n\t\tctx = mempool_alloc(bio_post_read_ctx_pool, GFP_NOFS);\n\t\tif (!ctx) {\n\t\t\tbio_put(bio);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\tctx->bio = bio;\n\t\tctx->enabled_steps = post_read_steps;\n\t\tbio->bi_private = ctx;\n\n\t\t/* wait the page to be moved by cleaning */\n\t\tf2fs_wait_on_block_writeback(sbi, blkaddr);\n\t}\n\n\treturn bio;\n}",
                        "code_after_change": "static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,\n\t\t\t\t\tunsigned nr_pages, unsigned op_flag)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct bio *bio;\n\tstruct bio_post_read_ctx *ctx;\n\tunsigned int post_read_steps = 0;\n\n\tif (!f2fs_is_valid_blkaddr(sbi, blkaddr, DATA_GENERIC))\n\t\treturn ERR_PTR(-EFAULT);\n\n\tbio = f2fs_bio_alloc(sbi, min_t(int, nr_pages, BIO_MAX_PAGES), false);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\tf2fs_target_device(sbi, blkaddr, bio);\n\tbio->bi_end_io = f2fs_read_end_io;\n\tbio_set_op_attrs(bio, REQ_OP_READ, op_flag);\n\n\tif (f2fs_encrypted_file(inode))\n\t\tpost_read_steps |= 1 << STEP_DECRYPT;\n\tif (post_read_steps) {\n\t\tctx = mempool_alloc(bio_post_read_ctx_pool, GFP_NOFS);\n\t\tif (!ctx) {\n\t\t\tbio_put(bio);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\tctx->bio = bio;\n\t\tctx->enabled_steps = post_read_steps;\n\t\tbio->bi_private = ctx;\n\n\t\t/* wait the page to be moved by cleaning */\n\t\tf2fs_wait_on_block_writeback(sbi, blkaddr);\n\t}\n\n\treturn bio;\n}",
                        "cve_id": "CVE-2018-14616"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 2359,
            "cve_id": "CVE-2019-19815",
            "code_snippet": "static int f2fs_mpage_readpages(struct address_space *mapping,\n\t\t\tstruct list_head *pages, struct page *page,\n\t\t\tunsigned nr_pages, bool is_readahead)\n{\n\tstruct bio *bio = NULL;\n\tsector_t last_block_in_bio = 0;\n\tstruct inode *inode = mapping->host;\n\tstruct f2fs_map_blocks map;\n\tint ret = 0;\n\n\tmap.m_pblk = 0;\n\tmap.m_lblk = 0;\n\tmap.m_len = 0;\n\tmap.m_flags = 0;\n\tmap.m_next_pgofs = NULL;\n\tmap.m_next_extent = NULL;\n\tmap.m_seg_type = NO_CHECK_TYPE;\n\tmap.m_may_create = false;\n\n\tfor (; nr_pages; nr_pages--) {\n\t\tif (pages) {\n\t\t\tpage = list_last_entry(pages, struct page, lru);\n\n\t\t\tprefetchw(&page->flags);\n\t\t\tlist_del(&page->lru);\n\t\t\tif (add_to_page_cache_lru(page, mapping,\n\t\t\t\t\t\t  page_index(page),\n\t\t\t\t\t\t  readahead_gfp_mask(mapping)))\n\t\t\t\tgoto next_page;\n\t\t}\n\n\t\tret = f2fs_read_single_page(inode, page, nr_pages, &map, &bio,\n\t\t\t\t\t&last_block_in_bio, is_readahead);\n\t\tif (ret) {\n\t\t\tSetPageError(page);\n\t\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\t\tunlock_page(page);\n\t\t}\nnext_page:\n\t\tif (pages)\n\t\t\tput_page(page);\n\t}\n\tBUG_ON(pages && !list_empty(pages));\n\tif (bio)\n\t\t__submit_bio(F2FS_I_SB(inode), bio, DATA);\n\treturn pages ? 0 : ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int __rds_rdma_map(struct rds_sock *rs, struct rds_get_mr_args *args,\n\t\t\t\tu64 *cookie_ret, struct rds_mr **mr_ret)\n{\n\tstruct rds_mr *mr = NULL, *found;\n\tunsigned int nr_pages;\n\tstruct page **pages = NULL;\n\tstruct scatterlist *sg;\n\tvoid *trans_private;\n\tunsigned long flags;\n\trds_rdma_cookie_t cookie;\n\tunsigned int nents;\n\tlong i;\n\tint ret;\n\n\tif (rs->rs_bound_addr == 0) {\n\t\tret = -ENOTCONN; /* XXX not a great errno */\n\t\tgoto out;\n\t}\n\n\tif (!rs->rs_transport->get_mr) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tnr_pages = rds_pages_in_vec(&args->vec);\n\tif (nr_pages == 0) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* Restrict the size of mr irrespective of underlying transport\n\t * To account for unaligned mr regions, subtract one from nr_pages\n\t */\n\tif ((nr_pages - 1) > (RDS_MAX_MSG_SIZE >> PAGE_SHIFT)) {\n\t\tret = -EMSGSIZE;\n\t\tgoto out;\n\t}\n\n\trdsdebug(\"RDS: get_mr addr %llx len %llu nr_pages %u\\n\",\n\t\targs->vec.addr, args->vec.bytes, nr_pages);\n\n\t/* XXX clamp nr_pages to limit the size of this alloc? */\n\tpages = kcalloc(nr_pages, sizeof(struct page *), GFP_KERNEL);\n\tif (!pages) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tmr = kzalloc(sizeof(struct rds_mr), GFP_KERNEL);\n\tif (!mr) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trefcount_set(&mr->r_refcount, 1);\n\tRB_CLEAR_NODE(&mr->r_rb_node);\n\tmr->r_trans = rs->rs_transport;\n\tmr->r_sock = rs;\n\n\tif (args->flags & RDS_RDMA_USE_ONCE)\n\t\tmr->r_use_once = 1;\n\tif (args->flags & RDS_RDMA_INVALIDATE)\n\t\tmr->r_invalidate = 1;\n\tif (args->flags & RDS_RDMA_READWRITE)\n\t\tmr->r_write = 1;\n\n\t/*\n\t * Pin the pages that make up the user buffer and transfer the page\n\t * pointers to the mr's sg array.  We check to see if we've mapped\n\t * the whole region after transferring the partial page references\n\t * to the sg array so that we can have one page ref cleanup path.\n\t *\n\t * For now we have no flag that tells us whether the mapping is\n\t * r/o or r/w. We need to assume r/w, or we'll do a lot of RDMA to\n\t * the zero page.\n\t */\n\tret = rds_pin_pages(args->vec.addr, nr_pages, pages, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tnents = ret;\n\tsg = kcalloc(nents, sizeof(*sg), GFP_KERNEL);\n\tif (!sg) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tWARN_ON(!nents);\n\tsg_init_table(sg, nents);\n\n\t/* Stick all pages into the scatterlist */\n\tfor (i = 0 ; i < nents; i++)\n\t\tsg_set_page(&sg[i], pages[i], PAGE_SIZE, 0);\n\n\trdsdebug(\"RDS: trans_private nents is %u\\n\", nents);\n\n\t/* Obtain a transport specific MR. If this succeeds, the\n\t * s/g list is now owned by the MR.\n\t * Note that dma_map() implies that pending writes are\n\t * flushed to RAM, so no dma_sync is needed here. */\n\ttrans_private = rs->rs_transport->get_mr(sg, nents, rs,\n\t\t\t\t\t\t &mr->r_key);\n\n\tif (IS_ERR(trans_private)) {\n\t\tfor (i = 0 ; i < nents; i++)\n\t\t\tput_page(sg_page(&sg[i]));\n\t\tkfree(sg);\n\t\tret = PTR_ERR(trans_private);\n\t\tgoto out;\n\t}\n\n\tmr->r_trans_private = trans_private;\n\n\trdsdebug(\"RDS: get_mr put_user key is %x cookie_addr %p\\n\",\n\t       mr->r_key, (void *)(unsigned long) args->cookie_addr);\n\n\t/* The user may pass us an unaligned address, but we can only\n\t * map page aligned regions. So we keep the offset, and build\n\t * a 64bit cookie containing <R_Key, offset> and pass that\n\t * around. */\n\tcookie = rds_rdma_make_cookie(mr->r_key, args->vec.addr & ~PAGE_MASK);\n\tif (cookie_ret)\n\t\t*cookie_ret = cookie;\n\n\tif (args->cookie_addr && put_user(cookie, (u64 __user *)(unsigned long) args->cookie_addr)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\t/* Inserting the new MR into the rbtree bumps its\n\t * reference count. */\n\tspin_lock_irqsave(&rs->rs_rdma_lock, flags);\n\tfound = rds_mr_tree_walk(&rs->rs_rdma_keys, mr->r_key, mr);\n\tspin_unlock_irqrestore(&rs->rs_rdma_lock, flags);\n\n\tBUG_ON(found && found != mr);\n\n\trdsdebug(\"RDS: get_mr key is %x\\n\", mr->r_key);\n\tif (mr_ret) {\n\t\trefcount_inc(&mr->r_refcount);\n\t\t*mr_ret = mr;\n\t}\n\n\tret = 0;\nout:\n\tkfree(pages);\n\tif (mr)\n\t\trds_mr_put(mr);\n\treturn ret;\n}",
                        "code_after_change": "static int __rds_rdma_map(struct rds_sock *rs, struct rds_get_mr_args *args,\n\t\t\t\tu64 *cookie_ret, struct rds_mr **mr_ret)\n{\n\tstruct rds_mr *mr = NULL, *found;\n\tunsigned int nr_pages;\n\tstruct page **pages = NULL;\n\tstruct scatterlist *sg;\n\tvoid *trans_private;\n\tunsigned long flags;\n\trds_rdma_cookie_t cookie;\n\tunsigned int nents;\n\tlong i;\n\tint ret;\n\n\tif (rs->rs_bound_addr == 0 || !rs->rs_transport) {\n\t\tret = -ENOTCONN; /* XXX not a great errno */\n\t\tgoto out;\n\t}\n\n\tif (!rs->rs_transport->get_mr) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tnr_pages = rds_pages_in_vec(&args->vec);\n\tif (nr_pages == 0) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* Restrict the size of mr irrespective of underlying transport\n\t * To account for unaligned mr regions, subtract one from nr_pages\n\t */\n\tif ((nr_pages - 1) > (RDS_MAX_MSG_SIZE >> PAGE_SHIFT)) {\n\t\tret = -EMSGSIZE;\n\t\tgoto out;\n\t}\n\n\trdsdebug(\"RDS: get_mr addr %llx len %llu nr_pages %u\\n\",\n\t\targs->vec.addr, args->vec.bytes, nr_pages);\n\n\t/* XXX clamp nr_pages to limit the size of this alloc? */\n\tpages = kcalloc(nr_pages, sizeof(struct page *), GFP_KERNEL);\n\tif (!pages) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tmr = kzalloc(sizeof(struct rds_mr), GFP_KERNEL);\n\tif (!mr) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trefcount_set(&mr->r_refcount, 1);\n\tRB_CLEAR_NODE(&mr->r_rb_node);\n\tmr->r_trans = rs->rs_transport;\n\tmr->r_sock = rs;\n\n\tif (args->flags & RDS_RDMA_USE_ONCE)\n\t\tmr->r_use_once = 1;\n\tif (args->flags & RDS_RDMA_INVALIDATE)\n\t\tmr->r_invalidate = 1;\n\tif (args->flags & RDS_RDMA_READWRITE)\n\t\tmr->r_write = 1;\n\n\t/*\n\t * Pin the pages that make up the user buffer and transfer the page\n\t * pointers to the mr's sg array.  We check to see if we've mapped\n\t * the whole region after transferring the partial page references\n\t * to the sg array so that we can have one page ref cleanup path.\n\t *\n\t * For now we have no flag that tells us whether the mapping is\n\t * r/o or r/w. We need to assume r/w, or we'll do a lot of RDMA to\n\t * the zero page.\n\t */\n\tret = rds_pin_pages(args->vec.addr, nr_pages, pages, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tnents = ret;\n\tsg = kcalloc(nents, sizeof(*sg), GFP_KERNEL);\n\tif (!sg) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tWARN_ON(!nents);\n\tsg_init_table(sg, nents);\n\n\t/* Stick all pages into the scatterlist */\n\tfor (i = 0 ; i < nents; i++)\n\t\tsg_set_page(&sg[i], pages[i], PAGE_SIZE, 0);\n\n\trdsdebug(\"RDS: trans_private nents is %u\\n\", nents);\n\n\t/* Obtain a transport specific MR. If this succeeds, the\n\t * s/g list is now owned by the MR.\n\t * Note that dma_map() implies that pending writes are\n\t * flushed to RAM, so no dma_sync is needed here. */\n\ttrans_private = rs->rs_transport->get_mr(sg, nents, rs,\n\t\t\t\t\t\t &mr->r_key);\n\n\tif (IS_ERR(trans_private)) {\n\t\tfor (i = 0 ; i < nents; i++)\n\t\t\tput_page(sg_page(&sg[i]));\n\t\tkfree(sg);\n\t\tret = PTR_ERR(trans_private);\n\t\tgoto out;\n\t}\n\n\tmr->r_trans_private = trans_private;\n\n\trdsdebug(\"RDS: get_mr put_user key is %x cookie_addr %p\\n\",\n\t       mr->r_key, (void *)(unsigned long) args->cookie_addr);\n\n\t/* The user may pass us an unaligned address, but we can only\n\t * map page aligned regions. So we keep the offset, and build\n\t * a 64bit cookie containing <R_Key, offset> and pass that\n\t * around. */\n\tcookie = rds_rdma_make_cookie(mr->r_key, args->vec.addr & ~PAGE_MASK);\n\tif (cookie_ret)\n\t\t*cookie_ret = cookie;\n\n\tif (args->cookie_addr && put_user(cookie, (u64 __user *)(unsigned long) args->cookie_addr)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\t/* Inserting the new MR into the rbtree bumps its\n\t * reference count. */\n\tspin_lock_irqsave(&rs->rs_rdma_lock, flags);\n\tfound = rds_mr_tree_walk(&rs->rs_rdma_keys, mr->r_key, mr);\n\tspin_unlock_irqrestore(&rs->rs_rdma_lock, flags);\n\n\tBUG_ON(found && found != mr);\n\n\trdsdebug(\"RDS: get_mr key is %x\\n\", mr->r_key);\n\tif (mr_ret) {\n\t\trefcount_inc(&mr->r_refcount);\n\t\t*mr_ret = mr;\n\t}\n\n\tret = 0;\nout:\n\tkfree(pages);\n\tif (mr)\n\t\trds_mr_put(mr);\n\treturn ret;\n}",
                        "cve_id": "CVE-2018-7492"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,\n\t\t\t\t\tunsigned nr_pages, unsigned op_flag)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct bio *bio;\n\tstruct bio_post_read_ctx *ctx;\n\tunsigned int post_read_steps = 0;\n\n\tbio = f2fs_bio_alloc(sbi, min_t(int, nr_pages, BIO_MAX_PAGES), false);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\tf2fs_target_device(sbi, blkaddr, bio);\n\tbio->bi_end_io = f2fs_read_end_io;\n\tbio_set_op_attrs(bio, REQ_OP_READ, op_flag);\n\n\tif (f2fs_encrypted_file(inode))\n\t\tpost_read_steps |= 1 << STEP_DECRYPT;\n\tif (post_read_steps) {\n\t\tctx = mempool_alloc(bio_post_read_ctx_pool, GFP_NOFS);\n\t\tif (!ctx) {\n\t\t\tbio_put(bio);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\tctx->bio = bio;\n\t\tctx->enabled_steps = post_read_steps;\n\t\tbio->bi_private = ctx;\n\n\t\t/* wait the page to be moved by cleaning */\n\t\tf2fs_wait_on_block_writeback(sbi, blkaddr);\n\t}\n\n\treturn bio;\n}",
                        "code_after_change": "static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,\n\t\t\t\t\tunsigned nr_pages, unsigned op_flag)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct bio *bio;\n\tstruct bio_post_read_ctx *ctx;\n\tunsigned int post_read_steps = 0;\n\n\tif (!f2fs_is_valid_blkaddr(sbi, blkaddr, DATA_GENERIC))\n\t\treturn ERR_PTR(-EFAULT);\n\n\tbio = f2fs_bio_alloc(sbi, min_t(int, nr_pages, BIO_MAX_PAGES), false);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\tf2fs_target_device(sbi, blkaddr, bio);\n\tbio->bi_end_io = f2fs_read_end_io;\n\tbio_set_op_attrs(bio, REQ_OP_READ, op_flag);\n\n\tif (f2fs_encrypted_file(inode))\n\t\tpost_read_steps |= 1 << STEP_DECRYPT;\n\tif (post_read_steps) {\n\t\tctx = mempool_alloc(bio_post_read_ctx_pool, GFP_NOFS);\n\t\tif (!ctx) {\n\t\t\tbio_put(bio);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\tctx->bio = bio;\n\t\tctx->enabled_steps = post_read_steps;\n\t\tbio->bi_private = ctx;\n\n\t\t/* wait the page to be moved by cleaning */\n\t\tf2fs_wait_on_block_writeback(sbi, blkaddr);\n\t}\n\n\treturn bio;\n}",
                        "cve_id": "CVE-2018-14616"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2516,
            "cve_id": "CVE-2020-10711",
            "code_snippet": "static int cipso_v4_parsetag_rbm(const struct cipso_v4_doi *doi_def,\n\t\t\t\t const unsigned char *tag,\n\t\t\t\t struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tu8 tag_len = tag[1];\n\tu32 level;\n\n\tret_val = cipso_v4_map_lvl_ntoh(doi_def, tag[3], &level);\n\tif (ret_val != 0)\n\t\treturn ret_val;\n\tsecattr->attr.mls.lvl = level;\n\tsecattr->flags |= NETLBL_SECATTR_MLS_LVL;\n\n\tif (tag_len > 4) {\n\t\tret_val = cipso_v4_map_cat_rbm_ntoh(doi_def,\n\t\t\t\t\t\t    &tag[4],\n\t\t\t\t\t\t    tag_len - 4,\n\t\t\t\t\t\t    secattr);\n\t\tif (ret_val != 0) {\n\t\t\tnetlbl_catmap_free(secattr->attr.mls.cat);\n\t\t\treturn ret_val;\n\t\t}\n\n\t\tif (secattr->attr.mls.cat)\n\t\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;\n\t}\n\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int cipso_v4_parsetag_rng(const struct cipso_v4_doi *doi_def,\n\t\t\t\t const unsigned char *tag,\n\t\t\t\t struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tu8 tag_len = tag[1];\n\tu32 level;\n\n\tret_val = cipso_v4_map_lvl_ntoh(doi_def, tag[3], &level);\n\tif (ret_val != 0)\n\t\treturn ret_val;\n\tsecattr->attr.mls.lvl = level;\n\tsecattr->flags |= NETLBL_SECATTR_MLS_LVL;\n\n\tif (tag_len > 4) {\n\t\tret_val = cipso_v4_map_cat_rng_ntoh(doi_def,\n\t\t\t\t\t\t    &tag[4],\n\t\t\t\t\t\t    tag_len - 4,\n\t\t\t\t\t\t    secattr);\n\t\tif (ret_val != 0) {\n\t\t\tnetlbl_catmap_free(secattr->attr.mls.cat);\n\t\t\treturn ret_val;\n\t\t}\n\n\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;\n\t}\n\n\treturn 0;\n}",
                        "code_after_change": "static int cipso_v4_parsetag_rng(const struct cipso_v4_doi *doi_def,\n\t\t\t\t const unsigned char *tag,\n\t\t\t\t struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val;\n\tu8 tag_len = tag[1];\n\tu32 level;\n\n\tret_val = cipso_v4_map_lvl_ntoh(doi_def, tag[3], &level);\n\tif (ret_val != 0)\n\t\treturn ret_val;\n\tsecattr->attr.mls.lvl = level;\n\tsecattr->flags |= NETLBL_SECATTR_MLS_LVL;\n\n\tif (tag_len > 4) {\n\t\tret_val = cipso_v4_map_cat_rng_ntoh(doi_def,\n\t\t\t\t\t\t    &tag[4],\n\t\t\t\t\t\t    tag_len - 4,\n\t\t\t\t\t\t    secattr);\n\t\tif (ret_val != 0) {\n\t\t\tnetlbl_catmap_free(secattr->attr.mls.cat);\n\t\t\treturn ret_val;\n\t\t}\n\n\t\tif (secattr->attr.mls.cat)\n\t\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;\n\t}\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2020-10711"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int calipso_opt_getattr(const unsigned char *calipso,\n\t\t\t       struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val = -ENOMSG;\n\tu32 doi, len = calipso[1], cat_len = calipso[6] * 4;\n\tstruct calipso_doi *doi_def;\n\n\tif (cat_len + 8 > len)\n\t\treturn -EINVAL;\n\n\tif (calipso_cache_check(calipso + 2, calipso[1], secattr) == 0)\n\t\treturn 0;\n\n\tdoi = get_unaligned_be32(calipso + 2);\n\trcu_read_lock();\n\tdoi_def = calipso_doi_search(doi);\n\tif (!doi_def)\n\t\tgoto getattr_return;\n\n\tsecattr->attr.mls.lvl = calipso[7];\n\tsecattr->flags |= NETLBL_SECATTR_MLS_LVL;\n\n\tif (cat_len) {\n\t\tret_val = calipso_map_cat_ntoh(doi_def,\n\t\t\t\t\t       calipso + 10,\n\t\t\t\t\t       cat_len,\n\t\t\t\t\t       secattr);\n\t\tif (ret_val != 0) {\n\t\t\tnetlbl_catmap_free(secattr->attr.mls.cat);\n\t\t\tgoto getattr_return;\n\t\t}\n\n\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;\n\t}\n\n\tsecattr->type = NETLBL_NLTYPE_CALIPSO;\n\ngetattr_return:\n\trcu_read_unlock();\n\treturn ret_val;\n}",
                        "code_after_change": "static int calipso_opt_getattr(const unsigned char *calipso,\n\t\t\t       struct netlbl_lsm_secattr *secattr)\n{\n\tint ret_val = -ENOMSG;\n\tu32 doi, len = calipso[1], cat_len = calipso[6] * 4;\n\tstruct calipso_doi *doi_def;\n\n\tif (cat_len + 8 > len)\n\t\treturn -EINVAL;\n\n\tif (calipso_cache_check(calipso + 2, calipso[1], secattr) == 0)\n\t\treturn 0;\n\n\tdoi = get_unaligned_be32(calipso + 2);\n\trcu_read_lock();\n\tdoi_def = calipso_doi_search(doi);\n\tif (!doi_def)\n\t\tgoto getattr_return;\n\n\tsecattr->attr.mls.lvl = calipso[7];\n\tsecattr->flags |= NETLBL_SECATTR_MLS_LVL;\n\n\tif (cat_len) {\n\t\tret_val = calipso_map_cat_ntoh(doi_def,\n\t\t\t\t\t       calipso + 10,\n\t\t\t\t\t       cat_len,\n\t\t\t\t\t       secattr);\n\t\tif (ret_val != 0) {\n\t\t\tnetlbl_catmap_free(secattr->attr.mls.cat);\n\t\t\tgoto getattr_return;\n\t\t}\n\n\t\tif (secattr->attr.mls.cat)\n\t\t\tsecattr->flags |= NETLBL_SECATTR_MLS_CAT;\n\t}\n\n\tsecattr->type = NETLBL_NLTYPE_CALIPSO;\n\ngetattr_return:\n\trcu_read_unlock();\n\treturn ret_val;\n}",
                        "cve_id": "CVE-2020-10711"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2540,
            "cve_id": "CVE-2020-11608",
            "code_snippet": "static void ov511_mode_init_regs(struct sd *sd)\n{\n\tstruct gspca_dev *gspca_dev = (struct gspca_dev *)sd;\n\tint hsegs, vsegs, packet_size, fps, needed;\n\tint interlaced = 0;\n\tstruct usb_host_interface *alt;\n\tstruct usb_interface *intf;\n\n\tintf = usb_ifnum_to_if(sd->gspca_dev.dev, sd->gspca_dev.iface);\n\talt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);\n\tif (!alt) {\n\t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n\t\tsd->gspca_dev.usb_err = -EIO;\n\t\treturn;\n\t}\n\n\tif (alt->desc.bNumEndpoints < 1) {\n\t\tsd->gspca_dev.usb_err = -ENODEV;\n\t\treturn;\n\t}\n\n\tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n\treg_w(sd, R51x_FIFO_PSIZE, packet_size >> 5);\n\n\treg_w(sd, R511_CAM_UV_EN, 0x01);\n\treg_w(sd, R511_SNAP_UV_EN, 0x01);\n\treg_w(sd, R511_SNAP_OPTS, 0x03);\n\n\t/* Here I'm assuming that snapshot size == image size.\n\t * I hope that's always true. --claudio\n\t */\n\thsegs = (sd->gspca_dev.pixfmt.width >> 3) - 1;\n\tvsegs = (sd->gspca_dev.pixfmt.height >> 3) - 1;\n\n\treg_w(sd, R511_CAM_PXCNT, hsegs);\n\treg_w(sd, R511_CAM_LNCNT, vsegs);\n\treg_w(sd, R511_CAM_PXDIV, 0x00);\n\treg_w(sd, R511_CAM_LNDIV, 0x00);\n\n\t/* YUV420, low pass filter on */\n\treg_w(sd, R511_CAM_OPTS, 0x03);\n\n\t/* Snapshot additions */\n\treg_w(sd, R511_SNAP_PXCNT, hsegs);\n\treg_w(sd, R511_SNAP_LNCNT, vsegs);\n\treg_w(sd, R511_SNAP_PXDIV, 0x00);\n\treg_w(sd, R511_SNAP_LNDIV, 0x00);\n\n\t/******** Set the framerate ********/\n\tif (frame_rate > 0)\n\t\tsd->frame_rate = frame_rate;\n\n\tswitch (sd->sensor) {\n\tcase SEN_OV6620:\n\t\t/* No framerate control, doesn't like higher rates yet */\n\t\tsd->clockdiv = 3;\n\t\tbreak;\n\n\t/* Note once the FIXME's in mode_init_ov_sensor_regs() are fixed\n\t   for more sensors we need to do this for them too */\n\tcase SEN_OV7620:\n\tcase SEN_OV7620AE:\n\tcase SEN_OV7640:\n\tcase SEN_OV7648:\n\tcase SEN_OV76BE:\n\t\tif (sd->gspca_dev.pixfmt.width == 320)\n\t\t\tinterlaced = 1;\n\t\t/* Fall through */\n\tcase SEN_OV6630:\n\tcase SEN_OV7610:\n\tcase SEN_OV7670:\n\t\tswitch (sd->frame_rate) {\n\t\tcase 30:\n\t\tcase 25:\n\t\t\t/* Not enough bandwidth to do 640x480 @ 30 fps */\n\t\t\tif (sd->gspca_dev.pixfmt.width != 640) {\n\t\t\t\tsd->clockdiv = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* For 640x480 case */\n\t\t\t/* fall through */\n\t\tdefault:\n/*\t\tcase 20: */\n/*\t\tcase 15: */\n\t\t\tsd->clockdiv = 1;\n\t\t\tbreak;\n\t\tcase 10:\n\t\t\tsd->clockdiv = 2;\n\t\t\tbreak;\n\t\tcase 5:\n\t\t\tsd->clockdiv = 5;\n\t\t\tbreak;\n\t\t}\n\t\tif (interlaced) {\n\t\t\tsd->clockdiv = (sd->clockdiv + 1) * 2 - 1;\n\t\t\t/* Higher then 10 does not work */\n\t\t\tif (sd->clockdiv > 10)\n\t\t\t\tsd->clockdiv = 10;\n\t\t}\n\t\tbreak;\n\n\tcase SEN_OV8610:\n\t\t/* No framerate control ?? */\n\t\tsd->clockdiv = 0;\n\t\tbreak;\n\t}\n\n\t/* Check if we have enough bandwidth to disable compression */\n\tfps = (interlaced ? 60 : 30) / (sd->clockdiv + 1) + 1;\n\tneeded = fps * sd->gspca_dev.pixfmt.width *\n\t\t\tsd->gspca_dev.pixfmt.height * 3 / 2;\n\t/* 1000 isoc packets/sec */\n\tif (needed > 1000 * packet_size) {\n\t\t/* Enable Y and UV quantization and compression */\n\t\treg_w(sd, R511_COMP_EN, 0x07);\n\t\treg_w(sd, R511_COMP_LUT_EN, 0x03);\n\t} else {\n\t\treg_w(sd, R511_COMP_EN, 0x06);\n\t\treg_w(sd, R511_COMP_LUT_EN, 0x00);\n\t}\n\n\treg_w(sd, R51x_SYS_RESET, OV511_RESET_OMNICE);\n\treg_w(sd, R51x_SYS_RESET, 0);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static void ov518_mode_init_regs(struct sd *sd)\n{\n\tstruct gspca_dev *gspca_dev = (struct gspca_dev *)sd;\n\tint hsegs, vsegs, packet_size;\n\tstruct usb_host_interface *alt;\n\tstruct usb_interface *intf;\n\n\tintf = usb_ifnum_to_if(sd->gspca_dev.dev, sd->gspca_dev.iface);\n\talt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);\n\tif (!alt) {\n\t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n\t\tsd->gspca_dev.usb_err = -EIO;\n\t\treturn;\n\t}\n\n\tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n\tov518_reg_w32(sd, R51x_FIFO_PSIZE, packet_size & ~7, 2);\n\n\t/******** Set the mode ********/\n\treg_w(sd, 0x2b, 0);\n\treg_w(sd, 0x2c, 0);\n\treg_w(sd, 0x2d, 0);\n\treg_w(sd, 0x2e, 0);\n\treg_w(sd, 0x3b, 0);\n\treg_w(sd, 0x3c, 0);\n\treg_w(sd, 0x3d, 0);\n\treg_w(sd, 0x3e, 0);\n\n\tif (sd->bridge == BRIDGE_OV518) {\n\t\t/* Set 8-bit (YVYU) input format */\n\t\treg_w_mask(sd, 0x20, 0x08, 0x08);\n\n\t\t/* Set 12-bit (4:2:0) output format */\n\t\treg_w_mask(sd, 0x28, 0x80, 0xf0);\n\t\treg_w_mask(sd, 0x38, 0x80, 0xf0);\n\t} else {\n\t\treg_w(sd, 0x28, 0x80);\n\t\treg_w(sd, 0x38, 0x80);\n\t}\n\n\thsegs = sd->gspca_dev.pixfmt.width / 16;\n\tvsegs = sd->gspca_dev.pixfmt.height / 4;\n\n\treg_w(sd, 0x29, hsegs);\n\treg_w(sd, 0x2a, vsegs);\n\n\treg_w(sd, 0x39, hsegs);\n\treg_w(sd, 0x3a, vsegs);\n\n\t/* Windows driver does this here; who knows why */\n\treg_w(sd, 0x2f, 0x80);\n\n\t/******** Set the framerate ********/\n\tif (sd->bridge == BRIDGE_OV518PLUS && sd->revision == 0 &&\n\t\t\t\t\t      sd->sensor == SEN_OV7620AE)\n\t\tsd->clockdiv = 0;\n\telse\n\t\tsd->clockdiv = 1;\n\n\t/* Mode independent, but framerate dependent, regs */\n\t/* 0x51: Clock divider; Only works on some cams which use 2 crystals */\n\treg_w(sd, 0x51, 0x04);\n\treg_w(sd, 0x22, 0x18);\n\treg_w(sd, 0x23, 0xff);\n\n\tif (sd->bridge == BRIDGE_OV518PLUS) {\n\t\tswitch (sd->sensor) {\n\t\tcase SEN_OV7620AE:\n\t\t\t/*\n\t\t\t * HdG: 640x480 needs special handling on device\n\t\t\t * revision 2, we check for device revision > 0 to\n\t\t\t * avoid regressions, as we don't know the correct\n\t\t\t * thing todo for revision 1.\n\t\t\t *\n\t\t\t * Also this likely means we don't need to\n\t\t\t * differentiate between the OV7620 and OV7620AE,\n\t\t\t * earlier testing hitting this same problem likely\n\t\t\t * happened to be with revision < 2 cams using an\n\t\t\t * OV7620 and revision 2 cams using an OV7620AE.\n\t\t\t */\n\t\t\tif (sd->revision > 0 &&\n\t\t\t\t\tsd->gspca_dev.pixfmt.width == 640) {\n\t\t\t\treg_w(sd, 0x20, 0x60);\n\t\t\t\treg_w(sd, 0x21, 0x1f);\n\t\t\t} else {\n\t\t\t\treg_w(sd, 0x20, 0x00);\n\t\t\t\treg_w(sd, 0x21, 0x19);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase SEN_OV7620:\n\t\t\treg_w(sd, 0x20, 0x00);\n\t\t\treg_w(sd, 0x21, 0x19);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treg_w(sd, 0x21, 0x19);\n\t\t}\n\t} else\n\t\treg_w(sd, 0x71, 0x17);\t/* Compression-related? */\n\n\t/* FIXME: Sensor-specific */\n\t/* Bit 5 is what matters here. Of course, it is \"reserved\" */\n\ti2c_w(sd, 0x54, 0x23);\n\n\treg_w(sd, 0x2f, 0x80);\n\n\tif (sd->bridge == BRIDGE_OV518PLUS) {\n\t\treg_w(sd, 0x24, 0x94);\n\t\treg_w(sd, 0x25, 0x90);\n\t\tov518_reg_w32(sd, 0xc4,    400, 2);\t/* 190h   */\n\t\tov518_reg_w32(sd, 0xc6,    540, 2);\t/* 21ch   */\n\t\tov518_reg_w32(sd, 0xc7,    540, 2);\t/* 21ch   */\n\t\tov518_reg_w32(sd, 0xc8,    108, 2);\t/* 6ch    */\n\t\tov518_reg_w32(sd, 0xca, 131098, 3);\t/* 2001ah */\n\t\tov518_reg_w32(sd, 0xcb,    532, 2);\t/* 214h   */\n\t\tov518_reg_w32(sd, 0xcc,   2400, 2);\t/* 960h   */\n\t\tov518_reg_w32(sd, 0xcd,     32, 2);\t/* 20h    */\n\t\tov518_reg_w32(sd, 0xce,    608, 2);\t/* 260h   */\n\t} else {\n\t\treg_w(sd, 0x24, 0x9f);\n\t\treg_w(sd, 0x25, 0x90);\n\t\tov518_reg_w32(sd, 0xc4,    400, 2);\t/* 190h   */\n\t\tov518_reg_w32(sd, 0xc6,    381, 2);\t/* 17dh   */\n\t\tov518_reg_w32(sd, 0xc7,    381, 2);\t/* 17dh   */\n\t\tov518_reg_w32(sd, 0xc8,    128, 2);\t/* 80h    */\n\t\tov518_reg_w32(sd, 0xca, 183331, 3);\t/* 2cc23h */\n\t\tov518_reg_w32(sd, 0xcb,    746, 2);\t/* 2eah   */\n\t\tov518_reg_w32(sd, 0xcc,   1750, 2);\t/* 6d6h   */\n\t\tov518_reg_w32(sd, 0xcd,     45, 2);\t/* 2dh    */\n\t\tov518_reg_w32(sd, 0xce,    851, 2);\t/* 353h   */\n\t}\n\n\treg_w(sd, 0x2f, 0x80);\n}",
                        "code_after_change": "static void ov518_mode_init_regs(struct sd *sd)\n{\n\tstruct gspca_dev *gspca_dev = (struct gspca_dev *)sd;\n\tint hsegs, vsegs, packet_size;\n\tstruct usb_host_interface *alt;\n\tstruct usb_interface *intf;\n\n\tintf = usb_ifnum_to_if(sd->gspca_dev.dev, sd->gspca_dev.iface);\n\talt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);\n\tif (!alt) {\n\t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n\t\tsd->gspca_dev.usb_err = -EIO;\n\t\treturn;\n\t}\n\n\tif (alt->desc.bNumEndpoints < 1) {\n\t\tsd->gspca_dev.usb_err = -ENODEV;\n\t\treturn;\n\t}\n\n\tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n\tov518_reg_w32(sd, R51x_FIFO_PSIZE, packet_size & ~7, 2);\n\n\t/******** Set the mode ********/\n\treg_w(sd, 0x2b, 0);\n\treg_w(sd, 0x2c, 0);\n\treg_w(sd, 0x2d, 0);\n\treg_w(sd, 0x2e, 0);\n\treg_w(sd, 0x3b, 0);\n\treg_w(sd, 0x3c, 0);\n\treg_w(sd, 0x3d, 0);\n\treg_w(sd, 0x3e, 0);\n\n\tif (sd->bridge == BRIDGE_OV518) {\n\t\t/* Set 8-bit (YVYU) input format */\n\t\treg_w_mask(sd, 0x20, 0x08, 0x08);\n\n\t\t/* Set 12-bit (4:2:0) output format */\n\t\treg_w_mask(sd, 0x28, 0x80, 0xf0);\n\t\treg_w_mask(sd, 0x38, 0x80, 0xf0);\n\t} else {\n\t\treg_w(sd, 0x28, 0x80);\n\t\treg_w(sd, 0x38, 0x80);\n\t}\n\n\thsegs = sd->gspca_dev.pixfmt.width / 16;\n\tvsegs = sd->gspca_dev.pixfmt.height / 4;\n\n\treg_w(sd, 0x29, hsegs);\n\treg_w(sd, 0x2a, vsegs);\n\n\treg_w(sd, 0x39, hsegs);\n\treg_w(sd, 0x3a, vsegs);\n\n\t/* Windows driver does this here; who knows why */\n\treg_w(sd, 0x2f, 0x80);\n\n\t/******** Set the framerate ********/\n\tif (sd->bridge == BRIDGE_OV518PLUS && sd->revision == 0 &&\n\t\t\t\t\t      sd->sensor == SEN_OV7620AE)\n\t\tsd->clockdiv = 0;\n\telse\n\t\tsd->clockdiv = 1;\n\n\t/* Mode independent, but framerate dependent, regs */\n\t/* 0x51: Clock divider; Only works on some cams which use 2 crystals */\n\treg_w(sd, 0x51, 0x04);\n\treg_w(sd, 0x22, 0x18);\n\treg_w(sd, 0x23, 0xff);\n\n\tif (sd->bridge == BRIDGE_OV518PLUS) {\n\t\tswitch (sd->sensor) {\n\t\tcase SEN_OV7620AE:\n\t\t\t/*\n\t\t\t * HdG: 640x480 needs special handling on device\n\t\t\t * revision 2, we check for device revision > 0 to\n\t\t\t * avoid regressions, as we don't know the correct\n\t\t\t * thing todo for revision 1.\n\t\t\t *\n\t\t\t * Also this likely means we don't need to\n\t\t\t * differentiate between the OV7620 and OV7620AE,\n\t\t\t * earlier testing hitting this same problem likely\n\t\t\t * happened to be with revision < 2 cams using an\n\t\t\t * OV7620 and revision 2 cams using an OV7620AE.\n\t\t\t */\n\t\t\tif (sd->revision > 0 &&\n\t\t\t\t\tsd->gspca_dev.pixfmt.width == 640) {\n\t\t\t\treg_w(sd, 0x20, 0x60);\n\t\t\t\treg_w(sd, 0x21, 0x1f);\n\t\t\t} else {\n\t\t\t\treg_w(sd, 0x20, 0x00);\n\t\t\t\treg_w(sd, 0x21, 0x19);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase SEN_OV7620:\n\t\t\treg_w(sd, 0x20, 0x00);\n\t\t\treg_w(sd, 0x21, 0x19);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treg_w(sd, 0x21, 0x19);\n\t\t}\n\t} else\n\t\treg_w(sd, 0x71, 0x17);\t/* Compression-related? */\n\n\t/* FIXME: Sensor-specific */\n\t/* Bit 5 is what matters here. Of course, it is \"reserved\" */\n\ti2c_w(sd, 0x54, 0x23);\n\n\treg_w(sd, 0x2f, 0x80);\n\n\tif (sd->bridge == BRIDGE_OV518PLUS) {\n\t\treg_w(sd, 0x24, 0x94);\n\t\treg_w(sd, 0x25, 0x90);\n\t\tov518_reg_w32(sd, 0xc4,    400, 2);\t/* 190h   */\n\t\tov518_reg_w32(sd, 0xc6,    540, 2);\t/* 21ch   */\n\t\tov518_reg_w32(sd, 0xc7,    540, 2);\t/* 21ch   */\n\t\tov518_reg_w32(sd, 0xc8,    108, 2);\t/* 6ch    */\n\t\tov518_reg_w32(sd, 0xca, 131098, 3);\t/* 2001ah */\n\t\tov518_reg_w32(sd, 0xcb,    532, 2);\t/* 214h   */\n\t\tov518_reg_w32(sd, 0xcc,   2400, 2);\t/* 960h   */\n\t\tov518_reg_w32(sd, 0xcd,     32, 2);\t/* 20h    */\n\t\tov518_reg_w32(sd, 0xce,    608, 2);\t/* 260h   */\n\t} else {\n\t\treg_w(sd, 0x24, 0x9f);\n\t\treg_w(sd, 0x25, 0x90);\n\t\tov518_reg_w32(sd, 0xc4,    400, 2);\t/* 190h   */\n\t\tov518_reg_w32(sd, 0xc6,    381, 2);\t/* 17dh   */\n\t\tov518_reg_w32(sd, 0xc7,    381, 2);\t/* 17dh   */\n\t\tov518_reg_w32(sd, 0xc8,    128, 2);\t/* 80h    */\n\t\tov518_reg_w32(sd, 0xca, 183331, 3);\t/* 2cc23h */\n\t\tov518_reg_w32(sd, 0xcb,    746, 2);\t/* 2eah   */\n\t\tov518_reg_w32(sd, 0xcc,   1750, 2);\t/* 6d6h   */\n\t\tov518_reg_w32(sd, 0xcd,     45, 2);\t/* 2dh    */\n\t\tov518_reg_w32(sd, 0xce,    851, 2);\t/* 353h   */\n\t}\n\n\treg_w(sd, 0x2f, 0x80);\n}",
                        "cve_id": "CVE-2020-11608"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2542,
            "cve_id": "CVE-2020-11609",
            "code_snippet": "static int stv06xx_isoc_init(struct gspca_dev *gspca_dev)\n{\n\tstruct usb_interface_cache *intfc;\n\tstruct usb_host_interface *alt;\n\tstruct sd *sd = (struct sd *) gspca_dev;\n\n\tintfc = gspca_dev->dev->actconfig->intf_cache[0];\n\n\tif (intfc->num_altsetting < 2)\n\t\treturn -ENODEV;\n\n\talt = &intfc->altsetting[1];\n\n\tif (alt->desc.bNumEndpoints < 1)\n\t\treturn -ENODEV;\n\n\t/* Start isoc bandwidth \"negotiation\" at max isoc bandwidth */\n\talt->endpoint[0].desc.wMaxPacketSize =\n\t\tcpu_to_le16(sd->sensor->max_packet_size[gspca_dev->curr_mode]);\n\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int sd_isoc_init(struct gspca_dev *gspca_dev)\n{\n\tstruct usb_host_interface *alt;\n\tint max_packet_size;\n\n\tswitch (gspca_dev->pixfmt.width) {\n\tcase 160:\n\t\tmax_packet_size = 450;\n\t\tbreak;\n\tcase 176:\n\t\tmax_packet_size = 600;\n\t\tbreak;\n\tdefault:\n\t\tmax_packet_size = 1022;\n\t\tbreak;\n\t}\n\n\t/* Start isoc bandwidth \"negotiation\" at max isoc bandwidth */\n\talt = &gspca_dev->dev->actconfig->intf_cache[0]->altsetting[1];\n\talt->endpoint[0].desc.wMaxPacketSize = cpu_to_le16(max_packet_size);\n\n\treturn 0;\n}",
                        "code_after_change": "static int sd_isoc_init(struct gspca_dev *gspca_dev)\n{\n\tstruct usb_interface_cache *intfc;\n\tstruct usb_host_interface *alt;\n\tint max_packet_size;\n\n\tswitch (gspca_dev->pixfmt.width) {\n\tcase 160:\n\t\tmax_packet_size = 450;\n\t\tbreak;\n\tcase 176:\n\t\tmax_packet_size = 600;\n\t\tbreak;\n\tdefault:\n\t\tmax_packet_size = 1022;\n\t\tbreak;\n\t}\n\n\tintfc = gspca_dev->dev->actconfig->intf_cache[0];\n\n\tif (intfc->num_altsetting < 2)\n\t\treturn -ENODEV;\n\n\talt = &intfc->altsetting[1];\n\n\tif (alt->desc.bNumEndpoints < 1)\n\t\treturn -ENODEV;\n\n\t/* Start isoc bandwidth \"negotiation\" at max isoc bandwidth */\n\talt->endpoint[0].desc.wMaxPacketSize = cpu_to_le16(max_packet_size);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2020-11668"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2546,
            "cve_id": "CVE-2020-11668",
            "code_snippet": "static int cit_get_packet_size(struct gspca_dev *gspca_dev)\n{\n\tstruct usb_host_interface *alt;\n\tstruct usb_interface *intf;\n\n\tintf = usb_ifnum_to_if(gspca_dev->dev, gspca_dev->iface);\n\talt = usb_altnum_to_altsetting(intf, gspca_dev->alt);\n\tif (!alt) {\n\t\tpr_err(\"Couldn't get altsetting\\n\");\n\t\treturn -EIO;\n\t}\n\n\tif (alt->desc.bNumEndpoints < 1)\n\t\treturn -ENODEV;\n\n\treturn le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int stv06xx_start(struct gspca_dev *gspca_dev)\n{\n\tstruct sd *sd = (struct sd *) gspca_dev;\n\tstruct usb_host_interface *alt;\n\tstruct usb_interface *intf;\n\tint err, packet_size;\n\n\tintf = usb_ifnum_to_if(sd->gspca_dev.dev, sd->gspca_dev.iface);\n\talt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);\n\tif (!alt) {\n\t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n\t\treturn -EIO;\n\t}\n\n\tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n\terr = stv06xx_write_bridge(sd, STV_ISO_SIZE_L, packet_size);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* Prepare the sensor for start */\n\terr = sd->sensor->start(sd);\n\tif (err < 0)\n\t\tgoto out;\n\n\t/* Start isochronous streaming */\n\terr = stv06xx_write_bridge(sd, STV_ISO_ENABLE, 1);\n\nout:\n\tif (err < 0)\n\t\tgspca_dbg(gspca_dev, D_STREAM, \"Starting stream failed\\n\");\n\telse\n\t\tgspca_dbg(gspca_dev, D_STREAM, \"Started streaming\\n\");\n\n\treturn (err < 0) ? err : 0;\n}",
                        "code_after_change": "static int stv06xx_start(struct gspca_dev *gspca_dev)\n{\n\tstruct sd *sd = (struct sd *) gspca_dev;\n\tstruct usb_host_interface *alt;\n\tstruct usb_interface *intf;\n\tint err, packet_size;\n\n\tintf = usb_ifnum_to_if(sd->gspca_dev.dev, sd->gspca_dev.iface);\n\talt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);\n\tif (!alt) {\n\t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n\t\treturn -EIO;\n\t}\n\n\tif (alt->desc.bNumEndpoints < 1)\n\t\treturn -ENODEV;\n\n\tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n\terr = stv06xx_write_bridge(sd, STV_ISO_SIZE_L, packet_size);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* Prepare the sensor for start */\n\terr = sd->sensor->start(sd);\n\tif (err < 0)\n\t\tgoto out;\n\n\t/* Start isochronous streaming */\n\terr = stv06xx_write_bridge(sd, STV_ISO_ENABLE, 1);\n\nout:\n\tif (err < 0)\n\t\tgspca_dbg(gspca_dev, D_STREAM, \"Starting stream failed\\n\");\n\telse\n\t\tgspca_dbg(gspca_dev, D_STREAM, \"Started streaming\\n\");\n\n\treturn (err < 0) ? err : 0;\n}",
                        "cve_id": "CVE-2020-11609"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2577,
            "cve_id": "CVE-2020-12364",
            "code_snippet": "static void guc_init_params(struct intel_guc *guc)\n{\n\tu32 *params = guc->params;\n\tint i;\n\n\tBUILD_BUG_ON(sizeof(guc->params) != GUC_CTL_MAX_DWORDS * sizeof(u32));\n\n\tparams[GUC_CTL_LOG_PARAMS] = guc_ctl_log_params_flags(guc);\n\tparams[GUC_CTL_FEATURE] = guc_ctl_feature_flags(guc);\n\tparams[GUC_CTL_DEBUG] = guc_ctl_debug_flags(guc);\n\tparams[GUC_CTL_ADS] = guc_ctl_ads_flags(guc);\n\n\tfor (i = 0; i < GUC_CTL_MAX_DWORDS; i++)\n\t\tDRM_DEBUG_DRIVER(\"param[%2d] = %#x\\n\", i, params[i]);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int smsusb_init_device(struct usb_interface *intf, int board_id)\n{\n\tstruct smsdevice_params_t params;\n\tstruct smsusb_device_t *dev;\n\tvoid *mdev;\n\tint i, rc;\n\n\t/* create device object */\n\tdev = kzalloc(sizeof(struct smsusb_device_t), GFP_KERNEL);\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\tmemset(&params, 0, sizeof(params));\n\tusb_set_intfdata(intf, dev);\n\tdev->udev = interface_to_usbdev(intf);\n\tdev->state = SMSUSB_DISCONNECTED;\n\n\tparams.device_type = sms_get_board(board_id)->type;\n\n\tswitch (params.device_type) {\n\tcase SMS_STELLAR:\n\t\tdev->buffer_size = USB1_BUFFER_SIZE;\n\n\t\tparams.setmode_handler = smsusb1_setmode;\n\t\tparams.detectmode_handler = smsusb1_detectmode;\n\t\tbreak;\n\tcase SMS_UNKNOWN_TYPE:\n\t\tpr_err(\"Unspecified sms device type!\\n\");\n\t\t/* fall-thru */\n\tdefault:\n\t\tdev->buffer_size = USB2_BUFFER_SIZE;\n\t\tdev->response_alignment =\n\t\t    le16_to_cpu(dev->udev->ep_in[1]->desc.wMaxPacketSize) -\n\t\t    sizeof(struct sms_msg_hdr);\n\n\t\tparams.flags |= SMS_DEVICE_FAMILY2;\n\t\tbreak;\n\t}\n\n\tfor (i = 0; i < intf->cur_altsetting->desc.bNumEndpoints; i++) {\n\t\tif (intf->cur_altsetting->endpoint[i].desc. bEndpointAddress & USB_DIR_IN)\n\t\t\tdev->in_ep = intf->cur_altsetting->endpoint[i].desc.bEndpointAddress;\n\t\telse\n\t\t\tdev->out_ep = intf->cur_altsetting->endpoint[i].desc.bEndpointAddress;\n\t}\n\n\tpr_debug(\"in_ep = %02x, out_ep = %02x\\n\",\n\t\tdev->in_ep, dev->out_ep);\n\n\tparams.device = &dev->udev->dev;\n\tparams.usb_device = dev->udev;\n\tparams.buffer_size = dev->buffer_size;\n\tparams.num_buffers = MAX_BUFFERS;\n\tparams.sendrequest_handler = smsusb_sendrequest;\n\tparams.context = dev;\n\tusb_make_path(dev->udev, params.devpath, sizeof(params.devpath));\n\n\tmdev = siano_media_device_register(dev, board_id);\n\n\t/* register in smscore */\n\trc = smscore_register_device(&params, &dev->coredev, 0, mdev);\n\tif (rc < 0) {\n\t\tpr_err(\"smscore_register_device(...) failed, rc %d\\n\", rc);\n\t\tsmsusb_term_device(intf);\n#ifdef CONFIG_MEDIA_CONTROLLER_DVB\n\t\tmedia_device_unregister(mdev);\n#endif\n\t\tkfree(mdev);\n\t\treturn rc;\n\t}\n\n\tsmscore_set_board_id(dev->coredev, board_id);\n\n\tdev->coredev->is_usb_device = true;\n\n\t/* initialize urbs */\n\tfor (i = 0; i < MAX_URBS; i++) {\n\t\tdev->surbs[i].dev = dev;\n\t\tusb_init_urb(&dev->surbs[i].urb);\n\t}\n\n\tpr_debug(\"smsusb_start_streaming(...).\\n\");\n\trc = smsusb_start_streaming(dev);\n\tif (rc < 0) {\n\t\tpr_err(\"smsusb_start_streaming(...) failed\\n\");\n\t\tsmsusb_term_device(intf);\n\t\treturn rc;\n\t}\n\n\tdev->state = SMSUSB_ACTIVE;\n\n\trc = smscore_start_device(dev->coredev);\n\tif (rc < 0) {\n\t\tpr_err(\"smscore_start_device(...) failed\\n\");\n\t\tsmsusb_term_device(intf);\n\t\treturn rc;\n\t}\n\n\tpr_debug(\"device 0x%p created\\n\", dev);\n\n\treturn rc;\n}",
                        "code_after_change": "static int smsusb_init_device(struct usb_interface *intf, int board_id)\n{\n\tstruct smsdevice_params_t params;\n\tstruct smsusb_device_t *dev;\n\tvoid *mdev;\n\tint i, rc;\n\tint in_maxp;\n\n\t/* create device object */\n\tdev = kzalloc(sizeof(struct smsusb_device_t), GFP_KERNEL);\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\tmemset(&params, 0, sizeof(params));\n\tusb_set_intfdata(intf, dev);\n\tdev->udev = interface_to_usbdev(intf);\n\tdev->state = SMSUSB_DISCONNECTED;\n\n\tfor (i = 0; i < intf->cur_altsetting->desc.bNumEndpoints; i++) {\n\t\tstruct usb_endpoint_descriptor *desc =\n\t\t\t\t&intf->cur_altsetting->endpoint[i].desc;\n\n\t\tif (desc->bEndpointAddress & USB_DIR_IN) {\n\t\t\tdev->in_ep = desc->bEndpointAddress;\n\t\t\tin_maxp = usb_endpoint_maxp(desc);\n\t\t} else {\n\t\t\tdev->out_ep = desc->bEndpointAddress;\n\t\t}\n\t}\n\n\tpr_debug(\"in_ep = %02x, out_ep = %02x\\n\", dev->in_ep, dev->out_ep);\n\tif (!dev->in_ep || !dev->out_ep) {\t/* Missing endpoints? */\n\t\tsmsusb_term_device(intf);\n\t\treturn -ENODEV;\n\t}\n\n\tparams.device_type = sms_get_board(board_id)->type;\n\n\tswitch (params.device_type) {\n\tcase SMS_STELLAR:\n\t\tdev->buffer_size = USB1_BUFFER_SIZE;\n\n\t\tparams.setmode_handler = smsusb1_setmode;\n\t\tparams.detectmode_handler = smsusb1_detectmode;\n\t\tbreak;\n\tcase SMS_UNKNOWN_TYPE:\n\t\tpr_err(\"Unspecified sms device type!\\n\");\n\t\t/* fall-thru */\n\tdefault:\n\t\tdev->buffer_size = USB2_BUFFER_SIZE;\n\t\tdev->response_alignment = in_maxp - sizeof(struct sms_msg_hdr);\n\n\t\tparams.flags |= SMS_DEVICE_FAMILY2;\n\t\tbreak;\n\t}\n\n\tparams.device = &dev->udev->dev;\n\tparams.usb_device = dev->udev;\n\tparams.buffer_size = dev->buffer_size;\n\tparams.num_buffers = MAX_BUFFERS;\n\tparams.sendrequest_handler = smsusb_sendrequest;\n\tparams.context = dev;\n\tusb_make_path(dev->udev, params.devpath, sizeof(params.devpath));\n\n\tmdev = siano_media_device_register(dev, board_id);\n\n\t/* register in smscore */\n\trc = smscore_register_device(&params, &dev->coredev, 0, mdev);\n\tif (rc < 0) {\n\t\tpr_err(\"smscore_register_device(...) failed, rc %d\\n\", rc);\n\t\tsmsusb_term_device(intf);\n#ifdef CONFIG_MEDIA_CONTROLLER_DVB\n\t\tmedia_device_unregister(mdev);\n#endif\n\t\tkfree(mdev);\n\t\treturn rc;\n\t}\n\n\tsmscore_set_board_id(dev->coredev, board_id);\n\n\tdev->coredev->is_usb_device = true;\n\n\t/* initialize urbs */\n\tfor (i = 0; i < MAX_URBS; i++) {\n\t\tdev->surbs[i].dev = dev;\n\t\tusb_init_urb(&dev->surbs[i].urb);\n\t}\n\n\tpr_debug(\"smsusb_start_streaming(...).\\n\");\n\trc = smsusb_start_streaming(dev);\n\tif (rc < 0) {\n\t\tpr_err(\"smsusb_start_streaming(...) failed\\n\");\n\t\tsmsusb_term_device(intf);\n\t\treturn rc;\n\t}\n\n\tdev->state = SMSUSB_ACTIVE;\n\n\trc = smscore_start_device(dev->coredev);\n\tif (rc < 0) {\n\t\tpr_err(\"smscore_start_device(...) failed\\n\");\n\t\tsmsusb_term_device(intf);\n\t\treturn rc;\n\t}\n\n\tpr_debug(\"device 0x%p created\\n\", dev);\n\n\treturn rc;\n}",
                        "cve_id": "CVE-2019-15218"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int intel_guc_ads_create(struct intel_guc *guc)\n{\n\tconst u32 size = PAGE_ALIGN(sizeof(struct __guc_ads_blob));\n\tint ret;\n\n\tGEM_BUG_ON(guc->ads_vma);\n\n\tret = intel_guc_allocate_and_map_vma(guc, size, &guc->ads_vma,\n\t\t\t\t\t     (void **)&guc->ads_blob);\n\n\tif (ret)\n\t\treturn ret;\n\n\t__guc_ads_init(guc);\n\n\treturn 0;\n}",
                        "code_after_change": "int intel_guc_ads_create(struct intel_guc *guc)\n{\n\tu32 size;\n\tint ret;\n\n\tGEM_BUG_ON(guc->ads_vma);\n\n\tsize = guc_ads_blob_size(guc);\n\n\tret = intel_guc_allocate_and_map_vma(guc, size, &guc->ads_vma,\n\t\t\t\t\t     (void **)&guc->ads_blob);\n\tif (ret)\n\t\treturn ret;\n\n\t__guc_ads_init(guc);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2020-12364"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "void intel_guc_ads_reset(struct intel_guc *guc)\n{\n\tif (!guc->ads_vma)\n\t\treturn;\n\t__guc_ads_init(guc);\n}",
                        "code_after_change": "void intel_guc_ads_reset(struct intel_guc *guc)\n{\n\tif (!guc->ads_vma)\n\t\treturn;\n\n\t__guc_ads_init(guc);\n\n\tguc_ads_private_data_reset(guc);\n}",
                        "cve_id": "CVE-2020-12364"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int amd_sfh_hid_client_init(struct amd_mp2_dev *privdata)\n{\n\tstruct amd_input_data *in_data = &privdata->in_data;\n\tstruct amdtp_cl_data *cl_data = privdata->cl_data;\n\tstruct amd_mp2_ops *mp2_ops = privdata->mp2_ops;\n\tstruct amd_mp2_sensor_info info;\n\tstruct request_list *req_list;\n\tstruct device *dev;\n\tu32 feature_report_size;\n\tu32 input_report_size;\n\tint rc, i, status;\n\tu8 cl_idx;\n\n\treq_list = &cl_data->req_list;\n\tdev = &privdata->pdev->dev;\n\tamd_sfh_set_desc_ops(mp2_ops);\n\n\tmp2_ops->suspend = amd_sfh_suspend;\n\tmp2_ops->resume = amd_sfh_resume;\n\n\tcl_data->num_hid_devices = amd_mp2_get_sensor_num(privdata, &cl_data->sensor_idx[0]);\n\tif (cl_data->num_hid_devices == 0)\n\t\treturn -ENODEV;\n\n\tINIT_DELAYED_WORK(&cl_data->work, amd_sfh_work);\n\tINIT_DELAYED_WORK(&cl_data->work_buffer, amd_sfh_work_buffer);\n\tINIT_LIST_HEAD(&req_list->list);\n\tcl_data->in_data = in_data;\n\n\tfor (i = 0; i < cl_data->num_hid_devices; i++) {\n\t\tin_data->sensor_virt_addr[i] = dma_alloc_coherent(dev, sizeof(int) * 8,\n\t\t\t\t\t\t\t\t  &cl_data->sensor_dma_addr[i],\n\t\t\t\t\t\t\t\t  GFP_KERNEL);\n\t\tcl_data->sensor_sts[i] = SENSOR_DISABLED;\n\t\tcl_data->sensor_requested_cnt[i] = 0;\n\t\tcl_data->cur_hid_dev = i;\n\t\tcl_idx = cl_data->sensor_idx[i];\n\t\tcl_data->report_descr_sz[i] = mp2_ops->get_desc_sz(cl_idx, descr_size);\n\t\tif (!cl_data->report_descr_sz[i]) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tfeature_report_size = mp2_ops->get_desc_sz(cl_idx, feature_size);\n\t\tif (!feature_report_size) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tinput_report_size =  mp2_ops->get_desc_sz(cl_idx, input_size);\n\t\tif (!input_report_size) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tcl_data->feature_report[i] = devm_kzalloc(dev, feature_report_size, GFP_KERNEL);\n\t\tif (!cl_data->feature_report[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tin_data->input_report[i] = devm_kzalloc(dev, input_report_size, GFP_KERNEL);\n\t\tif (!in_data->input_report[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tinfo.period = AMD_SFH_IDLE_LOOP;\n\t\tinfo.sensor_idx = cl_idx;\n\t\tinfo.dma_address = cl_data->sensor_dma_addr[i];\n\n\t\tcl_data->report_descr[i] =\n\t\t\tdevm_kzalloc(dev, cl_data->report_descr_sz[i], GFP_KERNEL);\n\t\tif (!cl_data->report_descr[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\trc = mp2_ops->get_rep_desc(cl_idx, cl_data->report_descr[i]);\n\t\tif (rc)\n\t\t\treturn rc;\n\t\tmp2_ops->start(privdata, info);\n\t\tstatus = amd_sfh_wait_for_response\n\t\t\t\t(privdata, cl_data->sensor_idx[i], SENSOR_ENABLED);\n\t\tif (status == SENSOR_ENABLED) {\n\t\t\tcl_data->sensor_sts[i] = SENSOR_ENABLED;\n\t\t\trc = amdtp_hid_probe(cl_data->cur_hid_dev, cl_data);\n\t\t\tif (rc) {\n\t\t\t\tmp2_ops->stop(privdata, cl_data->sensor_idx[i]);\n\t\t\t\tstatus = amd_sfh_wait_for_response\n\t\t\t\t\t(privdata, cl_data->sensor_idx[i], SENSOR_DISABLED);\n\t\t\t\tif (status != SENSOR_ENABLED)\n\t\t\t\t\tcl_data->sensor_sts[i] = SENSOR_DISABLED;\n\t\t\t\tdev_dbg(dev, \"sid 0x%x (%s) status 0x%x\\n\",\n\t\t\t\t\tcl_data->sensor_idx[i],\n\t\t\t\t\tget_sensor_name(cl_data->sensor_idx[i]),\n\t\t\t\t\tcl_data->sensor_sts[i]);\n\t\t\t\tgoto cleanup;\n\t\t\t}\n\t\t}\n\t\tdev_dbg(dev, \"sid 0x%x (%s) status 0x%x\\n\",\n\t\t\tcl_data->sensor_idx[i], get_sensor_name(cl_data->sensor_idx[i]),\n\t\t\tcl_data->sensor_sts[i]);\n\t}\n\tif (mp2_ops->discovery_status && mp2_ops->discovery_status(privdata) == 0) {\n\t\tamd_sfh_hid_client_deinit(privdata);\n\t\tfor (i = 0; i < cl_data->num_hid_devices; i++) {\n\t\t\tdevm_kfree(dev, cl_data->feature_report[i]);\n\t\t\tdevm_kfree(dev, in_data->input_report[i]);\n\t\t\tdevm_kfree(dev, cl_data->report_descr[i]);\n\t\t}\n\t\tdev_warn(dev, \"Failed to discover, sensors not enabled\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\tschedule_delayed_work(&cl_data->work_buffer, msecs_to_jiffies(AMD_SFH_IDLE_LOOP));\n\treturn 0;\n\ncleanup:\n\tfor (i = 0; i < cl_data->num_hid_devices; i++) {\n\t\tif (in_data->sensor_virt_addr[i]) {\n\t\t\tdma_free_coherent(&privdata->pdev->dev, 8 * sizeof(int),\n\t\t\t\t\t  in_data->sensor_virt_addr[i],\n\t\t\t\t\t  cl_data->sensor_dma_addr[i]);\n\t\t}\n\t\tdevm_kfree(dev, cl_data->feature_report[i]);\n\t\tdevm_kfree(dev, in_data->input_report[i]);\n\t\tdevm_kfree(dev, cl_data->report_descr[i]);\n\t}\n\treturn rc;\n}",
                        "code_after_change": "int amd_sfh_hid_client_init(struct amd_mp2_dev *privdata)\n{\n\tstruct amd_input_data *in_data = &privdata->in_data;\n\tstruct amdtp_cl_data *cl_data = privdata->cl_data;\n\tstruct amd_mp2_ops *mp2_ops = privdata->mp2_ops;\n\tstruct amd_mp2_sensor_info info;\n\tstruct request_list *req_list;\n\tstruct device *dev;\n\tu32 feature_report_size;\n\tu32 input_report_size;\n\tint rc, i, status;\n\tu8 cl_idx;\n\n\treq_list = &cl_data->req_list;\n\tdev = &privdata->pdev->dev;\n\tamd_sfh_set_desc_ops(mp2_ops);\n\n\tmp2_ops->suspend = amd_sfh_suspend;\n\tmp2_ops->resume = amd_sfh_resume;\n\n\tcl_data->num_hid_devices = amd_mp2_get_sensor_num(privdata, &cl_data->sensor_idx[0]);\n\tif (cl_data->num_hid_devices == 0)\n\t\treturn -ENODEV;\n\n\tINIT_DELAYED_WORK(&cl_data->work, amd_sfh_work);\n\tINIT_DELAYED_WORK(&cl_data->work_buffer, amd_sfh_work_buffer);\n\tINIT_LIST_HEAD(&req_list->list);\n\tcl_data->in_data = in_data;\n\n\tfor (i = 0; i < cl_data->num_hid_devices; i++) {\n\t\tin_data->sensor_virt_addr[i] = dma_alloc_coherent(dev, sizeof(int) * 8,\n\t\t\t\t\t\t\t\t  &cl_data->sensor_dma_addr[i],\n\t\t\t\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!in_data->sensor_virt_addr[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tcl_data->sensor_sts[i] = SENSOR_DISABLED;\n\t\tcl_data->sensor_requested_cnt[i] = 0;\n\t\tcl_data->cur_hid_dev = i;\n\t\tcl_idx = cl_data->sensor_idx[i];\n\t\tcl_data->report_descr_sz[i] = mp2_ops->get_desc_sz(cl_idx, descr_size);\n\t\tif (!cl_data->report_descr_sz[i]) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tfeature_report_size = mp2_ops->get_desc_sz(cl_idx, feature_size);\n\t\tif (!feature_report_size) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tinput_report_size =  mp2_ops->get_desc_sz(cl_idx, input_size);\n\t\tif (!input_report_size) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tcl_data->feature_report[i] = devm_kzalloc(dev, feature_report_size, GFP_KERNEL);\n\t\tif (!cl_data->feature_report[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tin_data->input_report[i] = devm_kzalloc(dev, input_report_size, GFP_KERNEL);\n\t\tif (!in_data->input_report[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tinfo.period = AMD_SFH_IDLE_LOOP;\n\t\tinfo.sensor_idx = cl_idx;\n\t\tinfo.dma_address = cl_data->sensor_dma_addr[i];\n\n\t\tcl_data->report_descr[i] =\n\t\t\tdevm_kzalloc(dev, cl_data->report_descr_sz[i], GFP_KERNEL);\n\t\tif (!cl_data->report_descr[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto cleanup;\n\t\t}\n\t\trc = mp2_ops->get_rep_desc(cl_idx, cl_data->report_descr[i]);\n\t\tif (rc)\n\t\t\treturn rc;\n\t\tmp2_ops->start(privdata, info);\n\t\tstatus = amd_sfh_wait_for_response\n\t\t\t\t(privdata, cl_data->sensor_idx[i], SENSOR_ENABLED);\n\t\tif (status == SENSOR_ENABLED) {\n\t\t\tcl_data->sensor_sts[i] = SENSOR_ENABLED;\n\t\t\trc = amdtp_hid_probe(cl_data->cur_hid_dev, cl_data);\n\t\t\tif (rc) {\n\t\t\t\tmp2_ops->stop(privdata, cl_data->sensor_idx[i]);\n\t\t\t\tstatus = amd_sfh_wait_for_response\n\t\t\t\t\t(privdata, cl_data->sensor_idx[i], SENSOR_DISABLED);\n\t\t\t\tif (status != SENSOR_ENABLED)\n\t\t\t\t\tcl_data->sensor_sts[i] = SENSOR_DISABLED;\n\t\t\t\tdev_dbg(dev, \"sid 0x%x (%s) status 0x%x\\n\",\n\t\t\t\t\tcl_data->sensor_idx[i],\n\t\t\t\t\tget_sensor_name(cl_data->sensor_idx[i]),\n\t\t\t\t\tcl_data->sensor_sts[i]);\n\t\t\t\tgoto cleanup;\n\t\t\t}\n\t\t}\n\t\tdev_dbg(dev, \"sid 0x%x (%s) status 0x%x\\n\",\n\t\t\tcl_data->sensor_idx[i], get_sensor_name(cl_data->sensor_idx[i]),\n\t\t\tcl_data->sensor_sts[i]);\n\t}\n\tif (mp2_ops->discovery_status && mp2_ops->discovery_status(privdata) == 0) {\n\t\tamd_sfh_hid_client_deinit(privdata);\n\t\tfor (i = 0; i < cl_data->num_hid_devices; i++) {\n\t\t\tdevm_kfree(dev, cl_data->feature_report[i]);\n\t\t\tdevm_kfree(dev, in_data->input_report[i]);\n\t\t\tdevm_kfree(dev, cl_data->report_descr[i]);\n\t\t}\n\t\tdev_warn(dev, \"Failed to discover, sensors not enabled\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\tschedule_delayed_work(&cl_data->work_buffer, msecs_to_jiffies(AMD_SFH_IDLE_LOOP));\n\treturn 0;\n\ncleanup:\n\tfor (i = 0; i < cl_data->num_hid_devices; i++) {\n\t\tif (in_data->sensor_virt_addr[i]) {\n\t\t\tdma_free_coherent(&privdata->pdev->dev, 8 * sizeof(int),\n\t\t\t\t\t  in_data->sensor_virt_addr[i],\n\t\t\t\t\t  cl_data->sensor_dma_addr[i]);\n\t\t}\n\t\tdevm_kfree(dev, cl_data->feature_report[i]);\n\t\tdevm_kfree(dev, in_data->input_report[i]);\n\t\tdevm_kfree(dev, cl_data->report_descr[i]);\n\t}\n\treturn rc;\n}",
                        "cve_id": "CVE-2023-3357"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int if_sdio_probe(struct sdio_func *func,\n\t\tconst struct sdio_device_id *id)\n{\n\tstruct if_sdio_card *card;\n\tstruct lbs_private *priv;\n\tint ret, i;\n\tunsigned int model;\n\tstruct if_sdio_packet *packet;\n\n\tfor (i = 0;i < func->card->num_info;i++) {\n\t\tif (sscanf(func->card->info[i],\n\t\t\t\t\"802.11 SDIO ID: %x\", &model) == 1)\n\t\t\tbreak;\n\t\tif (sscanf(func->card->info[i],\n\t\t\t\t\"ID: %x\", &model) == 1)\n\t\t\tbreak;\n\t\tif (!strcmp(func->card->info[i], \"IBIS Wireless SDIO Card\")) {\n\t\t\tmodel = MODEL_8385;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (i == func->card->num_info) {\n\t\tpr_err(\"unable to identify card model\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tcard = kzalloc(sizeof(struct if_sdio_card), GFP_KERNEL);\n\tif (!card)\n\t\treturn -ENOMEM;\n\n\tcard->func = func;\n\tcard->model = model;\n\n\tswitch (card->model) {\n\tcase MODEL_8385:\n\t\tcard->scratch_reg = IF_SDIO_SCRATCH_OLD;\n\t\tbreak;\n\tcase MODEL_8686:\n\t\tcard->scratch_reg = IF_SDIO_SCRATCH;\n\t\tbreak;\n\tcase MODEL_8688:\n\tdefault: /* for newer chipsets */\n\t\tcard->scratch_reg = IF_SDIO_FW_STATUS;\n\t\tbreak;\n\t}\n\n\tspin_lock_init(&card->lock);\n\tcard->workqueue = alloc_workqueue(\"libertas_sdio\", WQ_MEM_RECLAIM, 0);\n\tINIT_WORK(&card->packet_worker, if_sdio_host_to_card_worker);\n\tinit_waitqueue_head(&card->pwron_waitq);\n\n\t/* Check if we support this card */\n\tfor (i = 0; i < ARRAY_SIZE(fw_table); i++) {\n\t\tif (card->model == fw_table[i].model)\n\t\t\tbreak;\n\t}\n\tif (i == ARRAY_SIZE(fw_table)) {\n\t\tpr_err(\"unknown card model 0x%x\\n\", card->model);\n\t\tret = -ENODEV;\n\t\tgoto free;\n\t}\n\n\tsdio_set_drvdata(func, card);\n\n\tlbs_deb_sdio(\"class = 0x%X, vendor = 0x%X, \"\n\t\t\t\"device = 0x%X, model = 0x%X, ioport = 0x%X\\n\",\n\t\t\tfunc->class, func->vendor, func->device,\n\t\t\tmodel, (unsigned)card->ioport);\n\n\n\tpriv = lbs_add_card(card, &func->dev);\n\tif (IS_ERR(priv)) {\n\t\tret = PTR_ERR(priv);\n\t\tgoto free;\n\t}\n\n\tcard->priv = priv;\n\n\tpriv->card = card;\n\tpriv->hw_host_to_card = if_sdio_host_to_card;\n\tpriv->enter_deep_sleep = if_sdio_enter_deep_sleep;\n\tpriv->exit_deep_sleep = if_sdio_exit_deep_sleep;\n\tpriv->reset_deep_sleep_wakeup = if_sdio_reset_deep_sleep_wakeup;\n\tpriv->reset_card = if_sdio_reset_card;\n\tpriv->power_save = if_sdio_power_save;\n\tpriv->power_restore = if_sdio_power_restore;\n\tpriv->is_polling = !(func->card->host->caps & MMC_CAP_SDIO_IRQ);\n\tret = if_sdio_power_on(card);\n\tif (ret)\n\t\tgoto err_activate_card;\n\nout:\n\treturn ret;\n\nerr_activate_card:\n\tflush_workqueue(card->workqueue);\n\tlbs_remove_card(priv);\nfree:\n\tdestroy_workqueue(card->workqueue);\n\twhile (card->packets) {\n\t\tpacket = card->packets;\n\t\tcard->packets = card->packets->next;\n\t\tkfree(packet);\n\t}\n\n\tkfree(card);\n\n\tgoto out;\n}",
                        "code_after_change": "static int if_sdio_probe(struct sdio_func *func,\n\t\tconst struct sdio_device_id *id)\n{\n\tstruct if_sdio_card *card;\n\tstruct lbs_private *priv;\n\tint ret, i;\n\tunsigned int model;\n\tstruct if_sdio_packet *packet;\n\n\tfor (i = 0;i < func->card->num_info;i++) {\n\t\tif (sscanf(func->card->info[i],\n\t\t\t\t\"802.11 SDIO ID: %x\", &model) == 1)\n\t\t\tbreak;\n\t\tif (sscanf(func->card->info[i],\n\t\t\t\t\"ID: %x\", &model) == 1)\n\t\t\tbreak;\n\t\tif (!strcmp(func->card->info[i], \"IBIS Wireless SDIO Card\")) {\n\t\t\tmodel = MODEL_8385;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (i == func->card->num_info) {\n\t\tpr_err(\"unable to identify card model\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tcard = kzalloc(sizeof(struct if_sdio_card), GFP_KERNEL);\n\tif (!card)\n\t\treturn -ENOMEM;\n\n\tcard->func = func;\n\tcard->model = model;\n\n\tswitch (card->model) {\n\tcase MODEL_8385:\n\t\tcard->scratch_reg = IF_SDIO_SCRATCH_OLD;\n\t\tbreak;\n\tcase MODEL_8686:\n\t\tcard->scratch_reg = IF_SDIO_SCRATCH;\n\t\tbreak;\n\tcase MODEL_8688:\n\tdefault: /* for newer chipsets */\n\t\tcard->scratch_reg = IF_SDIO_FW_STATUS;\n\t\tbreak;\n\t}\n\n\tspin_lock_init(&card->lock);\n\tcard->workqueue = alloc_workqueue(\"libertas_sdio\", WQ_MEM_RECLAIM, 0);\n\tif (unlikely(!card->workqueue)) {\n\t\tret = -ENOMEM;\n\t\tgoto err_queue;\n\t}\n\tINIT_WORK(&card->packet_worker, if_sdio_host_to_card_worker);\n\tinit_waitqueue_head(&card->pwron_waitq);\n\n\t/* Check if we support this card */\n\tfor (i = 0; i < ARRAY_SIZE(fw_table); i++) {\n\t\tif (card->model == fw_table[i].model)\n\t\t\tbreak;\n\t}\n\tif (i == ARRAY_SIZE(fw_table)) {\n\t\tpr_err(\"unknown card model 0x%x\\n\", card->model);\n\t\tret = -ENODEV;\n\t\tgoto free;\n\t}\n\n\tsdio_set_drvdata(func, card);\n\n\tlbs_deb_sdio(\"class = 0x%X, vendor = 0x%X, \"\n\t\t\t\"device = 0x%X, model = 0x%X, ioport = 0x%X\\n\",\n\t\t\tfunc->class, func->vendor, func->device,\n\t\t\tmodel, (unsigned)card->ioport);\n\n\n\tpriv = lbs_add_card(card, &func->dev);\n\tif (IS_ERR(priv)) {\n\t\tret = PTR_ERR(priv);\n\t\tgoto free;\n\t}\n\n\tcard->priv = priv;\n\n\tpriv->card = card;\n\tpriv->hw_host_to_card = if_sdio_host_to_card;\n\tpriv->enter_deep_sleep = if_sdio_enter_deep_sleep;\n\tpriv->exit_deep_sleep = if_sdio_exit_deep_sleep;\n\tpriv->reset_deep_sleep_wakeup = if_sdio_reset_deep_sleep_wakeup;\n\tpriv->reset_card = if_sdio_reset_card;\n\tpriv->power_save = if_sdio_power_save;\n\tpriv->power_restore = if_sdio_power_restore;\n\tpriv->is_polling = !(func->card->host->caps & MMC_CAP_SDIO_IRQ);\n\tret = if_sdio_power_on(card);\n\tif (ret)\n\t\tgoto err_activate_card;\n\nout:\n\treturn ret;\n\nerr_activate_card:\n\tflush_workqueue(card->workqueue);\n\tlbs_remove_card(priv);\nfree:\n\tdestroy_workqueue(card->workqueue);\nerr_queue:\n\twhile (card->packets) {\n\t\tpacket = card->packets;\n\t\tcard->packets = card->packets->next;\n\t\tkfree(packet);\n\t}\n\n\tkfree(card);\n\n\tgoto out;\n}",
                        "cve_id": "CVE-2019-16232"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 2578,
            "cve_id": "CVE-2020-12364",
            "code_snippet": "static void __guc_ads_init(struct intel_guc *guc)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tstruct drm_i915_private *i915 = gt->i915;\n\tstruct __guc_ads_blob *blob = guc->ads_blob;\n\tconst u32 skipped_size = LRC_PPHWSP_SZ * PAGE_SIZE + LR_HW_CONTEXT_SIZE;\n\tu32 base;\n\tu8 engine_class;\n\n\t/* GuC scheduling policies */\n\tguc_policies_init(&blob->policies);\n\n\t/*\n\t * GuC expects a per-engine-class context image and size\n\t * (minus hwsp and ring context). The context image will be\n\t * used to reinitialize engines after a reset. It must exist\n\t * and be pinned in the GGTT, so that the address won't change after\n\t * we have told GuC where to find it. The context size will be used\n\t * to validate that the LRC base + size fall within allowed GGTT.\n\t */\n\tfor (engine_class = 0; engine_class <= MAX_ENGINE_CLASS; ++engine_class) {\n\t\tif (engine_class == OTHER_CLASS)\n\t\t\tcontinue;\n\t\t/*\n\t\t * TODO: Set context pointer to default state to allow\n\t\t * GuC to re-init guilty contexts after internal reset.\n\t\t */\n\t\tblob->ads.golden_context_lrca[engine_class] = 0;\n\t\tblob->ads.eng_state_size[engine_class] =\n\t\t\tintel_engine_context_size(guc_to_gt(guc),\n\t\t\t\t\t\t  engine_class) -\n\t\t\tskipped_size;\n\t}\n\n\t/* System info */\n\tblob->system_info.engine_enabled_masks[RENDER_CLASS] = 1;\n\tblob->system_info.engine_enabled_masks[COPY_ENGINE_CLASS] = 1;\n\tblob->system_info.engine_enabled_masks[VIDEO_DECODE_CLASS] = VDBOX_MASK(gt);\n\tblob->system_info.engine_enabled_masks[VIDEO_ENHANCEMENT_CLASS] = VEBOX_MASK(gt);\n\n\tblob->system_info.generic_gt_sysinfo[GUC_GENERIC_GT_SYSINFO_SLICE_ENABLED] =\n\t\thweight8(gt->info.sseu.slice_mask);\n\tblob->system_info.generic_gt_sysinfo[GUC_GENERIC_GT_SYSINFO_VDBOX_SFC_SUPPORT_MASK] =\n\t\tgt->info.vdbox_sfc_access;\n\n\tif (INTEL_GEN(i915) >= 12 && !IS_DGFX(i915)) {\n\t\tu32 distdbreg = intel_uncore_read(gt->uncore,\n\t\t\t\t\t\t  GEN12_DIST_DBS_POPULATED);\n\t\tblob->system_info.generic_gt_sysinfo[GUC_GENERIC_GT_SYSINFO_DOORBELL_COUNT_PER_SQIDI] =\n\t\t\t((distdbreg >> GEN12_DOORBELLS_PER_SQIDI_SHIFT) &\n\t\t\t GEN12_DOORBELLS_PER_SQIDI) + 1;\n\t}\n\n\tguc_mapping_table_init(guc_to_gt(guc), &blob->system_info);\n\n\tbase = intel_guc_ggtt_offset(guc, guc->ads_vma);\n\n\t/* Clients info  */\n\tguc_ct_pool_entries_init(blob->ct_pool, ARRAY_SIZE(blob->ct_pool));\n\n\tblob->clients_info.clients_num = 1;\n\tblob->clients_info.ct_pool_addr = base + ptr_offset(blob, ct_pool);\n\tblob->clients_info.ct_pool_count = ARRAY_SIZE(blob->ct_pool);\n\n\t/* ADS */\n\tblob->ads.scheduler_policies = base + ptr_offset(blob, policies);\n\tblob->ads.gt_system_info = base + ptr_offset(blob, system_info);\n\tblob->ads.clients_info = base + ptr_offset(blob, clients_info);\n\n\t/* Private Data */\n\tblob->ads.private_data = base + guc_ads_private_data_offset(guc);\n\n\ti915_gem_object_flush_map(guc->ads_vma->obj);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int intel_engine_setup(struct intel_gt *gt, enum intel_engine_id id)\n{\n\tconst struct engine_info *info = &intel_engines[id];\n\tstruct drm_i915_private *i915 = gt->i915;\n\tstruct intel_engine_cs *engine;\n\n\tBUILD_BUG_ON(MAX_ENGINE_CLASS >= BIT(GEN11_ENGINE_CLASS_WIDTH));\n\tBUILD_BUG_ON(MAX_ENGINE_INSTANCE >= BIT(GEN11_ENGINE_INSTANCE_WIDTH));\n\n\tif (GEM_DEBUG_WARN_ON(id >= ARRAY_SIZE(gt->engine)))\n\t\treturn -EINVAL;\n\n\tif (GEM_DEBUG_WARN_ON(info->class > MAX_ENGINE_CLASS))\n\t\treturn -EINVAL;\n\n\tif (GEM_DEBUG_WARN_ON(info->instance > MAX_ENGINE_INSTANCE))\n\t\treturn -EINVAL;\n\n\tif (GEM_DEBUG_WARN_ON(gt->engine_class[info->class][info->instance]))\n\t\treturn -EINVAL;\n\n\tengine = kzalloc(sizeof(*engine), GFP_KERNEL);\n\tif (!engine)\n\t\treturn -ENOMEM;\n\n\tBUILD_BUG_ON(BITS_PER_TYPE(engine->mask) < I915_NUM_ENGINES);\n\n\tengine->id = id;\n\tengine->legacy_idx = INVALID_ENGINE;\n\tengine->mask = BIT(id);\n\tengine->i915 = i915;\n\tengine->gt = gt;\n\tengine->uncore = gt->uncore;\n\tengine->hw_id = engine->guc_id = info->hw_id;\n\tengine->mmio_base = __engine_mmio_base(i915, info->mmio_bases);\n\n\tengine->class = info->class;\n\tengine->instance = info->instance;\n\t__sprint_engine_name(engine);\n\n\tengine->props.heartbeat_interval_ms =\n\t\tCONFIG_DRM_I915_HEARTBEAT_INTERVAL;\n\tengine->props.max_busywait_duration_ns =\n\t\tCONFIG_DRM_I915_MAX_REQUEST_BUSYWAIT;\n\tengine->props.preempt_timeout_ms =\n\t\tCONFIG_DRM_I915_PREEMPT_TIMEOUT;\n\tengine->props.stop_timeout_ms =\n\t\tCONFIG_DRM_I915_STOP_TIMEOUT;\n\tengine->props.timeslice_duration_ms =\n\t\tCONFIG_DRM_I915_TIMESLICE_DURATION;\n\n\t/* Override to uninterruptible for OpenCL workloads. */\n\tif (INTEL_GEN(i915) == 12 && engine->class == RENDER_CLASS)\n\t\tengine->props.preempt_timeout_ms = 0;\n\n\tengine->defaults = engine->props; /* never to change again */\n\n\tengine->context_size = intel_engine_context_size(gt, engine->class);\n\tif (WARN_ON(engine->context_size > BIT(20)))\n\t\tengine->context_size = 0;\n\tif (engine->context_size)\n\t\tDRIVER_CAPS(i915)->has_logical_contexts = true;\n\n\t/* Nothing to do here, execute in order of dependencies */\n\tengine->schedule = NULL;\n\n\tewma__engine_latency_init(&engine->latency);\n\tseqlock_init(&engine->stats.lock);\n\n\tATOMIC_INIT_NOTIFIER_HEAD(&engine->context_status_notifier);\n\n\t/* Scrub mmio state on takeover */\n\tintel_engine_sanitize_mmio(engine);\n\n\tgt->engine_class[info->class][info->instance] = engine;\n\tgt->engine[id] = engine;\n\n\treturn 0;\n}",
                        "code_after_change": "static int intel_engine_setup(struct intel_gt *gt, enum intel_engine_id id)\n{\n\tconst struct engine_info *info = &intel_engines[id];\n\tstruct drm_i915_private *i915 = gt->i915;\n\tstruct intel_engine_cs *engine;\n\n\tBUILD_BUG_ON(MAX_ENGINE_CLASS >= BIT(GEN11_ENGINE_CLASS_WIDTH));\n\tBUILD_BUG_ON(MAX_ENGINE_INSTANCE >= BIT(GEN11_ENGINE_INSTANCE_WIDTH));\n\n\tif (GEM_DEBUG_WARN_ON(id >= ARRAY_SIZE(gt->engine)))\n\t\treturn -EINVAL;\n\n\tif (GEM_DEBUG_WARN_ON(info->class > MAX_ENGINE_CLASS))\n\t\treturn -EINVAL;\n\n\tif (GEM_DEBUG_WARN_ON(info->instance > MAX_ENGINE_INSTANCE))\n\t\treturn -EINVAL;\n\n\tif (GEM_DEBUG_WARN_ON(gt->engine_class[info->class][info->instance]))\n\t\treturn -EINVAL;\n\n\tengine = kzalloc(sizeof(*engine), GFP_KERNEL);\n\tif (!engine)\n\t\treturn -ENOMEM;\n\n\tBUILD_BUG_ON(BITS_PER_TYPE(engine->mask) < I915_NUM_ENGINES);\n\n\tengine->id = id;\n\tengine->legacy_idx = INVALID_ENGINE;\n\tengine->mask = BIT(id);\n\tengine->i915 = i915;\n\tengine->gt = gt;\n\tengine->uncore = gt->uncore;\n\tengine->mmio_base = __engine_mmio_base(i915, info->mmio_bases);\n\tengine->hw_id = info->hw_id;\n\tengine->guc_id = MAKE_GUC_ID(info->class, info->instance);\n\n\tengine->class = info->class;\n\tengine->instance = info->instance;\n\t__sprint_engine_name(engine);\n\n\tengine->props.heartbeat_interval_ms =\n\t\tCONFIG_DRM_I915_HEARTBEAT_INTERVAL;\n\tengine->props.max_busywait_duration_ns =\n\t\tCONFIG_DRM_I915_MAX_REQUEST_BUSYWAIT;\n\tengine->props.preempt_timeout_ms =\n\t\tCONFIG_DRM_I915_PREEMPT_TIMEOUT;\n\tengine->props.stop_timeout_ms =\n\t\tCONFIG_DRM_I915_STOP_TIMEOUT;\n\tengine->props.timeslice_duration_ms =\n\t\tCONFIG_DRM_I915_TIMESLICE_DURATION;\n\n\t/* Override to uninterruptible for OpenCL workloads. */\n\tif (INTEL_GEN(i915) == 12 && engine->class == RENDER_CLASS)\n\t\tengine->props.preempt_timeout_ms = 0;\n\n\tengine->defaults = engine->props; /* never to change again */\n\n\tengine->context_size = intel_engine_context_size(gt, engine->class);\n\tif (WARN_ON(engine->context_size > BIT(20)))\n\t\tengine->context_size = 0;\n\tif (engine->context_size)\n\t\tDRIVER_CAPS(i915)->has_logical_contexts = true;\n\n\t/* Nothing to do here, execute in order of dependencies */\n\tengine->schedule = NULL;\n\n\tewma__engine_latency_init(&engine->latency);\n\tseqlock_init(&engine->stats.lock);\n\n\tATOMIC_INIT_NOTIFIER_HEAD(&engine->context_status_notifier);\n\n\t/* Scrub mmio state on takeover */\n\tintel_engine_sanitize_mmio(engine);\n\n\tgt->engine_class[info->class][info->instance] = engine;\n\tgt->engine[id] = engine;\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2020-12364"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int intel_guc_ads_create(struct intel_guc *guc)\n{\n\tconst u32 size = PAGE_ALIGN(sizeof(struct __guc_ads_blob));\n\tint ret;\n\n\tGEM_BUG_ON(guc->ads_vma);\n\n\tret = intel_guc_allocate_and_map_vma(guc, size, &guc->ads_vma,\n\t\t\t\t\t     (void **)&guc->ads_blob);\n\n\tif (ret)\n\t\treturn ret;\n\n\t__guc_ads_init(guc);\n\n\treturn 0;\n}",
                        "code_after_change": "int intel_guc_ads_create(struct intel_guc *guc)\n{\n\tu32 size;\n\tint ret;\n\n\tGEM_BUG_ON(guc->ads_vma);\n\n\tsize = guc_ads_blob_size(guc);\n\n\tret = intel_guc_allocate_and_map_vma(guc, size, &guc->ads_vma,\n\t\t\t\t\t     (void **)&guc->ads_blob);\n\tif (ret)\n\t\treturn ret;\n\n\t__guc_ads_init(guc);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2020-12364"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2628,
            "cve_id": "CVE-2020-14356",
            "code_snippet": "void cgroup_sk_free(struct sock_cgroup_data *skcd)\n{\n\tstruct cgroup *cgrp = sock_cgroup_ptr(skcd);\n\n\tif (skcd->no_refcnt)\n\t\treturn;\n\tcgroup_bpf_put(cgrp);\n\tcgroup_put(cgrp);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "void cgroup_sk_alloc(struct sock_cgroup_data *skcd)\n{\n\tif (cgroup_sk_alloc_disabled)\n\t\treturn;\n\n\t/* Socket clone path */\n\tif (skcd->val) {\n\t\t/*\n\t\t * We might be cloning a socket which is left in an empty\n\t\t * cgroup and the cgroup might have already been rmdir'd.\n\t\t * Don't use cgroup_get_live().\n\t\t */\n\t\tcgroup_get(sock_cgroup_ptr(skcd));\n\t\tcgroup_bpf_get(sock_cgroup_ptr(skcd));\n\t\treturn;\n\t}\n\n\t/* Don't associate the sock with unrelated interrupted task's cgroup. */\n\tif (in_interrupt())\n\t\treturn;\n\n\trcu_read_lock();\n\n\twhile (true) {\n\t\tstruct css_set *cset;\n\n\t\tcset = task_css_set(current);\n\t\tif (likely(cgroup_tryget(cset->dfl_cgrp))) {\n\t\t\tskcd->val = (unsigned long)cset->dfl_cgrp;\n\t\t\tcgroup_bpf_get(cset->dfl_cgrp);\n\t\t\tbreak;\n\t\t}\n\t\tcpu_relax();\n\t}\n\n\trcu_read_unlock();\n}",
                        "code_after_change": "static inline struct cgroup *sock_cgroup_ptr(struct sock_cgroup_data *skcd)\n{\n#if defined(CONFIG_CGROUP_NET_PRIO) || defined(CONFIG_CGROUP_NET_CLASSID)\n\tunsigned long v;\n\n\t/*\n\t * @skcd->val is 64bit but the following is safe on 32bit too as we\n\t * just need the lower ulong to be written and read atomically.\n\t */\n\tv = READ_ONCE(skcd->val);\n\n\tif (v & 3)\n\t\treturn &cgrp_dfl_root.cgrp;\n\n\treturn (struct cgroup *)(unsigned long)v ?: &cgrp_dfl_root.cgrp;\n#else\n\treturn (struct cgroup *)(unsigned long)skcd->val;\n#endif\n}",
                        "cve_id": "CVE-2020-14356"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static inline struct cgroup *sock_cgroup_ptr(struct sock_cgroup_data *skcd)\n{\n#if defined(CONFIG_CGROUP_NET_PRIO) || defined(CONFIG_CGROUP_NET_CLASSID)\n\tunsigned long v;\n\n\t/*\n\t * @skcd->val is 64bit but the following is safe on 32bit too as we\n\t * just need the lower ulong to be written and read atomically.\n\t */\n\tv = READ_ONCE(skcd->val);\n\n\tif (v & 1)\n\t\treturn &cgrp_dfl_root.cgrp;\n\n\treturn (struct cgroup *)(unsigned long)v ?: &cgrp_dfl_root.cgrp;\n#else\n\treturn (struct cgroup *)(unsigned long)skcd->val;\n#endif\n}",
                        "code_after_change": "void cgroup_sk_alloc(struct sock_cgroup_data *skcd)\n{\n\tif (cgroup_sk_alloc_disabled) {\n\t\tskcd->no_refcnt = 1;\n\t\treturn;\n\t}\n\n\t/* Don't associate the sock with unrelated interrupted task's cgroup. */\n\tif (in_interrupt())\n\t\treturn;\n\n\trcu_read_lock();\n\n\twhile (true) {\n\t\tstruct css_set *cset;\n\n\t\tcset = task_css_set(current);\n\t\tif (likely(cgroup_tryget(cset->dfl_cgrp))) {\n\t\t\tskcd->val = (unsigned long)cset->dfl_cgrp;\n\t\t\tcgroup_bpf_get(cset->dfl_cgrp);\n\t\t\tbreak;\n\t\t}\n\t\tcpu_relax();\n\t}\n\n\trcu_read_unlock();\n}",
                        "cve_id": "CVE-2020-14356"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int\next4_xattr_check_entries(struct ext4_xattr_entry *entry, void *end,\n\t\t\t void *value_start)\n{\n\tstruct ext4_xattr_entry *e = entry;\n\n\t/* Find the end of the names list */\n\twhile (!IS_LAST_ENTRY(e)) {\n\t\tstruct ext4_xattr_entry *next = EXT4_XATTR_NEXT(e);\n\t\tif ((void *)next >= end)\n\t\t\treturn -EFSCORRUPTED;\n\t\te = next;\n\t}\n\n\t/* Check the values */\n\twhile (!IS_LAST_ENTRY(entry)) {\n\t\tif (entry->e_value_size != 0 &&\n\t\t    entry->e_value_inum == 0) {\n\t\t\tu16 offs = le16_to_cpu(entry->e_value_offs);\n\t\t\tu32 size = le32_to_cpu(entry->e_value_size);\n\t\t\tvoid *value;\n\n\t\t\t/*\n\t\t\t * The value cannot overlap the names, and the value\n\t\t\t * with padding cannot extend beyond 'end'.  Check both\n\t\t\t * the padded and unpadded sizes, since the size may\n\t\t\t * overflow to 0 when adding padding.\n\t\t\t */\n\t\t\tif (offs > end - value_start)\n\t\t\t\treturn -EFSCORRUPTED;\n\t\t\tvalue = value_start + offs;\n\t\t\tif (value < (void *)e + sizeof(u32) ||\n\t\t\t    size > end - value ||\n\t\t\t    EXT4_XATTR_SIZE(size) > end - value)\n\t\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\t\tentry = EXT4_XATTR_NEXT(entry);\n\t}\n\n\treturn 0;\n}",
                        "code_after_change": "static int\next4_xattr_check_entries(struct ext4_xattr_entry *entry, void *end,\n\t\t\t void *value_start)\n{\n\tstruct ext4_xattr_entry *e = entry;\n\n\t/* Find the end of the names list */\n\twhile (!IS_LAST_ENTRY(e)) {\n\t\tstruct ext4_xattr_entry *next = EXT4_XATTR_NEXT(e);\n\t\tif ((void *)next >= end)\n\t\t\treturn -EFSCORRUPTED;\n\t\te = next;\n\t}\n\n\t/* Check the values */\n\twhile (!IS_LAST_ENTRY(entry)) {\n\t\tu32 size = le32_to_cpu(entry->e_value_size);\n\n\t\tif (size > INT_MAX)\n\t\t\treturn -EFSCORRUPTED;\n\n\t\tif (size != 0 && entry->e_value_inum == 0) {\n\t\t\tu16 offs = le16_to_cpu(entry->e_value_offs);\n\t\t\tvoid *value;\n\n\t\t\t/*\n\t\t\t * The value cannot overlap the names, and the value\n\t\t\t * with padding cannot extend beyond 'end'.  Check both\n\t\t\t * the padded and unpadded sizes, since the size may\n\t\t\t * overflow to 0 when adding padding.\n\t\t\t */\n\t\t\tif (offs > end - value_start)\n\t\t\t\treturn -EFSCORRUPTED;\n\t\t\tvalue = value_start + offs;\n\t\t\tif (value < (void *)e + sizeof(u32) ||\n\t\t\t    size > end - value ||\n\t\t\t    EXT4_XATTR_SIZE(size) > end - value)\n\t\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\t\tentry = EXT4_XATTR_NEXT(entry);\n\t}\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2018-1095"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void ftrace_syscall_exit(void *data, struct pt_regs *regs, long ret)\n{\n\tstruct trace_array *tr = data;\n\tstruct ftrace_event_file *ftrace_file;\n\tstruct syscall_trace_exit *entry;\n\tstruct syscall_metadata *sys_data;\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer *buffer;\n\tunsigned long irq_flags;\n\tint pc;\n\tint syscall_nr;\n\n\tsyscall_nr = trace_get_syscall_nr(current, regs);\n\tif (syscall_nr < 0)\n\t\treturn;\n\n\t/* Here we're inside tp handler's rcu_read_lock_sched (__DO_TRACE()) */\n\tftrace_file = rcu_dereference_sched(tr->exit_syscall_files[syscall_nr]);\n\tif (!ftrace_file)\n\t\treturn;\n\n\tif (ftrace_trigger_soft_disabled(ftrace_file))\n\t\treturn;\n\n\tsys_data = syscall_nr_to_meta(syscall_nr);\n\tif (!sys_data)\n\t\treturn;\n\n\tlocal_save_flags(irq_flags);\n\tpc = preempt_count();\n\n\tbuffer = tr->trace_buffer.buffer;\n\tevent = trace_buffer_lock_reserve(buffer,\n\t\t\tsys_data->exit_event->event.type, sizeof(*entry),\n\t\t\tirq_flags, pc);\n\tif (!event)\n\t\treturn;\n\n\tentry = ring_buffer_event_data(event);\n\tentry->nr = syscall_nr;\n\tentry->ret = syscall_get_return_value(current, regs);\n\n\tevent_trigger_unlock_commit(ftrace_file, buffer, event, entry,\n\t\t\t\t    irq_flags, pc);\n}",
                        "code_after_change": "static void netvsc_get_ethtool_stats(struct net_device *dev,\n\t\t\t\t     struct ethtool_stats *stats, u64 *data)\n{\n\tstruct net_device_context *ndc = netdev_priv(dev);\n\tstruct netvsc_device *nvdev = rtnl_dereference(ndc->nvdev);\n\tconst void *nds = &ndc->eth_stats;\n\tconst struct netvsc_stats *qstats;\n\tstruct netvsc_vf_pcpu_stats sum;\n\tstruct netvsc_ethtool_pcpu_stats *pcpu_sum;\n\tunsigned int start;\n\tu64 packets, bytes;\n\tu64 xdp_drop;\n\tint i, j, cpu;\n\n\tif (!nvdev)\n\t\treturn;\n\n\tfor (i = 0; i < NETVSC_GLOBAL_STATS_LEN; i++)\n\t\tdata[i] = *(unsigned long *)(nds + netvsc_stats[i].offset);\n\n\tnetvsc_get_vf_stats(dev, &sum);\n\tfor (j = 0; j < NETVSC_VF_STATS_LEN; j++)\n\t\tdata[i++] = *(u64 *)((void *)&sum + vf_stats[j].offset);\n\n\tfor (j = 0; j < nvdev->num_chn; j++) {\n\t\tqstats = &nvdev->chan_table[j].tx_stats;\n\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin_irq(&qstats->syncp);\n\t\t\tpackets = qstats->packets;\n\t\t\tbytes = qstats->bytes;\n\t\t} while (u64_stats_fetch_retry_irq(&qstats->syncp, start));\n\t\tdata[i++] = packets;\n\t\tdata[i++] = bytes;\n\n\t\tqstats = &nvdev->chan_table[j].rx_stats;\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin_irq(&qstats->syncp);\n\t\t\tpackets = qstats->packets;\n\t\t\tbytes = qstats->bytes;\n\t\t\txdp_drop = qstats->xdp_drop;\n\t\t} while (u64_stats_fetch_retry_irq(&qstats->syncp, start));\n\t\tdata[i++] = packets;\n\t\tdata[i++] = bytes;\n\t\tdata[i++] = xdp_drop;\n\t}\n\n\tpcpu_sum = kvmalloc_array(num_possible_cpus(),\n\t\t\t\t  sizeof(struct netvsc_ethtool_pcpu_stats),\n\t\t\t\t  GFP_KERNEL);\n\tif (!pcpu_sum)\n\t\treturn;\n\n\tnetvsc_get_pcpu_stats(dev, pcpu_sum);\n\tfor_each_present_cpu(cpu) {\n\t\tstruct netvsc_ethtool_pcpu_stats *this_sum = &pcpu_sum[cpu];\n\n\t\tfor (j = 0; j < ARRAY_SIZE(pcpu_stats); j++)\n\t\t\tdata[i++] = *(u64 *)((void *)this_sum\n\t\t\t\t\t     + pcpu_stats[j].offset);\n\t}\n\tkvfree(pcpu_sum);\n}",
                        "cve_id": "CVE-2014-7826"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void ftrace_syscall_enter(void *data, struct pt_regs *regs, long id)\n{\n\tstruct trace_array *tr = data;\n\tstruct ftrace_event_file *ftrace_file;\n\tstruct syscall_trace_enter *entry;\n\tstruct syscall_metadata *sys_data;\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer *buffer;\n\tunsigned long irq_flags;\n\tint pc;\n\tint syscall_nr;\n\tint size;\n\n\tsyscall_nr = trace_get_syscall_nr(current, regs);\n\tif (syscall_nr < 0)\n\t\treturn;\n\n\t/* Here we're inside tp handler's rcu_read_lock_sched (__DO_TRACE) */\n\tftrace_file = rcu_dereference_sched(tr->enter_syscall_files[syscall_nr]);\n\tif (!ftrace_file)\n\t\treturn;\n\n\tif (ftrace_trigger_soft_disabled(ftrace_file))\n\t\treturn;\n\n\tsys_data = syscall_nr_to_meta(syscall_nr);\n\tif (!sys_data)\n\t\treturn;\n\n\tsize = sizeof(*entry) + sizeof(unsigned long) * sys_data->nb_args;\n\n\tlocal_save_flags(irq_flags);\n\tpc = preempt_count();\n\n\tbuffer = tr->trace_buffer.buffer;\n\tevent = trace_buffer_lock_reserve(buffer,\n\t\t\tsys_data->enter_event->event.type, size, irq_flags, pc);\n\tif (!event)\n\t\treturn;\n\n\tentry = ring_buffer_event_data(event);\n\tentry->nr = syscall_nr;\n\tsyscall_get_arguments(current, regs, 0, sys_data->nb_args, entry->args);\n\n\tevent_trigger_unlock_commit(ftrace_file, buffer, event, entry,\n\t\t\t\t    irq_flags, pc);\n}",
                        "code_after_change": "static void ftrace_syscall_exit(void *data, struct pt_regs *regs, long ret)\n{\n\tstruct trace_array *tr = data;\n\tstruct ftrace_event_file *ftrace_file;\n\tstruct syscall_trace_exit *entry;\n\tstruct syscall_metadata *sys_data;\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer *buffer;\n\tunsigned long irq_flags;\n\tint pc;\n\tint syscall_nr;\n\n\tsyscall_nr = trace_get_syscall_nr(current, regs);\n\tif (syscall_nr < 0 || syscall_nr >= NR_syscalls)\n\t\treturn;\n\n\t/* Here we're inside tp handler's rcu_read_lock_sched (__DO_TRACE()) */\n\tftrace_file = rcu_dereference_sched(tr->exit_syscall_files[syscall_nr]);\n\tif (!ftrace_file)\n\t\treturn;\n\n\tif (ftrace_trigger_soft_disabled(ftrace_file))\n\t\treturn;\n\n\tsys_data = syscall_nr_to_meta(syscall_nr);\n\tif (!sys_data)\n\t\treturn;\n\n\tlocal_save_flags(irq_flags);\n\tpc = preempt_count();\n\n\tbuffer = tr->trace_buffer.buffer;\n\tevent = trace_buffer_lock_reserve(buffer,\n\t\t\tsys_data->exit_event->event.type, sizeof(*entry),\n\t\t\tirq_flags, pc);\n\tif (!event)\n\t\treturn;\n\n\tentry = ring_buffer_event_data(event);\n\tentry->nr = syscall_nr;\n\tentry->ret = syscall_get_return_value(current, regs);\n\n\tevent_trigger_unlock_commit(ftrace_file, buffer, event, entry,\n\t\t\t\t    irq_flags, pc);\n}",
                        "cve_id": "CVE-2014-7826"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 2699,
            "cve_id": "CVE-2020-25285",
            "code_snippet": "static int hugetlb_sysctl_handler_common(bool obey_mempolicy,\n\t\t\t struct ctl_table *table, int write,\n\t\t\t void *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp = h->max_huge_pages;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\tret = proc_hugetlb_doulongvec_minmax(table, write, buffer, length, ppos,\n\t\t\t\t\t     &tmp);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write)\n\t\tret = __nr_hugepages_store_common(obey_mempolicy, h,\n\t\t\t\t\t\t  NUMA_NO_NODE, tmp, *length);\nout:\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int hugetlb_overcommit_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\ttmp = h->nr_overcommit_huge_pages;\n\n\tif (write && hstate_is_gigantic(h))\n\t\treturn -EINVAL;\n\n\ttable->data = &tmp;\n\ttable->maxlen = sizeof(unsigned long);\n\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write) {\n\t\tspin_lock(&hugetlb_lock);\n\t\th->nr_overcommit_huge_pages = tmp;\n\t\tspin_unlock(&hugetlb_lock);\n\t}\nout:\n\treturn ret;\n}",
                        "code_after_change": "int hugetlb_overcommit_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\ttmp = h->nr_overcommit_huge_pages;\n\n\tif (write && hstate_is_gigantic(h))\n\t\treturn -EINVAL;\n\n\tret = proc_hugetlb_doulongvec_minmax(table, write, buffer, length, ppos,\n\t\t\t\t\t     &tmp);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write) {\n\t\tspin_lock(&hugetlb_lock);\n\t\th->nr_overcommit_huge_pages = tmp;\n\t\tspin_unlock(&hugetlb_lock);\n\t}\nout:\n\treturn ret;\n}",
                        "cve_id": "CVE-2020-25285"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int stub_send_ret_submit(struct stub_device *sdev)\n{\n\tunsigned long flags;\n\tstruct stub_priv *priv, *tmp;\n\n\tstruct msghdr msg;\n\tsize_t txsize;\n\n\tsize_t total_size = 0;\n\n\twhile ((priv = dequeue_from_priv_tx(sdev)) != NULL) {\n\t\tint ret;\n\t\tstruct urb *urb = priv->urb;\n\t\tstruct usbip_header pdu_header;\n\t\tstruct usbip_iso_packet_descriptor *iso_buffer = NULL;\n\t\tstruct kvec *iov = NULL;\n\t\tint iovnum = 0;\n\n\t\ttxsize = 0;\n\t\tmemset(&pdu_header, 0, sizeof(pdu_header));\n\t\tmemset(&msg, 0, sizeof(msg));\n\n\t\tif (usb_pipetype(urb->pipe) == PIPE_ISOCHRONOUS)\n\t\t\tiovnum = 2 + urb->number_of_packets;\n\t\telse\n\t\t\tiovnum = 2;\n\n\t\tiov = kcalloc(iovnum, sizeof(struct kvec), GFP_KERNEL);\n\n\t\tif (!iov) {\n\t\t\tusbip_event_add(&sdev->ud, SDEV_EVENT_ERROR_MALLOC);\n\t\t\treturn -1;\n\t\t}\n\n\t\tiovnum = 0;\n\n\t\t/* 1. setup usbip_header */\n\t\tsetup_ret_submit_pdu(&pdu_header, urb);\n\t\tusbip_dbg_stub_tx(\"setup txdata seqnum: %d urb: %p\\n\",\n\t\t\t\t  pdu_header.base.seqnum, urb);\n\t\tusbip_header_correct_endian(&pdu_header, 1);\n\n\t\tiov[iovnum].iov_base = &pdu_header;\n\t\tiov[iovnum].iov_len  = sizeof(pdu_header);\n\t\tiovnum++;\n\t\ttxsize += sizeof(pdu_header);\n\n\t\t/* 2. setup transfer buffer */\n\t\tif (usb_pipein(urb->pipe) &&\n\t\t    usb_pipetype(urb->pipe) != PIPE_ISOCHRONOUS &&\n\t\t    urb->actual_length > 0) {\n\t\t\tiov[iovnum].iov_base = urb->transfer_buffer;\n\t\t\tiov[iovnum].iov_len  = urb->actual_length;\n\t\t\tiovnum++;\n\t\t\ttxsize += urb->actual_length;\n\t\t} else if (usb_pipein(urb->pipe) &&\n\t\t\t   usb_pipetype(urb->pipe) == PIPE_ISOCHRONOUS) {\n\t\t\t/*\n\t\t\t * For isochronous packets: actual length is the sum of\n\t\t\t * the actual length of the individual, packets, but as\n\t\t\t * the packet offsets are not changed there will be\n\t\t\t * padding between the packets. To optimally use the\n\t\t\t * bandwidth the padding is not transmitted.\n\t\t\t */\n\n\t\t\tint i;\n\n\t\t\tfor (i = 0; i < urb->number_of_packets; i++) {\n\t\t\t\tiov[iovnum].iov_base = urb->transfer_buffer +\n\t\t\t\t\turb->iso_frame_desc[i].offset;\n\t\t\t\tiov[iovnum].iov_len =\n\t\t\t\t\turb->iso_frame_desc[i].actual_length;\n\t\t\t\tiovnum++;\n\t\t\t\ttxsize += urb->iso_frame_desc[i].actual_length;\n\t\t\t}\n\n\t\t\tif (txsize != sizeof(pdu_header) + urb->actual_length) {\n\t\t\t\tdev_err(&sdev->udev->dev,\n\t\t\t\t\t\"actual length of urb %d does not match iso packet sizes %zu\\n\",\n\t\t\t\t\turb->actual_length,\n\t\t\t\t\ttxsize-sizeof(pdu_header));\n\t\t\t\tkfree(iov);\n\t\t\t\tusbip_event_add(&sdev->ud,\n\t\t\t\t\t\tSDEV_EVENT_ERROR_TCP);\n\t\t\t   return -1;\n\t\t\t}\n\t\t}\n\n\t\t/* 3. setup iso_packet_descriptor */\n\t\tif (usb_pipetype(urb->pipe) == PIPE_ISOCHRONOUS) {\n\t\t\tssize_t len = 0;\n\n\t\t\tiso_buffer = usbip_alloc_iso_desc_pdu(urb, &len);\n\t\t\tif (!iso_buffer) {\n\t\t\t\tusbip_event_add(&sdev->ud,\n\t\t\t\t\t\tSDEV_EVENT_ERROR_MALLOC);\n\t\t\t\tkfree(iov);\n\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tiov[iovnum].iov_base = iso_buffer;\n\t\t\tiov[iovnum].iov_len  = len;\n\t\t\ttxsize += len;\n\t\t\tiovnum++;\n\t\t}\n\n\t\tret = kernel_sendmsg(sdev->ud.tcp_socket, &msg,\n\t\t\t\t\t\tiov,  iovnum, txsize);\n\t\tif (ret != txsize) {\n\t\t\tdev_err(&sdev->udev->dev,\n\t\t\t\t\"sendmsg failed!, retval %d for %zd\\n\",\n\t\t\t\tret, txsize);\n\t\t\tkfree(iov);\n\t\t\tkfree(iso_buffer);\n\t\t\tusbip_event_add(&sdev->ud, SDEV_EVENT_ERROR_TCP);\n\t\t\treturn -1;\n\t\t}\n\n\t\tkfree(iov);\n\t\tkfree(iso_buffer);\n\n\t\ttotal_size += txsize;\n\t}\n\n\tspin_lock_irqsave(&sdev->priv_lock, flags);\n\tlist_for_each_entry_safe(priv, tmp, &sdev->priv_free, list) {\n\t\tstub_free_priv_and_urb(priv);\n\t}\n\tspin_unlock_irqrestore(&sdev->priv_lock, flags);\n\n\treturn total_size;\n}",
                        "code_after_change": "static ssize_t o2nm_node_local_store(struct config_item *item, const char *page,\n\t\t\t\t     size_t count)\n{\n\tstruct o2nm_node *node = to_o2nm_node(item);\n\tstruct o2nm_cluster *cluster;\n\tunsigned long tmp;\n\tchar *p = (char *)page;\n\tssize_t ret;\n\n\ttmp = simple_strtoul(p, &p, 0);\n\tif (!p || (*p && (*p != '\\n')))\n\t\treturn -EINVAL;\n\n\ttmp = !!tmp; /* boolean of whether this node wants to be local */\n\n\t/* setting local turns on networking rx for now so we require having\n\t * set everything else first */\n\tif (!test_bit(O2NM_NODE_ATTR_ADDRESS, &node->nd_set_attributes) ||\n\t    !test_bit(O2NM_NODE_ATTR_NUM, &node->nd_set_attributes) ||\n\t    !test_bit(O2NM_NODE_ATTR_PORT, &node->nd_set_attributes))\n\t\treturn -EINVAL; /* XXX */\n\n\to2nm_lock_subsystem();\n\tcluster = to_o2nm_cluster_from_node(node);\n\tif (!cluster) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* the only failure case is trying to set a new local node\n\t * when a different one is already set */\n\tif (tmp && tmp == cluster->cl_has_local &&\n\t    cluster->cl_local_node != node->nd_num) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\t/* bring up the rx thread if we're setting the new local node. */\n\tif (tmp && !cluster->cl_has_local) {\n\t\tret = o2net_start_listening(node);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tif (!tmp && cluster->cl_has_local &&\n\t    cluster->cl_local_node == node->nd_num) {\n\t\to2net_stop_listening(node);\n\t\tcluster->cl_local_node = O2NM_INVALID_NODE_NUM;\n\t}\n\n\tnode->nd_local = tmp;\n\tif (node->nd_local) {\n\t\tcluster->cl_has_local = tmp;\n\t\tcluster->cl_local_node = node->nd_num;\n\t}\n\n\tret = count;\n\nout:\n\to2nm_unlock_subsystem();\n\treturn ret;\n}",
                        "cve_id": "CVE-2017-16914"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static ssize_t o2nm_node_local_store(struct config_item *item, const char *page,\n\t\t\t\t     size_t count)\n{\n\tstruct o2nm_node *node = to_o2nm_node(item);\n\tstruct o2nm_cluster *cluster = to_o2nm_cluster_from_node(node);\n\tunsigned long tmp;\n\tchar *p = (char *)page;\n\tssize_t ret;\n\n\ttmp = simple_strtoul(p, &p, 0);\n\tif (!p || (*p && (*p != '\\n')))\n\t\treturn -EINVAL;\n\n\ttmp = !!tmp; /* boolean of whether this node wants to be local */\n\n\t/* setting local turns on networking rx for now so we require having\n\t * set everything else first */\n\tif (!test_bit(O2NM_NODE_ATTR_ADDRESS, &node->nd_set_attributes) ||\n\t    !test_bit(O2NM_NODE_ATTR_NUM, &node->nd_set_attributes) ||\n\t    !test_bit(O2NM_NODE_ATTR_PORT, &node->nd_set_attributes))\n\t\treturn -EINVAL; /* XXX */\n\n\t/* the only failure case is trying to set a new local node\n\t * when a different one is already set */\n\tif (tmp && tmp == cluster->cl_has_local &&\n\t    cluster->cl_local_node != node->nd_num)\n\t\treturn -EBUSY;\n\n\t/* bring up the rx thread if we're setting the new local node. */\n\tif (tmp && !cluster->cl_has_local) {\n\t\tret = o2net_start_listening(node);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (!tmp && cluster->cl_has_local &&\n\t    cluster->cl_local_node == node->nd_num) {\n\t\to2net_stop_listening(node);\n\t\tcluster->cl_local_node = O2NM_INVALID_NODE_NUM;\n\t}\n\n\tnode->nd_local = tmp;\n\tif (node->nd_local) {\n\t\tcluster->cl_has_local = tmp;\n\t\tcluster->cl_local_node = node->nd_num;\n\t}\n\n\treturn count;\n}",
                        "code_after_change": "static int stub_send_ret_submit(struct stub_device *sdev)\n{\n\tunsigned long flags;\n\tstruct stub_priv *priv, *tmp;\n\n\tstruct msghdr msg;\n\tsize_t txsize;\n\n\tsize_t total_size = 0;\n\n\twhile ((priv = dequeue_from_priv_tx(sdev)) != NULL) {\n\t\tint ret;\n\t\tstruct urb *urb = priv->urb;\n\t\tstruct usbip_header pdu_header;\n\t\tstruct usbip_iso_packet_descriptor *iso_buffer = NULL;\n\t\tstruct kvec *iov = NULL;\n\t\tint iovnum = 0;\n\n\t\ttxsize = 0;\n\t\tmemset(&pdu_header, 0, sizeof(pdu_header));\n\t\tmemset(&msg, 0, sizeof(msg));\n\n\t\tif (urb->actual_length > 0 && !urb->transfer_buffer) {\n\t\t\tdev_err(&sdev->udev->dev,\n\t\t\t\t\"urb: actual_length %d transfer_buffer null\\n\",\n\t\t\t\turb->actual_length);\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (usb_pipetype(urb->pipe) == PIPE_ISOCHRONOUS)\n\t\t\tiovnum = 2 + urb->number_of_packets;\n\t\telse\n\t\t\tiovnum = 2;\n\n\t\tiov = kcalloc(iovnum, sizeof(struct kvec), GFP_KERNEL);\n\n\t\tif (!iov) {\n\t\t\tusbip_event_add(&sdev->ud, SDEV_EVENT_ERROR_MALLOC);\n\t\t\treturn -1;\n\t\t}\n\n\t\tiovnum = 0;\n\n\t\t/* 1. setup usbip_header */\n\t\tsetup_ret_submit_pdu(&pdu_header, urb);\n\t\tusbip_dbg_stub_tx(\"setup txdata seqnum: %d urb: %p\\n\",\n\t\t\t\t  pdu_header.base.seqnum, urb);\n\t\tusbip_header_correct_endian(&pdu_header, 1);\n\n\t\tiov[iovnum].iov_base = &pdu_header;\n\t\tiov[iovnum].iov_len  = sizeof(pdu_header);\n\t\tiovnum++;\n\t\ttxsize += sizeof(pdu_header);\n\n\t\t/* 2. setup transfer buffer */\n\t\tif (usb_pipein(urb->pipe) &&\n\t\t    usb_pipetype(urb->pipe) != PIPE_ISOCHRONOUS &&\n\t\t    urb->actual_length > 0) {\n\t\t\tiov[iovnum].iov_base = urb->transfer_buffer;\n\t\t\tiov[iovnum].iov_len  = urb->actual_length;\n\t\t\tiovnum++;\n\t\t\ttxsize += urb->actual_length;\n\t\t} else if (usb_pipein(urb->pipe) &&\n\t\t\t   usb_pipetype(urb->pipe) == PIPE_ISOCHRONOUS) {\n\t\t\t/*\n\t\t\t * For isochronous packets: actual length is the sum of\n\t\t\t * the actual length of the individual, packets, but as\n\t\t\t * the packet offsets are not changed there will be\n\t\t\t * padding between the packets. To optimally use the\n\t\t\t * bandwidth the padding is not transmitted.\n\t\t\t */\n\n\t\t\tint i;\n\n\t\t\tfor (i = 0; i < urb->number_of_packets; i++) {\n\t\t\t\tiov[iovnum].iov_base = urb->transfer_buffer +\n\t\t\t\t\turb->iso_frame_desc[i].offset;\n\t\t\t\tiov[iovnum].iov_len =\n\t\t\t\t\turb->iso_frame_desc[i].actual_length;\n\t\t\t\tiovnum++;\n\t\t\t\ttxsize += urb->iso_frame_desc[i].actual_length;\n\t\t\t}\n\n\t\t\tif (txsize != sizeof(pdu_header) + urb->actual_length) {\n\t\t\t\tdev_err(&sdev->udev->dev,\n\t\t\t\t\t\"actual length of urb %d does not match iso packet sizes %zu\\n\",\n\t\t\t\t\turb->actual_length,\n\t\t\t\t\ttxsize-sizeof(pdu_header));\n\t\t\t\tkfree(iov);\n\t\t\t\tusbip_event_add(&sdev->ud,\n\t\t\t\t\t\tSDEV_EVENT_ERROR_TCP);\n\t\t\t   return -1;\n\t\t\t}\n\t\t}\n\n\t\t/* 3. setup iso_packet_descriptor */\n\t\tif (usb_pipetype(urb->pipe) == PIPE_ISOCHRONOUS) {\n\t\t\tssize_t len = 0;\n\n\t\t\tiso_buffer = usbip_alloc_iso_desc_pdu(urb, &len);\n\t\t\tif (!iso_buffer) {\n\t\t\t\tusbip_event_add(&sdev->ud,\n\t\t\t\t\t\tSDEV_EVENT_ERROR_MALLOC);\n\t\t\t\tkfree(iov);\n\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tiov[iovnum].iov_base = iso_buffer;\n\t\t\tiov[iovnum].iov_len  = len;\n\t\t\ttxsize += len;\n\t\t\tiovnum++;\n\t\t}\n\n\t\tret = kernel_sendmsg(sdev->ud.tcp_socket, &msg,\n\t\t\t\t\t\tiov,  iovnum, txsize);\n\t\tif (ret != txsize) {\n\t\t\tdev_err(&sdev->udev->dev,\n\t\t\t\t\"sendmsg failed!, retval %d for %zd\\n\",\n\t\t\t\tret, txsize);\n\t\t\tkfree(iov);\n\t\t\tkfree(iso_buffer);\n\t\t\tusbip_event_add(&sdev->ud, SDEV_EVENT_ERROR_TCP);\n\t\t\treturn -1;\n\t\t}\n\n\t\tkfree(iov);\n\t\tkfree(iso_buffer);\n\n\t\ttotal_size += txsize;\n\t}\n\n\tspin_lock_irqsave(&sdev->priv_lock, flags);\n\tlist_for_each_entry_safe(priv, tmp, &sdev->priv_free, list) {\n\t\tstub_free_priv_and_urb(priv);\n\t}\n\tspin_unlock_irqrestore(&sdev->priv_lock, flags);\n\n\treturn total_size;\n}",
                        "cve_id": "CVE-2017-18216"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int submit_lookup_cmds(struct msm_gem_submit *submit,\n\t\tstruct drm_msm_gem_submit *args, struct drm_file *file)\n{\n\tunsigned i;\n\tsize_t sz;\n\tint ret = 0;\n\n\tfor (i = 0; i < args->nr_cmds; i++) {\n\t\tstruct drm_msm_gem_submit_cmd submit_cmd;\n\t\tvoid __user *userptr =\n\t\t\tu64_to_user_ptr(args->cmds + (i * sizeof(submit_cmd)));\n\n\t\tret = copy_from_user(&submit_cmd, userptr, sizeof(submit_cmd));\n\t\tif (ret) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/* validate input from userspace: */\n\t\tswitch (submit_cmd.type) {\n\t\tcase MSM_SUBMIT_CMD_BUF:\n\t\tcase MSM_SUBMIT_CMD_IB_TARGET_BUF:\n\t\tcase MSM_SUBMIT_CMD_CTX_RESTORE_BUF:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"invalid type: %08x\\n\", submit_cmd.type);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (submit_cmd.size % 4) {\n\t\t\tDRM_ERROR(\"non-aligned cmdstream buffer size: %u\\n\",\n\t\t\t\t\tsubmit_cmd.size);\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tsubmit->cmd[i].type = submit_cmd.type;\n\t\tsubmit->cmd[i].size = submit_cmd.size / 4;\n\t\tsubmit->cmd[i].offset = submit_cmd.submit_offset / 4;\n\t\tsubmit->cmd[i].idx  = submit_cmd.submit_idx;\n\t\tsubmit->cmd[i].nr_relocs = submit_cmd.nr_relocs;\n\n\t\tuserptr = u64_to_user_ptr(submit_cmd.relocs);\n\n\t\tsz = array_size(submit_cmd.nr_relocs,\n\t\t\t\tsizeof(struct drm_msm_gem_submit_reloc));\n\t\t/* check for overflow: */\n\t\tif (sz == SIZE_MAX) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tsubmit->cmd[i].relocs = kmalloc(sz, GFP_KERNEL);\n\t\tret = copy_from_user(submit->cmd[i].relocs, userptr, sz);\n\t\tif (ret) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\treturn ret;\n}",
                        "code_after_change": "static int submit_lookup_cmds(struct msm_gem_submit *submit,\n\t\tstruct drm_msm_gem_submit *args, struct drm_file *file)\n{\n\tunsigned i;\n\tsize_t sz;\n\tint ret = 0;\n\n\tfor (i = 0; i < args->nr_cmds; i++) {\n\t\tstruct drm_msm_gem_submit_cmd submit_cmd;\n\t\tvoid __user *userptr =\n\t\t\tu64_to_user_ptr(args->cmds + (i * sizeof(submit_cmd)));\n\n\t\tret = copy_from_user(&submit_cmd, userptr, sizeof(submit_cmd));\n\t\tif (ret) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/* validate input from userspace: */\n\t\tswitch (submit_cmd.type) {\n\t\tcase MSM_SUBMIT_CMD_BUF:\n\t\tcase MSM_SUBMIT_CMD_IB_TARGET_BUF:\n\t\tcase MSM_SUBMIT_CMD_CTX_RESTORE_BUF:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"invalid type: %08x\\n\", submit_cmd.type);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (submit_cmd.size % 4) {\n\t\t\tDRM_ERROR(\"non-aligned cmdstream buffer size: %u\\n\",\n\t\t\t\t\tsubmit_cmd.size);\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tsubmit->cmd[i].type = submit_cmd.type;\n\t\tsubmit->cmd[i].size = submit_cmd.size / 4;\n\t\tsubmit->cmd[i].offset = submit_cmd.submit_offset / 4;\n\t\tsubmit->cmd[i].idx  = submit_cmd.submit_idx;\n\t\tsubmit->cmd[i].nr_relocs = submit_cmd.nr_relocs;\n\n\t\tuserptr = u64_to_user_ptr(submit_cmd.relocs);\n\n\t\tsz = array_size(submit_cmd.nr_relocs,\n\t\t\t\tsizeof(struct drm_msm_gem_submit_reloc));\n\t\t/* check for overflow: */\n\t\tif (sz == SIZE_MAX) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tsubmit->cmd[i].relocs = kmalloc(sz, GFP_KERNEL);\n\t\tif (!submit->cmd[i].relocs) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tret = copy_from_user(submit->cmd[i].relocs, userptr, sz);\n\t\tif (ret) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\treturn ret;\n}",
                        "cve_id": "CVE-2023-3355"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int io_file_bitmap_get(struct io_ring_ctx *ctx)\n{\n\tstruct io_file_table *table = &ctx->file_table;\n\tunsigned long nr = ctx->file_alloc_end;\n\tint ret;\n\n\tdo {\n\t\tret = find_next_zero_bit(table->bitmap, nr, table->alloc_hint);\n\t\tif (ret != nr)\n\t\t\treturn ret;\n\n\t\tif (table->alloc_hint == ctx->file_alloc_start)\n\t\t\tbreak;\n\t\tnr = table->alloc_hint;\n\t\ttable->alloc_hint = ctx->file_alloc_start;\n\t} while (1);\n\n\treturn -ENFILE;\n}",
                        "code_after_change": "static int io_file_bitmap_get(struct io_ring_ctx *ctx)\n{\n\tstruct io_file_table *table = &ctx->file_table;\n\tunsigned long nr = ctx->file_alloc_end;\n\tint ret;\n\n\tif (!table->bitmap)\n\t\treturn -ENFILE;\n\n\tdo {\n\t\tret = find_next_zero_bit(table->bitmap, nr, table->alloc_hint);\n\t\tif (ret != nr)\n\t\t\treturn ret;\n\n\t\tif (table->alloc_hint == ctx->file_alloc_start)\n\t\t\tbreak;\n\t\tnr = table->alloc_hint;\n\t\ttable->alloc_hint = ctx->file_alloc_start;\n\t} while (1);\n\n\treturn -ENFILE;\n}",
                        "cve_id": "CVE-2023-1583"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 2734,
            "cve_id": "CVE-2020-27675",
            "code_snippet": "static void clear_evtchn_to_irq_row(unsigned row)\n{\n\tunsigned col;\n\n\tfor (col = 0; col < EVTCHN_PER_ROW; col++)\n\t\tWRITE_ONCE(evtchn_to_irq[row][col], -1);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int set_evtchn_to_irq(evtchn_port_t evtchn, unsigned int irq)\n{\n\tunsigned row;\n\tunsigned col;\n\n\tif (evtchn >= xen_evtchn_max_channels())\n\t\treturn -EINVAL;\n\n\trow = EVTCHN_ROW(evtchn);\n\tcol = EVTCHN_COL(evtchn);\n\n\tif (evtchn_to_irq[row] == NULL) {\n\t\t/* Unallocated irq entries return -1 anyway */\n\t\tif (irq == -1)\n\t\t\treturn 0;\n\n\t\tevtchn_to_irq[row] = (int *)get_zeroed_page(GFP_KERNEL);\n\t\tif (evtchn_to_irq[row] == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\tclear_evtchn_to_irq_row(row);\n\t}\n\n\tevtchn_to_irq[row][col] = irq;\n\treturn 0;\n}",
                        "code_after_change": "static int set_evtchn_to_irq(evtchn_port_t evtchn, unsigned int irq)\n{\n\tunsigned row;\n\tunsigned col;\n\n\tif (evtchn >= xen_evtchn_max_channels())\n\t\treturn -EINVAL;\n\n\trow = EVTCHN_ROW(evtchn);\n\tcol = EVTCHN_COL(evtchn);\n\n\tif (evtchn_to_irq[row] == NULL) {\n\t\t/* Unallocated irq entries return -1 anyway */\n\t\tif (irq == -1)\n\t\t\treturn 0;\n\n\t\tevtchn_to_irq[row] = (int *)get_zeroed_page(GFP_KERNEL);\n\t\tif (evtchn_to_irq[row] == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\tclear_evtchn_to_irq_row(row);\n\t}\n\n\tWRITE_ONCE(evtchn_to_irq[row][col], irq);\n\treturn 0;\n}",
                        "cve_id": "CVE-2020-27675"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2735,
            "cve_id": "CVE-2020-27675",
            "code_snippet": "static void xen_free_irq(unsigned irq)\n{\n\tstruct irq_info *info = info_for_irq(irq);\n\tunsigned long flags;\n\n\tif (WARN_ON(!info))\n\t\treturn;\n\n\twrite_lock_irqsave(&evtchn_rwlock, flags);\n\n\tlist_del(&info->list);\n\n\tset_info_for_irq(irq, NULL);\n\n\tWARN_ON(info->refcnt > 0);\n\n\twrite_unlock_irqrestore(&evtchn_rwlock, flags);\n\n\tkfree(info);\n\n\t/* Legacy IRQ descriptors are managed by the arch. */\n\tif (irq < nr_legacy_irqs())\n\t\treturn;\n\n\tirq_free_desc(irq);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static irqreturn_t i8042_interrupt(int irq, void *dev_id)\n{\n\tstruct i8042_port *port;\n\tstruct serio *serio;\n\tunsigned long flags;\n\tunsigned char str, data;\n\tunsigned int dfl;\n\tunsigned int port_no;\n\tbool filtered;\n\tint ret = 1;\n\n\tspin_lock_irqsave(&i8042_lock, flags);\n\n\tstr = i8042_read_status();\n\tif (unlikely(~str & I8042_STR_OBF)) {\n\t\tspin_unlock_irqrestore(&i8042_lock, flags);\n\t\tif (irq)\n\t\t\tdbg(\"Interrupt %d, without any data\\n\", irq);\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tdata = i8042_read_data();\n\n\tif (i8042_mux_present && (str & I8042_STR_AUXDATA)) {\n\t\tstatic unsigned long last_transmit;\n\t\tstatic unsigned char last_str;\n\n\t\tdfl = 0;\n\t\tif (str & I8042_STR_MUXERR) {\n\t\t\tdbg(\"MUX error, status is %02x, data is %02x\\n\",\n\t\t\t    str, data);\n/*\n * When MUXERR condition is signalled the data register can only contain\n * 0xfd, 0xfe or 0xff if implementation follows the spec. Unfortunately\n * it is not always the case. Some KBCs also report 0xfc when there is\n * nothing connected to the port while others sometimes get confused which\n * port the data came from and signal error leaving the data intact. They\n * _do not_ revert to legacy mode (actually I've never seen KBC reverting\n * to legacy mode yet, when we see one we'll add proper handling).\n * Anyway, we process 0xfc, 0xfd, 0xfe and 0xff as timeouts, and for the\n * rest assume that the data came from the same serio last byte\n * was transmitted (if transmission happened not too long ago).\n */\n\n\t\t\tswitch (data) {\n\t\t\t\tdefault:\n\t\t\t\t\tif (time_before(jiffies, last_transmit + HZ/10)) {\n\t\t\t\t\t\tstr = last_str;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\t/* fall through - report timeout */\n\t\t\t\tcase 0xfc:\n\t\t\t\tcase 0xfd:\n\t\t\t\tcase 0xfe: dfl = SERIO_TIMEOUT; data = 0xfe; break;\n\t\t\t\tcase 0xff: dfl = SERIO_PARITY;  data = 0xfe; break;\n\t\t\t}\n\t\t}\n\n\t\tport_no = I8042_MUX_PORT_NO + ((str >> 6) & 3);\n\t\tlast_str = str;\n\t\tlast_transmit = jiffies;\n\t} else {\n\n\t\tdfl = ((str & I8042_STR_PARITY) ? SERIO_PARITY : 0) |\n\t\t      ((str & I8042_STR_TIMEOUT && !i8042_notimeout) ? SERIO_TIMEOUT : 0);\n\n\t\tport_no = (str & I8042_STR_AUXDATA) ?\n\t\t\t\tI8042_AUX_PORT_NO : I8042_KBD_PORT_NO;\n\t}\n\n\tport = &i8042_ports[port_no];\n\tserio = port->exists ? port->serio : NULL;\n\n\tfilter_dbg(port->driver_bound, data, \"<- i8042 (interrupt, %d, %d%s%s)\\n\",\n\t\t   port_no, irq,\n\t\t   dfl & SERIO_PARITY ? \", bad parity\" : \"\",\n\t\t   dfl & SERIO_TIMEOUT ? \", timeout\" : \"\");\n\n\tfiltered = i8042_filter(data, str, serio);\n\n\tspin_unlock_irqrestore(&i8042_lock, flags);\n\n\tif (likely(port->exists && !filtered))\n\t\tserio_interrupt(serio, data, dfl);\n\n out:\n\treturn IRQ_RETVAL(ret);\n}",
                        "code_after_change": "evtchn_port_t evtchn_from_irq(unsigned irq)\n{\n\tconst struct irq_info *info = NULL;\n\n\tif (likely(irq < nr_irqs))\n\t\tinfo = info_for_irq(irq);\n\tif (!info)\n\t\treturn 0;\n\n\treturn info->evtchn;\n}",
                        "cve_id": "CVE-2017-18079"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 2752,
            "cve_id": "CVE-2020-27830",
            "code_snippet": "static int spk_ttyio_initialise_ldisc(struct spk_synth *synth)\n{\n\tint ret = 0;\n\tstruct tty_struct *tty;\n\tstruct ktermios tmp_termios;\n\tdev_t dev;\n\n\tret = get_dev_to_use(synth, &dev);\n\tif (ret)\n\t\treturn ret;\n\n\ttty = tty_kopen(dev);\n\tif (IS_ERR(tty))\n\t\treturn PTR_ERR(tty);\n\n\tif (tty->ops->open)\n\t\tret = tty->ops->open(tty, NULL);\n\telse\n\t\tret = -ENODEV;\n\n\tif (ret) {\n\t\ttty_unlock(tty);\n\t\treturn ret;\n\t}\n\n\tclear_bit(TTY_HUPPED, &tty->flags);\n\t/* ensure hardware flow control is enabled */\n\tget_termios(tty, &tmp_termios);\n\tif (!(tmp_termios.c_cflag & CRTSCTS)) {\n\t\ttmp_termios.c_cflag |= CRTSCTS;\n\t\ttty_set_termios(tty, &tmp_termios);\n\t\t/*\n\t\t * check c_cflag to see if it's updated as tty_set_termios\n\t\t * may not return error even when no tty bits are\n\t\t * changed by the request.\n\t\t */\n\t\tget_termios(tty, &tmp_termios);\n\t\tif (!(tmp_termios.c_cflag & CRTSCTS))\n\t\t\tpr_warn(\"speakup: Failed to set hardware flow control\\n\");\n\t}\n\n\ttty_unlock(tty);\n\n\tmutex_lock(&speakup_tty_mutex);\n\tspeakup_tty = tty;\n\tret = tty_set_ldisc(tty, N_SPEAKUP);\n\tif (ret)\n\t\tspeakup_tty = NULL;\n\tmutex_unlock(&speakup_tty_mutex);\n\n\tif (!ret)\n\t\t/* Success */\n\t\treturn 0;\n\n\tpr_err(\"speakup: Failed to set N_SPEAKUP on tty\\n\");\n\n\ttty_lock(tty);\n\tif (tty->ops->close)\n\t\ttty->ops->close(tty, NULL);\n\ttty_unlock(tty);\n\n\ttty_kclose(tty);\n\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int spk_ttyio_ldisc_open(struct tty_struct *tty)\n{\n\tstruct spk_ldisc_data *ldisc_data;\n\n\tif (!tty->ops->write)\n\t\treturn -EOPNOTSUPP;\n\n\tmutex_lock(&speakup_tty_mutex);\n\tif (speakup_tty) {\n\t\tmutex_unlock(&speakup_tty_mutex);\n\t\treturn -EBUSY;\n\t}\n\tspeakup_tty = tty;\n\n\tldisc_data = kmalloc(sizeof(*ldisc_data), GFP_KERNEL);\n\tif (!ldisc_data) {\n\t\tspeakup_tty = NULL;\n\t\tmutex_unlock(&speakup_tty_mutex);\n\t\treturn -ENOMEM;\n\t}\n\n\tinit_completion(&ldisc_data->completion);\n\tldisc_data->buf_free = true;\n\tspeakup_tty->disc_data = ldisc_data;\n\tmutex_unlock(&speakup_tty_mutex);\n\n\treturn 0;\n}",
                        "code_after_change": "static int spk_ttyio_ldisc_open(struct tty_struct *tty)\n{\n\tstruct spk_ldisc_data *ldisc_data;\n\n\tif (tty != speakup_tty)\n\t\t/* Somebody tried to use this line discipline outside speakup */\n\t\treturn -ENODEV;\n\n\tif (!tty->ops->write)\n\t\treturn -EOPNOTSUPP;\n\n\tldisc_data = kmalloc(sizeof(*ldisc_data), GFP_KERNEL);\n\tif (!ldisc_data)\n\t\treturn -ENOMEM;\n\n\tinit_completion(&ldisc_data->completion);\n\tldisc_data->buf_free = true;\n\ttty->disc_data = ldisc_data;\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2020-27830"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3212,
            "cve_id": "CVE-2021-38206",
            "code_snippet": "netdev_tx_t ieee80211_monitor_start_xmit(struct sk_buff *skb,\n\t\t\t\t\t struct net_device *dev)\n{\n\tstruct ieee80211_local *local = wdev_priv(dev->ieee80211_ptr);\n\tstruct ieee80211_chanctx_conf *chanctx_conf;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_hdr *hdr;\n\tstruct ieee80211_sub_if_data *tmp_sdata, *sdata;\n\tstruct cfg80211_chan_def *chandef;\n\tu16 len_rthdr;\n\tint hdrlen;\n\n\tmemset(info, 0, sizeof(*info));\n\tinfo->flags = IEEE80211_TX_CTL_REQ_TX_STATUS |\n\t\t      IEEE80211_TX_CTL_INJECTED;\n\n\t/* Sanity-check the length of the radiotap header */\n\tif (!ieee80211_validate_radiotap_len(skb))\n\t\tgoto fail;\n\n\t/* we now know there is a radiotap header with a length we can use */\n\tlen_rthdr = ieee80211_get_radiotap_len(skb->data);\n\n\t/*\n\t * fix up the pointers accounting for the radiotap\n\t * header still being in there.  We are being given\n\t * a precooked IEEE80211 header so no need for\n\t * normal processing\n\t */\n\tskb_set_mac_header(skb, len_rthdr);\n\t/*\n\t * these are just fixed to the end of the rt area since we\n\t * don't have any better information and at this point, nobody cares\n\t */\n\tskb_set_network_header(skb, len_rthdr);\n\tskb_set_transport_header(skb, len_rthdr);\n\n\tif (skb->len < len_rthdr + 2)\n\t\tgoto fail;\n\n\thdr = (struct ieee80211_hdr *)(skb->data + len_rthdr);\n\thdrlen = ieee80211_hdrlen(hdr->frame_control);\n\n\tif (skb->len < len_rthdr + hdrlen)\n\t\tgoto fail;\n\n\t/*\n\t * Initialize skb->protocol if the injected frame is a data frame\n\t * carrying a rfc1042 header\n\t */\n\tif (ieee80211_is_data(hdr->frame_control) &&\n\t    skb->len >= len_rthdr + hdrlen + sizeof(rfc1042_header) + 2) {\n\t\tu8 *payload = (u8 *)hdr + hdrlen;\n\n\t\tif (ether_addr_equal(payload, rfc1042_header))\n\t\t\tskb->protocol = cpu_to_be16((payload[6] << 8) |\n\t\t\t\t\t\t    payload[7]);\n\t}\n\n\trcu_read_lock();\n\n\t/*\n\t * We process outgoing injected frames that have a local address\n\t * we handle as though they are non-injected frames.\n\t * This code here isn't entirely correct, the local MAC address\n\t * isn't always enough to find the interface to use; for proper\n\t * VLAN support we have an nl80211-based mechanism.\n\t *\n\t * This is necessary, for example, for old hostapd versions that\n\t * don't use nl80211-based management TX/RX.\n\t */\n\tsdata = IEEE80211_DEV_TO_SUB_IF(dev);\n\n\tlist_for_each_entry_rcu(tmp_sdata, &local->interfaces, list) {\n\t\tif (!ieee80211_sdata_running(tmp_sdata))\n\t\t\tcontinue;\n\t\tif (tmp_sdata->vif.type == NL80211_IFTYPE_MONITOR ||\n\t\t    tmp_sdata->vif.type == NL80211_IFTYPE_AP_VLAN)\n\t\t\tcontinue;\n\t\tif (ether_addr_equal(tmp_sdata->vif.addr, hdr->addr2)) {\n\t\t\tsdata = tmp_sdata;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\tif (!chanctx_conf) {\n\t\ttmp_sdata = rcu_dereference(local->monitor_sdata);\n\t\tif (tmp_sdata)\n\t\t\tchanctx_conf =\n\t\t\t\trcu_dereference(tmp_sdata->vif.chanctx_conf);\n\t}\n\n\tif (chanctx_conf)\n\t\tchandef = &chanctx_conf->def;\n\telse if (!local->use_chanctx)\n\t\tchandef = &local->_oper_chandef;\n\telse\n\t\tgoto fail_rcu;\n\n\t/*\n\t * Frame injection is not allowed if beaconing is not allowed\n\t * or if we need radar detection. Beaconing is usually not allowed when\n\t * the mode or operation (Adhoc, AP, Mesh) does not support DFS.\n\t * Passive scan is also used in world regulatory domains where\n\t * your country is not known and as such it should be treated as\n\t * NO TX unless the channel is explicitly allowed in which case\n\t * your current regulatory domain would not have the passive scan\n\t * flag.\n\t *\n\t * Since AP mode uses monitor interfaces to inject/TX management\n\t * frames we can make AP mode the exception to this rule once it\n\t * supports radar detection as its implementation can deal with\n\t * radar detection by itself. We can do that later by adding a\n\t * monitor flag interfaces used for AP support.\n\t */\n\tif (!cfg80211_reg_can_beacon(local->hw.wiphy, chandef,\n\t\t\t\t     sdata->vif.type))\n\t\tgoto fail_rcu;\n\n\tinfo->band = chandef->chan->band;\n\n\t/* Initialize skb->priority according to frame type and TID class,\n\t * with respect to the sub interface that the frame will actually\n\t * be transmitted on. If the DONT_REORDER flag is set, the original\n\t * skb-priority is preserved to assure frames injected with this\n\t * flag are not reordered relative to each other.\n\t */\n\tieee80211_select_queue_80211(sdata, skb, hdr);\n\tskb_set_queue_mapping(skb, ieee80211_ac_from_tid(skb->priority));\n\n\t/*\n\t * Process the radiotap header. This will now take into account the\n\t * selected chandef above to accurately set injection rates and\n\t * retransmissions.\n\t */\n\tif (!ieee80211_parse_tx_radiotap(skb, dev))\n\t\tgoto fail_rcu;\n\n\t/* remove the injection radiotap header */\n\tskb_pull(skb, len_rthdr);\n\n\tieee80211_xmit(sdata, NULL, skb);\n\trcu_read_unlock();\n\n\treturn NETDEV_TX_OK;\n\nfail_rcu:\n\trcu_read_unlock();\nfail:\n\tdev_kfree_skb(skb);\n\treturn NETDEV_TX_OK; /* meaning, we dealt with the skb */\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static ieee80211_rx_result debug_noinline\nieee80211_rx_h_decrypt(struct ieee80211_rx_data *rx)\n{\n\tstruct sk_buff *skb = rx->skb;\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(skb);\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)skb->data;\n\tint keyidx;\n\tieee80211_rx_result result = RX_DROP_UNUSABLE;\n\tstruct ieee80211_key *sta_ptk = NULL;\n\tstruct ieee80211_key *ptk_idx = NULL;\n\tint mmie_keyidx = -1;\n\t__le16 fc;\n\n\tif (ieee80211_is_ext(hdr->frame_control))\n\t\treturn RX_CONTINUE;\n\n\t/*\n\t * Key selection 101\n\t *\n\t * There are five types of keys:\n\t *  - GTK (group keys)\n\t *  - IGTK (group keys for management frames)\n\t *  - BIGTK (group keys for Beacon frames)\n\t *  - PTK (pairwise keys)\n\t *  - STK (station-to-station pairwise keys)\n\t *\n\t * When selecting a key, we have to distinguish between multicast\n\t * (including broadcast) and unicast frames, the latter can only\n\t * use PTKs and STKs while the former always use GTKs, IGTKs, and\n\t * BIGTKs. Unless, of course, actual WEP keys (\"pre-RSNA\") are used,\n\t * then unicast frames can also use key indices like GTKs. Hence, if we\n\t * don't have a PTK/STK we check the key index for a WEP key.\n\t *\n\t * Note that in a regular BSS, multicast frames are sent by the\n\t * AP only, associated stations unicast the frame to the AP first\n\t * which then multicasts it on their behalf.\n\t *\n\t * There is also a slight problem in IBSS mode: GTKs are negotiated\n\t * with each station, that is something we don't currently handle.\n\t * The spec seems to expect that one negotiates the same key with\n\t * every station but there's no such requirement; VLANs could be\n\t * possible.\n\t */\n\n\t/* start without a key */\n\trx->key = NULL;\n\tfc = hdr->frame_control;\n\n\tif (rx->sta) {\n\t\tint keyid = rx->sta->ptk_idx;\n\t\tsta_ptk = rcu_dereference(rx->sta->ptk[keyid]);\n\n\t\tif (ieee80211_has_protected(fc) &&\n\t\t    !(status->flag & RX_FLAG_IV_STRIPPED)) {\n\t\t\tkeyid = ieee80211_get_keyid(rx->skb);\n\n\t\t\tif (unlikely(keyid < 0))\n\t\t\t\treturn RX_DROP_UNUSABLE;\n\n\t\t\tptk_idx = rcu_dereference(rx->sta->ptk[keyid]);\n\t\t}\n\t}\n\n\tif (!ieee80211_has_protected(fc))\n\t\tmmie_keyidx = ieee80211_get_mmie_keyidx(rx->skb);\n\n\tif (!is_multicast_ether_addr(hdr->addr1) && sta_ptk) {\n\t\trx->key = ptk_idx ? ptk_idx : sta_ptk;\n\t\tif ((status->flag & RX_FLAG_DECRYPTED) &&\n\t\t    (status->flag & RX_FLAG_IV_STRIPPED))\n\t\t\treturn RX_CONTINUE;\n\t\t/* Skip decryption if the frame is not protected. */\n\t\tif (!ieee80211_has_protected(fc))\n\t\t\treturn RX_CONTINUE;\n\t} else if (mmie_keyidx >= 0 && ieee80211_is_beacon(fc)) {\n\t\t/* Broadcast/multicast robust management frame / BIP */\n\t\tif ((status->flag & RX_FLAG_DECRYPTED) &&\n\t\t    (status->flag & RX_FLAG_IV_STRIPPED))\n\t\t\treturn RX_CONTINUE;\n\n\t\tif (mmie_keyidx < NUM_DEFAULT_KEYS + NUM_DEFAULT_MGMT_KEYS ||\n\t\t    mmie_keyidx >= NUM_DEFAULT_KEYS + NUM_DEFAULT_MGMT_KEYS +\n\t\t    NUM_DEFAULT_BEACON_KEYS) {\n\t\t\tcfg80211_rx_unprot_mlme_mgmt(rx->sdata->dev,\n\t\t\t\t\t\t     skb->data,\n\t\t\t\t\t\t     skb->len);\n\t\t\treturn RX_DROP_MONITOR; /* unexpected BIP keyidx */\n\t\t}\n\n\t\trx->key = ieee80211_rx_get_bigtk(rx, mmie_keyidx);\n\t\tif (!rx->key)\n\t\t\treturn RX_CONTINUE; /* Beacon protection not in use */\n\t} else if (mmie_keyidx >= 0) {\n\t\t/* Broadcast/multicast robust management frame / BIP */\n\t\tif ((status->flag & RX_FLAG_DECRYPTED) &&\n\t\t    (status->flag & RX_FLAG_IV_STRIPPED))\n\t\t\treturn RX_CONTINUE;\n\n\t\tif (mmie_keyidx < NUM_DEFAULT_KEYS ||\n\t\t    mmie_keyidx >= NUM_DEFAULT_KEYS + NUM_DEFAULT_MGMT_KEYS)\n\t\t\treturn RX_DROP_MONITOR; /* unexpected BIP keyidx */\n\t\tif (rx->link_sta) {\n\t\t\tif (ieee80211_is_group_privacy_action(skb) &&\n\t\t\t    test_sta_flag(rx->sta, WLAN_STA_MFP))\n\t\t\t\treturn RX_DROP_MONITOR;\n\n\t\t\trx->key = rcu_dereference(rx->link_sta->gtk[mmie_keyidx]);\n\t\t}\n\t\tif (!rx->key)\n\t\t\trx->key = rcu_dereference(rx->link->gtk[mmie_keyidx]);\n\t} else if (!ieee80211_has_protected(fc)) {\n\t\t/*\n\t\t * The frame was not protected, so skip decryption. However, we\n\t\t * need to set rx->key if there is a key that could have been\n\t\t * used so that the frame may be dropped if encryption would\n\t\t * have been expected.\n\t\t */\n\t\tstruct ieee80211_key *key = NULL;\n\t\tint i;\n\n\t\tif (ieee80211_is_beacon(fc)) {\n\t\t\tkey = ieee80211_rx_get_bigtk(rx, -1);\n\t\t} else if (ieee80211_is_mgmt(fc) &&\n\t\t\t   is_multicast_ether_addr(hdr->addr1)) {\n\t\t\tkey = rcu_dereference(rx->link->default_mgmt_key);\n\t\t} else {\n\t\t\tif (rx->link_sta) {\n\t\t\t\tfor (i = 0; i < NUM_DEFAULT_KEYS; i++) {\n\t\t\t\t\tkey = rcu_dereference(rx->link_sta->gtk[i]);\n\t\t\t\t\tif (key)\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (!key) {\n\t\t\t\tfor (i = 0; i < NUM_DEFAULT_KEYS; i++) {\n\t\t\t\t\tkey = rcu_dereference(rx->link->gtk[i]);\n\t\t\t\t\tif (key)\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif (key)\n\t\t\trx->key = key;\n\t\treturn RX_CONTINUE;\n\t} else {\n\t\t/*\n\t\t * The device doesn't give us the IV so we won't be\n\t\t * able to look up the key. That's ok though, we\n\t\t * don't need to decrypt the frame, we just won't\n\t\t * be able to keep statistics accurate.\n\t\t * Except for key threshold notifications, should\n\t\t * we somehow allow the driver to tell us which key\n\t\t * the hardware used if this flag is set?\n\t\t */\n\t\tif ((status->flag & RX_FLAG_DECRYPTED) &&\n\t\t    (status->flag & RX_FLAG_IV_STRIPPED))\n\t\t\treturn RX_CONTINUE;\n\n\t\tkeyidx = ieee80211_get_keyid(rx->skb);\n\n\t\tif (unlikely(keyidx < 0))\n\t\t\treturn RX_DROP_UNUSABLE;\n\n\t\t/* check per-station GTK first, if multicast packet */\n\t\tif (is_multicast_ether_addr(hdr->addr1) && rx->link_sta)\n\t\t\trx->key = rcu_dereference(rx->link_sta->gtk[keyidx]);\n\n\t\t/* if not found, try default key */\n\t\tif (!rx->key) {\n\t\t\tif (is_multicast_ether_addr(hdr->addr1))\n\t\t\t\trx->key = rcu_dereference(rx->link->gtk[keyidx]);\n\t\t\tif (!rx->key)\n\t\t\t\trx->key = rcu_dereference(rx->sdata->keys[keyidx]);\n\n\t\t\t/*\n\t\t\t * RSNA-protected unicast frames should always be\n\t\t\t * sent with pairwise or station-to-station keys,\n\t\t\t * but for WEP we allow using a key index as well.\n\t\t\t */\n\t\t\tif (rx->key &&\n\t\t\t    rx->key->conf.cipher != WLAN_CIPHER_SUITE_WEP40 &&\n\t\t\t    rx->key->conf.cipher != WLAN_CIPHER_SUITE_WEP104 &&\n\t\t\t    !is_multicast_ether_addr(hdr->addr1))\n\t\t\t\trx->key = NULL;\n\t\t}\n\t}\n\n\tif (rx->key) {\n\t\tif (unlikely(rx->key->flags & KEY_FLAG_TAINTED))\n\t\t\treturn RX_DROP_MONITOR;\n\n\t\t/* TODO: add threshold stuff again */\n\t} else {\n\t\treturn RX_DROP_MONITOR;\n\t}\n\n\tswitch (rx->key->conf.cipher) {\n\tcase WLAN_CIPHER_SUITE_WEP40:\n\tcase WLAN_CIPHER_SUITE_WEP104:\n\t\tresult = ieee80211_crypto_wep_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_TKIP:\n\t\tresult = ieee80211_crypto_tkip_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_CCMP:\n\t\tresult = ieee80211_crypto_ccmp_decrypt(\n\t\t\trx, IEEE80211_CCMP_MIC_LEN);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_CCMP_256:\n\t\tresult = ieee80211_crypto_ccmp_decrypt(\n\t\t\trx, IEEE80211_CCMP_256_MIC_LEN);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_AES_CMAC:\n\t\tresult = ieee80211_crypto_aes_cmac_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_BIP_CMAC_256:\n\t\tresult = ieee80211_crypto_aes_cmac_256_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_BIP_GMAC_128:\n\tcase WLAN_CIPHER_SUITE_BIP_GMAC_256:\n\t\tresult = ieee80211_crypto_aes_gmac_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_GCMP:\n\tcase WLAN_CIPHER_SUITE_GCMP_256:\n\t\tresult = ieee80211_crypto_gcmp_decrypt(rx);\n\t\tbreak;\n\tdefault:\n\t\tresult = RX_DROP_UNUSABLE;\n\t}\n\n\t/* the hdr variable is invalid after the decrypt handlers */\n\n\t/* either the frame has been decrypted or will be dropped */\n\tstatus->flag |= RX_FLAG_DECRYPTED;\n\n\tif (unlikely(ieee80211_is_beacon(fc) && result == RX_DROP_UNUSABLE))\n\t\tcfg80211_rx_unprot_mlme_mgmt(rx->sdata->dev,\n\t\t\t\t\t     skb->data, skb->len);\n\n\treturn result;\n}",
                        "code_after_change": "static ieee80211_rx_result debug_noinline\nieee80211_rx_h_decrypt(struct ieee80211_rx_data *rx)\n{\n\tstruct sk_buff *skb = rx->skb;\n\tstruct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(skb);\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)skb->data;\n\tint keyidx;\n\tieee80211_rx_result result = RX_DROP_UNUSABLE;\n\tstruct ieee80211_key *sta_ptk = NULL;\n\tstruct ieee80211_key *ptk_idx = NULL;\n\tint mmie_keyidx = -1;\n\t__le16 fc;\n\n\tif (ieee80211_is_ext(hdr->frame_control))\n\t\treturn RX_CONTINUE;\n\n\t/*\n\t * Key selection 101\n\t *\n\t * There are five types of keys:\n\t *  - GTK (group keys)\n\t *  - IGTK (group keys for management frames)\n\t *  - BIGTK (group keys for Beacon frames)\n\t *  - PTK (pairwise keys)\n\t *  - STK (station-to-station pairwise keys)\n\t *\n\t * When selecting a key, we have to distinguish between multicast\n\t * (including broadcast) and unicast frames, the latter can only\n\t * use PTKs and STKs while the former always use GTKs, IGTKs, and\n\t * BIGTKs. Unless, of course, actual WEP keys (\"pre-RSNA\") are used,\n\t * then unicast frames can also use key indices like GTKs. Hence, if we\n\t * don't have a PTK/STK we check the key index for a WEP key.\n\t *\n\t * Note that in a regular BSS, multicast frames are sent by the\n\t * AP only, associated stations unicast the frame to the AP first\n\t * which then multicasts it on their behalf.\n\t *\n\t * There is also a slight problem in IBSS mode: GTKs are negotiated\n\t * with each station, that is something we don't currently handle.\n\t * The spec seems to expect that one negotiates the same key with\n\t * every station but there's no such requirement; VLANs could be\n\t * possible.\n\t */\n\n\t/* start without a key */\n\trx->key = NULL;\n\tfc = hdr->frame_control;\n\n\tif (rx->sta) {\n\t\tint keyid = rx->sta->ptk_idx;\n\t\tsta_ptk = rcu_dereference(rx->sta->ptk[keyid]);\n\n\t\tif (ieee80211_has_protected(fc) &&\n\t\t    !(status->flag & RX_FLAG_IV_STRIPPED)) {\n\t\t\tkeyid = ieee80211_get_keyid(rx->skb);\n\n\t\t\tif (unlikely(keyid < 0))\n\t\t\t\treturn RX_DROP_UNUSABLE;\n\n\t\t\tptk_idx = rcu_dereference(rx->sta->ptk[keyid]);\n\t\t}\n\t}\n\n\tif (!ieee80211_has_protected(fc))\n\t\tmmie_keyidx = ieee80211_get_mmie_keyidx(rx->skb);\n\n\tif (!is_multicast_ether_addr(hdr->addr1) && sta_ptk) {\n\t\trx->key = ptk_idx ? ptk_idx : sta_ptk;\n\t\tif ((status->flag & RX_FLAG_DECRYPTED) &&\n\t\t    (status->flag & RX_FLAG_IV_STRIPPED))\n\t\t\treturn RX_CONTINUE;\n\t\t/* Skip decryption if the frame is not protected. */\n\t\tif (!ieee80211_has_protected(fc))\n\t\t\treturn RX_CONTINUE;\n\t} else if (mmie_keyidx >= 0 && ieee80211_is_beacon(fc)) {\n\t\t/* Broadcast/multicast robust management frame / BIP */\n\t\tif ((status->flag & RX_FLAG_DECRYPTED) &&\n\t\t    (status->flag & RX_FLAG_IV_STRIPPED))\n\t\t\treturn RX_CONTINUE;\n\n\t\tif (mmie_keyidx < NUM_DEFAULT_KEYS + NUM_DEFAULT_MGMT_KEYS ||\n\t\t    mmie_keyidx >= NUM_DEFAULT_KEYS + NUM_DEFAULT_MGMT_KEYS +\n\t\t\t\t   NUM_DEFAULT_BEACON_KEYS) {\n\t\t\tif (rx->sdata->dev)\n\t\t\t\tcfg80211_rx_unprot_mlme_mgmt(rx->sdata->dev,\n\t\t\t\t\t\t\t     skb->data,\n\t\t\t\t\t\t\t     skb->len);\n\t\t\treturn RX_DROP_MONITOR; /* unexpected BIP keyidx */\n\t\t}\n\n\t\trx->key = ieee80211_rx_get_bigtk(rx, mmie_keyidx);\n\t\tif (!rx->key)\n\t\t\treturn RX_CONTINUE; /* Beacon protection not in use */\n\t} else if (mmie_keyidx >= 0) {\n\t\t/* Broadcast/multicast robust management frame / BIP */\n\t\tif ((status->flag & RX_FLAG_DECRYPTED) &&\n\t\t    (status->flag & RX_FLAG_IV_STRIPPED))\n\t\t\treturn RX_CONTINUE;\n\n\t\tif (mmie_keyidx < NUM_DEFAULT_KEYS ||\n\t\t    mmie_keyidx >= NUM_DEFAULT_KEYS + NUM_DEFAULT_MGMT_KEYS)\n\t\t\treturn RX_DROP_MONITOR; /* unexpected BIP keyidx */\n\t\tif (rx->link_sta) {\n\t\t\tif (ieee80211_is_group_privacy_action(skb) &&\n\t\t\t    test_sta_flag(rx->sta, WLAN_STA_MFP))\n\t\t\t\treturn RX_DROP_MONITOR;\n\n\t\t\trx->key = rcu_dereference(rx->link_sta->gtk[mmie_keyidx]);\n\t\t}\n\t\tif (!rx->key)\n\t\t\trx->key = rcu_dereference(rx->link->gtk[mmie_keyidx]);\n\t} else if (!ieee80211_has_protected(fc)) {\n\t\t/*\n\t\t * The frame was not protected, so skip decryption. However, we\n\t\t * need to set rx->key if there is a key that could have been\n\t\t * used so that the frame may be dropped if encryption would\n\t\t * have been expected.\n\t\t */\n\t\tstruct ieee80211_key *key = NULL;\n\t\tint i;\n\n\t\tif (ieee80211_is_beacon(fc)) {\n\t\t\tkey = ieee80211_rx_get_bigtk(rx, -1);\n\t\t} else if (ieee80211_is_mgmt(fc) &&\n\t\t\t   is_multicast_ether_addr(hdr->addr1)) {\n\t\t\tkey = rcu_dereference(rx->link->default_mgmt_key);\n\t\t} else {\n\t\t\tif (rx->link_sta) {\n\t\t\t\tfor (i = 0; i < NUM_DEFAULT_KEYS; i++) {\n\t\t\t\t\tkey = rcu_dereference(rx->link_sta->gtk[i]);\n\t\t\t\t\tif (key)\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (!key) {\n\t\t\t\tfor (i = 0; i < NUM_DEFAULT_KEYS; i++) {\n\t\t\t\t\tkey = rcu_dereference(rx->link->gtk[i]);\n\t\t\t\t\tif (key)\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif (key)\n\t\t\trx->key = key;\n\t\treturn RX_CONTINUE;\n\t} else {\n\t\t/*\n\t\t * The device doesn't give us the IV so we won't be\n\t\t * able to look up the key. That's ok though, we\n\t\t * don't need to decrypt the frame, we just won't\n\t\t * be able to keep statistics accurate.\n\t\t * Except for key threshold notifications, should\n\t\t * we somehow allow the driver to tell us which key\n\t\t * the hardware used if this flag is set?\n\t\t */\n\t\tif ((status->flag & RX_FLAG_DECRYPTED) &&\n\t\t    (status->flag & RX_FLAG_IV_STRIPPED))\n\t\t\treturn RX_CONTINUE;\n\n\t\tkeyidx = ieee80211_get_keyid(rx->skb);\n\n\t\tif (unlikely(keyidx < 0))\n\t\t\treturn RX_DROP_UNUSABLE;\n\n\t\t/* check per-station GTK first, if multicast packet */\n\t\tif (is_multicast_ether_addr(hdr->addr1) && rx->link_sta)\n\t\t\trx->key = rcu_dereference(rx->link_sta->gtk[keyidx]);\n\n\t\t/* if not found, try default key */\n\t\tif (!rx->key) {\n\t\t\tif (is_multicast_ether_addr(hdr->addr1))\n\t\t\t\trx->key = rcu_dereference(rx->link->gtk[keyidx]);\n\t\t\tif (!rx->key)\n\t\t\t\trx->key = rcu_dereference(rx->sdata->keys[keyidx]);\n\n\t\t\t/*\n\t\t\t * RSNA-protected unicast frames should always be\n\t\t\t * sent with pairwise or station-to-station keys,\n\t\t\t * but for WEP we allow using a key index as well.\n\t\t\t */\n\t\t\tif (rx->key &&\n\t\t\t    rx->key->conf.cipher != WLAN_CIPHER_SUITE_WEP40 &&\n\t\t\t    rx->key->conf.cipher != WLAN_CIPHER_SUITE_WEP104 &&\n\t\t\t    !is_multicast_ether_addr(hdr->addr1))\n\t\t\t\trx->key = NULL;\n\t\t}\n\t}\n\n\tif (rx->key) {\n\t\tif (unlikely(rx->key->flags & KEY_FLAG_TAINTED))\n\t\t\treturn RX_DROP_MONITOR;\n\n\t\t/* TODO: add threshold stuff again */\n\t} else {\n\t\treturn RX_DROP_MONITOR;\n\t}\n\n\tswitch (rx->key->conf.cipher) {\n\tcase WLAN_CIPHER_SUITE_WEP40:\n\tcase WLAN_CIPHER_SUITE_WEP104:\n\t\tresult = ieee80211_crypto_wep_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_TKIP:\n\t\tresult = ieee80211_crypto_tkip_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_CCMP:\n\t\tresult = ieee80211_crypto_ccmp_decrypt(\n\t\t\trx, IEEE80211_CCMP_MIC_LEN);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_CCMP_256:\n\t\tresult = ieee80211_crypto_ccmp_decrypt(\n\t\t\trx, IEEE80211_CCMP_256_MIC_LEN);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_AES_CMAC:\n\t\tresult = ieee80211_crypto_aes_cmac_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_BIP_CMAC_256:\n\t\tresult = ieee80211_crypto_aes_cmac_256_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_BIP_GMAC_128:\n\tcase WLAN_CIPHER_SUITE_BIP_GMAC_256:\n\t\tresult = ieee80211_crypto_aes_gmac_decrypt(rx);\n\t\tbreak;\n\tcase WLAN_CIPHER_SUITE_GCMP:\n\tcase WLAN_CIPHER_SUITE_GCMP_256:\n\t\tresult = ieee80211_crypto_gcmp_decrypt(rx);\n\t\tbreak;\n\tdefault:\n\t\tresult = RX_DROP_UNUSABLE;\n\t}\n\n\t/* the hdr variable is invalid after the decrypt handlers */\n\n\t/* either the frame has been decrypted or will be dropped */\n\tstatus->flag |= RX_FLAG_DECRYPTED;\n\n\tif (unlikely(ieee80211_is_beacon(fc) && result == RX_DROP_UNUSABLE &&\n\t\t     rx->sdata->dev))\n\t\tcfg80211_rx_unprot_mlme_mgmt(rx->sdata->dev,\n\t\t\t\t\t     skb->data, skb->len);\n\n\treturn result;\n}",
                        "cve_id": "CVE-2022-42722"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3262,
            "cve_id": "CVE-2021-4095",
            "code_snippet": "static int kvm_xen_shared_info_init(struct kvm *kvm, gfn_t gfn)\n{\n\tstruct gfn_to_pfn_cache *gpc = &kvm->arch.xen.shinfo_cache;\n\tstruct pvclock_wall_clock *wc;\n\tgpa_t gpa = gfn_to_gpa(gfn);\n\tu32 *wc_sec_hi;\n\tu32 wc_version;\n\tu64 wall_nsec;\n\tint ret = 0;\n\tint idx = srcu_read_lock(&kvm->srcu);\n\n\tif (gfn == GPA_INVALID) {\n\t\tkvm_gfn_to_pfn_cache_destroy(kvm, gpc);\n\t\tgoto out;\n\t}\n\n\tdo {\n\t\tret = kvm_gfn_to_pfn_cache_init(kvm, gpc, NULL, false, true,\n\t\t\t\t\t\tgpa, PAGE_SIZE, false);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * This code mirrors kvm_write_wall_clock() except that it writes\n\t\t * directly through the pfn cache and doesn't mark the page dirty.\n\t\t */\n\t\twall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);\n\n\t\t/* It could be invalid again already, so we need to check */\n\t\tread_lock_irq(&gpc->lock);\n\n\t\tif (gpc->valid)\n\t\t\tbreak;\n\n\t\tread_unlock_irq(&gpc->lock);\n\t} while (1);\n\n\t/* Paranoia checks on the 32-bit struct layout */\n\tBUILD_BUG_ON(offsetof(struct compat_shared_info, wc) != 0x900);\n\tBUILD_BUG_ON(offsetof(struct compat_shared_info, arch.wc_sec_hi) != 0x924);\n\tBUILD_BUG_ON(offsetof(struct pvclock_vcpu_time_info, version) != 0);\n\n#ifdef CONFIG_X86_64\n\t/* Paranoia checks on the 64-bit struct layout */\n\tBUILD_BUG_ON(offsetof(struct shared_info, wc) != 0xc00);\n\tBUILD_BUG_ON(offsetof(struct shared_info, wc_sec_hi) != 0xc0c);\n\n\tif (IS_ENABLED(CONFIG_64BIT) && kvm->arch.xen.long_mode) {\n\t\tstruct shared_info *shinfo = gpc->khva;\n\n\t\twc_sec_hi = &shinfo->wc_sec_hi;\n\t\twc = &shinfo->wc;\n\t} else\n#endif\n\t{\n\t\tstruct compat_shared_info *shinfo = gpc->khva;\n\n\t\twc_sec_hi = &shinfo->arch.wc_sec_hi;\n\t\twc = &shinfo->wc;\n\t}\n\n\t/* Increment and ensure an odd value */\n\twc_version = wc->version = (wc->version + 1) | 1;\n\tsmp_wmb();\n\n\twc->nsec = do_div(wall_nsec,  1000000000);\n\twc->sec = (u32)wall_nsec;\n\t*wc_sec_hi = wall_nsec >> 32;\n\tsmp_wmb();\n\n\twc->version = wc_version + 1;\n\tread_unlock_irq(&gpc->lock);\n\n\tkvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);\n\nout:\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_ofs)\n{\n\tint version;\n\tint r;\n\tstruct pvclock_wall_clock wc;\n\tu32 wc_sec_hi;\n\tu64 wall_nsec;\n\n\tif (!wall_clock)\n\t\treturn;\n\n\tr = kvm_read_guest(kvm, wall_clock, &version, sizeof(version));\n\tif (r)\n\t\treturn;\n\n\tif (version & 1)\n\t\t++version;  /* first time write, random junk */\n\n\t++version;\n\n\tif (kvm_write_guest(kvm, wall_clock, &version, sizeof(version)))\n\t\treturn;\n\n\t/*\n\t * The guest calculates current wall clock time by adding\n\t * system time (updated by kvm_guest_time_update below) to the\n\t * wall clock specified here.  We do the reverse here.\n\t */\n\twall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);\n\n\twc.nsec = do_div(wall_nsec, 1000000000);\n\twc.sec = (u32)wall_nsec; /* overflow in 2106 guest time */\n\twc.version = version;\n\n\tkvm_write_guest(kvm, wall_clock, &wc, sizeof(wc));\n\n\tif (sec_hi_ofs) {\n\t\twc_sec_hi = wall_nsec >> 32;\n\t\tkvm_write_guest(kvm, wall_clock + sec_hi_ofs,\n\t\t\t\t&wc_sec_hi, sizeof(wc_sec_hi));\n\t}\n\n\tversion++;\n\tkvm_write_guest(kvm, wall_clock, &version, sizeof(version));\n}",
                        "code_after_change": "static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_ofs)\n{\n\tint version;\n\tint r;\n\tstruct pvclock_wall_clock wc;\n\tu32 wc_sec_hi;\n\tu64 wall_nsec;\n\n\tif (!wall_clock)\n\t\treturn;\n\n\tr = kvm_read_guest(kvm, wall_clock, &version, sizeof(version));\n\tif (r)\n\t\treturn;\n\n\tif (version & 1)\n\t\t++version;  /* first time write, random junk */\n\n\t++version;\n\n\tif (kvm_write_guest(kvm, wall_clock, &version, sizeof(version)))\n\t\treturn;\n\n\t/*\n\t * The guest calculates current wall clock time by adding\n\t * system time (updated by kvm_guest_time_update below) to the\n\t * wall clock specified here.  We do the reverse here.\n\t */\n\twall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);\n\n\twc.nsec = do_div(wall_nsec, 1000000000);\n\twc.sec = (u32)wall_nsec; /* overflow in 2106 guest time */\n\twc.version = version;\n\n\tkvm_write_guest(kvm, wall_clock, &wc, sizeof(wc));\n\n\tif (sec_hi_ofs) {\n\t\twc_sec_hi = wall_nsec >> 32;\n\t\tkvm_write_guest(kvm, wall_clock + sec_hi_ofs,\n\t\t\t\t&wc_sec_hi, sizeof(wc_sec_hi));\n\t}\n\n\tversion++;\n\tkvm_write_guest(kvm, wall_clock, &version, sizeof(version));\n}",
                        "cve_id": "CVE-2021-4095"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3389,
            "cve_id": "CVE-2022-1205",
            "code_snippet": "void ax25_disconnect(ax25_cb *ax25, int reason)\n{\n\tax25_clear_queues(ax25);\n\n\tif (reason == ENETUNREACH) {\n\t\tdel_timer_sync(&ax25->timer);\n\t\tdel_timer_sync(&ax25->t1timer);\n\t\tdel_timer_sync(&ax25->t2timer);\n\t\tdel_timer_sync(&ax25->t3timer);\n\t\tdel_timer_sync(&ax25->idletimer);\n\t} else {\n\t\tif (!ax25->sk || !sock_flag(ax25->sk, SOCK_DESTROY))\n\t\t\tax25_stop_heartbeat(ax25);\n\t\tax25_stop_t1timer(ax25);\n\t\tax25_stop_t2timer(ax25);\n\t\tax25_stop_t3timer(ax25);\n\t\tax25_stop_idletimer(ax25);\n\t}\n\n\tax25->state = AX25_STATE_0;\n\n\tax25_link_failed(ax25, reason);\n\n\tif (ax25->sk != NULL) {\n\t\tlocal_bh_disable();\n\t\tbh_lock_sock(ax25->sk);\n\t\tax25->sk->sk_state     = TCP_CLOSE;\n\t\tax25->sk->sk_err       = reason;\n\t\tax25->sk->sk_shutdown |= SEND_SHUTDOWN;\n\t\tif (!sock_flag(ax25->sk, SOCK_DEAD)) {\n\t\t\tax25->sk->sk_state_change(ax25->sk);\n\t\t\tsock_set_flag(ax25->sk, SOCK_DEAD);\n\t\t}\n\t\tbh_unlock_sock(ax25->sk);\n\t\tlocal_bh_enable();\n\t}\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static void ax25_kill_by_device(struct net_device *dev)\n{\n\tax25_dev *ax25_dev;\n\tax25_cb *s;\n\tstruct sock *sk;\n\n\tif ((ax25_dev = ax25_dev_ax25dev(dev)) == NULL)\n\t\treturn;\n\n\tspin_lock_bh(&ax25_list_lock);\nagain:\n\tax25_for_each(s, &ax25_list) {\n\t\tif (s->ax25_dev == ax25_dev) {\n\t\t\tsk = s->sk;\n\t\t\tif (!sk) {\n\t\t\t\tspin_unlock_bh(&ax25_list_lock);\n\t\t\t\ts->ax25_dev = NULL;\n\t\t\t\tax25_disconnect(s, ENETUNREACH);\n\t\t\t\tspin_lock_bh(&ax25_list_lock);\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t\tsock_hold(sk);\n\t\t\tspin_unlock_bh(&ax25_list_lock);\n\t\t\tlock_sock(sk);\n\t\t\ts->ax25_dev = NULL;\n\t\t\tif (sk->sk_socket) {\n\t\t\t\tdev_put_track(ax25_dev->dev, &ax25_dev->dev_tracker);\n\t\t\t\tax25_dev_put(ax25_dev);\n\t\t\t}\n\t\t\tax25_disconnect(s, ENETUNREACH);\n\t\t\trelease_sock(sk);\n\t\t\tspin_lock_bh(&ax25_list_lock);\n\t\t\tsock_put(sk);\n\t\t\t/* The entry could have been deleted from the\n\t\t\t * list meanwhile and thus the next pointer is\n\t\t\t * no longer valid.  Play it safe and restart\n\t\t\t * the scan.  Forward progress is ensured\n\t\t\t * because we set s->ax25_dev to NULL and we\n\t\t\t * are never passed a NULL 'dev' argument.\n\t\t\t */\n\t\t\tgoto again;\n\t\t}\n\t}\n\tspin_unlock_bh(&ax25_list_lock);\n}",
                        "code_after_change": "static void ax25_kill_by_device(struct net_device *dev)\n{\n\tax25_dev *ax25_dev;\n\tax25_cb *s;\n\tstruct sock *sk;\n\n\tif ((ax25_dev = ax25_dev_ax25dev(dev)) == NULL)\n\t\treturn;\n\n\tspin_lock_bh(&ax25_list_lock);\nagain:\n\tax25_for_each(s, &ax25_list) {\n\t\tif (s->ax25_dev == ax25_dev) {\n\t\t\tsk = s->sk;\n\t\t\tif (!sk) {\n\t\t\t\tspin_unlock_bh(&ax25_list_lock);\n\t\t\t\tax25_disconnect(s, ENETUNREACH);\n\t\t\t\ts->ax25_dev = NULL;\n\t\t\t\tspin_lock_bh(&ax25_list_lock);\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t\tsock_hold(sk);\n\t\t\tspin_unlock_bh(&ax25_list_lock);\n\t\t\tlock_sock(sk);\n\t\t\tax25_disconnect(s, ENETUNREACH);\n\t\t\ts->ax25_dev = NULL;\n\t\t\tif (sk->sk_socket) {\n\t\t\t\tdev_put_track(ax25_dev->dev, &ax25_dev->dev_tracker);\n\t\t\t\tax25_dev_put(ax25_dev);\n\t\t\t}\n\t\t\trelease_sock(sk);\n\t\t\tspin_lock_bh(&ax25_list_lock);\n\t\t\tsock_put(sk);\n\t\t\t/* The entry could have been deleted from the\n\t\t\t * list meanwhile and thus the next pointer is\n\t\t\t * no longer valid.  Play it safe and restart\n\t\t\t * the scan.  Forward progress is ensured\n\t\t\t * because we set s->ax25_dev to NULL and we\n\t\t\t * are never passed a NULL 'dev' argument.\n\t\t\t */\n\t\t\tgoto again;\n\t\t}\n\t}\n\tspin_unlock_bh(&ax25_list_lock);\n}",
                        "cve_id": "CVE-2022-1205"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3405,
            "cve_id": "CVE-2022-1671",
            "code_snippet": "static int rxrpc_preparse_s(struct key_preparsed_payload *prep)\n{\n\tconst struct rxrpc_security *sec;\n\tunsigned int service, sec_class;\n\tint n;\n\n\t_enter(\"%zu\", prep->datalen);\n\n\tif (!prep->orig_description)\n\t\treturn -EINVAL;\n\n\tif (sscanf(prep->orig_description, \"%u:%u%n\", &service, &sec_class, &n) != 2)\n\t\treturn -EINVAL;\n\n\tsec = rxrpc_security_lookup(sec_class);\n\tif (!sec)\n\t\treturn -ENOPKG;\n\n\tprep->payload.data[1] = (struct rxrpc_security *)sec;\n\n\tif (!sec->preparse_server_key)\n\t\treturn -EINVAL;\n\n\treturn sec->preparse_server_key(prep);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static void rxrpc_free_preparse_s(struct key_preparsed_payload *prep)\n{\n\tconst struct rxrpc_security *sec = prep->payload.data[1];\n\n\tif (sec)\n\t\tsec->free_preparse_server_key(prep);\n}",
                        "code_after_change": "static void rxrpc_free_preparse_s(struct key_preparsed_payload *prep)\n{\n\tconst struct rxrpc_security *sec = prep->payload.data[1];\n\n\tif (sec && sec->free_preparse_server_key)\n\t\tsec->free_preparse_server_key(prep);\n}",
                        "cve_id": "CVE-2022-1671"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3428,
            "cve_id": "CVE-2022-1852",
            "code_snippet": "int x86_decode_emulated_instruction(struct kvm_vcpu *vcpu, int emulation_type,\n\t\t\t\t    void *insn, int insn_len)\n{\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tint r;\n\n\tinit_emulate_ctxt(vcpu);\n\n\tr = x86_decode_insn(ctxt, insn, insn_len, emulation_type);\n\n\ttrace_kvm_emulate_insn_start(vcpu);\n\t++vcpu->stat.insn_emulation;\n\n\treturn r;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,\n\t\t\t    int emulation_type, void *insn, int insn_len)\n{\n\tint r;\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tbool writeback = true;\n\tbool write_fault_to_spt;\n\n\tif (unlikely(!kvm_can_emulate_insn(vcpu, emulation_type, insn, insn_len)))\n\t\treturn 1;\n\n\tvcpu->arch.l1tf_flush_l1d = true;\n\n\t/*\n\t * Clear write_fault_to_shadow_pgtable here to ensure it is\n\t * never reused.\n\t */\n\twrite_fault_to_spt = vcpu->arch.write_fault_to_shadow_pgtable;\n\tvcpu->arch.write_fault_to_shadow_pgtable = false;\n\n\tif (!(emulation_type & EMULTYPE_NO_DECODE)) {\n\t\tkvm_clear_exception_queue(vcpu);\n\n\t\tr = x86_decode_emulated_instruction(vcpu, emulation_type,\n\t\t\t\t\t\t    insn, insn_len);\n\t\tif (r != EMULATION_OK)  {\n\t\t\tif ((emulation_type & EMULTYPE_TRAP_UD) ||\n\t\t\t    (emulation_type & EMULTYPE_TRAP_UD_FORCED)) {\n\t\t\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\tif (reexecute_instruction(vcpu, cr2_or_gpa,\n\t\t\t\t\t\t  write_fault_to_spt,\n\t\t\t\t\t\t  emulation_type))\n\t\t\t\treturn 1;\n\t\t\tif (ctxt->have_exception) {\n\t\t\t\t/*\n\t\t\t\t * #UD should result in just EMULATION_FAILED, and trap-like\n\t\t\t\t * exception should not be encountered during decode.\n\t\t\t\t */\n\t\t\t\tWARN_ON_ONCE(ctxt->exception.vector == UD_VECTOR ||\n\t\t\t\t\t     exception_type(ctxt->exception.vector) == EXCPT_TRAP);\n\t\t\t\tinject_emulated_exception(vcpu);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t\t}\n\t}\n\n\tif ((emulation_type & EMULTYPE_VMWARE_GP) &&\n\t    !is_vmware_backdoor_opcode(ctxt)) {\n\t\tkvm_queue_exception_e(vcpu, GP_VECTOR, 0);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * EMULTYPE_SKIP without EMULTYPE_COMPLETE_USER_EXIT is intended for\n\t * use *only* by vendor callbacks for kvm_skip_emulated_instruction().\n\t * The caller is responsible for updating interruptibility state and\n\t * injecting single-step #DBs.\n\t */\n\tif (emulation_type & EMULTYPE_SKIP) {\n\t\tif (ctxt->mode != X86EMUL_MODE_PROT64)\n\t\t\tctxt->eip = (u32)ctxt->_eip;\n\t\telse\n\t\t\tctxt->eip = ctxt->_eip;\n\n\t\tif (emulation_type & EMULTYPE_COMPLETE_USER_EXIT) {\n\t\t\tr = 1;\n\t\t\tgoto writeback;\n\t\t}\n\n\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\tif (ctxt->eflags & X86_EFLAGS_RF)\n\t\t\tkvm_set_rflags(vcpu, ctxt->eflags & ~X86_EFLAGS_RF);\n\t\treturn 1;\n\t}\n\n\tif (retry_instruction(ctxt, cr2_or_gpa, emulation_type))\n\t\treturn 1;\n\n\t/* this is needed for vmware backdoor interface to work since it\n\t   changes registers values  during IO operation */\n\tif (vcpu->arch.emulate_regs_need_sync_from_vcpu) {\n\t\tvcpu->arch.emulate_regs_need_sync_from_vcpu = false;\n\t\temulator_invalidate_register_cache(ctxt);\n\t}\n\nrestart:\n\tif (emulation_type & EMULTYPE_PF) {\n\t\t/* Save the faulting GPA (cr2) in the address field */\n\t\tctxt->exception.address = cr2_or_gpa;\n\n\t\t/* With shadow page tables, cr2 contains a GVA or nGPA. */\n\t\tif (vcpu->arch.mmu->root_role.direct) {\n\t\t\tctxt->gpa_available = true;\n\t\t\tctxt->gpa_val = cr2_or_gpa;\n\t\t}\n\t} else {\n\t\t/* Sanitize the address out of an abundance of paranoia. */\n\t\tctxt->exception.address = 0;\n\t}\n\n\tr = x86_emulate_insn(ctxt);\n\n\tif (r == EMULATION_INTERCEPTED)\n\t\treturn 1;\n\n\tif (r == EMULATION_FAILED) {\n\t\tif (reexecute_instruction(vcpu, cr2_or_gpa, write_fault_to_spt,\n\t\t\t\t\temulation_type))\n\t\t\treturn 1;\n\n\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t}\n\n\tif (ctxt->have_exception) {\n\t\tr = 1;\n\t\tif (inject_emulated_exception(vcpu))\n\t\t\treturn r;\n\t} else if (vcpu->arch.pio.count) {\n\t\tif (!vcpu->arch.pio.in) {\n\t\t\t/* FIXME: return into emulator if single-stepping.  */\n\t\t\tvcpu->arch.pio.count = 0;\n\t\t} else {\n\t\t\twriteback = false;\n\t\t\tvcpu->arch.complete_userspace_io = complete_emulated_pio;\n\t\t}\n\t\tr = 0;\n\t} else if (vcpu->mmio_needed) {\n\t\t++vcpu->stat.mmio_exits;\n\n\t\tif (!vcpu->mmio_is_write)\n\t\t\twriteback = false;\n\t\tr = 0;\n\t\tvcpu->arch.complete_userspace_io = complete_emulated_mmio;\n\t} else if (vcpu->arch.complete_userspace_io) {\n\t\twriteback = false;\n\t\tr = 0;\n\t} else if (r == EMULATION_RESTART)\n\t\tgoto restart;\n\telse\n\t\tr = 1;\n\nwriteback:\n\tif (writeback) {\n\t\tunsigned long rflags = static_call(kvm_x86_get_rflags)(vcpu);\n\t\ttoggle_interruptibility(vcpu, ctxt->interruptibility);\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = false;\n\t\tif (!ctxt->have_exception ||\n\t\t    exception_type(ctxt->exception.vector) == EXCPT_TRAP) {\n\t\t\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);\n\t\t\tif (ctxt->is_branch)\n\t\t\t\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);\n\t\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\t\tif (r && (ctxt->tf || (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)))\n\t\t\t\tr = kvm_vcpu_do_singlestep(vcpu);\n\t\t\tstatic_call_cond(kvm_x86_update_emulated_instruction)(vcpu);\n\t\t\t__kvm_set_rflags(vcpu, ctxt->eflags);\n\t\t}\n\n\t\t/*\n\t\t * For STI, interrupts are shadowed; so KVM_REQ_EVENT will\n\t\t * do nothing, and it will be requested again as soon as\n\t\t * the shadow expires.  But we still need to check here,\n\t\t * because POPF has no interrupt shadow.\n\t\t */\n\t\tif (unlikely((ctxt->eflags & ~rflags) & X86_EFLAGS_IF))\n\t\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t} else\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = true;\n\n\treturn r;\n}",
                        "code_after_change": "int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,\n\t\t\t    int emulation_type, void *insn, int insn_len)\n{\n\tint r;\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tbool writeback = true;\n\tbool write_fault_to_spt;\n\n\tif (unlikely(!kvm_can_emulate_insn(vcpu, emulation_type, insn, insn_len)))\n\t\treturn 1;\n\n\tvcpu->arch.l1tf_flush_l1d = true;\n\n\t/*\n\t * Clear write_fault_to_shadow_pgtable here to ensure it is\n\t * never reused.\n\t */\n\twrite_fault_to_spt = vcpu->arch.write_fault_to_shadow_pgtable;\n\tvcpu->arch.write_fault_to_shadow_pgtable = false;\n\n\tif (!(emulation_type & EMULTYPE_NO_DECODE)) {\n\t\tkvm_clear_exception_queue(vcpu);\n\n\t\t/*\n\t\t * Return immediately if RIP hits a code breakpoint, such #DBs\n\t\t * are fault-like and are higher priority than any faults on\n\t\t * the code fetch itself.\n\t\t */\n\t\tif (!(emulation_type & EMULTYPE_SKIP) &&\n\t\t    kvm_vcpu_check_code_breakpoint(vcpu, &r))\n\t\t\treturn r;\n\n\t\tr = x86_decode_emulated_instruction(vcpu, emulation_type,\n\t\t\t\t\t\t    insn, insn_len);\n\t\tif (r != EMULATION_OK)  {\n\t\t\tif ((emulation_type & EMULTYPE_TRAP_UD) ||\n\t\t\t    (emulation_type & EMULTYPE_TRAP_UD_FORCED)) {\n\t\t\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\tif (reexecute_instruction(vcpu, cr2_or_gpa,\n\t\t\t\t\t\t  write_fault_to_spt,\n\t\t\t\t\t\t  emulation_type))\n\t\t\t\treturn 1;\n\t\t\tif (ctxt->have_exception) {\n\t\t\t\t/*\n\t\t\t\t * #UD should result in just EMULATION_FAILED, and trap-like\n\t\t\t\t * exception should not be encountered during decode.\n\t\t\t\t */\n\t\t\t\tWARN_ON_ONCE(ctxt->exception.vector == UD_VECTOR ||\n\t\t\t\t\t     exception_type(ctxt->exception.vector) == EXCPT_TRAP);\n\t\t\t\tinject_emulated_exception(vcpu);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t\t}\n\t}\n\n\tif ((emulation_type & EMULTYPE_VMWARE_GP) &&\n\t    !is_vmware_backdoor_opcode(ctxt)) {\n\t\tkvm_queue_exception_e(vcpu, GP_VECTOR, 0);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * EMULTYPE_SKIP without EMULTYPE_COMPLETE_USER_EXIT is intended for\n\t * use *only* by vendor callbacks for kvm_skip_emulated_instruction().\n\t * The caller is responsible for updating interruptibility state and\n\t * injecting single-step #DBs.\n\t */\n\tif (emulation_type & EMULTYPE_SKIP) {\n\t\tif (ctxt->mode != X86EMUL_MODE_PROT64)\n\t\t\tctxt->eip = (u32)ctxt->_eip;\n\t\telse\n\t\t\tctxt->eip = ctxt->_eip;\n\n\t\tif (emulation_type & EMULTYPE_COMPLETE_USER_EXIT) {\n\t\t\tr = 1;\n\t\t\tgoto writeback;\n\t\t}\n\n\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\tif (ctxt->eflags & X86_EFLAGS_RF)\n\t\t\tkvm_set_rflags(vcpu, ctxt->eflags & ~X86_EFLAGS_RF);\n\t\treturn 1;\n\t}\n\n\tif (retry_instruction(ctxt, cr2_or_gpa, emulation_type))\n\t\treturn 1;\n\n\t/* this is needed for vmware backdoor interface to work since it\n\t   changes registers values  during IO operation */\n\tif (vcpu->arch.emulate_regs_need_sync_from_vcpu) {\n\t\tvcpu->arch.emulate_regs_need_sync_from_vcpu = false;\n\t\temulator_invalidate_register_cache(ctxt);\n\t}\n\nrestart:\n\tif (emulation_type & EMULTYPE_PF) {\n\t\t/* Save the faulting GPA (cr2) in the address field */\n\t\tctxt->exception.address = cr2_or_gpa;\n\n\t\t/* With shadow page tables, cr2 contains a GVA or nGPA. */\n\t\tif (vcpu->arch.mmu->root_role.direct) {\n\t\t\tctxt->gpa_available = true;\n\t\t\tctxt->gpa_val = cr2_or_gpa;\n\t\t}\n\t} else {\n\t\t/* Sanitize the address out of an abundance of paranoia. */\n\t\tctxt->exception.address = 0;\n\t}\n\n\tr = x86_emulate_insn(ctxt);\n\n\tif (r == EMULATION_INTERCEPTED)\n\t\treturn 1;\n\n\tif (r == EMULATION_FAILED) {\n\t\tif (reexecute_instruction(vcpu, cr2_or_gpa, write_fault_to_spt,\n\t\t\t\t\temulation_type))\n\t\t\treturn 1;\n\n\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t}\n\n\tif (ctxt->have_exception) {\n\t\tr = 1;\n\t\tif (inject_emulated_exception(vcpu))\n\t\t\treturn r;\n\t} else if (vcpu->arch.pio.count) {\n\t\tif (!vcpu->arch.pio.in) {\n\t\t\t/* FIXME: return into emulator if single-stepping.  */\n\t\t\tvcpu->arch.pio.count = 0;\n\t\t} else {\n\t\t\twriteback = false;\n\t\t\tvcpu->arch.complete_userspace_io = complete_emulated_pio;\n\t\t}\n\t\tr = 0;\n\t} else if (vcpu->mmio_needed) {\n\t\t++vcpu->stat.mmio_exits;\n\n\t\tif (!vcpu->mmio_is_write)\n\t\t\twriteback = false;\n\t\tr = 0;\n\t\tvcpu->arch.complete_userspace_io = complete_emulated_mmio;\n\t} else if (vcpu->arch.complete_userspace_io) {\n\t\twriteback = false;\n\t\tr = 0;\n\t} else if (r == EMULATION_RESTART)\n\t\tgoto restart;\n\telse\n\t\tr = 1;\n\nwriteback:\n\tif (writeback) {\n\t\tunsigned long rflags = static_call(kvm_x86_get_rflags)(vcpu);\n\t\ttoggle_interruptibility(vcpu, ctxt->interruptibility);\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = false;\n\t\tif (!ctxt->have_exception ||\n\t\t    exception_type(ctxt->exception.vector) == EXCPT_TRAP) {\n\t\t\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);\n\t\t\tif (ctxt->is_branch)\n\t\t\t\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);\n\t\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\t\tif (r && (ctxt->tf || (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)))\n\t\t\t\tr = kvm_vcpu_do_singlestep(vcpu);\n\t\t\tstatic_call_cond(kvm_x86_update_emulated_instruction)(vcpu);\n\t\t\t__kvm_set_rflags(vcpu, ctxt->eflags);\n\t\t}\n\n\t\t/*\n\t\t * For STI, interrupts are shadowed; so KVM_REQ_EVENT will\n\t\t * do nothing, and it will be requested again as soon as\n\t\t * the shadow expires.  But we still need to check here,\n\t\t * because POPF has no interrupt shadow.\n\t\t */\n\t\tif (unlikely((ctxt->eflags & ~rflags) & X86_EFLAGS_IF))\n\t\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t} else\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = true;\n\n\treturn r;\n}",
                        "cve_id": "CVE-2022-1852"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "bool kvm_irq_delivery_to_apic_fast(struct kvm *kvm, struct kvm_lapic *src,\n\t\tstruct kvm_lapic_irq *irq, int *r, struct dest_map *dest_map)\n{\n\tstruct kvm_apic_map *map;\n\tunsigned long bitmap;\n\tstruct kvm_lapic **dst = NULL;\n\tint i;\n\tbool ret;\n\n\t*r = -1;\n\n\tif (irq->shorthand == APIC_DEST_SELF) {\n\t\t*r = kvm_apic_set_irq(src->vcpu, irq, dest_map);\n\t\treturn true;\n\t}\n\n\trcu_read_lock();\n\tmap = rcu_dereference(kvm->arch.apic_map);\n\n\tret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dst, &bitmap);\n\tif (ret) {\n\t\t*r = 0;\n\t\tfor_each_set_bit(i, &bitmap, 16) {\n\t\t\tif (!dst[i])\n\t\t\t\tcontinue;\n\t\t\t*r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);\n\t\t}\n\t}\n\n\trcu_read_unlock();\n\treturn ret;\n}",
                        "code_after_change": "bool kvm_irq_delivery_to_apic_fast(struct kvm *kvm, struct kvm_lapic *src,\n\t\tstruct kvm_lapic_irq *irq, int *r, struct dest_map *dest_map)\n{\n\tstruct kvm_apic_map *map;\n\tunsigned long bitmap;\n\tstruct kvm_lapic **dst = NULL;\n\tint i;\n\tbool ret;\n\n\t*r = -1;\n\n\tif (irq->shorthand == APIC_DEST_SELF) {\n\t\tif (KVM_BUG_ON(!src, kvm)) {\n\t\t\t*r = 0;\n\t\t\treturn true;\n\t\t}\n\t\t*r = kvm_apic_set_irq(src->vcpu, irq, dest_map);\n\t\treturn true;\n\t}\n\n\trcu_read_lock();\n\tmap = rcu_dereference(kvm->arch.apic_map);\n\n\tret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dst, &bitmap);\n\tif (ret) {\n\t\t*r = 0;\n\t\tfor_each_set_bit(i, &bitmap, 16) {\n\t\t\tif (!dst[i])\n\t\t\t\tcontinue;\n\t\t\t*r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);\n\t\t}\n\t}\n\n\trcu_read_unlock();\n\treturn ret;\n}",
                        "cve_id": "CVE-2022-2153"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void vcpu_scan_ioapic(struct kvm_vcpu *vcpu)\n{\n\tif (!kvm_apic_hw_enabled(vcpu->arch.apic))\n\t\treturn;\n\n\tbitmap_zero(vcpu->arch.ioapic_handled_vectors, 256);\n\n\tif (irqchip_split(vcpu->kvm))\n\t\tkvm_scan_ioapic_routes(vcpu, vcpu->arch.ioapic_handled_vectors);\n\telse {\n\t\tif (vcpu->arch.apicv_active)\n\t\t\tkvm_x86_ops->sync_pir_to_irr(vcpu);\n\t\tkvm_ioapic_scan_entry(vcpu, vcpu->arch.ioapic_handled_vectors);\n\t}\n\n\tif (is_guest_mode(vcpu))\n\t\tvcpu->arch.load_eoi_exitmap_pending = true;\n\telse\n\t\tkvm_make_request(KVM_REQ_LOAD_EOI_EXITMAP, vcpu);\n}",
                        "code_after_change": "static void vcpu_scan_ioapic(struct kvm_vcpu *vcpu)\n{\n\tif (!kvm_apic_hw_enabled(vcpu->arch.apic))\n\t\treturn;\n\n\tbitmap_zero(vcpu->arch.ioapic_handled_vectors, 256);\n\n\tif (irqchip_split(vcpu->kvm))\n\t\tkvm_scan_ioapic_routes(vcpu, vcpu->arch.ioapic_handled_vectors);\n\telse {\n\t\tif (vcpu->arch.apicv_active)\n\t\t\tkvm_x86_ops->sync_pir_to_irr(vcpu);\n\t\tif (ioapic_in_kernel(vcpu->kvm))\n\t\t\tkvm_ioapic_scan_entry(vcpu, vcpu->arch.ioapic_handled_vectors);\n\t}\n\n\tif (is_guest_mode(vcpu))\n\t\tvcpu->arch.load_eoi_exitmap_pending = true;\n\telse\n\t\tkvm_make_request(KVM_REQ_LOAD_EOI_EXITMAP, vcpu);\n}",
                        "cve_id": "CVE-2018-19407"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int x86_decode_insn(struct x86_emulate_ctxt *ctxt, void *insn, int insn_len)\n{\n\tint rc = X86EMUL_CONTINUE;\n\tint mode = ctxt->mode;\n\tint def_op_bytes, def_ad_bytes, goffset, simd_prefix;\n\tbool op_prefix = false;\n\tbool has_seg_override = false;\n\tstruct opcode opcode;\n\n\tctxt->memop.type = OP_NONE;\n\tctxt->memopp = NULL;\n\tctxt->_eip = ctxt->eip;\n\tctxt->fetch.ptr = ctxt->fetch.data;\n\tctxt->fetch.end = ctxt->fetch.data + insn_len;\n\tctxt->opcode_len = 1;\n\tif (insn_len > 0)\n\t\tmemcpy(ctxt->fetch.data, insn, insn_len);\n\telse {\n\t\trc = __do_insn_fetch_bytes(ctxt, 1);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\treturn rc;\n\t}\n\n\tswitch (mode) {\n\tcase X86EMUL_MODE_REAL:\n\tcase X86EMUL_MODE_VM86:\n\tcase X86EMUL_MODE_PROT16:\n\t\tdef_op_bytes = def_ad_bytes = 2;\n\t\tbreak;\n\tcase X86EMUL_MODE_PROT32:\n\t\tdef_op_bytes = def_ad_bytes = 4;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase X86EMUL_MODE_PROT64:\n\t\tdef_op_bytes = 4;\n\t\tdef_ad_bytes = 8;\n\t\tbreak;\n#endif\n\tdefault:\n\t\treturn EMULATION_FAILED;\n\t}\n\n\tctxt->op_bytes = def_op_bytes;\n\tctxt->ad_bytes = def_ad_bytes;\n\n\t/* Legacy prefixes. */\n\tfor (;;) {\n\t\tswitch (ctxt->b = insn_fetch(u8, ctxt)) {\n\t\tcase 0x66:\t/* operand-size override */\n\t\t\top_prefix = true;\n\t\t\t/* switch between 2/4 bytes */\n\t\t\tctxt->op_bytes = def_op_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x67:\t/* address-size override */\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\t/* switch between 4/8 bytes */\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 12;\n\t\t\telse\n\t\t\t\t/* switch between 2/4 bytes */\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x26:\t/* ES override */\n\t\tcase 0x2e:\t/* CS override */\n\t\tcase 0x36:\t/* SS override */\n\t\tcase 0x3e:\t/* DS override */\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = (ctxt->b >> 3) & 3;\n\t\t\tbreak;\n\t\tcase 0x64:\t/* FS override */\n\t\tcase 0x65:\t/* GS override */\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->b & 7;\n\t\t\tbreak;\n\t\tcase 0x40 ... 0x4f: /* REX */\n\t\t\tif (mode != X86EMUL_MODE_PROT64)\n\t\t\t\tgoto done_prefixes;\n\t\t\tctxt->rex_prefix = ctxt->b;\n\t\t\tcontinue;\n\t\tcase 0xf0:\t/* LOCK */\n\t\t\tctxt->lock_prefix = 1;\n\t\t\tbreak;\n\t\tcase 0xf2:\t/* REPNE/REPNZ */\n\t\tcase 0xf3:\t/* REP/REPE/REPZ */\n\t\t\tctxt->rep_prefix = ctxt->b;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto done_prefixes;\n\t\t}\n\n\t\t/* Any legacy prefix after a REX prefix nullifies its effect. */\n\n\t\tctxt->rex_prefix = 0;\n\t}\n\ndone_prefixes:\n\n\t/* REX prefix. */\n\tif (ctxt->rex_prefix & 8)\n\t\tctxt->op_bytes = 8;\t/* REX.W */\n\n\t/* Opcode byte(s). */\n\topcode = opcode_table[ctxt->b];\n\t/* Two-byte opcode? */\n\tif (ctxt->b == 0x0f) {\n\t\tctxt->opcode_len = 2;\n\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\topcode = twobyte_table[ctxt->b];\n\n\t\t/* 0F_38 opcode map */\n\t\tif (ctxt->b == 0x38) {\n\t\t\tctxt->opcode_len = 3;\n\t\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\t\topcode = opcode_map_0f_38[ctxt->b];\n\t\t}\n\t}\n\tctxt->d = opcode.flags;\n\n\tif (ctxt->d & ModRM)\n\t\tctxt->modrm = insn_fetch(u8, ctxt);\n\n\t/* vex-prefix instructions are not implemented */\n\tif (ctxt->opcode_len == 1 && (ctxt->b == 0xc5 || ctxt->b == 0xc4) &&\n\t    (mode == X86EMUL_MODE_PROT64 || (ctxt->modrm & 0xc0) == 0xc0)) {\n\t\tctxt->d = NotImpl;\n\t}\n\n\twhile (ctxt->d & GroupMask) {\n\t\tswitch (ctxt->d & GroupMask) {\n\t\tcase Group:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase GroupDual:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\tif ((ctxt->modrm >> 6) == 3)\n\t\t\t\topcode = opcode.u.gdual->mod3[goffset];\n\t\t\telse\n\t\t\t\topcode = opcode.u.gdual->mod012[goffset];\n\t\t\tbreak;\n\t\tcase RMExt:\n\t\t\tgoffset = ctxt->modrm & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase Prefix:\n\t\t\tif (ctxt->rep_prefix && op_prefix)\n\t\t\t\treturn EMULATION_FAILED;\n\t\t\tsimd_prefix = op_prefix ? 0x66 : ctxt->rep_prefix;\n\t\t\tswitch (simd_prefix) {\n\t\t\tcase 0x00: opcode = opcode.u.gprefix->pfx_no; break;\n\t\t\tcase 0x66: opcode = opcode.u.gprefix->pfx_66; break;\n\t\t\tcase 0xf2: opcode = opcode.u.gprefix->pfx_f2; break;\n\t\t\tcase 0xf3: opcode = opcode.u.gprefix->pfx_f3; break;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase Escape:\n\t\t\tif (ctxt->modrm > 0xbf)\n\t\t\t\topcode = opcode.u.esc->high[ctxt->modrm - 0xc0];\n\t\t\telse\n\t\t\t\topcode = opcode.u.esc->op[(ctxt->modrm >> 3) & 7];\n\t\t\tbreak;\n\t\tcase InstrDual:\n\t\t\tif ((ctxt->modrm >> 6) == 3)\n\t\t\t\topcode = opcode.u.idual->mod3;\n\t\t\telse\n\t\t\t\topcode = opcode.u.idual->mod012;\n\t\t\tbreak;\n\t\tcase ModeDual:\n\t\t\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\t\t\topcode = opcode.u.mdual->mode64;\n\t\t\telse\n\t\t\t\topcode = opcode.u.mdual->mode32;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn EMULATION_FAILED;\n\t\t}\n\n\t\tctxt->d &= ~(u64)GroupMask;\n\t\tctxt->d |= opcode.flags;\n\t}\n\n\t/* Unrecognised? */\n\tif (ctxt->d == 0)\n\t\treturn EMULATION_FAILED;\n\n\tctxt->execute = opcode.u.execute;\n\n\tif (unlikely(ctxt->ud) && likely(!(ctxt->d & EmulateOnUD)))\n\t\treturn EMULATION_FAILED;\n\n\tif (unlikely(ctxt->d &\n\t    (NotImpl|Stack|Op3264|Sse|Mmx|Intercept|CheckPerm|NearBranch|\n\t     No16))) {\n\t\t/*\n\t\t * These are copied unconditionally here, and checked unconditionally\n\t\t * in x86_emulate_insn.\n\t\t */\n\t\tctxt->check_perm = opcode.check_perm;\n\t\tctxt->intercept = opcode.intercept;\n\n\t\tif (ctxt->d & NotImpl)\n\t\t\treturn EMULATION_FAILED;\n\n\t\tif (mode == X86EMUL_MODE_PROT64) {\n\t\t\tif (ctxt->op_bytes == 4 && (ctxt->d & Stack))\n\t\t\t\tctxt->op_bytes = 8;\n\t\t\telse if (ctxt->d & NearBranch)\n\t\t\t\tctxt->op_bytes = 8;\n\t\t}\n\n\t\tif (ctxt->d & Op3264) {\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\tctxt->op_bytes = 8;\n\t\t\telse\n\t\t\t\tctxt->op_bytes = 4;\n\t\t}\n\n\t\tif ((ctxt->d & No16) && ctxt->op_bytes == 2)\n\t\t\tctxt->op_bytes = 4;\n\n\t\tif (ctxt->d & Sse)\n\t\t\tctxt->op_bytes = 16;\n\t\telse if (ctxt->d & Mmx)\n\t\t\tctxt->op_bytes = 8;\n\t}\n\n\t/* ModRM and SIB bytes. */\n\tif (ctxt->d & ModRM) {\n\t\trc = decode_modrm(ctxt, &ctxt->memop);\n\t\tif (!has_seg_override) {\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->modrm_seg;\n\t\t}\n\t} else if (ctxt->d & MemAbs)\n\t\trc = decode_abs(ctxt, &ctxt->memop);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\tif (!has_seg_override)\n\t\tctxt->seg_override = VCPU_SREG_DS;\n\n\tctxt->memop.addr.mem.seg = ctxt->seg_override;\n\n\t/*\n\t * Decode and fetch the source operand: register, memory\n\t * or immediate.\n\t */\n\trc = decode_operand(ctxt, &ctxt->src, (ctxt->d >> SrcShift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t/*\n\t * Decode and fetch the second source operand: register, memory\n\t * or immediate.\n\t */\n\trc = decode_operand(ctxt, &ctxt->src2, (ctxt->d >> Src2Shift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t/* Decode and fetch the destination operand: register or memory. */\n\trc = decode_operand(ctxt, &ctxt->dst, (ctxt->d >> DstShift) & OpMask);\n\n\tif (ctxt->rip_relative)\n\t\tctxt->memopp->addr.mem.ea = address_mask(ctxt,\n\t\t\t\t\tctxt->memopp->addr.mem.ea + ctxt->_eip);\n\ndone:\n\treturn (rc != X86EMUL_CONTINUE) ? EMULATION_FAILED : EMULATION_OK;\n}",
                        "code_after_change": "int x86_decode_insn(struct x86_emulate_ctxt *ctxt, void *insn, int insn_len)\n{\n\tint rc = X86EMUL_CONTINUE;\n\tint mode = ctxt->mode;\n\tint def_op_bytes, def_ad_bytes, goffset, simd_prefix;\n\tbool op_prefix = false;\n\tbool has_seg_override = false;\n\tstruct opcode opcode;\n\n\tctxt->memop.type = OP_NONE;\n\tctxt->memopp = NULL;\n\tctxt->_eip = ctxt->eip;\n\tctxt->fetch.ptr = ctxt->fetch.data;\n\tctxt->fetch.end = ctxt->fetch.data + insn_len;\n\tctxt->opcode_len = 1;\n\tif (insn_len > 0)\n\t\tmemcpy(ctxt->fetch.data, insn, insn_len);\n\telse {\n\t\trc = __do_insn_fetch_bytes(ctxt, 1);\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\treturn rc;\n\t}\n\n\tswitch (mode) {\n\tcase X86EMUL_MODE_REAL:\n\tcase X86EMUL_MODE_VM86:\n\tcase X86EMUL_MODE_PROT16:\n\t\tdef_op_bytes = def_ad_bytes = 2;\n\t\tbreak;\n\tcase X86EMUL_MODE_PROT32:\n\t\tdef_op_bytes = def_ad_bytes = 4;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase X86EMUL_MODE_PROT64:\n\t\tdef_op_bytes = 4;\n\t\tdef_ad_bytes = 8;\n\t\tbreak;\n#endif\n\tdefault:\n\t\treturn EMULATION_FAILED;\n\t}\n\n\tctxt->op_bytes = def_op_bytes;\n\tctxt->ad_bytes = def_ad_bytes;\n\n\t/* Legacy prefixes. */\n\tfor (;;) {\n\t\tswitch (ctxt->b = insn_fetch(u8, ctxt)) {\n\t\tcase 0x66:\t/* operand-size override */\n\t\t\top_prefix = true;\n\t\t\t/* switch between 2/4 bytes */\n\t\t\tctxt->op_bytes = def_op_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x67:\t/* address-size override */\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\t/* switch between 4/8 bytes */\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 12;\n\t\t\telse\n\t\t\t\t/* switch between 2/4 bytes */\n\t\t\t\tctxt->ad_bytes = def_ad_bytes ^ 6;\n\t\t\tbreak;\n\t\tcase 0x26:\t/* ES override */\n\t\tcase 0x2e:\t/* CS override */\n\t\tcase 0x36:\t/* SS override */\n\t\tcase 0x3e:\t/* DS override */\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = (ctxt->b >> 3) & 3;\n\t\t\tbreak;\n\t\tcase 0x64:\t/* FS override */\n\t\tcase 0x65:\t/* GS override */\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->b & 7;\n\t\t\tbreak;\n\t\tcase 0x40 ... 0x4f: /* REX */\n\t\t\tif (mode != X86EMUL_MODE_PROT64)\n\t\t\t\tgoto done_prefixes;\n\t\t\tctxt->rex_prefix = ctxt->b;\n\t\t\tcontinue;\n\t\tcase 0xf0:\t/* LOCK */\n\t\t\tctxt->lock_prefix = 1;\n\t\t\tbreak;\n\t\tcase 0xf2:\t/* REPNE/REPNZ */\n\t\tcase 0xf3:\t/* REP/REPE/REPZ */\n\t\t\tctxt->rep_prefix = ctxt->b;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto done_prefixes;\n\t\t}\n\n\t\t/* Any legacy prefix after a REX prefix nullifies its effect. */\n\n\t\tctxt->rex_prefix = 0;\n\t}\n\ndone_prefixes:\n\n\t/* REX prefix. */\n\tif (ctxt->rex_prefix & 8)\n\t\tctxt->op_bytes = 8;\t/* REX.W */\n\n\t/* Opcode byte(s). */\n\topcode = opcode_table[ctxt->b];\n\t/* Two-byte opcode? */\n\tif (ctxt->b == 0x0f) {\n\t\tctxt->opcode_len = 2;\n\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\topcode = twobyte_table[ctxt->b];\n\n\t\t/* 0F_38 opcode map */\n\t\tif (ctxt->b == 0x38) {\n\t\t\tctxt->opcode_len = 3;\n\t\t\tctxt->b = insn_fetch(u8, ctxt);\n\t\t\topcode = opcode_map_0f_38[ctxt->b];\n\t\t}\n\t}\n\tctxt->d = opcode.flags;\n\n\tif (ctxt->d & ModRM)\n\t\tctxt->modrm = insn_fetch(u8, ctxt);\n\n\t/* vex-prefix instructions are not implemented */\n\tif (ctxt->opcode_len == 1 && (ctxt->b == 0xc5 || ctxt->b == 0xc4) &&\n\t    (mode == X86EMUL_MODE_PROT64 || (ctxt->modrm & 0xc0) == 0xc0)) {\n\t\tctxt->d = NotImpl;\n\t}\n\n\twhile (ctxt->d & GroupMask) {\n\t\tswitch (ctxt->d & GroupMask) {\n\t\tcase Group:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase GroupDual:\n\t\t\tgoffset = (ctxt->modrm >> 3) & 7;\n\t\t\tif ((ctxt->modrm >> 6) == 3)\n\t\t\t\topcode = opcode.u.gdual->mod3[goffset];\n\t\t\telse\n\t\t\t\topcode = opcode.u.gdual->mod012[goffset];\n\t\t\tbreak;\n\t\tcase RMExt:\n\t\t\tgoffset = ctxt->modrm & 7;\n\t\t\topcode = opcode.u.group[goffset];\n\t\t\tbreak;\n\t\tcase Prefix:\n\t\t\tif (ctxt->rep_prefix && op_prefix)\n\t\t\t\treturn EMULATION_FAILED;\n\t\t\tsimd_prefix = op_prefix ? 0x66 : ctxt->rep_prefix;\n\t\t\tswitch (simd_prefix) {\n\t\t\tcase 0x00: opcode = opcode.u.gprefix->pfx_no; break;\n\t\t\tcase 0x66: opcode = opcode.u.gprefix->pfx_66; break;\n\t\t\tcase 0xf2: opcode = opcode.u.gprefix->pfx_f2; break;\n\t\t\tcase 0xf3: opcode = opcode.u.gprefix->pfx_f3; break;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase Escape:\n\t\t\tif (ctxt->modrm > 0xbf)\n\t\t\t\topcode = opcode.u.esc->high[ctxt->modrm - 0xc0];\n\t\t\telse\n\t\t\t\topcode = opcode.u.esc->op[(ctxt->modrm >> 3) & 7];\n\t\t\tbreak;\n\t\tcase InstrDual:\n\t\t\tif ((ctxt->modrm >> 6) == 3)\n\t\t\t\topcode = opcode.u.idual->mod3;\n\t\t\telse\n\t\t\t\topcode = opcode.u.idual->mod012;\n\t\t\tbreak;\n\t\tcase ModeDual:\n\t\t\tif (ctxt->mode == X86EMUL_MODE_PROT64)\n\t\t\t\topcode = opcode.u.mdual->mode64;\n\t\t\telse\n\t\t\t\topcode = opcode.u.mdual->mode32;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn EMULATION_FAILED;\n\t\t}\n\n\t\tctxt->d &= ~(u64)GroupMask;\n\t\tctxt->d |= opcode.flags;\n\t}\n\n\t/* Unrecognised? */\n\tif (ctxt->d == 0)\n\t\treturn EMULATION_FAILED;\n\n\tctxt->execute = opcode.u.execute;\n\n\tif (unlikely(ctxt->ud) && likely(!(ctxt->d & EmulateOnUD)))\n\t\treturn EMULATION_FAILED;\n\n\tif (unlikely(ctxt->d &\n\t    (NotImpl|Stack|Op3264|Sse|Mmx|Intercept|CheckPerm|NearBranch|\n\t     No16))) {\n\t\t/*\n\t\t * These are copied unconditionally here, and checked unconditionally\n\t\t * in x86_emulate_insn.\n\t\t */\n\t\tctxt->check_perm = opcode.check_perm;\n\t\tctxt->intercept = opcode.intercept;\n\n\t\tif (ctxt->d & NotImpl)\n\t\t\treturn EMULATION_FAILED;\n\n\t\tif (mode == X86EMUL_MODE_PROT64) {\n\t\t\tif (ctxt->op_bytes == 4 && (ctxt->d & Stack))\n\t\t\t\tctxt->op_bytes = 8;\n\t\t\telse if (ctxt->d & NearBranch)\n\t\t\t\tctxt->op_bytes = 8;\n\t\t}\n\n\t\tif (ctxt->d & Op3264) {\n\t\t\tif (mode == X86EMUL_MODE_PROT64)\n\t\t\t\tctxt->op_bytes = 8;\n\t\t\telse\n\t\t\t\tctxt->op_bytes = 4;\n\t\t}\n\n\t\tif ((ctxt->d & No16) && ctxt->op_bytes == 2)\n\t\t\tctxt->op_bytes = 4;\n\n\t\tif (ctxt->d & Sse)\n\t\t\tctxt->op_bytes = 16;\n\t\telse if (ctxt->d & Mmx)\n\t\t\tctxt->op_bytes = 8;\n\t}\n\n\t/* ModRM and SIB bytes. */\n\tif (ctxt->d & ModRM) {\n\t\trc = decode_modrm(ctxt, &ctxt->memop);\n\t\tif (!has_seg_override) {\n\t\t\thas_seg_override = true;\n\t\t\tctxt->seg_override = ctxt->modrm_seg;\n\t\t}\n\t} else if (ctxt->d & MemAbs)\n\t\trc = decode_abs(ctxt, &ctxt->memop);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\tif (!has_seg_override)\n\t\tctxt->seg_override = VCPU_SREG_DS;\n\n\tctxt->memop.addr.mem.seg = ctxt->seg_override;\n\n\t/*\n\t * Decode and fetch the source operand: register, memory\n\t * or immediate.\n\t */\n\trc = decode_operand(ctxt, &ctxt->src, (ctxt->d >> SrcShift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t/*\n\t * Decode and fetch the second source operand: register, memory\n\t * or immediate.\n\t */\n\trc = decode_operand(ctxt, &ctxt->src2, (ctxt->d >> Src2Shift) & OpMask);\n\tif (rc != X86EMUL_CONTINUE)\n\t\tgoto done;\n\n\t/* Decode and fetch the destination operand: register or memory. */\n\trc = decode_operand(ctxt, &ctxt->dst, (ctxt->d >> DstShift) & OpMask);\n\n\tif (ctxt->rip_relative && likely(ctxt->memopp))\n\t\tctxt->memopp->addr.mem.ea = address_mask(ctxt,\n\t\t\t\t\tctxt->memopp->addr.mem.ea + ctxt->_eip);\n\ndone:\n\treturn (rc != X86EMUL_CONTINUE) ? EMULATION_FAILED : EMULATION_OK;\n}",
                        "cve_id": "CVE-2016-8630"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3575,
            "cve_id": "CVE-2022-23222",
            "code_snippet": "static int check_cond_jmp_op(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_insn *insn, int *insn_idx)\n{\n\tstruct bpf_verifier_state *this_branch = env->cur_state;\n\tstruct bpf_verifier_state *other_branch;\n\tstruct bpf_reg_state *regs = this_branch->frame[this_branch->curframe]->regs;\n\tstruct bpf_reg_state *dst_reg, *other_branch_regs, *src_reg = NULL;\n\tu8 opcode = BPF_OP(insn->code);\n\tbool is_jmp32;\n\tint pred = -1;\n\tint err;\n\n\t/* Only conditional jumps are expected to reach here. */\n\tif (opcode == BPF_JA || opcode > BPF_JSLE) {\n\t\tverbose(env, \"invalid BPF_JMP/JMP32 opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (insn->imm != 0) {\n\t\t\tverbose(env, \"BPF_JMP/JMP32 uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* check src1 operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\t\tinsn->src_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tsrc_reg = &regs[insn->src_reg];\n\t} else {\n\t\tif (insn->src_reg != BPF_REG_0) {\n\t\t\tverbose(env, \"BPF_JMP/JMP32 uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tdst_reg = &regs[insn->dst_reg];\n\tis_jmp32 = BPF_CLASS(insn->code) == BPF_JMP32;\n\n\tif (BPF_SRC(insn->code) == BPF_K) {\n\t\tpred = is_branch_taken(dst_reg, insn->imm, opcode, is_jmp32);\n\t} else if (src_reg->type == SCALAR_VALUE &&\n\t\t   is_jmp32 && tnum_is_const(tnum_subreg(src_reg->var_off))) {\n\t\tpred = is_branch_taken(dst_reg,\n\t\t\t\t       tnum_subreg(src_reg->var_off).value,\n\t\t\t\t       opcode,\n\t\t\t\t       is_jmp32);\n\t} else if (src_reg->type == SCALAR_VALUE &&\n\t\t   !is_jmp32 && tnum_is_const(src_reg->var_off)) {\n\t\tpred = is_branch_taken(dst_reg,\n\t\t\t\t       src_reg->var_off.value,\n\t\t\t\t       opcode,\n\t\t\t\t       is_jmp32);\n\t} else if (reg_is_pkt_pointer_any(dst_reg) &&\n\t\t   reg_is_pkt_pointer_any(src_reg) &&\n\t\t   !is_jmp32) {\n\t\tpred = is_pkt_ptr_branch_taken(dst_reg, src_reg, opcode);\n\t}\n\n\tif (pred >= 0) {\n\t\t/* If we get here with a dst_reg pointer type it is because\n\t\t * above is_branch_taken() special cased the 0 comparison.\n\t\t */\n\t\tif (!__is_pointer_value(false, dst_reg))\n\t\t\terr = mark_chain_precision(env, insn->dst_reg);\n\t\tif (BPF_SRC(insn->code) == BPF_X && !err &&\n\t\t    !__is_pointer_value(false, src_reg))\n\t\t\terr = mark_chain_precision(env, insn->src_reg);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (pred == 1) {\n\t\t/* Only follow the goto, ignore fall-through. If needed, push\n\t\t * the fall-through branch for simulation under speculative\n\t\t * execution.\n\t\t */\n\t\tif (!env->bypass_spec_v1 &&\n\t\t    !sanitize_speculative_path(env, insn, *insn_idx + 1,\n\t\t\t\t\t       *insn_idx))\n\t\t\treturn -EFAULT;\n\t\t*insn_idx += insn->off;\n\t\treturn 0;\n\t} else if (pred == 0) {\n\t\t/* Only follow the fall-through branch, since that's where the\n\t\t * program will go. If needed, push the goto branch for\n\t\t * simulation under speculative execution.\n\t\t */\n\t\tif (!env->bypass_spec_v1 &&\n\t\t    !sanitize_speculative_path(env, insn,\n\t\t\t\t\t       *insn_idx + insn->off + 1,\n\t\t\t\t\t       *insn_idx))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\n\tother_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx,\n\t\t\t\t  false);\n\tif (!other_branch)\n\t\treturn -EFAULT;\n\tother_branch_regs = other_branch->frame[other_branch->curframe]->regs;\n\n\t/* detect if we are comparing against a constant value so we can adjust\n\t * our min/max values for our dst register.\n\t * this is only legit if both are scalars (or pointers to the same\n\t * object, I suppose, but we don't support that right now), because\n\t * otherwise the different base pointers mean the offsets aren't\n\t * comparable.\n\t */\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tstruct bpf_reg_state *src_reg = &regs[insn->src_reg];\n\n\t\tif (dst_reg->type == SCALAR_VALUE &&\n\t\t    src_reg->type == SCALAR_VALUE) {\n\t\t\tif (tnum_is_const(src_reg->var_off) ||\n\t\t\t    (is_jmp32 &&\n\t\t\t     tnum_is_const(tnum_subreg(src_reg->var_off))))\n\t\t\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\tdst_reg,\n\t\t\t\t\t\tsrc_reg->var_off.value,\n\t\t\t\t\t\ttnum_subreg(src_reg->var_off).value,\n\t\t\t\t\t\topcode, is_jmp32);\n\t\t\telse if (tnum_is_const(dst_reg->var_off) ||\n\t\t\t\t (is_jmp32 &&\n\t\t\t\t  tnum_is_const(tnum_subreg(dst_reg->var_off))))\n\t\t\t\treg_set_min_max_inv(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    src_reg,\n\t\t\t\t\t\t    dst_reg->var_off.value,\n\t\t\t\t\t\t    tnum_subreg(dst_reg->var_off).value,\n\t\t\t\t\t\t    opcode, is_jmp32);\n\t\t\telse if (!is_jmp32 &&\n\t\t\t\t (opcode == BPF_JEQ || opcode == BPF_JNE))\n\t\t\t\t/* Comparing for equality, we can combine knowledge */\n\t\t\t\treg_combine_min_max(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\t    src_reg, dst_reg, opcode);\n\t\t\tif (src_reg->id &&\n\t\t\t    !WARN_ON_ONCE(src_reg->id != other_branch_regs[insn->src_reg].id)) {\n\t\t\t\tfind_equal_scalars(this_branch, src_reg);\n\t\t\t\tfind_equal_scalars(other_branch, &other_branch_regs[insn->src_reg]);\n\t\t\t}\n\n\t\t}\n\t} else if (dst_reg->type == SCALAR_VALUE) {\n\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\tdst_reg, insn->imm, (u32)insn->imm,\n\t\t\t\t\topcode, is_jmp32);\n\t}\n\n\tif (dst_reg->type == SCALAR_VALUE && dst_reg->id &&\n\t    !WARN_ON_ONCE(dst_reg->id != other_branch_regs[insn->dst_reg].id)) {\n\t\tfind_equal_scalars(this_branch, dst_reg);\n\t\tfind_equal_scalars(other_branch, &other_branch_regs[insn->dst_reg]);\n\t}\n\n\t/* detect if R == 0 where R is returned from bpf_map_lookup_elem().\n\t * NOTE: these optimizations below are related with pointer comparison\n\t *       which will never be JMP32.\n\t */\n\tif (!is_jmp32 && BPF_SRC(insn->code) == BPF_K &&\n\t    insn->imm == 0 && (opcode == BPF_JEQ || opcode == BPF_JNE) &&\n\t    type_may_be_null(dst_reg->type)) {\n\t\t/* Mark all identical registers in each branch as either\n\t\t * safe or unknown depending R == 0 or R != 0 conditional.\n\t\t */\n\t\tmark_ptr_or_null_regs(this_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JNE);\n\t\tmark_ptr_or_null_regs(other_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JEQ);\n\t} else if (!try_match_pkt_pointers(insn, dst_reg, &regs[insn->src_reg],\n\t\t\t\t\t   this_branch, other_branch) &&\n\t\t   is_pointer_value(env, insn->dst_reg)) {\n\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\tinsn->dst_reg);\n\t\treturn -EACCES;\n\t}\n\tif (env->log.level & BPF_LOG_LEVEL)\n\t\tprint_insn_state(env, this_branch->frame[this_branch->curframe]);\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "code_after_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tif (ptr_reg->type & PTR_MAYBE_NULL) {\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\t}\n\n\tswitch (base_type(ptr_reg->type)) {\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3576,
            "cve_id": "CVE-2022-23222",
            "code_snippet": "static bool is_spillable_regtype(enum bpf_reg_type type)\n{\n\tswitch (base_type(type)) {\n\tcase PTR_TO_MAP_VALUE:\n\tcase PTR_TO_STACK:\n\tcase PTR_TO_CTX:\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\tcase PTR_TO_BTF_ID:\n\tcase PTR_TO_RDONLY_BUF:\n\tcase PTR_TO_RDWR_BUF:\n\tcase PTR_TO_PERCPU_BTF_ID:\n\tcase PTR_TO_MEM:\n\tcase PTR_TO_FUNC:\n\tcase PTR_TO_MAP_KEY:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static bool reg_type_mismatch_ok(enum bpf_reg_type type)\n{\n\tswitch (type) {\n\tcase PTR_TO_CTX:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\tcase PTR_TO_BTF_ID:\n\tcase PTR_TO_BTF_ID_OR_NULL:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}",
                        "code_after_change": "static bool reg_type_mismatch_ok(enum bpf_reg_type type)\n{\n\tswitch (base_type(type)) {\n\tcase PTR_TO_CTX:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\tcase PTR_TO_BTF_ID:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void mark_ptr_not_null_reg(struct bpf_reg_state *reg)\n{\n\tswitch (reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL: {\n\t\tconst struct bpf_map *map = reg->map_ptr;\n\n\t\tif (map->inner_map_meta) {\n\t\t\treg->type = CONST_PTR_TO_MAP;\n\t\t\treg->map_ptr = map->inner_map_meta;\n\t\t\t/* transfer reg's id which is unique for every map_lookup_elem\n\t\t\t * as UID of the inner map.\n\t\t\t */\n\t\t\tif (map_value_has_timer(map->inner_map_meta))\n\t\t\t\treg->map_uid = reg->id;\n\t\t} else if (map->map_type == BPF_MAP_TYPE_XSKMAP) {\n\t\t\treg->type = PTR_TO_XDP_SOCK;\n\t\t} else if (map->map_type == BPF_MAP_TYPE_SOCKMAP ||\n\t\t\t   map->map_type == BPF_MAP_TYPE_SOCKHASH) {\n\t\t\treg->type = PTR_TO_SOCKET;\n\t\t} else {\n\t\t\treg->type = PTR_TO_MAP_VALUE;\n\t\t}\n\t\tbreak;\n\t}\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\treg->type = PTR_TO_SOCKET;\n\t\tbreak;\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\t\treg->type = PTR_TO_SOCK_COMMON;\n\t\tbreak;\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\t\treg->type = PTR_TO_TCP_SOCK;\n\t\tbreak;\n\tcase PTR_TO_BTF_ID_OR_NULL:\n\t\treg->type = PTR_TO_BTF_ID;\n\t\tbreak;\n\tcase PTR_TO_MEM_OR_NULL:\n\t\treg->type = PTR_TO_MEM;\n\t\tbreak;\n\tcase PTR_TO_RDONLY_BUF_OR_NULL:\n\t\treg->type = PTR_TO_RDONLY_BUF;\n\t\tbreak;\n\tcase PTR_TO_RDWR_BUF_OR_NULL:\n\t\treg->type = PTR_TO_RDWR_BUF;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"unknown nullable register type\");\n\t}\n}",
                        "code_after_change": "static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_KEY:\n\t\treturn check_mem_region_access(env, regno, reg->off, access_size,\n\t\t\t\t\t       reg->map_ptr->key_size, false);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tcase PTR_TO_STACK:\n\t\treturn check_stack_range_initialized(\n\t\t\t\tenv,\n\t\t\t\tregno, reg->off, access_size,\n\t\t\t\tzero_size_allowed, ACCESS_HELPER, meta);\n\tdefault: /* scalar_value or invalid ptr */\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s \", regno,\n\t\t\treg_type_str(env, reg->type));\n\t\tverbose(env, \"expected=%s\\n\", reg_type_str(env, PTR_TO_STACK));\n\t\treturn -EACCES;\n\t}\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_KEY:\n\t\treturn check_mem_region_access(env, regno, reg->off, access_size,\n\t\t\t\t\t       reg->map_ptr->key_size, false);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tcase PTR_TO_STACK:\n\t\treturn check_stack_range_initialized(\n\t\t\t\tenv,\n\t\t\t\tregno, reg->off, access_size,\n\t\t\t\tzero_size_allowed, ACCESS_HELPER, meta);\n\tdefault: /* scalar_value or invalid ptr */\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s expected=%s\\n\", regno,\n\t\t\treg_type_str[reg->type],\n\t\t\treg_type_str[PTR_TO_STACK]);\n\t\treturn -EACCES;\n\t}\n}",
                        "code_after_change": "static bool regsafe(struct bpf_verifier_env *env, struct bpf_reg_state *rold,\n\t\t    struct bpf_reg_state *rcur, struct bpf_id_pair *idmap)\n{\n\tbool equal;\n\n\tif (!(rold->live & REG_LIVE_READ))\n\t\t/* explored state didn't use this */\n\t\treturn true;\n\n\tequal = memcmp(rold, rcur, offsetof(struct bpf_reg_state, parent)) == 0;\n\n\tif (rold->type == PTR_TO_STACK)\n\t\t/* two stack pointers are equal only if they're pointing to\n\t\t * the same stack frame, since fp-8 in foo != fp-8 in bar\n\t\t */\n\t\treturn equal && rold->frameno == rcur->frameno;\n\n\tif (equal)\n\t\treturn true;\n\n\tif (rold->type == NOT_INIT)\n\t\t/* explored state can't have used this */\n\t\treturn true;\n\tif (rcur->type == NOT_INIT)\n\t\treturn false;\n\tswitch (base_type(rold->type)) {\n\tcase SCALAR_VALUE:\n\t\tif (env->explore_alu_limits)\n\t\t\treturn false;\n\t\tif (rcur->type == SCALAR_VALUE) {\n\t\t\tif (!rold->precise && !rcur->precise)\n\t\t\t\treturn true;\n\t\t\t/* new val must satisfy old val knowledge */\n\t\t\treturn range_within(rold, rcur) &&\n\t\t\t       tnum_in(rold->var_off, rcur->var_off);\n\t\t} else {\n\t\t\t/* We're trying to use a pointer in place of a scalar.\n\t\t\t * Even if the scalar was unbounded, this could lead to\n\t\t\t * pointer leaks because scalars are allowed to leak\n\t\t\t * while pointers are not. We could make this safe in\n\t\t\t * special cases if root is calling us, but it's\n\t\t\t * probably not worth the hassle.\n\t\t\t */\n\t\t\treturn false;\n\t\t}\n\tcase PTR_TO_MAP_KEY:\n\tcase PTR_TO_MAP_VALUE:\n\t\t/* a PTR_TO_MAP_VALUE could be safe to use as a\n\t\t * PTR_TO_MAP_VALUE_OR_NULL into the same map.\n\t\t * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-\n\t\t * checked, doing so could have affected others with the same\n\t\t * id, and we can't check for that because we lost the id when\n\t\t * we converted to a PTR_TO_MAP_VALUE.\n\t\t */\n\t\tif (type_may_be_null(rold->type)) {\n\t\t\tif (!type_may_be_null(rcur->type))\n\t\t\t\treturn false;\n\t\t\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)))\n\t\t\t\treturn false;\n\t\t\t/* Check our ids match any regs they're supposed to */\n\t\t\treturn check_ids(rold->id, rcur->id, idmap);\n\t\t}\n\n\t\t/* If the new min/max/var_off satisfy the old ones and\n\t\t * everything else matches, we are OK.\n\t\t * 'id' is not compared, since it's only used for maps with\n\t\t * bpf_spin_lock inside map element and in such cases if\n\t\t * the rest of the prog is valid for one map element then\n\t\t * it's valid for all map elements regardless of the key\n\t\t * used in bpf_map_lookup()\n\t\t */\n\t\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&\n\t\t       range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET:\n\t\tif (rcur->type != rold->type)\n\t\t\treturn false;\n\t\t/* We must have at least as much range as the old ptr\n\t\t * did, so that any accesses which were safe before are\n\t\t * still safe.  This is true even if old range < old off,\n\t\t * since someone could have accessed through (ptr - k), or\n\t\t * even done ptr -= k in a register, to get a safe access.\n\t\t */\n\t\tif (rold->range > rcur->range)\n\t\t\treturn false;\n\t\t/* If the offsets don't match, we can't trust our alignment;\n\t\t * nor can we be sure that we won't fall out of range.\n\t\t */\n\t\tif (rold->off != rcur->off)\n\t\t\treturn false;\n\t\t/* id relations must be preserved */\n\t\tif (rold->id && !check_ids(rold->id, rcur->id, idmap))\n\t\t\treturn false;\n\t\t/* new val must satisfy old val knowledge */\n\t\treturn range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_CTX:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\t\t/* Only valid matches are exact, which memcmp() above\n\t\t * would have accepted\n\t\t */\n\tdefault:\n\t\t/* Don't know what's going on, just say it's not safe */\n\t\treturn false;\n\t}\n\n\t/* Shouldn't get here; if we do, say it's not safe */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static bool regsafe(struct bpf_verifier_env *env, struct bpf_reg_state *rold,\n\t\t    struct bpf_reg_state *rcur, struct bpf_id_pair *idmap)\n{\n\tbool equal;\n\n\tif (!(rold->live & REG_LIVE_READ))\n\t\t/* explored state didn't use this */\n\t\treturn true;\n\n\tequal = memcmp(rold, rcur, offsetof(struct bpf_reg_state, parent)) == 0;\n\n\tif (rold->type == PTR_TO_STACK)\n\t\t/* two stack pointers are equal only if they're pointing to\n\t\t * the same stack frame, since fp-8 in foo != fp-8 in bar\n\t\t */\n\t\treturn equal && rold->frameno == rcur->frameno;\n\n\tif (equal)\n\t\treturn true;\n\n\tif (rold->type == NOT_INIT)\n\t\t/* explored state can't have used this */\n\t\treturn true;\n\tif (rcur->type == NOT_INIT)\n\t\treturn false;\n\tswitch (rold->type) {\n\tcase SCALAR_VALUE:\n\t\tif (env->explore_alu_limits)\n\t\t\treturn false;\n\t\tif (rcur->type == SCALAR_VALUE) {\n\t\t\tif (!rold->precise && !rcur->precise)\n\t\t\t\treturn true;\n\t\t\t/* new val must satisfy old val knowledge */\n\t\t\treturn range_within(rold, rcur) &&\n\t\t\t       tnum_in(rold->var_off, rcur->var_off);\n\t\t} else {\n\t\t\t/* We're trying to use a pointer in place of a scalar.\n\t\t\t * Even if the scalar was unbounded, this could lead to\n\t\t\t * pointer leaks because scalars are allowed to leak\n\t\t\t * while pointers are not. We could make this safe in\n\t\t\t * special cases if root is calling us, but it's\n\t\t\t * probably not worth the hassle.\n\t\t\t */\n\t\t\treturn false;\n\t\t}\n\tcase PTR_TO_MAP_KEY:\n\tcase PTR_TO_MAP_VALUE:\n\t\t/* If the new min/max/var_off satisfy the old ones and\n\t\t * everything else matches, we are OK.\n\t\t * 'id' is not compared, since it's only used for maps with\n\t\t * bpf_spin_lock inside map element and in such cases if\n\t\t * the rest of the prog is valid for one map element then\n\t\t * it's valid for all map elements regardless of the key\n\t\t * used in bpf_map_lookup()\n\t\t */\n\t\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&\n\t\t       range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\t/* a PTR_TO_MAP_VALUE could be safe to use as a\n\t\t * PTR_TO_MAP_VALUE_OR_NULL into the same map.\n\t\t * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-\n\t\t * checked, doing so could have affected others with the same\n\t\t * id, and we can't check for that because we lost the id when\n\t\t * we converted to a PTR_TO_MAP_VALUE.\n\t\t */\n\t\tif (rcur->type != PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\treturn false;\n\t\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)))\n\t\t\treturn false;\n\t\t/* Check our ids match any regs they're supposed to */\n\t\treturn check_ids(rold->id, rcur->id, idmap);\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET:\n\t\tif (rcur->type != rold->type)\n\t\t\treturn false;\n\t\t/* We must have at least as much range as the old ptr\n\t\t * did, so that any accesses which were safe before are\n\t\t * still safe.  This is true even if old range < old off,\n\t\t * since someone could have accessed through (ptr - k), or\n\t\t * even done ptr -= k in a register, to get a safe access.\n\t\t */\n\t\tif (rold->range > rcur->range)\n\t\t\treturn false;\n\t\t/* If the offsets don't match, we can't trust our alignment;\n\t\t * nor can we be sure that we won't fall out of range.\n\t\t */\n\t\tif (rold->off != rcur->off)\n\t\t\treturn false;\n\t\t/* id relations must be preserved */\n\t\tif (rold->id && !check_ids(rold->id, rcur->id, idmap))\n\t\t\treturn false;\n\t\t/* new val must satisfy old val knowledge */\n\t\treturn range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_CTX:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\t/* Only valid matches are exact, which memcmp() above\n\t\t * would have accepted\n\t\t */\n\tdefault:\n\t\t/* Don't know what's going on, just say it's not safe */\n\t\treturn false;\n\t}\n\n\t/* Shouldn't get here; if we do, say it's not safe */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}",
                        "code_after_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tif (ptr_reg->type & PTR_MAYBE_NULL) {\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\t}\n\n\tswitch (base_type(ptr_reg->type)) {\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "code_after_change": "static int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct tnum enforce_attach_type_range = tnum_unknown;\n\tconst struct bpf_prog *prog = env->prog;\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\tenum bpf_prog_type prog_type = resolve_prog_type(env->prog);\n\tint err;\n\tstruct bpf_func_state *frame = env->cur_state->frame[0];\n\tconst bool is_subprog = frame->subprogno;\n\n\t/* LSM and struct_ops func-ptr's return type could be \"void\" */\n\tif (!is_subprog &&\n\t    (prog_type == BPF_PROG_TYPE_STRUCT_OPS ||\n\t     prog_type == BPF_PROG_TYPE_LSM) &&\n\t    !prog->aux->attach_func_proto->type)\n\t\treturn 0;\n\n\t/* eBPF calling convention is such that R0 is used\n\t * to return the value from eBPF program.\n\t * Make sure that it's readable at this time\n\t * of bpf_exit, which means that program wrote\n\t * something into it earlier\n\t */\n\terr = check_reg_arg(env, BPF_REG_0, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\tverbose(env, \"R0 leaks addr as return value\\n\");\n\t\treturn -EACCES;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\n\tif (frame->in_async_callback_fn) {\n\t\t/* enforce return zero from async callbacks like timer */\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"In async callback the register R0 is not a known value (%s)\\n\",\n\t\t\t\treg_type_str(env, reg->type));\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!tnum_in(tnum_const(0), reg->var_off)) {\n\t\t\tverbose_invalid_scalar(env, reg, &range, \"async callback\", \"R0\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (is_subprog) {\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"At subprogram exit the register R0 is not a scalar value (%s)\\n\",\n\t\t\t\treg_type_str(env, reg->type));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tswitch (prog_type) {\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_UDP4_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_UDP6_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETSOCKNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETSOCKNAME)\n\t\t\trange = tnum_range(1, 1);\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET4_BIND ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_BIND)\n\t\t\trange = tnum_range(0, 3);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET_EGRESS) {\n\t\t\trange = tnum_range(0, 3);\n\t\t\tenforce_attach_type_range = tnum_range(2, 3);\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\tcase BPF_PROG_TYPE_CGROUP_SYSCTL:\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\t\tbreak;\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT:\n\t\tif (!env->prog->aux->attach_btf_id)\n\t\t\treturn 0;\n\t\trange = tnum_const(0);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_TRACING:\n\t\tswitch (env->prog->expected_attach_type) {\n\t\tcase BPF_TRACE_FENTRY:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\trange = tnum_const(0);\n\t\t\tbreak;\n\t\tcase BPF_TRACE_RAW_TP:\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\treturn 0;\n\t\tcase BPF_TRACE_ITER:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -ENOTSUPP;\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_SK_LOOKUP:\n\t\trange = tnum_range(SK_DROP, SK_PASS);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_EXT:\n\t\t/* freplace program can return anything as its return value\n\t\t * depends on the to-be-replaced kernel func or bpf program.\n\t\t */\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str(env, reg->type));\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose_invalid_scalar(env, reg, &range, \"program exit\", \"R0\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_is_unknown(enforce_attach_type_range) &&\n\t    tnum_in(enforce_attach_type_range, reg->var_off))\n\t\tenv->prog->enforce_expected_attach_type = 1;\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 3577,
            "cve_id": "CVE-2022-23222",
            "code_snippet": "static int check_helper_call(struct bpf_verifier_env *env, struct bpf_insn *insn,\n\t\t\t     int *insn_idx_p)\n{\n\tconst struct bpf_func_proto *fn = NULL;\n\tenum bpf_return_type ret_type;\n\tenum bpf_type_flag ret_flag;\n\tstruct bpf_reg_state *regs;\n\tstruct bpf_call_arg_meta meta;\n\tint insn_idx = *insn_idx_p;\n\tbool changes_data;\n\tint i, err, func_id;\n\n\t/* find function prototype */\n\tfunc_id = insn->imm;\n\tif (func_id < 0 || func_id >= __BPF_FUNC_MAX_ID) {\n\t\tverbose(env, \"invalid func %s#%d\\n\", func_id_name(func_id),\n\t\t\tfunc_id);\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->ops->get_func_proto)\n\t\tfn = env->ops->get_func_proto(func_id, env->prog);\n\tif (!fn) {\n\t\tverbose(env, \"unknown func %s#%d\\n\", func_id_name(func_id),\n\t\t\tfunc_id);\n\t\treturn -EINVAL;\n\t}\n\n\t/* eBPF programs must be GPL compatible to use GPL-ed functions */\n\tif (!env->prog->gpl_compatible && fn->gpl_only) {\n\t\tverbose(env, \"cannot call GPL-restricted function from non-GPL compatible program\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (fn->allowed && !fn->allowed(env->prog)) {\n\t\tverbose(env, \"helper call is not allowed in probe\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* With LD_ABS/IND some JITs save/restore skb from r1. */\n\tchanges_data = bpf_helper_changes_pkt_data(fn->func);\n\tif (changes_data && fn->arg1_type != ARG_PTR_TO_CTX) {\n\t\tverbose(env, \"kernel subsystem misconfigured func %s#%d: r1 != ctx\\n\",\n\t\t\tfunc_id_name(func_id), func_id);\n\t\treturn -EINVAL;\n\t}\n\n\tmemset(&meta, 0, sizeof(meta));\n\tmeta.pkt_access = fn->pkt_access;\n\n\terr = check_func_proto(fn, func_id);\n\tif (err) {\n\t\tverbose(env, \"kernel subsystem misconfigured func %s#%d\\n\",\n\t\t\tfunc_id_name(func_id), func_id);\n\t\treturn err;\n\t}\n\n\tmeta.func_id = func_id;\n\t/* check args */\n\tfor (i = 0; i < MAX_BPF_FUNC_REG_ARGS; i++) {\n\t\terr = check_func_arg(env, i, &meta, fn);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = record_func_map(env, &meta, func_id, insn_idx);\n\tif (err)\n\t\treturn err;\n\n\terr = record_func_key(env, &meta, func_id, insn_idx);\n\tif (err)\n\t\treturn err;\n\n\t/* Mark slots with STACK_MISC in case of raw mode, stack offset\n\t * is inferred from register state.\n\t */\n\tfor (i = 0; i < meta.access_size; i++) {\n\t\terr = check_mem_access(env, insn_idx, meta.regno, i, BPF_B,\n\t\t\t\t       BPF_WRITE, -1, false);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (is_release_function(func_id)) {\n\t\terr = release_reference(env, meta.ref_obj_id);\n\t\tif (err) {\n\t\t\tverbose(env, \"func %s#%d reference has not been acquired before\\n\",\n\t\t\t\tfunc_id_name(func_id), func_id);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tregs = cur_regs(env);\n\n\tswitch (func_id) {\n\tcase BPF_FUNC_tail_call:\n\t\terr = check_reference_leak(env);\n\t\tif (err) {\n\t\t\tverbose(env, \"tail_call would lead to reference leak\\n\");\n\t\t\treturn err;\n\t\t}\n\t\tbreak;\n\tcase BPF_FUNC_get_local_storage:\n\t\t/* check that flags argument in get_local_storage(map, flags) is 0,\n\t\t * this is required because get_local_storage() can't return an error.\n\t\t */\n\t\tif (!register_is_null(&regs[BPF_REG_2])) {\n\t\t\tverbose(env, \"get_local_storage() doesn't support non-zero flags\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tcase BPF_FUNC_for_each_map_elem:\n\t\terr = __check_func_call(env, insn, insn_idx_p, meta.subprogno,\n\t\t\t\t\tset_map_elem_callback_state);\n\t\tbreak;\n\tcase BPF_FUNC_timer_set_callback:\n\t\terr = __check_func_call(env, insn, insn_idx_p, meta.subprogno,\n\t\t\t\t\tset_timer_callback_state);\n\t\tbreak;\n\tcase BPF_FUNC_find_vma:\n\t\terr = __check_func_call(env, insn, insn_idx_p, meta.subprogno,\n\t\t\t\t\tset_find_vma_callback_state);\n\t\tbreak;\n\tcase BPF_FUNC_snprintf:\n\t\terr = check_bpf_snprintf_call(env, regs);\n\t\tbreak;\n\tcase BPF_FUNC_loop:\n\t\terr = __check_func_call(env, insn, insn_idx_p, meta.subprogno,\n\t\t\t\t\tset_loop_callback_state);\n\t\tbreak;\n\t}\n\n\tif (err)\n\t\treturn err;\n\n\t/* reset caller saved regs */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* helper call returns 64-bit value. */\n\tregs[BPF_REG_0].subreg_def = DEF_NOT_SUBREG;\n\n\t/* update return register (already marked as written above) */\n\tret_type = fn->ret_type;\n\tret_flag = type_flag(fn->ret_type);\n\tif (ret_type == RET_INTEGER) {\n\t\t/* sets type to SCALAR_VALUE */\n\t\tmark_reg_unknown(env, regs, BPF_REG_0);\n\t} else if (ret_type == RET_VOID) {\n\t\tregs[BPF_REG_0].type = NOT_INIT;\n\t} else if (base_type(ret_type) == RET_PTR_TO_MAP_VALUE) {\n\t\t/* There is no offset yet applied, variable or fixed */\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\t/* remember map_ptr, so that check_map_access()\n\t\t * can check 'value_size' boundary of memory access\n\t\t * to map element returned from bpf_map_lookup_elem()\n\t\t */\n\t\tif (meta.map_ptr == NULL) {\n\t\t\tverbose(env,\n\t\t\t\t\"kernel subsystem misconfigured verifier\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tregs[BPF_REG_0].map_ptr = meta.map_ptr;\n\t\tregs[BPF_REG_0].map_uid = meta.map_uid;\n\t\tregs[BPF_REG_0].type = PTR_TO_MAP_VALUE | ret_flag;\n\t\tif (!type_may_be_null(ret_type) &&\n\t\t    map_value_has_spin_lock(meta.map_ptr)) {\n\t\t\tregs[BPF_REG_0].id = ++env->id_gen;\n\t\t}\n\t} else if (base_type(ret_type) == RET_PTR_TO_SOCKET) {\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_SOCKET | ret_flag;\n\t} else if (base_type(ret_type) == RET_PTR_TO_SOCK_COMMON) {\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_SOCK_COMMON | ret_flag;\n\t} else if (base_type(ret_type) == RET_PTR_TO_TCP_SOCK) {\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_TCP_SOCK | ret_flag;\n\t} else if (base_type(ret_type) == RET_PTR_TO_ALLOC_MEM) {\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_MEM | ret_flag;\n\t\tregs[BPF_REG_0].mem_size = meta.mem_size;\n\t} else if (base_type(ret_type) == RET_PTR_TO_MEM_OR_BTF_ID) {\n\t\tconst struct btf_type *t;\n\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tt = btf_type_skip_modifiers(meta.ret_btf, meta.ret_btf_id, NULL);\n\t\tif (!btf_type_is_struct(t)) {\n\t\t\tu32 tsize;\n\t\t\tconst struct btf_type *ret;\n\t\t\tconst char *tname;\n\n\t\t\t/* resolve the type size of ksym. */\n\t\t\tret = btf_resolve_size(meta.ret_btf, t, &tsize);\n\t\t\tif (IS_ERR(ret)) {\n\t\t\t\ttname = btf_name_by_offset(meta.ret_btf, t->name_off);\n\t\t\t\tverbose(env, \"unable to resolve the size of type '%s': %ld\\n\",\n\t\t\t\t\ttname, PTR_ERR(ret));\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tregs[BPF_REG_0].type = PTR_TO_MEM | ret_flag;\n\t\t\tregs[BPF_REG_0].mem_size = tsize;\n\t\t} else {\n\t\t\tregs[BPF_REG_0].type = PTR_TO_BTF_ID | ret_flag;\n\t\t\tregs[BPF_REG_0].btf = meta.ret_btf;\n\t\t\tregs[BPF_REG_0].btf_id = meta.ret_btf_id;\n\t\t}\n\t} else if (base_type(ret_type) == RET_PTR_TO_BTF_ID) {\n\t\tint ret_btf_id;\n\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_BTF_ID | ret_flag;\n\t\tret_btf_id = *fn->ret_btf_id;\n\t\tif (ret_btf_id == 0) {\n\t\t\tverbose(env, \"invalid return type %u of func %s#%d\\n\",\n\t\t\t\tbase_type(ret_type), func_id_name(func_id),\n\t\t\t\tfunc_id);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t/* current BPF helper definitions are only coming from\n\t\t * built-in code with type IDs from  vmlinux BTF\n\t\t */\n\t\tregs[BPF_REG_0].btf = btf_vmlinux;\n\t\tregs[BPF_REG_0].btf_id = ret_btf_id;\n\t} else {\n\t\tverbose(env, \"unknown return type %u of func %s#%d\\n\",\n\t\t\tbase_type(ret_type), func_id_name(func_id), func_id);\n\t\treturn -EINVAL;\n\t}\n\n\tif (type_may_be_null(regs[BPF_REG_0].type))\n\t\tregs[BPF_REG_0].id = ++env->id_gen;\n\n\tif (is_ptr_cast_function(func_id)) {\n\t\t/* For release_reference() */\n\t\tregs[BPF_REG_0].ref_obj_id = meta.ref_obj_id;\n\t} else if (is_acquire_function(func_id, meta.map_ptr)) {\n\t\tint id = acquire_reference_state(env, insn_idx);\n\n\t\tif (id < 0)\n\t\t\treturn id;\n\t\t/* For mark_ptr_or_null_reg() */\n\t\tregs[BPF_REG_0].id = id;\n\t\t/* For release_reference() */\n\t\tregs[BPF_REG_0].ref_obj_id = id;\n\t}\n\n\tdo_refine_retval_range(regs, fn->ret_type, func_id, &meta);\n\n\terr = check_map_func_compatibility(env, meta.map_ptr, func_id);\n\tif (err)\n\t\treturn err;\n\n\tif ((func_id == BPF_FUNC_get_stack ||\n\t     func_id == BPF_FUNC_get_task_stack) &&\n\t    !env->prog->has_callchain_buf) {\n\t\tconst char *err_str;\n\n#ifdef CONFIG_PERF_EVENTS\n\t\terr = get_callchain_buffers(sysctl_perf_event_max_stack);\n\t\terr_str = \"cannot get callchain buffer for func %s#%d\\n\";\n#else\n\t\terr = -ENOTSUPP;\n\t\terr_str = \"func %s#%d not supported without CONFIG_PERF_EVENTS\\n\";\n#endif\n\t\tif (err) {\n\t\t\tverbose(env, err_str, func_id_name(func_id), func_id);\n\t\t\treturn err;\n\t\t}\n\n\t\tenv->prog->has_callchain_buf = true;\n\t}\n\n\tif (func_id == BPF_FUNC_get_stackid || func_id == BPF_FUNC_get_stack)\n\t\tenv->prog->call_get_stack = true;\n\n\tif (func_id == BPF_FUNC_get_func_ip) {\n\t\tif (check_get_func_ip(env))\n\t\t\treturn -ENOTSUPP;\n\t\tenv->prog->call_get_func_ip = true;\n\t}\n\n\tif (changes_data)\n\t\tclear_all_pkt_pointers(env);\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct tnum enforce_attach_type_range = tnum_unknown;\n\tconst struct bpf_prog *prog = env->prog;\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\tenum bpf_prog_type prog_type = resolve_prog_type(env->prog);\n\tint err;\n\tstruct bpf_func_state *frame = env->cur_state->frame[0];\n\tconst bool is_subprog = frame->subprogno;\n\n\t/* LSM and struct_ops func-ptr's return type could be \"void\" */\n\tif (!is_subprog &&\n\t    (prog_type == BPF_PROG_TYPE_STRUCT_OPS ||\n\t     prog_type == BPF_PROG_TYPE_LSM) &&\n\t    !prog->aux->attach_func_proto->type)\n\t\treturn 0;\n\n\t/* eBPF calling convention is such that R0 is used\n\t * to return the value from eBPF program.\n\t * Make sure that it's readable at this time\n\t * of bpf_exit, which means that program wrote\n\t * something into it earlier\n\t */\n\terr = check_reg_arg(env, BPF_REG_0, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\tverbose(env, \"R0 leaks addr as return value\\n\");\n\t\treturn -EACCES;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\n\tif (frame->in_async_callback_fn) {\n\t\t/* enforce return zero from async callbacks like timer */\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"In async callback the register R0 is not a known value (%s)\\n\",\n\t\t\t\treg_type_str[reg->type]);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!tnum_in(tnum_const(0), reg->var_off)) {\n\t\t\tverbose_invalid_scalar(env, reg, &range, \"async callback\", \"R0\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (is_subprog) {\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"At subprogram exit the register R0 is not a scalar value (%s)\\n\",\n\t\t\t\treg_type_str[reg->type]);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tswitch (prog_type) {\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_UDP4_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_UDP6_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETSOCKNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETSOCKNAME)\n\t\t\trange = tnum_range(1, 1);\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET4_BIND ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_BIND)\n\t\t\trange = tnum_range(0, 3);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET_EGRESS) {\n\t\t\trange = tnum_range(0, 3);\n\t\t\tenforce_attach_type_range = tnum_range(2, 3);\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\tcase BPF_PROG_TYPE_CGROUP_SYSCTL:\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\t\tbreak;\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT:\n\t\tif (!env->prog->aux->attach_btf_id)\n\t\t\treturn 0;\n\t\trange = tnum_const(0);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_TRACING:\n\t\tswitch (env->prog->expected_attach_type) {\n\t\tcase BPF_TRACE_FENTRY:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\trange = tnum_const(0);\n\t\t\tbreak;\n\t\tcase BPF_TRACE_RAW_TP:\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\treturn 0;\n\t\tcase BPF_TRACE_ITER:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -ENOTSUPP;\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_SK_LOOKUP:\n\t\trange = tnum_range(SK_DROP, SK_PASS);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_EXT:\n\t\t/* freplace program can return anything as its return value\n\t\t * depends on the to-be-replaced kernel func or bpf program.\n\t\t */\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose_invalid_scalar(env, reg, &range, \"program exit\", \"R0\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_is_unknown(enforce_attach_type_range) &&\n\t    tnum_in(enforce_attach_type_range, reg->var_off))\n\t\tenv->prog->enforce_expected_attach_type = 1;\n\treturn 0;\n}",
                        "code_after_change": "static int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct tnum enforce_attach_type_range = tnum_unknown;\n\tconst struct bpf_prog *prog = env->prog;\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\tenum bpf_prog_type prog_type = resolve_prog_type(env->prog);\n\tint err;\n\tstruct bpf_func_state *frame = env->cur_state->frame[0];\n\tconst bool is_subprog = frame->subprogno;\n\n\t/* LSM and struct_ops func-ptr's return type could be \"void\" */\n\tif (!is_subprog &&\n\t    (prog_type == BPF_PROG_TYPE_STRUCT_OPS ||\n\t     prog_type == BPF_PROG_TYPE_LSM) &&\n\t    !prog->aux->attach_func_proto->type)\n\t\treturn 0;\n\n\t/* eBPF calling convention is such that R0 is used\n\t * to return the value from eBPF program.\n\t * Make sure that it's readable at this time\n\t * of bpf_exit, which means that program wrote\n\t * something into it earlier\n\t */\n\terr = check_reg_arg(env, BPF_REG_0, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\tverbose(env, \"R0 leaks addr as return value\\n\");\n\t\treturn -EACCES;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\n\tif (frame->in_async_callback_fn) {\n\t\t/* enforce return zero from async callbacks like timer */\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"In async callback the register R0 is not a known value (%s)\\n\",\n\t\t\t\treg_type_str(env, reg->type));\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!tnum_in(tnum_const(0), reg->var_off)) {\n\t\t\tverbose_invalid_scalar(env, reg, &range, \"async callback\", \"R0\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (is_subprog) {\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"At subprogram exit the register R0 is not a scalar value (%s)\\n\",\n\t\t\t\treg_type_str(env, reg->type));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tswitch (prog_type) {\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_UDP4_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_UDP6_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETSOCKNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETSOCKNAME)\n\t\t\trange = tnum_range(1, 1);\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET4_BIND ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_BIND)\n\t\t\trange = tnum_range(0, 3);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET_EGRESS) {\n\t\t\trange = tnum_range(0, 3);\n\t\t\tenforce_attach_type_range = tnum_range(2, 3);\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\tcase BPF_PROG_TYPE_CGROUP_SYSCTL:\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\t\tbreak;\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT:\n\t\tif (!env->prog->aux->attach_btf_id)\n\t\t\treturn 0;\n\t\trange = tnum_const(0);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_TRACING:\n\t\tswitch (env->prog->expected_attach_type) {\n\t\tcase BPF_TRACE_FENTRY:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\trange = tnum_const(0);\n\t\t\tbreak;\n\t\tcase BPF_TRACE_RAW_TP:\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\treturn 0;\n\t\tcase BPF_TRACE_ITER:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -ENOTSUPP;\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_SK_LOOKUP:\n\t\trange = tnum_range(SK_DROP, SK_PASS);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_EXT:\n\t\t/* freplace program can return anything as its return value\n\t\t * depends on the to-be-replaced kernel func or bpf program.\n\t\t */\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str(env, reg->type));\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose_invalid_scalar(env, reg, &range, \"program exit\", \"R0\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_is_unknown(enforce_attach_type_range) &&\n\t    tnum_in(enforce_attach_type_range, reg->var_off))\n\t\tenv->prog->enforce_expected_attach_type = 1;\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_KEY:\n\t\treturn check_mem_region_access(env, regno, reg->off, access_size,\n\t\t\t\t\t       reg->map_ptr->key_size, false);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tcase PTR_TO_STACK:\n\t\treturn check_stack_range_initialized(\n\t\t\t\tenv,\n\t\t\t\tregno, reg->off, access_size,\n\t\t\t\tzero_size_allowed, ACCESS_HELPER, meta);\n\tdefault: /* scalar_value or invalid ptr */\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s expected=%s\\n\", regno,\n\t\t\treg_type_str[reg->type],\n\t\t\treg_type_str[PTR_TO_STACK]);\n\t\treturn -EACCES;\n\t}\n}",
                        "code_after_change": "static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_KEY:\n\t\treturn check_mem_region_access(env, regno, reg->off, access_size,\n\t\t\t\t\t       reg->map_ptr->key_size, false);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tcase PTR_TO_STACK:\n\t\treturn check_stack_range_initialized(\n\t\t\t\tenv,\n\t\t\t\tregno, reg->off, access_size,\n\t\t\t\tzero_size_allowed, ACCESS_HELPER, meta);\n\tdefault: /* scalar_value or invalid ptr */\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s \", regno,\n\t\t\treg_type_str(env, reg->type));\n\t\tverbose(env, \"expected=%s\\n\", reg_type_str(env, PTR_TO_STACK));\n\t\treturn -EACCES;\n\t}\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "code_after_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tif (ptr_reg->type & PTR_MAYBE_NULL) {\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\t}\n\n\tswitch (base_type(ptr_reg->type)) {\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state,\n\t\t\t\t bool print_all)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tif (!print_all && !reg_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\tverbose(env, \"P\");\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tif (t == PTR_TO_BTF_ID ||\n\t\t\t    t == PTR_TO_BTF_ID_OR_NULL ||\n\t\t\t    t == PTR_TO_PERCPU_BTF_ID)\n\t\t\t\tverbose(env, \"%s\", kernel_type_name(reg->btf, reg->btf_id));\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (reg_type_may_be_refcounted_or_null(t))\n\t\t\t\tverbose(env, \",ref_obj_id=%d\", reg->ref_obj_id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (t == CONST_PTR_TO_MAP ||\n\t\t\t\t t == PTR_TO_MAP_KEY ||\n\t\t\t\t t == PTR_TO_MAP_VALUE ||\n\t\t\t\t t == PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t\tif (reg->s32_min_value != reg->smin_value &&\n\t\t\t\t    reg->s32_min_value != S32_MIN)\n\t\t\t\t\tverbose(env, \",s32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_min_value));\n\t\t\t\tif (reg->s32_max_value != reg->smax_value &&\n\t\t\t\t    reg->s32_max_value != S32_MAX)\n\t\t\t\t\tverbose(env, \",s32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_max_value));\n\t\t\t\tif (reg->u32_min_value != reg->umin_value &&\n\t\t\t\t    reg->u32_min_value != U32_MIN)\n\t\t\t\t\tverbose(env, \",u32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_min_value));\n\t\t\t\tif (reg->u32_max_value != reg->umax_value &&\n\t\t\t\t    reg->u32_max_value != U32_MAX)\n\t\t\t\t\tverbose(env, \",u32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_max_value));\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tif (!print_all && !stack_slot_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (is_spilled_reg(&state->stack[i])) {\n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tt = reg->type;\n\t\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\t\tverbose(env, \"P\");\n\t\t\tif (t == SCALAR_VALUE && tnum_is_const(reg->var_off))\n\t\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t\t}\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tif (state->in_callback_fn)\n\t\tverbose(env, \" cb\");\n\tif (state->in_async_callback_fn)\n\t\tverbose(env, \" async_cb\");\n\tverbose(env, \"\\n\");\n\tmark_verifier_state_clean(env);\n}",
                        "code_after_change": "static void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state,\n\t\t\t\t bool print_all)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tif (!print_all && !reg_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str(env, t));\n\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\tverbose(env, \"P\");\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tif (base_type(t) == PTR_TO_BTF_ID ||\n\t\t\t    base_type(t) == PTR_TO_PERCPU_BTF_ID)\n\t\t\t\tverbose(env, \"%s\", kernel_type_name(reg->btf, reg->btf_id));\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (reg_type_may_be_refcounted_or_null(t))\n\t\t\t\tverbose(env, \",ref_obj_id=%d\", reg->ref_obj_id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (base_type(t) == CONST_PTR_TO_MAP ||\n\t\t\t\t base_type(t) == PTR_TO_MAP_KEY ||\n\t\t\t\t base_type(t) == PTR_TO_MAP_VALUE)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t\tif (reg->s32_min_value != reg->smin_value &&\n\t\t\t\t    reg->s32_min_value != S32_MIN)\n\t\t\t\t\tverbose(env, \",s32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_min_value));\n\t\t\t\tif (reg->s32_max_value != reg->smax_value &&\n\t\t\t\t    reg->s32_max_value != S32_MAX)\n\t\t\t\t\tverbose(env, \",s32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_max_value));\n\t\t\t\tif (reg->u32_min_value != reg->umin_value &&\n\t\t\t\t    reg->u32_min_value != U32_MIN)\n\t\t\t\t\tverbose(env, \",u32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_min_value));\n\t\t\t\tif (reg->u32_max_value != reg->umax_value &&\n\t\t\t\t    reg->u32_max_value != U32_MAX)\n\t\t\t\t\tverbose(env, \",u32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_max_value));\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tif (!print_all && !stack_slot_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (is_spilled_reg(&state->stack[i])) {\n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tt = reg->type;\n\t\t\tverbose(env, \"=%s\", reg_type_str(env, t));\n\t\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\t\tverbose(env, \"P\");\n\t\t\tif (t == SCALAR_VALUE && tnum_is_const(reg->var_off))\n\t\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t\t}\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tif (state->in_callback_fn)\n\t\tverbose(env, \" cb\");\n\tif (state->in_async_callback_fn)\n\t\tverbose(env, \" async_cb\");\n\tverbose(env, \"\\n\");\n\tmark_verifier_state_clean(env);\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "bool btf_ctx_access(int off, int size, enum bpf_access_type type,\n\t\t    const struct bpf_prog *prog,\n\t\t    struct bpf_insn_access_aux *info)\n{\n\tconst struct btf_type *t = prog->aux->attach_func_proto;\n\tstruct bpf_prog *tgt_prog = prog->aux->dst_prog;\n\tstruct btf *btf = bpf_prog_get_target_btf(prog);\n\tconst char *tname = prog->aux->attach_func_name;\n\tstruct bpf_verifier_log *log = info->log;\n\tconst struct btf_param *args;\n\tu32 nr_args, arg;\n\tint i, ret;\n\n\tif (off % 8) {\n\t\tbpf_log(log, \"func '%s' offset %d is not multiple of 8\\n\",\n\t\t\ttname, off);\n\t\treturn false;\n\t}\n\targ = off / 8;\n\targs = (const struct btf_param *)(t + 1);\n\t/* if (t == NULL) Fall back to default BPF prog with\n\t * MAX_BPF_FUNC_REG_ARGS u64 arguments.\n\t */\n\tnr_args = t ? btf_type_vlen(t) : MAX_BPF_FUNC_REG_ARGS;\n\tif (prog->aux->attach_btf_trace) {\n\t\t/* skip first 'void *__data' argument in btf_trace_##name typedef */\n\t\targs++;\n\t\tnr_args--;\n\t}\n\n\tif (arg > nr_args) {\n\t\tbpf_log(log, \"func '%s' doesn't have %d-th argument\\n\",\n\t\t\ttname, arg + 1);\n\t\treturn false;\n\t}\n\n\tif (arg == nr_args) {\n\t\tswitch (prog->expected_attach_type) {\n\t\tcase BPF_LSM_MAC:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\t/* When LSM programs are attached to void LSM hooks\n\t\t\t * they use FEXIT trampolines and when attached to\n\t\t\t * int LSM hooks, they use MODIFY_RETURN trampolines.\n\t\t\t *\n\t\t\t * While the LSM programs are BPF_MODIFY_RETURN-like\n\t\t\t * the check:\n\t\t\t *\n\t\t\t *\tif (ret_type != 'int')\n\t\t\t *\t\treturn -EINVAL;\n\t\t\t *\n\t\t\t * is _not_ done here. This is still safe as LSM hooks\n\t\t\t * have only void and int return types.\n\t\t\t */\n\t\t\tif (!t)\n\t\t\t\treturn true;\n\t\t\tt = btf_type_by_id(btf, t->type);\n\t\t\tbreak;\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\t/* For now the BPF_MODIFY_RETURN can only be attached to\n\t\t\t * functions that return an int.\n\t\t\t */\n\t\t\tif (!t)\n\t\t\t\treturn false;\n\n\t\t\tt = btf_type_skip_modifiers(btf, t->type, NULL);\n\t\t\tif (!btf_type_is_small_int(t)) {\n\t\t\t\tbpf_log(log,\n\t\t\t\t\t\"ret type %s not allowed for fmod_ret\\n\",\n\t\t\t\t\tbtf_kind_str[BTF_INFO_KIND(t->info)]);\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbpf_log(log, \"func '%s' doesn't have %d-th argument\\n\",\n\t\t\t\ttname, arg + 1);\n\t\t\treturn false;\n\t\t}\n\t} else {\n\t\tif (!t)\n\t\t\t/* Default prog with MAX_BPF_FUNC_REG_ARGS args */\n\t\t\treturn true;\n\t\tt = btf_type_by_id(btf, args[arg].type);\n\t}\n\n\t/* skip modifiers */\n\twhile (btf_type_is_modifier(t))\n\t\tt = btf_type_by_id(btf, t->type);\n\tif (btf_type_is_small_int(t) || btf_type_is_enum(t))\n\t\t/* accessing a scalar */\n\t\treturn true;\n\tif (!btf_type_is_ptr(t)) {\n\t\tbpf_log(log,\n\t\t\t\"func '%s' arg%d '%s' has type %s. Only pointer access is allowed\\n\",\n\t\t\ttname, arg,\n\t\t\t__btf_name_by_offset(btf, t->name_off),\n\t\t\tbtf_kind_str[BTF_INFO_KIND(t->info)]);\n\t\treturn false;\n\t}\n\n\t/* check for PTR_TO_RDONLY_BUF_OR_NULL or PTR_TO_RDWR_BUF_OR_NULL */\n\tfor (i = 0; i < prog->aux->ctx_arg_info_size; i++) {\n\t\tconst struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i];\n\n\t\tif (ctx_arg_info->offset == off &&\n\t\t    (ctx_arg_info->reg_type == PTR_TO_RDONLY_BUF_OR_NULL ||\n\t\t     ctx_arg_info->reg_type == PTR_TO_RDWR_BUF_OR_NULL)) {\n\t\t\tinfo->reg_type = ctx_arg_info->reg_type;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tif (t->type == 0)\n\t\t/* This is a pointer to void.\n\t\t * It is the same as scalar from the verifier safety pov.\n\t\t * No further pointer walking is allowed.\n\t\t */\n\t\treturn true;\n\n\tif (is_int_ptr(btf, t))\n\t\treturn true;\n\n\t/* this is a pointer to another type */\n\tfor (i = 0; i < prog->aux->ctx_arg_info_size; i++) {\n\t\tconst struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i];\n\n\t\tif (ctx_arg_info->offset == off) {\n\t\t\tif (!ctx_arg_info->btf_id) {\n\t\t\t\tbpf_log(log,\"invalid btf_id for context argument offset %u\\n\", off);\n\t\t\t\treturn false;\n\t\t\t}\n\n\t\t\tinfo->reg_type = ctx_arg_info->reg_type;\n\t\t\tinfo->btf = btf_vmlinux;\n\t\t\tinfo->btf_id = ctx_arg_info->btf_id;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tinfo->reg_type = PTR_TO_BTF_ID;\n\tif (tgt_prog) {\n\t\tenum bpf_prog_type tgt_type;\n\n\t\tif (tgt_prog->type == BPF_PROG_TYPE_EXT)\n\t\t\ttgt_type = tgt_prog->aux->saved_dst_prog_type;\n\t\telse\n\t\t\ttgt_type = tgt_prog->type;\n\n\t\tret = btf_translate_to_vmlinux(log, btf, t, tgt_type, arg);\n\t\tif (ret > 0) {\n\t\t\tinfo->btf = btf_vmlinux;\n\t\t\tinfo->btf_id = ret;\n\t\t\treturn true;\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tinfo->btf = btf;\n\tinfo->btf_id = t->type;\n\tt = btf_type_by_id(btf, t->type);\n\t/* skip modifiers */\n\twhile (btf_type_is_modifier(t)) {\n\t\tinfo->btf_id = t->type;\n\t\tt = btf_type_by_id(btf, t->type);\n\t}\n\tif (!btf_type_is_struct(t)) {\n\t\tbpf_log(log,\n\t\t\t\"func '%s' arg%d type %s is not a struct\\n\",\n\t\t\ttname, arg, btf_kind_str[BTF_INFO_KIND(t->info)]);\n\t\treturn false;\n\t}\n\tbpf_log(log, \"func '%s' arg%d has btf_id %d type %s '%s'\\n\",\n\t\ttname, arg, info->btf_id, btf_kind_str[BTF_INFO_KIND(t->info)],\n\t\t__btf_name_by_offset(btf, t->name_off));\n\treturn true;\n}",
                        "code_after_change": "bool btf_ctx_access(int off, int size, enum bpf_access_type type,\n\t\t    const struct bpf_prog *prog,\n\t\t    struct bpf_insn_access_aux *info)\n{\n\tconst struct btf_type *t = prog->aux->attach_func_proto;\n\tstruct bpf_prog *tgt_prog = prog->aux->dst_prog;\n\tstruct btf *btf = bpf_prog_get_target_btf(prog);\n\tconst char *tname = prog->aux->attach_func_name;\n\tstruct bpf_verifier_log *log = info->log;\n\tconst struct btf_param *args;\n\tu32 nr_args, arg;\n\tint i, ret;\n\n\tif (off % 8) {\n\t\tbpf_log(log, \"func '%s' offset %d is not multiple of 8\\n\",\n\t\t\ttname, off);\n\t\treturn false;\n\t}\n\targ = off / 8;\n\targs = (const struct btf_param *)(t + 1);\n\t/* if (t == NULL) Fall back to default BPF prog with\n\t * MAX_BPF_FUNC_REG_ARGS u64 arguments.\n\t */\n\tnr_args = t ? btf_type_vlen(t) : MAX_BPF_FUNC_REG_ARGS;\n\tif (prog->aux->attach_btf_trace) {\n\t\t/* skip first 'void *__data' argument in btf_trace_##name typedef */\n\t\targs++;\n\t\tnr_args--;\n\t}\n\n\tif (arg > nr_args) {\n\t\tbpf_log(log, \"func '%s' doesn't have %d-th argument\\n\",\n\t\t\ttname, arg + 1);\n\t\treturn false;\n\t}\n\n\tif (arg == nr_args) {\n\t\tswitch (prog->expected_attach_type) {\n\t\tcase BPF_LSM_MAC:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\t/* When LSM programs are attached to void LSM hooks\n\t\t\t * they use FEXIT trampolines and when attached to\n\t\t\t * int LSM hooks, they use MODIFY_RETURN trampolines.\n\t\t\t *\n\t\t\t * While the LSM programs are BPF_MODIFY_RETURN-like\n\t\t\t * the check:\n\t\t\t *\n\t\t\t *\tif (ret_type != 'int')\n\t\t\t *\t\treturn -EINVAL;\n\t\t\t *\n\t\t\t * is _not_ done here. This is still safe as LSM hooks\n\t\t\t * have only void and int return types.\n\t\t\t */\n\t\t\tif (!t)\n\t\t\t\treturn true;\n\t\t\tt = btf_type_by_id(btf, t->type);\n\t\t\tbreak;\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\t/* For now the BPF_MODIFY_RETURN can only be attached to\n\t\t\t * functions that return an int.\n\t\t\t */\n\t\t\tif (!t)\n\t\t\t\treturn false;\n\n\t\t\tt = btf_type_skip_modifiers(btf, t->type, NULL);\n\t\t\tif (!btf_type_is_small_int(t)) {\n\t\t\t\tbpf_log(log,\n\t\t\t\t\t\"ret type %s not allowed for fmod_ret\\n\",\n\t\t\t\t\tbtf_kind_str[BTF_INFO_KIND(t->info)]);\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbpf_log(log, \"func '%s' doesn't have %d-th argument\\n\",\n\t\t\t\ttname, arg + 1);\n\t\t\treturn false;\n\t\t}\n\t} else {\n\t\tif (!t)\n\t\t\t/* Default prog with MAX_BPF_FUNC_REG_ARGS args */\n\t\t\treturn true;\n\t\tt = btf_type_by_id(btf, args[arg].type);\n\t}\n\n\t/* skip modifiers */\n\twhile (btf_type_is_modifier(t))\n\t\tt = btf_type_by_id(btf, t->type);\n\tif (btf_type_is_small_int(t) || btf_type_is_enum(t))\n\t\t/* accessing a scalar */\n\t\treturn true;\n\tif (!btf_type_is_ptr(t)) {\n\t\tbpf_log(log,\n\t\t\t\"func '%s' arg%d '%s' has type %s. Only pointer access is allowed\\n\",\n\t\t\ttname, arg,\n\t\t\t__btf_name_by_offset(btf, t->name_off),\n\t\t\tbtf_kind_str[BTF_INFO_KIND(t->info)]);\n\t\treturn false;\n\t}\n\n\t/* check for PTR_TO_RDONLY_BUF_OR_NULL or PTR_TO_RDWR_BUF_OR_NULL */\n\tfor (i = 0; i < prog->aux->ctx_arg_info_size; i++) {\n\t\tconst struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i];\n\t\tu32 type, flag;\n\n\t\ttype = base_type(ctx_arg_info->reg_type);\n\t\tflag = type_flag(ctx_arg_info->reg_type);\n\t\tif (ctx_arg_info->offset == off &&\n\t\t    (type == PTR_TO_RDWR_BUF || type == PTR_TO_RDONLY_BUF) &&\n\t\t    (flag & PTR_MAYBE_NULL)) {\n\t\t\tinfo->reg_type = ctx_arg_info->reg_type;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tif (t->type == 0)\n\t\t/* This is a pointer to void.\n\t\t * It is the same as scalar from the verifier safety pov.\n\t\t * No further pointer walking is allowed.\n\t\t */\n\t\treturn true;\n\n\tif (is_int_ptr(btf, t))\n\t\treturn true;\n\n\t/* this is a pointer to another type */\n\tfor (i = 0; i < prog->aux->ctx_arg_info_size; i++) {\n\t\tconst struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i];\n\n\t\tif (ctx_arg_info->offset == off) {\n\t\t\tif (!ctx_arg_info->btf_id) {\n\t\t\t\tbpf_log(log,\"invalid btf_id for context argument offset %u\\n\", off);\n\t\t\t\treturn false;\n\t\t\t}\n\n\t\t\tinfo->reg_type = ctx_arg_info->reg_type;\n\t\t\tinfo->btf = btf_vmlinux;\n\t\t\tinfo->btf_id = ctx_arg_info->btf_id;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tinfo->reg_type = PTR_TO_BTF_ID;\n\tif (tgt_prog) {\n\t\tenum bpf_prog_type tgt_type;\n\n\t\tif (tgt_prog->type == BPF_PROG_TYPE_EXT)\n\t\t\ttgt_type = tgt_prog->aux->saved_dst_prog_type;\n\t\telse\n\t\t\ttgt_type = tgt_prog->type;\n\n\t\tret = btf_translate_to_vmlinux(log, btf, t, tgt_type, arg);\n\t\tif (ret > 0) {\n\t\t\tinfo->btf = btf_vmlinux;\n\t\t\tinfo->btf_id = ret;\n\t\t\treturn true;\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tinfo->btf = btf;\n\tinfo->btf_id = t->type;\n\tt = btf_type_by_id(btf, t->type);\n\t/* skip modifiers */\n\twhile (btf_type_is_modifier(t)) {\n\t\tinfo->btf_id = t->type;\n\t\tt = btf_type_by_id(btf, t->type);\n\t}\n\tif (!btf_type_is_struct(t)) {\n\t\tbpf_log(log,\n\t\t\t\"func '%s' arg%d type %s is not a struct\\n\",\n\t\t\ttname, arg, btf_kind_str[BTF_INFO_KIND(t->info)]);\n\t\treturn false;\n\t}\n\tbpf_log(log, \"func '%s' arg%d has btf_id %d type %s '%s'\\n\",\n\t\ttname, arg, info->btf_id, btf_kind_str[BTF_INFO_KIND(t->info)],\n\t\t__btf_name_by_offset(btf, t->name_off));\n\treturn true;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 3578,
            "cve_id": "CVE-2022-23222",
            "code_snippet": "static int mark_reg_read(struct bpf_verifier_env *env,\n\t\t\t const struct bpf_reg_state *state,\n\t\t\t struct bpf_reg_state *parent, u8 flag)\n{\n\tbool writes = parent == state->parent; /* Observe write marks */\n\tint cnt = 0;\n\n\twhile (parent) {\n\t\t/* if read wasn't screened by an earlier write ... */\n\t\tif (writes && state->live & REG_LIVE_WRITTEN)\n\t\t\tbreak;\n\t\tif (parent->live & REG_LIVE_DONE) {\n\t\t\tverbose(env, \"verifier BUG type %s var_off %lld off %d\\n\",\n\t\t\t\treg_type_str(env, parent->type),\n\t\t\t\tparent->var_off.value, parent->off);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\t/* The first condition is more likely to be true than the\n\t\t * second, checked it first.\n\t\t */\n\t\tif ((parent->live & REG_LIVE_READ) == flag ||\n\t\t    parent->live & REG_LIVE_READ64)\n\t\t\t/* The parentage chain never changes and\n\t\t\t * this parent was already marked as LIVE_READ.\n\t\t\t * There is no need to keep walking the chain again and\n\t\t\t * keep re-marking all parents as LIVE_READ.\n\t\t\t * This case happens when the same register is read\n\t\t\t * multiple times without writes into it in-between.\n\t\t\t * Also, if parent has the stronger REG_LIVE_READ64 set,\n\t\t\t * then no need to set the weak REG_LIVE_READ32.\n\t\t\t */\n\t\t\tbreak;\n\t\t/* ... then we depend on parent's value */\n\t\tparent->live |= flag;\n\t\t/* REG_LIVE_READ64 overrides REG_LIVE_READ32. */\n\t\tif (flag == REG_LIVE_READ64)\n\t\t\tparent->live &= ~REG_LIVE_READ32;\n\t\tstate = parent;\n\t\tparent = state->parent;\n\t\twrites = true;\n\t\tcnt++;\n\t}\n\n\tif (env->longest_mark_read_walk < cnt)\n\t\tenv->longest_mark_read_walk = cnt;\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static bool regsafe(struct bpf_verifier_env *env, struct bpf_reg_state *rold,\n\t\t    struct bpf_reg_state *rcur, struct bpf_id_pair *idmap)\n{\n\tbool equal;\n\n\tif (!(rold->live & REG_LIVE_READ))\n\t\t/* explored state didn't use this */\n\t\treturn true;\n\n\tequal = memcmp(rold, rcur, offsetof(struct bpf_reg_state, parent)) == 0;\n\n\tif (rold->type == PTR_TO_STACK)\n\t\t/* two stack pointers are equal only if they're pointing to\n\t\t * the same stack frame, since fp-8 in foo != fp-8 in bar\n\t\t */\n\t\treturn equal && rold->frameno == rcur->frameno;\n\n\tif (equal)\n\t\treturn true;\n\n\tif (rold->type == NOT_INIT)\n\t\t/* explored state can't have used this */\n\t\treturn true;\n\tif (rcur->type == NOT_INIT)\n\t\treturn false;\n\tswitch (rold->type) {\n\tcase SCALAR_VALUE:\n\t\tif (env->explore_alu_limits)\n\t\t\treturn false;\n\t\tif (rcur->type == SCALAR_VALUE) {\n\t\t\tif (!rold->precise && !rcur->precise)\n\t\t\t\treturn true;\n\t\t\t/* new val must satisfy old val knowledge */\n\t\t\treturn range_within(rold, rcur) &&\n\t\t\t       tnum_in(rold->var_off, rcur->var_off);\n\t\t} else {\n\t\t\t/* We're trying to use a pointer in place of a scalar.\n\t\t\t * Even if the scalar was unbounded, this could lead to\n\t\t\t * pointer leaks because scalars are allowed to leak\n\t\t\t * while pointers are not. We could make this safe in\n\t\t\t * special cases if root is calling us, but it's\n\t\t\t * probably not worth the hassle.\n\t\t\t */\n\t\t\treturn false;\n\t\t}\n\tcase PTR_TO_MAP_KEY:\n\tcase PTR_TO_MAP_VALUE:\n\t\t/* If the new min/max/var_off satisfy the old ones and\n\t\t * everything else matches, we are OK.\n\t\t * 'id' is not compared, since it's only used for maps with\n\t\t * bpf_spin_lock inside map element and in such cases if\n\t\t * the rest of the prog is valid for one map element then\n\t\t * it's valid for all map elements regardless of the key\n\t\t * used in bpf_map_lookup()\n\t\t */\n\t\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&\n\t\t       range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\t/* a PTR_TO_MAP_VALUE could be safe to use as a\n\t\t * PTR_TO_MAP_VALUE_OR_NULL into the same map.\n\t\t * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-\n\t\t * checked, doing so could have affected others with the same\n\t\t * id, and we can't check for that because we lost the id when\n\t\t * we converted to a PTR_TO_MAP_VALUE.\n\t\t */\n\t\tif (rcur->type != PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\treturn false;\n\t\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)))\n\t\t\treturn false;\n\t\t/* Check our ids match any regs they're supposed to */\n\t\treturn check_ids(rold->id, rcur->id, idmap);\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET:\n\t\tif (rcur->type != rold->type)\n\t\t\treturn false;\n\t\t/* We must have at least as much range as the old ptr\n\t\t * did, so that any accesses which were safe before are\n\t\t * still safe.  This is true even if old range < old off,\n\t\t * since someone could have accessed through (ptr - k), or\n\t\t * even done ptr -= k in a register, to get a safe access.\n\t\t */\n\t\tif (rold->range > rcur->range)\n\t\t\treturn false;\n\t\t/* If the offsets don't match, we can't trust our alignment;\n\t\t * nor can we be sure that we won't fall out of range.\n\t\t */\n\t\tif (rold->off != rcur->off)\n\t\t\treturn false;\n\t\t/* id relations must be preserved */\n\t\tif (rold->id && !check_ids(rold->id, rcur->id, idmap))\n\t\t\treturn false;\n\t\t/* new val must satisfy old val knowledge */\n\t\treturn range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_CTX:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\t/* Only valid matches are exact, which memcmp() above\n\t\t * would have accepted\n\t\t */\n\tdefault:\n\t\t/* Don't know what's going on, just say it's not safe */\n\t\treturn false;\n\t}\n\n\t/* Shouldn't get here; if we do, say it's not safe */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}",
                        "code_after_change": "static bool regsafe(struct bpf_verifier_env *env, struct bpf_reg_state *rold,\n\t\t    struct bpf_reg_state *rcur, struct bpf_id_pair *idmap)\n{\n\tbool equal;\n\n\tif (!(rold->live & REG_LIVE_READ))\n\t\t/* explored state didn't use this */\n\t\treturn true;\n\n\tequal = memcmp(rold, rcur, offsetof(struct bpf_reg_state, parent)) == 0;\n\n\tif (rold->type == PTR_TO_STACK)\n\t\t/* two stack pointers are equal only if they're pointing to\n\t\t * the same stack frame, since fp-8 in foo != fp-8 in bar\n\t\t */\n\t\treturn equal && rold->frameno == rcur->frameno;\n\n\tif (equal)\n\t\treturn true;\n\n\tif (rold->type == NOT_INIT)\n\t\t/* explored state can't have used this */\n\t\treturn true;\n\tif (rcur->type == NOT_INIT)\n\t\treturn false;\n\tswitch (base_type(rold->type)) {\n\tcase SCALAR_VALUE:\n\t\tif (env->explore_alu_limits)\n\t\t\treturn false;\n\t\tif (rcur->type == SCALAR_VALUE) {\n\t\t\tif (!rold->precise && !rcur->precise)\n\t\t\t\treturn true;\n\t\t\t/* new val must satisfy old val knowledge */\n\t\t\treturn range_within(rold, rcur) &&\n\t\t\t       tnum_in(rold->var_off, rcur->var_off);\n\t\t} else {\n\t\t\t/* We're trying to use a pointer in place of a scalar.\n\t\t\t * Even if the scalar was unbounded, this could lead to\n\t\t\t * pointer leaks because scalars are allowed to leak\n\t\t\t * while pointers are not. We could make this safe in\n\t\t\t * special cases if root is calling us, but it's\n\t\t\t * probably not worth the hassle.\n\t\t\t */\n\t\t\treturn false;\n\t\t}\n\tcase PTR_TO_MAP_KEY:\n\tcase PTR_TO_MAP_VALUE:\n\t\t/* a PTR_TO_MAP_VALUE could be safe to use as a\n\t\t * PTR_TO_MAP_VALUE_OR_NULL into the same map.\n\t\t * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-\n\t\t * checked, doing so could have affected others with the same\n\t\t * id, and we can't check for that because we lost the id when\n\t\t * we converted to a PTR_TO_MAP_VALUE.\n\t\t */\n\t\tif (type_may_be_null(rold->type)) {\n\t\t\tif (!type_may_be_null(rcur->type))\n\t\t\t\treturn false;\n\t\t\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)))\n\t\t\t\treturn false;\n\t\t\t/* Check our ids match any regs they're supposed to */\n\t\t\treturn check_ids(rold->id, rcur->id, idmap);\n\t\t}\n\n\t\t/* If the new min/max/var_off satisfy the old ones and\n\t\t * everything else matches, we are OK.\n\t\t * 'id' is not compared, since it's only used for maps with\n\t\t * bpf_spin_lock inside map element and in such cases if\n\t\t * the rest of the prog is valid for one map element then\n\t\t * it's valid for all map elements regardless of the key\n\t\t * used in bpf_map_lookup()\n\t\t */\n\t\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&\n\t\t       range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET:\n\t\tif (rcur->type != rold->type)\n\t\t\treturn false;\n\t\t/* We must have at least as much range as the old ptr\n\t\t * did, so that any accesses which were safe before are\n\t\t * still safe.  This is true even if old range < old off,\n\t\t * since someone could have accessed through (ptr - k), or\n\t\t * even done ptr -= k in a register, to get a safe access.\n\t\t */\n\t\tif (rold->range > rcur->range)\n\t\t\treturn false;\n\t\t/* If the offsets don't match, we can't trust our alignment;\n\t\t * nor can we be sure that we won't fall out of range.\n\t\t */\n\t\tif (rold->off != rcur->off)\n\t\t\treturn false;\n\t\t/* id relations must be preserved */\n\t\tif (rold->id && !check_ids(rold->id, rcur->id, idmap))\n\t\t\treturn false;\n\t\t/* new val must satisfy old val knowledge */\n\t\treturn range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_CTX:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\t\t/* Only valid matches are exact, which memcmp() above\n\t\t * would have accepted\n\t\t */\n\tdefault:\n\t\t/* Don't know what's going on, just say it's not safe */\n\t\treturn false;\n\t}\n\n\t/* Shouldn't get here; if we do, say it's not safe */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "code_after_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tif (ptr_reg->type & PTR_MAYBE_NULL) {\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\t}\n\n\tswitch (base_type(ptr_reg->type)) {\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state,\n\t\t\t\t bool print_all)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tif (!print_all && !reg_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\tverbose(env, \"P\");\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tif (t == PTR_TO_BTF_ID ||\n\t\t\t    t == PTR_TO_BTF_ID_OR_NULL ||\n\t\t\t    t == PTR_TO_PERCPU_BTF_ID)\n\t\t\t\tverbose(env, \"%s\", kernel_type_name(reg->btf, reg->btf_id));\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (reg_type_may_be_refcounted_or_null(t))\n\t\t\t\tverbose(env, \",ref_obj_id=%d\", reg->ref_obj_id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (t == CONST_PTR_TO_MAP ||\n\t\t\t\t t == PTR_TO_MAP_KEY ||\n\t\t\t\t t == PTR_TO_MAP_VALUE ||\n\t\t\t\t t == PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t\tif (reg->s32_min_value != reg->smin_value &&\n\t\t\t\t    reg->s32_min_value != S32_MIN)\n\t\t\t\t\tverbose(env, \",s32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_min_value));\n\t\t\t\tif (reg->s32_max_value != reg->smax_value &&\n\t\t\t\t    reg->s32_max_value != S32_MAX)\n\t\t\t\t\tverbose(env, \",s32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_max_value));\n\t\t\t\tif (reg->u32_min_value != reg->umin_value &&\n\t\t\t\t    reg->u32_min_value != U32_MIN)\n\t\t\t\t\tverbose(env, \",u32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_min_value));\n\t\t\t\tif (reg->u32_max_value != reg->umax_value &&\n\t\t\t\t    reg->u32_max_value != U32_MAX)\n\t\t\t\t\tverbose(env, \",u32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_max_value));\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tif (!print_all && !stack_slot_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (is_spilled_reg(&state->stack[i])) {\n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tt = reg->type;\n\t\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\t\tverbose(env, \"P\");\n\t\t\tif (t == SCALAR_VALUE && tnum_is_const(reg->var_off))\n\t\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t\t}\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tif (state->in_callback_fn)\n\t\tverbose(env, \" cb\");\n\tif (state->in_async_callback_fn)\n\t\tverbose(env, \" async_cb\");\n\tverbose(env, \"\\n\");\n\tmark_verifier_state_clean(env);\n}",
                        "code_after_change": "static void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state,\n\t\t\t\t bool print_all)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tif (!print_all && !reg_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str(env, t));\n\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\tverbose(env, \"P\");\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tif (base_type(t) == PTR_TO_BTF_ID ||\n\t\t\t    base_type(t) == PTR_TO_PERCPU_BTF_ID)\n\t\t\t\tverbose(env, \"%s\", kernel_type_name(reg->btf, reg->btf_id));\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (reg_type_may_be_refcounted_or_null(t))\n\t\t\t\tverbose(env, \",ref_obj_id=%d\", reg->ref_obj_id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (base_type(t) == CONST_PTR_TO_MAP ||\n\t\t\t\t base_type(t) == PTR_TO_MAP_KEY ||\n\t\t\t\t base_type(t) == PTR_TO_MAP_VALUE)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t\tif (reg->s32_min_value != reg->smin_value &&\n\t\t\t\t    reg->s32_min_value != S32_MIN)\n\t\t\t\t\tverbose(env, \",s32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_min_value));\n\t\t\t\tif (reg->s32_max_value != reg->smax_value &&\n\t\t\t\t    reg->s32_max_value != S32_MAX)\n\t\t\t\t\tverbose(env, \",s32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_max_value));\n\t\t\t\tif (reg->u32_min_value != reg->umin_value &&\n\t\t\t\t    reg->u32_min_value != U32_MIN)\n\t\t\t\t\tverbose(env, \",u32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_min_value));\n\t\t\t\tif (reg->u32_max_value != reg->umax_value &&\n\t\t\t\t    reg->u32_max_value != U32_MAX)\n\t\t\t\t\tverbose(env, \",u32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_max_value));\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tif (!print_all && !stack_slot_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (is_spilled_reg(&state->stack[i])) {\n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tt = reg->type;\n\t\t\tverbose(env, \"=%s\", reg_type_str(env, t));\n\t\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\t\tverbose(env, \"P\");\n\t\t\tif (t == SCALAR_VALUE && tnum_is_const(reg->var_off))\n\t\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t\t}\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tif (state->in_callback_fn)\n\t\tverbose(env, \" cb\");\n\tif (state->in_async_callback_fn)\n\t\tverbose(env, \" async_cb\");\n\tverbose(env, \"\\n\");\n\tmark_verifier_state_clean(env);\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3579,
            "cve_id": "CVE-2022-23222",
            "code_snippet": "static bool reg_type_may_be_refcounted_or_null(enum bpf_reg_type type)\n{\n\treturn base_type(type) == PTR_TO_SOCKET ||\n\t\tbase_type(type) == PTR_TO_TCP_SOCK ||\n\t\tbase_type(type) == PTR_TO_MEM;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static bool reg_type_mismatch_ok(enum bpf_reg_type type)\n{\n\tswitch (type) {\n\tcase PTR_TO_CTX:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\tcase PTR_TO_BTF_ID:\n\tcase PTR_TO_BTF_ID_OR_NULL:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}",
                        "code_after_change": "static bool reg_type_mismatch_ok(enum bpf_reg_type type)\n{\n\tswitch (base_type(type)) {\n\tcase PTR_TO_CTX:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\tcase PTR_TO_BTF_ID:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void mark_ptr_not_null_reg(struct bpf_reg_state *reg)\n{\n\tswitch (reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL: {\n\t\tconst struct bpf_map *map = reg->map_ptr;\n\n\t\tif (map->inner_map_meta) {\n\t\t\treg->type = CONST_PTR_TO_MAP;\n\t\t\treg->map_ptr = map->inner_map_meta;\n\t\t\t/* transfer reg's id which is unique for every map_lookup_elem\n\t\t\t * as UID of the inner map.\n\t\t\t */\n\t\t\tif (map_value_has_timer(map->inner_map_meta))\n\t\t\t\treg->map_uid = reg->id;\n\t\t} else if (map->map_type == BPF_MAP_TYPE_XSKMAP) {\n\t\t\treg->type = PTR_TO_XDP_SOCK;\n\t\t} else if (map->map_type == BPF_MAP_TYPE_SOCKMAP ||\n\t\t\t   map->map_type == BPF_MAP_TYPE_SOCKHASH) {\n\t\t\treg->type = PTR_TO_SOCKET;\n\t\t} else {\n\t\t\treg->type = PTR_TO_MAP_VALUE;\n\t\t}\n\t\tbreak;\n\t}\n\tcase PTR_TO_SOCKET_OR_NULL:\n\t\treg->type = PTR_TO_SOCKET;\n\t\tbreak;\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\t\treg->type = PTR_TO_SOCK_COMMON;\n\t\tbreak;\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\t\treg->type = PTR_TO_TCP_SOCK;\n\t\tbreak;\n\tcase PTR_TO_BTF_ID_OR_NULL:\n\t\treg->type = PTR_TO_BTF_ID;\n\t\tbreak;\n\tcase PTR_TO_MEM_OR_NULL:\n\t\treg->type = PTR_TO_MEM;\n\t\tbreak;\n\tcase PTR_TO_RDONLY_BUF_OR_NULL:\n\t\treg->type = PTR_TO_RDONLY_BUF;\n\t\tbreak;\n\tcase PTR_TO_RDWR_BUF_OR_NULL:\n\t\treg->type = PTR_TO_RDWR_BUF;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"unknown nullable register type\");\n\t}\n}",
                        "code_after_change": "static void mark_ptr_not_null_reg(struct bpf_reg_state *reg)\n{\n\tif (base_type(reg->type) == PTR_TO_MAP_VALUE) {\n\t\tconst struct bpf_map *map = reg->map_ptr;\n\n\t\tif (map->inner_map_meta) {\n\t\t\treg->type = CONST_PTR_TO_MAP;\n\t\t\treg->map_ptr = map->inner_map_meta;\n\t\t\t/* transfer reg's id which is unique for every map_lookup_elem\n\t\t\t * as UID of the inner map.\n\t\t\t */\n\t\t\tif (map_value_has_timer(map->inner_map_meta))\n\t\t\t\treg->map_uid = reg->id;\n\t\t} else if (map->map_type == BPF_MAP_TYPE_XSKMAP) {\n\t\t\treg->type = PTR_TO_XDP_SOCK;\n\t\t} else if (map->map_type == BPF_MAP_TYPE_SOCKMAP ||\n\t\t\t   map->map_type == BPF_MAP_TYPE_SOCKHASH) {\n\t\t\treg->type = PTR_TO_SOCKET;\n\t\t} else {\n\t\t\treg->type = PTR_TO_MAP_VALUE;\n\t\t}\n\t\treturn;\n\t}\n\n\treg->type &= ~PTR_MAYBE_NULL;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}",
                        "code_after_change": "static void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state,\n\t\t\t\t bool print_all)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tif (!print_all && !reg_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str(env, t));\n\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\tverbose(env, \"P\");\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tif (base_type(t) == PTR_TO_BTF_ID ||\n\t\t\t    base_type(t) == PTR_TO_PERCPU_BTF_ID)\n\t\t\t\tverbose(env, \"%s\", kernel_type_name(reg->btf, reg->btf_id));\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (reg_type_may_be_refcounted_or_null(t))\n\t\t\t\tverbose(env, \",ref_obj_id=%d\", reg->ref_obj_id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (base_type(t) == CONST_PTR_TO_MAP ||\n\t\t\t\t base_type(t) == PTR_TO_MAP_KEY ||\n\t\t\t\t base_type(t) == PTR_TO_MAP_VALUE)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t\tif (reg->s32_min_value != reg->smin_value &&\n\t\t\t\t    reg->s32_min_value != S32_MIN)\n\t\t\t\t\tverbose(env, \",s32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_min_value));\n\t\t\t\tif (reg->s32_max_value != reg->smax_value &&\n\t\t\t\t    reg->s32_max_value != S32_MAX)\n\t\t\t\t\tverbose(env, \",s32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_max_value));\n\t\t\t\tif (reg->u32_min_value != reg->umin_value &&\n\t\t\t\t    reg->u32_min_value != U32_MIN)\n\t\t\t\t\tverbose(env, \",u32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_min_value));\n\t\t\t\tif (reg->u32_max_value != reg->umax_value &&\n\t\t\t\t    reg->u32_max_value != U32_MAX)\n\t\t\t\t\tverbose(env, \",u32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_max_value));\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tif (!print_all && !stack_slot_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (is_spilled_reg(&state->stack[i])) {\n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tt = reg->type;\n\t\t\tverbose(env, \"=%s\", reg_type_str(env, t));\n\t\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\t\tverbose(env, \"P\");\n\t\t\tif (t == SCALAR_VALUE && tnum_is_const(reg->var_off))\n\t\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t\t}\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tif (state->in_callback_fn)\n\t\tverbose(env, \" cb\");\n\tif (state->in_async_callback_fn)\n\t\tverbose(env, \" async_cb\");\n\tverbose(env, \"\\n\");\n\tmark_verifier_state_clean(env);\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_KEY:\n\t\treturn check_mem_region_access(env, regno, reg->off, access_size,\n\t\t\t\t\t       reg->map_ptr->key_size, false);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tcase PTR_TO_STACK:\n\t\treturn check_stack_range_initialized(\n\t\t\t\tenv,\n\t\t\t\tregno, reg->off, access_size,\n\t\t\t\tzero_size_allowed, ACCESS_HELPER, meta);\n\tdefault: /* scalar_value or invalid ptr */\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s expected=%s\\n\", regno,\n\t\t\treg_type_str[reg->type],\n\t\t\treg_type_str[PTR_TO_STACK]);\n\t\treturn -EACCES;\n\t}\n}",
                        "code_after_change": "static bool regsafe(struct bpf_verifier_env *env, struct bpf_reg_state *rold,\n\t\t    struct bpf_reg_state *rcur, struct bpf_id_pair *idmap)\n{\n\tbool equal;\n\n\tif (!(rold->live & REG_LIVE_READ))\n\t\t/* explored state didn't use this */\n\t\treturn true;\n\n\tequal = memcmp(rold, rcur, offsetof(struct bpf_reg_state, parent)) == 0;\n\n\tif (rold->type == PTR_TO_STACK)\n\t\t/* two stack pointers are equal only if they're pointing to\n\t\t * the same stack frame, since fp-8 in foo != fp-8 in bar\n\t\t */\n\t\treturn equal && rold->frameno == rcur->frameno;\n\n\tif (equal)\n\t\treturn true;\n\n\tif (rold->type == NOT_INIT)\n\t\t/* explored state can't have used this */\n\t\treturn true;\n\tif (rcur->type == NOT_INIT)\n\t\treturn false;\n\tswitch (base_type(rold->type)) {\n\tcase SCALAR_VALUE:\n\t\tif (env->explore_alu_limits)\n\t\t\treturn false;\n\t\tif (rcur->type == SCALAR_VALUE) {\n\t\t\tif (!rold->precise && !rcur->precise)\n\t\t\t\treturn true;\n\t\t\t/* new val must satisfy old val knowledge */\n\t\t\treturn range_within(rold, rcur) &&\n\t\t\t       tnum_in(rold->var_off, rcur->var_off);\n\t\t} else {\n\t\t\t/* We're trying to use a pointer in place of a scalar.\n\t\t\t * Even if the scalar was unbounded, this could lead to\n\t\t\t * pointer leaks because scalars are allowed to leak\n\t\t\t * while pointers are not. We could make this safe in\n\t\t\t * special cases if root is calling us, but it's\n\t\t\t * probably not worth the hassle.\n\t\t\t */\n\t\t\treturn false;\n\t\t}\n\tcase PTR_TO_MAP_KEY:\n\tcase PTR_TO_MAP_VALUE:\n\t\t/* a PTR_TO_MAP_VALUE could be safe to use as a\n\t\t * PTR_TO_MAP_VALUE_OR_NULL into the same map.\n\t\t * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-\n\t\t * checked, doing so could have affected others with the same\n\t\t * id, and we can't check for that because we lost the id when\n\t\t * we converted to a PTR_TO_MAP_VALUE.\n\t\t */\n\t\tif (type_may_be_null(rold->type)) {\n\t\t\tif (!type_may_be_null(rcur->type))\n\t\t\t\treturn false;\n\t\t\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)))\n\t\t\t\treturn false;\n\t\t\t/* Check our ids match any regs they're supposed to */\n\t\t\treturn check_ids(rold->id, rcur->id, idmap);\n\t\t}\n\n\t\t/* If the new min/max/var_off satisfy the old ones and\n\t\t * everything else matches, we are OK.\n\t\t * 'id' is not compared, since it's only used for maps with\n\t\t * bpf_spin_lock inside map element and in such cases if\n\t\t * the rest of the prog is valid for one map element then\n\t\t * it's valid for all map elements regardless of the key\n\t\t * used in bpf_map_lookup()\n\t\t */\n\t\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&\n\t\t       range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET:\n\t\tif (rcur->type != rold->type)\n\t\t\treturn false;\n\t\t/* We must have at least as much range as the old ptr\n\t\t * did, so that any accesses which were safe before are\n\t\t * still safe.  This is true even if old range < old off,\n\t\t * since someone could have accessed through (ptr - k), or\n\t\t * even done ptr -= k in a register, to get a safe access.\n\t\t */\n\t\tif (rold->range > rcur->range)\n\t\t\treturn false;\n\t\t/* If the offsets don't match, we can't trust our alignment;\n\t\t * nor can we be sure that we won't fall out of range.\n\t\t */\n\t\tif (rold->off != rcur->off)\n\t\t\treturn false;\n\t\t/* id relations must be preserved */\n\t\tif (rold->id && !check_ids(rold->id, rcur->id, idmap))\n\t\t\treturn false;\n\t\t/* new val must satisfy old val knowledge */\n\t\treturn range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_CTX:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\t\t/* Only valid matches are exact, which memcmp() above\n\t\t * would have accepted\n\t\t */\n\tdefault:\n\t\t/* Don't know what's going on, just say it's not safe */\n\t\treturn false;\n\t}\n\n\t/* Shouldn't get here; if we do, say it's not safe */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static bool regsafe(struct bpf_verifier_env *env, struct bpf_reg_state *rold,\n\t\t    struct bpf_reg_state *rcur, struct bpf_id_pair *idmap)\n{\n\tbool equal;\n\n\tif (!(rold->live & REG_LIVE_READ))\n\t\t/* explored state didn't use this */\n\t\treturn true;\n\n\tequal = memcmp(rold, rcur, offsetof(struct bpf_reg_state, parent)) == 0;\n\n\tif (rold->type == PTR_TO_STACK)\n\t\t/* two stack pointers are equal only if they're pointing to\n\t\t * the same stack frame, since fp-8 in foo != fp-8 in bar\n\t\t */\n\t\treturn equal && rold->frameno == rcur->frameno;\n\n\tif (equal)\n\t\treturn true;\n\n\tif (rold->type == NOT_INIT)\n\t\t/* explored state can't have used this */\n\t\treturn true;\n\tif (rcur->type == NOT_INIT)\n\t\treturn false;\n\tswitch (rold->type) {\n\tcase SCALAR_VALUE:\n\t\tif (env->explore_alu_limits)\n\t\t\treturn false;\n\t\tif (rcur->type == SCALAR_VALUE) {\n\t\t\tif (!rold->precise && !rcur->precise)\n\t\t\t\treturn true;\n\t\t\t/* new val must satisfy old val knowledge */\n\t\t\treturn range_within(rold, rcur) &&\n\t\t\t       tnum_in(rold->var_off, rcur->var_off);\n\t\t} else {\n\t\t\t/* We're trying to use a pointer in place of a scalar.\n\t\t\t * Even if the scalar was unbounded, this could lead to\n\t\t\t * pointer leaks because scalars are allowed to leak\n\t\t\t * while pointers are not. We could make this safe in\n\t\t\t * special cases if root is calling us, but it's\n\t\t\t * probably not worth the hassle.\n\t\t\t */\n\t\t\treturn false;\n\t\t}\n\tcase PTR_TO_MAP_KEY:\n\tcase PTR_TO_MAP_VALUE:\n\t\t/* If the new min/max/var_off satisfy the old ones and\n\t\t * everything else matches, we are OK.\n\t\t * 'id' is not compared, since it's only used for maps with\n\t\t * bpf_spin_lock inside map element and in such cases if\n\t\t * the rest of the prog is valid for one map element then\n\t\t * it's valid for all map elements regardless of the key\n\t\t * used in bpf_map_lookup()\n\t\t */\n\t\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&\n\t\t       range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\t/* a PTR_TO_MAP_VALUE could be safe to use as a\n\t\t * PTR_TO_MAP_VALUE_OR_NULL into the same map.\n\t\t * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-\n\t\t * checked, doing so could have affected others with the same\n\t\t * id, and we can't check for that because we lost the id when\n\t\t * we converted to a PTR_TO_MAP_VALUE.\n\t\t */\n\t\tif (rcur->type != PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\treturn false;\n\t\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)))\n\t\t\treturn false;\n\t\t/* Check our ids match any regs they're supposed to */\n\t\treturn check_ids(rold->id, rcur->id, idmap);\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET:\n\t\tif (rcur->type != rold->type)\n\t\t\treturn false;\n\t\t/* We must have at least as much range as the old ptr\n\t\t * did, so that any accesses which were safe before are\n\t\t * still safe.  This is true even if old range < old off,\n\t\t * since someone could have accessed through (ptr - k), or\n\t\t * even done ptr -= k in a register, to get a safe access.\n\t\t */\n\t\tif (rold->range > rcur->range)\n\t\t\treturn false;\n\t\t/* If the offsets don't match, we can't trust our alignment;\n\t\t * nor can we be sure that we won't fall out of range.\n\t\t */\n\t\tif (rold->off != rcur->off)\n\t\t\treturn false;\n\t\t/* id relations must be preserved */\n\t\tif (rold->id && !check_ids(rold->id, rcur->id, idmap))\n\t\t\treturn false;\n\t\t/* new val must satisfy old val knowledge */\n\t\treturn range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_CTX:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\t/* Only valid matches are exact, which memcmp() above\n\t\t * would have accepted\n\t\t */\n\tdefault:\n\t\t/* Don't know what's going on, just say it's not safe */\n\t\treturn false;\n\t}\n\n\t/* Shouldn't get here; if we do, say it's not safe */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}",
                        "code_after_change": "static bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str(env, type), val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str(env, type), reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str(env, type));\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str(env, type));\n\t\treturn false;\n\t}\n\n\treturn true;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 3580,
            "cve_id": "CVE-2022-23222",
            "code_snippet": "static int check_sock_access(struct bpf_verifier_env *env, int insn_idx,\n\t\t\t     u32 regno, int off, int size,\n\t\t\t     enum bpf_access_type t)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = &regs[regno];\n\tstruct bpf_insn_access_aux info = {};\n\tbool valid;\n\n\tif (reg->smin_value < 0) {\n\t\tverbose(env, \"R%d min value is negative, either use unsigned index or do a if (index >=0) check.\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (reg->type) {\n\tcase PTR_TO_SOCK_COMMON:\n\t\tvalid = bpf_sock_common_is_valid_access(off, size, t, &info);\n\t\tbreak;\n\tcase PTR_TO_SOCKET:\n\t\tvalid = bpf_sock_is_valid_access(off, size, t, &info);\n\t\tbreak;\n\tcase PTR_TO_TCP_SOCK:\n\t\tvalid = bpf_tcp_sock_is_valid_access(off, size, t, &info);\n\t\tbreak;\n\tcase PTR_TO_XDP_SOCK:\n\t\tvalid = bpf_xdp_sock_is_valid_access(off, size, t, &info);\n\t\tbreak;\n\tdefault:\n\t\tvalid = false;\n\t}\n\n\n\tif (valid) {\n\t\tenv->insn_aux_data[insn_idx].ctx_field_size =\n\t\t\tinfo.ctx_field_size;\n\t\treturn 0;\n\t}\n\n\tverbose(env, \"R%d invalid %s access off=%d size=%d\\n\",\n\t\tregno, reg_type_str(env, reg->type), off, size);\n\n\treturn -EACCES;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_KEY:\n\t\treturn check_mem_region_access(env, regno, reg->off, access_size,\n\t\t\t\t\t       reg->map_ptr->key_size, false);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tcase PTR_TO_STACK:\n\t\treturn check_stack_range_initialized(\n\t\t\t\tenv,\n\t\t\t\tregno, reg->off, access_size,\n\t\t\t\tzero_size_allowed, ACCESS_HELPER, meta);\n\tdefault: /* scalar_value or invalid ptr */\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s expected=%s\\n\", regno,\n\t\t\treg_type_str[reg->type],\n\t\t\treg_type_str[PTR_TO_STACK]);\n\t\treturn -EACCES;\n\t}\n}",
                        "code_after_change": "static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_KEY:\n\t\treturn check_mem_region_access(env, regno, reg->off, access_size,\n\t\t\t\t\t       reg->map_ptr->key_size, false);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tcase PTR_TO_STACK:\n\t\treturn check_stack_range_initialized(\n\t\t\t\tenv,\n\t\t\t\tregno, reg->off, access_size,\n\t\t\t\tzero_size_allowed, ACCESS_HELPER, meta);\n\tdefault: /* scalar_value or invalid ptr */\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s \", regno,\n\t\t\treg_type_str(env, reg->type));\n\t\tverbose(env, \"expected=%s\\n\", reg_type_str(env, PTR_TO_STACK));\n\t\treturn -EACCES;\n\t}\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3581,
            "cve_id": "CVE-2022-23222",
            "code_snippet": "static int do_check(struct bpf_verifier_env *env)\n{\n\tbool pop_log = !(env->log.level & BPF_LOG_LEVEL2);\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tstruct bpf_reg_state *regs;\n\tint insn_cnt = env->prog->len;\n\tbool do_print_state = false;\n\tint prev_insn_idx = -1;\n\n\tfor (;;) {\n\t\tstruct bpf_insn *insn;\n\t\tu8 class;\n\t\tint err;\n\n\t\tenv->prev_insn_idx = prev_insn_idx;\n\t\tif (env->insn_idx >= insn_cnt) {\n\t\t\tverbose(env, \"invalid insn idx %d insn_cnt %d\\n\",\n\t\t\t\tenv->insn_idx, insn_cnt);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tinsn = &insns[env->insn_idx];\n\t\tclass = BPF_CLASS(insn->code);\n\n\t\tif (++env->insn_processed > BPF_COMPLEXITY_LIMIT_INSNS) {\n\t\t\tverbose(env,\n\t\t\t\t\"BPF program is too large. Processed %d insn\\n\",\n\t\t\t\tenv->insn_processed);\n\t\t\treturn -E2BIG;\n\t\t}\n\n\t\terr = is_state_visited(env, env->insn_idx);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tif (err == 1) {\n\t\t\t/* found equivalent state, can prune the search */\n\t\t\tif (env->log.level & BPF_LOG_LEVEL) {\n\t\t\t\tif (do_print_state)\n\t\t\t\t\tverbose(env, \"\\nfrom %d to %d%s: safe\\n\",\n\t\t\t\t\t\tenv->prev_insn_idx, env->insn_idx,\n\t\t\t\t\t\tenv->cur_state->speculative ?\n\t\t\t\t\t\t\" (speculative execution)\" : \"\");\n\t\t\t\telse\n\t\t\t\t\tverbose(env, \"%d: safe\\n\", env->insn_idx);\n\t\t\t}\n\t\t\tgoto process_bpf_exit;\n\t\t}\n\n\t\tif (signal_pending(current))\n\t\t\treturn -EAGAIN;\n\n\t\tif (need_resched())\n\t\t\tcond_resched();\n\n\t\tif (env->log.level & BPF_LOG_LEVEL2 && do_print_state) {\n\t\t\tverbose(env, \"\\nfrom %d to %d%s:\",\n\t\t\t\tenv->prev_insn_idx, env->insn_idx,\n\t\t\t\tenv->cur_state->speculative ?\n\t\t\t\t\" (speculative execution)\" : \"\");\n\t\t\tprint_verifier_state(env, state->frame[state->curframe], true);\n\t\t\tdo_print_state = false;\n\t\t}\n\n\t\tif (env->log.level & BPF_LOG_LEVEL) {\n\t\t\tconst struct bpf_insn_cbs cbs = {\n\t\t\t\t.cb_call\t= disasm_kfunc_name,\n\t\t\t\t.cb_print\t= verbose,\n\t\t\t\t.private_data\t= env,\n\t\t\t};\n\n\t\t\tif (verifier_state_scratched(env))\n\t\t\t\tprint_insn_state(env, state->frame[state->curframe]);\n\n\t\t\tverbose_linfo(env, env->insn_idx, \"; \");\n\t\t\tenv->prev_log_len = env->log.len_used;\n\t\t\tverbose(env, \"%d: \", env->insn_idx);\n\t\t\tprint_bpf_insn(&cbs, insn, env->allow_ptr_leaks);\n\t\t\tenv->prev_insn_print_len = env->log.len_used - env->prev_log_len;\n\t\t\tenv->prev_log_len = env->log.len_used;\n\t\t}\n\n\t\tif (bpf_prog_is_dev_bound(env->prog->aux)) {\n\t\t\terr = bpf_prog_offload_verify_insn(env, env->insn_idx,\n\t\t\t\t\t\t\t   env->prev_insn_idx);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tregs = cur_regs(env);\n\t\tsanitize_mark_insn_seen(env);\n\t\tprev_insn_idx = env->insn_idx;\n\n\t\tif (class == BPF_ALU || class == BPF_ALU64) {\n\t\t\terr = check_alu_op(env, insn);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_LDX) {\n\t\t\tenum bpf_reg_type *prev_src_type, src_reg_type;\n\n\t\t\t/* check for reserved fields is already done */\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tsrc_reg_type = regs[insn->src_reg].type;\n\n\t\t\t/* check that memory (src_reg + off) is readable,\n\t\t\t * the state of dst_reg will be updated by this func\n\t\t\t */\n\t\t\terr = check_mem_access(env, env->insn_idx, insn->src_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_READ, insn->dst_reg, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_src_type = &env->insn_aux_data[env->insn_idx].ptr_type;\n\n\t\t\tif (*prev_src_type == NOT_INIT) {\n\t\t\t\t/* saw a valid insn\n\t\t\t\t * dst_reg = *(u32 *)(src_reg + off)\n\t\t\t\t * save type to validate intersecting paths\n\t\t\t\t */\n\t\t\t\t*prev_src_type = src_reg_type;\n\n\t\t\t} else if (reg_type_mismatch(src_reg_type, *prev_src_type)) {\n\t\t\t\t/* ABuser program is trying to use the same insn\n\t\t\t\t * dst_reg = *(u32*) (src_reg + off)\n\t\t\t\t * with different pointer types:\n\t\t\t\t * src_reg == ctx in one branch and\n\t\t\t\t * src_reg == stack|map in some other branch.\n\t\t\t\t * Reject it.\n\t\t\t\t */\n\t\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_STX) {\n\t\t\tenum bpf_reg_type *prev_dst_type, dst_reg_type;\n\n\t\t\tif (BPF_MODE(insn->code) == BPF_ATOMIC) {\n\t\t\t\terr = check_atomic(env, env->insn_idx, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\tenv->insn_idx++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (BPF_MODE(insn->code) != BPF_MEM || insn->imm != 0) {\n\t\t\t\tverbose(env, \"BPF_STX uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\t/* check src2 operand */\n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tdst_reg_type = regs[insn->dst_reg].type;\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, env->insn_idx, insn->dst_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_WRITE, insn->src_reg, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_dst_type = &env->insn_aux_data[env->insn_idx].ptr_type;\n\n\t\t\tif (*prev_dst_type == NOT_INIT) {\n\t\t\t\t*prev_dst_type = dst_reg_type;\n\t\t\t} else if (reg_type_mismatch(dst_reg_type, *prev_dst_type)) {\n\t\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_ST) {\n\t\t\tif (BPF_MODE(insn->code) != BPF_MEM ||\n\t\t\t    insn->src_reg != BPF_REG_0) {\n\t\t\t\tverbose(env, \"BPF_ST uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tif (is_ctx_reg(env, insn->dst_reg)) {\n\t\t\t\tverbose(env, \"BPF_ST stores into R%d %s is not allowed\\n\",\n\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\treg_type_str(env, reg_state(env, insn->dst_reg)->type));\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, env->insn_idx, insn->dst_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_WRITE, -1, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_JMP || class == BPF_JMP32) {\n\t\t\tu8 opcode = BPF_OP(insn->code);\n\n\t\t\tenv->jmps_processed++;\n\t\t\tif (opcode == BPF_CALL) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    (insn->src_reg != BPF_PSEUDO_KFUNC_CALL\n\t\t\t\t     && insn->off != 0) ||\n\t\t\t\t    (insn->src_reg != BPF_REG_0 &&\n\t\t\t\t     insn->src_reg != BPF_PSEUDO_CALL &&\n\t\t\t\t     insn->src_reg != BPF_PSEUDO_KFUNC_CALL) ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0 ||\n\t\t\t\t    class == BPF_JMP32) {\n\t\t\t\t\tverbose(env, \"BPF_CALL uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (env->cur_state->active_spin_lock &&\n\t\t\t\t    (insn->src_reg == BPF_PSEUDO_CALL ||\n\t\t\t\t     insn->imm != BPF_FUNC_spin_unlock)) {\n\t\t\t\t\tverbose(env, \"function calls are not allowed while holding a lock\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tif (insn->src_reg == BPF_PSEUDO_CALL)\n\t\t\t\t\terr = check_func_call(env, insn, &env->insn_idx);\n\t\t\t\telse if (insn->src_reg == BPF_PSEUDO_KFUNC_CALL)\n\t\t\t\t\terr = check_kfunc_call(env, insn);\n\t\t\t\telse\n\t\t\t\t\terr = check_helper_call(env, insn, &env->insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t} else if (opcode == BPF_JA) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0 ||\n\t\t\t\t    class == BPF_JMP32) {\n\t\t\t\t\tverbose(env, \"BPF_JA uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tenv->insn_idx += insn->off + 1;\n\t\t\t\tcontinue;\n\n\t\t\t} else if (opcode == BPF_EXIT) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0 ||\n\t\t\t\t    class == BPF_JMP32) {\n\t\t\t\t\tverbose(env, \"BPF_EXIT uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (env->cur_state->active_spin_lock) {\n\t\t\t\t\tverbose(env, \"bpf_spin_unlock is missing\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (state->curframe) {\n\t\t\t\t\t/* exit from nested function */\n\t\t\t\t\terr = prepare_func_exit(env, &env->insn_idx);\n\t\t\t\t\tif (err)\n\t\t\t\t\t\treturn err;\n\t\t\t\t\tdo_print_state = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\terr = check_reference_leak(env);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\terr = check_return_code(env);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\nprocess_bpf_exit:\n\t\t\t\tmark_verifier_state_scratched(env);\n\t\t\t\tupdate_branch_counts(env, env->cur_state);\n\t\t\t\terr = pop_stack(env, &prev_insn_idx,\n\t\t\t\t\t\t&env->insn_idx, pop_log);\n\t\t\t\tif (err < 0) {\n\t\t\t\t\tif (err != -ENOENT)\n\t\t\t\t\t\treturn err;\n\t\t\t\t\tbreak;\n\t\t\t\t} else {\n\t\t\t\t\tdo_print_state = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terr = check_cond_jmp_op(env, insn, &env->insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t} else if (class == BPF_LD) {\n\t\t\tu8 mode = BPF_MODE(insn->code);\n\n\t\t\tif (mode == BPF_ABS || mode == BPF_IND) {\n\t\t\t\terr = check_ld_abs(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (mode == BPF_IMM) {\n\t\t\t\terr = check_ld_imm(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tenv->insn_idx++;\n\t\t\t\tsanitize_mark_insn_seen(env);\n\t\t\t} else {\n\t\t\t\tverbose(env, \"invalid BPF_LD mode\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tverbose(env, \"unknown insn class %d\\n\", class);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tenv->insn_idx++;\n\t}\n\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "code_after_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tif (ptr_reg->type & PTR_MAYBE_NULL) {\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\t}\n\n\tswitch (base_type(ptr_reg->type)) {\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct tnum enforce_attach_type_range = tnum_unknown;\n\tconst struct bpf_prog *prog = env->prog;\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\tenum bpf_prog_type prog_type = resolve_prog_type(env->prog);\n\tint err;\n\tstruct bpf_func_state *frame = env->cur_state->frame[0];\n\tconst bool is_subprog = frame->subprogno;\n\n\t/* LSM and struct_ops func-ptr's return type could be \"void\" */\n\tif (!is_subprog &&\n\t    (prog_type == BPF_PROG_TYPE_STRUCT_OPS ||\n\t     prog_type == BPF_PROG_TYPE_LSM) &&\n\t    !prog->aux->attach_func_proto->type)\n\t\treturn 0;\n\n\t/* eBPF calling convention is such that R0 is used\n\t * to return the value from eBPF program.\n\t * Make sure that it's readable at this time\n\t * of bpf_exit, which means that program wrote\n\t * something into it earlier\n\t */\n\terr = check_reg_arg(env, BPF_REG_0, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\tverbose(env, \"R0 leaks addr as return value\\n\");\n\t\treturn -EACCES;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\n\tif (frame->in_async_callback_fn) {\n\t\t/* enforce return zero from async callbacks like timer */\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"In async callback the register R0 is not a known value (%s)\\n\",\n\t\t\t\treg_type_str[reg->type]);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!tnum_in(tnum_const(0), reg->var_off)) {\n\t\t\tverbose_invalid_scalar(env, reg, &range, \"async callback\", \"R0\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (is_subprog) {\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"At subprogram exit the register R0 is not a scalar value (%s)\\n\",\n\t\t\t\treg_type_str[reg->type]);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tswitch (prog_type) {\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_UDP4_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_UDP6_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETSOCKNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETSOCKNAME)\n\t\t\trange = tnum_range(1, 1);\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET4_BIND ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_BIND)\n\t\t\trange = tnum_range(0, 3);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET_EGRESS) {\n\t\t\trange = tnum_range(0, 3);\n\t\t\tenforce_attach_type_range = tnum_range(2, 3);\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\tcase BPF_PROG_TYPE_CGROUP_SYSCTL:\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\t\tbreak;\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT:\n\t\tif (!env->prog->aux->attach_btf_id)\n\t\t\treturn 0;\n\t\trange = tnum_const(0);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_TRACING:\n\t\tswitch (env->prog->expected_attach_type) {\n\t\tcase BPF_TRACE_FENTRY:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\trange = tnum_const(0);\n\t\t\tbreak;\n\t\tcase BPF_TRACE_RAW_TP:\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\treturn 0;\n\t\tcase BPF_TRACE_ITER:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -ENOTSUPP;\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_SK_LOOKUP:\n\t\trange = tnum_range(SK_DROP, SK_PASS);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_EXT:\n\t\t/* freplace program can return anything as its return value\n\t\t * depends on the to-be-replaced kernel func or bpf program.\n\t\t */\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose_invalid_scalar(env, reg, &range, \"program exit\", \"R0\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_is_unknown(enforce_attach_type_range) &&\n\t    tnum_in(enforce_attach_type_range, reg->var_off))\n\t\tenv->prog->enforce_expected_attach_type = 1;\n\treturn 0;\n}",
                        "code_after_change": "static int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct tnum enforce_attach_type_range = tnum_unknown;\n\tconst struct bpf_prog *prog = env->prog;\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\tenum bpf_prog_type prog_type = resolve_prog_type(env->prog);\n\tint err;\n\tstruct bpf_func_state *frame = env->cur_state->frame[0];\n\tconst bool is_subprog = frame->subprogno;\n\n\t/* LSM and struct_ops func-ptr's return type could be \"void\" */\n\tif (!is_subprog &&\n\t    (prog_type == BPF_PROG_TYPE_STRUCT_OPS ||\n\t     prog_type == BPF_PROG_TYPE_LSM) &&\n\t    !prog->aux->attach_func_proto->type)\n\t\treturn 0;\n\n\t/* eBPF calling convention is such that R0 is used\n\t * to return the value from eBPF program.\n\t * Make sure that it's readable at this time\n\t * of bpf_exit, which means that program wrote\n\t * something into it earlier\n\t */\n\terr = check_reg_arg(env, BPF_REG_0, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\tverbose(env, \"R0 leaks addr as return value\\n\");\n\t\treturn -EACCES;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\n\tif (frame->in_async_callback_fn) {\n\t\t/* enforce return zero from async callbacks like timer */\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"In async callback the register R0 is not a known value (%s)\\n\",\n\t\t\t\treg_type_str(env, reg->type));\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!tnum_in(tnum_const(0), reg->var_off)) {\n\t\t\tverbose_invalid_scalar(env, reg, &range, \"async callback\", \"R0\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (is_subprog) {\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"At subprogram exit the register R0 is not a scalar value (%s)\\n\",\n\t\t\t\treg_type_str(env, reg->type));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tswitch (prog_type) {\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_UDP4_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_UDP6_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETSOCKNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETSOCKNAME)\n\t\t\trange = tnum_range(1, 1);\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET4_BIND ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_BIND)\n\t\t\trange = tnum_range(0, 3);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET_EGRESS) {\n\t\t\trange = tnum_range(0, 3);\n\t\t\tenforce_attach_type_range = tnum_range(2, 3);\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\tcase BPF_PROG_TYPE_CGROUP_SYSCTL:\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\t\tbreak;\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT:\n\t\tif (!env->prog->aux->attach_btf_id)\n\t\t\treturn 0;\n\t\trange = tnum_const(0);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_TRACING:\n\t\tswitch (env->prog->expected_attach_type) {\n\t\tcase BPF_TRACE_FENTRY:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\trange = tnum_const(0);\n\t\t\tbreak;\n\t\tcase BPF_TRACE_RAW_TP:\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\treturn 0;\n\t\tcase BPF_TRACE_ITER:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -ENOTSUPP;\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_SK_LOOKUP:\n\t\trange = tnum_range(SK_DROP, SK_PASS);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_EXT:\n\t\t/* freplace program can return anything as its return value\n\t\t * depends on the to-be-replaced kernel func or bpf program.\n\t\t */\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str(env, reg->type));\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose_invalid_scalar(env, reg, &range, \"program exit\", \"R0\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_is_unknown(enforce_attach_type_range) &&\n\t    tnum_in(enforce_attach_type_range, reg->var_off))\n\t\tenv->prog->enforce_expected_attach_type = 1;\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state,\n\t\t\t\t bool print_all)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tif (!print_all && !reg_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\tverbose(env, \"P\");\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tif (t == PTR_TO_BTF_ID ||\n\t\t\t    t == PTR_TO_BTF_ID_OR_NULL ||\n\t\t\t    t == PTR_TO_PERCPU_BTF_ID)\n\t\t\t\tverbose(env, \"%s\", kernel_type_name(reg->btf, reg->btf_id));\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (reg_type_may_be_refcounted_or_null(t))\n\t\t\t\tverbose(env, \",ref_obj_id=%d\", reg->ref_obj_id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (t == CONST_PTR_TO_MAP ||\n\t\t\t\t t == PTR_TO_MAP_KEY ||\n\t\t\t\t t == PTR_TO_MAP_VALUE ||\n\t\t\t\t t == PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t\tif (reg->s32_min_value != reg->smin_value &&\n\t\t\t\t    reg->s32_min_value != S32_MIN)\n\t\t\t\t\tverbose(env, \",s32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_min_value));\n\t\t\t\tif (reg->s32_max_value != reg->smax_value &&\n\t\t\t\t    reg->s32_max_value != S32_MAX)\n\t\t\t\t\tverbose(env, \",s32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_max_value));\n\t\t\t\tif (reg->u32_min_value != reg->umin_value &&\n\t\t\t\t    reg->u32_min_value != U32_MIN)\n\t\t\t\t\tverbose(env, \",u32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_min_value));\n\t\t\t\tif (reg->u32_max_value != reg->umax_value &&\n\t\t\t\t    reg->u32_max_value != U32_MAX)\n\t\t\t\t\tverbose(env, \",u32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_max_value));\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tif (!print_all && !stack_slot_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (is_spilled_reg(&state->stack[i])) {\n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tt = reg->type;\n\t\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\t\tverbose(env, \"P\");\n\t\t\tif (t == SCALAR_VALUE && tnum_is_const(reg->var_off))\n\t\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t\t}\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tif (state->in_callback_fn)\n\t\tverbose(env, \" cb\");\n\tif (state->in_async_callback_fn)\n\t\tverbose(env, \" async_cb\");\n\tverbose(env, \"\\n\");\n\tmark_verifier_state_clean(env);\n}",
                        "code_after_change": "static void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state,\n\t\t\t\t bool print_all)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tif (!print_all && !reg_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str(env, t));\n\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\tverbose(env, \"P\");\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tif (base_type(t) == PTR_TO_BTF_ID ||\n\t\t\t    base_type(t) == PTR_TO_PERCPU_BTF_ID)\n\t\t\t\tverbose(env, \"%s\", kernel_type_name(reg->btf, reg->btf_id));\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (reg_type_may_be_refcounted_or_null(t))\n\t\t\t\tverbose(env, \",ref_obj_id=%d\", reg->ref_obj_id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (base_type(t) == CONST_PTR_TO_MAP ||\n\t\t\t\t base_type(t) == PTR_TO_MAP_KEY ||\n\t\t\t\t base_type(t) == PTR_TO_MAP_VALUE)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t\tif (reg->s32_min_value != reg->smin_value &&\n\t\t\t\t    reg->s32_min_value != S32_MIN)\n\t\t\t\t\tverbose(env, \",s32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_min_value));\n\t\t\t\tif (reg->s32_max_value != reg->smax_value &&\n\t\t\t\t    reg->s32_max_value != S32_MAX)\n\t\t\t\t\tverbose(env, \",s32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_max_value));\n\t\t\t\tif (reg->u32_min_value != reg->umin_value &&\n\t\t\t\t    reg->u32_min_value != U32_MIN)\n\t\t\t\t\tverbose(env, \",u32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_min_value));\n\t\t\t\tif (reg->u32_max_value != reg->umax_value &&\n\t\t\t\t    reg->u32_max_value != U32_MAX)\n\t\t\t\t\tverbose(env, \",u32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_max_value));\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tif (!print_all && !stack_slot_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (is_spilled_reg(&state->stack[i])) {\n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tt = reg->type;\n\t\t\tverbose(env, \"=%s\", reg_type_str(env, t));\n\t\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\t\tverbose(env, \"P\");\n\t\t\tif (t == SCALAR_VALUE && tnum_is_const(reg->var_off))\n\t\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t\t}\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tif (state->in_callback_fn)\n\t\tverbose(env, \" cb\");\n\tif (state->in_async_callback_fn)\n\t\tverbose(env, \" async_cb\");\n\tverbose(env, \"\\n\");\n\tmark_verifier_state_clean(env);\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}",
                        "code_after_change": "static bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str(env, type), val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str(env, type), reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str(env, type));\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str(env, type));\n\t\treturn false;\n\t}\n\n\treturn true;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_KEY:\n\t\treturn check_mem_region_access(env, regno, reg->off, access_size,\n\t\t\t\t\t       reg->map_ptr->key_size, false);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tcase PTR_TO_STACK:\n\t\treturn check_stack_range_initialized(\n\t\t\t\tenv,\n\t\t\t\tregno, reg->off, access_size,\n\t\t\t\tzero_size_allowed, ACCESS_HELPER, meta);\n\tdefault: /* scalar_value or invalid ptr */\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s expected=%s\\n\", regno,\n\t\t\treg_type_str[reg->type],\n\t\t\treg_type_str[PTR_TO_STACK]);\n\t\treturn -EACCES;\n\t}\n}",
                        "code_after_change": "static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_KEY:\n\t\treturn check_mem_region_access(env, regno, reg->off, access_size,\n\t\t\t\t\t       reg->map_ptr->key_size, false);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tcase PTR_TO_STACK:\n\t\treturn check_stack_range_initialized(\n\t\t\t\tenv,\n\t\t\t\tregno, reg->off, access_size,\n\t\t\t\tzero_size_allowed, ACCESS_HELPER, meta);\n\tdefault: /* scalar_value or invalid ptr */\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s \", regno,\n\t\t\treg_type_str(env, reg->type));\n\t\tverbose(env, \"expected=%s\\n\", reg_type_str(env, PTR_TO_STACK));\n\t\treturn -EACCES;\n\t}\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3582,
            "cve_id": "CVE-2022-23222",
            "code_snippet": "static int check_mem_access(struct bpf_verifier_env *env, int insn_idx, u32 regno,\n\t\t\t    int off, int bpf_size, enum bpf_access_type t,\n\t\t\t    int value_regno, bool strict_alignment_once)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = regs + regno;\n\tstruct bpf_func_state *state;\n\tint size, err = 0;\n\n\tsize = bpf_size_to_bytes(bpf_size);\n\tif (size < 0)\n\t\treturn size;\n\n\t/* alignment checks will add in reg->off themselves */\n\terr = check_ptr_alignment(env, reg, off, size, strict_alignment_once);\n\tif (err)\n\t\treturn err;\n\n\t/* for access checks, reg->off is just part of off */\n\toff += reg->off;\n\n\tif (reg->type == PTR_TO_MAP_KEY) {\n\t\tif (t == BPF_WRITE) {\n\t\t\tverbose(env, \"write to change key R%d not allowed\\n\", regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\terr = check_mem_region_access(env, regno, off, size,\n\t\t\t\t\t      reg->map_ptr->key_size, false);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_MAP_VALUE) {\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into map\\n\", value_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_map_access_type(env, regno, off, size, t);\n\t\tif (err)\n\t\t\treturn err;\n\t\terr = check_map_access(env, regno, off, size, false);\n\t\tif (!err && t == BPF_READ && value_regno >= 0) {\n\t\t\tstruct bpf_map *map = reg->map_ptr;\n\n\t\t\t/* if map is read-only, track its contents as scalars */\n\t\t\tif (tnum_is_const(reg->var_off) &&\n\t\t\t    bpf_map_is_rdonly(map) &&\n\t\t\t    map->ops->map_direct_value_addr) {\n\t\t\t\tint map_off = off + reg->var_off.value;\n\t\t\t\tu64 val = 0;\n\n\t\t\t\terr = bpf_map_direct_read(map, map_off, size,\n\t\t\t\t\t\t\t  &val);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tregs[value_regno].type = SCALAR_VALUE;\n\t\t\t\t__mark_reg_known(&regs[value_regno], val);\n\t\t\t} else {\n\t\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t\t\t}\n\t\t}\n\t} else if (reg->type == PTR_TO_MEM) {\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into mem\\n\", value_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_mem_region_access(env, regno, off, size,\n\t\t\t\t\t      reg->mem_size, false);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_CTX) {\n\t\tenum bpf_reg_type reg_type = SCALAR_VALUE;\n\t\tstruct btf *btf = NULL;\n\t\tu32 btf_id = 0;\n\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into ctx\\n\", value_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\terr = check_ctx_reg(env, reg, regno);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\terr = check_ctx_access(env, insn_idx, off, size, t, &reg_type, &btf, &btf_id);\n\t\tif (err)\n\t\t\tverbose_linfo(env, insn_idx, \"; \");\n\t\tif (!err && t == BPF_READ && value_regno >= 0) {\n\t\t\t/* ctx access returns either a scalar, or a\n\t\t\t * PTR_TO_PACKET[_META,_END]. In the latter\n\t\t\t * case, we know the offset is zero.\n\t\t\t */\n\t\t\tif (reg_type == SCALAR_VALUE) {\n\t\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t\t\t} else {\n\t\t\t\tmark_reg_known_zero(env, regs,\n\t\t\t\t\t\t    value_regno);\n\t\t\t\tif (type_may_be_null(reg_type))\n\t\t\t\t\tregs[value_regno].id = ++env->id_gen;\n\t\t\t\t/* A load of ctx field could have different\n\t\t\t\t * actual load size with the one encoded in the\n\t\t\t\t * insn. When the dst is PTR, it is for sure not\n\t\t\t\t * a sub-register.\n\t\t\t\t */\n\t\t\t\tregs[value_regno].subreg_def = DEF_NOT_SUBREG;\n\t\t\t\tif (base_type(reg_type) == PTR_TO_BTF_ID) {\n\t\t\t\t\tregs[value_regno].btf = btf;\n\t\t\t\t\tregs[value_regno].btf_id = btf_id;\n\t\t\t\t}\n\t\t\t}\n\t\t\tregs[value_regno].type = reg_type;\n\t\t}\n\n\t} else if (reg->type == PTR_TO_STACK) {\n\t\t/* Basic bounds checks. */\n\t\terr = check_stack_access_within_bounds(env, regno, off, size, ACCESS_DIRECT, t);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tstate = func(env, reg);\n\t\terr = update_stack_depth(env, state, off);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (t == BPF_READ)\n\t\t\terr = check_stack_read(env, regno, off, size,\n\t\t\t\t\t       value_regno);\n\t\telse\n\t\t\terr = check_stack_write(env, regno, off, size,\n\t\t\t\t\t\tvalue_regno, insn_idx);\n\t} else if (reg_is_pkt_pointer(reg)) {\n\t\tif (t == BPF_WRITE && !may_access_direct_pkt_data(env, NULL, t)) {\n\t\t\tverbose(env, \"cannot write into packet\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into packet\\n\",\n\t\t\t\tvalue_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_packet_access(env, regno, off, size, false);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_FLOW_KEYS) {\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into flow keys\\n\",\n\t\t\t\tvalue_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\terr = check_flow_keys_access(env, off, size);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (type_is_sk_pointer(reg->type)) {\n\t\tif (t == BPF_WRITE) {\n\t\t\tverbose(env, \"R%d cannot write into %s\\n\",\n\t\t\t\tregno, reg_type_str(env, reg->type));\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_sock_access(env, insn_idx, regno, off, size, t);\n\t\tif (!err && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_TP_BUFFER) {\n\t\terr = check_tp_buffer_access(env, reg, regno, off, size);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_BTF_ID) {\n\t\terr = check_ptr_to_btf_access(env, regs, regno, off, size, t,\n\t\t\t\t\t      value_regno);\n\t} else if (reg->type == CONST_PTR_TO_MAP) {\n\t\terr = check_ptr_to_map_access(env, regs, regno, off, size, t,\n\t\t\t\t\t      value_regno);\n\t} else if (reg->type == PTR_TO_RDONLY_BUF) {\n\t\tif (t == BPF_WRITE) {\n\t\t\tverbose(env, \"R%d cannot write into %s\\n\",\n\t\t\t\tregno, reg_type_str(env, reg->type));\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_buffer_access(env, reg, regno, off, size, false,\n\t\t\t\t\t  \"rdonly\",\n\t\t\t\t\t  &env->prog->aux->max_rdonly_access);\n\t\tif (!err && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_RDWR_BUF) {\n\t\terr = check_buffer_access(env, reg, regno, off, size, false,\n\t\t\t\t\t  \"rdwr\",\n\t\t\t\t\t  &env->prog->aux->max_rdwr_access);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else {\n\t\tverbose(env, \"R%d invalid mem access '%s'\\n\", regno,\n\t\t\treg_type_str(env, reg->type));\n\t\treturn -EACCES;\n\t}\n\n\tif (!err && size < BPF_REG_SIZE && value_regno >= 0 && t == BPF_READ &&\n\t    regs[value_regno].type == SCALAR_VALUE) {\n\t\t/* b/h/w load zero-extends, mark upper bits as known 0 */\n\t\tcoerce_reg_to_size(&regs[value_regno], size);\n\t}\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_KEY:\n\t\treturn check_mem_region_access(env, regno, reg->off, access_size,\n\t\t\t\t\t       reg->map_ptr->key_size, false);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tcase PTR_TO_STACK:\n\t\treturn check_stack_range_initialized(\n\t\t\t\tenv,\n\t\t\t\tregno, reg->off, access_size,\n\t\t\t\tzero_size_allowed, ACCESS_HELPER, meta);\n\tdefault: /* scalar_value or invalid ptr */\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s expected=%s\\n\", regno,\n\t\t\treg_type_str[reg->type],\n\t\t\treg_type_str[PTR_TO_STACK]);\n\t\treturn -EACCES;\n\t}\n}",
                        "code_after_change": "static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_KEY:\n\t\treturn check_mem_region_access(env, regno, reg->off, access_size,\n\t\t\t\t\t       reg->map_ptr->key_size, false);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tcase PTR_TO_STACK:\n\t\treturn check_stack_range_initialized(\n\t\t\t\tenv,\n\t\t\t\tregno, reg->off, access_size,\n\t\t\t\tzero_size_allowed, ACCESS_HELPER, meta);\n\tdefault: /* scalar_value or invalid ptr */\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s \", regno,\n\t\t\treg_type_str(env, reg->type));\n\t\tverbose(env, \"expected=%s\\n\", reg_type_str(env, PTR_TO_STACK));\n\t\treturn -EACCES;\n\t}\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state,\n\t\t\t\t bool print_all)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tif (!print_all && !reg_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\tverbose(env, \"P\");\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tif (t == PTR_TO_BTF_ID ||\n\t\t\t    t == PTR_TO_BTF_ID_OR_NULL ||\n\t\t\t    t == PTR_TO_PERCPU_BTF_ID)\n\t\t\t\tverbose(env, \"%s\", kernel_type_name(reg->btf, reg->btf_id));\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (reg_type_may_be_refcounted_or_null(t))\n\t\t\t\tverbose(env, \",ref_obj_id=%d\", reg->ref_obj_id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (t == CONST_PTR_TO_MAP ||\n\t\t\t\t t == PTR_TO_MAP_KEY ||\n\t\t\t\t t == PTR_TO_MAP_VALUE ||\n\t\t\t\t t == PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t\tif (reg->s32_min_value != reg->smin_value &&\n\t\t\t\t    reg->s32_min_value != S32_MIN)\n\t\t\t\t\tverbose(env, \",s32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_min_value));\n\t\t\t\tif (reg->s32_max_value != reg->smax_value &&\n\t\t\t\t    reg->s32_max_value != S32_MAX)\n\t\t\t\t\tverbose(env, \",s32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_max_value));\n\t\t\t\tif (reg->u32_min_value != reg->umin_value &&\n\t\t\t\t    reg->u32_min_value != U32_MIN)\n\t\t\t\t\tverbose(env, \",u32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_min_value));\n\t\t\t\tif (reg->u32_max_value != reg->umax_value &&\n\t\t\t\t    reg->u32_max_value != U32_MAX)\n\t\t\t\t\tverbose(env, \",u32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_max_value));\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tif (!print_all && !stack_slot_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (is_spilled_reg(&state->stack[i])) {\n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tt = reg->type;\n\t\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\t\tverbose(env, \"P\");\n\t\t\tif (t == SCALAR_VALUE && tnum_is_const(reg->var_off))\n\t\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t\t}\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tif (state->in_callback_fn)\n\t\tverbose(env, \" cb\");\n\tif (state->in_async_callback_fn)\n\t\tverbose(env, \" async_cb\");\n\tverbose(env, \"\\n\");\n\tmark_verifier_state_clean(env);\n}",
                        "code_after_change": "static void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state,\n\t\t\t\t bool print_all)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tif (!print_all && !reg_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str(env, t));\n\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\tverbose(env, \"P\");\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tif (base_type(t) == PTR_TO_BTF_ID ||\n\t\t\t    base_type(t) == PTR_TO_PERCPU_BTF_ID)\n\t\t\t\tverbose(env, \"%s\", kernel_type_name(reg->btf, reg->btf_id));\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (reg_type_may_be_refcounted_or_null(t))\n\t\t\t\tverbose(env, \",ref_obj_id=%d\", reg->ref_obj_id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (base_type(t) == CONST_PTR_TO_MAP ||\n\t\t\t\t base_type(t) == PTR_TO_MAP_KEY ||\n\t\t\t\t base_type(t) == PTR_TO_MAP_VALUE)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t\tif (reg->s32_min_value != reg->smin_value &&\n\t\t\t\t    reg->s32_min_value != S32_MIN)\n\t\t\t\t\tverbose(env, \",s32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_min_value));\n\t\t\t\tif (reg->s32_max_value != reg->smax_value &&\n\t\t\t\t    reg->s32_max_value != S32_MAX)\n\t\t\t\t\tverbose(env, \",s32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_max_value));\n\t\t\t\tif (reg->u32_min_value != reg->umin_value &&\n\t\t\t\t    reg->u32_min_value != U32_MIN)\n\t\t\t\t\tverbose(env, \",u32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_min_value));\n\t\t\t\tif (reg->u32_max_value != reg->umax_value &&\n\t\t\t\t    reg->u32_max_value != U32_MAX)\n\t\t\t\t\tverbose(env, \",u32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_max_value));\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tif (!print_all && !stack_slot_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (is_spilled_reg(&state->stack[i])) {\n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tt = reg->type;\n\t\t\tverbose(env, \"=%s\", reg_type_str(env, t));\n\t\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\t\tverbose(env, \"P\");\n\t\t\tif (t == SCALAR_VALUE && tnum_is_const(reg->var_off))\n\t\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t\t}\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tif (state->in_callback_fn)\n\t\tverbose(env, \" cb\");\n\tif (state->in_async_callback_fn)\n\t\tverbose(env, \" async_cb\");\n\tverbose(env, \"\\n\");\n\tmark_verifier_state_clean(env);\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct tnum enforce_attach_type_range = tnum_unknown;\n\tconst struct bpf_prog *prog = env->prog;\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\tenum bpf_prog_type prog_type = resolve_prog_type(env->prog);\n\tint err;\n\tstruct bpf_func_state *frame = env->cur_state->frame[0];\n\tconst bool is_subprog = frame->subprogno;\n\n\t/* LSM and struct_ops func-ptr's return type could be \"void\" */\n\tif (!is_subprog &&\n\t    (prog_type == BPF_PROG_TYPE_STRUCT_OPS ||\n\t     prog_type == BPF_PROG_TYPE_LSM) &&\n\t    !prog->aux->attach_func_proto->type)\n\t\treturn 0;\n\n\t/* eBPF calling convention is such that R0 is used\n\t * to return the value from eBPF program.\n\t * Make sure that it's readable at this time\n\t * of bpf_exit, which means that program wrote\n\t * something into it earlier\n\t */\n\terr = check_reg_arg(env, BPF_REG_0, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\tverbose(env, \"R0 leaks addr as return value\\n\");\n\t\treturn -EACCES;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\n\tif (frame->in_async_callback_fn) {\n\t\t/* enforce return zero from async callbacks like timer */\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"In async callback the register R0 is not a known value (%s)\\n\",\n\t\t\t\treg_type_str[reg->type]);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!tnum_in(tnum_const(0), reg->var_off)) {\n\t\t\tverbose_invalid_scalar(env, reg, &range, \"async callback\", \"R0\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (is_subprog) {\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"At subprogram exit the register R0 is not a scalar value (%s)\\n\",\n\t\t\t\treg_type_str[reg->type]);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tswitch (prog_type) {\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_UDP4_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_UDP6_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETSOCKNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETSOCKNAME)\n\t\t\trange = tnum_range(1, 1);\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET4_BIND ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_BIND)\n\t\t\trange = tnum_range(0, 3);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET_EGRESS) {\n\t\t\trange = tnum_range(0, 3);\n\t\t\tenforce_attach_type_range = tnum_range(2, 3);\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\tcase BPF_PROG_TYPE_CGROUP_SYSCTL:\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\t\tbreak;\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT:\n\t\tif (!env->prog->aux->attach_btf_id)\n\t\t\treturn 0;\n\t\trange = tnum_const(0);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_TRACING:\n\t\tswitch (env->prog->expected_attach_type) {\n\t\tcase BPF_TRACE_FENTRY:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\trange = tnum_const(0);\n\t\t\tbreak;\n\t\tcase BPF_TRACE_RAW_TP:\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\treturn 0;\n\t\tcase BPF_TRACE_ITER:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -ENOTSUPP;\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_SK_LOOKUP:\n\t\trange = tnum_range(SK_DROP, SK_PASS);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_EXT:\n\t\t/* freplace program can return anything as its return value\n\t\t * depends on the to-be-replaced kernel func or bpf program.\n\t\t */\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose_invalid_scalar(env, reg, &range, \"program exit\", \"R0\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_is_unknown(enforce_attach_type_range) &&\n\t    tnum_in(enforce_attach_type_range, reg->var_off))\n\t\tenv->prog->enforce_expected_attach_type = 1;\n\treturn 0;\n}",
                        "code_after_change": "static int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct tnum enforce_attach_type_range = tnum_unknown;\n\tconst struct bpf_prog *prog = env->prog;\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\tenum bpf_prog_type prog_type = resolve_prog_type(env->prog);\n\tint err;\n\tstruct bpf_func_state *frame = env->cur_state->frame[0];\n\tconst bool is_subprog = frame->subprogno;\n\n\t/* LSM and struct_ops func-ptr's return type could be \"void\" */\n\tif (!is_subprog &&\n\t    (prog_type == BPF_PROG_TYPE_STRUCT_OPS ||\n\t     prog_type == BPF_PROG_TYPE_LSM) &&\n\t    !prog->aux->attach_func_proto->type)\n\t\treturn 0;\n\n\t/* eBPF calling convention is such that R0 is used\n\t * to return the value from eBPF program.\n\t * Make sure that it's readable at this time\n\t * of bpf_exit, which means that program wrote\n\t * something into it earlier\n\t */\n\terr = check_reg_arg(env, BPF_REG_0, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\tverbose(env, \"R0 leaks addr as return value\\n\");\n\t\treturn -EACCES;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\n\tif (frame->in_async_callback_fn) {\n\t\t/* enforce return zero from async callbacks like timer */\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"In async callback the register R0 is not a known value (%s)\\n\",\n\t\t\t\treg_type_str(env, reg->type));\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!tnum_in(tnum_const(0), reg->var_off)) {\n\t\t\tverbose_invalid_scalar(env, reg, &range, \"async callback\", \"R0\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (is_subprog) {\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"At subprogram exit the register R0 is not a scalar value (%s)\\n\",\n\t\t\t\treg_type_str(env, reg->type));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tswitch (prog_type) {\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_UDP4_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_UDP6_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETSOCKNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETSOCKNAME)\n\t\t\trange = tnum_range(1, 1);\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET4_BIND ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_BIND)\n\t\t\trange = tnum_range(0, 3);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET_EGRESS) {\n\t\t\trange = tnum_range(0, 3);\n\t\t\tenforce_attach_type_range = tnum_range(2, 3);\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\tcase BPF_PROG_TYPE_CGROUP_SYSCTL:\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\t\tbreak;\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT:\n\t\tif (!env->prog->aux->attach_btf_id)\n\t\t\treturn 0;\n\t\trange = tnum_const(0);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_TRACING:\n\t\tswitch (env->prog->expected_attach_type) {\n\t\tcase BPF_TRACE_FENTRY:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\trange = tnum_const(0);\n\t\t\tbreak;\n\t\tcase BPF_TRACE_RAW_TP:\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\treturn 0;\n\t\tcase BPF_TRACE_ITER:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -ENOTSUPP;\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_SK_LOOKUP:\n\t\trange = tnum_range(SK_DROP, SK_PASS);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_EXT:\n\t\t/* freplace program can return anything as its return value\n\t\t * depends on the to-be-replaced kernel func or bpf program.\n\t\t */\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str(env, reg->type));\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose_invalid_scalar(env, reg, &range, \"program exit\", \"R0\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_is_unknown(enforce_attach_type_range) &&\n\t    tnum_in(enforce_attach_type_range, reg->var_off))\n\t\tenv->prog->enforce_expected_attach_type = 1;\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "code_after_change": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tif (ptr_reg->type & PTR_MAYBE_NULL) {\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\t}\n\n\tswitch (base_type(ptr_reg->type)) {\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3583,
            "cve_id": "CVE-2022-23222",
            "code_snippet": "static int check_ctx_access(struct bpf_verifier_env *env, int insn_idx, int off, int size,\n\t\t\t    enum bpf_access_type t, enum bpf_reg_type *reg_type,\n\t\t\t    struct btf **btf, u32 *btf_id)\n{\n\tstruct bpf_insn_access_aux info = {\n\t\t.reg_type = *reg_type,\n\t\t.log = &env->log,\n\t};\n\n\tif (env->ops->is_valid_access &&\n\t    env->ops->is_valid_access(off, size, t, env->prog, &info)) {\n\t\t/* A non zero info.ctx_field_size indicates that this field is a\n\t\t * candidate for later verifier transformation to load the whole\n\t\t * field and then apply a mask when accessed with a narrower\n\t\t * access than actual ctx access size. A zero info.ctx_field_size\n\t\t * will only allow for whole field access and rejects any other\n\t\t * type of narrower access.\n\t\t */\n\t\t*reg_type = info.reg_type;\n\n\t\tif (base_type(*reg_type) == PTR_TO_BTF_ID) {\n\t\t\t*btf = info.btf;\n\t\t\t*btf_id = info.btf_id;\n\t\t} else {\n\t\t\tenv->insn_aux_data[insn_idx].ctx_field_size = info.ctx_field_size;\n\t\t}\n\t\t/* remember the offset of last byte accessed in ctx */\n\t\tif (env->prog->aux->max_ctx_offset < off + size)\n\t\t\tenv->prog->aux->max_ctx_offset = off + size;\n\t\treturn 0;\n\t}\n\n\tverbose(env, \"invalid bpf_context access off=%d size=%d\\n\", off, size);\n\treturn -EACCES;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "bool btf_ctx_access(int off, int size, enum bpf_access_type type,\n\t\t    const struct bpf_prog *prog,\n\t\t    struct bpf_insn_access_aux *info)\n{\n\tconst struct btf_type *t = prog->aux->attach_func_proto;\n\tstruct bpf_prog *tgt_prog = prog->aux->dst_prog;\n\tstruct btf *btf = bpf_prog_get_target_btf(prog);\n\tconst char *tname = prog->aux->attach_func_name;\n\tstruct bpf_verifier_log *log = info->log;\n\tconst struct btf_param *args;\n\tu32 nr_args, arg;\n\tint i, ret;\n\n\tif (off % 8) {\n\t\tbpf_log(log, \"func '%s' offset %d is not multiple of 8\\n\",\n\t\t\ttname, off);\n\t\treturn false;\n\t}\n\targ = off / 8;\n\targs = (const struct btf_param *)(t + 1);\n\t/* if (t == NULL) Fall back to default BPF prog with\n\t * MAX_BPF_FUNC_REG_ARGS u64 arguments.\n\t */\n\tnr_args = t ? btf_type_vlen(t) : MAX_BPF_FUNC_REG_ARGS;\n\tif (prog->aux->attach_btf_trace) {\n\t\t/* skip first 'void *__data' argument in btf_trace_##name typedef */\n\t\targs++;\n\t\tnr_args--;\n\t}\n\n\tif (arg > nr_args) {\n\t\tbpf_log(log, \"func '%s' doesn't have %d-th argument\\n\",\n\t\t\ttname, arg + 1);\n\t\treturn false;\n\t}\n\n\tif (arg == nr_args) {\n\t\tswitch (prog->expected_attach_type) {\n\t\tcase BPF_LSM_MAC:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\t/* When LSM programs are attached to void LSM hooks\n\t\t\t * they use FEXIT trampolines and when attached to\n\t\t\t * int LSM hooks, they use MODIFY_RETURN trampolines.\n\t\t\t *\n\t\t\t * While the LSM programs are BPF_MODIFY_RETURN-like\n\t\t\t * the check:\n\t\t\t *\n\t\t\t *\tif (ret_type != 'int')\n\t\t\t *\t\treturn -EINVAL;\n\t\t\t *\n\t\t\t * is _not_ done here. This is still safe as LSM hooks\n\t\t\t * have only void and int return types.\n\t\t\t */\n\t\t\tif (!t)\n\t\t\t\treturn true;\n\t\t\tt = btf_type_by_id(btf, t->type);\n\t\t\tbreak;\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\t/* For now the BPF_MODIFY_RETURN can only be attached to\n\t\t\t * functions that return an int.\n\t\t\t */\n\t\t\tif (!t)\n\t\t\t\treturn false;\n\n\t\t\tt = btf_type_skip_modifiers(btf, t->type, NULL);\n\t\t\tif (!btf_type_is_small_int(t)) {\n\t\t\t\tbpf_log(log,\n\t\t\t\t\t\"ret type %s not allowed for fmod_ret\\n\",\n\t\t\t\t\tbtf_kind_str[BTF_INFO_KIND(t->info)]);\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbpf_log(log, \"func '%s' doesn't have %d-th argument\\n\",\n\t\t\t\ttname, arg + 1);\n\t\t\treturn false;\n\t\t}\n\t} else {\n\t\tif (!t)\n\t\t\t/* Default prog with MAX_BPF_FUNC_REG_ARGS args */\n\t\t\treturn true;\n\t\tt = btf_type_by_id(btf, args[arg].type);\n\t}\n\n\t/* skip modifiers */\n\twhile (btf_type_is_modifier(t))\n\t\tt = btf_type_by_id(btf, t->type);\n\tif (btf_type_is_small_int(t) || btf_type_is_enum(t))\n\t\t/* accessing a scalar */\n\t\treturn true;\n\tif (!btf_type_is_ptr(t)) {\n\t\tbpf_log(log,\n\t\t\t\"func '%s' arg%d '%s' has type %s. Only pointer access is allowed\\n\",\n\t\t\ttname, arg,\n\t\t\t__btf_name_by_offset(btf, t->name_off),\n\t\t\tbtf_kind_str[BTF_INFO_KIND(t->info)]);\n\t\treturn false;\n\t}\n\n\t/* check for PTR_TO_RDONLY_BUF_OR_NULL or PTR_TO_RDWR_BUF_OR_NULL */\n\tfor (i = 0; i < prog->aux->ctx_arg_info_size; i++) {\n\t\tconst struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i];\n\n\t\tif (ctx_arg_info->offset == off &&\n\t\t    (ctx_arg_info->reg_type == PTR_TO_RDONLY_BUF_OR_NULL ||\n\t\t     ctx_arg_info->reg_type == PTR_TO_RDWR_BUF_OR_NULL)) {\n\t\t\tinfo->reg_type = ctx_arg_info->reg_type;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tif (t->type == 0)\n\t\t/* This is a pointer to void.\n\t\t * It is the same as scalar from the verifier safety pov.\n\t\t * No further pointer walking is allowed.\n\t\t */\n\t\treturn true;\n\n\tif (is_int_ptr(btf, t))\n\t\treturn true;\n\n\t/* this is a pointer to another type */\n\tfor (i = 0; i < prog->aux->ctx_arg_info_size; i++) {\n\t\tconst struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i];\n\n\t\tif (ctx_arg_info->offset == off) {\n\t\t\tif (!ctx_arg_info->btf_id) {\n\t\t\t\tbpf_log(log,\"invalid btf_id for context argument offset %u\\n\", off);\n\t\t\t\treturn false;\n\t\t\t}\n\n\t\t\tinfo->reg_type = ctx_arg_info->reg_type;\n\t\t\tinfo->btf = btf_vmlinux;\n\t\t\tinfo->btf_id = ctx_arg_info->btf_id;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tinfo->reg_type = PTR_TO_BTF_ID;\n\tif (tgt_prog) {\n\t\tenum bpf_prog_type tgt_type;\n\n\t\tif (tgt_prog->type == BPF_PROG_TYPE_EXT)\n\t\t\ttgt_type = tgt_prog->aux->saved_dst_prog_type;\n\t\telse\n\t\t\ttgt_type = tgt_prog->type;\n\n\t\tret = btf_translate_to_vmlinux(log, btf, t, tgt_type, arg);\n\t\tif (ret > 0) {\n\t\t\tinfo->btf = btf_vmlinux;\n\t\t\tinfo->btf_id = ret;\n\t\t\treturn true;\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tinfo->btf = btf;\n\tinfo->btf_id = t->type;\n\tt = btf_type_by_id(btf, t->type);\n\t/* skip modifiers */\n\twhile (btf_type_is_modifier(t)) {\n\t\tinfo->btf_id = t->type;\n\t\tt = btf_type_by_id(btf, t->type);\n\t}\n\tif (!btf_type_is_struct(t)) {\n\t\tbpf_log(log,\n\t\t\t\"func '%s' arg%d type %s is not a struct\\n\",\n\t\t\ttname, arg, btf_kind_str[BTF_INFO_KIND(t->info)]);\n\t\treturn false;\n\t}\n\tbpf_log(log, \"func '%s' arg%d has btf_id %d type %s '%s'\\n\",\n\t\ttname, arg, info->btf_id, btf_kind_str[BTF_INFO_KIND(t->info)],\n\t\t__btf_name_by_offset(btf, t->name_off));\n\treturn true;\n}",
                        "code_after_change": "bool btf_ctx_access(int off, int size, enum bpf_access_type type,\n\t\t    const struct bpf_prog *prog,\n\t\t    struct bpf_insn_access_aux *info)\n{\n\tconst struct btf_type *t = prog->aux->attach_func_proto;\n\tstruct bpf_prog *tgt_prog = prog->aux->dst_prog;\n\tstruct btf *btf = bpf_prog_get_target_btf(prog);\n\tconst char *tname = prog->aux->attach_func_name;\n\tstruct bpf_verifier_log *log = info->log;\n\tconst struct btf_param *args;\n\tu32 nr_args, arg;\n\tint i, ret;\n\n\tif (off % 8) {\n\t\tbpf_log(log, \"func '%s' offset %d is not multiple of 8\\n\",\n\t\t\ttname, off);\n\t\treturn false;\n\t}\n\targ = off / 8;\n\targs = (const struct btf_param *)(t + 1);\n\t/* if (t == NULL) Fall back to default BPF prog with\n\t * MAX_BPF_FUNC_REG_ARGS u64 arguments.\n\t */\n\tnr_args = t ? btf_type_vlen(t) : MAX_BPF_FUNC_REG_ARGS;\n\tif (prog->aux->attach_btf_trace) {\n\t\t/* skip first 'void *__data' argument in btf_trace_##name typedef */\n\t\targs++;\n\t\tnr_args--;\n\t}\n\n\tif (arg > nr_args) {\n\t\tbpf_log(log, \"func '%s' doesn't have %d-th argument\\n\",\n\t\t\ttname, arg + 1);\n\t\treturn false;\n\t}\n\n\tif (arg == nr_args) {\n\t\tswitch (prog->expected_attach_type) {\n\t\tcase BPF_LSM_MAC:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\t/* When LSM programs are attached to void LSM hooks\n\t\t\t * they use FEXIT trampolines and when attached to\n\t\t\t * int LSM hooks, they use MODIFY_RETURN trampolines.\n\t\t\t *\n\t\t\t * While the LSM programs are BPF_MODIFY_RETURN-like\n\t\t\t * the check:\n\t\t\t *\n\t\t\t *\tif (ret_type != 'int')\n\t\t\t *\t\treturn -EINVAL;\n\t\t\t *\n\t\t\t * is _not_ done here. This is still safe as LSM hooks\n\t\t\t * have only void and int return types.\n\t\t\t */\n\t\t\tif (!t)\n\t\t\t\treturn true;\n\t\t\tt = btf_type_by_id(btf, t->type);\n\t\t\tbreak;\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\t/* For now the BPF_MODIFY_RETURN can only be attached to\n\t\t\t * functions that return an int.\n\t\t\t */\n\t\t\tif (!t)\n\t\t\t\treturn false;\n\n\t\t\tt = btf_type_skip_modifiers(btf, t->type, NULL);\n\t\t\tif (!btf_type_is_small_int(t)) {\n\t\t\t\tbpf_log(log,\n\t\t\t\t\t\"ret type %s not allowed for fmod_ret\\n\",\n\t\t\t\t\tbtf_kind_str[BTF_INFO_KIND(t->info)]);\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbpf_log(log, \"func '%s' doesn't have %d-th argument\\n\",\n\t\t\t\ttname, arg + 1);\n\t\t\treturn false;\n\t\t}\n\t} else {\n\t\tif (!t)\n\t\t\t/* Default prog with MAX_BPF_FUNC_REG_ARGS args */\n\t\t\treturn true;\n\t\tt = btf_type_by_id(btf, args[arg].type);\n\t}\n\n\t/* skip modifiers */\n\twhile (btf_type_is_modifier(t))\n\t\tt = btf_type_by_id(btf, t->type);\n\tif (btf_type_is_small_int(t) || btf_type_is_enum(t))\n\t\t/* accessing a scalar */\n\t\treturn true;\n\tif (!btf_type_is_ptr(t)) {\n\t\tbpf_log(log,\n\t\t\t\"func '%s' arg%d '%s' has type %s. Only pointer access is allowed\\n\",\n\t\t\ttname, arg,\n\t\t\t__btf_name_by_offset(btf, t->name_off),\n\t\t\tbtf_kind_str[BTF_INFO_KIND(t->info)]);\n\t\treturn false;\n\t}\n\n\t/* check for PTR_TO_RDONLY_BUF_OR_NULL or PTR_TO_RDWR_BUF_OR_NULL */\n\tfor (i = 0; i < prog->aux->ctx_arg_info_size; i++) {\n\t\tconst struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i];\n\t\tu32 type, flag;\n\n\t\ttype = base_type(ctx_arg_info->reg_type);\n\t\tflag = type_flag(ctx_arg_info->reg_type);\n\t\tif (ctx_arg_info->offset == off &&\n\t\t    (type == PTR_TO_RDWR_BUF || type == PTR_TO_RDONLY_BUF) &&\n\t\t    (flag & PTR_MAYBE_NULL)) {\n\t\t\tinfo->reg_type = ctx_arg_info->reg_type;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tif (t->type == 0)\n\t\t/* This is a pointer to void.\n\t\t * It is the same as scalar from the verifier safety pov.\n\t\t * No further pointer walking is allowed.\n\t\t */\n\t\treturn true;\n\n\tif (is_int_ptr(btf, t))\n\t\treturn true;\n\n\t/* this is a pointer to another type */\n\tfor (i = 0; i < prog->aux->ctx_arg_info_size; i++) {\n\t\tconst struct bpf_ctx_arg_aux *ctx_arg_info = &prog->aux->ctx_arg_info[i];\n\n\t\tif (ctx_arg_info->offset == off) {\n\t\t\tif (!ctx_arg_info->btf_id) {\n\t\t\t\tbpf_log(log,\"invalid btf_id for context argument offset %u\\n\", off);\n\t\t\t\treturn false;\n\t\t\t}\n\n\t\t\tinfo->reg_type = ctx_arg_info->reg_type;\n\t\t\tinfo->btf = btf_vmlinux;\n\t\t\tinfo->btf_id = ctx_arg_info->btf_id;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tinfo->reg_type = PTR_TO_BTF_ID;\n\tif (tgt_prog) {\n\t\tenum bpf_prog_type tgt_type;\n\n\t\tif (tgt_prog->type == BPF_PROG_TYPE_EXT)\n\t\t\ttgt_type = tgt_prog->aux->saved_dst_prog_type;\n\t\telse\n\t\t\ttgt_type = tgt_prog->type;\n\n\t\tret = btf_translate_to_vmlinux(log, btf, t, tgt_type, arg);\n\t\tif (ret > 0) {\n\t\t\tinfo->btf = btf_vmlinux;\n\t\t\tinfo->btf_id = ret;\n\t\t\treturn true;\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tinfo->btf = btf;\n\tinfo->btf_id = t->type;\n\tt = btf_type_by_id(btf, t->type);\n\t/* skip modifiers */\n\twhile (btf_type_is_modifier(t)) {\n\t\tinfo->btf_id = t->type;\n\t\tt = btf_type_by_id(btf, t->type);\n\t}\n\tif (!btf_type_is_struct(t)) {\n\t\tbpf_log(log,\n\t\t\t\"func '%s' arg%d type %s is not a struct\\n\",\n\t\t\ttname, arg, btf_kind_str[BTF_INFO_KIND(t->info)]);\n\t\treturn false;\n\t}\n\tbpf_log(log, \"func '%s' arg%d has btf_id %d type %s '%s'\\n\",\n\t\ttname, arg, info->btf_id, btf_kind_str[BTF_INFO_KIND(t->info)],\n\t\t__btf_name_by_offset(btf, t->name_off));\n\treturn true;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3584,
            "cve_id": "CVE-2022-23222",
            "code_snippet": "static int check_reg_type(struct bpf_verifier_env *env, u32 regno,\n\t\t\t  enum bpf_arg_type arg_type,\n\t\t\t  const u32 *arg_btf_id)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\tenum bpf_reg_type expected, type = reg->type;\n\tconst struct bpf_reg_types *compatible;\n\tint i, j;\n\n\tcompatible = compatible_reg_types[base_type(arg_type)];\n\tif (!compatible) {\n\t\tverbose(env, \"verifier internal error: unsupported arg type %d\\n\", arg_type);\n\t\treturn -EFAULT;\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(compatible->types); i++) {\n\t\texpected = compatible->types[i];\n\t\tif (expected == NOT_INIT)\n\t\t\tbreak;\n\n\t\tif (type == expected)\n\t\t\tgoto found;\n\t}\n\n\tverbose(env, \"R%d type=%s expected=\", regno, reg_type_str(env, type));\n\tfor (j = 0; j + 1 < i; j++)\n\t\tverbose(env, \"%s, \", reg_type_str(env, compatible->types[j]));\n\tverbose(env, \"%s\\n\", reg_type_str(env, compatible->types[j]));\n\treturn -EACCES;\n\nfound:\n\tif (type == PTR_TO_BTF_ID) {\n\t\tif (!arg_btf_id) {\n\t\t\tif (!compatible->btf_id) {\n\t\t\t\tverbose(env, \"verifier internal error: missing arg compatible BTF ID\\n\");\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t\targ_btf_id = compatible->btf_id;\n\t\t}\n\n\t\tif (!btf_struct_ids_match(&env->log, reg->btf, reg->btf_id, reg->off,\n\t\t\t\t\t  btf_vmlinux, *arg_btf_id)) {\n\t\t\tverbose(env, \"R%d is of type %s but %s is expected\\n\",\n\t\t\t\tregno, kernel_type_name(reg->btf, reg->btf_id),\n\t\t\t\tkernel_type_name(btf_vmlinux, *arg_btf_id));\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tif (!tnum_is_const(reg->var_off) || reg->var_off.value) {\n\t\t\tverbose(env, \"R%d is a pointer to in-kernel struct with non-zero offset\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state,\n\t\t\t\t bool print_all)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tif (!print_all && !reg_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\tverbose(env, \"P\");\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tif (t == PTR_TO_BTF_ID ||\n\t\t\t    t == PTR_TO_BTF_ID_OR_NULL ||\n\t\t\t    t == PTR_TO_PERCPU_BTF_ID)\n\t\t\t\tverbose(env, \"%s\", kernel_type_name(reg->btf, reg->btf_id));\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (reg_type_may_be_refcounted_or_null(t))\n\t\t\t\tverbose(env, \",ref_obj_id=%d\", reg->ref_obj_id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (t == CONST_PTR_TO_MAP ||\n\t\t\t\t t == PTR_TO_MAP_KEY ||\n\t\t\t\t t == PTR_TO_MAP_VALUE ||\n\t\t\t\t t == PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t\tif (reg->s32_min_value != reg->smin_value &&\n\t\t\t\t    reg->s32_min_value != S32_MIN)\n\t\t\t\t\tverbose(env, \",s32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_min_value));\n\t\t\t\tif (reg->s32_max_value != reg->smax_value &&\n\t\t\t\t    reg->s32_max_value != S32_MAX)\n\t\t\t\t\tverbose(env, \",s32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_max_value));\n\t\t\t\tif (reg->u32_min_value != reg->umin_value &&\n\t\t\t\t    reg->u32_min_value != U32_MIN)\n\t\t\t\t\tverbose(env, \",u32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_min_value));\n\t\t\t\tif (reg->u32_max_value != reg->umax_value &&\n\t\t\t\t    reg->u32_max_value != U32_MAX)\n\t\t\t\t\tverbose(env, \",u32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_max_value));\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tif (!print_all && !stack_slot_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (is_spilled_reg(&state->stack[i])) {\n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tt = reg->type;\n\t\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\t\tverbose(env, \"P\");\n\t\t\tif (t == SCALAR_VALUE && tnum_is_const(reg->var_off))\n\t\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t\t}\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tif (state->in_callback_fn)\n\t\tverbose(env, \" cb\");\n\tif (state->in_async_callback_fn)\n\t\tverbose(env, \" async_cb\");\n\tverbose(env, \"\\n\");\n\tmark_verifier_state_clean(env);\n}",
                        "code_after_change": "static void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state,\n\t\t\t\t bool print_all)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tif (!print_all && !reg_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str(env, t));\n\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\tverbose(env, \"P\");\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tif (base_type(t) == PTR_TO_BTF_ID ||\n\t\t\t    base_type(t) == PTR_TO_PERCPU_BTF_ID)\n\t\t\t\tverbose(env, \"%s\", kernel_type_name(reg->btf, reg->btf_id));\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (reg_type_may_be_refcounted_or_null(t))\n\t\t\t\tverbose(env, \",ref_obj_id=%d\", reg->ref_obj_id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (base_type(t) == CONST_PTR_TO_MAP ||\n\t\t\t\t base_type(t) == PTR_TO_MAP_KEY ||\n\t\t\t\t base_type(t) == PTR_TO_MAP_VALUE)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t\tif (reg->s32_min_value != reg->smin_value &&\n\t\t\t\t    reg->s32_min_value != S32_MIN)\n\t\t\t\t\tverbose(env, \",s32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_min_value));\n\t\t\t\tif (reg->s32_max_value != reg->smax_value &&\n\t\t\t\t    reg->s32_max_value != S32_MAX)\n\t\t\t\t\tverbose(env, \",s32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_max_value));\n\t\t\t\tif (reg->u32_min_value != reg->umin_value &&\n\t\t\t\t    reg->u32_min_value != U32_MIN)\n\t\t\t\t\tverbose(env, \",u32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_min_value));\n\t\t\t\tif (reg->u32_max_value != reg->umax_value &&\n\t\t\t\t    reg->u32_max_value != U32_MAX)\n\t\t\t\t\tverbose(env, \",u32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_max_value));\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tif (!print_all && !stack_slot_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (is_spilled_reg(&state->stack[i])) {\n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tt = reg->type;\n\t\t\tverbose(env, \"=%s\", reg_type_str(env, t));\n\t\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\t\tverbose(env, \"P\");\n\t\t\tif (t == SCALAR_VALUE && tnum_is_const(reg->var_off))\n\t\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t\t}\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tif (state->in_callback_fn)\n\t\tverbose(env, \" cb\");\n\tif (state->in_async_callback_fn)\n\t\tverbose(env, \" async_cb\");\n\tverbose(env, \"\\n\");\n\tmark_verifier_state_clean(env);\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_KEY:\n\t\treturn check_mem_region_access(env, regno, reg->off, access_size,\n\t\t\t\t\t       reg->map_ptr->key_size, false);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tcase PTR_TO_STACK:\n\t\treturn check_stack_range_initialized(\n\t\t\t\tenv,\n\t\t\t\tregno, reg->off, access_size,\n\t\t\t\tzero_size_allowed, ACCESS_HELPER, meta);\n\tdefault: /* scalar_value or invalid ptr */\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s expected=%s\\n\", regno,\n\t\t\treg_type_str[reg->type],\n\t\t\treg_type_str[PTR_TO_STACK]);\n\t\treturn -EACCES;\n\t}\n}",
                        "code_after_change": "static int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_KEY:\n\t\treturn check_mem_region_access(env, regno, reg->off, access_size,\n\t\t\t\t\t       reg->map_ptr->key_size, false);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tcase PTR_TO_STACK:\n\t\treturn check_stack_range_initialized(\n\t\t\t\tenv,\n\t\t\t\tregno, reg->off, access_size,\n\t\t\t\tzero_size_allowed, ACCESS_HELPER, meta);\n\tdefault: /* scalar_value or invalid ptr */\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s \", regno,\n\t\t\treg_type_str(env, reg->type));\n\t\tverbose(env, \"expected=%s\\n\", reg_type_str(env, PTR_TO_STACK));\n\t\treturn -EACCES;\n\t}\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}",
                        "code_after_change": "static bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str(env, type), val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str(env, type), reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str(env, type));\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str(env, type));\n\t\treturn false;\n\t}\n\n\treturn true;\n}",
                        "cve_id": "CVE-2022-23222"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3689,
            "cve_id": "CVE-2022-3110",
            "code_snippet": "s32\t_rtw_init_xmit_priv(struct xmit_priv *pxmitpriv, struct adapter *padapter)\n{\n\tint i;\n\tstruct xmit_buf *pxmitbuf;\n\tstruct xmit_frame *pxframe;\n\tint\tres = _SUCCESS;\n\tu32 max_xmit_extbuf_size = MAX_XMIT_EXTBUF_SZ;\n\tu32 num_xmit_extbuf = NR_XMIT_EXTBUFF;\n\n\t/*  We don't need to memset padapter->XXX to zero, because adapter is allocated by vzalloc(). */\n\n\tspin_lock_init(&pxmitpriv->lock);\n\tsema_init(&pxmitpriv->terminate_xmitthread_sema, 0);\n\n\t/*\n\t * Please insert all the queue initializaiton using rtw_init_queue below\n\t */\n\n\tpxmitpriv->adapter = padapter;\n\n\trtw_init_queue(&pxmitpriv->be_pending);\n\trtw_init_queue(&pxmitpriv->bk_pending);\n\trtw_init_queue(&pxmitpriv->vi_pending);\n\trtw_init_queue(&pxmitpriv->vo_pending);\n\trtw_init_queue(&pxmitpriv->bm_pending);\n\n\trtw_init_queue(&pxmitpriv->free_xmit_queue);\n\n\t/*\n\t * Please allocate memory with the sz = (struct xmit_frame) * NR_XMITFRAME,\n\t * and initialize free_xmit_frame below.\n\t * Please also apply  free_txobj to link_up all the xmit_frames...\n\t */\n\n\tpxmitpriv->pallocated_frame_buf = vzalloc(NR_XMITFRAME * sizeof(struct xmit_frame) + 4);\n\n\tif (!pxmitpriv->pallocated_frame_buf) {\n\t\tpxmitpriv->pxmit_frame_buf = NULL;\n\t\tres = _FAIL;\n\t\tgoto exit;\n\t}\n\tpxmitpriv->pxmit_frame_buf = (u8 *)N_BYTE_ALIGMENT((size_t)(pxmitpriv->pallocated_frame_buf), 4);\n\t/* pxmitpriv->pxmit_frame_buf = pxmitpriv->pallocated_frame_buf + 4 - */\n\t/* \t\t\t\t\t\t((size_t) (pxmitpriv->pallocated_frame_buf) &3); */\n\n\tpxframe = (struct xmit_frame *)pxmitpriv->pxmit_frame_buf;\n\n\tfor (i = 0; i < NR_XMITFRAME; i++) {\n\t\tINIT_LIST_HEAD(&pxframe->list);\n\n\t\tpxframe->padapter = padapter;\n\t\tpxframe->frame_tag = NULL_FRAMETAG;\n\n\t\tpxframe->pkt = NULL;\n\n\t\tpxframe->buf_addr = NULL;\n\t\tpxframe->pxmitbuf = NULL;\n\n\t\tlist_add_tail(&pxframe->list, &pxmitpriv->free_xmit_queue.queue);\n\n\t\tpxframe++;\n\t}\n\n\tpxmitpriv->free_xmitframe_cnt = NR_XMITFRAME;\n\n\tpxmitpriv->frag_len = MAX_FRAG_THRESHOLD;\n\n\t/* init xmit_buf */\n\trtw_init_queue(&pxmitpriv->free_xmitbuf_queue);\n\trtw_init_queue(&pxmitpriv->pending_xmitbuf_queue);\n\n\tpxmitpriv->pallocated_xmitbuf = vzalloc(NR_XMITBUFF * sizeof(struct xmit_buf) + 4);\n\n\tif (!pxmitpriv->pallocated_xmitbuf) {\n\t\tres = _FAIL;\n\t\tgoto exit;\n\t}\n\n\tpxmitpriv->pxmitbuf = (u8 *)N_BYTE_ALIGMENT((size_t)(pxmitpriv->pallocated_xmitbuf), 4);\n\t/* pxmitpriv->pxmitbuf = pxmitpriv->pallocated_xmitbuf + 4 - */\n\t/* \t\t\t\t\t\t((size_t) (pxmitpriv->pallocated_xmitbuf) &3); */\n\n\tpxmitbuf = (struct xmit_buf *)pxmitpriv->pxmitbuf;\n\n\tfor (i = 0; i < NR_XMITBUFF; i++) {\n\t\tINIT_LIST_HEAD(&pxmitbuf->list);\n\n\t\tpxmitbuf->priv_data = NULL;\n\t\tpxmitbuf->padapter = padapter;\n\t\tpxmitbuf->ext_tag = false;\n\n\t\t/* Tx buf allocation may fail sometimes, so sleep and retry. */\n\t\tres = rtw_os_xmit_resource_alloc(padapter, pxmitbuf, (MAX_XMITBUF_SZ + XMITBUF_ALIGN_SZ));\n\t\tif (res == _FAIL) {\n\t\t\tmsleep(10);\n\t\t\tres = rtw_os_xmit_resource_alloc(padapter, pxmitbuf, (MAX_XMITBUF_SZ + XMITBUF_ALIGN_SZ));\n\t\t\tif (res == _FAIL)\n\t\t\t\tgoto exit;\n\t\t}\n\n\t\tpxmitbuf->flags = XMIT_VO_QUEUE;\n\n\t\tlist_add_tail(&pxmitbuf->list, &pxmitpriv->free_xmitbuf_queue.queue);\n\t\tpxmitbuf++;\n\t}\n\n\tpxmitpriv->free_xmitbuf_cnt = NR_XMITBUFF;\n\n\t/*  Init xmit extension buff */\n\trtw_init_queue(&pxmitpriv->free_xmit_extbuf_queue);\n\n\tpxmitpriv->pallocated_xmit_extbuf = vzalloc(num_xmit_extbuf * sizeof(struct xmit_buf) + 4);\n\n\tif (!pxmitpriv->pallocated_xmit_extbuf) {\n\t\tres = _FAIL;\n\t\tgoto exit;\n\t}\n\n\tpxmitpriv->pxmit_extbuf = (u8 *)N_BYTE_ALIGMENT((size_t)(pxmitpriv->pallocated_xmit_extbuf), 4);\n\n\tpxmitbuf = (struct xmit_buf *)pxmitpriv->pxmit_extbuf;\n\n\tfor (i = 0; i < num_xmit_extbuf; i++) {\n\t\tINIT_LIST_HEAD(&pxmitbuf->list);\n\n\t\tpxmitbuf->priv_data = NULL;\n\t\tpxmitbuf->padapter = padapter;\n\t\tpxmitbuf->ext_tag = true;\n\n\t\tres = rtw_os_xmit_resource_alloc(padapter, pxmitbuf, max_xmit_extbuf_size + XMITBUF_ALIGN_SZ);\n\t\tif (res == _FAIL) {\n\t\t\tres = _FAIL;\n\t\t\tgoto exit;\n\t\t}\n\n\t\tlist_add_tail(&pxmitbuf->list, &pxmitpriv->free_xmit_extbuf_queue.queue);\n\t\tpxmitbuf++;\n\t}\n\n\tpxmitpriv->free_xmit_extbuf_cnt = num_xmit_extbuf;\n\n\tres = rtw_alloc_hwxmits(padapter);\n\tif (res) {\n\t\tres = _FAIL;\n\t\tgoto exit;\n\t}\n\n\trtw_init_hwxmits(pxmitpriv->hwxmits, pxmitpriv->hwxmit_entry);\n\n\tfor (i = 0; i < 4; i++)\n\t\tpxmitpriv->wmm_para_seq[i] = i;\n\n\tpxmitpriv->txirp_cnt = 1;\n\n\tsema_init(&pxmitpriv->tx_retevt, 0);\n\n\t/* per AC pending irp */\n\tpxmitpriv->beq_cnt = 0;\n\tpxmitpriv->bkq_cnt = 0;\n\tpxmitpriv->viq_cnt = 0;\n\tpxmitpriv->voq_cnt = 0;\n\n\tpxmitpriv->ack_tx = false;\n\tmutex_init(&pxmitpriv->ack_tx_mutex);\n\trtw_sctx_init(&pxmitpriv->ack_tx_ops, 0);\n\n\trtl8188eu_init_xmit_priv(padapter);\n\nexit:\n\n\treturn res;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "void rtw_alloc_hwxmits(struct adapter *padapter)\n{\n\tstruct hw_xmit *hwxmits;\n\tstruct xmit_priv *pxmitpriv = &padapter->xmitpriv;\n\n\tpxmitpriv->hwxmit_entry = HWXMIT_ENTRY;\n\n\tpxmitpriv->hwxmits = kzalloc(sizeof(struct hw_xmit) * pxmitpriv->hwxmit_entry, GFP_KERNEL);\n\n\thwxmits = pxmitpriv->hwxmits;\n\n\tif (pxmitpriv->hwxmit_entry == 5) {\n\t\thwxmits[0] .sta_queue = &pxmitpriv->bm_pending;\n\t\thwxmits[1] .sta_queue = &pxmitpriv->vo_pending;\n\t\thwxmits[2] .sta_queue = &pxmitpriv->vi_pending;\n\t\thwxmits[3] .sta_queue = &pxmitpriv->bk_pending;\n\t\thwxmits[4] .sta_queue = &pxmitpriv->be_pending;\n\t} else if (pxmitpriv->hwxmit_entry == 4) {\n\t\thwxmits[0] .sta_queue = &pxmitpriv->vo_pending;\n\t\thwxmits[1] .sta_queue = &pxmitpriv->vi_pending;\n\t\thwxmits[2] .sta_queue = &pxmitpriv->be_pending;\n\t\thwxmits[3] .sta_queue = &pxmitpriv->bk_pending;\n\t} else {\n\t}\n}",
                        "code_after_change": "int rtw_alloc_hwxmits(struct adapter *padapter)\n{\n\tstruct hw_xmit *hwxmits;\n\tstruct xmit_priv *pxmitpriv = &padapter->xmitpriv;\n\n\tpxmitpriv->hwxmit_entry = HWXMIT_ENTRY;\n\n\tpxmitpriv->hwxmits = kzalloc(sizeof(struct hw_xmit) * pxmitpriv->hwxmit_entry, GFP_KERNEL);\n\tif (!pxmitpriv->hwxmits)\n\t\treturn -ENOMEM;\n\n\thwxmits = pxmitpriv->hwxmits;\n\n\tif (pxmitpriv->hwxmit_entry == 5) {\n\t\thwxmits[0] .sta_queue = &pxmitpriv->bm_pending;\n\t\thwxmits[1] .sta_queue = &pxmitpriv->vo_pending;\n\t\thwxmits[2] .sta_queue = &pxmitpriv->vi_pending;\n\t\thwxmits[3] .sta_queue = &pxmitpriv->bk_pending;\n\t\thwxmits[4] .sta_queue = &pxmitpriv->be_pending;\n\t} else if (pxmitpriv->hwxmit_entry == 4) {\n\t\thwxmits[0] .sta_queue = &pxmitpriv->vo_pending;\n\t\thwxmits[1] .sta_queue = &pxmitpriv->vi_pending;\n\t\thwxmits[2] .sta_queue = &pxmitpriv->be_pending;\n\t\thwxmits[3] .sta_queue = &pxmitpriv->bk_pending;\n\t} else {\n\t}\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-3110"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 3692,
            "cve_id": "CVE-2022-3112",
            "code_snippet": "int amvdec_add_ts(struct amvdec_session *sess, u64 ts,\n\t\t  struct v4l2_timecode tc, u32 offset, u32 vbuf_flags)\n{\n\tstruct amvdec_timestamp *new_ts;\n\tunsigned long flags;\n\n\tnew_ts = kzalloc(sizeof(*new_ts), GFP_KERNEL);\n\tif (!new_ts)\n\t\treturn -ENOMEM;\n\n\tnew_ts->ts = ts;\n\tnew_ts->tc = tc;\n\tnew_ts->offset = offset;\n\tnew_ts->flags = vbuf_flags;\n\n\tspin_lock_irqsave(&sess->ts_spinlock, flags);\n\tlist_add_tail(&new_ts->list, &sess->timestamps);\n\tspin_unlock_irqrestore(&sess->ts_spinlock, flags);\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int\nesparser_queue(struct amvdec_session *sess, struct vb2_v4l2_buffer *vbuf)\n{\n\tint ret;\n\tstruct vb2_buffer *vb = &vbuf->vb2_buf;\n\tstruct amvdec_core *core = sess->core;\n\tstruct amvdec_codec_ops *codec_ops = sess->fmt_out->codec_ops;\n\tu32 payload_size = vb2_get_plane_payload(vb, 0);\n\tdma_addr_t phy = vb2_dma_contig_plane_dma_addr(vb, 0);\n\tu32 num_dst_bufs = 0;\n\tu32 offset;\n\tu32 pad_size;\n\n\t/*\n\t * When max ref frame is held by VP9, this should be -= 3 to prevent a\n\t * shortage of CAPTURE buffers on the decoder side.\n\t * For the future, a good enhancement of the way this is handled could\n\t * be to notify new capture buffers to the decoding modules, so that\n\t * they could pause when there is no capture buffer available and\n\t * resume on this notification.\n\t */\n\tif (sess->fmt_out->pixfmt == V4L2_PIX_FMT_VP9) {\n\t\tif (codec_ops->num_pending_bufs)\n\t\t\tnum_dst_bufs = codec_ops->num_pending_bufs(sess);\n\n\t\tnum_dst_bufs += v4l2_m2m_num_dst_bufs_ready(sess->m2m_ctx);\n\t\tif (sess->fmt_out->pixfmt == V4L2_PIX_FMT_VP9)\n\t\t\tnum_dst_bufs -= 3;\n\n\t\tif (esparser_vififo_get_free_space(sess) < payload_size ||\n\t\t    atomic_read(&sess->esparser_queued_bufs) >= num_dst_bufs)\n\t\t\treturn -EAGAIN;\n\t} else if (esparser_vififo_get_free_space(sess) < payload_size) {\n\t\treturn -EAGAIN;\n\t}\n\n\tv4l2_m2m_src_buf_remove_by_buf(sess->m2m_ctx, vbuf);\n\n\toffset = esparser_get_offset(sess);\n\n\tamvdec_add_ts(sess, vb->timestamp, vbuf->timecode, offset, vbuf->flags);\n\tdev_dbg(core->dev, \"esparser: ts = %llu pld_size = %u offset = %08X flags = %08X\\n\",\n\t\tvb->timestamp, payload_size, offset, vbuf->flags);\n\n\tvbuf->flags = 0;\n\tvbuf->field = V4L2_FIELD_NONE;\n\tvbuf->sequence = sess->sequence_out++;\n\n\tif (sess->fmt_out->pixfmt == V4L2_PIX_FMT_VP9) {\n\t\tpayload_size = vp9_update_header(core, vb);\n\n\t\t/* If unable to alter buffer to add headers */\n\t\tif (payload_size == 0) {\n\t\t\tamvdec_remove_ts(sess, vb->timestamp);\n\t\t\tv4l2_m2m_buf_done(vbuf, VB2_BUF_STATE_ERROR);\n\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tpad_size = esparser_pad_start_code(core, vb, payload_size);\n\tret = esparser_write_data(core, phy, payload_size + pad_size);\n\n\tif (ret <= 0) {\n\t\tdev_warn(core->dev, \"esparser: input parsing error\\n\");\n\t\tamvdec_remove_ts(sess, vb->timestamp);\n\t\tv4l2_m2m_buf_done(vbuf, VB2_BUF_STATE_ERROR);\n\t\tamvdec_write_parser(core, PARSER_FETCH_CMD, 0);\n\n\t\treturn 0;\n\t}\n\n\tatomic_inc(&sess->esparser_queued_bufs);\n\tv4l2_m2m_buf_done(vbuf, VB2_BUF_STATE_DONE);\n\n\treturn 0;\n}",
                        "code_after_change": "static int\nesparser_queue(struct amvdec_session *sess, struct vb2_v4l2_buffer *vbuf)\n{\n\tint ret;\n\tstruct vb2_buffer *vb = &vbuf->vb2_buf;\n\tstruct amvdec_core *core = sess->core;\n\tstruct amvdec_codec_ops *codec_ops = sess->fmt_out->codec_ops;\n\tu32 payload_size = vb2_get_plane_payload(vb, 0);\n\tdma_addr_t phy = vb2_dma_contig_plane_dma_addr(vb, 0);\n\tu32 num_dst_bufs = 0;\n\tu32 offset;\n\tu32 pad_size;\n\n\t/*\n\t * When max ref frame is held by VP9, this should be -= 3 to prevent a\n\t * shortage of CAPTURE buffers on the decoder side.\n\t * For the future, a good enhancement of the way this is handled could\n\t * be to notify new capture buffers to the decoding modules, so that\n\t * they could pause when there is no capture buffer available and\n\t * resume on this notification.\n\t */\n\tif (sess->fmt_out->pixfmt == V4L2_PIX_FMT_VP9) {\n\t\tif (codec_ops->num_pending_bufs)\n\t\t\tnum_dst_bufs = codec_ops->num_pending_bufs(sess);\n\n\t\tnum_dst_bufs += v4l2_m2m_num_dst_bufs_ready(sess->m2m_ctx);\n\t\tif (sess->fmt_out->pixfmt == V4L2_PIX_FMT_VP9)\n\t\t\tnum_dst_bufs -= 3;\n\n\t\tif (esparser_vififo_get_free_space(sess) < payload_size ||\n\t\t    atomic_read(&sess->esparser_queued_bufs) >= num_dst_bufs)\n\t\t\treturn -EAGAIN;\n\t} else if (esparser_vififo_get_free_space(sess) < payload_size) {\n\t\treturn -EAGAIN;\n\t}\n\n\tv4l2_m2m_src_buf_remove_by_buf(sess->m2m_ctx, vbuf);\n\n\toffset = esparser_get_offset(sess);\n\n\tret = amvdec_add_ts(sess, vb->timestamp, vbuf->timecode, offset, vbuf->flags);\n\tif (ret) {\n\t\tv4l2_m2m_buf_done(vbuf, VB2_BUF_STATE_ERROR);\n\t\treturn ret;\n\t}\n\n\tdev_dbg(core->dev, \"esparser: ts = %llu pld_size = %u offset = %08X flags = %08X\\n\",\n\t\tvb->timestamp, payload_size, offset, vbuf->flags);\n\n\tvbuf->flags = 0;\n\tvbuf->field = V4L2_FIELD_NONE;\n\tvbuf->sequence = sess->sequence_out++;\n\n\tif (sess->fmt_out->pixfmt == V4L2_PIX_FMT_VP9) {\n\t\tpayload_size = vp9_update_header(core, vb);\n\n\t\t/* If unable to alter buffer to add headers */\n\t\tif (payload_size == 0) {\n\t\t\tamvdec_remove_ts(sess, vb->timestamp);\n\t\t\tv4l2_m2m_buf_done(vbuf, VB2_BUF_STATE_ERROR);\n\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tpad_size = esparser_pad_start_code(core, vb, payload_size);\n\tret = esparser_write_data(core, phy, payload_size + pad_size);\n\n\tif (ret <= 0) {\n\t\tdev_warn(core->dev, \"esparser: input parsing error\\n\");\n\t\tamvdec_remove_ts(sess, vb->timestamp);\n\t\tv4l2_m2m_buf_done(vbuf, VB2_BUF_STATE_ERROR);\n\t\tamvdec_write_parser(core, PARSER_FETCH_CMD, 0);\n\n\t\treturn 0;\n\t}\n\n\tatomic_inc(&sess->esparser_queued_bufs);\n\tv4l2_m2m_buf_done(vbuf, VB2_BUF_STATE_DONE);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2022-3112"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 4014,
            "cve_id": "CVE-2023-1382",
            "code_snippet": "static void tipc_topsrv_accept(struct work_struct *work)\n{\n\tstruct tipc_topsrv *srv = container_of(work, struct tipc_topsrv, awork);\n\tstruct socket *newsock, *lsock;\n\tstruct tipc_conn *con;\n\tstruct sock *newsk;\n\tint ret;\n\n\tspin_lock_bh(&srv->idr_lock);\n\tif (!srv->listener) {\n\t\tspin_unlock_bh(&srv->idr_lock);\n\t\treturn;\n\t}\n\tlsock = srv->listener;\n\tspin_unlock_bh(&srv->idr_lock);\n\n\twhile (1) {\n\t\tret = kernel_accept(lsock, &newsock, O_NONBLOCK);\n\t\tif (ret < 0)\n\t\t\treturn;\n\t\tcon = tipc_conn_alloc(srv, newsock);\n\t\tif (IS_ERR(con)) {\n\t\t\tret = PTR_ERR(con);\n\t\t\tsock_release(newsock);\n\t\t\treturn;\n\t\t}\n\t\t/* Register callbacks */\n\t\tnewsk = newsock->sk;\n\t\twrite_lock_bh(&newsk->sk_callback_lock);\n\t\tnewsk->sk_data_ready = tipc_conn_data_ready;\n\t\tnewsk->sk_write_space = tipc_conn_write_space;\n\t\tnewsk->sk_user_data = con;\n\t\twrite_unlock_bh(&newsk->sk_callback_lock);\n\n\t\t/* Wake up receive process in case of 'SYN+' message */\n\t\tnewsk->sk_data_ready(newsk);\n\t}\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "struct sock *sk_clone_lock(const struct sock *sk, const gfp_t priority)\n{\n\tstruct proto *prot = READ_ONCE(sk->sk_prot);\n\tstruct sock *newsk;\n\tbool is_charged = true;\n\n\tnewsk = sk_prot_alloc(prot, priority, sk->sk_family);\n\tif (newsk != NULL) {\n\t\tstruct sk_filter *filter;\n\n\t\tsock_copy(newsk, sk);\n\n\t\tnewsk->sk_prot_creator = prot;\n\n\t\t/* SANITY */\n\t\tif (likely(newsk->sk_net_refcnt))\n\t\t\tget_net(sock_net(newsk));\n\t\tsk_node_init(&newsk->sk_node);\n\t\tsock_lock_init(newsk);\n\t\tbh_lock_sock(newsk);\n\t\tnewsk->sk_backlog.head\t= newsk->sk_backlog.tail = NULL;\n\t\tnewsk->sk_backlog.len = 0;\n\n\t\tatomic_set(&newsk->sk_rmem_alloc, 0);\n\t\t/*\n\t\t * sk_wmem_alloc set to one (see sk_free() and sock_wfree())\n\t\t */\n\t\trefcount_set(&newsk->sk_wmem_alloc, 1);\n\t\tatomic_set(&newsk->sk_omem_alloc, 0);\n\t\tsk_init_common(newsk);\n\n\t\tnewsk->sk_dst_cache\t= NULL;\n\t\tnewsk->sk_dst_pending_confirm = 0;\n\t\tnewsk->sk_wmem_queued\t= 0;\n\t\tnewsk->sk_forward_alloc = 0;\n\t\tatomic_set(&newsk->sk_drops, 0);\n\t\tnewsk->sk_send_head\t= NULL;\n\t\tnewsk->sk_userlocks\t= sk->sk_userlocks & ~SOCK_BINDPORT_LOCK;\n\t\tatomic_set(&newsk->sk_zckey, 0);\n\n\t\tsock_reset_flag(newsk, SOCK_DONE);\n\n\t\t/* sk->sk_memcg will be populated at accept() time */\n\t\tnewsk->sk_memcg = NULL;\n\n\t\tcgroup_sk_alloc(&newsk->sk_cgrp_data);\n\n\t\trcu_read_lock();\n\t\tfilter = rcu_dereference(sk->sk_filter);\n\t\tif (filter != NULL)\n\t\t\t/* though it's an empty new sock, the charging may fail\n\t\t\t * if sysctl_optmem_max was changed between creation of\n\t\t\t * original socket and cloning\n\t\t\t */\n\t\t\tis_charged = sk_filter_charge(newsk, filter);\n\t\tRCU_INIT_POINTER(newsk->sk_filter, filter);\n\t\trcu_read_unlock();\n\n\t\tif (unlikely(!is_charged || xfrm_sk_clone_policy(newsk, sk))) {\n\t\t\t/* We need to make sure that we don't uncharge the new\n\t\t\t * socket if we couldn't charge it in the first place\n\t\t\t * as otherwise we uncharge the parent's filter.\n\t\t\t */\n\t\t\tif (!is_charged)\n\t\t\t\tRCU_INIT_POINTER(newsk->sk_filter, NULL);\n\t\t\tsk_free_unlock_clone(newsk);\n\t\t\tnewsk = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tRCU_INIT_POINTER(newsk->sk_reuseport_cb, NULL);\n\n\t\tif (bpf_sk_storage_clone(sk, newsk)) {\n\t\t\tsk_free_unlock_clone(newsk);\n\t\t\tnewsk = NULL;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/* Clear sk_user_data if parent had the pointer tagged\n\t\t * as not suitable for copying when cloning.\n\t\t */\n\t\tif (sk_user_data_is_nocopy(newsk))\n\t\t\tnewsk->sk_user_data = NULL;\n\n\t\tnewsk->sk_err\t   = 0;\n\t\tnewsk->sk_err_soft = 0;\n\t\tnewsk->sk_priority = 0;\n\t\tnewsk->sk_incoming_cpu = raw_smp_processor_id();\n\t\tif (likely(newsk->sk_net_refcnt))\n\t\t\tsock_inuse_add(sock_net(newsk), 1);\n\n\t\t/*\n\t\t * Before updating sk_refcnt, we must commit prior changes to memory\n\t\t * (Documentation/RCU/rculist_nulls.txt for details)\n\t\t */\n\t\tsmp_wmb();\n\t\trefcount_set(&newsk->sk_refcnt, 2);\n\n\t\t/*\n\t\t * Increment the counter in the same struct proto as the master\n\t\t * sock (sk_refcnt_debug_inc uses newsk->sk_prot->socks, that\n\t\t * is the same as sk->sk_prot->socks, as this field was copied\n\t\t * with memcpy).\n\t\t *\n\t\t * This _changes_ the previous behaviour, where\n\t\t * tcp_create_openreq_child always was incrementing the\n\t\t * equivalent to tcp_prot->socks (inet_sock_nr), so this have\n\t\t * to be taken into account in all callers. -acme\n\t\t */\n\t\tsk_refcnt_debug_inc(newsk);\n\t\tsk_set_socket(newsk, NULL);\n\t\tsk_tx_queue_clear(newsk);\n\t\tRCU_INIT_POINTER(newsk->sk_wq, NULL);\n\n\t\tif (newsk->sk_prot->sockets_allocated)\n\t\t\tsk_sockets_allocated_inc(newsk);\n\n\t\tif (sock_needs_netstamp(sk) &&\n\t\t    newsk->sk_flags & SK_FLAGS_TIMESTAMP)\n\t\t\tnet_enable_timestamp();\n\t}\nout:\n\treturn newsk;\n}",
                        "code_after_change": "struct sock *sk_clone_lock(const struct sock *sk, const gfp_t priority)\n{\n\tstruct proto *prot = READ_ONCE(sk->sk_prot);\n\tstruct sock *newsk;\n\tbool is_charged = true;\n\n\tnewsk = sk_prot_alloc(prot, priority, sk->sk_family);\n\tif (newsk != NULL) {\n\t\tstruct sk_filter *filter;\n\n\t\tsock_copy(newsk, sk);\n\n\t\tnewsk->sk_prot_creator = prot;\n\n\t\t/* SANITY */\n\t\tif (likely(newsk->sk_net_refcnt))\n\t\t\tget_net(sock_net(newsk));\n\t\tsk_node_init(&newsk->sk_node);\n\t\tsock_lock_init(newsk);\n\t\tbh_lock_sock(newsk);\n\t\tnewsk->sk_backlog.head\t= newsk->sk_backlog.tail = NULL;\n\t\tnewsk->sk_backlog.len = 0;\n\n\t\tatomic_set(&newsk->sk_rmem_alloc, 0);\n\t\t/*\n\t\t * sk_wmem_alloc set to one (see sk_free() and sock_wfree())\n\t\t */\n\t\trefcount_set(&newsk->sk_wmem_alloc, 1);\n\t\tatomic_set(&newsk->sk_omem_alloc, 0);\n\t\tsk_init_common(newsk);\n\n\t\tnewsk->sk_dst_cache\t= NULL;\n\t\tnewsk->sk_dst_pending_confirm = 0;\n\t\tnewsk->sk_wmem_queued\t= 0;\n\t\tnewsk->sk_forward_alloc = 0;\n\t\tatomic_set(&newsk->sk_drops, 0);\n\t\tnewsk->sk_send_head\t= NULL;\n\t\tnewsk->sk_userlocks\t= sk->sk_userlocks & ~SOCK_BINDPORT_LOCK;\n\t\tatomic_set(&newsk->sk_zckey, 0);\n\n\t\tsock_reset_flag(newsk, SOCK_DONE);\n\n\t\t/* sk->sk_memcg will be populated at accept() time */\n\t\tnewsk->sk_memcg = NULL;\n\n\t\tcgroup_sk_clone(&newsk->sk_cgrp_data);\n\n\t\trcu_read_lock();\n\t\tfilter = rcu_dereference(sk->sk_filter);\n\t\tif (filter != NULL)\n\t\t\t/* though it's an empty new sock, the charging may fail\n\t\t\t * if sysctl_optmem_max was changed between creation of\n\t\t\t * original socket and cloning\n\t\t\t */\n\t\t\tis_charged = sk_filter_charge(newsk, filter);\n\t\tRCU_INIT_POINTER(newsk->sk_filter, filter);\n\t\trcu_read_unlock();\n\n\t\tif (unlikely(!is_charged || xfrm_sk_clone_policy(newsk, sk))) {\n\t\t\t/* We need to make sure that we don't uncharge the new\n\t\t\t * socket if we couldn't charge it in the first place\n\t\t\t * as otherwise we uncharge the parent's filter.\n\t\t\t */\n\t\t\tif (!is_charged)\n\t\t\t\tRCU_INIT_POINTER(newsk->sk_filter, NULL);\n\t\t\tsk_free_unlock_clone(newsk);\n\t\t\tnewsk = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tRCU_INIT_POINTER(newsk->sk_reuseport_cb, NULL);\n\n\t\tif (bpf_sk_storage_clone(sk, newsk)) {\n\t\t\tsk_free_unlock_clone(newsk);\n\t\t\tnewsk = NULL;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/* Clear sk_user_data if parent had the pointer tagged\n\t\t * as not suitable for copying when cloning.\n\t\t */\n\t\tif (sk_user_data_is_nocopy(newsk))\n\t\t\tnewsk->sk_user_data = NULL;\n\n\t\tnewsk->sk_err\t   = 0;\n\t\tnewsk->sk_err_soft = 0;\n\t\tnewsk->sk_priority = 0;\n\t\tnewsk->sk_incoming_cpu = raw_smp_processor_id();\n\t\tif (likely(newsk->sk_net_refcnt))\n\t\t\tsock_inuse_add(sock_net(newsk), 1);\n\n\t\t/*\n\t\t * Before updating sk_refcnt, we must commit prior changes to memory\n\t\t * (Documentation/RCU/rculist_nulls.txt for details)\n\t\t */\n\t\tsmp_wmb();\n\t\trefcount_set(&newsk->sk_refcnt, 2);\n\n\t\t/*\n\t\t * Increment the counter in the same struct proto as the master\n\t\t * sock (sk_refcnt_debug_inc uses newsk->sk_prot->socks, that\n\t\t * is the same as sk->sk_prot->socks, as this field was copied\n\t\t * with memcpy).\n\t\t *\n\t\t * This _changes_ the previous behaviour, where\n\t\t * tcp_create_openreq_child always was incrementing the\n\t\t * equivalent to tcp_prot->socks (inet_sock_nr), so this have\n\t\t * to be taken into account in all callers. -acme\n\t\t */\n\t\tsk_refcnt_debug_inc(newsk);\n\t\tsk_set_socket(newsk, NULL);\n\t\tsk_tx_queue_clear(newsk);\n\t\tRCU_INIT_POINTER(newsk->sk_wq, NULL);\n\n\t\tif (newsk->sk_prot->sockets_allocated)\n\t\t\tsk_sockets_allocated_inc(newsk);\n\n\t\tif (sock_needs_netstamp(sk) &&\n\t\t    newsk->sk_flags & SK_FLAGS_TIMESTAMP)\n\t\t\tnet_enable_timestamp();\n\t}\nout:\n\treturn newsk;\n}",
                        "cve_id": "CVE-2020-14356"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 4021,
            "cve_id": "CVE-2023-1583",
            "code_snippet": "void __io_sqe_files_unregister(struct io_ring_ctx *ctx)\n{\n\tint i;\n\n\tfor (i = 0; i < ctx->nr_user_files; i++) {\n\t\tstruct file *file = io_file_from_index(&ctx->file_table, i);\n\n\t\t/* skip scm accounted files, they'll be freed by ->ring_sock */\n\t\tif (!file || io_file_need_scm(file))\n\t\t\tcontinue;\n\t\tio_file_bitmap_clear(&ctx->file_table, i);\n\t\tfput(file);\n\t}\n\n#if defined(CONFIG_UNIX)\n\tif (ctx->ring_sock) {\n\t\tstruct sock *sock = ctx->ring_sock->sk;\n\t\tstruct sk_buff *skb;\n\n\t\twhile ((skb = skb_dequeue(&sock->sk_receive_queue)) != NULL)\n\t\t\tkfree_skb(skb);\n\t}\n#endif\n\tio_free_file_tables(&ctx->file_table);\n\tio_file_table_set_alloc_range(ctx, 0, 0);\n\tio_rsrc_data_free(ctx->file_data);\n\tctx->file_data = NULL;\n\tctx->nr_user_files = 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "__cold void io_uring_show_fdinfo(struct seq_file *m, struct file *f)\n{\n\tstruct io_ring_ctx *ctx = f->private_data;\n\tstruct io_sq_data *sq = NULL;\n\tstruct io_overflow_cqe *ocqe;\n\tstruct io_rings *r = ctx->rings;\n\tunsigned int sq_mask = ctx->sq_entries - 1, cq_mask = ctx->cq_entries - 1;\n\tunsigned int sq_head = READ_ONCE(r->sq.head);\n\tunsigned int sq_tail = READ_ONCE(r->sq.tail);\n\tunsigned int cq_head = READ_ONCE(r->cq.head);\n\tunsigned int cq_tail = READ_ONCE(r->cq.tail);\n\tunsigned int cq_shift = 0;\n\tunsigned int sq_shift = 0;\n\tunsigned int sq_entries, cq_entries;\n\tbool has_lock;\n\tunsigned int i;\n\n\tif (ctx->flags & IORING_SETUP_CQE32)\n\t\tcq_shift = 1;\n\tif (ctx->flags & IORING_SETUP_SQE128)\n\t\tsq_shift = 1;\n\n\t/*\n\t * we may get imprecise sqe and cqe info if uring is actively running\n\t * since we get cached_sq_head and cached_cq_tail without uring_lock\n\t * and sq_tail and cq_head are changed by userspace. But it's ok since\n\t * we usually use these info when it is stuck.\n\t */\n\tseq_printf(m, \"SqMask:\\t0x%x\\n\", sq_mask);\n\tseq_printf(m, \"SqHead:\\t%u\\n\", sq_head);\n\tseq_printf(m, \"SqTail:\\t%u\\n\", sq_tail);\n\tseq_printf(m, \"CachedSqHead:\\t%u\\n\", ctx->cached_sq_head);\n\tseq_printf(m, \"CqMask:\\t0x%x\\n\", cq_mask);\n\tseq_printf(m, \"CqHead:\\t%u\\n\", cq_head);\n\tseq_printf(m, \"CqTail:\\t%u\\n\", cq_tail);\n\tseq_printf(m, \"CachedCqTail:\\t%u\\n\", ctx->cached_cq_tail);\n\tseq_printf(m, \"SQEs:\\t%u\\n\", sq_tail - sq_head);\n\tsq_entries = min(sq_tail - sq_head, ctx->sq_entries);\n\tfor (i = 0; i < sq_entries; i++) {\n\t\tunsigned int entry = i + sq_head;\n\t\tstruct io_uring_sqe *sqe;\n\t\tunsigned int sq_idx;\n\n\t\tif (ctx->flags & IORING_SETUP_NO_SQARRAY)\n\t\t\tbreak;\n\t\tsq_idx = READ_ONCE(ctx->sq_array[entry & sq_mask]);\n\t\tif (sq_idx > sq_mask)\n\t\t\tcontinue;\n\t\tsqe = &ctx->sq_sqes[sq_idx << sq_shift];\n\t\tseq_printf(m, \"%5u: opcode:%s, fd:%d, flags:%x, off:%llu, \"\n\t\t\t      \"addr:0x%llx, rw_flags:0x%x, buf_index:%d \"\n\t\t\t      \"user_data:%llu\",\n\t\t\t   sq_idx, io_uring_get_opcode(sqe->opcode), sqe->fd,\n\t\t\t   sqe->flags, (unsigned long long) sqe->off,\n\t\t\t   (unsigned long long) sqe->addr, sqe->rw_flags,\n\t\t\t   sqe->buf_index, sqe->user_data);\n\t\tif (sq_shift) {\n\t\t\tu64 *sqeb = (void *) (sqe + 1);\n\t\t\tint size = sizeof(struct io_uring_sqe) / sizeof(u64);\n\t\t\tint j;\n\n\t\t\tfor (j = 0; j < size; j++) {\n\t\t\t\tseq_printf(m, \", e%d:0x%llx\", j,\n\t\t\t\t\t\t(unsigned long long) *sqeb);\n\t\t\t\tsqeb++;\n\t\t\t}\n\t\t}\n\t\tseq_printf(m, \"\\n\");\n\t}\n\tseq_printf(m, \"CQEs:\\t%u\\n\", cq_tail - cq_head);\n\tcq_entries = min(cq_tail - cq_head, ctx->cq_entries);\n\tfor (i = 0; i < cq_entries; i++) {\n\t\tunsigned int entry = i + cq_head;\n\t\tstruct io_uring_cqe *cqe = &r->cqes[(entry & cq_mask) << cq_shift];\n\n\t\tseq_printf(m, \"%5u: user_data:%llu, res:%d, flag:%x\",\n\t\t\t   entry & cq_mask, cqe->user_data, cqe->res,\n\t\t\t   cqe->flags);\n\t\tif (cq_shift)\n\t\t\tseq_printf(m, \", extra1:%llu, extra2:%llu\\n\",\n\t\t\t\t\tcqe->big_cqe[0], cqe->big_cqe[1]);\n\t\tseq_printf(m, \"\\n\");\n\t}\n\n\t/*\n\t * Avoid ABBA deadlock between the seq lock and the io_uring mutex,\n\t * since fdinfo case grabs it in the opposite direction of normal use\n\t * cases. If we fail to get the lock, we just don't iterate any\n\t * structures that could be going away outside the io_uring mutex.\n\t */\n\thas_lock = mutex_trylock(&ctx->uring_lock);\n\n\tif (has_lock && (ctx->flags & IORING_SETUP_SQPOLL)) {\n\t\tsq = ctx->sq_data;\n\t\tif (!sq->thread)\n\t\t\tsq = NULL;\n\t}\n\n\tseq_printf(m, \"SqThread:\\t%d\\n\", sq ? task_pid_nr(sq->thread) : -1);\n\tseq_printf(m, \"SqThreadCpu:\\t%d\\n\", sq ? task_cpu(sq->thread) : -1);\n\tseq_printf(m, \"UserFiles:\\t%u\\n\", ctx->nr_user_files);\n\tfor (i = 0; has_lock && i < ctx->nr_user_files; i++) {\n\t\tstruct file *f = io_file_from_index(&ctx->file_table, i);\n\n\t\tif (f)\n\t\t\tseq_printf(m, \"%5u: %s\\n\", i, file_dentry(f)->d_iname);\n\t\telse\n\t\t\tseq_printf(m, \"%5u: <none>\\n\", i);\n\t}\n\tseq_printf(m, \"UserBufs:\\t%u\\n\", ctx->nr_user_bufs);\n\tfor (i = 0; has_lock && i < ctx->nr_user_bufs; i++) {\n\t\tstruct io_mapped_ubuf *buf = ctx->user_bufs[i];\n\t\tunsigned int len = buf->ubuf_end - buf->ubuf;\n\n\t\tseq_printf(m, \"%5u: 0x%llx/%u\\n\", i, buf->ubuf, len);\n\t}\n\tif (has_lock && !xa_empty(&ctx->personalities)) {\n\t\tunsigned long index;\n\t\tconst struct cred *cred;\n\n\t\tseq_printf(m, \"Personalities:\\n\");\n\t\txa_for_each(&ctx->personalities, index, cred)\n\t\t\tio_uring_show_cred(m, index, cred);\n\t}\n\n\tseq_puts(m, \"PollList:\\n\");\n\tfor (i = 0; i < (1U << ctx->cancel_table.hash_bits); i++) {\n\t\tstruct io_hash_bucket *hb = &ctx->cancel_table.hbs[i];\n\t\tstruct io_hash_bucket *hbl = &ctx->cancel_table_locked.hbs[i];\n\t\tstruct io_kiocb *req;\n\n\t\tspin_lock(&hb->lock);\n\t\thlist_for_each_entry(req, &hb->list, hash_node)\n\t\t\tseq_printf(m, \"  op=%d, task_works=%d\\n\", req->opcode,\n\t\t\t\t\ttask_work_pending(req->task));\n\t\tspin_unlock(&hb->lock);\n\n\t\tif (!has_lock)\n\t\t\tcontinue;\n\t\thlist_for_each_entry(req, &hbl->list, hash_node)\n\t\t\tseq_printf(m, \"  op=%d, task_works=%d\\n\", req->opcode,\n\t\t\t\t\ttask_work_pending(req->task));\n\t}\n\n\tif (has_lock)\n\t\tmutex_unlock(&ctx->uring_lock);\n\n\tseq_puts(m, \"CqOverflowList:\\n\");\n\tspin_lock(&ctx->completion_lock);\n\tlist_for_each_entry(ocqe, &ctx->cq_overflow_list, list) {\n\t\tstruct io_uring_cqe *cqe = &ocqe->cqe;\n\n\t\tseq_printf(m, \"  user_data=%llu, res=%d, flags=%x\\n\",\n\t\t\t   cqe->user_data, cqe->res, cqe->flags);\n\n\t}\n\n\tspin_unlock(&ctx->completion_lock);\n}",
                        "code_after_change": "__cold void io_uring_show_fdinfo(struct seq_file *m, struct file *f)\n{\n\tstruct io_ring_ctx *ctx = f->private_data;\n\tstruct io_overflow_cqe *ocqe;\n\tstruct io_rings *r = ctx->rings;\n\tunsigned int sq_mask = ctx->sq_entries - 1, cq_mask = ctx->cq_entries - 1;\n\tunsigned int sq_head = READ_ONCE(r->sq.head);\n\tunsigned int sq_tail = READ_ONCE(r->sq.tail);\n\tunsigned int cq_head = READ_ONCE(r->cq.head);\n\tunsigned int cq_tail = READ_ONCE(r->cq.tail);\n\tunsigned int cq_shift = 0;\n\tunsigned int sq_shift = 0;\n\tunsigned int sq_entries, cq_entries;\n\tint sq_pid = -1, sq_cpu = -1;\n\tbool has_lock;\n\tunsigned int i;\n\n\tif (ctx->flags & IORING_SETUP_CQE32)\n\t\tcq_shift = 1;\n\tif (ctx->flags & IORING_SETUP_SQE128)\n\t\tsq_shift = 1;\n\n\t/*\n\t * we may get imprecise sqe and cqe info if uring is actively running\n\t * since we get cached_sq_head and cached_cq_tail without uring_lock\n\t * and sq_tail and cq_head are changed by userspace. But it's ok since\n\t * we usually use these info when it is stuck.\n\t */\n\tseq_printf(m, \"SqMask:\\t0x%x\\n\", sq_mask);\n\tseq_printf(m, \"SqHead:\\t%u\\n\", sq_head);\n\tseq_printf(m, \"SqTail:\\t%u\\n\", sq_tail);\n\tseq_printf(m, \"CachedSqHead:\\t%u\\n\", ctx->cached_sq_head);\n\tseq_printf(m, \"CqMask:\\t0x%x\\n\", cq_mask);\n\tseq_printf(m, \"CqHead:\\t%u\\n\", cq_head);\n\tseq_printf(m, \"CqTail:\\t%u\\n\", cq_tail);\n\tseq_printf(m, \"CachedCqTail:\\t%u\\n\", ctx->cached_cq_tail);\n\tseq_printf(m, \"SQEs:\\t%u\\n\", sq_tail - sq_head);\n\tsq_entries = min(sq_tail - sq_head, ctx->sq_entries);\n\tfor (i = 0; i < sq_entries; i++) {\n\t\tunsigned int entry = i + sq_head;\n\t\tstruct io_uring_sqe *sqe;\n\t\tunsigned int sq_idx;\n\n\t\tif (ctx->flags & IORING_SETUP_NO_SQARRAY)\n\t\t\tbreak;\n\t\tsq_idx = READ_ONCE(ctx->sq_array[entry & sq_mask]);\n\t\tif (sq_idx > sq_mask)\n\t\t\tcontinue;\n\t\tsqe = &ctx->sq_sqes[sq_idx << sq_shift];\n\t\tseq_printf(m, \"%5u: opcode:%s, fd:%d, flags:%x, off:%llu, \"\n\t\t\t      \"addr:0x%llx, rw_flags:0x%x, buf_index:%d \"\n\t\t\t      \"user_data:%llu\",\n\t\t\t   sq_idx, io_uring_get_opcode(sqe->opcode), sqe->fd,\n\t\t\t   sqe->flags, (unsigned long long) sqe->off,\n\t\t\t   (unsigned long long) sqe->addr, sqe->rw_flags,\n\t\t\t   sqe->buf_index, sqe->user_data);\n\t\tif (sq_shift) {\n\t\t\tu64 *sqeb = (void *) (sqe + 1);\n\t\t\tint size = sizeof(struct io_uring_sqe) / sizeof(u64);\n\t\t\tint j;\n\n\t\t\tfor (j = 0; j < size; j++) {\n\t\t\t\tseq_printf(m, \", e%d:0x%llx\", j,\n\t\t\t\t\t\t(unsigned long long) *sqeb);\n\t\t\t\tsqeb++;\n\t\t\t}\n\t\t}\n\t\tseq_printf(m, \"\\n\");\n\t}\n\tseq_printf(m, \"CQEs:\\t%u\\n\", cq_tail - cq_head);\n\tcq_entries = min(cq_tail - cq_head, ctx->cq_entries);\n\tfor (i = 0; i < cq_entries; i++) {\n\t\tunsigned int entry = i + cq_head;\n\t\tstruct io_uring_cqe *cqe = &r->cqes[(entry & cq_mask) << cq_shift];\n\n\t\tseq_printf(m, \"%5u: user_data:%llu, res:%d, flag:%x\",\n\t\t\t   entry & cq_mask, cqe->user_data, cqe->res,\n\t\t\t   cqe->flags);\n\t\tif (cq_shift)\n\t\t\tseq_printf(m, \", extra1:%llu, extra2:%llu\\n\",\n\t\t\t\t\tcqe->big_cqe[0], cqe->big_cqe[1]);\n\t\tseq_printf(m, \"\\n\");\n\t}\n\n\t/*\n\t * Avoid ABBA deadlock between the seq lock and the io_uring mutex,\n\t * since fdinfo case grabs it in the opposite direction of normal use\n\t * cases. If we fail to get the lock, we just don't iterate any\n\t * structures that could be going away outside the io_uring mutex.\n\t */\n\thas_lock = mutex_trylock(&ctx->uring_lock);\n\n\tif (has_lock && (ctx->flags & IORING_SETUP_SQPOLL)) {\n\t\tstruct io_sq_data *sq = ctx->sq_data;\n\n\t\tif (mutex_trylock(&sq->lock)) {\n\t\t\tif (sq->thread) {\n\t\t\t\tsq_pid = task_pid_nr(sq->thread);\n\t\t\t\tsq_cpu = task_cpu(sq->thread);\n\t\t\t}\n\t\t\tmutex_unlock(&sq->lock);\n\t\t}\n\t}\n\n\tseq_printf(m, \"SqThread:\\t%d\\n\", sq_pid);\n\tseq_printf(m, \"SqThreadCpu:\\t%d\\n\", sq_cpu);\n\tseq_printf(m, \"UserFiles:\\t%u\\n\", ctx->nr_user_files);\n\tfor (i = 0; has_lock && i < ctx->nr_user_files; i++) {\n\t\tstruct file *f = io_file_from_index(&ctx->file_table, i);\n\n\t\tif (f)\n\t\t\tseq_printf(m, \"%5u: %s\\n\", i, file_dentry(f)->d_iname);\n\t\telse\n\t\t\tseq_printf(m, \"%5u: <none>\\n\", i);\n\t}\n\tseq_printf(m, \"UserBufs:\\t%u\\n\", ctx->nr_user_bufs);\n\tfor (i = 0; has_lock && i < ctx->nr_user_bufs; i++) {\n\t\tstruct io_mapped_ubuf *buf = ctx->user_bufs[i];\n\t\tunsigned int len = buf->ubuf_end - buf->ubuf;\n\n\t\tseq_printf(m, \"%5u: 0x%llx/%u\\n\", i, buf->ubuf, len);\n\t}\n\tif (has_lock && !xa_empty(&ctx->personalities)) {\n\t\tunsigned long index;\n\t\tconst struct cred *cred;\n\n\t\tseq_printf(m, \"Personalities:\\n\");\n\t\txa_for_each(&ctx->personalities, index, cred)\n\t\t\tio_uring_show_cred(m, index, cred);\n\t}\n\n\tseq_puts(m, \"PollList:\\n\");\n\tfor (i = 0; i < (1U << ctx->cancel_table.hash_bits); i++) {\n\t\tstruct io_hash_bucket *hb = &ctx->cancel_table.hbs[i];\n\t\tstruct io_hash_bucket *hbl = &ctx->cancel_table_locked.hbs[i];\n\t\tstruct io_kiocb *req;\n\n\t\tspin_lock(&hb->lock);\n\t\thlist_for_each_entry(req, &hb->list, hash_node)\n\t\t\tseq_printf(m, \"  op=%d, task_works=%d\\n\", req->opcode,\n\t\t\t\t\ttask_work_pending(req->task));\n\t\tspin_unlock(&hb->lock);\n\n\t\tif (!has_lock)\n\t\t\tcontinue;\n\t\thlist_for_each_entry(req, &hbl->list, hash_node)\n\t\t\tseq_printf(m, \"  op=%d, task_works=%d\\n\", req->opcode,\n\t\t\t\t\ttask_work_pending(req->task));\n\t}\n\n\tif (has_lock)\n\t\tmutex_unlock(&ctx->uring_lock);\n\n\tseq_puts(m, \"CqOverflowList:\\n\");\n\tspin_lock(&ctx->completion_lock);\n\tlist_for_each_entry(ocqe, &ctx->cq_overflow_list, list) {\n\t\tstruct io_uring_cqe *cqe = &ocqe->cqe;\n\n\t\tseq_printf(m, \"  user_data=%llu, res=%d, flags=%x\\n\",\n\t\t\t   cqe->user_data, cqe->res, cqe->flags);\n\n\t}\n\n\tspin_unlock(&ctx->completion_lock);\n}",
                        "cve_id": "CVE-2023-46862"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int io_file_bitmap_get(struct io_ring_ctx *ctx)\n{\n\tstruct io_file_table *table = &ctx->file_table;\n\tunsigned long nr = ctx->file_alloc_end;\n\tint ret;\n\n\tdo {\n\t\tret = find_next_zero_bit(table->bitmap, nr, table->alloc_hint);\n\t\tif (ret != nr)\n\t\t\treturn ret;\n\n\t\tif (table->alloc_hint == ctx->file_alloc_start)\n\t\t\tbreak;\n\t\tnr = table->alloc_hint;\n\t\ttable->alloc_hint = ctx->file_alloc_start;\n\t} while (1);\n\n\treturn -ENFILE;\n}",
                        "code_after_change": "static int io_file_bitmap_get(struct io_ring_ctx *ctx)\n{\n\tstruct io_file_table *table = &ctx->file_table;\n\tunsigned long nr = ctx->file_alloc_end;\n\tint ret;\n\n\tif (!table->bitmap)\n\t\treturn -ENFILE;\n\n\tdo {\n\t\tret = find_next_zero_bit(table->bitmap, nr, table->alloc_hint);\n\t\tif (ret != nr)\n\t\t\treturn ret;\n\n\t\tif (table->alloc_hint == ctx->file_alloc_start)\n\t\t\tbreak;\n\t\tnr = table->alloc_hint;\n\t\ttable->alloc_hint = ctx->file_alloc_start;\n\t} while (1);\n\n\treturn -ENFILE;\n}",
                        "cve_id": "CVE-2023-1583"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 4075,
            "cve_id": "CVE-2023-2166",
            "code_snippet": "static int canfd_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t     struct packet_type *pt, struct net_device *orig_dev)\n{\n\tif (unlikely(dev->type != ARPHRD_CAN || !can_get_ml_priv(dev) || !can_is_canfd_skb(skb))) {\n\t\tpr_warn_once(\"PF_CAN: dropped non conform CAN FD skbuff: dev type %d, len %d\\n\",\n\t\t\t     dev->type, skb->len);\n\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\n\tcan_receive(skb, dev);\n\treturn NET_RX_SUCCESS;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int can_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t   struct packet_type *pt, struct net_device *orig_dev)\n{\n\tif (unlikely(dev->type != ARPHRD_CAN || (!can_is_can_skb(skb)))) {\n\t\tpr_warn_once(\"PF_CAN: dropped non conform CAN skbuff: dev type %d, len %d\\n\",\n\t\t\t     dev->type, skb->len);\n\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\n\tcan_receive(skb, dev);\n\treturn NET_RX_SUCCESS;\n}",
                        "code_after_change": "static int can_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t   struct packet_type *pt, struct net_device *orig_dev)\n{\n\tif (unlikely(dev->type != ARPHRD_CAN || !can_get_ml_priv(dev) || !can_is_can_skb(skb))) {\n\t\tpr_warn_once(\"PF_CAN: dropped non conform CAN skbuff: dev type %d, len %d\\n\",\n\t\t\t     dev->type, skb->len);\n\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\n\tcan_receive(skb, dev);\n\treturn NET_RX_SUCCESS;\n}",
                        "cve_id": "CVE-2023-2166"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 4081,
            "cve_id": "CVE-2023-2177",
            "code_snippet": "int sctp_stream_init(struct sctp_stream *stream, __u16 outcnt, __u16 incnt,\n\t\t     gfp_t gfp)\n{\n\tstruct sctp_sched_ops *sched = sctp_sched_ops_from_stream(stream);\n\tint i, ret = 0;\n\n\tgfp |= __GFP_NOWARN;\n\n\t/* Initial stream->out size may be very big, so free it and alloc\n\t * a new one with new outcnt to save memory if needed.\n\t */\n\tif (outcnt == stream->outcnt)\n\t\tgoto handle_in;\n\n\t/* Filter out chunks queued on streams that won't exist anymore */\n\tsched->unsched_all(stream);\n\tsctp_stream_outq_migrate(stream, NULL, outcnt);\n\tsched->sched_all(stream);\n\n\tret = sctp_stream_alloc_out(stream, outcnt, gfp);\n\tif (ret)\n\t\treturn ret;\n\n\tfor (i = 0; i < stream->outcnt; i++)\n\t\tSCTP_SO(stream, i)->state = SCTP_STREAM_OPEN;\n\nhandle_in:\n\tsctp_stream_interleave_init(stream);\n\tif (!incnt)\n\t\treturn 0;\n\n\treturn sctp_stream_alloc_in(stream, incnt, gfp);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static struct sctp_association *sctp_association_init(\n\t\t\t\t\tstruct sctp_association *asoc,\n\t\t\t\t\tconst struct sctp_endpoint *ep,\n\t\t\t\t\tconst struct sock *sk,\n\t\t\t\t\tenum sctp_scope scope, gfp_t gfp)\n{\n\tstruct sctp_sock *sp;\n\tstruct sctp_paramhdr *p;\n\tint i;\n\n\t/* Retrieve the SCTP per socket area.  */\n\tsp = sctp_sk((struct sock *)sk);\n\n\t/* Discarding const is appropriate here.  */\n\tasoc->ep = (struct sctp_endpoint *)ep;\n\tasoc->base.sk = (struct sock *)sk;\n\tasoc->base.net = sock_net(sk);\n\n\tsctp_endpoint_hold(asoc->ep);\n\tsock_hold(asoc->base.sk);\n\n\t/* Initialize the common base substructure.  */\n\tasoc->base.type = SCTP_EP_TYPE_ASSOCIATION;\n\n\t/* Initialize the object handling fields.  */\n\trefcount_set(&asoc->base.refcnt, 1);\n\n\t/* Initialize the bind addr area.  */\n\tsctp_bind_addr_init(&asoc->base.bind_addr, ep->base.bind_addr.port);\n\n\tasoc->state = SCTP_STATE_CLOSED;\n\tasoc->cookie_life = ms_to_ktime(sp->assocparams.sasoc_cookie_life);\n\tasoc->user_frag = sp->user_frag;\n\n\t/* Set the association max_retrans and RTO values from the\n\t * socket values.\n\t */\n\tasoc->max_retrans = sp->assocparams.sasoc_asocmaxrxt;\n\tasoc->pf_retrans  = sp->pf_retrans;\n\tasoc->ps_retrans  = sp->ps_retrans;\n\tasoc->pf_expose   = sp->pf_expose;\n\n\tasoc->rto_initial = msecs_to_jiffies(sp->rtoinfo.srto_initial);\n\tasoc->rto_max = msecs_to_jiffies(sp->rtoinfo.srto_max);\n\tasoc->rto_min = msecs_to_jiffies(sp->rtoinfo.srto_min);\n\n\t/* Initialize the association's heartbeat interval based on the\n\t * sock configured value.\n\t */\n\tasoc->hbinterval = msecs_to_jiffies(sp->hbinterval);\n\tasoc->probe_interval = msecs_to_jiffies(sp->probe_interval);\n\n\tasoc->encap_port = sp->encap_port;\n\n\t/* Initialize path max retrans value. */\n\tasoc->pathmaxrxt = sp->pathmaxrxt;\n\n\tasoc->flowlabel = sp->flowlabel;\n\tasoc->dscp = sp->dscp;\n\n\t/* Set association default SACK delay */\n\tasoc->sackdelay = msecs_to_jiffies(sp->sackdelay);\n\tasoc->sackfreq = sp->sackfreq;\n\n\t/* Set the association default flags controlling\n\t * Heartbeat, SACK delay, and Path MTU Discovery.\n\t */\n\tasoc->param_flags = sp->param_flags;\n\n\t/* Initialize the maximum number of new data packets that can be sent\n\t * in a burst.\n\t */\n\tasoc->max_burst = sp->max_burst;\n\n\tasoc->subscribe = sp->subscribe;\n\n\t/* initialize association timers */\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_T1_COOKIE] = asoc->rto_initial;\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_T1_INIT] = asoc->rto_initial;\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_T2_SHUTDOWN] = asoc->rto_initial;\n\n\t/* sctpimpguide Section 2.12.2\n\t * If the 'T5-shutdown-guard' timer is used, it SHOULD be set to the\n\t * recommended value of 5 times 'RTO.Max'.\n\t */\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_T5_SHUTDOWN_GUARD]\n\t\t= 5 * asoc->rto_max;\n\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_SACK] = asoc->sackdelay;\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_AUTOCLOSE] = sp->autoclose * HZ;\n\n\t/* Initializes the timers */\n\tfor (i = SCTP_EVENT_TIMEOUT_NONE; i < SCTP_NUM_TIMEOUT_TYPES; ++i)\n\t\ttimer_setup(&asoc->timers[i], sctp_timer_events[i], 0);\n\n\t/* Pull default initialization values from the sock options.\n\t * Note: This assumes that the values have already been\n\t * validated in the sock.\n\t */\n\tasoc->c.sinit_max_instreams = sp->initmsg.sinit_max_instreams;\n\tasoc->c.sinit_num_ostreams  = sp->initmsg.sinit_num_ostreams;\n\tasoc->max_init_attempts\t= sp->initmsg.sinit_max_attempts;\n\n\tasoc->max_init_timeo =\n\t\t msecs_to_jiffies(sp->initmsg.sinit_max_init_timeo);\n\n\t/* Set the local window size for receive.\n\t * This is also the rcvbuf space per association.\n\t * RFC 6 - A SCTP receiver MUST be able to receive a minimum of\n\t * 1500 bytes in one SCTP packet.\n\t */\n\tif ((sk->sk_rcvbuf/2) < SCTP_DEFAULT_MINWINDOW)\n\t\tasoc->rwnd = SCTP_DEFAULT_MINWINDOW;\n\telse\n\t\tasoc->rwnd = sk->sk_rcvbuf/2;\n\n\tasoc->a_rwnd = asoc->rwnd;\n\n\t/* Use my own max window until I learn something better.  */\n\tasoc->peer.rwnd = SCTP_DEFAULT_MAXWINDOW;\n\n\t/* Initialize the receive memory counter */\n\tatomic_set(&asoc->rmem_alloc, 0);\n\n\tinit_waitqueue_head(&asoc->wait);\n\n\tasoc->c.my_vtag = sctp_generate_tag(ep);\n\tasoc->c.my_port = ep->base.bind_addr.port;\n\n\tasoc->c.initial_tsn = sctp_generate_tsn(ep);\n\n\tasoc->next_tsn = asoc->c.initial_tsn;\n\n\tasoc->ctsn_ack_point = asoc->next_tsn - 1;\n\tasoc->adv_peer_ack_point = asoc->ctsn_ack_point;\n\tasoc->highest_sacked = asoc->ctsn_ack_point;\n\tasoc->last_cwr_tsn = asoc->ctsn_ack_point;\n\n\t/* ADDIP Section 4.1 Asconf Chunk Procedures\n\t *\n\t * When an endpoint has an ASCONF signaled change to be sent to the\n\t * remote endpoint it should do the following:\n\t * ...\n\t * A2) a serial number should be assigned to the chunk. The serial\n\t * number SHOULD be a monotonically increasing number. The serial\n\t * numbers SHOULD be initialized at the start of the\n\t * association to the same value as the initial TSN.\n\t */\n\tasoc->addip_serial = asoc->c.initial_tsn;\n\tasoc->strreset_outseq = asoc->c.initial_tsn;\n\n\tINIT_LIST_HEAD(&asoc->addip_chunk_list);\n\tINIT_LIST_HEAD(&asoc->asconf_ack_list);\n\n\t/* Make an empty list of remote transport addresses.  */\n\tINIT_LIST_HEAD(&asoc->peer.transport_addr_list);\n\n\t/* RFC 2960 5.1 Normal Establishment of an Association\n\t *\n\t * After the reception of the first data chunk in an\n\t * association the endpoint must immediately respond with a\n\t * sack to acknowledge the data chunk.  Subsequent\n\t * acknowledgements should be done as described in Section\n\t * 6.2.\n\t *\n\t * [We implement this by telling a new association that it\n\t * already received one packet.]\n\t */\n\tasoc->peer.sack_needed = 1;\n\tasoc->peer.sack_generation = 1;\n\n\t/* Create an input queue.  */\n\tsctp_inq_init(&asoc->base.inqueue);\n\tsctp_inq_set_th_handler(&asoc->base.inqueue, sctp_assoc_bh_rcv);\n\n\t/* Create an output queue.  */\n\tsctp_outq_init(asoc, &asoc->outqueue);\n\n\tif (!sctp_ulpq_init(&asoc->ulpq, asoc))\n\t\tgoto fail_init;\n\n\tif (sctp_stream_init(&asoc->stream, asoc->c.sinit_num_ostreams,\n\t\t\t     0, gfp))\n\t\tgoto fail_init;\n\n\t/* Initialize default path MTU. */\n\tasoc->pathmtu = sp->pathmtu;\n\tsctp_assoc_update_frag_point(asoc);\n\n\t/* Assume that peer would support both address types unless we are\n\t * told otherwise.\n\t */\n\tasoc->peer.ipv4_address = 1;\n\tif (asoc->base.sk->sk_family == PF_INET6)\n\t\tasoc->peer.ipv6_address = 1;\n\tINIT_LIST_HEAD(&asoc->asocs);\n\n\tasoc->default_stream = sp->default_stream;\n\tasoc->default_ppid = sp->default_ppid;\n\tasoc->default_flags = sp->default_flags;\n\tasoc->default_context = sp->default_context;\n\tasoc->default_timetolive = sp->default_timetolive;\n\tasoc->default_rcv_context = sp->default_rcv_context;\n\n\t/* AUTH related initializations */\n\tINIT_LIST_HEAD(&asoc->endpoint_shared_keys);\n\tif (sctp_auth_asoc_copy_shkeys(ep, asoc, gfp))\n\t\tgoto stream_free;\n\n\tasoc->active_key_id = ep->active_key_id;\n\tasoc->strreset_enable = ep->strreset_enable;\n\n\t/* Save the hmacs and chunks list into this association */\n\tif (ep->auth_hmacs_list)\n\t\tmemcpy(asoc->c.auth_hmacs, ep->auth_hmacs_list,\n\t\t\tntohs(ep->auth_hmacs_list->param_hdr.length));\n\tif (ep->auth_chunk_list)\n\t\tmemcpy(asoc->c.auth_chunks, ep->auth_chunk_list,\n\t\t\tntohs(ep->auth_chunk_list->param_hdr.length));\n\n\t/* Get the AUTH random number for this association */\n\tp = (struct sctp_paramhdr *)asoc->c.auth_random;\n\tp->type = SCTP_PARAM_RANDOM;\n\tp->length = htons(sizeof(*p) + SCTP_AUTH_RANDOM_LENGTH);\n\tget_random_bytes(p+1, SCTP_AUTH_RANDOM_LENGTH);\n\n\treturn asoc;\n\nstream_free:\n\tsctp_stream_free(&asoc->stream);\nfail_init:\n\tsock_put(asoc->base.sk);\n\tsctp_endpoint_put(asoc->ep);\n\treturn NULL;\n}",
                        "code_after_change": "static struct sctp_association *sctp_association_init(\n\t\t\t\t\tstruct sctp_association *asoc,\n\t\t\t\t\tconst struct sctp_endpoint *ep,\n\t\t\t\t\tconst struct sock *sk,\n\t\t\t\t\tenum sctp_scope scope, gfp_t gfp)\n{\n\tstruct sctp_sock *sp;\n\tstruct sctp_paramhdr *p;\n\tint i;\n\n\t/* Retrieve the SCTP per socket area.  */\n\tsp = sctp_sk((struct sock *)sk);\n\n\t/* Discarding const is appropriate here.  */\n\tasoc->ep = (struct sctp_endpoint *)ep;\n\tasoc->base.sk = (struct sock *)sk;\n\tasoc->base.net = sock_net(sk);\n\n\tsctp_endpoint_hold(asoc->ep);\n\tsock_hold(asoc->base.sk);\n\n\t/* Initialize the common base substructure.  */\n\tasoc->base.type = SCTP_EP_TYPE_ASSOCIATION;\n\n\t/* Initialize the object handling fields.  */\n\trefcount_set(&asoc->base.refcnt, 1);\n\n\t/* Initialize the bind addr area.  */\n\tsctp_bind_addr_init(&asoc->base.bind_addr, ep->base.bind_addr.port);\n\n\tasoc->state = SCTP_STATE_CLOSED;\n\tasoc->cookie_life = ms_to_ktime(sp->assocparams.sasoc_cookie_life);\n\tasoc->user_frag = sp->user_frag;\n\n\t/* Set the association max_retrans and RTO values from the\n\t * socket values.\n\t */\n\tasoc->max_retrans = sp->assocparams.sasoc_asocmaxrxt;\n\tasoc->pf_retrans  = sp->pf_retrans;\n\tasoc->ps_retrans  = sp->ps_retrans;\n\tasoc->pf_expose   = sp->pf_expose;\n\n\tasoc->rto_initial = msecs_to_jiffies(sp->rtoinfo.srto_initial);\n\tasoc->rto_max = msecs_to_jiffies(sp->rtoinfo.srto_max);\n\tasoc->rto_min = msecs_to_jiffies(sp->rtoinfo.srto_min);\n\n\t/* Initialize the association's heartbeat interval based on the\n\t * sock configured value.\n\t */\n\tasoc->hbinterval = msecs_to_jiffies(sp->hbinterval);\n\tasoc->probe_interval = msecs_to_jiffies(sp->probe_interval);\n\n\tasoc->encap_port = sp->encap_port;\n\n\t/* Initialize path max retrans value. */\n\tasoc->pathmaxrxt = sp->pathmaxrxt;\n\n\tasoc->flowlabel = sp->flowlabel;\n\tasoc->dscp = sp->dscp;\n\n\t/* Set association default SACK delay */\n\tasoc->sackdelay = msecs_to_jiffies(sp->sackdelay);\n\tasoc->sackfreq = sp->sackfreq;\n\n\t/* Set the association default flags controlling\n\t * Heartbeat, SACK delay, and Path MTU Discovery.\n\t */\n\tasoc->param_flags = sp->param_flags;\n\n\t/* Initialize the maximum number of new data packets that can be sent\n\t * in a burst.\n\t */\n\tasoc->max_burst = sp->max_burst;\n\n\tasoc->subscribe = sp->subscribe;\n\n\t/* initialize association timers */\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_T1_COOKIE] = asoc->rto_initial;\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_T1_INIT] = asoc->rto_initial;\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_T2_SHUTDOWN] = asoc->rto_initial;\n\n\t/* sctpimpguide Section 2.12.2\n\t * If the 'T5-shutdown-guard' timer is used, it SHOULD be set to the\n\t * recommended value of 5 times 'RTO.Max'.\n\t */\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_T5_SHUTDOWN_GUARD]\n\t\t= 5 * asoc->rto_max;\n\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_SACK] = asoc->sackdelay;\n\tasoc->timeouts[SCTP_EVENT_TIMEOUT_AUTOCLOSE] = sp->autoclose * HZ;\n\n\t/* Initializes the timers */\n\tfor (i = SCTP_EVENT_TIMEOUT_NONE; i < SCTP_NUM_TIMEOUT_TYPES; ++i)\n\t\ttimer_setup(&asoc->timers[i], sctp_timer_events[i], 0);\n\n\t/* Pull default initialization values from the sock options.\n\t * Note: This assumes that the values have already been\n\t * validated in the sock.\n\t */\n\tasoc->c.sinit_max_instreams = sp->initmsg.sinit_max_instreams;\n\tasoc->c.sinit_num_ostreams  = sp->initmsg.sinit_num_ostreams;\n\tasoc->max_init_attempts\t= sp->initmsg.sinit_max_attempts;\n\n\tasoc->max_init_timeo =\n\t\t msecs_to_jiffies(sp->initmsg.sinit_max_init_timeo);\n\n\t/* Set the local window size for receive.\n\t * This is also the rcvbuf space per association.\n\t * RFC 6 - A SCTP receiver MUST be able to receive a minimum of\n\t * 1500 bytes in one SCTP packet.\n\t */\n\tif ((sk->sk_rcvbuf/2) < SCTP_DEFAULT_MINWINDOW)\n\t\tasoc->rwnd = SCTP_DEFAULT_MINWINDOW;\n\telse\n\t\tasoc->rwnd = sk->sk_rcvbuf/2;\n\n\tasoc->a_rwnd = asoc->rwnd;\n\n\t/* Use my own max window until I learn something better.  */\n\tasoc->peer.rwnd = SCTP_DEFAULT_MAXWINDOW;\n\n\t/* Initialize the receive memory counter */\n\tatomic_set(&asoc->rmem_alloc, 0);\n\n\tinit_waitqueue_head(&asoc->wait);\n\n\tasoc->c.my_vtag = sctp_generate_tag(ep);\n\tasoc->c.my_port = ep->base.bind_addr.port;\n\n\tasoc->c.initial_tsn = sctp_generate_tsn(ep);\n\n\tasoc->next_tsn = asoc->c.initial_tsn;\n\n\tasoc->ctsn_ack_point = asoc->next_tsn - 1;\n\tasoc->adv_peer_ack_point = asoc->ctsn_ack_point;\n\tasoc->highest_sacked = asoc->ctsn_ack_point;\n\tasoc->last_cwr_tsn = asoc->ctsn_ack_point;\n\n\t/* ADDIP Section 4.1 Asconf Chunk Procedures\n\t *\n\t * When an endpoint has an ASCONF signaled change to be sent to the\n\t * remote endpoint it should do the following:\n\t * ...\n\t * A2) a serial number should be assigned to the chunk. The serial\n\t * number SHOULD be a monotonically increasing number. The serial\n\t * numbers SHOULD be initialized at the start of the\n\t * association to the same value as the initial TSN.\n\t */\n\tasoc->addip_serial = asoc->c.initial_tsn;\n\tasoc->strreset_outseq = asoc->c.initial_tsn;\n\n\tINIT_LIST_HEAD(&asoc->addip_chunk_list);\n\tINIT_LIST_HEAD(&asoc->asconf_ack_list);\n\n\t/* Make an empty list of remote transport addresses.  */\n\tINIT_LIST_HEAD(&asoc->peer.transport_addr_list);\n\n\t/* RFC 2960 5.1 Normal Establishment of an Association\n\t *\n\t * After the reception of the first data chunk in an\n\t * association the endpoint must immediately respond with a\n\t * sack to acknowledge the data chunk.  Subsequent\n\t * acknowledgements should be done as described in Section\n\t * 6.2.\n\t *\n\t * [We implement this by telling a new association that it\n\t * already received one packet.]\n\t */\n\tasoc->peer.sack_needed = 1;\n\tasoc->peer.sack_generation = 1;\n\n\t/* Create an input queue.  */\n\tsctp_inq_init(&asoc->base.inqueue);\n\tsctp_inq_set_th_handler(&asoc->base.inqueue, sctp_assoc_bh_rcv);\n\n\t/* Create an output queue.  */\n\tsctp_outq_init(asoc, &asoc->outqueue);\n\n\tif (!sctp_ulpq_init(&asoc->ulpq, asoc))\n\t\tgoto fail_init;\n\n\tif (sctp_stream_init(&asoc->stream, asoc->c.sinit_num_ostreams, 0, gfp))\n\t\tgoto stream_free;\n\n\t/* Initialize default path MTU. */\n\tasoc->pathmtu = sp->pathmtu;\n\tsctp_assoc_update_frag_point(asoc);\n\n\t/* Assume that peer would support both address types unless we are\n\t * told otherwise.\n\t */\n\tasoc->peer.ipv4_address = 1;\n\tif (asoc->base.sk->sk_family == PF_INET6)\n\t\tasoc->peer.ipv6_address = 1;\n\tINIT_LIST_HEAD(&asoc->asocs);\n\n\tasoc->default_stream = sp->default_stream;\n\tasoc->default_ppid = sp->default_ppid;\n\tasoc->default_flags = sp->default_flags;\n\tasoc->default_context = sp->default_context;\n\tasoc->default_timetolive = sp->default_timetolive;\n\tasoc->default_rcv_context = sp->default_rcv_context;\n\n\t/* AUTH related initializations */\n\tINIT_LIST_HEAD(&asoc->endpoint_shared_keys);\n\tif (sctp_auth_asoc_copy_shkeys(ep, asoc, gfp))\n\t\tgoto stream_free;\n\n\tasoc->active_key_id = ep->active_key_id;\n\tasoc->strreset_enable = ep->strreset_enable;\n\n\t/* Save the hmacs and chunks list into this association */\n\tif (ep->auth_hmacs_list)\n\t\tmemcpy(asoc->c.auth_hmacs, ep->auth_hmacs_list,\n\t\t\tntohs(ep->auth_hmacs_list->param_hdr.length));\n\tif (ep->auth_chunk_list)\n\t\tmemcpy(asoc->c.auth_chunks, ep->auth_chunk_list,\n\t\t\tntohs(ep->auth_chunk_list->param_hdr.length));\n\n\t/* Get the AUTH random number for this association */\n\tp = (struct sctp_paramhdr *)asoc->c.auth_random;\n\tp->type = SCTP_PARAM_RANDOM;\n\tp->length = htons(sizeof(*p) + SCTP_AUTH_RANDOM_LENGTH);\n\tget_random_bytes(p+1, SCTP_AUTH_RANDOM_LENGTH);\n\n\treturn asoc;\n\nstream_free:\n\tsctp_stream_free(&asoc->stream);\nfail_init:\n\tsock_put(asoc->base.sk);\n\tsctp_endpoint_put(asoc->ep);\n\treturn NULL;\n}",
                        "cve_id": "CVE-2023-2177"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int stv06xx_start(struct gspca_dev *gspca_dev)\n{\n\tstruct sd *sd = (struct sd *) gspca_dev;\n\tstruct usb_host_interface *alt;\n\tstruct usb_interface *intf;\n\tint err, packet_size;\n\n\tintf = usb_ifnum_to_if(sd->gspca_dev.dev, sd->gspca_dev.iface);\n\talt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);\n\tif (!alt) {\n\t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n\t\treturn -EIO;\n\t}\n\n\tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n\terr = stv06xx_write_bridge(sd, STV_ISO_SIZE_L, packet_size);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* Prepare the sensor for start */\n\terr = sd->sensor->start(sd);\n\tif (err < 0)\n\t\tgoto out;\n\n\t/* Start isochronous streaming */\n\terr = stv06xx_write_bridge(sd, STV_ISO_ENABLE, 1);\n\nout:\n\tif (err < 0)\n\t\tgspca_dbg(gspca_dev, D_STREAM, \"Starting stream failed\\n\");\n\telse\n\t\tgspca_dbg(gspca_dev, D_STREAM, \"Started streaming\\n\");\n\n\treturn (err < 0) ? err : 0;\n}",
                        "code_after_change": "static int stv06xx_start(struct gspca_dev *gspca_dev)\n{\n\tstruct sd *sd = (struct sd *) gspca_dev;\n\tstruct usb_host_interface *alt;\n\tstruct usb_interface *intf;\n\tint err, packet_size;\n\n\tintf = usb_ifnum_to_if(sd->gspca_dev.dev, sd->gspca_dev.iface);\n\talt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);\n\tif (!alt) {\n\t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n\t\treturn -EIO;\n\t}\n\n\tif (alt->desc.bNumEndpoints < 1)\n\t\treturn -ENODEV;\n\n\tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n\terr = stv06xx_write_bridge(sd, STV_ISO_SIZE_L, packet_size);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* Prepare the sensor for start */\n\terr = sd->sensor->start(sd);\n\tif (err < 0)\n\t\tgoto out;\n\n\t/* Start isochronous streaming */\n\terr = stv06xx_write_bridge(sd, STV_ISO_ENABLE, 1);\n\nout:\n\tif (err < 0)\n\t\tgspca_dbg(gspca_dev, D_STREAM, \"Starting stream failed\\n\");\n\telse\n\t\tgspca_dbg(gspca_dev, D_STREAM, \"Started streaming\\n\");\n\n\treturn (err < 0) ? err : 0;\n}",
                        "cve_id": "CVE-2020-11609"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int __rds_rdma_map(struct rds_sock *rs, struct rds_get_mr_args *args,\n\t\t\t\tu64 *cookie_ret, struct rds_mr **mr_ret)\n{\n\tstruct rds_mr *mr = NULL, *found;\n\tunsigned int nr_pages;\n\tstruct page **pages = NULL;\n\tstruct scatterlist *sg;\n\tvoid *trans_private;\n\tunsigned long flags;\n\trds_rdma_cookie_t cookie;\n\tunsigned int nents;\n\tlong i;\n\tint ret;\n\n\tif (rs->rs_bound_addr == 0) {\n\t\tret = -ENOTCONN; /* XXX not a great errno */\n\t\tgoto out;\n\t}\n\n\tif (!rs->rs_transport->get_mr) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tnr_pages = rds_pages_in_vec(&args->vec);\n\tif (nr_pages == 0) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* Restrict the size of mr irrespective of underlying transport\n\t * To account for unaligned mr regions, subtract one from nr_pages\n\t */\n\tif ((nr_pages - 1) > (RDS_MAX_MSG_SIZE >> PAGE_SHIFT)) {\n\t\tret = -EMSGSIZE;\n\t\tgoto out;\n\t}\n\n\trdsdebug(\"RDS: get_mr addr %llx len %llu nr_pages %u\\n\",\n\t\targs->vec.addr, args->vec.bytes, nr_pages);\n\n\t/* XXX clamp nr_pages to limit the size of this alloc? */\n\tpages = kcalloc(nr_pages, sizeof(struct page *), GFP_KERNEL);\n\tif (!pages) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tmr = kzalloc(sizeof(struct rds_mr), GFP_KERNEL);\n\tif (!mr) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trefcount_set(&mr->r_refcount, 1);\n\tRB_CLEAR_NODE(&mr->r_rb_node);\n\tmr->r_trans = rs->rs_transport;\n\tmr->r_sock = rs;\n\n\tif (args->flags & RDS_RDMA_USE_ONCE)\n\t\tmr->r_use_once = 1;\n\tif (args->flags & RDS_RDMA_INVALIDATE)\n\t\tmr->r_invalidate = 1;\n\tif (args->flags & RDS_RDMA_READWRITE)\n\t\tmr->r_write = 1;\n\n\t/*\n\t * Pin the pages that make up the user buffer and transfer the page\n\t * pointers to the mr's sg array.  We check to see if we've mapped\n\t * the whole region after transferring the partial page references\n\t * to the sg array so that we can have one page ref cleanup path.\n\t *\n\t * For now we have no flag that tells us whether the mapping is\n\t * r/o or r/w. We need to assume r/w, or we'll do a lot of RDMA to\n\t * the zero page.\n\t */\n\tret = rds_pin_pages(args->vec.addr, nr_pages, pages, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tnents = ret;\n\tsg = kcalloc(nents, sizeof(*sg), GFP_KERNEL);\n\tif (!sg) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tWARN_ON(!nents);\n\tsg_init_table(sg, nents);\n\n\t/* Stick all pages into the scatterlist */\n\tfor (i = 0 ; i < nents; i++)\n\t\tsg_set_page(&sg[i], pages[i], PAGE_SIZE, 0);\n\n\trdsdebug(\"RDS: trans_private nents is %u\\n\", nents);\n\n\t/* Obtain a transport specific MR. If this succeeds, the\n\t * s/g list is now owned by the MR.\n\t * Note that dma_map() implies that pending writes are\n\t * flushed to RAM, so no dma_sync is needed here. */\n\ttrans_private = rs->rs_transport->get_mr(sg, nents, rs,\n\t\t\t\t\t\t &mr->r_key);\n\n\tif (IS_ERR(trans_private)) {\n\t\tfor (i = 0 ; i < nents; i++)\n\t\t\tput_page(sg_page(&sg[i]));\n\t\tkfree(sg);\n\t\tret = PTR_ERR(trans_private);\n\t\tgoto out;\n\t}\n\n\tmr->r_trans_private = trans_private;\n\n\trdsdebug(\"RDS: get_mr put_user key is %x cookie_addr %p\\n\",\n\t       mr->r_key, (void *)(unsigned long) args->cookie_addr);\n\n\t/* The user may pass us an unaligned address, but we can only\n\t * map page aligned regions. So we keep the offset, and build\n\t * a 64bit cookie containing <R_Key, offset> and pass that\n\t * around. */\n\tcookie = rds_rdma_make_cookie(mr->r_key, args->vec.addr & ~PAGE_MASK);\n\tif (cookie_ret)\n\t\t*cookie_ret = cookie;\n\n\tif (args->cookie_addr && put_user(cookie, (u64 __user *)(unsigned long) args->cookie_addr)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\t/* Inserting the new MR into the rbtree bumps its\n\t * reference count. */\n\tspin_lock_irqsave(&rs->rs_rdma_lock, flags);\n\tfound = rds_mr_tree_walk(&rs->rs_rdma_keys, mr->r_key, mr);\n\tspin_unlock_irqrestore(&rs->rs_rdma_lock, flags);\n\n\tBUG_ON(found && found != mr);\n\n\trdsdebug(\"RDS: get_mr key is %x\\n\", mr->r_key);\n\tif (mr_ret) {\n\t\trefcount_inc(&mr->r_refcount);\n\t\t*mr_ret = mr;\n\t}\n\n\tret = 0;\nout:\n\tkfree(pages);\n\tif (mr)\n\t\trds_mr_put(mr);\n\treturn ret;\n}",
                        "code_after_change": "static int __rds_rdma_map(struct rds_sock *rs, struct rds_get_mr_args *args,\n\t\t\t\tu64 *cookie_ret, struct rds_mr **mr_ret)\n{\n\tstruct rds_mr *mr = NULL, *found;\n\tunsigned int nr_pages;\n\tstruct page **pages = NULL;\n\tstruct scatterlist *sg;\n\tvoid *trans_private;\n\tunsigned long flags;\n\trds_rdma_cookie_t cookie;\n\tunsigned int nents;\n\tlong i;\n\tint ret;\n\n\tif (rs->rs_bound_addr == 0 || !rs->rs_transport) {\n\t\tret = -ENOTCONN; /* XXX not a great errno */\n\t\tgoto out;\n\t}\n\n\tif (!rs->rs_transport->get_mr) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tnr_pages = rds_pages_in_vec(&args->vec);\n\tif (nr_pages == 0) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* Restrict the size of mr irrespective of underlying transport\n\t * To account for unaligned mr regions, subtract one from nr_pages\n\t */\n\tif ((nr_pages - 1) > (RDS_MAX_MSG_SIZE >> PAGE_SHIFT)) {\n\t\tret = -EMSGSIZE;\n\t\tgoto out;\n\t}\n\n\trdsdebug(\"RDS: get_mr addr %llx len %llu nr_pages %u\\n\",\n\t\targs->vec.addr, args->vec.bytes, nr_pages);\n\n\t/* XXX clamp nr_pages to limit the size of this alloc? */\n\tpages = kcalloc(nr_pages, sizeof(struct page *), GFP_KERNEL);\n\tif (!pages) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tmr = kzalloc(sizeof(struct rds_mr), GFP_KERNEL);\n\tif (!mr) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\trefcount_set(&mr->r_refcount, 1);\n\tRB_CLEAR_NODE(&mr->r_rb_node);\n\tmr->r_trans = rs->rs_transport;\n\tmr->r_sock = rs;\n\n\tif (args->flags & RDS_RDMA_USE_ONCE)\n\t\tmr->r_use_once = 1;\n\tif (args->flags & RDS_RDMA_INVALIDATE)\n\t\tmr->r_invalidate = 1;\n\tif (args->flags & RDS_RDMA_READWRITE)\n\t\tmr->r_write = 1;\n\n\t/*\n\t * Pin the pages that make up the user buffer and transfer the page\n\t * pointers to the mr's sg array.  We check to see if we've mapped\n\t * the whole region after transferring the partial page references\n\t * to the sg array so that we can have one page ref cleanup path.\n\t *\n\t * For now we have no flag that tells us whether the mapping is\n\t * r/o or r/w. We need to assume r/w, or we'll do a lot of RDMA to\n\t * the zero page.\n\t */\n\tret = rds_pin_pages(args->vec.addr, nr_pages, pages, 1);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tnents = ret;\n\tsg = kcalloc(nents, sizeof(*sg), GFP_KERNEL);\n\tif (!sg) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tWARN_ON(!nents);\n\tsg_init_table(sg, nents);\n\n\t/* Stick all pages into the scatterlist */\n\tfor (i = 0 ; i < nents; i++)\n\t\tsg_set_page(&sg[i], pages[i], PAGE_SIZE, 0);\n\n\trdsdebug(\"RDS: trans_private nents is %u\\n\", nents);\n\n\t/* Obtain a transport specific MR. If this succeeds, the\n\t * s/g list is now owned by the MR.\n\t * Note that dma_map() implies that pending writes are\n\t * flushed to RAM, so no dma_sync is needed here. */\n\ttrans_private = rs->rs_transport->get_mr(sg, nents, rs,\n\t\t\t\t\t\t &mr->r_key);\n\n\tif (IS_ERR(trans_private)) {\n\t\tfor (i = 0 ; i < nents; i++)\n\t\t\tput_page(sg_page(&sg[i]));\n\t\tkfree(sg);\n\t\tret = PTR_ERR(trans_private);\n\t\tgoto out;\n\t}\n\n\tmr->r_trans_private = trans_private;\n\n\trdsdebug(\"RDS: get_mr put_user key is %x cookie_addr %p\\n\",\n\t       mr->r_key, (void *)(unsigned long) args->cookie_addr);\n\n\t/* The user may pass us an unaligned address, but we can only\n\t * map page aligned regions. So we keep the offset, and build\n\t * a 64bit cookie containing <R_Key, offset> and pass that\n\t * around. */\n\tcookie = rds_rdma_make_cookie(mr->r_key, args->vec.addr & ~PAGE_MASK);\n\tif (cookie_ret)\n\t\t*cookie_ret = cookie;\n\n\tif (args->cookie_addr && put_user(cookie, (u64 __user *)(unsigned long) args->cookie_addr)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\t/* Inserting the new MR into the rbtree bumps its\n\t * reference count. */\n\tspin_lock_irqsave(&rs->rs_rdma_lock, flags);\n\tfound = rds_mr_tree_walk(&rs->rs_rdma_keys, mr->r_key, mr);\n\tspin_unlock_irqrestore(&rs->rs_rdma_lock, flags);\n\n\tBUG_ON(found && found != mr);\n\n\trdsdebug(\"RDS: get_mr key is %x\\n\", mr->r_key);\n\tif (mr_ret) {\n\t\trefcount_inc(&mr->r_refcount);\n\t\t*mr_ret = mr;\n\t}\n\n\tret = 0;\nout:\n\tkfree(pages);\n\tif (mr)\n\t\trds_mr_put(mr);\n\treturn ret;\n}",
                        "cve_id": "CVE-2018-7492"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int pb0100_start(struct sd *sd)\n{\n\tint err, packet_size, max_packet_size;\n\tstruct usb_host_interface *alt;\n\tstruct usb_interface *intf;\n\tstruct gspca_dev *gspca_dev = (struct gspca_dev *)sd;\n\tstruct cam *cam = &sd->gspca_dev.cam;\n\tu32 mode = cam->cam_mode[sd->gspca_dev.curr_mode].priv;\n\n\tintf = usb_ifnum_to_if(sd->gspca_dev.dev, sd->gspca_dev.iface);\n\talt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);\n\tif (!alt)\n\t\treturn -ENODEV;\n\tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n\n\t/* If we don't have enough bandwidth use a lower framerate */\n\tmax_packet_size = sd->sensor->max_packet_size[sd->gspca_dev.curr_mode];\n\tif (packet_size < max_packet_size)\n\t\tstv06xx_write_sensor(sd, PB_ROWSPEED, BIT(4)|BIT(3)|BIT(1));\n\telse\n\t\tstv06xx_write_sensor(sd, PB_ROWSPEED, BIT(5)|BIT(3)|BIT(1));\n\n\t/* Setup sensor window */\n\tif (mode & PB0100_CROP_TO_VGA) {\n\t\tstv06xx_write_sensor(sd, PB_RSTART, 30);\n\t\tstv06xx_write_sensor(sd, PB_CSTART, 20);\n\t\tstv06xx_write_sensor(sd, PB_RWSIZE, 240 - 1);\n\t\tstv06xx_write_sensor(sd, PB_CWSIZE, 320 - 1);\n\t} else {\n\t\tstv06xx_write_sensor(sd, PB_RSTART, 8);\n\t\tstv06xx_write_sensor(sd, PB_CSTART, 4);\n\t\tstv06xx_write_sensor(sd, PB_RWSIZE, 288 - 1);\n\t\tstv06xx_write_sensor(sd, PB_CWSIZE, 352 - 1);\n\t}\n\n\tif (mode & PB0100_SUBSAMPLE) {\n\t\tstv06xx_write_bridge(sd, STV_Y_CTRL, 0x02); /* Wrong, FIXME */\n\t\tstv06xx_write_bridge(sd, STV_X_CTRL, 0x06);\n\n\t\tstv06xx_write_bridge(sd, STV_SCAN_RATE, 0x10);\n\t} else {\n\t\tstv06xx_write_bridge(sd, STV_Y_CTRL, 0x01);\n\t\tstv06xx_write_bridge(sd, STV_X_CTRL, 0x0a);\n\t\t/* larger -> slower */\n\t\tstv06xx_write_bridge(sd, STV_SCAN_RATE, 0x20);\n\t}\n\n\terr = stv06xx_write_sensor(sd, PB_CONTROL, BIT(5)|BIT(3)|BIT(1));\n\tgspca_dbg(gspca_dev, D_STREAM, \"Started stream, status: %d\\n\", err);\n\n\treturn (err < 0) ? err : 0;\n}",
                        "code_after_change": "static int pb0100_start(struct sd *sd)\n{\n\tint err, packet_size, max_packet_size;\n\tstruct usb_host_interface *alt;\n\tstruct usb_interface *intf;\n\tstruct gspca_dev *gspca_dev = (struct gspca_dev *)sd;\n\tstruct cam *cam = &sd->gspca_dev.cam;\n\tu32 mode = cam->cam_mode[sd->gspca_dev.curr_mode].priv;\n\n\tintf = usb_ifnum_to_if(sd->gspca_dev.dev, sd->gspca_dev.iface);\n\talt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);\n\tif (!alt)\n\t\treturn -ENODEV;\n\n\tif (alt->desc.bNumEndpoints < 1)\n\t\treturn -ENODEV;\n\n\tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n\n\t/* If we don't have enough bandwidth use a lower framerate */\n\tmax_packet_size = sd->sensor->max_packet_size[sd->gspca_dev.curr_mode];\n\tif (packet_size < max_packet_size)\n\t\tstv06xx_write_sensor(sd, PB_ROWSPEED, BIT(4)|BIT(3)|BIT(1));\n\telse\n\t\tstv06xx_write_sensor(sd, PB_ROWSPEED, BIT(5)|BIT(3)|BIT(1));\n\n\t/* Setup sensor window */\n\tif (mode & PB0100_CROP_TO_VGA) {\n\t\tstv06xx_write_sensor(sd, PB_RSTART, 30);\n\t\tstv06xx_write_sensor(sd, PB_CSTART, 20);\n\t\tstv06xx_write_sensor(sd, PB_RWSIZE, 240 - 1);\n\t\tstv06xx_write_sensor(sd, PB_CWSIZE, 320 - 1);\n\t} else {\n\t\tstv06xx_write_sensor(sd, PB_RSTART, 8);\n\t\tstv06xx_write_sensor(sd, PB_CSTART, 4);\n\t\tstv06xx_write_sensor(sd, PB_RWSIZE, 288 - 1);\n\t\tstv06xx_write_sensor(sd, PB_CWSIZE, 352 - 1);\n\t}\n\n\tif (mode & PB0100_SUBSAMPLE) {\n\t\tstv06xx_write_bridge(sd, STV_Y_CTRL, 0x02); /* Wrong, FIXME */\n\t\tstv06xx_write_bridge(sd, STV_X_CTRL, 0x06);\n\n\t\tstv06xx_write_bridge(sd, STV_SCAN_RATE, 0x10);\n\t} else {\n\t\tstv06xx_write_bridge(sd, STV_Y_CTRL, 0x01);\n\t\tstv06xx_write_bridge(sd, STV_X_CTRL, 0x0a);\n\t\t/* larger -> slower */\n\t\tstv06xx_write_bridge(sd, STV_SCAN_RATE, 0x20);\n\t}\n\n\terr = stv06xx_write_sensor(sd, PB_CONTROL, BIT(5)|BIT(3)|BIT(1));\n\tgspca_dbg(gspca_dev, D_STREAM, \"Started stream, status: %d\\n\", err);\n\n\treturn (err < 0) ? err : 0;\n}",
                        "cve_id": "CVE-2020-11609"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static bool assoc_array_insert_into_terminal_node(struct assoc_array_edit *edit,\n\t\t\t\t\t\t  const struct assoc_array_ops *ops,\n\t\t\t\t\t\t  const void *index_key,\n\t\t\t\t\t\t  struct assoc_array_walk_result *result)\n{\n\tstruct assoc_array_shortcut *shortcut, *new_s0;\n\tstruct assoc_array_node *node, *new_n0, *new_n1, *side;\n\tstruct assoc_array_ptr *ptr;\n\tunsigned long dissimilarity, base_seg, blank;\n\tsize_t keylen;\n\tbool have_meta;\n\tint level, diff;\n\tint slot, next_slot, free_slot, i, j;\n\n\tnode\t= result->terminal_node.node;\n\tlevel\t= result->terminal_node.level;\n\tedit->segment_cache[ASSOC_ARRAY_FAN_OUT] = result->terminal_node.slot;\n\n\tpr_devel(\"-->%s()\\n\", __func__);\n\n\t/* We arrived at a node which doesn't have an onward node or shortcut\n\t * pointer that we have to follow.  This means that (a) the leaf we\n\t * want must go here (either by insertion or replacement) or (b) we\n\t * need to split this node and insert in one of the fragments.\n\t */\n\tfree_slot = -1;\n\n\t/* Firstly, we have to check the leaves in this node to see if there's\n\t * a matching one we should replace in place.\n\t */\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tptr = node->slots[i];\n\t\tif (!ptr) {\n\t\t\tfree_slot = i;\n\t\t\tcontinue;\n\t\t}\n\t\tif (assoc_array_ptr_is_leaf(ptr) &&\n\t\t    ops->compare_object(assoc_array_ptr_to_leaf(ptr),\n\t\t\t\t\tindex_key)) {\n\t\t\tpr_devel(\"replace in slot %d\\n\", i);\n\t\t\tedit->leaf_p = &node->slots[i];\n\t\t\tedit->dead_leaf = node->slots[i];\n\t\t\tpr_devel(\"<--%s() = ok [replace]\\n\", __func__);\n\t\t\treturn true;\n\t\t}\n\t}\n\n\t/* If there is a free slot in this node then we can just insert the\n\t * leaf here.\n\t */\n\tif (free_slot >= 0) {\n\t\tpr_devel(\"insert in free slot %d\\n\", free_slot);\n\t\tedit->leaf_p = &node->slots[free_slot];\n\t\tedit->adjust_count_on = node;\n\t\tpr_devel(\"<--%s() = ok [insert]\\n\", __func__);\n\t\treturn true;\n\t}\n\n\t/* The node has no spare slots - so we're either going to have to split\n\t * it or insert another node before it.\n\t *\n\t * Whatever, we're going to need at least two new nodes - so allocate\n\t * those now.  We may also need a new shortcut, but we deal with that\n\t * when we need it.\n\t */\n\tnew_n0 = kzalloc(sizeof(struct assoc_array_node), GFP_KERNEL);\n\tif (!new_n0)\n\t\treturn false;\n\tedit->new_meta[0] = assoc_array_node_to_ptr(new_n0);\n\tnew_n1 = kzalloc(sizeof(struct assoc_array_node), GFP_KERNEL);\n\tif (!new_n1)\n\t\treturn false;\n\tedit->new_meta[1] = assoc_array_node_to_ptr(new_n1);\n\n\t/* We need to find out how similar the leaves are. */\n\tpr_devel(\"no spare slots\\n\");\n\thave_meta = false;\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tptr = node->slots[i];\n\t\tif (assoc_array_ptr_is_meta(ptr)) {\n\t\t\tedit->segment_cache[i] = 0xff;\n\t\t\thave_meta = true;\n\t\t\tcontinue;\n\t\t}\n\t\tbase_seg = ops->get_object_key_chunk(\n\t\t\tassoc_array_ptr_to_leaf(ptr), level);\n\t\tbase_seg >>= level & ASSOC_ARRAY_KEY_CHUNK_MASK;\n\t\tedit->segment_cache[i] = base_seg & ASSOC_ARRAY_FAN_MASK;\n\t}\n\n\tif (have_meta) {\n\t\tpr_devel(\"have meta\\n\");\n\t\tgoto split_node;\n\t}\n\n\t/* The node contains only leaves */\n\tdissimilarity = 0;\n\tbase_seg = edit->segment_cache[0];\n\tfor (i = 1; i < ASSOC_ARRAY_FAN_OUT; i++)\n\t\tdissimilarity |= edit->segment_cache[i] ^ base_seg;\n\n\tpr_devel(\"only leaves; dissimilarity=%lx\\n\", dissimilarity);\n\n\tif ((dissimilarity & ASSOC_ARRAY_FAN_MASK) == 0) {\n\t\t/* The old leaves all cluster in the same slot.  We will need\n\t\t * to insert a shortcut if the new node wants to cluster with them.\n\t\t */\n\t\tif ((edit->segment_cache[ASSOC_ARRAY_FAN_OUT] ^ base_seg) == 0)\n\t\t\tgoto all_leaves_cluster_together;\n\n\t\t/* Otherwise we can just insert a new node ahead of the old\n\t\t * one.\n\t\t */\n\t\tgoto present_leaves_cluster_but_not_new_leaf;\n\t}\n\nsplit_node:\n\tpr_devel(\"split node\\n\");\n\n\t/* We need to split the current node; we know that the node doesn't\n\t * simply contain a full set of leaves that cluster together (it\n\t * contains meta pointers and/or non-clustering leaves).\n\t *\n\t * We need to expel at least two leaves out of a set consisting of the\n\t * leaves in the node and the new leaf.\n\t *\n\t * We need a new node (n0) to replace the current one and a new node to\n\t * take the expelled nodes (n1).\n\t */\n\tedit->set[0].to = assoc_array_node_to_ptr(new_n0);\n\tnew_n0->back_pointer = node->back_pointer;\n\tnew_n0->parent_slot = node->parent_slot;\n\tnew_n1->back_pointer = assoc_array_node_to_ptr(new_n0);\n\tnew_n1->parent_slot = -1; /* Need to calculate this */\n\ndo_split_node:\n\tpr_devel(\"do_split_node\\n\");\n\n\tnew_n0->nr_leaves_on_branch = node->nr_leaves_on_branch;\n\tnew_n1->nr_leaves_on_branch = 0;\n\n\t/* Begin by finding two matching leaves.  There have to be at least two\n\t * that match - even if there are meta pointers - because any leaf that\n\t * would match a slot with a meta pointer in it must be somewhere\n\t * behind that meta pointer and cannot be here.  Further, given N\n\t * remaining leaf slots, we now have N+1 leaves to go in them.\n\t */\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tslot = edit->segment_cache[i];\n\t\tif (slot != 0xff)\n\t\t\tfor (j = i + 1; j < ASSOC_ARRAY_FAN_OUT + 1; j++)\n\t\t\t\tif (edit->segment_cache[j] == slot)\n\t\t\t\t\tgoto found_slot_for_multiple_occupancy;\n\t}\nfound_slot_for_multiple_occupancy:\n\tpr_devel(\"same slot: %x %x [%02x]\\n\", i, j, slot);\n\tBUG_ON(i >= ASSOC_ARRAY_FAN_OUT);\n\tBUG_ON(j >= ASSOC_ARRAY_FAN_OUT + 1);\n\tBUG_ON(slot >= ASSOC_ARRAY_FAN_OUT);\n\n\tnew_n1->parent_slot = slot;\n\n\t/* Metadata pointers cannot change slot */\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++)\n\t\tif (assoc_array_ptr_is_meta(node->slots[i]))\n\t\t\tnew_n0->slots[i] = node->slots[i];\n\t\telse\n\t\t\tnew_n0->slots[i] = NULL;\n\tBUG_ON(new_n0->slots[slot] != NULL);\n\tnew_n0->slots[slot] = assoc_array_node_to_ptr(new_n1);\n\n\t/* Filter the leaf pointers between the new nodes */\n\tfree_slot = -1;\n\tnext_slot = 0;\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tif (assoc_array_ptr_is_meta(node->slots[i]))\n\t\t\tcontinue;\n\t\tif (edit->segment_cache[i] == slot) {\n\t\t\tnew_n1->slots[next_slot++] = node->slots[i];\n\t\t\tnew_n1->nr_leaves_on_branch++;\n\t\t} else {\n\t\t\tdo {\n\t\t\t\tfree_slot++;\n\t\t\t} while (new_n0->slots[free_slot] != NULL);\n\t\t\tnew_n0->slots[free_slot] = node->slots[i];\n\t\t}\n\t}\n\n\tpr_devel(\"filtered: f=%x n=%x\\n\", free_slot, next_slot);\n\n\tif (edit->segment_cache[ASSOC_ARRAY_FAN_OUT] != slot) {\n\t\tdo {\n\t\t\tfree_slot++;\n\t\t} while (new_n0->slots[free_slot] != NULL);\n\t\tedit->leaf_p = &new_n0->slots[free_slot];\n\t\tedit->adjust_count_on = new_n0;\n\t} else {\n\t\tedit->leaf_p = &new_n1->slots[next_slot++];\n\t\tedit->adjust_count_on = new_n1;\n\t}\n\n\tBUG_ON(next_slot <= 1);\n\n\tedit->set_backpointers_to = assoc_array_node_to_ptr(new_n0);\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tif (edit->segment_cache[i] == 0xff) {\n\t\t\tptr = node->slots[i];\n\t\t\tBUG_ON(assoc_array_ptr_is_leaf(ptr));\n\t\t\tif (assoc_array_ptr_is_node(ptr)) {\n\t\t\t\tside = assoc_array_ptr_to_node(ptr);\n\t\t\t\tedit->set_backpointers[i] = &side->back_pointer;\n\t\t\t} else {\n\t\t\t\tshortcut = assoc_array_ptr_to_shortcut(ptr);\n\t\t\t\tedit->set_backpointers[i] = &shortcut->back_pointer;\n\t\t\t}\n\t\t}\n\t}\n\n\tptr = node->back_pointer;\n\tif (!ptr)\n\t\tedit->set[0].ptr = &edit->array->root;\n\telse if (assoc_array_ptr_is_node(ptr))\n\t\tedit->set[0].ptr = &assoc_array_ptr_to_node(ptr)->slots[node->parent_slot];\n\telse\n\t\tedit->set[0].ptr = &assoc_array_ptr_to_shortcut(ptr)->next_node;\n\tedit->excised_meta[0] = assoc_array_node_to_ptr(node);\n\tpr_devel(\"<--%s() = ok [split node]\\n\", __func__);\n\treturn true;\n\npresent_leaves_cluster_but_not_new_leaf:\n\t/* All the old leaves cluster in the same slot, but the new leaf wants\n\t * to go into a different slot, so we create a new node to hold the new\n\t * leaf and a pointer to a new node holding all the old leaves.\n\t */\n\tpr_devel(\"present leaves cluster but not new leaf\\n\");\n\n\tnew_n0->back_pointer = node->back_pointer;\n\tnew_n0->parent_slot = node->parent_slot;\n\tnew_n0->nr_leaves_on_branch = node->nr_leaves_on_branch;\n\tnew_n1->back_pointer = assoc_array_node_to_ptr(new_n0);\n\tnew_n1->parent_slot = edit->segment_cache[0];\n\tnew_n1->nr_leaves_on_branch = node->nr_leaves_on_branch;\n\tedit->adjust_count_on = new_n0;\n\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++)\n\t\tnew_n1->slots[i] = node->slots[i];\n\n\tnew_n0->slots[edit->segment_cache[0]] = assoc_array_node_to_ptr(new_n0);\n\tedit->leaf_p = &new_n0->slots[edit->segment_cache[ASSOC_ARRAY_FAN_OUT]];\n\n\tedit->set[0].ptr = &assoc_array_ptr_to_node(node->back_pointer)->slots[node->parent_slot];\n\tedit->set[0].to = assoc_array_node_to_ptr(new_n0);\n\tedit->excised_meta[0] = assoc_array_node_to_ptr(node);\n\tpr_devel(\"<--%s() = ok [insert node before]\\n\", __func__);\n\treturn true;\n\nall_leaves_cluster_together:\n\t/* All the leaves, new and old, want to cluster together in this node\n\t * in the same slot, so we have to replace this node with a shortcut to\n\t * skip over the identical parts of the key and then place a pair of\n\t * nodes, one inside the other, at the end of the shortcut and\n\t * distribute the keys between them.\n\t *\n\t * Firstly we need to work out where the leaves start diverging as a\n\t * bit position into their keys so that we know how big the shortcut\n\t * needs to be.\n\t *\n\t * We only need to make a single pass of N of the N+1 leaves because if\n\t * any keys differ between themselves at bit X then at least one of\n\t * them must also differ with the base key at bit X or before.\n\t */\n\tpr_devel(\"all leaves cluster together\\n\");\n\tdiff = INT_MAX;\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tint x = ops->diff_objects(assoc_array_ptr_to_leaf(node->slots[i]),\n\t\t\t\t\t  index_key);\n\t\tif (x < diff) {\n\t\t\tBUG_ON(x < 0);\n\t\t\tdiff = x;\n\t\t}\n\t}\n\tBUG_ON(diff == INT_MAX);\n\tBUG_ON(diff < level + ASSOC_ARRAY_LEVEL_STEP);\n\n\tkeylen = round_up(diff, ASSOC_ARRAY_KEY_CHUNK_SIZE);\n\tkeylen >>= ASSOC_ARRAY_KEY_CHUNK_SHIFT;\n\n\tnew_s0 = kzalloc(sizeof(struct assoc_array_shortcut) +\n\t\t\t keylen * sizeof(unsigned long), GFP_KERNEL);\n\tif (!new_s0)\n\t\treturn false;\n\tedit->new_meta[2] = assoc_array_shortcut_to_ptr(new_s0);\n\n\tedit->set[0].to = assoc_array_shortcut_to_ptr(new_s0);\n\tnew_s0->back_pointer = node->back_pointer;\n\tnew_s0->parent_slot = node->parent_slot;\n\tnew_s0->next_node = assoc_array_node_to_ptr(new_n0);\n\tnew_n0->back_pointer = assoc_array_shortcut_to_ptr(new_s0);\n\tnew_n0->parent_slot = 0;\n\tnew_n1->back_pointer = assoc_array_node_to_ptr(new_n0);\n\tnew_n1->parent_slot = -1; /* Need to calculate this */\n\n\tnew_s0->skip_to_level = level = diff & ~ASSOC_ARRAY_LEVEL_STEP_MASK;\n\tpr_devel(\"skip_to_level = %d [diff %d]\\n\", level, diff);\n\tBUG_ON(level <= 0);\n\n\tfor (i = 0; i < keylen; i++)\n\t\tnew_s0->index_key[i] =\n\t\t\tops->get_key_chunk(index_key, i * ASSOC_ARRAY_KEY_CHUNK_SIZE);\n\n\tblank = ULONG_MAX << (level & ASSOC_ARRAY_KEY_CHUNK_MASK);\n\tpr_devel(\"blank off [%zu] %d: %lx\\n\", keylen - 1, level, blank);\n\tnew_s0->index_key[keylen - 1] &= ~blank;\n\n\t/* This now reduces to a node splitting exercise for which we'll need\n\t * to regenerate the disparity table.\n\t */\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tptr = node->slots[i];\n\t\tbase_seg = ops->get_object_key_chunk(assoc_array_ptr_to_leaf(ptr),\n\t\t\t\t\t\t     level);\n\t\tbase_seg >>= level & ASSOC_ARRAY_KEY_CHUNK_MASK;\n\t\tedit->segment_cache[i] = base_seg & ASSOC_ARRAY_FAN_MASK;\n\t}\n\n\tbase_seg = ops->get_key_chunk(index_key, level);\n\tbase_seg >>= level & ASSOC_ARRAY_KEY_CHUNK_MASK;\n\tedit->segment_cache[ASSOC_ARRAY_FAN_OUT] = base_seg & ASSOC_ARRAY_FAN_MASK;\n\tgoto do_split_node;\n}",
                        "code_after_change": "static bool assoc_array_insert_into_terminal_node(struct assoc_array_edit *edit,\n\t\t\t\t\t\t  const struct assoc_array_ops *ops,\n\t\t\t\t\t\t  const void *index_key,\n\t\t\t\t\t\t  struct assoc_array_walk_result *result)\n{\n\tstruct assoc_array_shortcut *shortcut, *new_s0;\n\tstruct assoc_array_node *node, *new_n0, *new_n1, *side;\n\tstruct assoc_array_ptr *ptr;\n\tunsigned long dissimilarity, base_seg, blank;\n\tsize_t keylen;\n\tbool have_meta;\n\tint level, diff;\n\tint slot, next_slot, free_slot, i, j;\n\n\tnode\t= result->terminal_node.node;\n\tlevel\t= result->terminal_node.level;\n\tedit->segment_cache[ASSOC_ARRAY_FAN_OUT] = result->terminal_node.slot;\n\n\tpr_devel(\"-->%s()\\n\", __func__);\n\n\t/* We arrived at a node which doesn't have an onward node or shortcut\n\t * pointer that we have to follow.  This means that (a) the leaf we\n\t * want must go here (either by insertion or replacement) or (b) we\n\t * need to split this node and insert in one of the fragments.\n\t */\n\tfree_slot = -1;\n\n\t/* Firstly, we have to check the leaves in this node to see if there's\n\t * a matching one we should replace in place.\n\t */\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tptr = node->slots[i];\n\t\tif (!ptr) {\n\t\t\tfree_slot = i;\n\t\t\tcontinue;\n\t\t}\n\t\tif (assoc_array_ptr_is_leaf(ptr) &&\n\t\t    ops->compare_object(assoc_array_ptr_to_leaf(ptr),\n\t\t\t\t\tindex_key)) {\n\t\t\tpr_devel(\"replace in slot %d\\n\", i);\n\t\t\tedit->leaf_p = &node->slots[i];\n\t\t\tedit->dead_leaf = node->slots[i];\n\t\t\tpr_devel(\"<--%s() = ok [replace]\\n\", __func__);\n\t\t\treturn true;\n\t\t}\n\t}\n\n\t/* If there is a free slot in this node then we can just insert the\n\t * leaf here.\n\t */\n\tif (free_slot >= 0) {\n\t\tpr_devel(\"insert in free slot %d\\n\", free_slot);\n\t\tedit->leaf_p = &node->slots[free_slot];\n\t\tedit->adjust_count_on = node;\n\t\tpr_devel(\"<--%s() = ok [insert]\\n\", __func__);\n\t\treturn true;\n\t}\n\n\t/* The node has no spare slots - so we're either going to have to split\n\t * it or insert another node before it.\n\t *\n\t * Whatever, we're going to need at least two new nodes - so allocate\n\t * those now.  We may also need a new shortcut, but we deal with that\n\t * when we need it.\n\t */\n\tnew_n0 = kzalloc(sizeof(struct assoc_array_node), GFP_KERNEL);\n\tif (!new_n0)\n\t\treturn false;\n\tedit->new_meta[0] = assoc_array_node_to_ptr(new_n0);\n\tnew_n1 = kzalloc(sizeof(struct assoc_array_node), GFP_KERNEL);\n\tif (!new_n1)\n\t\treturn false;\n\tedit->new_meta[1] = assoc_array_node_to_ptr(new_n1);\n\n\t/* We need to find out how similar the leaves are. */\n\tpr_devel(\"no spare slots\\n\");\n\thave_meta = false;\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tptr = node->slots[i];\n\t\tif (assoc_array_ptr_is_meta(ptr)) {\n\t\t\tedit->segment_cache[i] = 0xff;\n\t\t\thave_meta = true;\n\t\t\tcontinue;\n\t\t}\n\t\tbase_seg = ops->get_object_key_chunk(\n\t\t\tassoc_array_ptr_to_leaf(ptr), level);\n\t\tbase_seg >>= level & ASSOC_ARRAY_KEY_CHUNK_MASK;\n\t\tedit->segment_cache[i] = base_seg & ASSOC_ARRAY_FAN_MASK;\n\t}\n\n\tif (have_meta) {\n\t\tpr_devel(\"have meta\\n\");\n\t\tgoto split_node;\n\t}\n\n\t/* The node contains only leaves */\n\tdissimilarity = 0;\n\tbase_seg = edit->segment_cache[0];\n\tfor (i = 1; i < ASSOC_ARRAY_FAN_OUT; i++)\n\t\tdissimilarity |= edit->segment_cache[i] ^ base_seg;\n\n\tpr_devel(\"only leaves; dissimilarity=%lx\\n\", dissimilarity);\n\n\tif ((dissimilarity & ASSOC_ARRAY_FAN_MASK) == 0) {\n\t\t/* The old leaves all cluster in the same slot.  We will need\n\t\t * to insert a shortcut if the new node wants to cluster with them.\n\t\t */\n\t\tif ((edit->segment_cache[ASSOC_ARRAY_FAN_OUT] ^ base_seg) == 0)\n\t\t\tgoto all_leaves_cluster_together;\n\n\t\t/* Otherwise all the old leaves cluster in the same slot, but\n\t\t * the new leaf wants to go into a different slot - so we\n\t\t * create a new node (n0) to hold the new leaf and a pointer to\n\t\t * a new node (n1) holding all the old leaves.\n\t\t *\n\t\t * This can be done by falling through to the node splitting\n\t\t * path.\n\t\t */\n\t\tpr_devel(\"present leaves cluster but not new leaf\\n\");\n\t}\n\nsplit_node:\n\tpr_devel(\"split node\\n\");\n\n\t/* We need to split the current node.  The node must contain anything\n\t * from a single leaf (in the one leaf case, this leaf will cluster\n\t * with the new leaf) and the rest meta-pointers, to all leaves, some\n\t * of which may cluster.\n\t *\n\t * It won't contain the case in which all the current leaves plus the\n\t * new leaves want to cluster in the same slot.\n\t *\n\t * We need to expel at least two leaves out of a set consisting of the\n\t * leaves in the node and the new leaf.  The current meta pointers can\n\t * just be copied as they shouldn't cluster with any of the leaves.\n\t *\n\t * We need a new node (n0) to replace the current one and a new node to\n\t * take the expelled nodes (n1).\n\t */\n\tedit->set[0].to = assoc_array_node_to_ptr(new_n0);\n\tnew_n0->back_pointer = node->back_pointer;\n\tnew_n0->parent_slot = node->parent_slot;\n\tnew_n1->back_pointer = assoc_array_node_to_ptr(new_n0);\n\tnew_n1->parent_slot = -1; /* Need to calculate this */\n\ndo_split_node:\n\tpr_devel(\"do_split_node\\n\");\n\n\tnew_n0->nr_leaves_on_branch = node->nr_leaves_on_branch;\n\tnew_n1->nr_leaves_on_branch = 0;\n\n\t/* Begin by finding two matching leaves.  There have to be at least two\n\t * that match - even if there are meta pointers - because any leaf that\n\t * would match a slot with a meta pointer in it must be somewhere\n\t * behind that meta pointer and cannot be here.  Further, given N\n\t * remaining leaf slots, we now have N+1 leaves to go in them.\n\t */\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tslot = edit->segment_cache[i];\n\t\tif (slot != 0xff)\n\t\t\tfor (j = i + 1; j < ASSOC_ARRAY_FAN_OUT + 1; j++)\n\t\t\t\tif (edit->segment_cache[j] == slot)\n\t\t\t\t\tgoto found_slot_for_multiple_occupancy;\n\t}\nfound_slot_for_multiple_occupancy:\n\tpr_devel(\"same slot: %x %x [%02x]\\n\", i, j, slot);\n\tBUG_ON(i >= ASSOC_ARRAY_FAN_OUT);\n\tBUG_ON(j >= ASSOC_ARRAY_FAN_OUT + 1);\n\tBUG_ON(slot >= ASSOC_ARRAY_FAN_OUT);\n\n\tnew_n1->parent_slot = slot;\n\n\t/* Metadata pointers cannot change slot */\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++)\n\t\tif (assoc_array_ptr_is_meta(node->slots[i]))\n\t\t\tnew_n0->slots[i] = node->slots[i];\n\t\telse\n\t\t\tnew_n0->slots[i] = NULL;\n\tBUG_ON(new_n0->slots[slot] != NULL);\n\tnew_n0->slots[slot] = assoc_array_node_to_ptr(new_n1);\n\n\t/* Filter the leaf pointers between the new nodes */\n\tfree_slot = -1;\n\tnext_slot = 0;\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tif (assoc_array_ptr_is_meta(node->slots[i]))\n\t\t\tcontinue;\n\t\tif (edit->segment_cache[i] == slot) {\n\t\t\tnew_n1->slots[next_slot++] = node->slots[i];\n\t\t\tnew_n1->nr_leaves_on_branch++;\n\t\t} else {\n\t\t\tdo {\n\t\t\t\tfree_slot++;\n\t\t\t} while (new_n0->slots[free_slot] != NULL);\n\t\t\tnew_n0->slots[free_slot] = node->slots[i];\n\t\t}\n\t}\n\n\tpr_devel(\"filtered: f=%x n=%x\\n\", free_slot, next_slot);\n\n\tif (edit->segment_cache[ASSOC_ARRAY_FAN_OUT] != slot) {\n\t\tdo {\n\t\t\tfree_slot++;\n\t\t} while (new_n0->slots[free_slot] != NULL);\n\t\tedit->leaf_p = &new_n0->slots[free_slot];\n\t\tedit->adjust_count_on = new_n0;\n\t} else {\n\t\tedit->leaf_p = &new_n1->slots[next_slot++];\n\t\tedit->adjust_count_on = new_n1;\n\t}\n\n\tBUG_ON(next_slot <= 1);\n\n\tedit->set_backpointers_to = assoc_array_node_to_ptr(new_n0);\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tif (edit->segment_cache[i] == 0xff) {\n\t\t\tptr = node->slots[i];\n\t\t\tBUG_ON(assoc_array_ptr_is_leaf(ptr));\n\t\t\tif (assoc_array_ptr_is_node(ptr)) {\n\t\t\t\tside = assoc_array_ptr_to_node(ptr);\n\t\t\t\tedit->set_backpointers[i] = &side->back_pointer;\n\t\t\t} else {\n\t\t\t\tshortcut = assoc_array_ptr_to_shortcut(ptr);\n\t\t\t\tedit->set_backpointers[i] = &shortcut->back_pointer;\n\t\t\t}\n\t\t}\n\t}\n\n\tptr = node->back_pointer;\n\tif (!ptr)\n\t\tedit->set[0].ptr = &edit->array->root;\n\telse if (assoc_array_ptr_is_node(ptr))\n\t\tedit->set[0].ptr = &assoc_array_ptr_to_node(ptr)->slots[node->parent_slot];\n\telse\n\t\tedit->set[0].ptr = &assoc_array_ptr_to_shortcut(ptr)->next_node;\n\tedit->excised_meta[0] = assoc_array_node_to_ptr(node);\n\tpr_devel(\"<--%s() = ok [split node]\\n\", __func__);\n\treturn true;\n\nall_leaves_cluster_together:\n\t/* All the leaves, new and old, want to cluster together in this node\n\t * in the same slot, so we have to replace this node with a shortcut to\n\t * skip over the identical parts of the key and then place a pair of\n\t * nodes, one inside the other, at the end of the shortcut and\n\t * distribute the keys between them.\n\t *\n\t * Firstly we need to work out where the leaves start diverging as a\n\t * bit position into their keys so that we know how big the shortcut\n\t * needs to be.\n\t *\n\t * We only need to make a single pass of N of the N+1 leaves because if\n\t * any keys differ between themselves at bit X then at least one of\n\t * them must also differ with the base key at bit X or before.\n\t */\n\tpr_devel(\"all leaves cluster together\\n\");\n\tdiff = INT_MAX;\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tint x = ops->diff_objects(assoc_array_ptr_to_leaf(node->slots[i]),\n\t\t\t\t\t  index_key);\n\t\tif (x < diff) {\n\t\t\tBUG_ON(x < 0);\n\t\t\tdiff = x;\n\t\t}\n\t}\n\tBUG_ON(diff == INT_MAX);\n\tBUG_ON(diff < level + ASSOC_ARRAY_LEVEL_STEP);\n\n\tkeylen = round_up(diff, ASSOC_ARRAY_KEY_CHUNK_SIZE);\n\tkeylen >>= ASSOC_ARRAY_KEY_CHUNK_SHIFT;\n\n\tnew_s0 = kzalloc(sizeof(struct assoc_array_shortcut) +\n\t\t\t keylen * sizeof(unsigned long), GFP_KERNEL);\n\tif (!new_s0)\n\t\treturn false;\n\tedit->new_meta[2] = assoc_array_shortcut_to_ptr(new_s0);\n\n\tedit->set[0].to = assoc_array_shortcut_to_ptr(new_s0);\n\tnew_s0->back_pointer = node->back_pointer;\n\tnew_s0->parent_slot = node->parent_slot;\n\tnew_s0->next_node = assoc_array_node_to_ptr(new_n0);\n\tnew_n0->back_pointer = assoc_array_shortcut_to_ptr(new_s0);\n\tnew_n0->parent_slot = 0;\n\tnew_n1->back_pointer = assoc_array_node_to_ptr(new_n0);\n\tnew_n1->parent_slot = -1; /* Need to calculate this */\n\n\tnew_s0->skip_to_level = level = diff & ~ASSOC_ARRAY_LEVEL_STEP_MASK;\n\tpr_devel(\"skip_to_level = %d [diff %d]\\n\", level, diff);\n\tBUG_ON(level <= 0);\n\n\tfor (i = 0; i < keylen; i++)\n\t\tnew_s0->index_key[i] =\n\t\t\tops->get_key_chunk(index_key, i * ASSOC_ARRAY_KEY_CHUNK_SIZE);\n\n\tblank = ULONG_MAX << (level & ASSOC_ARRAY_KEY_CHUNK_MASK);\n\tpr_devel(\"blank off [%zu] %d: %lx\\n\", keylen - 1, level, blank);\n\tnew_s0->index_key[keylen - 1] &= ~blank;\n\n\t/* This now reduces to a node splitting exercise for which we'll need\n\t * to regenerate the disparity table.\n\t */\n\tfor (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++) {\n\t\tptr = node->slots[i];\n\t\tbase_seg = ops->get_object_key_chunk(assoc_array_ptr_to_leaf(ptr),\n\t\t\t\t\t\t     level);\n\t\tbase_seg >>= level & ASSOC_ARRAY_KEY_CHUNK_MASK;\n\t\tedit->segment_cache[i] = base_seg & ASSOC_ARRAY_FAN_MASK;\n\t}\n\n\tbase_seg = ops->get_key_chunk(index_key, level);\n\tbase_seg >>= level & ASSOC_ARRAY_KEY_CHUNK_MASK;\n\tedit->segment_cache[ASSOC_ARRAY_FAN_OUT] = base_seg & ASSOC_ARRAY_FAN_MASK;\n\tgoto do_split_node;\n}",
                        "cve_id": "CVE-2017-12193"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 4089,
            "cve_id": "CVE-2023-22997",
            "code_snippet": "static ssize_t module_xz_decompress(struct load_info *info,\n\t\t\t\t    const void *buf, size_t size)\n{\n\tstatic const u8 signature[] = { 0xfd, '7', 'z', 'X', 'Z', 0 };\n\tstruct xz_dec *xz_dec;\n\tstruct xz_buf xz_buf;\n\tenum xz_ret xz_ret;\n\tsize_t new_size = 0;\n\tssize_t retval;\n\n\tif (size < sizeof(signature) ||\n\t    memcmp(buf, signature, sizeof(signature))) {\n\t\tpr_err(\"not an xz compressed module\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\txz_dec = xz_dec_init(XZ_DYNALLOC, (u32)-1);\n\tif (!xz_dec)\n\t\treturn -ENOMEM;\n\n\txz_buf.in_size = size;\n\txz_buf.in = buf;\n\txz_buf.in_pos = 0;\n\n\tdo {\n\t\tstruct page *page = module_get_next_page(info);\n\n\t\tif (IS_ERR(page)) {\n\t\t\tretval = PTR_ERR(page);\n\t\t\tgoto out;\n\t\t}\n\n\t\txz_buf.out = kmap_local_page(page);\n\t\txz_buf.out_pos = 0;\n\t\txz_buf.out_size = PAGE_SIZE;\n\t\txz_ret = xz_dec_run(xz_dec, &xz_buf);\n\t\tkunmap_local(xz_buf.out);\n\n\t\tnew_size += xz_buf.out_pos;\n\t} while (xz_buf.out_pos == PAGE_SIZE && xz_ret == XZ_OK);\n\n\tif (xz_ret != XZ_STREAM_END) {\n\t\tpr_err(\"decompression failed with status %d\\n\", xz_ret);\n\t\tretval = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tretval = new_size;\n\n out:\n\txz_dec_end(xz_dec);\n\treturn retval;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static ssize_t module_gzip_decompress(struct load_info *info,\n\t\t\t\t      const void *buf, size_t size)\n{\n\tstruct z_stream_s s = { 0 };\n\tsize_t new_size = 0;\n\tsize_t gzip_hdr_len;\n\tssize_t retval;\n\tint rc;\n\n\tgzip_hdr_len = module_gzip_header_len(buf, size);\n\tif (!gzip_hdr_len) {\n\t\tpr_err(\"not a gzip compressed module\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\ts.next_in = buf + gzip_hdr_len;\n\ts.avail_in = size - gzip_hdr_len;\n\n\ts.workspace = kmalloc(zlib_inflate_workspacesize(), GFP_KERNEL);\n\tif (!s.workspace)\n\t\treturn -ENOMEM;\n\n\trc = zlib_inflateInit2(&s, -MAX_WBITS);\n\tif (rc != Z_OK) {\n\t\tpr_err(\"failed to initialize decompressor: %d\\n\", rc);\n\t\tretval = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tdo {\n\t\tstruct page *page = module_get_next_page(info);\n\n\t\tif (!page) {\n\t\t\tretval = -ENOMEM;\n\t\t\tgoto out_inflate_end;\n\t\t}\n\n\t\ts.next_out = kmap_local_page(page);\n\t\ts.avail_out = PAGE_SIZE;\n\t\trc = zlib_inflate(&s, 0);\n\t\tkunmap_local(s.next_out);\n\n\t\tnew_size += PAGE_SIZE - s.avail_out;\n\t} while (rc == Z_OK);\n\n\tif (rc != Z_STREAM_END) {\n\t\tpr_err(\"decompression failed with status %d\\n\", rc);\n\t\tretval = -EINVAL;\n\t\tgoto out_inflate_end;\n\t}\n\n\tretval = new_size;\n\nout_inflate_end:\n\tzlib_inflateEnd(&s);\nout:\n\tkfree(s.workspace);\n\treturn retval;\n}",
                        "code_after_change": "static ssize_t module_gzip_decompress(struct load_info *info,\n\t\t\t\t      const void *buf, size_t size)\n{\n\tstruct z_stream_s s = { 0 };\n\tsize_t new_size = 0;\n\tsize_t gzip_hdr_len;\n\tssize_t retval;\n\tint rc;\n\n\tgzip_hdr_len = module_gzip_header_len(buf, size);\n\tif (!gzip_hdr_len) {\n\t\tpr_err(\"not a gzip compressed module\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\ts.next_in = buf + gzip_hdr_len;\n\ts.avail_in = size - gzip_hdr_len;\n\n\ts.workspace = kmalloc(zlib_inflate_workspacesize(), GFP_KERNEL);\n\tif (!s.workspace)\n\t\treturn -ENOMEM;\n\n\trc = zlib_inflateInit2(&s, -MAX_WBITS);\n\tif (rc != Z_OK) {\n\t\tpr_err(\"failed to initialize decompressor: %d\\n\", rc);\n\t\tretval = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tdo {\n\t\tstruct page *page = module_get_next_page(info);\n\n\t\tif (IS_ERR(page)) {\n\t\t\tretval = PTR_ERR(page);\n\t\t\tgoto out_inflate_end;\n\t\t}\n\n\t\ts.next_out = kmap_local_page(page);\n\t\ts.avail_out = PAGE_SIZE;\n\t\trc = zlib_inflate(&s, 0);\n\t\tkunmap_local(s.next_out);\n\n\t\tnew_size += PAGE_SIZE - s.avail_out;\n\t} while (rc == Z_OK);\n\n\tif (rc != Z_STREAM_END) {\n\t\tpr_err(\"decompression failed with status %d\\n\", rc);\n\t\tretval = -EINVAL;\n\t\tgoto out_inflate_end;\n\t}\n\n\tretval = new_size;\n\nout_inflate_end:\n\tzlib_inflateEnd(&s);\nout:\n\tkfree(s.workspace);\n\treturn retval;\n}",
                        "cve_id": "CVE-2023-22997"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 4138,
            "cve_id": "CVE-2023-28327",
            "code_snippet": "static int unix_diag_get_exact(struct sk_buff *in_skb,\n\t\t\t       const struct nlmsghdr *nlh,\n\t\t\t       struct unix_diag_req *req)\n{\n\tstruct net *net = sock_net(in_skb->sk);\n\tunsigned int extra_len;\n\tstruct sk_buff *rep;\n\tstruct sock *sk;\n\tint err;\n\n\terr = -EINVAL;\n\tif (req->udiag_ino == 0)\n\t\tgoto out_nosk;\n\n\tsk = unix_lookup_by_ino(net, req->udiag_ino);\n\terr = -ENOENT;\n\tif (sk == NULL)\n\t\tgoto out_nosk;\n\n\terr = sock_diag_check_cookie(sk, req->udiag_cookie);\n\tif (err)\n\t\tgoto out;\n\n\textra_len = 256;\nagain:\n\terr = -ENOMEM;\n\trep = nlmsg_new(sizeof(struct unix_diag_msg) + extra_len, GFP_KERNEL);\n\tif (!rep)\n\t\tgoto out;\n\n\terr = sk_diag_fill(sk, rep, req, sk_user_ns(NETLINK_CB(in_skb).sk),\n\t\t\t   NETLINK_CB(in_skb).portid,\n\t\t\t   nlh->nlmsg_seq, 0, req->udiag_ino);\n\tif (err < 0) {\n\t\tnlmsg_free(rep);\n\t\textra_len += 256;\n\t\tif (extra_len >= PAGE_SIZE)\n\t\t\tgoto out;\n\n\t\tgoto again;\n\t}\n\terr = nlmsg_unicast(net->diag_nlsk, rep, NETLINK_CB(in_skb).portid);\n\nout:\n\tif (sk)\n\t\tsock_put(sk);\nout_nosk:\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int inet_rtm_getroute(struct sk_buff *in_skb, struct nlmsghdr *nlh,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(in_skb->sk);\n\tstruct rtmsg *rtm;\n\tstruct nlattr *tb[RTA_MAX+1];\n\tstruct fib_result res = {};\n\tstruct rtable *rt = NULL;\n\tstruct flowi4 fl4;\n\t__be32 dst = 0;\n\t__be32 src = 0;\n\tu32 iif;\n\tint err;\n\tint mark;\n\tstruct sk_buff *skb;\n\tu32 table_id = RT_TABLE_MAIN;\n\tkuid_t uid;\n\n\terr = nlmsg_parse(nlh, sizeof(*rtm), tb, RTA_MAX, rtm_ipv4_policy,\n\t\t\t  extack);\n\tif (err < 0)\n\t\tgoto errout;\n\n\trtm = nlmsg_data(nlh);\n\n\tskb = alloc_skb(NLMSG_GOODSIZE, GFP_KERNEL);\n\tif (!skb) {\n\t\terr = -ENOBUFS;\n\t\tgoto errout;\n\t}\n\n\t/* Reserve room for dummy headers, this skb can pass\n\t   through good chunk of routing engine.\n\t */\n\tskb_reset_mac_header(skb);\n\tskb_reset_network_header(skb);\n\n\tsrc = tb[RTA_SRC] ? nla_get_in_addr(tb[RTA_SRC]) : 0;\n\tdst = tb[RTA_DST] ? nla_get_in_addr(tb[RTA_DST]) : 0;\n\tiif = tb[RTA_IIF] ? nla_get_u32(tb[RTA_IIF]) : 0;\n\tmark = tb[RTA_MARK] ? nla_get_u32(tb[RTA_MARK]) : 0;\n\tif (tb[RTA_UID])\n\t\tuid = make_kuid(current_user_ns(), nla_get_u32(tb[RTA_UID]));\n\telse\n\t\tuid = (iif ? INVALID_UID : current_uid());\n\n\t/* Bugfix: need to give ip_route_input enough of an IP header to\n\t * not gag.\n\t */\n\tip_hdr(skb)->protocol = IPPROTO_UDP;\n\tip_hdr(skb)->saddr = src;\n\tip_hdr(skb)->daddr = dst;\n\n\tskb_reserve(skb, MAX_HEADER + sizeof(struct iphdr));\n\n\tmemset(&fl4, 0, sizeof(fl4));\n\tfl4.daddr = dst;\n\tfl4.saddr = src;\n\tfl4.flowi4_tos = rtm->rtm_tos;\n\tfl4.flowi4_oif = tb[RTA_OIF] ? nla_get_u32(tb[RTA_OIF]) : 0;\n\tfl4.flowi4_mark = mark;\n\tfl4.flowi4_uid = uid;\n\n\trcu_read_lock();\n\n\tif (iif) {\n\t\tstruct net_device *dev;\n\n\t\tdev = dev_get_by_index_rcu(net, iif);\n\t\tif (!dev) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto errout_free;\n\t\t}\n\n\t\tskb->protocol\t= htons(ETH_P_IP);\n\t\tskb->dev\t= dev;\n\t\tskb->mark\t= mark;\n\t\terr = ip_route_input_rcu(skb, dst, src, rtm->rtm_tos,\n\t\t\t\t\t dev, &res);\n\n\t\trt = skb_rtable(skb);\n\t\tif (err == 0 && rt->dst.error)\n\t\t\terr = -rt->dst.error;\n\t} else {\n\t\trt = ip_route_output_key_hash_rcu(net, &fl4, &res, skb);\n\t\terr = 0;\n\t\tif (IS_ERR(rt))\n\t\t\terr = PTR_ERR(rt);\n\t\telse\n\t\t\tskb_dst_set(skb, &rt->dst);\n\t}\n\n\tif (err)\n\t\tgoto errout_free;\n\n\tif (rtm->rtm_flags & RTM_F_NOTIFY)\n\t\trt->rt_flags |= RTCF_NOTIFY;\n\n\tif (rtm->rtm_flags & RTM_F_LOOKUP_TABLE)\n\t\ttable_id = rt->rt_table_id;\n\n\tif (rtm->rtm_flags & RTM_F_FIB_MATCH)\n\t\terr = fib_dump_info(skb, NETLINK_CB(in_skb).portid,\n\t\t\t\t    nlh->nlmsg_seq, RTM_NEWROUTE, table_id,\n\t\t\t\t    rt->rt_type, res.prefix, res.prefixlen,\n\t\t\t\t    fl4.flowi4_tos, res.fi, 0);\n\telse\n\t\terr = rt_fill_info(net, dst, src, table_id, &fl4, skb,\n\t\t\t\t   NETLINK_CB(in_skb).portid, nlh->nlmsg_seq);\n\tif (err < 0)\n\t\tgoto errout_free;\n\n\trcu_read_unlock();\n\n\terr = rtnl_unicast(skb, net, NETLINK_CB(in_skb).portid);\nerrout:\n\treturn err;\n\nerrout_free:\n\trcu_read_unlock();\n\tkfree_skb(skb);\n\tgoto errout;\n}",
                        "code_after_change": "static int rtnl_getlink(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct net *tgt_net = net;\n\tstruct ifinfomsg *ifm;\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct net_device *dev = NULL;\n\tstruct sk_buff *nskb;\n\tint netnsid = -1;\n\tint err;\n\tu32 ext_filter_mask = 0;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IF_NETNSID]) {\n\t\tnetnsid = nla_get_s32(tb[IFLA_IF_NETNSID]);\n\t\ttgt_net = get_target_net(NETLINK_CB(skb).sk, netnsid);\n\t\tif (IS_ERR(tgt_net))\n\t\t\treturn PTR_ERR(tgt_net);\n\t}\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\n\tif (tb[IFLA_EXT_MASK])\n\t\text_filter_mask = nla_get_u32(tb[IFLA_EXT_MASK]);\n\n\terr = -EINVAL;\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(tgt_net, ifm->ifi_index);\n\telse if (tb[IFLA_IFNAME])\n\t\tdev = __dev_get_by_name(tgt_net, ifname);\n\telse\n\t\tgoto out;\n\n\terr = -ENODEV;\n\tif (dev == NULL)\n\t\tgoto out;\n\n\terr = -ENOBUFS;\n\tnskb = nlmsg_new(if_nlmsg_size(dev, ext_filter_mask), GFP_KERNEL);\n\tif (nskb == NULL)\n\t\tgoto out;\n\n\terr = rtnl_fill_ifinfo(nskb, dev, net,\n\t\t\t       RTM_NEWLINK, NETLINK_CB(skb).portid,\n\t\t\t       nlh->nlmsg_seq, 0, 0, ext_filter_mask,\n\t\t\t       0, NULL, netnsid);\n\tif (err < 0) {\n\t\t/* -EMSGSIZE implies BUG in if_nlmsg_size */\n\t\tWARN_ON(err == -EMSGSIZE);\n\t\tkfree_skb(nskb);\n\t} else\n\t\terr = rtnl_unicast(nskb, net, NETLINK_CB(skb).portid);\nout:\n\tif (netnsid >= 0)\n\t\tput_net(tgt_net);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2017-13686"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int rtnl_getlink(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct net *tgt_net = net;\n\tstruct ifinfomsg *ifm;\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct net_device *dev = NULL;\n\tstruct sk_buff *nskb;\n\tint netnsid = -1;\n\tint err;\n\tu32 ext_filter_mask = 0;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IF_NETNSID]) {\n\t\tnetnsid = nla_get_s32(tb[IFLA_IF_NETNSID]);\n\t\ttgt_net = get_target_net(skb, netnsid);\n\t\tif (IS_ERR(tgt_net))\n\t\t\treturn PTR_ERR(tgt_net);\n\t}\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\n\tif (tb[IFLA_EXT_MASK])\n\t\text_filter_mask = nla_get_u32(tb[IFLA_EXT_MASK]);\n\n\terr = -EINVAL;\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(tgt_net, ifm->ifi_index);\n\telse if (tb[IFLA_IFNAME])\n\t\tdev = __dev_get_by_name(tgt_net, ifname);\n\telse\n\t\tgoto out;\n\n\terr = -ENODEV;\n\tif (dev == NULL)\n\t\tgoto out;\n\n\terr = -ENOBUFS;\n\tnskb = nlmsg_new(if_nlmsg_size(dev, ext_filter_mask), GFP_KERNEL);\n\tif (nskb == NULL)\n\t\tgoto out;\n\n\terr = rtnl_fill_ifinfo(nskb, dev, net,\n\t\t\t       RTM_NEWLINK, NETLINK_CB(skb).portid,\n\t\t\t       nlh->nlmsg_seq, 0, 0, ext_filter_mask,\n\t\t\t       0, NULL, netnsid);\n\tif (err < 0) {\n\t\t/* -EMSGSIZE implies BUG in if_nlmsg_size */\n\t\tWARN_ON(err == -EMSGSIZE);\n\t\tkfree_skb(nskb);\n\t} else\n\t\terr = rtnl_unicast(nskb, net, NETLINK_CB(skb).portid);\nout:\n\tif (netnsid >= 0)\n\t\tput_net(tgt_net);\n\n\treturn err;\n}",
                        "code_after_change": "static int inet_rtm_getroute(struct sk_buff *in_skb, struct nlmsghdr *nlh,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(in_skb->sk);\n\tstruct rtmsg *rtm;\n\tstruct nlattr *tb[RTA_MAX+1];\n\tstruct fib_result res = {};\n\tstruct rtable *rt = NULL;\n\tstruct flowi4 fl4;\n\t__be32 dst = 0;\n\t__be32 src = 0;\n\tu32 iif;\n\tint err;\n\tint mark;\n\tstruct sk_buff *skb;\n\tu32 table_id = RT_TABLE_MAIN;\n\tkuid_t uid;\n\n\terr = nlmsg_parse(nlh, sizeof(*rtm), tb, RTA_MAX, rtm_ipv4_policy,\n\t\t\t  extack);\n\tif (err < 0)\n\t\tgoto errout;\n\n\trtm = nlmsg_data(nlh);\n\n\tskb = alloc_skb(NLMSG_GOODSIZE, GFP_KERNEL);\n\tif (!skb) {\n\t\terr = -ENOBUFS;\n\t\tgoto errout;\n\t}\n\n\t/* Reserve room for dummy headers, this skb can pass\n\t   through good chunk of routing engine.\n\t */\n\tskb_reset_mac_header(skb);\n\tskb_reset_network_header(skb);\n\n\tsrc = tb[RTA_SRC] ? nla_get_in_addr(tb[RTA_SRC]) : 0;\n\tdst = tb[RTA_DST] ? nla_get_in_addr(tb[RTA_DST]) : 0;\n\tiif = tb[RTA_IIF] ? nla_get_u32(tb[RTA_IIF]) : 0;\n\tmark = tb[RTA_MARK] ? nla_get_u32(tb[RTA_MARK]) : 0;\n\tif (tb[RTA_UID])\n\t\tuid = make_kuid(current_user_ns(), nla_get_u32(tb[RTA_UID]));\n\telse\n\t\tuid = (iif ? INVALID_UID : current_uid());\n\n\t/* Bugfix: need to give ip_route_input enough of an IP header to\n\t * not gag.\n\t */\n\tip_hdr(skb)->protocol = IPPROTO_UDP;\n\tip_hdr(skb)->saddr = src;\n\tip_hdr(skb)->daddr = dst;\n\n\tskb_reserve(skb, MAX_HEADER + sizeof(struct iphdr));\n\n\tmemset(&fl4, 0, sizeof(fl4));\n\tfl4.daddr = dst;\n\tfl4.saddr = src;\n\tfl4.flowi4_tos = rtm->rtm_tos;\n\tfl4.flowi4_oif = tb[RTA_OIF] ? nla_get_u32(tb[RTA_OIF]) : 0;\n\tfl4.flowi4_mark = mark;\n\tfl4.flowi4_uid = uid;\n\n\trcu_read_lock();\n\n\tif (iif) {\n\t\tstruct net_device *dev;\n\n\t\tdev = dev_get_by_index_rcu(net, iif);\n\t\tif (!dev) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto errout_free;\n\t\t}\n\n\t\tskb->protocol\t= htons(ETH_P_IP);\n\t\tskb->dev\t= dev;\n\t\tskb->mark\t= mark;\n\t\terr = ip_route_input_rcu(skb, dst, src, rtm->rtm_tos,\n\t\t\t\t\t dev, &res);\n\n\t\trt = skb_rtable(skb);\n\t\tif (err == 0 && rt->dst.error)\n\t\t\terr = -rt->dst.error;\n\t} else {\n\t\trt = ip_route_output_key_hash_rcu(net, &fl4, &res, skb);\n\t\terr = 0;\n\t\tif (IS_ERR(rt))\n\t\t\terr = PTR_ERR(rt);\n\t\telse\n\t\t\tskb_dst_set(skb, &rt->dst);\n\t}\n\n\tif (err)\n\t\tgoto errout_free;\n\n\tif (rtm->rtm_flags & RTM_F_NOTIFY)\n\t\trt->rt_flags |= RTCF_NOTIFY;\n\n\tif (rtm->rtm_flags & RTM_F_LOOKUP_TABLE)\n\t\ttable_id = rt->rt_table_id;\n\n\tif (rtm->rtm_flags & RTM_F_FIB_MATCH) {\n\t\tif (!res.fi) {\n\t\t\terr = fib_props[res.type].error;\n\t\t\tif (!err)\n\t\t\t\terr = -EHOSTUNREACH;\n\t\t\tgoto errout_free;\n\t\t}\n\t\terr = fib_dump_info(skb, NETLINK_CB(in_skb).portid,\n\t\t\t\t    nlh->nlmsg_seq, RTM_NEWROUTE, table_id,\n\t\t\t\t    rt->rt_type, res.prefix, res.prefixlen,\n\t\t\t\t    fl4.flowi4_tos, res.fi, 0);\n\t} else {\n\t\terr = rt_fill_info(net, dst, src, table_id, &fl4, skb,\n\t\t\t\t   NETLINK_CB(in_skb).portid, nlh->nlmsg_seq);\n\t}\n\tif (err < 0)\n\t\tgoto errout_free;\n\n\trcu_read_unlock();\n\n\terr = rtnl_unicast(skb, net, NETLINK_CB(in_skb).portid);\nerrout:\n\treturn err;\n\nerrout_free:\n\trcu_read_unlock();\n\tkfree_skb(skb);\n\tgoto errout;\n}",
                        "cve_id": "CVE-2018-14646"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 4144,
            "cve_id": "CVE-2023-28466",
            "code_snippet": "static int do_tls_getsockopt(struct sock *sk, int optname,\n\t\t\t     char __user *optval, int __user *optlen)\n{\n\tint rc = 0;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase TLS_TX:\n\tcase TLS_RX:\n\t\trc = do_tls_getsockopt_conf(sk, optval, optlen,\n\t\t\t\t\t    optname == TLS_TX);\n\t\tbreak;\n\tcase TLS_TX_ZEROCOPY_RO:\n\t\trc = do_tls_getsockopt_tx_zc(sk, optval, optlen);\n\t\tbreak;\n\tcase TLS_RX_EXPECT_NO_PAD:\n\t\trc = do_tls_getsockopt_no_pad(sk, optval, optlen);\n\t\tbreak;\n\tdefault:\n\t\trc = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\n\treturn rc;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int sco_sock_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t       char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tint len, err = 0;\n\tstruct bt_voice voice;\n\tu32 phys;\n\tint pkt_status;\n\n\tBT_DBG(\"sk %p\", sk);\n\n\tif (level == SOL_SCO)\n\t\treturn sco_sock_getsockopt_old(sock, optname, optval, optlen);\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\n\tcase BT_DEFER_SETUP:\n\t\tif (sk->sk_state != BT_BOUND && sk->sk_state != BT_LISTEN) {\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (put_user(test_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags),\n\t\t\t     (u32 __user *)optval))\n\t\t\terr = -EFAULT;\n\n\t\tbreak;\n\n\tcase BT_VOICE:\n\t\tvoice.setting = sco_pi(sk)->setting;\n\n\t\tlen = min_t(unsigned int, len, sizeof(voice));\n\t\tif (copy_to_user(optval, (char *)&voice, len))\n\t\t\terr = -EFAULT;\n\n\t\tbreak;\n\n\tcase BT_PHY:\n\t\tif (sk->sk_state != BT_CONNECTED) {\n\t\t\terr = -ENOTCONN;\n\t\t\tbreak;\n\t\t}\n\n\t\tphys = hci_conn_get_phy(sco_pi(sk)->conn->hcon);\n\n\t\tif (put_user(phys, (u32 __user *) optval))\n\t\t\terr = -EFAULT;\n\t\tbreak;\n\n\tcase BT_PKT_STATUS:\n\t\tpkt_status = (sco_pi(sk)->cmsg_mask & SCO_CMSG_PKT_STATUS);\n\n\t\tif (put_user(pkt_status, (int __user *)optval))\n\t\t\terr = -EFAULT;\n\t\tbreak;\n\n\tcase BT_SNDMTU:\n\tcase BT_RCVMTU:\n\t\tif (put_user(sco_pi(sk)->conn->mtu, (u32 __user *)optval))\n\t\t\terr = -EFAULT;\n\t\tbreak;\n\n\tdefault:\n\t\terr = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\treturn err;\n}",
                        "code_after_change": "static int sco_sock_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t       char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tint len, err = 0;\n\tstruct bt_voice voice;\n\tu32 phys;\n\tint pkt_status;\n\n\tBT_DBG(\"sk %p\", sk);\n\n\tif (level == SOL_SCO)\n\t\treturn sco_sock_getsockopt_old(sock, optname, optval, optlen);\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\n\tcase BT_DEFER_SETUP:\n\t\tif (sk->sk_state != BT_BOUND && sk->sk_state != BT_LISTEN) {\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (put_user(test_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags),\n\t\t\t     (u32 __user *)optval))\n\t\t\terr = -EFAULT;\n\n\t\tbreak;\n\n\tcase BT_VOICE:\n\t\tvoice.setting = sco_pi(sk)->setting;\n\n\t\tlen = min_t(unsigned int, len, sizeof(voice));\n\t\tif (copy_to_user(optval, (char *)&voice, len))\n\t\t\terr = -EFAULT;\n\n\t\tbreak;\n\n\tcase BT_PHY:\n\t\tif (sk->sk_state != BT_CONNECTED) {\n\t\t\terr = -ENOTCONN;\n\t\t\tbreak;\n\t\t}\n\n\t\tphys = hci_conn_get_phy(sco_pi(sk)->conn->hcon);\n\n\t\tif (put_user(phys, (u32 __user *) optval))\n\t\t\terr = -EFAULT;\n\t\tbreak;\n\n\tcase BT_PKT_STATUS:\n\t\tpkt_status = (sco_pi(sk)->cmsg_mask & SCO_CMSG_PKT_STATUS);\n\n\t\tif (put_user(pkt_status, (int __user *)optval))\n\t\t\terr = -EFAULT;\n\t\tbreak;\n\n\tcase BT_SNDMTU:\n\tcase BT_RCVMTU:\n\t\tif (sk->sk_state != BT_CONNECTED) {\n\t\t\terr = -ENOTCONN;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (put_user(sco_pi(sk)->conn->mtu, (u32 __user *)optval))\n\t\t\terr = -EFAULT;\n\t\tbreak;\n\n\tdefault:\n\t\terr = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\treturn err;\n}",
                        "cve_id": "CVE-2020-35499"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int do_tls_getsockopt_conf(struct sock *sk, char __user *optval,\n\t\t\t\t  int __user *optlen, int tx)\n{\n\tint rc = 0;\n\tstruct tls_context *ctx = tls_get_ctx(sk);\n\tstruct tls_crypto_info *crypto_info;\n\tstruct cipher_context *cctx;\n\tint len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tif (!optval || (len < sizeof(*crypto_info))) {\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (!ctx) {\n\t\trc = -EBUSY;\n\t\tgoto out;\n\t}\n\n\t/* get user crypto info */\n\tif (tx) {\n\t\tcrypto_info = &ctx->crypto_send.info;\n\t\tcctx = &ctx->tx;\n\t} else {\n\t\tcrypto_info = &ctx->crypto_recv.info;\n\t\tcctx = &ctx->rx;\n\t}\n\n\tif (!TLS_CRYPTO_INFO_READY(crypto_info)) {\n\t\trc = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tif (len == sizeof(*crypto_info)) {\n\t\tif (copy_to_user(optval, crypto_info, sizeof(*crypto_info)))\n\t\t\trc = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tswitch (crypto_info->cipher_type) {\n\tcase TLS_CIPHER_AES_GCM_128: {\n\t\tstruct tls12_crypto_info_aes_gcm_128 *\n\t\t  crypto_info_aes_gcm_128 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aes_gcm_128,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aes_gcm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(crypto_info_aes_gcm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_GCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_GCM_128_IV_SIZE);\n\t\tmemcpy(crypto_info_aes_gcm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_GCM_128_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aes_gcm_128,\n\t\t\t\t sizeof(*crypto_info_aes_gcm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_AES_GCM_256: {\n\t\tstruct tls12_crypto_info_aes_gcm_256 *\n\t\t  crypto_info_aes_gcm_256 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aes_gcm_256,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aes_gcm_256)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(crypto_info_aes_gcm_256->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_GCM_256_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_GCM_256_IV_SIZE);\n\t\tmemcpy(crypto_info_aes_gcm_256->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_GCM_256_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aes_gcm_256,\n\t\t\t\t sizeof(*crypto_info_aes_gcm_256)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_AES_CCM_128: {\n\t\tstruct tls12_crypto_info_aes_ccm_128 *aes_ccm_128 =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_aes_ccm_128, info);\n\n\t\tif (len != sizeof(*aes_ccm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(aes_ccm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_CCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_CCM_128_IV_SIZE);\n\t\tmemcpy(aes_ccm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_CCM_128_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval, aes_ccm_128, sizeof(*aes_ccm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_CHACHA20_POLY1305: {\n\t\tstruct tls12_crypto_info_chacha20_poly1305 *chacha20_poly1305 =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_chacha20_poly1305,\n\t\t\t\tinfo);\n\n\t\tif (len != sizeof(*chacha20_poly1305)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(chacha20_poly1305->iv,\n\t\t       cctx->iv + TLS_CIPHER_CHACHA20_POLY1305_SALT_SIZE,\n\t\t       TLS_CIPHER_CHACHA20_POLY1305_IV_SIZE);\n\t\tmemcpy(chacha20_poly1305->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_CHACHA20_POLY1305_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval, chacha20_poly1305,\n\t\t\t\tsizeof(*chacha20_poly1305)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_SM4_GCM: {\n\t\tstruct tls12_crypto_info_sm4_gcm *sm4_gcm_info =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_sm4_gcm, info);\n\n\t\tif (len != sizeof(*sm4_gcm_info)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(sm4_gcm_info->iv,\n\t\t       cctx->iv + TLS_CIPHER_SM4_GCM_SALT_SIZE,\n\t\t       TLS_CIPHER_SM4_GCM_IV_SIZE);\n\t\tmemcpy(sm4_gcm_info->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_SM4_GCM_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval, sm4_gcm_info, sizeof(*sm4_gcm_info)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_SM4_CCM: {\n\t\tstruct tls12_crypto_info_sm4_ccm *sm4_ccm_info =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_sm4_ccm, info);\n\n\t\tif (len != sizeof(*sm4_ccm_info)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(sm4_ccm_info->iv,\n\t\t       cctx->iv + TLS_CIPHER_SM4_CCM_SALT_SIZE,\n\t\t       TLS_CIPHER_SM4_CCM_IV_SIZE);\n\t\tmemcpy(sm4_ccm_info->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_SM4_CCM_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval, sm4_ccm_info, sizeof(*sm4_ccm_info)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_ARIA_GCM_128: {\n\t\tstruct tls12_crypto_info_aria_gcm_128 *\n\t\t  crypto_info_aria_gcm_128 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aria_gcm_128,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aria_gcm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(crypto_info_aria_gcm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_ARIA_GCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_ARIA_GCM_128_IV_SIZE);\n\t\tmemcpy(crypto_info_aria_gcm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_ARIA_GCM_128_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aria_gcm_128,\n\t\t\t\t sizeof(*crypto_info_aria_gcm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_ARIA_GCM_256: {\n\t\tstruct tls12_crypto_info_aria_gcm_256 *\n\t\t  crypto_info_aria_gcm_256 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aria_gcm_256,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aria_gcm_256)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tlock_sock(sk);\n\t\tmemcpy(crypto_info_aria_gcm_256->iv,\n\t\t       cctx->iv + TLS_CIPHER_ARIA_GCM_256_SALT_SIZE,\n\t\t       TLS_CIPHER_ARIA_GCM_256_IV_SIZE);\n\t\tmemcpy(crypto_info_aria_gcm_256->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_ARIA_GCM_256_REC_SEQ_SIZE);\n\t\trelease_sock(sk);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aria_gcm_256,\n\t\t\t\t sizeof(*crypto_info_aria_gcm_256)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tdefault:\n\t\trc = -EINVAL;\n\t}\n\nout:\n\treturn rc;\n}",
                        "code_after_change": "static int do_tls_getsockopt_conf(struct sock *sk, char __user *optval,\n\t\t\t\t  int __user *optlen, int tx)\n{\n\tint rc = 0;\n\tstruct tls_context *ctx = tls_get_ctx(sk);\n\tstruct tls_crypto_info *crypto_info;\n\tstruct cipher_context *cctx;\n\tint len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tif (!optval || (len < sizeof(*crypto_info))) {\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (!ctx) {\n\t\trc = -EBUSY;\n\t\tgoto out;\n\t}\n\n\t/* get user crypto info */\n\tif (tx) {\n\t\tcrypto_info = &ctx->crypto_send.info;\n\t\tcctx = &ctx->tx;\n\t} else {\n\t\tcrypto_info = &ctx->crypto_recv.info;\n\t\tcctx = &ctx->rx;\n\t}\n\n\tif (!TLS_CRYPTO_INFO_READY(crypto_info)) {\n\t\trc = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tif (len == sizeof(*crypto_info)) {\n\t\tif (copy_to_user(optval, crypto_info, sizeof(*crypto_info)))\n\t\t\trc = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tswitch (crypto_info->cipher_type) {\n\tcase TLS_CIPHER_AES_GCM_128: {\n\t\tstruct tls12_crypto_info_aes_gcm_128 *\n\t\t  crypto_info_aes_gcm_128 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aes_gcm_128,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aes_gcm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(crypto_info_aes_gcm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_GCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_GCM_128_IV_SIZE);\n\t\tmemcpy(crypto_info_aes_gcm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_GCM_128_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aes_gcm_128,\n\t\t\t\t sizeof(*crypto_info_aes_gcm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_AES_GCM_256: {\n\t\tstruct tls12_crypto_info_aes_gcm_256 *\n\t\t  crypto_info_aes_gcm_256 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aes_gcm_256,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aes_gcm_256)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(crypto_info_aes_gcm_256->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_GCM_256_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_GCM_256_IV_SIZE);\n\t\tmemcpy(crypto_info_aes_gcm_256->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_GCM_256_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aes_gcm_256,\n\t\t\t\t sizeof(*crypto_info_aes_gcm_256)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_AES_CCM_128: {\n\t\tstruct tls12_crypto_info_aes_ccm_128 *aes_ccm_128 =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_aes_ccm_128, info);\n\n\t\tif (len != sizeof(*aes_ccm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(aes_ccm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_AES_CCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_AES_CCM_128_IV_SIZE);\n\t\tmemcpy(aes_ccm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_AES_CCM_128_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval, aes_ccm_128, sizeof(*aes_ccm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_CHACHA20_POLY1305: {\n\t\tstruct tls12_crypto_info_chacha20_poly1305 *chacha20_poly1305 =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_chacha20_poly1305,\n\t\t\t\tinfo);\n\n\t\tif (len != sizeof(*chacha20_poly1305)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(chacha20_poly1305->iv,\n\t\t       cctx->iv + TLS_CIPHER_CHACHA20_POLY1305_SALT_SIZE,\n\t\t       TLS_CIPHER_CHACHA20_POLY1305_IV_SIZE);\n\t\tmemcpy(chacha20_poly1305->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_CHACHA20_POLY1305_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval, chacha20_poly1305,\n\t\t\t\tsizeof(*chacha20_poly1305)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_SM4_GCM: {\n\t\tstruct tls12_crypto_info_sm4_gcm *sm4_gcm_info =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_sm4_gcm, info);\n\n\t\tif (len != sizeof(*sm4_gcm_info)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(sm4_gcm_info->iv,\n\t\t       cctx->iv + TLS_CIPHER_SM4_GCM_SALT_SIZE,\n\t\t       TLS_CIPHER_SM4_GCM_IV_SIZE);\n\t\tmemcpy(sm4_gcm_info->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_SM4_GCM_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval, sm4_gcm_info, sizeof(*sm4_gcm_info)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_SM4_CCM: {\n\t\tstruct tls12_crypto_info_sm4_ccm *sm4_ccm_info =\n\t\t\tcontainer_of(crypto_info,\n\t\t\t\tstruct tls12_crypto_info_sm4_ccm, info);\n\n\t\tif (len != sizeof(*sm4_ccm_info)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(sm4_ccm_info->iv,\n\t\t       cctx->iv + TLS_CIPHER_SM4_CCM_SALT_SIZE,\n\t\t       TLS_CIPHER_SM4_CCM_IV_SIZE);\n\t\tmemcpy(sm4_ccm_info->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_SM4_CCM_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval, sm4_ccm_info, sizeof(*sm4_ccm_info)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_ARIA_GCM_128: {\n\t\tstruct tls12_crypto_info_aria_gcm_128 *\n\t\t  crypto_info_aria_gcm_128 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aria_gcm_128,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aria_gcm_128)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(crypto_info_aria_gcm_128->iv,\n\t\t       cctx->iv + TLS_CIPHER_ARIA_GCM_128_SALT_SIZE,\n\t\t       TLS_CIPHER_ARIA_GCM_128_IV_SIZE);\n\t\tmemcpy(crypto_info_aria_gcm_128->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_ARIA_GCM_128_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aria_gcm_128,\n\t\t\t\t sizeof(*crypto_info_aria_gcm_128)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_ARIA_GCM_256: {\n\t\tstruct tls12_crypto_info_aria_gcm_256 *\n\t\t  crypto_info_aria_gcm_256 =\n\t\t  container_of(crypto_info,\n\t\t\t       struct tls12_crypto_info_aria_gcm_256,\n\t\t\t       info);\n\n\t\tif (len != sizeof(*crypto_info_aria_gcm_256)) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(crypto_info_aria_gcm_256->iv,\n\t\t       cctx->iv + TLS_CIPHER_ARIA_GCM_256_SALT_SIZE,\n\t\t       TLS_CIPHER_ARIA_GCM_256_IV_SIZE);\n\t\tmemcpy(crypto_info_aria_gcm_256->rec_seq, cctx->rec_seq,\n\t\t       TLS_CIPHER_ARIA_GCM_256_REC_SEQ_SIZE);\n\t\tif (copy_to_user(optval,\n\t\t\t\t crypto_info_aria_gcm_256,\n\t\t\t\t sizeof(*crypto_info_aria_gcm_256)))\n\t\t\trc = -EFAULT;\n\t\tbreak;\n\t}\n\tdefault:\n\t\trc = -EINVAL;\n\t}\n\nout:\n\treturn rc;\n}",
                        "cve_id": "CVE-2023-28466"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 4155,
            "cve_id": "CVE-2023-3106",
            "code_snippet": "static int xfrm_dump_sa(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct xfrm_state_walk *walk = (struct xfrm_state_walk *) &cb->args[1];\n\tstruct xfrm_dump_info info;\n\n\tBUILD_BUG_ON(sizeof(struct xfrm_state_walk) >\n\t\t     sizeof(cb->args) - sizeof(cb->args[0]));\n\n\tinfo.in_skb = cb->skb;\n\tinfo.out_skb = skb;\n\tinfo.nlmsg_seq = cb->nlh->nlmsg_seq;\n\tinfo.nlmsg_flags = NLM_F_MULTI;\n\n\tif (!cb->args[0]) {\n\t\tstruct nlattr *attrs[XFRMA_MAX+1];\n\t\tstruct xfrm_address_filter *filter = NULL;\n\t\tu8 proto = 0;\n\t\tint err;\n\n\t\terr = nlmsg_parse(cb->nlh, 0, attrs, XFRMA_MAX,\n\t\t\t\t  xfrma_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\tif (attrs[XFRMA_ADDRESS_FILTER]) {\n\t\t\tfilter = kmemdup(nla_data(attrs[XFRMA_ADDRESS_FILTER]),\n\t\t\t\t\t sizeof(*filter), GFP_KERNEL);\n\t\t\tif (filter == NULL)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tif (attrs[XFRMA_PROTO])\n\t\t\tproto = nla_get_u8(attrs[XFRMA_PROTO]);\n\n\t\txfrm_state_walk_init(walk, proto, filter);\n\t\tcb->args[0] = 1;\n\t}\n\n\t(void) xfrm_state_walk(net, walk, dump_one_state, &info);\n\n\treturn skb->len;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int xfrm_dump_sa_done(struct netlink_callback *cb)\n{\n\tstruct xfrm_state_walk *walk = (struct xfrm_state_walk *) &cb->args[1];\n\tstruct sock *sk = cb->skb->sk;\n\tstruct net *net = sock_net(sk);\n\n\txfrm_state_walk_done(walk, net);\n\treturn 0;\n}",
                        "code_after_change": "static int xfrm_dump_sa_done(struct netlink_callback *cb)\n{\n\tstruct xfrm_state_walk *walk = (struct xfrm_state_walk *) &cb->args[1];\n\tstruct sock *sk = cb->skb->sk;\n\tstruct net *net = sock_net(sk);\n\n\tif (cb->args[0])\n\t\txfrm_state_walk_done(walk, net);\n\treturn 0;\n}",
                        "cve_id": "CVE-2023-3106"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 4189,
            "cve_id": "CVE-2023-32252",
            "code_snippet": "void ksmbd_conn_lock(struct ksmbd_conn *conn)\n{\n\tmutex_lock(&conn->srv_mutex);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static void ksmbd_conn_unlock(struct ksmbd_conn *conn)\n{\n\tmutex_unlock(&conn->srv_mutex);\n}",
                        "code_after_change": "void ksmbd_conn_unlock(struct ksmbd_conn *conn)\n{\n\tmutex_unlock(&conn->srv_mutex);\n}",
                        "cve_id": "CVE-2023-32252"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void destroy_previous_session(struct ksmbd_conn *conn,\n\t\t\t\t     struct ksmbd_user *user, u64 id)\n{\n\tstruct ksmbd_session *prev_sess = ksmbd_session_lookup_slowpath(id);\n\tstruct ksmbd_user *prev_user;\n\tstruct channel *chann;\n\tlong index;\n\n\tif (!prev_sess)\n\t\treturn;\n\n\tprev_user = prev_sess->user;\n\n\tif (!prev_user ||\n\t    strcmp(user->name, prev_user->name) ||\n\t    user->passkey_sz != prev_user->passkey_sz ||\n\t    memcmp(user->passkey, prev_user->passkey, user->passkey_sz))\n\t\treturn;\n\n\tprev_sess->state = SMB2_SESSION_EXPIRED;\n\txa_for_each(&prev_sess->ksmbd_chann_list, index, chann)\n\t\tchann->conn->status = KSMBD_SESS_EXITING;\n}",
                        "code_after_change": "static void destroy_previous_session(struct ksmbd_conn *conn,\n\t\t\t\t     struct ksmbd_user *user, u64 id)\n{\n\tstruct ksmbd_session *prev_sess = ksmbd_session_lookup_slowpath(id);\n\tstruct ksmbd_user *prev_user;\n\tstruct channel *chann;\n\tlong index;\n\n\tif (!prev_sess)\n\t\treturn;\n\n\tprev_user = prev_sess->user;\n\n\tif (!prev_user ||\n\t    strcmp(user->name, prev_user->name) ||\n\t    user->passkey_sz != prev_user->passkey_sz ||\n\t    memcmp(user->passkey, prev_user->passkey, user->passkey_sz))\n\t\treturn;\n\n\tprev_sess->state = SMB2_SESSION_EXPIRED;\n\txa_for_each(&prev_sess->ksmbd_chann_list, index, chann)\n\t\tksmbd_conn_set_exiting(chann->conn);\n}",
                        "cve_id": "CVE-2023-32252"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int init_smb2_neg_rsp(struct ksmbd_work *work)\n{\n\tstruct smb2_hdr *rsp_hdr;\n\tstruct smb2_negotiate_rsp *rsp;\n\tstruct ksmbd_conn *conn = work->conn;\n\n\t*(__be32 *)work->response_buf =\n\t\tcpu_to_be32(conn->vals->header_size);\n\n\trsp_hdr = smb2_get_msg(work->response_buf);\n\tmemset(rsp_hdr, 0, sizeof(struct smb2_hdr) + 2);\n\trsp_hdr->ProtocolId = SMB2_PROTO_NUMBER;\n\trsp_hdr->StructureSize = SMB2_HEADER_STRUCTURE_SIZE;\n\trsp_hdr->CreditRequest = cpu_to_le16(2);\n\trsp_hdr->Command = SMB2_NEGOTIATE;\n\trsp_hdr->Flags = (SMB2_FLAGS_SERVER_TO_REDIR);\n\trsp_hdr->NextCommand = 0;\n\trsp_hdr->MessageId = 0;\n\trsp_hdr->Id.SyncId.ProcessId = 0;\n\trsp_hdr->Id.SyncId.TreeId = 0;\n\trsp_hdr->SessionId = 0;\n\tmemset(rsp_hdr->Signature, 0, 16);\n\n\trsp = smb2_get_msg(work->response_buf);\n\n\tWARN_ON(ksmbd_conn_good(work));\n\n\trsp->StructureSize = cpu_to_le16(65);\n\tksmbd_debug(SMB, \"conn->dialect 0x%x\\n\", conn->dialect);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying connection\n\t */\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\t/* Default Max Message Size till SMB2.0, 64K*/\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\tle16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf,\n\t\t\tsizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tif (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY)\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\tconn->use_spnego = true;\n\n\tksmbd_conn_set_need_negotiate(work);\n\treturn 0;\n}",
                        "code_after_change": "int init_smb2_neg_rsp(struct ksmbd_work *work)\n{\n\tstruct smb2_hdr *rsp_hdr;\n\tstruct smb2_negotiate_rsp *rsp;\n\tstruct ksmbd_conn *conn = work->conn;\n\n\t*(__be32 *)work->response_buf =\n\t\tcpu_to_be32(conn->vals->header_size);\n\n\trsp_hdr = smb2_get_msg(work->response_buf);\n\tmemset(rsp_hdr, 0, sizeof(struct smb2_hdr) + 2);\n\trsp_hdr->ProtocolId = SMB2_PROTO_NUMBER;\n\trsp_hdr->StructureSize = SMB2_HEADER_STRUCTURE_SIZE;\n\trsp_hdr->CreditRequest = cpu_to_le16(2);\n\trsp_hdr->Command = SMB2_NEGOTIATE;\n\trsp_hdr->Flags = (SMB2_FLAGS_SERVER_TO_REDIR);\n\trsp_hdr->NextCommand = 0;\n\trsp_hdr->MessageId = 0;\n\trsp_hdr->Id.SyncId.ProcessId = 0;\n\trsp_hdr->Id.SyncId.TreeId = 0;\n\trsp_hdr->SessionId = 0;\n\tmemset(rsp_hdr->Signature, 0, 16);\n\n\trsp = smb2_get_msg(work->response_buf);\n\n\tWARN_ON(ksmbd_conn_good(conn));\n\n\trsp->StructureSize = cpu_to_le16(65);\n\tksmbd_debug(SMB, \"conn->dialect 0x%x\\n\", conn->dialect);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying connection\n\t */\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\t/* Default Max Message Size till SMB2.0, 64K*/\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\tle16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf,\n\t\t\tsizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tif (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY)\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\tconn->use_spnego = true;\n\n\tksmbd_conn_set_need_negotiate(conn);\n\treturn 0;\n}",
                        "cve_id": "CVE-2023-32252"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 4190,
            "cve_id": "CVE-2023-32252",
            "code_snippet": "bool ksmbd_conn_alive(struct ksmbd_conn *conn)\n{\n\tif (!ksmbd_server_running())\n\t\treturn false;\n\n\tif (ksmbd_conn_exiting(conn))\n\t\treturn false;\n\n\tif (kthread_should_stop())\n\t\treturn false;\n\n\tif (atomic_read(&conn->stats.open_files_count) > 0)\n\t\treturn true;\n\n\t/*\n\t * Stop current session if the time that get last request from client\n\t * is bigger than deadtime user configured and opening file count is\n\t * zero.\n\t */\n\tif (server_conf.deadtime > 0 &&\n\t    time_after(jiffies, conn->last_active + server_conf.deadtime)) {\n\t\tksmbd_debug(CONN, \"No response from client in %lu minutes\\n\",\n\t\t\t    server_conf.deadtime / SMB_ECHO_INTERVAL);\n\t\treturn false;\n\t}\n\treturn true;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int smb2_handle_negotiate(struct ksmbd_work *work)\n{\n\tstruct ksmbd_conn *conn = work->conn;\n\tstruct smb2_negotiate_req *req = smb2_get_msg(work->request_buf);\n\tstruct smb2_negotiate_rsp *rsp = smb2_get_msg(work->response_buf);\n\tint rc = 0;\n\tunsigned int smb2_buf_len, smb2_neg_size;\n\t__le32 status;\n\n\tksmbd_debug(SMB, \"Received negotiate request\\n\");\n\tconn->need_neg = false;\n\tif (ksmbd_conn_good(work)) {\n\t\tpr_err(\"conn->tcp_status is already in CifsGood State\\n\");\n\t\twork->send_no_response = 1;\n\t\treturn rc;\n\t}\n\n\tif (req->DialectCount == 0) {\n\t\tpr_err(\"malformed packet\\n\");\n\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tsmb2_buf_len = get_rfc1002_len(work->request_buf);\n\tsmb2_neg_size = offsetof(struct smb2_negotiate_req, Dialects);\n\tif (smb2_neg_size > smb2_buf_len) {\n\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tif (conn->dialect == SMB311_PROT_ID) {\n\t\tunsigned int nego_ctxt_off = le32_to_cpu(req->NegotiateContextOffset);\n\n\t\tif (smb2_buf_len < nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (smb2_neg_size > nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (smb2_neg_size + le16_to_cpu(req->DialectCount) * sizeof(__le16) >\n\t\t    nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t} else {\n\t\tif (smb2_neg_size + le16_to_cpu(req->DialectCount) * sizeof(__le16) >\n\t\t    smb2_buf_len) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\tconn->cli_cap = le32_to_cpu(req->Capabilities);\n\tswitch (conn->dialect) {\n\tcase SMB311_PROT_ID:\n\t\tconn->preauth_info =\n\t\t\tkzalloc(sizeof(struct preauth_integrity_info),\n\t\t\t\tGFP_KERNEL);\n\t\tif (!conn->preauth_info) {\n\t\t\trc = -ENOMEM;\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tstatus = deassemble_neg_contexts(conn, req,\n\t\t\t\t\t\t get_rfc1002_len(work->request_buf));\n\t\tif (status != STATUS_SUCCESS) {\n\t\t\tpr_err(\"deassemble_neg_contexts error(0x%x)\\n\",\n\t\t\t       status);\n\t\t\trsp->hdr.Status = status;\n\t\t\trc = -EINVAL;\n\t\t\tkfree(conn->preauth_info);\n\t\t\tconn->preauth_info = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\trc = init_smb3_11_server(conn);\n\t\tif (rc < 0) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\tkfree(conn->preauth_info);\n\t\t\tconn->preauth_info = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tksmbd_gen_preauth_integrity_hash(conn,\n\t\t\t\t\t\t work->request_buf,\n\t\t\t\t\t\t conn->preauth_info->Preauth_HashValue);\n\t\trsp->NegotiateContextOffset =\n\t\t\t\tcpu_to_le32(OFFSET_OF_NEG_CONTEXT);\n\t\tassemble_neg_contexts(conn, rsp, work->response_buf);\n\t\tbreak;\n\tcase SMB302_PROT_ID:\n\t\tinit_smb3_02_server(conn);\n\t\tbreak;\n\tcase SMB30_PROT_ID:\n\t\tinit_smb3_0_server(conn);\n\t\tbreak;\n\tcase SMB21_PROT_ID:\n\t\tinit_smb2_1_server(conn);\n\t\tbreak;\n\tcase SMB2X_PROT_ID:\n\tcase BAD_PROT_ID:\n\tdefault:\n\t\tksmbd_debug(SMB, \"Server dialect :0x%x not supported\\n\",\n\t\t\t    conn->dialect);\n\t\trsp->hdr.Status = STATUS_NOT_SUPPORTED;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\n\t/* For stats */\n\tconn->connection_type = conn->dialect;\n\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\tmemcpy(conn->ClientGUID, req->ClientGUID,\n\t\t\tSMB2_CLIENT_GUID_SIZE);\n\tconn->cli_sec_mode = le16_to_cpu(req->SecurityMode);\n\n\trsp->StructureSize = cpu_to_le16(65);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying server\n\t */\n\tmemset(rsp->ServerGUID, 0, SMB2_CLIENT_GUID_SIZE);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\tksmbd_debug(SMB, \"negotiate context offset %d, count %d\\n\",\n\t\t    le32_to_cpu(rsp->NegotiateContextOffset),\n\t\t    le16_to_cpu(rsp->NegotiateContextCount));\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\t\t\t  le16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf, sizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tconn->use_spnego = true;\n\n\tif ((server_conf.signing == KSMBD_CONFIG_OPT_AUTO ||\n\t     server_conf.signing == KSMBD_CONFIG_OPT_DISABLED) &&\n\t    req->SecurityMode & SMB2_NEGOTIATE_SIGNING_REQUIRED_LE)\n\t\tconn->sign = true;\n\telse if (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY) {\n\t\tserver_conf.enforced_signing = true;\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\t\tconn->sign = true;\n\t}\n\n\tconn->srv_sec_mode = le16_to_cpu(rsp->SecurityMode);\n\tksmbd_conn_set_need_negotiate(work);\n\nerr_out:\n\tif (rc < 0)\n\t\tsmb2_set_err_rsp(work);\n\n\treturn rc;\n}",
                        "code_after_change": "int smb2_handle_negotiate(struct ksmbd_work *work)\n{\n\tstruct ksmbd_conn *conn = work->conn;\n\tstruct smb2_negotiate_req *req = smb2_get_msg(work->request_buf);\n\tstruct smb2_negotiate_rsp *rsp = smb2_get_msg(work->response_buf);\n\tint rc = 0;\n\tunsigned int smb2_buf_len, smb2_neg_size;\n\t__le32 status;\n\n\tksmbd_debug(SMB, \"Received negotiate request\\n\");\n\tconn->need_neg = false;\n\tif (ksmbd_conn_good(conn)) {\n\t\tpr_err(\"conn->tcp_status is already in CifsGood State\\n\");\n\t\twork->send_no_response = 1;\n\t\treturn rc;\n\t}\n\n\tif (req->DialectCount == 0) {\n\t\tpr_err(\"malformed packet\\n\");\n\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tsmb2_buf_len = get_rfc1002_len(work->request_buf);\n\tsmb2_neg_size = offsetof(struct smb2_negotiate_req, Dialects);\n\tif (smb2_neg_size > smb2_buf_len) {\n\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tif (conn->dialect == SMB311_PROT_ID) {\n\t\tunsigned int nego_ctxt_off = le32_to_cpu(req->NegotiateContextOffset);\n\n\t\tif (smb2_buf_len < nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (smb2_neg_size > nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (smb2_neg_size + le16_to_cpu(req->DialectCount) * sizeof(__le16) >\n\t\t    nego_ctxt_off) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t} else {\n\t\tif (smb2_neg_size + le16_to_cpu(req->DialectCount) * sizeof(__le16) >\n\t\t    smb2_buf_len) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\trc = -EINVAL;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\tconn->cli_cap = le32_to_cpu(req->Capabilities);\n\tswitch (conn->dialect) {\n\tcase SMB311_PROT_ID:\n\t\tconn->preauth_info =\n\t\t\tkzalloc(sizeof(struct preauth_integrity_info),\n\t\t\t\tGFP_KERNEL);\n\t\tif (!conn->preauth_info) {\n\t\t\trc = -ENOMEM;\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tstatus = deassemble_neg_contexts(conn, req,\n\t\t\t\t\t\t get_rfc1002_len(work->request_buf));\n\t\tif (status != STATUS_SUCCESS) {\n\t\t\tpr_err(\"deassemble_neg_contexts error(0x%x)\\n\",\n\t\t\t       status);\n\t\t\trsp->hdr.Status = status;\n\t\t\trc = -EINVAL;\n\t\t\tkfree(conn->preauth_info);\n\t\t\tconn->preauth_info = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\trc = init_smb3_11_server(conn);\n\t\tif (rc < 0) {\n\t\t\trsp->hdr.Status = STATUS_INVALID_PARAMETER;\n\t\t\tkfree(conn->preauth_info);\n\t\t\tconn->preauth_info = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tksmbd_gen_preauth_integrity_hash(conn,\n\t\t\t\t\t\t work->request_buf,\n\t\t\t\t\t\t conn->preauth_info->Preauth_HashValue);\n\t\trsp->NegotiateContextOffset =\n\t\t\t\tcpu_to_le32(OFFSET_OF_NEG_CONTEXT);\n\t\tassemble_neg_contexts(conn, rsp, work->response_buf);\n\t\tbreak;\n\tcase SMB302_PROT_ID:\n\t\tinit_smb3_02_server(conn);\n\t\tbreak;\n\tcase SMB30_PROT_ID:\n\t\tinit_smb3_0_server(conn);\n\t\tbreak;\n\tcase SMB21_PROT_ID:\n\t\tinit_smb2_1_server(conn);\n\t\tbreak;\n\tcase SMB2X_PROT_ID:\n\tcase BAD_PROT_ID:\n\tdefault:\n\t\tksmbd_debug(SMB, \"Server dialect :0x%x not supported\\n\",\n\t\t\t    conn->dialect);\n\t\trsp->hdr.Status = STATUS_NOT_SUPPORTED;\n\t\trc = -EINVAL;\n\t\tgoto err_out;\n\t}\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\n\t/* For stats */\n\tconn->connection_type = conn->dialect;\n\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\tmemcpy(conn->ClientGUID, req->ClientGUID,\n\t\t\tSMB2_CLIENT_GUID_SIZE);\n\tconn->cli_sec_mode = le16_to_cpu(req->SecurityMode);\n\n\trsp->StructureSize = cpu_to_le16(65);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying server\n\t */\n\tmemset(rsp->ServerGUID, 0, SMB2_CLIENT_GUID_SIZE);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\tksmbd_debug(SMB, \"negotiate context offset %d, count %d\\n\",\n\t\t    le32_to_cpu(rsp->NegotiateContextOffset),\n\t\t    le16_to_cpu(rsp->NegotiateContextCount));\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\t\t\t  le16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf, sizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tconn->use_spnego = true;\n\n\tif ((server_conf.signing == KSMBD_CONFIG_OPT_AUTO ||\n\t     server_conf.signing == KSMBD_CONFIG_OPT_DISABLED) &&\n\t    req->SecurityMode & SMB2_NEGOTIATE_SIGNING_REQUIRED_LE)\n\t\tconn->sign = true;\n\telse if (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY) {\n\t\tserver_conf.enforced_signing = true;\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\t\tconn->sign = true;\n\t}\n\n\tconn->srv_sec_mode = le16_to_cpu(rsp->SecurityMode);\n\tksmbd_conn_set_need_negotiate(conn);\n\nerr_out:\n\tif (rc < 0)\n\t\tsmb2_set_err_rsp(work);\n\n\treturn rc;\n}",
                        "cve_id": "CVE-2023-32252"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int init_smb2_neg_rsp(struct ksmbd_work *work)\n{\n\tstruct smb2_hdr *rsp_hdr;\n\tstruct smb2_negotiate_rsp *rsp;\n\tstruct ksmbd_conn *conn = work->conn;\n\n\t*(__be32 *)work->response_buf =\n\t\tcpu_to_be32(conn->vals->header_size);\n\n\trsp_hdr = smb2_get_msg(work->response_buf);\n\tmemset(rsp_hdr, 0, sizeof(struct smb2_hdr) + 2);\n\trsp_hdr->ProtocolId = SMB2_PROTO_NUMBER;\n\trsp_hdr->StructureSize = SMB2_HEADER_STRUCTURE_SIZE;\n\trsp_hdr->CreditRequest = cpu_to_le16(2);\n\trsp_hdr->Command = SMB2_NEGOTIATE;\n\trsp_hdr->Flags = (SMB2_FLAGS_SERVER_TO_REDIR);\n\trsp_hdr->NextCommand = 0;\n\trsp_hdr->MessageId = 0;\n\trsp_hdr->Id.SyncId.ProcessId = 0;\n\trsp_hdr->Id.SyncId.TreeId = 0;\n\trsp_hdr->SessionId = 0;\n\tmemset(rsp_hdr->Signature, 0, 16);\n\n\trsp = smb2_get_msg(work->response_buf);\n\n\tWARN_ON(ksmbd_conn_good(work));\n\n\trsp->StructureSize = cpu_to_le16(65);\n\tksmbd_debug(SMB, \"conn->dialect 0x%x\\n\", conn->dialect);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying connection\n\t */\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\t/* Default Max Message Size till SMB2.0, 64K*/\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\tle16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf,\n\t\t\tsizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tif (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY)\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\tconn->use_spnego = true;\n\n\tksmbd_conn_set_need_negotiate(work);\n\treturn 0;\n}",
                        "code_after_change": "int init_smb2_neg_rsp(struct ksmbd_work *work)\n{\n\tstruct smb2_hdr *rsp_hdr;\n\tstruct smb2_negotiate_rsp *rsp;\n\tstruct ksmbd_conn *conn = work->conn;\n\n\t*(__be32 *)work->response_buf =\n\t\tcpu_to_be32(conn->vals->header_size);\n\n\trsp_hdr = smb2_get_msg(work->response_buf);\n\tmemset(rsp_hdr, 0, sizeof(struct smb2_hdr) + 2);\n\trsp_hdr->ProtocolId = SMB2_PROTO_NUMBER;\n\trsp_hdr->StructureSize = SMB2_HEADER_STRUCTURE_SIZE;\n\trsp_hdr->CreditRequest = cpu_to_le16(2);\n\trsp_hdr->Command = SMB2_NEGOTIATE;\n\trsp_hdr->Flags = (SMB2_FLAGS_SERVER_TO_REDIR);\n\trsp_hdr->NextCommand = 0;\n\trsp_hdr->MessageId = 0;\n\trsp_hdr->Id.SyncId.ProcessId = 0;\n\trsp_hdr->Id.SyncId.TreeId = 0;\n\trsp_hdr->SessionId = 0;\n\tmemset(rsp_hdr->Signature, 0, 16);\n\n\trsp = smb2_get_msg(work->response_buf);\n\n\tWARN_ON(ksmbd_conn_good(conn));\n\n\trsp->StructureSize = cpu_to_le16(65);\n\tksmbd_debug(SMB, \"conn->dialect 0x%x\\n\", conn->dialect);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying connection\n\t */\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\t/* Default Max Message Size till SMB2.0, 64K*/\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\tle16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf,\n\t\t\tsizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tif (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY)\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\tconn->use_spnego = true;\n\n\tksmbd_conn_set_need_negotiate(conn);\n\treturn 0;\n}",
                        "cve_id": "CVE-2023-32252"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int ksmbd_tcp_readv(struct tcp_transport *t, struct kvec *iov_orig,\n\t\t\t   unsigned int nr_segs, unsigned int to_read,\n\t\t\t   int max_retries)\n{\n\tint length = 0;\n\tint total_read;\n\tunsigned int segs;\n\tstruct msghdr ksmbd_msg;\n\tstruct kvec *iov;\n\tstruct ksmbd_conn *conn = KSMBD_TRANS(t)->conn;\n\n\tiov = get_conn_iovec(t, nr_segs);\n\tif (!iov)\n\t\treturn -ENOMEM;\n\n\tksmbd_msg.msg_control = NULL;\n\tksmbd_msg.msg_controllen = 0;\n\n\tfor (total_read = 0; to_read; total_read += length, to_read -= length) {\n\t\ttry_to_freeze();\n\n\t\tif (!ksmbd_conn_alive(conn)) {\n\t\t\ttotal_read = -ESHUTDOWN;\n\t\t\tbreak;\n\t\t}\n\t\tsegs = kvec_array_init(iov, iov_orig, nr_segs, total_read);\n\n\t\tlength = kernel_recvmsg(t->sock, &ksmbd_msg,\n\t\t\t\t\tiov, segs, to_read, 0);\n\n\t\tif (length == -EINTR) {\n\t\t\ttotal_read = -ESHUTDOWN;\n\t\t\tbreak;\n\t\t} else if (conn->status == KSMBD_SESS_NEED_RECONNECT) {\n\t\t\ttotal_read = -EAGAIN;\n\t\t\tbreak;\n\t\t} else if (length == -ERESTARTSYS || length == -EAGAIN) {\n\t\t\t/*\n\t\t\t * If max_retries is negative, Allow unlimited\n\t\t\t * retries to keep connection with inactive sessions.\n\t\t\t */\n\t\t\tif (max_retries == 0) {\n\t\t\t\ttotal_read = length;\n\t\t\t\tbreak;\n\t\t\t} else if (max_retries > 0) {\n\t\t\t\tmax_retries--;\n\t\t\t}\n\n\t\t\tusleep_range(1000, 2000);\n\t\t\tlength = 0;\n\t\t\tcontinue;\n\t\t} else if (length <= 0) {\n\t\t\ttotal_read = length;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn total_read;\n}",
                        "code_after_change": "static int ksmbd_tcp_readv(struct tcp_transport *t, struct kvec *iov_orig,\n\t\t\t   unsigned int nr_segs, unsigned int to_read,\n\t\t\t   int max_retries)\n{\n\tint length = 0;\n\tint total_read;\n\tunsigned int segs;\n\tstruct msghdr ksmbd_msg;\n\tstruct kvec *iov;\n\tstruct ksmbd_conn *conn = KSMBD_TRANS(t)->conn;\n\n\tiov = get_conn_iovec(t, nr_segs);\n\tif (!iov)\n\t\treturn -ENOMEM;\n\n\tksmbd_msg.msg_control = NULL;\n\tksmbd_msg.msg_controllen = 0;\n\n\tfor (total_read = 0; to_read; total_read += length, to_read -= length) {\n\t\ttry_to_freeze();\n\n\t\tif (!ksmbd_conn_alive(conn)) {\n\t\t\ttotal_read = -ESHUTDOWN;\n\t\t\tbreak;\n\t\t}\n\t\tsegs = kvec_array_init(iov, iov_orig, nr_segs, total_read);\n\n\t\tlength = kernel_recvmsg(t->sock, &ksmbd_msg,\n\t\t\t\t\tiov, segs, to_read, 0);\n\n\t\tif (length == -EINTR) {\n\t\t\ttotal_read = -ESHUTDOWN;\n\t\t\tbreak;\n\t\t} else if (ksmbd_conn_need_reconnect(conn)) {\n\t\t\ttotal_read = -EAGAIN;\n\t\t\tbreak;\n\t\t} else if (length == -ERESTARTSYS || length == -EAGAIN) {\n\t\t\t/*\n\t\t\t * If max_retries is negative, Allow unlimited\n\t\t\t * retries to keep connection with inactive sessions.\n\t\t\t */\n\t\t\tif (max_retries == 0) {\n\t\t\t\ttotal_read = length;\n\t\t\t\tbreak;\n\t\t\t} else if (max_retries > 0) {\n\t\t\t\tmax_retries--;\n\t\t\t}\n\n\t\t\tusleep_range(1000, 2000);\n\t\t\tlength = 0;\n\t\t\tcontinue;\n\t\t} else if (length <= 0) {\n\t\t\ttotal_read = length;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn total_read;\n}",
                        "cve_id": "CVE-2023-32252"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void destroy_previous_session(struct ksmbd_conn *conn,\n\t\t\t\t     struct ksmbd_user *user, u64 id)\n{\n\tstruct ksmbd_session *prev_sess = ksmbd_session_lookup_slowpath(id);\n\tstruct ksmbd_user *prev_user;\n\tstruct channel *chann;\n\tlong index;\n\n\tif (!prev_sess)\n\t\treturn;\n\n\tprev_user = prev_sess->user;\n\n\tif (!prev_user ||\n\t    strcmp(user->name, prev_user->name) ||\n\t    user->passkey_sz != prev_user->passkey_sz ||\n\t    memcmp(user->passkey, prev_user->passkey, user->passkey_sz))\n\t\treturn;\n\n\tprev_sess->state = SMB2_SESSION_EXPIRED;\n\txa_for_each(&prev_sess->ksmbd_chann_list, index, chann)\n\t\tchann->conn->status = KSMBD_SESS_EXITING;\n}",
                        "code_after_change": "static void destroy_previous_session(struct ksmbd_conn *conn,\n\t\t\t\t     struct ksmbd_user *user, u64 id)\n{\n\tstruct ksmbd_session *prev_sess = ksmbd_session_lookup_slowpath(id);\n\tstruct ksmbd_user *prev_user;\n\tstruct channel *chann;\n\tlong index;\n\n\tif (!prev_sess)\n\t\treturn;\n\n\tprev_user = prev_sess->user;\n\n\tif (!prev_user ||\n\t    strcmp(user->name, prev_user->name) ||\n\t    user->passkey_sz != prev_user->passkey_sz ||\n\t    memcmp(user->passkey, prev_user->passkey, user->passkey_sz))\n\t\treturn;\n\n\tprev_sess->state = SMB2_SESSION_EXPIRED;\n\txa_for_each(&prev_sess->ksmbd_chann_list, index, chann)\n\t\tksmbd_conn_set_exiting(chann->conn);\n}",
                        "cve_id": "CVE-2023-32252"
                    },
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int smb2_get_info_filesystem(struct ksmbd_work *work,\n\t\t\t\t    struct smb2_query_info_req *req,\n\t\t\t\t    struct smb2_query_info_rsp *rsp)\n{\n\tstruct ksmbd_session *sess = work->sess;\n\tstruct ksmbd_conn *conn = work->conn;\n\tstruct ksmbd_share_config *share = work->tcon->share_conf;\n\tint fsinfoclass = 0;\n\tstruct kstatfs stfs;\n\tstruct path path;\n\tint rc = 0, len;\n\tint fs_infoclass_size = 0;\n\n\trc = kern_path(share->path, LOOKUP_NO_SYMLINKS, &path);\n\tif (rc) {\n\t\tpr_err(\"cannot create vfs path\\n\");\n\t\treturn -EIO;\n\t}\n\n\trc = vfs_statfs(&path, &stfs);\n\tif (rc) {\n\t\tpr_err(\"cannot do stat of path %s\\n\", share->path);\n\t\tpath_put(&path);\n\t\treturn -EIO;\n\t}\n\n\tfsinfoclass = req->FileInfoClass;\n\n\tswitch (fsinfoclass) {\n\tcase FS_DEVICE_INFORMATION:\n\t{\n\t\tstruct filesystem_device_info *info;\n\n\t\tinfo = (struct filesystem_device_info *)rsp->Buffer;\n\n\t\tinfo->DeviceType = cpu_to_le32(stfs.f_type);\n\t\tinfo->DeviceCharacteristics = cpu_to_le32(0x00000020);\n\t\trsp->OutputBufferLength = cpu_to_le32(8);\n\t\tinc_rfc1001_len(work->response_buf, 8);\n\t\tfs_infoclass_size = FS_DEVICE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_ATTRIBUTE_INFORMATION:\n\t{\n\t\tstruct filesystem_attribute_info *info;\n\t\tsize_t sz;\n\n\t\tinfo = (struct filesystem_attribute_info *)rsp->Buffer;\n\t\tinfo->Attributes = cpu_to_le32(FILE_SUPPORTS_OBJECT_IDS |\n\t\t\t\t\t       FILE_PERSISTENT_ACLS |\n\t\t\t\t\t       FILE_UNICODE_ON_DISK |\n\t\t\t\t\t       FILE_CASE_PRESERVED_NAMES |\n\t\t\t\t\t       FILE_CASE_SENSITIVE_SEARCH |\n\t\t\t\t\t       FILE_SUPPORTS_BLOCK_REFCOUNTING);\n\n\t\tinfo->Attributes |= cpu_to_le32(server_conf.share_fake_fscaps);\n\n\t\tif (test_share_config_flag(work->tcon->share_conf,\n\t\t    KSMBD_SHARE_FLAG_STREAMS))\n\t\t\tinfo->Attributes |= cpu_to_le32(FILE_NAMED_STREAMS);\n\n\t\tinfo->MaxPathNameComponentLength = cpu_to_le32(stfs.f_namelen);\n\t\tlen = smbConvertToUTF16((__le16 *)info->FileSystemName,\n\t\t\t\t\t\"NTFS\", PATH_MAX, conn->local_nls, 0);\n\t\tlen = len * 2;\n\t\tinfo->FileSystemNameLen = cpu_to_le32(len);\n\t\tsz = sizeof(struct filesystem_attribute_info) - 2 + len;\n\t\trsp->OutputBufferLength = cpu_to_le32(sz);\n\t\tinc_rfc1001_len(work->response_buf, sz);\n\t\tfs_infoclass_size = FS_ATTRIBUTE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_VOLUME_INFORMATION:\n\t{\n\t\tstruct filesystem_vol_info *info;\n\t\tsize_t sz;\n\t\tunsigned int serial_crc = 0;\n\n\t\tinfo = (struct filesystem_vol_info *)(rsp->Buffer);\n\t\tinfo->VolumeCreationTime = 0;\n\t\tserial_crc = crc32_le(serial_crc, share->name,\n\t\t\t\t      strlen(share->name));\n\t\tserial_crc = crc32_le(serial_crc, share->path,\n\t\t\t\t      strlen(share->path));\n\t\tserial_crc = crc32_le(serial_crc, ksmbd_netbios_name(),\n\t\t\t\t      strlen(ksmbd_netbios_name()));\n\t\t/* Taking dummy value of serial number*/\n\t\tinfo->SerialNumber = cpu_to_le32(serial_crc);\n\t\tlen = smbConvertToUTF16((__le16 *)info->VolumeLabel,\n\t\t\t\t\tshare->name, PATH_MAX,\n\t\t\t\t\tconn->local_nls, 0);\n\t\tlen = len * 2;\n\t\tinfo->VolumeLabelSize = cpu_to_le32(len);\n\t\tinfo->Reserved = 0;\n\t\tsz = sizeof(struct filesystem_vol_info) - 2 + len;\n\t\trsp->OutputBufferLength = cpu_to_le32(sz);\n\t\tinc_rfc1001_len(work->response_buf, sz);\n\t\tfs_infoclass_size = FS_VOLUME_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_SIZE_INFORMATION:\n\t{\n\t\tstruct filesystem_info *info;\n\n\t\tinfo = (struct filesystem_info *)(rsp->Buffer);\n\t\tinfo->TotalAllocationUnits = cpu_to_le64(stfs.f_blocks);\n\t\tinfo->FreeAllocationUnits = cpu_to_le64(stfs.f_bfree);\n\t\tinfo->SectorsPerAllocationUnit = cpu_to_le32(1);\n\t\tinfo->BytesPerSector = cpu_to_le32(stfs.f_bsize);\n\t\trsp->OutputBufferLength = cpu_to_le32(24);\n\t\tinc_rfc1001_len(work->response_buf, 24);\n\t\tfs_infoclass_size = FS_SIZE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_FULL_SIZE_INFORMATION:\n\t{\n\t\tstruct smb2_fs_full_size_info *info;\n\n\t\tinfo = (struct smb2_fs_full_size_info *)(rsp->Buffer);\n\t\tinfo->TotalAllocationUnits = cpu_to_le64(stfs.f_blocks);\n\t\tinfo->CallerAvailableAllocationUnits =\n\t\t\t\t\tcpu_to_le64(stfs.f_bavail);\n\t\tinfo->ActualAvailableAllocationUnits =\n\t\t\t\t\tcpu_to_le64(stfs.f_bfree);\n\t\tinfo->SectorsPerAllocationUnit = cpu_to_le32(1);\n\t\tinfo->BytesPerSector = cpu_to_le32(stfs.f_bsize);\n\t\trsp->OutputBufferLength = cpu_to_le32(32);\n\t\tinc_rfc1001_len(work->response_buf, 32);\n\t\tfs_infoclass_size = FS_FULL_SIZE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_OBJECT_ID_INFORMATION:\n\t{\n\t\tstruct object_id_info *info;\n\n\t\tinfo = (struct object_id_info *)(rsp->Buffer);\n\n\t\tif (!user_guest(sess->user))\n\t\t\tmemcpy(info->objid, user_passkey(sess->user), 16);\n\t\telse\n\t\t\tmemset(info->objid, 0, 16);\n\n\t\tinfo->extended_info.magic = cpu_to_le32(EXTENDED_INFO_MAGIC);\n\t\tinfo->extended_info.version = cpu_to_le32(1);\n\t\tinfo->extended_info.release = cpu_to_le32(1);\n\t\tinfo->extended_info.rel_date = 0;\n\t\tmemcpy(info->extended_info.version_string, \"1.1.0\", strlen(\"1.1.0\"));\n\t\trsp->OutputBufferLength = cpu_to_le32(64);\n\t\tinc_rfc1001_len(work->response_buf, 64);\n\t\tfs_infoclass_size = FS_OBJECT_ID_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_SECTOR_SIZE_INFORMATION:\n\t{\n\t\tstruct smb3_fs_ss_info *info;\n\t\tunsigned int sector_size =\n\t\t\tmin_t(unsigned int, path.mnt->mnt_sb->s_blocksize, 4096);\n\n\t\tinfo = (struct smb3_fs_ss_info *)(rsp->Buffer);\n\n\t\tinfo->LogicalBytesPerSector = cpu_to_le32(sector_size);\n\t\tinfo->PhysicalBytesPerSectorForAtomicity =\n\t\t\t\tcpu_to_le32(sector_size);\n\t\tinfo->PhysicalBytesPerSectorForPerf = cpu_to_le32(sector_size);\n\t\tinfo->FSEffPhysicalBytesPerSectorForAtomicity =\n\t\t\t\tcpu_to_le32(sector_size);\n\t\tinfo->Flags = cpu_to_le32(SSINFO_FLAGS_ALIGNED_DEVICE |\n\t\t\t\t    SSINFO_FLAGS_PARTITION_ALIGNED_ON_DEVICE);\n\t\tinfo->ByteOffsetForSectorAlignment = 0;\n\t\tinfo->ByteOffsetForPartitionAlignment = 0;\n\t\trsp->OutputBufferLength = cpu_to_le32(28);\n\t\tinc_rfc1001_len(work->response_buf, 28);\n\t\tfs_infoclass_size = FS_SECTOR_SIZE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_CONTROL_INFORMATION:\n\t{\n\t\t/*\n\t\t * TODO : The current implementation is based on\n\t\t * test result with win7(NTFS) server. It's need to\n\t\t * modify this to get valid Quota values\n\t\t * from Linux kernel\n\t\t */\n\t\tstruct smb2_fs_control_info *info;\n\n\t\tinfo = (struct smb2_fs_control_info *)(rsp->Buffer);\n\t\tinfo->FreeSpaceStartFiltering = 0;\n\t\tinfo->FreeSpaceThreshold = 0;\n\t\tinfo->FreeSpaceStopFiltering = 0;\n\t\tinfo->DefaultQuotaThreshold = cpu_to_le64(SMB2_NO_FID);\n\t\tinfo->DefaultQuotaLimit = cpu_to_le64(SMB2_NO_FID);\n\t\tinfo->Padding = 0;\n\t\trsp->OutputBufferLength = cpu_to_le32(48);\n\t\tinc_rfc1001_len(work->response_buf, 48);\n\t\tfs_infoclass_size = FS_CONTROL_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_POSIX_INFORMATION:\n\t{\n\t\tstruct filesystem_posix_info *info;\n\n\t\tif (!work->tcon->posix_extensions) {\n\t\t\tpr_err(\"client doesn't negotiate with SMB3.1.1 POSIX Extensions\\n\");\n\t\t\trc = -EOPNOTSUPP;\n\t\t} else {\n\t\t\tinfo = (struct filesystem_posix_info *)(rsp->Buffer);\n\t\t\tinfo->OptimalTransferSize = cpu_to_le32(stfs.f_bsize);\n\t\t\tinfo->BlockSize = cpu_to_le32(stfs.f_bsize);\n\t\t\tinfo->TotalBlocks = cpu_to_le64(stfs.f_blocks);\n\t\t\tinfo->BlocksAvail = cpu_to_le64(stfs.f_bfree);\n\t\t\tinfo->UserBlocksAvail = cpu_to_le64(stfs.f_bavail);\n\t\t\tinfo->TotalFileNodes = cpu_to_le64(stfs.f_files);\n\t\t\tinfo->FreeFileNodes = cpu_to_le64(stfs.f_ffree);\n\t\t\trsp->OutputBufferLength = cpu_to_le32(56);\n\t\t\tinc_rfc1001_len(work->response_buf, 56);\n\t\t\tfs_infoclass_size = FS_POSIX_INFORMATION_SIZE;\n\t\t}\n\t\tbreak;\n\t}\n\tdefault:\n\t\tpath_put(&path);\n\t\treturn -EOPNOTSUPP;\n\t}\n\trc = buffer_check_err(le32_to_cpu(req->OutputBufferLength),\n\t\t\t      rsp, work->response_buf,\n\t\t\t      fs_infoclass_size);\n\tpath_put(&path);\n\treturn rc;\n}",
                        "code_after_change": "static int smb2_get_info_filesystem(struct ksmbd_work *work,\n\t\t\t\t    struct smb2_query_info_req *req,\n\t\t\t\t    struct smb2_query_info_rsp *rsp)\n{\n\tstruct ksmbd_session *sess = work->sess;\n\tstruct ksmbd_conn *conn = work->conn;\n\tstruct ksmbd_share_config *share = work->tcon->share_conf;\n\tint fsinfoclass = 0;\n\tstruct kstatfs stfs;\n\tstruct path path;\n\tint rc = 0, len;\n\tint fs_infoclass_size = 0;\n\n\tif (!share->path)\n\t\treturn -EIO;\n\n\trc = kern_path(share->path, LOOKUP_NO_SYMLINKS, &path);\n\tif (rc) {\n\t\tpr_err(\"cannot create vfs path\\n\");\n\t\treturn -EIO;\n\t}\n\n\trc = vfs_statfs(&path, &stfs);\n\tif (rc) {\n\t\tpr_err(\"cannot do stat of path %s\\n\", share->path);\n\t\tpath_put(&path);\n\t\treturn -EIO;\n\t}\n\n\tfsinfoclass = req->FileInfoClass;\n\n\tswitch (fsinfoclass) {\n\tcase FS_DEVICE_INFORMATION:\n\t{\n\t\tstruct filesystem_device_info *info;\n\n\t\tinfo = (struct filesystem_device_info *)rsp->Buffer;\n\n\t\tinfo->DeviceType = cpu_to_le32(stfs.f_type);\n\t\tinfo->DeviceCharacteristics = cpu_to_le32(0x00000020);\n\t\trsp->OutputBufferLength = cpu_to_le32(8);\n\t\tinc_rfc1001_len(work->response_buf, 8);\n\t\tfs_infoclass_size = FS_DEVICE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_ATTRIBUTE_INFORMATION:\n\t{\n\t\tstruct filesystem_attribute_info *info;\n\t\tsize_t sz;\n\n\t\tinfo = (struct filesystem_attribute_info *)rsp->Buffer;\n\t\tinfo->Attributes = cpu_to_le32(FILE_SUPPORTS_OBJECT_IDS |\n\t\t\t\t\t       FILE_PERSISTENT_ACLS |\n\t\t\t\t\t       FILE_UNICODE_ON_DISK |\n\t\t\t\t\t       FILE_CASE_PRESERVED_NAMES |\n\t\t\t\t\t       FILE_CASE_SENSITIVE_SEARCH |\n\t\t\t\t\t       FILE_SUPPORTS_BLOCK_REFCOUNTING);\n\n\t\tinfo->Attributes |= cpu_to_le32(server_conf.share_fake_fscaps);\n\n\t\tif (test_share_config_flag(work->tcon->share_conf,\n\t\t    KSMBD_SHARE_FLAG_STREAMS))\n\t\t\tinfo->Attributes |= cpu_to_le32(FILE_NAMED_STREAMS);\n\n\t\tinfo->MaxPathNameComponentLength = cpu_to_le32(stfs.f_namelen);\n\t\tlen = smbConvertToUTF16((__le16 *)info->FileSystemName,\n\t\t\t\t\t\"NTFS\", PATH_MAX, conn->local_nls, 0);\n\t\tlen = len * 2;\n\t\tinfo->FileSystemNameLen = cpu_to_le32(len);\n\t\tsz = sizeof(struct filesystem_attribute_info) - 2 + len;\n\t\trsp->OutputBufferLength = cpu_to_le32(sz);\n\t\tinc_rfc1001_len(work->response_buf, sz);\n\t\tfs_infoclass_size = FS_ATTRIBUTE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_VOLUME_INFORMATION:\n\t{\n\t\tstruct filesystem_vol_info *info;\n\t\tsize_t sz;\n\t\tunsigned int serial_crc = 0;\n\n\t\tinfo = (struct filesystem_vol_info *)(rsp->Buffer);\n\t\tinfo->VolumeCreationTime = 0;\n\t\tserial_crc = crc32_le(serial_crc, share->name,\n\t\t\t\t      strlen(share->name));\n\t\tserial_crc = crc32_le(serial_crc, share->path,\n\t\t\t\t      strlen(share->path));\n\t\tserial_crc = crc32_le(serial_crc, ksmbd_netbios_name(),\n\t\t\t\t      strlen(ksmbd_netbios_name()));\n\t\t/* Taking dummy value of serial number*/\n\t\tinfo->SerialNumber = cpu_to_le32(serial_crc);\n\t\tlen = smbConvertToUTF16((__le16 *)info->VolumeLabel,\n\t\t\t\t\tshare->name, PATH_MAX,\n\t\t\t\t\tconn->local_nls, 0);\n\t\tlen = len * 2;\n\t\tinfo->VolumeLabelSize = cpu_to_le32(len);\n\t\tinfo->Reserved = 0;\n\t\tsz = sizeof(struct filesystem_vol_info) - 2 + len;\n\t\trsp->OutputBufferLength = cpu_to_le32(sz);\n\t\tinc_rfc1001_len(work->response_buf, sz);\n\t\tfs_infoclass_size = FS_VOLUME_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_SIZE_INFORMATION:\n\t{\n\t\tstruct filesystem_info *info;\n\n\t\tinfo = (struct filesystem_info *)(rsp->Buffer);\n\t\tinfo->TotalAllocationUnits = cpu_to_le64(stfs.f_blocks);\n\t\tinfo->FreeAllocationUnits = cpu_to_le64(stfs.f_bfree);\n\t\tinfo->SectorsPerAllocationUnit = cpu_to_le32(1);\n\t\tinfo->BytesPerSector = cpu_to_le32(stfs.f_bsize);\n\t\trsp->OutputBufferLength = cpu_to_le32(24);\n\t\tinc_rfc1001_len(work->response_buf, 24);\n\t\tfs_infoclass_size = FS_SIZE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_FULL_SIZE_INFORMATION:\n\t{\n\t\tstruct smb2_fs_full_size_info *info;\n\n\t\tinfo = (struct smb2_fs_full_size_info *)(rsp->Buffer);\n\t\tinfo->TotalAllocationUnits = cpu_to_le64(stfs.f_blocks);\n\t\tinfo->CallerAvailableAllocationUnits =\n\t\t\t\t\tcpu_to_le64(stfs.f_bavail);\n\t\tinfo->ActualAvailableAllocationUnits =\n\t\t\t\t\tcpu_to_le64(stfs.f_bfree);\n\t\tinfo->SectorsPerAllocationUnit = cpu_to_le32(1);\n\t\tinfo->BytesPerSector = cpu_to_le32(stfs.f_bsize);\n\t\trsp->OutputBufferLength = cpu_to_le32(32);\n\t\tinc_rfc1001_len(work->response_buf, 32);\n\t\tfs_infoclass_size = FS_FULL_SIZE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_OBJECT_ID_INFORMATION:\n\t{\n\t\tstruct object_id_info *info;\n\n\t\tinfo = (struct object_id_info *)(rsp->Buffer);\n\n\t\tif (!user_guest(sess->user))\n\t\t\tmemcpy(info->objid, user_passkey(sess->user), 16);\n\t\telse\n\t\t\tmemset(info->objid, 0, 16);\n\n\t\tinfo->extended_info.magic = cpu_to_le32(EXTENDED_INFO_MAGIC);\n\t\tinfo->extended_info.version = cpu_to_le32(1);\n\t\tinfo->extended_info.release = cpu_to_le32(1);\n\t\tinfo->extended_info.rel_date = 0;\n\t\tmemcpy(info->extended_info.version_string, \"1.1.0\", strlen(\"1.1.0\"));\n\t\trsp->OutputBufferLength = cpu_to_le32(64);\n\t\tinc_rfc1001_len(work->response_buf, 64);\n\t\tfs_infoclass_size = FS_OBJECT_ID_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_SECTOR_SIZE_INFORMATION:\n\t{\n\t\tstruct smb3_fs_ss_info *info;\n\t\tunsigned int sector_size =\n\t\t\tmin_t(unsigned int, path.mnt->mnt_sb->s_blocksize, 4096);\n\n\t\tinfo = (struct smb3_fs_ss_info *)(rsp->Buffer);\n\n\t\tinfo->LogicalBytesPerSector = cpu_to_le32(sector_size);\n\t\tinfo->PhysicalBytesPerSectorForAtomicity =\n\t\t\t\tcpu_to_le32(sector_size);\n\t\tinfo->PhysicalBytesPerSectorForPerf = cpu_to_le32(sector_size);\n\t\tinfo->FSEffPhysicalBytesPerSectorForAtomicity =\n\t\t\t\tcpu_to_le32(sector_size);\n\t\tinfo->Flags = cpu_to_le32(SSINFO_FLAGS_ALIGNED_DEVICE |\n\t\t\t\t    SSINFO_FLAGS_PARTITION_ALIGNED_ON_DEVICE);\n\t\tinfo->ByteOffsetForSectorAlignment = 0;\n\t\tinfo->ByteOffsetForPartitionAlignment = 0;\n\t\trsp->OutputBufferLength = cpu_to_le32(28);\n\t\tinc_rfc1001_len(work->response_buf, 28);\n\t\tfs_infoclass_size = FS_SECTOR_SIZE_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_CONTROL_INFORMATION:\n\t{\n\t\t/*\n\t\t * TODO : The current implementation is based on\n\t\t * test result with win7(NTFS) server. It's need to\n\t\t * modify this to get valid Quota values\n\t\t * from Linux kernel\n\t\t */\n\t\tstruct smb2_fs_control_info *info;\n\n\t\tinfo = (struct smb2_fs_control_info *)(rsp->Buffer);\n\t\tinfo->FreeSpaceStartFiltering = 0;\n\t\tinfo->FreeSpaceThreshold = 0;\n\t\tinfo->FreeSpaceStopFiltering = 0;\n\t\tinfo->DefaultQuotaThreshold = cpu_to_le64(SMB2_NO_FID);\n\t\tinfo->DefaultQuotaLimit = cpu_to_le64(SMB2_NO_FID);\n\t\tinfo->Padding = 0;\n\t\trsp->OutputBufferLength = cpu_to_le32(48);\n\t\tinc_rfc1001_len(work->response_buf, 48);\n\t\tfs_infoclass_size = FS_CONTROL_INFORMATION_SIZE;\n\t\tbreak;\n\t}\n\tcase FS_POSIX_INFORMATION:\n\t{\n\t\tstruct filesystem_posix_info *info;\n\n\t\tif (!work->tcon->posix_extensions) {\n\t\t\tpr_err(\"client doesn't negotiate with SMB3.1.1 POSIX Extensions\\n\");\n\t\t\trc = -EOPNOTSUPP;\n\t\t} else {\n\t\t\tinfo = (struct filesystem_posix_info *)(rsp->Buffer);\n\t\t\tinfo->OptimalTransferSize = cpu_to_le32(stfs.f_bsize);\n\t\t\tinfo->BlockSize = cpu_to_le32(stfs.f_bsize);\n\t\t\tinfo->TotalBlocks = cpu_to_le64(stfs.f_blocks);\n\t\t\tinfo->BlocksAvail = cpu_to_le64(stfs.f_bfree);\n\t\t\tinfo->UserBlocksAvail = cpu_to_le64(stfs.f_bavail);\n\t\t\tinfo->TotalFileNodes = cpu_to_le64(stfs.f_files);\n\t\t\tinfo->FreeFileNodes = cpu_to_le64(stfs.f_ffree);\n\t\t\trsp->OutputBufferLength = cpu_to_le32(56);\n\t\t\tinc_rfc1001_len(work->response_buf, 56);\n\t\t\tfs_infoclass_size = FS_POSIX_INFORMATION_SIZE;\n\t\t}\n\t\tbreak;\n\t}\n\tdefault:\n\t\tpath_put(&path);\n\t\treturn -EOPNOTSUPP;\n\t}\n\trc = buffer_check_err(le32_to_cpu(req->OutputBufferLength),\n\t\t\t      rsp, work->response_buf,\n\t\t\t      fs_infoclass_size);\n\tpath_put(&path);\n\treturn rc;\n}",
                        "cve_id": "CVE-2023-32248"
                    },
                    "vul_output": "NO"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 0
        },
        {
            "id": 4191,
            "cve_id": "CVE-2023-32252",
            "code_snippet": "static inline int check_conn_state(struct ksmbd_work *work)\n{\n\tstruct smb_hdr *rsp_hdr;\n\n\tif (ksmbd_conn_exiting(work->conn) ||\n\t    ksmbd_conn_need_reconnect(work->conn)) {\n\t\trsp_hdr = work->response_buf;\n\t\trsp_hdr->Status.CifsError = STATUS_CONNECTION_DISCONNECTED;\n\t\treturn 1;\n\t}\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "int init_smb2_neg_rsp(struct ksmbd_work *work)\n{\n\tstruct smb2_hdr *rsp_hdr;\n\tstruct smb2_negotiate_rsp *rsp;\n\tstruct ksmbd_conn *conn = work->conn;\n\n\t*(__be32 *)work->response_buf =\n\t\tcpu_to_be32(conn->vals->header_size);\n\n\trsp_hdr = smb2_get_msg(work->response_buf);\n\tmemset(rsp_hdr, 0, sizeof(struct smb2_hdr) + 2);\n\trsp_hdr->ProtocolId = SMB2_PROTO_NUMBER;\n\trsp_hdr->StructureSize = SMB2_HEADER_STRUCTURE_SIZE;\n\trsp_hdr->CreditRequest = cpu_to_le16(2);\n\trsp_hdr->Command = SMB2_NEGOTIATE;\n\trsp_hdr->Flags = (SMB2_FLAGS_SERVER_TO_REDIR);\n\trsp_hdr->NextCommand = 0;\n\trsp_hdr->MessageId = 0;\n\trsp_hdr->Id.SyncId.ProcessId = 0;\n\trsp_hdr->Id.SyncId.TreeId = 0;\n\trsp_hdr->SessionId = 0;\n\tmemset(rsp_hdr->Signature, 0, 16);\n\n\trsp = smb2_get_msg(work->response_buf);\n\n\tWARN_ON(ksmbd_conn_good(work));\n\n\trsp->StructureSize = cpu_to_le16(65);\n\tksmbd_debug(SMB, \"conn->dialect 0x%x\\n\", conn->dialect);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying connection\n\t */\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\t/* Default Max Message Size till SMB2.0, 64K*/\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\tle16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf,\n\t\t\tsizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tif (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY)\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\tconn->use_spnego = true;\n\n\tksmbd_conn_set_need_negotiate(work);\n\treturn 0;\n}",
                        "code_after_change": "int init_smb2_neg_rsp(struct ksmbd_work *work)\n{\n\tstruct smb2_hdr *rsp_hdr;\n\tstruct smb2_negotiate_rsp *rsp;\n\tstruct ksmbd_conn *conn = work->conn;\n\n\t*(__be32 *)work->response_buf =\n\t\tcpu_to_be32(conn->vals->header_size);\n\n\trsp_hdr = smb2_get_msg(work->response_buf);\n\tmemset(rsp_hdr, 0, sizeof(struct smb2_hdr) + 2);\n\trsp_hdr->ProtocolId = SMB2_PROTO_NUMBER;\n\trsp_hdr->StructureSize = SMB2_HEADER_STRUCTURE_SIZE;\n\trsp_hdr->CreditRequest = cpu_to_le16(2);\n\trsp_hdr->Command = SMB2_NEGOTIATE;\n\trsp_hdr->Flags = (SMB2_FLAGS_SERVER_TO_REDIR);\n\trsp_hdr->NextCommand = 0;\n\trsp_hdr->MessageId = 0;\n\trsp_hdr->Id.SyncId.ProcessId = 0;\n\trsp_hdr->Id.SyncId.TreeId = 0;\n\trsp_hdr->SessionId = 0;\n\tmemset(rsp_hdr->Signature, 0, 16);\n\n\trsp = smb2_get_msg(work->response_buf);\n\n\tWARN_ON(ksmbd_conn_good(conn));\n\n\trsp->StructureSize = cpu_to_le16(65);\n\tksmbd_debug(SMB, \"conn->dialect 0x%x\\n\", conn->dialect);\n\trsp->DialectRevision = cpu_to_le16(conn->dialect);\n\t/* Not setting conn guid rsp->ServerGUID, as it\n\t * not used by client for identifying connection\n\t */\n\trsp->Capabilities = cpu_to_le32(conn->vals->capabilities);\n\t/* Default Max Message Size till SMB2.0, 64K*/\n\trsp->MaxTransactSize = cpu_to_le32(conn->vals->max_trans_size);\n\trsp->MaxReadSize = cpu_to_le32(conn->vals->max_read_size);\n\trsp->MaxWriteSize = cpu_to_le32(conn->vals->max_write_size);\n\n\trsp->SystemTime = cpu_to_le64(ksmbd_systime());\n\trsp->ServerStartTime = 0;\n\n\trsp->SecurityBufferOffset = cpu_to_le16(128);\n\trsp->SecurityBufferLength = cpu_to_le16(AUTH_GSS_LENGTH);\n\tksmbd_copy_gss_neg_header((char *)(&rsp->hdr) +\n\t\tle16_to_cpu(rsp->SecurityBufferOffset));\n\tinc_rfc1001_len(work->response_buf,\n\t\t\tsizeof(struct smb2_negotiate_rsp) -\n\t\t\tsizeof(struct smb2_hdr) + AUTH_GSS_LENGTH);\n\trsp->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED_LE;\n\tif (server_conf.signing == KSMBD_CONFIG_OPT_MANDATORY)\n\t\trsp->SecurityMode |= SMB2_NEGOTIATE_SIGNING_REQUIRED_LE;\n\tconn->use_spnego = true;\n\n\tksmbd_conn_set_need_negotiate(conn);\n\treturn 0;\n}",
                        "cve_id": "CVE-2023-32252"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        },
        {
            "id": 4226,
            "cve_id": "CVE-2023-3358",
            "code_snippet": "void ishtp_cl_release_dma_acked_mem(struct ishtp_device *dev,\n\t\t\t\t    void *msg_addr,\n\t\t\t\t    uint8_t size)\n{\n\tunsigned long\tflags;\n\tint acked_slots = (size / DMA_SLOT_SIZE)\n\t\t+ 1 * (size % DMA_SLOT_SIZE != 0);\n\tint i, j;\n\n\tif ((msg_addr - dev->ishtp_host_dma_tx_buf) % DMA_SLOT_SIZE) {\n\t\tdev_err(dev->devc, \"Bad DMA Tx ack address\\n\");\n\t\treturn;\n\t}\n\n\tif (!dev->ishtp_dma_tx_map) {\n\t\tdev_err(dev->devc, \"Fail to allocate Tx map\\n\");\n\t\treturn;\n\t}\n\n\ti = (msg_addr - dev->ishtp_host_dma_tx_buf) / DMA_SLOT_SIZE;\n\tspin_lock_irqsave(&dev->ishtp_dma_tx_lock, flags);\n\tfor (j = 0; j < acked_slots; j++) {\n\t\tif ((i + j) >= dev->ishtp_dma_num_slots ||\n\t\t\t\t\t!dev->ishtp_dma_tx_map[i+j]) {\n\t\t\t/* no such slot, or memory is already free */\n\t\t\tspin_unlock_irqrestore(&dev->ishtp_dma_tx_lock, flags);\n\t\t\tdev_err(dev->devc, \"Bad DMA Tx ack address\\n\");\n\t\t\treturn;\n\t\t}\n\t\tdev->ishtp_dma_tx_map[i+j] = 0;\n\t}\n\tspin_unlock_irqrestore(&dev->ishtp_dma_tx_lock, flags);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "void *ishtp_cl_get_dma_send_buf(struct ishtp_device *dev,\n\t\t\t\tuint32_t size)\n{\n\tunsigned long\tflags;\n\tint i, j, free;\n\t/* additional slot is needed if there is rem */\n\tint required_slots = (size / DMA_SLOT_SIZE)\n\t\t+ 1 * (size % DMA_SLOT_SIZE != 0);\n\n\tspin_lock_irqsave(&dev->ishtp_dma_tx_lock, flags);\n\tfor (i = 0; i <= (dev->ishtp_dma_num_slots - required_slots); i++) {\n\t\tfree = 1;\n\t\tfor (j = 0; j < required_slots; j++)\n\t\t\tif (dev->ishtp_dma_tx_map[i+j]) {\n\t\t\t\tfree = 0;\n\t\t\t\ti += j;\n\t\t\t\tbreak;\n\t\t\t}\n\t\tif (free) {\n\t\t\t/* mark memory as \"caught\" */\n\t\t\tfor (j = 0; j < required_slots; j++)\n\t\t\t\tdev->ishtp_dma_tx_map[i+j] = 1;\n\t\t\tspin_unlock_irqrestore(&dev->ishtp_dma_tx_lock, flags);\n\t\t\treturn (i * DMA_SLOT_SIZE) +\n\t\t\t\t(unsigned char *)dev->ishtp_host_dma_tx_buf;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&dev->ishtp_dma_tx_lock, flags);\n\tdev_err(dev->devc, \"No free DMA buffer to send msg\\n\");\n\treturn NULL;\n}",
                        "code_after_change": "void *ishtp_cl_get_dma_send_buf(struct ishtp_device *dev,\n\t\t\t\tuint32_t size)\n{\n\tunsigned long\tflags;\n\tint i, j, free;\n\t/* additional slot is needed if there is rem */\n\tint required_slots = (size / DMA_SLOT_SIZE)\n\t\t+ 1 * (size % DMA_SLOT_SIZE != 0);\n\n\tif (!dev->ishtp_dma_tx_map) {\n\t\tdev_err(dev->devc, \"Fail to allocate Tx map\\n\");\n\t\treturn NULL;\n\t}\n\n\tspin_lock_irqsave(&dev->ishtp_dma_tx_lock, flags);\n\tfor (i = 0; i <= (dev->ishtp_dma_num_slots - required_slots); i++) {\n\t\tfree = 1;\n\t\tfor (j = 0; j < required_slots; j++)\n\t\t\tif (dev->ishtp_dma_tx_map[i+j]) {\n\t\t\t\tfree = 0;\n\t\t\t\ti += j;\n\t\t\t\tbreak;\n\t\t\t}\n\t\tif (free) {\n\t\t\t/* mark memory as \"caught\" */\n\t\t\tfor (j = 0; j < required_slots; j++)\n\t\t\t\tdev->ishtp_dma_tx_map[i+j] = 1;\n\t\t\tspin_unlock_irqrestore(&dev->ishtp_dma_tx_lock, flags);\n\t\t\treturn (i * DMA_SLOT_SIZE) +\n\t\t\t\t(unsigned char *)dev->ishtp_host_dma_tx_buf;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&dev->ishtp_dma_tx_lock, flags);\n\tdev_err(dev->devc, \"No free DMA buffer to send msg\\n\");\n\treturn NULL;\n}",
                        "cve_id": "CVE-2023-3358"
                    },
                    "vul_output": "YES"
                }
            ],
            "detection_model": "gpt-4o-mini",
            "final_result": 1
        }
    ]
}