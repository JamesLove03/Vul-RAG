{
    "vul_data": [
        {
            "id": 995,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "struct ipv6_txoptions *\nipv6_dup_options(struct sock *sk, struct ipv6_txoptions *opt)\n{\n\tstruct ipv6_txoptions *opt2;\n\n\topt2 = sock_kmalloc(sk, opt->tot_len, GFP_ATOMIC);\n\tif (opt2) {\n\t\tlong dif = (char *)opt2 - (char *)opt;\n\t\tmemcpy(opt2, opt, opt->tot_len);\n\t\tif (opt2->hopopt)\n\t\t\t*((char **)&opt2->hopopt) += dif;\n\t\tif (opt2->dst0opt)\n\t\t\t*((char **)&opt2->dst0opt) += dif;\n\t\tif (opt2->dst1opt)\n\t\t\t*((char **)&opt2->dst1opt) += dif;\n\t\tif (opt2->srcrt)\n\t\t\t*((char **)&opt2->srcrt) += dif;\n\t}\n\treturn opt2;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "struct ipv6_txoptions *\nipv6_renew_options(struct sock *sk, struct ipv6_txoptions *opt,\n\t\t   int newtype,\n\t\t   struct ipv6_opt_hdr __user *newopt, int newoptlen)\n{\n\tint tot_len = 0;\n\tchar *p;\n\tstruct ipv6_txoptions *opt2;\n\tint err;\n\n\tif (opt) {\n\t\tif (newtype != IPV6_HOPOPTS && opt->hopopt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->hopopt));\n\t\tif (newtype != IPV6_RTHDRDSTOPTS && opt->dst0opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst0opt));\n\t\tif (newtype != IPV6_RTHDR && opt->srcrt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->srcrt));\n\t\tif (newtype != IPV6_DSTOPTS && opt->dst1opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst1opt));\n\t}\n\n\tif (newopt && newoptlen)\n\t\ttot_len += CMSG_ALIGN(newoptlen);\n\n\tif (!tot_len)\n\t\treturn NULL;\n\n\ttot_len += sizeof(*opt2);\n\topt2 = sock_kmalloc(sk, tot_len, GFP_ATOMIC);\n\tif (!opt2)\n\t\treturn ERR_PTR(-ENOBUFS);\n\n\tmemset(opt2, 0, tot_len);\n\n\topt2->tot_len = tot_len;\n\tp = (char *)(opt2 + 1);\n\n\terr = ipv6_renew_option(opt ? opt->hopopt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_HOPOPTS,\n\t\t\t\t&opt2->hopopt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst0opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDRDSTOPTS,\n\t\t\t\t&opt2->dst0opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->srcrt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDR,\n\t\t\t\t(struct ipv6_opt_hdr **)&opt2->srcrt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst1opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_DSTOPTS,\n\t\t\t\t&opt2->dst1opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\topt2->opt_nflen = (opt2->hopopt ? ipv6_optlen(opt2->hopopt) : 0) +\n\t\t\t  (opt2->dst0opt ? ipv6_optlen(opt2->dst0opt) : 0) +\n\t\t\t  (opt2->srcrt ? ipv6_optlen(opt2->srcrt) : 0);\n\topt2->opt_flen = (opt2->dst1opt ? ipv6_optlen(opt2->dst1opt) : 0);\n\n\treturn opt2;\nout:\n\tsock_kfree_s(sk, opt2, opt2->tot_len);\n\treturn ERR_PTR(err);\n}",
                        "code_after_change": "struct ipv6_txoptions *\nipv6_renew_options(struct sock *sk, struct ipv6_txoptions *opt,\n\t\t   int newtype,\n\t\t   struct ipv6_opt_hdr __user *newopt, int newoptlen)\n{\n\tint tot_len = 0;\n\tchar *p;\n\tstruct ipv6_txoptions *opt2;\n\tint err;\n\n\tif (opt) {\n\t\tif (newtype != IPV6_HOPOPTS && opt->hopopt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->hopopt));\n\t\tif (newtype != IPV6_RTHDRDSTOPTS && opt->dst0opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst0opt));\n\t\tif (newtype != IPV6_RTHDR && opt->srcrt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->srcrt));\n\t\tif (newtype != IPV6_DSTOPTS && opt->dst1opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst1opt));\n\t}\n\n\tif (newopt && newoptlen)\n\t\ttot_len += CMSG_ALIGN(newoptlen);\n\n\tif (!tot_len)\n\t\treturn NULL;\n\n\ttot_len += sizeof(*opt2);\n\topt2 = sock_kmalloc(sk, tot_len, GFP_ATOMIC);\n\tif (!opt2)\n\t\treturn ERR_PTR(-ENOBUFS);\n\n\tmemset(opt2, 0, tot_len);\n\tatomic_set(&opt2->refcnt, 1);\n\topt2->tot_len = tot_len;\n\tp = (char *)(opt2 + 1);\n\n\terr = ipv6_renew_option(opt ? opt->hopopt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_HOPOPTS,\n\t\t\t\t&opt2->hopopt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst0opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDRDSTOPTS,\n\t\t\t\t&opt2->dst0opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->srcrt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDR,\n\t\t\t\t(struct ipv6_opt_hdr **)&opt2->srcrt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst1opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_DSTOPTS,\n\t\t\t\t&opt2->dst1opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\topt2->opt_nflen = (opt2->hopopt ? ipv6_optlen(opt2->hopopt) : 0) +\n\t\t\t  (opt2->dst0opt ? ipv6_optlen(opt2->dst0opt) : 0) +\n\t\t\t  (opt2->srcrt ? ipv6_optlen(opt2->srcrt) : 0);\n\topt2->opt_flen = (opt2->dst1opt ? ipv6_optlen(opt2->dst1opt) : 0);\n\n\treturn opt2;\nout:\n\tsock_kfree_s(sk, opt2, opt2->tot_len);\n\treturn ERR_PTR(err);\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstruct ipv6_txoptions *\nipv6_dup_options(struct sock *sk, struct ipv6_txoptions *opt)\n{\n\tstruct ipv6_txoptions *opt2;\n\n\topt2 = sock_kmalloc(sk, opt->tot_len, GFP_ATOMIC);\n\tif (opt2) {\n\t\tlong dif = (char *)opt2 - (char *)opt;\n\t\tmemcpy(opt2, opt, opt->tot_len);\n\t\tif (opt2->hopopt)\n\t\t\t*((char **)&opt2->hopopt) += dif;\n\t\tif (opt2->dst0opt)\n\t\t\t*((char **)&opt2->dst0opt) += dif;\n\t\tif (opt2->dst1opt)\n\t\t\t*((char **)&opt2->dst1opt) += dif;\n\t\tif (opt2->srcrt)\n\t\t\t*((char **)&opt2->srcrt) += dif;\n\t}\n\treturn opt2;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstruct ipv6_txoptions *\nipv6_renew_options(struct sock *sk, struct ipv6_txoptions *opt,\n\t\t   int newtype,\n\t\t   struct ipv6_opt_hdr __user *newopt, int newoptlen)\n{\n\tint tot_len = 0;\n\tchar *p;\n\tstruct ipv6_txoptions *opt2;\n\tint err;\n\n\tif (opt) {\n\t\tif (newtype != IPV6_HOPOPTS && opt->hopopt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->hopopt));\n\t\tif (newtype != IPV6_RTHDRDSTOPTS && opt->dst0opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst0opt));\n\t\tif (newtype != IPV6_RTHDR && opt->srcrt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->srcrt));\n\t\tif (newtype != IPV6_DSTOPTS && opt->dst1opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst1opt));\n\t}\n\n\tif (newopt && newoptlen)\n\t\ttot_len += CMSG_ALIGN(newoptlen);\n\n\tif (!tot_len)\n\t\treturn NULL;\n\n\ttot_len += sizeof(*opt2);\n\topt2 = sock_kmalloc(sk, tot_len, GFP_ATOMIC);\n\tif (!opt2)\n\t\treturn ERR_PTR(-ENOBUFS);\n\n\tmemset(opt2, 0, tot_len);\n\n\topt2->tot_len = tot_len;\n\tp = (char *)(opt2 + 1);\n\n\terr = ipv6_renew_option(opt ? opt->hopopt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_HOPOPTS,\n\t\t\t\t&opt2->hopopt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst0opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDRDSTOPTS,\n\t\t\t\t&opt2->dst0opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->srcrt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDR,\n\t\t\t\t(struct ipv6_opt_hdr **)&opt2->srcrt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst1opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_DSTOPTS,\n\t\t\t\t&opt2->dst1opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\topt2->opt_nflen = (opt2->hopopt ? ipv6_optlen(opt2->hopopt) : 0) +\n\t\t\t  (opt2->dst0opt ? ipv6_optlen(opt2->dst0opt) : 0) +\n\t\t\t  (opt2->srcrt ? ipv6_optlen(opt2->srcrt) : 0);\n\topt2->opt_flen = (opt2->dst1opt ? ipv6_optlen(opt2->dst1opt) : 0);\n\n\treturn opt2;\nout:\n\tsock_kfree_s(sk, opt2, opt2->tot_len);\n\treturn ERR_PTR(err);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstruct ipv6_txoptions *\nipv6_renew_options(struct sock *sk, struct ipv6_txoptions *opt,\n\t\t   int newtype,\n\t\t   struct ipv6_opt_hdr __user *newopt, int newoptlen)\n{\n\tint tot_len = 0;\n\tchar *p;\n\tstruct ipv6_txoptions *opt2;\n\tint err;\n\n\tif (opt) {\n\t\tif (newtype != IPV6_HOPOPTS && opt->hopopt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->hopopt));\n\t\tif (newtype != IPV6_RTHDRDSTOPTS && opt->dst0opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst0opt));\n\t\tif (newtype != IPV6_RTHDR && opt->srcrt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->srcrt));\n\t\tif (newtype != IPV6_DSTOPTS && opt->dst1opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst1opt));\n\t}\n\n\tif (newopt && newoptlen)\n\t\ttot_len += CMSG_ALIGN(newoptlen);\n\n\tif (!tot_len)\n\t\treturn NULL;\n\n\ttot_len += sizeof(*opt2);\n\topt2 = sock_kmalloc(sk, tot_len, GFP_ATOMIC);\n\tif (!opt2)\n\t\treturn ERR_PTR(-ENOBUFS);\n\n\tmemset(opt2, 0, tot_len);\n\tatomic_set(&opt2->refcnt, 1);\n\topt2->tot_len = tot_len;\n\tp = (char *)(opt2 + 1);\n\n\terr = ipv6_renew_option(opt ? opt->hopopt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_HOPOPTS,\n\t\t\t\t&opt2->hopopt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst0opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDRDSTOPTS,\n\t\t\t\t&opt2->dst0opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->srcrt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDR,\n\t\t\t\t(struct ipv6_opt_hdr **)&opt2->srcrt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst1opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_DSTOPTS,\n\t\t\t\t&opt2->dst1opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\topt2->opt_nflen = (opt2->hopopt ? ipv6_optlen(opt2->hopopt) : 0) +\n\t\t\t  (opt2->dst0opt ? ipv6_optlen(opt2->dst0opt) : 0) +\n\t\t\t  (opt2->srcrt ? ipv6_optlen(opt2->srcrt) : 0);\n\topt2->opt_flen = (opt2->dst1opt ? ipv6_optlen(opt2->dst1opt) : 0);\n\n\treturn opt2;\nout:\n\tsock_kfree_s(sk, opt2, opt2->tot_len);\n\treturn ERR_PTR(err);\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "void inet6_destroy_sock(struct sock *sk)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct ipv6_txoptions *opt;\n\n\t/* Release rx options */\n\n\tskb = xchg(&np->pktoptions, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\tskb = xchg(&np->rxpmtu, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\t/* Free flowlabels */\n\tfl6_free_socklist(sk);\n\n\t/* Free tx options */\n\n\topt = xchg(&np->opt, NULL);\n\tif (opt)\n\t\tsock_kfree_s(sk, opt, opt->tot_len);\n}",
                        "code_after_change": "void inet6_destroy_sock(struct sock *sk)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct ipv6_txoptions *opt;\n\n\t/* Release rx options */\n\n\tskb = xchg(&np->pktoptions, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\tskb = xchg(&np->rxpmtu, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\t/* Free flowlabels */\n\tfl6_free_socklist(sk);\n\n\t/* Free tx options */\n\n\topt = xchg((__force struct ipv6_txoptions **)&np->opt, NULL);\n\tif (opt) {\n\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\ttxopt_put(opt);\n\t}\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstruct ipv6_txoptions *\nipv6_dup_options(struct sock *sk, struct ipv6_txoptions *opt)\n{\n\tstruct ipv6_txoptions *opt2;\n\n\topt2 = sock_kmalloc(sk, opt->tot_len, GFP_ATOMIC);\n\tif (opt2) {\n\t\tlong dif = (char *)opt2 - (char *)opt;\n\t\tmemcpy(opt2, opt, opt->tot_len);\n\t\tif (opt2->hopopt)\n\t\t\t*((char **)&opt2->hopopt) += dif;\n\t\tif (opt2->dst0opt)\n\t\t\t*((char **)&opt2->dst0opt) += dif;\n\t\tif (opt2->dst1opt)\n\t\t\t*((char **)&opt2->dst1opt) += dif;\n\t\tif (opt2->srcrt)\n\t\t\t*((char **)&opt2->srcrt) += dif;\n\t}\n\treturn opt2;\n}\n'''\nRelated vulnerability code snippet:\n'''\nvoid inet6_destroy_sock(struct sock *sk)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct ipv6_txoptions *opt;\n\n\t/* Release rx options */\n\n\tskb = xchg(&np->pktoptions, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\tskb = xchg(&np->rxpmtu, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\t/* Free flowlabels */\n\tfl6_free_socklist(sk);\n\n\t/* Free tx options */\n\n\topt = xchg(&np->opt, NULL);\n\tif (opt)\n\t\tsock_kfree_s(sk, opt, opt->tot_len);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nvoid inet6_destroy_sock(struct sock *sk)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct ipv6_txoptions *opt;\n\n\t/* Release rx options */\n\n\tskb = xchg(&np->pktoptions, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\tskb = xchg(&np->rxpmtu, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\t/* Free flowlabels */\n\tfl6_free_socklist(sk);\n\n\t/* Free tx options */\n\n\topt = xchg((__force struct ipv6_txoptions **)&np->opt, NULL);\n\tif (opt) {\n\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\ttxopt_put(opt);\n\t}\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "YES"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 1
        },
        {
            "id": 1004,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_options_received tcp_opt;\n\tstruct inet_request_sock *ireq;\n\tstruct tcp_request_sock *treq;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\t__u32 cookie = ntohl(th->ack_seq) - 1;\n\tstruct sock *ret = sk;\n\tstruct request_sock *req;\n\tint mss;\n\tstruct dst_entry *dst;\n\t__u8 rcv_wscale;\n\n\tif (!sysctl_tcp_syncookies || !th->ack || th->rst)\n\t\tgoto out;\n\n\tif (tcp_synq_no_recent_overflow(sk))\n\t\tgoto out;\n\n\tmss = __cookie_v6_check(ipv6_hdr(skb), th, cookie);\n\tif (mss == 0) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESFAILED);\n\t\tgoto out;\n\t}\n\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESRECV);\n\n\t/* check for timestamp cookie support */\n\tmemset(&tcp_opt, 0, sizeof(tcp_opt));\n\ttcp_parse_options(skb, &tcp_opt, 0, NULL);\n\n\tif (!cookie_timestamp_decode(&tcp_opt))\n\t\tgoto out;\n\n\tret = NULL;\n\treq = inet_reqsk_alloc(&tcp6_request_sock_ops, sk, false);\n\tif (!req)\n\t\tgoto out;\n\n\tireq = inet_rsk(req);\n\ttreq = tcp_rsk(req);\n\ttreq->tfo_listener = false;\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\tgoto out_free;\n\n\treq->mss = mss;\n\tireq->ir_rmt_port = th->source;\n\tireq->ir_num = ntohs(th->dest);\n\tireq->ir_v6_rmt_addr = ipv6_hdr(skb)->saddr;\n\tireq->ir_v6_loc_addr = ipv6_hdr(skb)->daddr;\n\tif (ipv6_opt_accepted(sk, skb, &TCP_SKB_CB(skb)->header.h6) ||\n\t    np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo ||\n\t    np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) {\n\t\tatomic_inc(&skb->users);\n\t\tireq->pktopts = skb;\n\t}\n\n\tireq->ir_iif = sk->sk_bound_dev_if;\n\t/* So that link locals have meaning */\n\tif (!sk->sk_bound_dev_if &&\n\t    ipv6_addr_type(&ireq->ir_v6_rmt_addr) & IPV6_ADDR_LINKLOCAL)\n\t\tireq->ir_iif = tcp_v6_iif(skb);\n\n\tireq->ir_mark = inet_request_mark(sk, skb);\n\n\treq->num_retrans = 0;\n\tireq->snd_wscale\t= tcp_opt.snd_wscale;\n\tireq->sack_ok\t\t= tcp_opt.sack_ok;\n\tireq->wscale_ok\t\t= tcp_opt.wscale_ok;\n\tireq->tstamp_ok\t\t= tcp_opt.saw_tstamp;\n\treq->ts_recent\t\t= tcp_opt.saw_tstamp ? tcp_opt.rcv_tsval : 0;\n\ttreq->snt_synack.v64\t= 0;\n\ttreq->rcv_isn = ntohl(th->seq) - 1;\n\ttreq->snt_isn = cookie;\n\n\t/*\n\t * We need to lookup the dst_entry to get the correct window size.\n\t * This is taken from tcp_v6_syn_recv_sock.  Somebody please enlighten\n\t * me if there is a preferred way.\n\t */\n\t{\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = IPPROTO_TCP;\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\t\tfl6.saddr = ireq->ir_v6_loc_addr;\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = ireq->ir_mark;\n\t\tfl6.fl6_dport = ireq->ir_rmt_port;\n\t\tfl6.fl6_sport = inet_sk(sk)->inet_sport;\n\t\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\t\tif (IS_ERR(dst))\n\t\t\tgoto out_free;\n\t}\n\n\treq->rsk_window_clamp = tp->window_clamp ? :dst_metric(dst, RTAX_WINDOW);\n\ttcp_select_initial_window(tcp_full_space(sk), req->mss,\n\t\t\t\t  &req->rsk_rcv_wnd, &req->rsk_window_clamp,\n\t\t\t\t  ireq->wscale_ok, &rcv_wscale,\n\t\t\t\t  dst_metric(dst, RTAX_INITRWND));\n\n\tireq->rcv_wscale = rcv_wscale;\n\tireq->ecn_ok = cookie_ecn_ok(&tcp_opt, sock_net(sk), dst);\n\n\tret = tcp_get_cookie_sock(sk, skb, req, dst);\nout:\n\treturn ret;\nout_free:\n\treqsk_free(req);\n\treturn NULL;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dccp_v6_send_response(const struct sock *sk, struct request_sock *req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct in6_addr *final_p, final;\n\tstruct flowi6 fl6;\n\tint err = -1;\n\tstruct dst_entry *dst;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\tfl6.saddr = ireq->ir_v6_loc_addr;\n\tfl6.flowlabel = 0;\n\tfl6.flowi6_oif = ireq->ir_iif;\n\tfl6.fl6_dport = ireq->ir_rmt_port;\n\tfl6.fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto done;\n\t}\n\n\tskb = dccp_make_response(sk, dst, req);\n\tif (skb != NULL) {\n\t\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\t\tdh->dccph_checksum = dccp_v6_csum_finish(skb,\n\t\t\t\t\t\t\t &ireq->ir_v6_loc_addr,\n\t\t\t\t\t\t\t &ireq->ir_v6_rmt_addr);\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\terr = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);\n\t\terr = net_xmit_eval(err);\n\t}\n\ndone:\n\tdst_release(dst);\n\treturn err;\n}",
                        "code_after_change": "static int dccp_v6_send_response(const struct sock *sk, struct request_sock *req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct in6_addr *final_p, final;\n\tstruct flowi6 fl6;\n\tint err = -1;\n\tstruct dst_entry *dst;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\tfl6.saddr = ireq->ir_v6_loc_addr;\n\tfl6.flowlabel = 0;\n\tfl6.flowi6_oif = ireq->ir_iif;\n\tfl6.fl6_dport = ireq->ir_rmt_port;\n\tfl6.fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\n\trcu_read_lock();\n\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);\n\trcu_read_unlock();\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto done;\n\t}\n\n\tskb = dccp_make_response(sk, dst, req);\n\tif (skb != NULL) {\n\t\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\t\tdh->dccph_checksum = dccp_v6_csum_finish(skb,\n\t\t\t\t\t\t\t &ireq->ir_v6_loc_addr,\n\t\t\t\t\t\t\t &ireq->ir_v6_rmt_addr);\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\trcu_read_lock();\n\t\terr = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),\n\t\t\t       np->tclass);\n\t\trcu_read_unlock();\n\t\terr = net_xmit_eval(err);\n\t}\n\ndone:\n\tdst_release(dst);\n\treturn err;\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstruct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_options_received tcp_opt;\n\tstruct inet_request_sock *ireq;\n\tstruct tcp_request_sock *treq;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\t__u32 cookie = ntohl(th->ack_seq) - 1;\n\tstruct sock *ret = sk;\n\tstruct request_sock *req;\n\tint mss;\n\tstruct dst_entry *dst;\n\t__u8 rcv_wscale;\n\n\tif (!sysctl_tcp_syncookies || !th->ack || th->rst)\n\t\tgoto out;\n\n\tif (tcp_synq_no_recent_overflow(sk))\n\t\tgoto out;\n\n\tmss = __cookie_v6_check(ipv6_hdr(skb), th, cookie);\n\tif (mss == 0) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESFAILED);\n\t\tgoto out;\n\t}\n\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESRECV);\n\n\t/* check for timestamp cookie support */\n\tmemset(&tcp_opt, 0, sizeof(tcp_opt));\n\ttcp_parse_options(skb, &tcp_opt, 0, NULL);\n\n\tif (!cookie_timestamp_decode(&tcp_opt))\n\t\tgoto out;\n\n\tret = NULL;\n\treq = inet_reqsk_alloc(&tcp6_request_sock_ops, sk, false);\n\tif (!req)\n\t\tgoto out;\n\n\tireq = inet_rsk(req);\n\ttreq = tcp_rsk(req);\n\ttreq->tfo_listener = false;\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\tgoto out_free;\n\n\treq->mss = mss;\n\tireq->ir_rmt_port = th->source;\n\tireq->ir_num = ntohs(th->dest);\n\tireq->ir_v6_rmt_addr = ipv6_hdr(skb)->saddr;\n\tireq->ir_v6_loc_addr = ipv6_hdr(skb)->daddr;\n\tif (ipv6_opt_accepted(sk, skb, &TCP_SKB_CB(skb)->header.h6) ||\n\t    np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo ||\n\t    np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) {\n\t\tatomic_inc(&skb->users);\n\t\tireq->pktopts = skb;\n\t}\n\n\tireq->ir_iif = sk->sk_bound_dev_if;\n\t/* So that link locals have meaning */\n\tif (!sk->sk_bound_dev_if &&\n\t    ipv6_addr_type(&ireq->ir_v6_rmt_addr) & IPV6_ADDR_LINKLOCAL)\n\t\tireq->ir_iif = tcp_v6_iif(skb);\n\n\tireq->ir_mark = inet_request_mark(sk, skb);\n\n\treq->num_retrans = 0;\n\tireq->snd_wscale\t= tcp_opt.snd_wscale;\n\tireq->sack_ok\t\t= tcp_opt.sack_ok;\n\tireq->wscale_ok\t\t= tcp_opt.wscale_ok;\n\tireq->tstamp_ok\t\t= tcp_opt.saw_tstamp;\n\treq->ts_recent\t\t= tcp_opt.saw_tstamp ? tcp_opt.rcv_tsval : 0;\n\ttreq->snt_synack.v64\t= 0;\n\ttreq->rcv_isn = ntohl(th->seq) - 1;\n\ttreq->snt_isn = cookie;\n\n\t/*\n\t * We need to lookup the dst_entry to get the correct window size.\n\t * This is taken from tcp_v6_syn_recv_sock.  Somebody please enlighten\n\t * me if there is a preferred way.\n\t */\n\t{\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = IPPROTO_TCP;\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\t\tfl6.saddr = ireq->ir_v6_loc_addr;\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = ireq->ir_mark;\n\t\tfl6.fl6_dport = ireq->ir_rmt_port;\n\t\tfl6.fl6_sport = inet_sk(sk)->inet_sport;\n\t\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\t\tif (IS_ERR(dst))\n\t\t\tgoto out_free;\n\t}\n\n\treq->rsk_window_clamp = tp->window_clamp ? :dst_metric(dst, RTAX_WINDOW);\n\ttcp_select_initial_window(tcp_full_space(sk), req->mss,\n\t\t\t\t  &req->rsk_rcv_wnd, &req->rsk_window_clamp,\n\t\t\t\t  ireq->wscale_ok, &rcv_wscale,\n\t\t\t\t  dst_metric(dst, RTAX_INITRWND));\n\n\tireq->rcv_wscale = rcv_wscale;\n\tireq->ecn_ok = cookie_ecn_ok(&tcp_opt, sock_net(sk), dst);\n\n\tret = tcp_get_cookie_sock(sk, skb, req, dst);\nout:\n\treturn ret;\nout_free:\n\treqsk_free(req);\n\treturn NULL;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dccp_v6_send_response(const struct sock *sk, struct request_sock *req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct in6_addr *final_p, final;\n\tstruct flowi6 fl6;\n\tint err = -1;\n\tstruct dst_entry *dst;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\tfl6.saddr = ireq->ir_v6_loc_addr;\n\tfl6.flowlabel = 0;\n\tfl6.flowi6_oif = ireq->ir_iif;\n\tfl6.fl6_dport = ireq->ir_rmt_port;\n\tfl6.fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto done;\n\t}\n\n\tskb = dccp_make_response(sk, dst, req);\n\tif (skb != NULL) {\n\t\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\t\tdh->dccph_checksum = dccp_v6_csum_finish(skb,\n\t\t\t\t\t\t\t &ireq->ir_v6_loc_addr,\n\t\t\t\t\t\t\t &ireq->ir_v6_rmt_addr);\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\terr = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);\n\t\terr = net_xmit_eval(err);\n\t}\n\ndone:\n\tdst_release(dst);\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dccp_v6_send_response(const struct sock *sk, struct request_sock *req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct in6_addr *final_p, final;\n\tstruct flowi6 fl6;\n\tint err = -1;\n\tstruct dst_entry *dst;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\tfl6.saddr = ireq->ir_v6_loc_addr;\n\tfl6.flowlabel = 0;\n\tfl6.flowi6_oif = ireq->ir_iif;\n\tfl6.fl6_dport = ireq->ir_rmt_port;\n\tfl6.fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\n\trcu_read_lock();\n\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);\n\trcu_read_unlock();\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto done;\n\t}\n\n\tskb = dccp_make_response(sk, dst, req);\n\tif (skb != NULL) {\n\t\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\t\tdh->dccph_checksum = dccp_v6_csum_finish(skb,\n\t\t\t\t\t\t\t &ireq->ir_v6_loc_addr,\n\t\t\t\t\t\t\t &ireq->ir_v6_rmt_addr);\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\trcu_read_lock();\n\t\terr = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),\n\t\t\t       np->tclass);\n\t\trcu_read_unlock();\n\t\terr = net_xmit_eval(err);\n\t}\n\ndone:\n\tdst_release(dst);\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "struct dst_entry *inet6_csk_route_req(const struct sock *sk,\n\t\t\t\t      struct flowi6 *fl6,\n\t\t\t\t      const struct request_sock *req,\n\t\t\t\t      u8 proto)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = proto;\n\tfl6->daddr = ireq->ir_v6_rmt_addr;\n\tfinal_p = fl6_update_dst(fl6, np->opt, &final);\n\tfl6->saddr = ireq->ir_v6_loc_addr;\n\tfl6->flowi6_oif = ireq->ir_iif;\n\tfl6->flowi6_mark = ireq->ir_mark;\n\tfl6->fl6_dport = ireq->ir_rmt_port;\n\tfl6->fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, fl6, final_p);\n\tif (IS_ERR(dst))\n\t\treturn NULL;\n\n\treturn dst;\n}",
                        "code_after_change": "struct dst_entry *inet6_csk_route_req(const struct sock *sk,\n\t\t\t\t      struct flowi6 *fl6,\n\t\t\t\t      const struct request_sock *req,\n\t\t\t\t      u8 proto)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = proto;\n\tfl6->daddr = ireq->ir_v6_rmt_addr;\n\trcu_read_lock();\n\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);\n\trcu_read_unlock();\n\tfl6->saddr = ireq->ir_v6_loc_addr;\n\tfl6->flowi6_oif = ireq->ir_iif;\n\tfl6->flowi6_mark = ireq->ir_mark;\n\tfl6->fl6_dport = ireq->ir_rmt_port;\n\tfl6->fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, fl6, final_p);\n\tif (IS_ERR(dst))\n\t\treturn NULL;\n\n\treturn dst;\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstruct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_options_received tcp_opt;\n\tstruct inet_request_sock *ireq;\n\tstruct tcp_request_sock *treq;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\t__u32 cookie = ntohl(th->ack_seq) - 1;\n\tstruct sock *ret = sk;\n\tstruct request_sock *req;\n\tint mss;\n\tstruct dst_entry *dst;\n\t__u8 rcv_wscale;\n\n\tif (!sysctl_tcp_syncookies || !th->ack || th->rst)\n\t\tgoto out;\n\n\tif (tcp_synq_no_recent_overflow(sk))\n\t\tgoto out;\n\n\tmss = __cookie_v6_check(ipv6_hdr(skb), th, cookie);\n\tif (mss == 0) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESFAILED);\n\t\tgoto out;\n\t}\n\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESRECV);\n\n\t/* check for timestamp cookie support */\n\tmemset(&tcp_opt, 0, sizeof(tcp_opt));\n\ttcp_parse_options(skb, &tcp_opt, 0, NULL);\n\n\tif (!cookie_timestamp_decode(&tcp_opt))\n\t\tgoto out;\n\n\tret = NULL;\n\treq = inet_reqsk_alloc(&tcp6_request_sock_ops, sk, false);\n\tif (!req)\n\t\tgoto out;\n\n\tireq = inet_rsk(req);\n\ttreq = tcp_rsk(req);\n\ttreq->tfo_listener = false;\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\tgoto out_free;\n\n\treq->mss = mss;\n\tireq->ir_rmt_port = th->source;\n\tireq->ir_num = ntohs(th->dest);\n\tireq->ir_v6_rmt_addr = ipv6_hdr(skb)->saddr;\n\tireq->ir_v6_loc_addr = ipv6_hdr(skb)->daddr;\n\tif (ipv6_opt_accepted(sk, skb, &TCP_SKB_CB(skb)->header.h6) ||\n\t    np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo ||\n\t    np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) {\n\t\tatomic_inc(&skb->users);\n\t\tireq->pktopts = skb;\n\t}\n\n\tireq->ir_iif = sk->sk_bound_dev_if;\n\t/* So that link locals have meaning */\n\tif (!sk->sk_bound_dev_if &&\n\t    ipv6_addr_type(&ireq->ir_v6_rmt_addr) & IPV6_ADDR_LINKLOCAL)\n\t\tireq->ir_iif = tcp_v6_iif(skb);\n\n\tireq->ir_mark = inet_request_mark(sk, skb);\n\n\treq->num_retrans = 0;\n\tireq->snd_wscale\t= tcp_opt.snd_wscale;\n\tireq->sack_ok\t\t= tcp_opt.sack_ok;\n\tireq->wscale_ok\t\t= tcp_opt.wscale_ok;\n\tireq->tstamp_ok\t\t= tcp_opt.saw_tstamp;\n\treq->ts_recent\t\t= tcp_opt.saw_tstamp ? tcp_opt.rcv_tsval : 0;\n\ttreq->snt_synack.v64\t= 0;\n\ttreq->rcv_isn = ntohl(th->seq) - 1;\n\ttreq->snt_isn = cookie;\n\n\t/*\n\t * We need to lookup the dst_entry to get the correct window size.\n\t * This is taken from tcp_v6_syn_recv_sock.  Somebody please enlighten\n\t * me if there is a preferred way.\n\t */\n\t{\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = IPPROTO_TCP;\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\t\tfl6.saddr = ireq->ir_v6_loc_addr;\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = ireq->ir_mark;\n\t\tfl6.fl6_dport = ireq->ir_rmt_port;\n\t\tfl6.fl6_sport = inet_sk(sk)->inet_sport;\n\t\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\t\tif (IS_ERR(dst))\n\t\t\tgoto out_free;\n\t}\n\n\treq->rsk_window_clamp = tp->window_clamp ? :dst_metric(dst, RTAX_WINDOW);\n\ttcp_select_initial_window(tcp_full_space(sk), req->mss,\n\t\t\t\t  &req->rsk_rcv_wnd, &req->rsk_window_clamp,\n\t\t\t\t  ireq->wscale_ok, &rcv_wscale,\n\t\t\t\t  dst_metric(dst, RTAX_INITRWND));\n\n\tireq->rcv_wscale = rcv_wscale;\n\tireq->ecn_ok = cookie_ecn_ok(&tcp_opt, sock_net(sk), dst);\n\n\tret = tcp_get_cookie_sock(sk, skb, req, dst);\nout:\n\treturn ret;\nout_free:\n\treqsk_free(req);\n\treturn NULL;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstruct dst_entry *inet6_csk_route_req(const struct sock *sk,\n\t\t\t\t      struct flowi6 *fl6,\n\t\t\t\t      const struct request_sock *req,\n\t\t\t\t      u8 proto)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = proto;\n\tfl6->daddr = ireq->ir_v6_rmt_addr;\n\tfinal_p = fl6_update_dst(fl6, np->opt, &final);\n\tfl6->saddr = ireq->ir_v6_loc_addr;\n\tfl6->flowi6_oif = ireq->ir_iif;\n\tfl6->flowi6_mark = ireq->ir_mark;\n\tfl6->fl6_dport = ireq->ir_rmt_port;\n\tfl6->fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, fl6, final_p);\n\tif (IS_ERR(dst))\n\t\treturn NULL;\n\n\treturn dst;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstruct dst_entry *inet6_csk_route_req(const struct sock *sk,\n\t\t\t\t      struct flowi6 *fl6,\n\t\t\t\t      const struct request_sock *req,\n\t\t\t\t      u8 proto)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = proto;\n\tfl6->daddr = ireq->ir_v6_rmt_addr;\n\trcu_read_lock();\n\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);\n\trcu_read_unlock();\n\tfl6->saddr = ireq->ir_v6_loc_addr;\n\tfl6->flowi6_oif = ireq->ir_iif;\n\tfl6->flowi6_mark = ireq->ir_mark;\n\tfl6->fl6_dport = ireq->ir_rmt_port;\n\tfl6->fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, fl6, final_p);\n\tif (IS_ERR(dst))\n\t\treturn NULL;\n\n\treturn dst;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "YES"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 1
        },
        {
            "id": 1121,
            "cve_id": "CVE-2016-8632",
            "code_snippet": "int tipc_enable_l2_media(struct net *net, struct tipc_bearer *b,\n\t\t\t struct nlattr *attr[])\n{\n\tstruct net_device *dev;\n\tchar *driver_name = strchr((const char *)b->name, ':') + 1;\n\n\t/* Find device with specified name */\n\tdev = dev_get_by_name(net, driver_name);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/* Associate TIPC bearer with L2 bearer */\n\trcu_assign_pointer(b->media_ptr, dev);\n\tmemset(&b->bcast_addr, 0, sizeof(b->bcast_addr));\n\tmemcpy(b->bcast_addr.value, dev->broadcast, b->media->hwaddr_len);\n\tb->bcast_addr.media_id = b->media->type_id;\n\tb->bcast_addr.broadcast = 1;\n\tb->mtu = dev->mtu;\n\tb->media->raw2addr(b, &b->addr, (char *)dev->dev_addr);\n\trcu_assign_pointer(dev->tipc_ptr, b);\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tipc_udp_enable(struct net *net, struct tipc_bearer *b,\n\t\t\t   struct nlattr *attrs[])\n{\n\tint err = -EINVAL;\n\tstruct udp_bearer *ub;\n\tstruct udp_media_addr remote = {0};\n\tstruct udp_media_addr local = {0};\n\tstruct udp_port_cfg udp_conf = {0};\n\tstruct udp_tunnel_sock_cfg tuncfg = {NULL};\n\tstruct nlattr *opts[TIPC_NLA_UDP_MAX + 1];\n\n\tub = kzalloc(sizeof(*ub), GFP_ATOMIC);\n\tif (!ub)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&ub->rcast.list);\n\n\tif (!attrs[TIPC_NLA_BEARER_UDP_OPTS])\n\t\tgoto err;\n\n\tif (nla_parse_nested(opts, TIPC_NLA_UDP_MAX,\n\t\t\t     attrs[TIPC_NLA_BEARER_UDP_OPTS],\n\t\t\t     tipc_nl_udp_policy))\n\t\tgoto err;\n\n\tif (!opts[TIPC_NLA_UDP_LOCAL] || !opts[TIPC_NLA_UDP_REMOTE]) {\n\t\tpr_err(\"Invalid UDP bearer configuration\");\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\terr = tipc_parse_udp_addr(opts[TIPC_NLA_UDP_LOCAL], &local,\n\t\t\t\t  &ub->ifindex);\n\tif (err)\n\t\tgoto err;\n\n\terr = tipc_parse_udp_addr(opts[TIPC_NLA_UDP_REMOTE], &remote, NULL);\n\tif (err)\n\t\tgoto err;\n\n\tb->bcast_addr.media_id = TIPC_MEDIA_TYPE_UDP;\n\tb->bcast_addr.broadcast = 1;\n\trcu_assign_pointer(b->media_ptr, ub);\n\trcu_assign_pointer(ub->bearer, b);\n\ttipc_udp_media_addr_set(&b->addr, &local);\n\tif (local.proto == htons(ETH_P_IP)) {\n\t\tstruct net_device *dev;\n\n\t\tdev = __ip_dev_find(net, local.ipv4.s_addr, false);\n\t\tif (!dev) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto err;\n\t\t}\n\t\tudp_conf.family = AF_INET;\n\t\tudp_conf.local_ip.s_addr = htonl(INADDR_ANY);\n\t\tudp_conf.use_udp_checksums = false;\n\t\tub->ifindex = dev->ifindex;\n\t\tb->mtu = dev->mtu - sizeof(struct iphdr)\n\t\t\t- sizeof(struct udphdr);\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else if (local.proto == htons(ETH_P_IPV6)) {\n\t\tudp_conf.family = AF_INET6;\n\t\tudp_conf.use_udp6_tx_checksums = true;\n\t\tudp_conf.use_udp6_rx_checksums = true;\n\t\tudp_conf.local_ip6 = in6addr_any;\n\t\tb->mtu = 1280;\n#endif\n\t} else {\n\t\terr = -EAFNOSUPPORT;\n\t\tgoto err;\n\t}\n\tudp_conf.local_udp_port = local.port;\n\terr = udp_sock_create(net, &udp_conf, &ub->ubsock);\n\tif (err)\n\t\tgoto err;\n\ttuncfg.sk_user_data = ub;\n\ttuncfg.encap_type = 1;\n\ttuncfg.encap_rcv = tipc_udp_recv;\n\ttuncfg.encap_destroy = NULL;\n\tsetup_udp_tunnel_sock(net, ub->ubsock, &tuncfg);\n\n\t/**\n\t * The bcast media address port is used for all peers and the ip\n\t * is used if it's a multicast address.\n\t */\n\tmemcpy(&b->bcast_addr.value, &remote, sizeof(remote));\n\tif (tipc_udp_is_mcast_addr(&remote))\n\t\terr = enable_mcast(ub, &remote);\n\telse\n\t\terr = tipc_udp_rcast_add(b, &remote);\n\tif (err)\n\t\tgoto err;\n\n\treturn 0;\nerr:\n\tif (ub->ubsock)\n\t\tudp_tunnel_sock_release(ub->ubsock);\n\tkfree(ub);\n\treturn err;\n}",
                        "code_after_change": "static int tipc_l2_device_event(struct notifier_block *nb, unsigned long evt,\n\t\t\t\tvoid *ptr)\n{\n\tstruct net_device *dev = netdev_notifier_info_to_dev(ptr);\n\tstruct net *net = dev_net(dev);\n\tstruct tipc_bearer *b;\n\n\tb = rtnl_dereference(dev->tipc_ptr);\n\tif (!b)\n\t\treturn NOTIFY_DONE;\n\n\tswitch (evt) {\n\tcase NETDEV_CHANGE:\n\t\tif (netif_carrier_ok(dev))\n\t\t\tbreak;\n\tcase NETDEV_UP:\n\t\ttest_and_set_bit_lock(0, &b->up);\n\t\tbreak;\n\tcase NETDEV_GOING_DOWN:\n\t\tclear_bit_unlock(0, &b->up);\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_CHANGEMTU:\n\t\tif (tipc_mtu_bad(dev, 0)) {\n\t\t\tbearer_disable(net, b);\n\t\t\tbreak;\n\t\t}\n\t\tb->mtu = dev->mtu;\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_CHANGEADDR:\n\t\tb->media->raw2addr(b, &b->addr,\n\t\t\t\t   (char *)dev->dev_addr);\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_UNREGISTER:\n\tcase NETDEV_CHANGENAME:\n\t\tbearer_disable(dev_net(dev), b);\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}",
                        "cve_id": "CVE-2016-8632"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nint tipc_enable_l2_media(struct net *net, struct tipc_bearer *b,\n\t\t\t struct nlattr *attr[])\n{\n\tstruct net_device *dev;\n\tchar *driver_name = strchr((const char *)b->name, ':') + 1;\n\n\t/* Find device with specified name */\n\tdev = dev_get_by_name(net, driver_name);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/* Associate TIPC bearer with L2 bearer */\n\trcu_assign_pointer(b->media_ptr, dev);\n\tmemset(&b->bcast_addr, 0, sizeof(b->bcast_addr));\n\tmemcpy(b->bcast_addr.value, dev->broadcast, b->media->hwaddr_len);\n\tb->bcast_addr.media_id = b->media->type_id;\n\tb->bcast_addr.broadcast = 1;\n\tb->mtu = dev->mtu;\n\tb->media->raw2addr(b, &b->addr, (char *)dev->dev_addr);\n\trcu_assign_pointer(dev->tipc_ptr, b);\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int tipc_udp_enable(struct net *net, struct tipc_bearer *b,\n\t\t\t   struct nlattr *attrs[])\n{\n\tint err = -EINVAL;\n\tstruct udp_bearer *ub;\n\tstruct udp_media_addr remote = {0};\n\tstruct udp_media_addr local = {0};\n\tstruct udp_port_cfg udp_conf = {0};\n\tstruct udp_tunnel_sock_cfg tuncfg = {NULL};\n\tstruct nlattr *opts[TIPC_NLA_UDP_MAX + 1];\n\n\tub = kzalloc(sizeof(*ub), GFP_ATOMIC);\n\tif (!ub)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&ub->rcast.list);\n\n\tif (!attrs[TIPC_NLA_BEARER_UDP_OPTS])\n\t\tgoto err;\n\n\tif (nla_parse_nested(opts, TIPC_NLA_UDP_MAX,\n\t\t\t     attrs[TIPC_NLA_BEARER_UDP_OPTS],\n\t\t\t     tipc_nl_udp_policy))\n\t\tgoto err;\n\n\tif (!opts[TIPC_NLA_UDP_LOCAL] || !opts[TIPC_NLA_UDP_REMOTE]) {\n\t\tpr_err(\"Invalid UDP bearer configuration\");\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\terr = tipc_parse_udp_addr(opts[TIPC_NLA_UDP_LOCAL], &local,\n\t\t\t\t  &ub->ifindex);\n\tif (err)\n\t\tgoto err;\n\n\terr = tipc_parse_udp_addr(opts[TIPC_NLA_UDP_REMOTE], &remote, NULL);\n\tif (err)\n\t\tgoto err;\n\n\tb->bcast_addr.media_id = TIPC_MEDIA_TYPE_UDP;\n\tb->bcast_addr.broadcast = 1;\n\trcu_assign_pointer(b->media_ptr, ub);\n\trcu_assign_pointer(ub->bearer, b);\n\ttipc_udp_media_addr_set(&b->addr, &local);\n\tif (local.proto == htons(ETH_P_IP)) {\n\t\tstruct net_device *dev;\n\n\t\tdev = __ip_dev_find(net, local.ipv4.s_addr, false);\n\t\tif (!dev) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto err;\n\t\t}\n\t\tudp_conf.family = AF_INET;\n\t\tudp_conf.local_ip.s_addr = htonl(INADDR_ANY);\n\t\tudp_conf.use_udp_checksums = false;\n\t\tub->ifindex = dev->ifindex;\n\t\tb->mtu = dev->mtu - sizeof(struct iphdr)\n\t\t\t- sizeof(struct udphdr);\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else if (local.proto == htons(ETH_P_IPV6)) {\n\t\tudp_conf.family = AF_INET6;\n\t\tudp_conf.use_udp6_tx_checksums = true;\n\t\tudp_conf.use_udp6_rx_checksums = true;\n\t\tudp_conf.local_ip6 = in6addr_any;\n\t\tb->mtu = 1280;\n#endif\n\t} else {\n\t\terr = -EAFNOSUPPORT;\n\t\tgoto err;\n\t}\n\tudp_conf.local_udp_port = local.port;\n\terr = udp_sock_create(net, &udp_conf, &ub->ubsock);\n\tif (err)\n\t\tgoto err;\n\ttuncfg.sk_user_data = ub;\n\ttuncfg.encap_type = 1;\n\ttuncfg.encap_rcv = tipc_udp_recv;\n\ttuncfg.encap_destroy = NULL;\n\tsetup_udp_tunnel_sock(net, ub->ubsock, &tuncfg);\n\n\t/**\n\t * The bcast media address port is used for all peers and the ip\n\t * is used if it's a multicast address.\n\t */\n\tmemcpy(&b->bcast_addr.value, &remote, sizeof(remote));\n\tif (tipc_udp_is_mcast_addr(&remote))\n\t\terr = enable_mcast(ub, &remote);\n\telse\n\t\terr = tipc_udp_rcast_add(b, &remote);\n\tif (err)\n\t\tgoto err;\n\n\treturn 0;\nerr:\n\tif (ub->ubsock)\n\t\tudp_tunnel_sock_release(ub->ubsock);\n\tkfree(ub);\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int tipc_l2_device_event(struct notifier_block *nb, unsigned long evt,\n\t\t\t\tvoid *ptr)\n{\n\tstruct net_device *dev = netdev_notifier_info_to_dev(ptr);\n\tstruct net *net = dev_net(dev);\n\tstruct tipc_bearer *b;\n\n\tb = rtnl_dereference(dev->tipc_ptr);\n\tif (!b)\n\t\treturn NOTIFY_DONE;\n\n\tswitch (evt) {\n\tcase NETDEV_CHANGE:\n\t\tif (netif_carrier_ok(dev))\n\t\t\tbreak;\n\tcase NETDEV_UP:\n\t\ttest_and_set_bit_lock(0, &b->up);\n\t\tbreak;\n\tcase NETDEV_GOING_DOWN:\n\t\tclear_bit_unlock(0, &b->up);\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_CHANGEMTU:\n\t\tif (tipc_mtu_bad(dev, 0)) {\n\t\t\tbearer_disable(net, b);\n\t\t\tbreak;\n\t\t}\n\t\tb->mtu = dev->mtu;\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_CHANGEADDR:\n\t\tb->media->raw2addr(b, &b->addr,\n\t\t\t\t   (char *)dev->dev_addr);\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_UNREGISTER:\n\tcase NETDEV_CHANGENAME:\n\t\tbearer_disable(dev_net(dev), b);\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tipc_l2_device_event(struct notifier_block *nb, unsigned long evt,\n\t\t\t\tvoid *ptr)\n{\n\tstruct net_device *dev = netdev_notifier_info_to_dev(ptr);\n\tstruct net *net = dev_net(dev);\n\tstruct tipc_bearer *b;\n\n\tb = rtnl_dereference(dev->tipc_ptr);\n\tif (!b)\n\t\treturn NOTIFY_DONE;\n\n\tb->mtu = dev->mtu;\n\n\tswitch (evt) {\n\tcase NETDEV_CHANGE:\n\t\tif (netif_carrier_ok(dev))\n\t\t\tbreak;\n\tcase NETDEV_UP:\n\t\ttest_and_set_bit_lock(0, &b->up);\n\t\tbreak;\n\tcase NETDEV_GOING_DOWN:\n\t\tclear_bit_unlock(0, &b->up);\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_CHANGEMTU:\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_CHANGEADDR:\n\t\tb->media->raw2addr(b, &b->addr,\n\t\t\t\t   (char *)dev->dev_addr);\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_UNREGISTER:\n\tcase NETDEV_CHANGENAME:\n\t\tbearer_disable(dev_net(dev), b);\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}",
                        "code_after_change": "static int tipc_udp_enable(struct net *net, struct tipc_bearer *b,\n\t\t\t   struct nlattr *attrs[])\n{\n\tint err = -EINVAL;\n\tstruct udp_bearer *ub;\n\tstruct udp_media_addr remote = {0};\n\tstruct udp_media_addr local = {0};\n\tstruct udp_port_cfg udp_conf = {0};\n\tstruct udp_tunnel_sock_cfg tuncfg = {NULL};\n\tstruct nlattr *opts[TIPC_NLA_UDP_MAX + 1];\n\n\tub = kzalloc(sizeof(*ub), GFP_ATOMIC);\n\tif (!ub)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&ub->rcast.list);\n\n\tif (!attrs[TIPC_NLA_BEARER_UDP_OPTS])\n\t\tgoto err;\n\n\tif (nla_parse_nested(opts, TIPC_NLA_UDP_MAX,\n\t\t\t     attrs[TIPC_NLA_BEARER_UDP_OPTS],\n\t\t\t     tipc_nl_udp_policy))\n\t\tgoto err;\n\n\tif (!opts[TIPC_NLA_UDP_LOCAL] || !opts[TIPC_NLA_UDP_REMOTE]) {\n\t\tpr_err(\"Invalid UDP bearer configuration\");\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\terr = tipc_parse_udp_addr(opts[TIPC_NLA_UDP_LOCAL], &local,\n\t\t\t\t  &ub->ifindex);\n\tif (err)\n\t\tgoto err;\n\n\terr = tipc_parse_udp_addr(opts[TIPC_NLA_UDP_REMOTE], &remote, NULL);\n\tif (err)\n\t\tgoto err;\n\n\tb->bcast_addr.media_id = TIPC_MEDIA_TYPE_UDP;\n\tb->bcast_addr.broadcast = 1;\n\trcu_assign_pointer(b->media_ptr, ub);\n\trcu_assign_pointer(ub->bearer, b);\n\ttipc_udp_media_addr_set(&b->addr, &local);\n\tif (local.proto == htons(ETH_P_IP)) {\n\t\tstruct net_device *dev;\n\n\t\tdev = __ip_dev_find(net, local.ipv4.s_addr, false);\n\t\tif (!dev) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto err;\n\t\t}\n\t\tudp_conf.family = AF_INET;\n\t\tudp_conf.local_ip.s_addr = htonl(INADDR_ANY);\n\t\tudp_conf.use_udp_checksums = false;\n\t\tub->ifindex = dev->ifindex;\n\t\tif (tipc_mtu_bad(dev, sizeof(struct iphdr) +\n\t\t\t\t      sizeof(struct udphdr))) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\t\tb->mtu = dev->mtu - sizeof(struct iphdr)\n\t\t\t- sizeof(struct udphdr);\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else if (local.proto == htons(ETH_P_IPV6)) {\n\t\tudp_conf.family = AF_INET6;\n\t\tudp_conf.use_udp6_tx_checksums = true;\n\t\tudp_conf.use_udp6_rx_checksums = true;\n\t\tudp_conf.local_ip6 = in6addr_any;\n\t\tb->mtu = 1280;\n#endif\n\t} else {\n\t\terr = -EAFNOSUPPORT;\n\t\tgoto err;\n\t}\n\tudp_conf.local_udp_port = local.port;\n\terr = udp_sock_create(net, &udp_conf, &ub->ubsock);\n\tif (err)\n\t\tgoto err;\n\ttuncfg.sk_user_data = ub;\n\ttuncfg.encap_type = 1;\n\ttuncfg.encap_rcv = tipc_udp_recv;\n\ttuncfg.encap_destroy = NULL;\n\tsetup_udp_tunnel_sock(net, ub->ubsock, &tuncfg);\n\n\t/**\n\t * The bcast media address port is used for all peers and the ip\n\t * is used if it's a multicast address.\n\t */\n\tmemcpy(&b->bcast_addr.value, &remote, sizeof(remote));\n\tif (tipc_udp_is_mcast_addr(&remote))\n\t\terr = enable_mcast(ub, &remote);\n\telse\n\t\terr = tipc_udp_rcast_add(b, &remote);\n\tif (err)\n\t\tgoto err;\n\n\treturn 0;\nerr:\n\tif (ub->ubsock)\n\t\tudp_tunnel_sock_release(ub->ubsock);\n\tkfree(ub);\n\treturn err;\n}",
                        "cve_id": "CVE-2016-8632"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nint tipc_enable_l2_media(struct net *net, struct tipc_bearer *b,\n\t\t\t struct nlattr *attr[])\n{\n\tstruct net_device *dev;\n\tchar *driver_name = strchr((const char *)b->name, ':') + 1;\n\n\t/* Find device with specified name */\n\tdev = dev_get_by_name(net, driver_name);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/* Associate TIPC bearer with L2 bearer */\n\trcu_assign_pointer(b->media_ptr, dev);\n\tmemset(&b->bcast_addr, 0, sizeof(b->bcast_addr));\n\tmemcpy(b->bcast_addr.value, dev->broadcast, b->media->hwaddr_len);\n\tb->bcast_addr.media_id = b->media->type_id;\n\tb->bcast_addr.broadcast = 1;\n\tb->mtu = dev->mtu;\n\tb->media->raw2addr(b, &b->addr, (char *)dev->dev_addr);\n\trcu_assign_pointer(dev->tipc_ptr, b);\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int tipc_l2_device_event(struct notifier_block *nb, unsigned long evt,\n\t\t\t\tvoid *ptr)\n{\n\tstruct net_device *dev = netdev_notifier_info_to_dev(ptr);\n\tstruct net *net = dev_net(dev);\n\tstruct tipc_bearer *b;\n\n\tb = rtnl_dereference(dev->tipc_ptr);\n\tif (!b)\n\t\treturn NOTIFY_DONE;\n\n\tb->mtu = dev->mtu;\n\n\tswitch (evt) {\n\tcase NETDEV_CHANGE:\n\t\tif (netif_carrier_ok(dev))\n\t\t\tbreak;\n\tcase NETDEV_UP:\n\t\ttest_and_set_bit_lock(0, &b->up);\n\t\tbreak;\n\tcase NETDEV_GOING_DOWN:\n\t\tclear_bit_unlock(0, &b->up);\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_CHANGEMTU:\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_CHANGEADDR:\n\t\tb->media->raw2addr(b, &b->addr,\n\t\t\t\t   (char *)dev->dev_addr);\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_UNREGISTER:\n\tcase NETDEV_CHANGENAME:\n\t\tbearer_disable(dev_net(dev), b);\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int tipc_udp_enable(struct net *net, struct tipc_bearer *b,\n\t\t\t   struct nlattr *attrs[])\n{\n\tint err = -EINVAL;\n\tstruct udp_bearer *ub;\n\tstruct udp_media_addr remote = {0};\n\tstruct udp_media_addr local = {0};\n\tstruct udp_port_cfg udp_conf = {0};\n\tstruct udp_tunnel_sock_cfg tuncfg = {NULL};\n\tstruct nlattr *opts[TIPC_NLA_UDP_MAX + 1];\n\n\tub = kzalloc(sizeof(*ub), GFP_ATOMIC);\n\tif (!ub)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&ub->rcast.list);\n\n\tif (!attrs[TIPC_NLA_BEARER_UDP_OPTS])\n\t\tgoto err;\n\n\tif (nla_parse_nested(opts, TIPC_NLA_UDP_MAX,\n\t\t\t     attrs[TIPC_NLA_BEARER_UDP_OPTS],\n\t\t\t     tipc_nl_udp_policy))\n\t\tgoto err;\n\n\tif (!opts[TIPC_NLA_UDP_LOCAL] || !opts[TIPC_NLA_UDP_REMOTE]) {\n\t\tpr_err(\"Invalid UDP bearer configuration\");\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\terr = tipc_parse_udp_addr(opts[TIPC_NLA_UDP_LOCAL], &local,\n\t\t\t\t  &ub->ifindex);\n\tif (err)\n\t\tgoto err;\n\n\terr = tipc_parse_udp_addr(opts[TIPC_NLA_UDP_REMOTE], &remote, NULL);\n\tif (err)\n\t\tgoto err;\n\n\tb->bcast_addr.media_id = TIPC_MEDIA_TYPE_UDP;\n\tb->bcast_addr.broadcast = 1;\n\trcu_assign_pointer(b->media_ptr, ub);\n\trcu_assign_pointer(ub->bearer, b);\n\ttipc_udp_media_addr_set(&b->addr, &local);\n\tif (local.proto == htons(ETH_P_IP)) {\n\t\tstruct net_device *dev;\n\n\t\tdev = __ip_dev_find(net, local.ipv4.s_addr, false);\n\t\tif (!dev) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto err;\n\t\t}\n\t\tudp_conf.family = AF_INET;\n\t\tudp_conf.local_ip.s_addr = htonl(INADDR_ANY);\n\t\tudp_conf.use_udp_checksums = false;\n\t\tub->ifindex = dev->ifindex;\n\t\tif (tipc_mtu_bad(dev, sizeof(struct iphdr) +\n\t\t\t\t      sizeof(struct udphdr))) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\t\tb->mtu = dev->mtu - sizeof(struct iphdr)\n\t\t\t- sizeof(struct udphdr);\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else if (local.proto == htons(ETH_P_IPV6)) {\n\t\tudp_conf.family = AF_INET6;\n\t\tudp_conf.use_udp6_tx_checksums = true;\n\t\tudp_conf.use_udp6_rx_checksums = true;\n\t\tudp_conf.local_ip6 = in6addr_any;\n\t\tb->mtu = 1280;\n#endif\n\t} else {\n\t\terr = -EAFNOSUPPORT;\n\t\tgoto err;\n\t}\n\tudp_conf.local_udp_port = local.port;\n\terr = udp_sock_create(net, &udp_conf, &ub->ubsock);\n\tif (err)\n\t\tgoto err;\n\ttuncfg.sk_user_data = ub;\n\ttuncfg.encap_type = 1;\n\ttuncfg.encap_rcv = tipc_udp_recv;\n\ttuncfg.encap_destroy = NULL;\n\tsetup_udp_tunnel_sock(net, ub->ubsock, &tuncfg);\n\n\t/**\n\t * The bcast media address port is used for all peers and the ip\n\t * is used if it's a multicast address.\n\t */\n\tmemcpy(&b->bcast_addr.value, &remote, sizeof(remote));\n\tif (tipc_udp_is_mcast_addr(&remote))\n\t\terr = enable_mcast(ub, &remote);\n\telse\n\t\terr = tipc_udp_rcast_add(b, &remote);\n\tif (err)\n\t\tgoto err;\n\n\treturn 0;\nerr:\n\tif (ub->ubsock)\n\t\tudp_tunnel_sock_release(ub->ubsock);\n\tkfree(ub);\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "YES"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 1
        },
        {
            "id": 454,
            "cve_id": "CVE-2014-0181",
            "code_snippet": "static int tc_ctl_action(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_ACT_MAX + 1];\n\tu32 portid = skb ? NETLINK_CB(skb).portid : 0;\n\tint ret = 0, ovr = 0;\n\n\tif ((n->nlmsg_type != RTM_GETACTION) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tret = nlmsg_parse(n, sizeof(struct tcamsg), tca, TCA_ACT_MAX, NULL);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (tca[TCA_ACT_TAB] == NULL) {\n\t\tpr_notice(\"tc_ctl_action: received NO action attribs\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* n->nlmsg_flags & NLM_F_CREATE */\n\tswitch (n->nlmsg_type) {\n\tcase RTM_NEWACTION:\n\t\t/* we are going to assume all other flags\n\t\t * imply create only if it doesn't exist\n\t\t * Note that CREATE | EXCL implies that\n\t\t * but since we want avoid ambiguity (eg when flags\n\t\t * is zero) then just set this\n\t\t */\n\t\tif (n->nlmsg_flags & NLM_F_REPLACE)\n\t\t\tovr = 1;\nreplay:\n\t\tret = tcf_action_add(net, tca[TCA_ACT_TAB], n, portid, ovr);\n\t\tif (ret == -EAGAIN)\n\t\t\tgoto replay;\n\t\tbreak;\n\tcase RTM_DELACTION:\n\t\tret = tca_action_gd(net, tca[TCA_ACT_TAB], n,\n\t\t\t\t    portid, RTM_DELACTION);\n\t\tbreak;\n\tcase RTM_GETACTION:\n\t\tret = tca_action_gd(net, tca[TCA_ACT_TAB], n,\n\t\t\t\t    portid, RTM_GETACTION);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}",
                        "code_after_change": "static int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int tc_ctl_action(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_ACT_MAX + 1];\n\tu32 portid = skb ? NETLINK_CB(skb).portid : 0;\n\tint ret = 0, ovr = 0;\n\n\tif ((n->nlmsg_type != RTM_GETACTION) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tret = nlmsg_parse(n, sizeof(struct tcamsg), tca, TCA_ACT_MAX, NULL);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (tca[TCA_ACT_TAB] == NULL) {\n\t\tpr_notice(\"tc_ctl_action: received NO action attribs\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* n->nlmsg_flags & NLM_F_CREATE */\n\tswitch (n->nlmsg_type) {\n\tcase RTM_NEWACTION:\n\t\t/* we are going to assume all other flags\n\t\t * imply create only if it doesn't exist\n\t\t * Note that CREATE | EXCL implies that\n\t\t * but since we want avoid ambiguity (eg when flags\n\t\t * is zero) then just set this\n\t\t */\n\t\tif (n->nlmsg_flags & NLM_F_REPLACE)\n\t\t\tovr = 1;\nreplay:\n\t\tret = tcf_action_add(net, tca[TCA_ACT_TAB], n, portid, ovr);\n\t\tif (ret == -EAGAIN)\n\t\t\tgoto replay;\n\t\tbreak;\n\tcase RTM_DELACTION:\n\t\tret = tca_action_gd(net, tca[TCA_ACT_TAB], n,\n\t\t\t\t    portid, RTM_DELACTION);\n\t\tbreak;\n\tcase RTM_GETACTION:\n\t\tret = tca_action_gd(net, tca[TCA_ACT_TAB], n,\n\t\t\t\t    portid, RTM_GETACTION);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}",
                        "code_after_change": "static int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int tc_ctl_action(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_ACT_MAX + 1];\n\tu32 portid = skb ? NETLINK_CB(skb).portid : 0;\n\tint ret = 0, ovr = 0;\n\n\tif ((n->nlmsg_type != RTM_GETACTION) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tret = nlmsg_parse(n, sizeof(struct tcamsg), tca, TCA_ACT_MAX, NULL);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (tca[TCA_ACT_TAB] == NULL) {\n\t\tpr_notice(\"tc_ctl_action: received NO action attribs\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* n->nlmsg_flags & NLM_F_CREATE */\n\tswitch (n->nlmsg_type) {\n\tcase RTM_NEWACTION:\n\t\t/* we are going to assume all other flags\n\t\t * imply create only if it doesn't exist\n\t\t * Note that CREATE | EXCL implies that\n\t\t * but since we want avoid ambiguity (eg when flags\n\t\t * is zero) then just set this\n\t\t */\n\t\tif (n->nlmsg_flags & NLM_F_REPLACE)\n\t\t\tovr = 1;\nreplay:\n\t\tret = tcf_action_add(net, tca[TCA_ACT_TAB], n, portid, ovr);\n\t\tif (ret == -EAGAIN)\n\t\t\tgoto replay;\n\t\tbreak;\n\tcase RTM_DELACTION:\n\t\tret = tca_action_gd(net, tca[TCA_ACT_TAB], n,\n\t\t\t\t    portid, RTM_DELACTION);\n\t\tbreak;\n\tcase RTM_GETACTION:\n\t\tret = tca_action_gd(net, tca[TCA_ACT_TAB], n,\n\t\t\t\t    portid, RTM_GETACTION);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "YES"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 1
        },
        {
            "id": 1003,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "static int rawv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct raw6_frag_vec rfv;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tu16 proto;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (sin6) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (sin6->sin6_family && sin6->sin6_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\t/* port is the proto value [0..255] carried in nexthdr */\n\t\tproto = ntohs(sin6->sin6_port);\n\n\t\tif (!proto)\n\t\t\tproto = inet->inet_num;\n\t\telse if (proto != inet->inet_num)\n\t\t\treturn -EINVAL;\n\n\t\tif (proto > 255)\n\t\t\treturn -EINVAL;\n\n\t\tdaddr = &sin6->sin6_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tproto = inet->inet_num;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\tif (!opt)\n\t\topt = np->opt;\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = proto;\n\trfv.msg = msg;\n\trfv.hlen = 0;\n\terr = rawv6_probe_proto_opt(&rfv, &fl6);\n\tif (err)\n\t\tgoto out;\n\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tif (inet->hdrincl)\n\t\tfl6.flowi6_flags |= FLOWI_FLAG_KNOWN_NH;\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tif (inet->hdrincl)\n\t\terr = rawv6_send_hdrinc(sk, msg, len, &fl6, &dst, msg->msg_flags);\n\telse {\n\t\tlock_sock(sk);\n\t\terr = ip6_append_data(sk, raw6_getfrag, &rfv,\n\t\t\tlen, 0, hlimit, tclass, opt, &fl6, (struct rt6_info *)dst,\n\t\t\tmsg->msg_flags, dontfrag);\n\n\t\tif (err)\n\t\t\tip6_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE))\n\t\t\terr = rawv6_push_pending_frames(sk, &fl6, rp);\n\t\trelease_sock(sk);\n\t}\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err < 0 ? err : len;\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint transhdrlen = 4; /* zero session-id */\n\tint ulen = len + transhdrlen;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (lsa) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (lsa->l2tp_family && lsa->l2tp_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\tdaddr = &lsa->l2tp_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = lsa->l2tp_flowinfo & IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (flowlabel == NULL)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    lsa->l2tp_scope_id &&\n\t\t    ipv6_addr_type(daddr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tfl6.flowi6_oif = lsa->l2tp_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel & IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\n\tif (opt == NULL)\n\t\topt = np->opt;\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tlock_sock(sk);\n\terr = ip6_append_data(sk, ip_generic_getfrag, msg,\n\t\t\t      ulen, transhdrlen, hlimit, tclass, opt,\n\t\t\t      &fl6, (struct rt6_info *)dst,\n\t\t\t      msg->msg_flags, dontfrag);\n\tif (err)\n\t\tip6_flush_pending_frames(sk);\n\telse if (!(msg->msg_flags & MSG_MORE))\n\t\terr = l2tp_ip6_push_pending_frames(sk);\n\trelease_sock(sk);\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\n\treturn err < 0 ? err : len;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
                        "code_after_change": "static int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint transhdrlen = 4; /* zero session-id */\n\tint ulen = len + transhdrlen;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (lsa) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (lsa->l2tp_family && lsa->l2tp_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\tdaddr = &lsa->l2tp_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = lsa->l2tp_flowinfo & IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (flowlabel == NULL)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    lsa->l2tp_scope_id &&\n\t\t    ipv6_addr_type(daddr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tfl6.flowi6_oif = lsa->l2tp_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel & IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tlock_sock(sk);\n\terr = ip6_append_data(sk, ip_generic_getfrag, msg,\n\t\t\t      ulen, transhdrlen, hlimit, tclass, opt,\n\t\t\t      &fl6, (struct rt6_info *)dst,\n\t\t\t      msg->msg_flags, dontfrag);\n\tif (err)\n\t\tip6_flush_pending_frames(sk);\n\telse if (!(msg->msg_flags & MSG_MORE))\n\t\terr = l2tp_ip6_push_pending_frames(sk);\n\trelease_sock(sk);\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\n\treturn err < 0 ? err : len;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int rawv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct raw6_frag_vec rfv;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tu16 proto;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (sin6) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (sin6->sin6_family && sin6->sin6_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\t/* port is the proto value [0..255] carried in nexthdr */\n\t\tproto = ntohs(sin6->sin6_port);\n\n\t\tif (!proto)\n\t\t\tproto = inet->inet_num;\n\t\telse if (proto != inet->inet_num)\n\t\t\treturn -EINVAL;\n\n\t\tif (proto > 255)\n\t\t\treturn -EINVAL;\n\n\t\tdaddr = &sin6->sin6_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tproto = inet->inet_num;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\tif (!opt)\n\t\topt = np->opt;\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = proto;\n\trfv.msg = msg;\n\trfv.hlen = 0;\n\terr = rawv6_probe_proto_opt(&rfv, &fl6);\n\tif (err)\n\t\tgoto out;\n\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tif (inet->hdrincl)\n\t\tfl6.flowi6_flags |= FLOWI_FLAG_KNOWN_NH;\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tif (inet->hdrincl)\n\t\terr = rawv6_send_hdrinc(sk, msg, len, &fl6, &dst, msg->msg_flags);\n\telse {\n\t\tlock_sock(sk);\n\t\terr = ip6_append_data(sk, raw6_getfrag, &rfv,\n\t\t\tlen, 0, hlimit, tclass, opt, &fl6, (struct rt6_info *)dst,\n\t\t\tmsg->msg_flags, dontfrag);\n\n\t\tif (err)\n\t\t\tip6_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE))\n\t\t\terr = rawv6_push_pending_frames(sk, &fl6, rp);\n\t\trelease_sock(sk);\n\t}\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err < 0 ? err : len;\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint transhdrlen = 4; /* zero session-id */\n\tint ulen = len + transhdrlen;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (lsa) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (lsa->l2tp_family && lsa->l2tp_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\tdaddr = &lsa->l2tp_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = lsa->l2tp_flowinfo & IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (flowlabel == NULL)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    lsa->l2tp_scope_id &&\n\t\t    ipv6_addr_type(daddr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tfl6.flowi6_oif = lsa->l2tp_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel & IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\n\tif (opt == NULL)\n\t\topt = np->opt;\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tlock_sock(sk);\n\terr = ip6_append_data(sk, ip_generic_getfrag, msg,\n\t\t\t      ulen, transhdrlen, hlimit, tclass, opt,\n\t\t\t      &fl6, (struct rt6_info *)dst,\n\t\t\t      msg->msg_flags, dontfrag);\n\tif (err)\n\t\tip6_flush_pending_frames(sk);\n\telse if (!(msg->msg_flags & MSG_MORE))\n\t\terr = l2tp_ip6_push_pending_frames(sk);\n\trelease_sock(sk);\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\n\treturn err < 0 ? err : len;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint transhdrlen = 4; /* zero session-id */\n\tint ulen = len + transhdrlen;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (lsa) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (lsa->l2tp_family && lsa->l2tp_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\tdaddr = &lsa->l2tp_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = lsa->l2tp_flowinfo & IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (flowlabel == NULL)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    lsa->l2tp_scope_id &&\n\t\t    ipv6_addr_type(daddr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tfl6.flowi6_oif = lsa->l2tp_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel & IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tlock_sock(sk);\n\terr = ip6_append_data(sk, ip_generic_getfrag, msg,\n\t\t\t      ulen, transhdrlen, hlimit, tclass, opt,\n\t\t\t      &fl6, (struct rt6_info *)dst,\n\t\t\t      msg->msg_flags, dontfrag);\n\tif (err)\n\t\tip6_flush_pending_frames(sk);\n\telse if (!(msg->msg_flags & MSG_MORE))\n\t\terr = l2tp_ip6_push_pending_frames(sk);\n\trelease_sock(sk);\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\n\treturn err < 0 ? err : len;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t*daddr, *final_p, final;\n\tstruct dst_entry\t*dst;\n\tstruct flowi6\t\tfl6;\n\tstruct ip6_flowlabel\t*flowlabel = NULL;\n\tstruct ipv6_txoptions\t*opt;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tusin->sin6_addr.s6_addr[15] = 0x01;\n\t}\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6.flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet->inet_dport;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tif (!fl6.flowi6_oif && (addr_type&IPV6_ADDR_MULTICAST))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\topt = flowlabel ? flowlabel->opt : np->opt;\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\terr = 0;\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\t/* source address lookup done in ip6_dst_lookup */\n\n\tif (ipv6_addr_any(&np->saddr))\n\t\tnp->saddr = fl6.saddr;\n\n\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tsk->sk_v6_rcv_saddr = fl6.saddr;\n\t\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\t\tif (sk->sk_prot->rehash)\n\t\t\tsk->sk_prot->rehash(sk);\n\t}\n\n\tip6_dst_store(sk, dst,\n\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t      &np->saddr :\n#endif\n\t\t      NULL);\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err;\n}",
                        "code_after_change": "static int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t*daddr, *final_p, final;\n\tstruct dst_entry\t*dst;\n\tstruct flowi6\t\tfl6;\n\tstruct ip6_flowlabel\t*flowlabel = NULL;\n\tstruct ipv6_txoptions\t*opt;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tusin->sin6_addr.s6_addr[15] = 0x01;\n\t}\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6.flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet->inet_dport;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tif (!fl6.flowi6_oif && (addr_type&IPV6_ADDR_MULTICAST))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\trcu_read_lock();\n\topt = flowlabel ? flowlabel->opt : rcu_dereference(np->opt);\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\trcu_read_unlock();\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\terr = 0;\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\t/* source address lookup done in ip6_dst_lookup */\n\n\tif (ipv6_addr_any(&np->saddr))\n\t\tnp->saddr = fl6.saddr;\n\n\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tsk->sk_v6_rcv_saddr = fl6.saddr;\n\t\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\t\tif (sk->sk_prot->rehash)\n\t\t\tsk->sk_prot->rehash(sk);\n\t}\n\n\tip6_dst_store(sk, dst,\n\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t      &np->saddr :\n#endif\n\t\t      NULL);\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err;\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int rawv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct raw6_frag_vec rfv;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tu16 proto;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (sin6) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (sin6->sin6_family && sin6->sin6_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\t/* port is the proto value [0..255] carried in nexthdr */\n\t\tproto = ntohs(sin6->sin6_port);\n\n\t\tif (!proto)\n\t\t\tproto = inet->inet_num;\n\t\telse if (proto != inet->inet_num)\n\t\t\treturn -EINVAL;\n\n\t\tif (proto > 255)\n\t\t\treturn -EINVAL;\n\n\t\tdaddr = &sin6->sin6_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tproto = inet->inet_num;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\tif (!opt)\n\t\topt = np->opt;\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = proto;\n\trfv.msg = msg;\n\trfv.hlen = 0;\n\terr = rawv6_probe_proto_opt(&rfv, &fl6);\n\tif (err)\n\t\tgoto out;\n\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tif (inet->hdrincl)\n\t\tfl6.flowi6_flags |= FLOWI_FLAG_KNOWN_NH;\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tif (inet->hdrincl)\n\t\terr = rawv6_send_hdrinc(sk, msg, len, &fl6, &dst, msg->msg_flags);\n\telse {\n\t\tlock_sock(sk);\n\t\terr = ip6_append_data(sk, raw6_getfrag, &rfv,\n\t\t\tlen, 0, hlimit, tclass, opt, &fl6, (struct rt6_info *)dst,\n\t\t\tmsg->msg_flags, dontfrag);\n\n\t\tif (err)\n\t\t\tip6_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE))\n\t\t\terr = rawv6_push_pending_frames(sk, &fl6, rp);\n\t\trelease_sock(sk);\n\t}\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err < 0 ? err : len;\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t*daddr, *final_p, final;\n\tstruct dst_entry\t*dst;\n\tstruct flowi6\t\tfl6;\n\tstruct ip6_flowlabel\t*flowlabel = NULL;\n\tstruct ipv6_txoptions\t*opt;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tusin->sin6_addr.s6_addr[15] = 0x01;\n\t}\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6.flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet->inet_dport;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tif (!fl6.flowi6_oif && (addr_type&IPV6_ADDR_MULTICAST))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\topt = flowlabel ? flowlabel->opt : np->opt;\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\terr = 0;\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\t/* source address lookup done in ip6_dst_lookup */\n\n\tif (ipv6_addr_any(&np->saddr))\n\t\tnp->saddr = fl6.saddr;\n\n\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tsk->sk_v6_rcv_saddr = fl6.saddr;\n\t\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\t\tif (sk->sk_prot->rehash)\n\t\t\tsk->sk_prot->rehash(sk);\n\t}\n\n\tip6_dst_store(sk, dst,\n\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t      &np->saddr :\n#endif\n\t\t      NULL);\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t*daddr, *final_p, final;\n\tstruct dst_entry\t*dst;\n\tstruct flowi6\t\tfl6;\n\tstruct ip6_flowlabel\t*flowlabel = NULL;\n\tstruct ipv6_txoptions\t*opt;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tusin->sin6_addr.s6_addr[15] = 0x01;\n\t}\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6.flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet->inet_dport;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tif (!fl6.flowi6_oif && (addr_type&IPV6_ADDR_MULTICAST))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\trcu_read_lock();\n\topt = flowlabel ? flowlabel->opt : rcu_dereference(np->opt);\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\trcu_read_unlock();\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\terr = 0;\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\t/* source address lookup done in ip6_dst_lookup */\n\n\tif (ipv6_addr_any(&np->saddr))\n\t\tnp->saddr = fl6.saddr;\n\n\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tsk->sk_v6_rcv_saddr = fl6.saddr;\n\t\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\t\tif (sk->sk_prot->rehash)\n\t\t\tsk->sk_prot->rehash(sk);\n\t}\n\n\tip6_dst_store(sk, dst,\n\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t      &np->saddr :\n#endif\n\t\t      NULL);\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *)uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel & IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\t/*\n\t * connect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t * DCCP over IPv4\n\t */\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\n\t\terr = dccp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &dccp_ipv6_af_ops;\n\t\t\tsk->sk_backlog_rcv = dccp_v6_do_rcv;\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (saddr == NULL) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (np->opt != NULL)\n\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n\t\t\t\t\t  np->opt->opt_nflen);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet6_hash_connect(&dccp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tdp->dccps_iss = secure_dccpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t      sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t      inet->inet_sport,\n\t\t\t\t\t\t      inet->inet_dport);\n\terr = dccp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\tdccp_set_state(sk, DCCP_CLOSED);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
                        "code_after_change": "static int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *)uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct ipv6_txoptions *opt;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel & IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\t/*\n\t * connect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t * DCCP over IPv4\n\t */\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\n\t\terr = dccp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &dccp_ipv6_af_ops;\n\t\t\tsk->sk_backlog_rcv = dccp_v6_do_rcv;\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (saddr == NULL) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet6_hash_connect(&dccp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tdp->dccps_iss = secure_dccpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t      sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t      inet->inet_sport,\n\t\t\t\t\t\t      inet->inet_dport);\n\terr = dccp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\tdccp_set_state(sk, DCCP_CLOSED);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int rawv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct raw6_frag_vec rfv;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tu16 proto;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (sin6) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (sin6->sin6_family && sin6->sin6_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\t/* port is the proto value [0..255] carried in nexthdr */\n\t\tproto = ntohs(sin6->sin6_port);\n\n\t\tif (!proto)\n\t\t\tproto = inet->inet_num;\n\t\telse if (proto != inet->inet_num)\n\t\t\treturn -EINVAL;\n\n\t\tif (proto > 255)\n\t\t\treturn -EINVAL;\n\n\t\tdaddr = &sin6->sin6_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tproto = inet->inet_num;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\tif (!opt)\n\t\topt = np->opt;\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = proto;\n\trfv.msg = msg;\n\trfv.hlen = 0;\n\terr = rawv6_probe_proto_opt(&rfv, &fl6);\n\tif (err)\n\t\tgoto out;\n\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tif (inet->hdrincl)\n\t\tfl6.flowi6_flags |= FLOWI_FLAG_KNOWN_NH;\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tif (inet->hdrincl)\n\t\terr = rawv6_send_hdrinc(sk, msg, len, &fl6, &dst, msg->msg_flags);\n\telse {\n\t\tlock_sock(sk);\n\t\terr = ip6_append_data(sk, raw6_getfrag, &rfv,\n\t\t\tlen, 0, hlimit, tclass, opt, &fl6, (struct rt6_info *)dst,\n\t\t\tmsg->msg_flags, dontfrag);\n\n\t\tif (err)\n\t\t\tip6_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE))\n\t\t\terr = rawv6_push_pending_frames(sk, &fl6, rp);\n\t\trelease_sock(sk);\n\t}\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err < 0 ? err : len;\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *)uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel & IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\t/*\n\t * connect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t * DCCP over IPv4\n\t */\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\n\t\terr = dccp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &dccp_ipv6_af_ops;\n\t\t\tsk->sk_backlog_rcv = dccp_v6_do_rcv;\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (saddr == NULL) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (np->opt != NULL)\n\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n\t\t\t\t\t  np->opt->opt_nflen);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet6_hash_connect(&dccp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tdp->dccps_iss = secure_dccpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t      sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t      inet->inet_sport,\n\t\t\t\t\t\t      inet->inet_dport);\n\terr = dccp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\tdccp_set_state(sk, DCCP_CLOSED);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *)uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct ipv6_txoptions *opt;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel & IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\t/*\n\t * connect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t * DCCP over IPv4\n\t */\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\n\t\terr = dccp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &dccp_ipv6_af_ops;\n\t\t\tsk->sk_backlog_rcv = dccp_v6_do_rcv;\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (saddr == NULL) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet6_hash_connect(&dccp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tdp->dccps_iss = secure_dccpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t      sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t      inet->inet_sport,\n\t\t\t\t\t\t      inet->inet_dport);\n\terr = dccp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\tdccp_set_state(sk, DCCP_CLOSED);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 1008,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "int udpv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_len = msg->msg_namelen;\n\tint ulen = len;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint err;\n\tint connected = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\n\t/* destination address check */\n\tif (sin6) {\n\t\tif (addr_len < offsetof(struct sockaddr, sa_data))\n\t\t\treturn -EINVAL;\n\n\t\tswitch (sin6->sin6_family) {\n\t\tcase AF_INET6:\n\t\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\t\treturn -EINVAL;\n\t\t\tdaddr = &sin6->sin6_addr;\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tgoto do_udp_sendmsg;\n\t\tcase AF_UNSPEC:\n\t\t\tmsg->msg_name = sin6 = NULL;\n\t\t\tmsg->msg_namelen = addr_len = 0;\n\t\t\tdaddr = NULL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (!up->pending) {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t} else\n\t\tdaddr = NULL;\n\n\tif (daddr) {\n\t\tif (ipv6_addr_v4mapped(daddr)) {\n\t\t\tstruct sockaddr_in sin;\n\t\t\tsin.sin_family = AF_INET;\n\t\t\tsin.sin_port = sin6 ? sin6->sin6_port : inet->inet_dport;\n\t\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\t\tmsg->msg_name = &sin;\n\t\t\tmsg->msg_namelen = sizeof(sin);\ndo_udp_sendmsg:\n\t\t\tif (__ipv6_only_sock(sk))\n\t\t\t\treturn -ENETUNREACH;\n\t\t\treturn udp_sendmsg(sk, msg, len);\n\t\t}\n\t}\n\n\tif (up->pending == AF_INET)\n\t\treturn udp_sendmsg(sk, msg, len);\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t   */\n\tif (len > INT_MAX - sizeof(struct udphdr))\n\t\treturn -EMSGSIZE;\n\n\tgetfrag  =  is_udplite ?  udplite_getfrag : ip_generic_getfrag;\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET6)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t\t}\n\t\t\tdst = NULL;\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (sin6) {\n\t\tif (sin6->sin6_port == 0)\n\t\t\treturn -EINVAL;\n\n\t\tfl6.fl6_dport = sin6->sin6_port;\n\t\tdaddr = &sin6->sin6_addr;\n\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tconnected = 1;\n\t}\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->sticky_pktinfo.ipi6_ifindex;\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(*opt);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t\tconnected = 0;\n\t}\n\tif (!opt)\n\t\topt = np->opt;\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\tif (final_p)\n\t\tconnected = 0;\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr)) {\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\t\tconnected = 0;\n\t} else if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_sk_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\t/* Lockless fast path for the non-corking case */\n\tif (!corkreq) {\n\t\tstruct sk_buff *skb;\n\n\t\tskb = ip6_make_skb(sk, getfrag, msg, ulen,\n\t\t\t\t   sizeof(struct udphdr), hlimit, tclass, opt,\n\t\t\t\t   &fl6, (struct rt6_info *)dst,\n\t\t\t\t   msg->msg_flags, dontfrag);\n\t\terr = PTR_ERR(skb);\n\t\tif (!IS_ERR_OR_NULL(skb))\n\t\t\terr = udp_v6_send_skb(skb, &fl6);\n\t\tgoto release_dst;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tnet_dbg_ratelimited(\"udp cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tup->pending = AF_INET6;\n\ndo_append_data:\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\tup->len += ulen;\n\terr = ip6_append_data(sk, getfrag, msg, ulen,\n\t\tsizeof(struct udphdr), hlimit, tclass, opt, &fl6,\n\t\t(struct rt6_info *)dst,\n\t\tcorkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags, dontfrag);\n\tif (err)\n\t\tudp_v6_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_v6_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\n\tif (err > 0)\n\t\terr = np->recverr ? net_xmit_errno(err) : 0;\n\trelease_sock(sk);\n\nrelease_dst:\n\tif (dst) {\n\t\tif (connected) {\n\t\t\tip6_dst_store(sk, dst,\n\t\t\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t\t\t      &np->saddr :\n#endif\n\t\t\t\t      NULL);\n\t\t} else {\n\t\t\tdst_release(dst);\n\t\t}\n\t\tdst = NULL;\n\t}\n\nout:\n\tdst_release(dst);\n\tfl6_sock_release(flowlabel);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint transhdrlen = 4; /* zero session-id */\n\tint ulen = len + transhdrlen;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (lsa) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (lsa->l2tp_family && lsa->l2tp_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\tdaddr = &lsa->l2tp_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = lsa->l2tp_flowinfo & IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (flowlabel == NULL)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    lsa->l2tp_scope_id &&\n\t\t    ipv6_addr_type(daddr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tfl6.flowi6_oif = lsa->l2tp_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel & IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\n\tif (opt == NULL)\n\t\topt = np->opt;\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tlock_sock(sk);\n\terr = ip6_append_data(sk, ip_generic_getfrag, msg,\n\t\t\t      ulen, transhdrlen, hlimit, tclass, opt,\n\t\t\t      &fl6, (struct rt6_info *)dst,\n\t\t\t      msg->msg_flags, dontfrag);\n\tif (err)\n\t\tip6_flush_pending_frames(sk);\n\telse if (!(msg->msg_flags & MSG_MORE))\n\t\terr = l2tp_ip6_push_pending_frames(sk);\n\trelease_sock(sk);\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\n\treturn err < 0 ? err : len;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
                        "code_after_change": "static int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint transhdrlen = 4; /* zero session-id */\n\tint ulen = len + transhdrlen;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (lsa) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (lsa->l2tp_family && lsa->l2tp_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\tdaddr = &lsa->l2tp_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = lsa->l2tp_flowinfo & IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (flowlabel == NULL)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    lsa->l2tp_scope_id &&\n\t\t    ipv6_addr_type(daddr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tfl6.flowi6_oif = lsa->l2tp_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel & IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tlock_sock(sk);\n\terr = ip6_append_data(sk, ip_generic_getfrag, msg,\n\t\t\t      ulen, transhdrlen, hlimit, tclass, opt,\n\t\t\t      &fl6, (struct rt6_info *)dst,\n\t\t\t      msg->msg_flags, dontfrag);\n\tif (err)\n\t\tip6_flush_pending_frames(sk);\n\telse if (!(msg->msg_flags & MSG_MORE))\n\t\terr = l2tp_ip6_push_pending_frames(sk);\n\trelease_sock(sk);\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\n\treturn err < 0 ? err : len;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nint udpv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_len = msg->msg_namelen;\n\tint ulen = len;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint err;\n\tint connected = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\n\t/* destination address check */\n\tif (sin6) {\n\t\tif (addr_len < offsetof(struct sockaddr, sa_data))\n\t\t\treturn -EINVAL;\n\n\t\tswitch (sin6->sin6_family) {\n\t\tcase AF_INET6:\n\t\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\t\treturn -EINVAL;\n\t\t\tdaddr = &sin6->sin6_addr;\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tgoto do_udp_sendmsg;\n\t\tcase AF_UNSPEC:\n\t\t\tmsg->msg_name = sin6 = NULL;\n\t\t\tmsg->msg_namelen = addr_len = 0;\n\t\t\tdaddr = NULL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (!up->pending) {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t} else\n\t\tdaddr = NULL;\n\n\tif (daddr) {\n\t\tif (ipv6_addr_v4mapped(daddr)) {\n\t\t\tstruct sockaddr_in sin;\n\t\t\tsin.sin_family = AF_INET;\n\t\t\tsin.sin_port = sin6 ? sin6->sin6_port : inet->inet_dport;\n\t\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\t\tmsg->msg_name = &sin;\n\t\t\tmsg->msg_namelen = sizeof(sin);\ndo_udp_sendmsg:\n\t\t\tif (__ipv6_only_sock(sk))\n\t\t\t\treturn -ENETUNREACH;\n\t\t\treturn udp_sendmsg(sk, msg, len);\n\t\t}\n\t}\n\n\tif (up->pending == AF_INET)\n\t\treturn udp_sendmsg(sk, msg, len);\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t   */\n\tif (len > INT_MAX - sizeof(struct udphdr))\n\t\treturn -EMSGSIZE;\n\n\tgetfrag  =  is_udplite ?  udplite_getfrag : ip_generic_getfrag;\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET6)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t\t}\n\t\t\tdst = NULL;\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (sin6) {\n\t\tif (sin6->sin6_port == 0)\n\t\t\treturn -EINVAL;\n\n\t\tfl6.fl6_dport = sin6->sin6_port;\n\t\tdaddr = &sin6->sin6_addr;\n\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tconnected = 1;\n\t}\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->sticky_pktinfo.ipi6_ifindex;\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(*opt);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t\tconnected = 0;\n\t}\n\tif (!opt)\n\t\topt = np->opt;\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\tif (final_p)\n\t\tconnected = 0;\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr)) {\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\t\tconnected = 0;\n\t} else if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_sk_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\t/* Lockless fast path for the non-corking case */\n\tif (!corkreq) {\n\t\tstruct sk_buff *skb;\n\n\t\tskb = ip6_make_skb(sk, getfrag, msg, ulen,\n\t\t\t\t   sizeof(struct udphdr), hlimit, tclass, opt,\n\t\t\t\t   &fl6, (struct rt6_info *)dst,\n\t\t\t\t   msg->msg_flags, dontfrag);\n\t\terr = PTR_ERR(skb);\n\t\tif (!IS_ERR_OR_NULL(skb))\n\t\t\terr = udp_v6_send_skb(skb, &fl6);\n\t\tgoto release_dst;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tnet_dbg_ratelimited(\"udp cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tup->pending = AF_INET6;\n\ndo_append_data:\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\tup->len += ulen;\n\terr = ip6_append_data(sk, getfrag, msg, ulen,\n\t\tsizeof(struct udphdr), hlimit, tclass, opt, &fl6,\n\t\t(struct rt6_info *)dst,\n\t\tcorkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags, dontfrag);\n\tif (err)\n\t\tudp_v6_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_v6_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\n\tif (err > 0)\n\t\terr = np->recverr ? net_xmit_errno(err) : 0;\n\trelease_sock(sk);\n\nrelease_dst:\n\tif (dst) {\n\t\tif (connected) {\n\t\t\tip6_dst_store(sk, dst,\n\t\t\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t\t\t      &np->saddr :\n#endif\n\t\t\t\t      NULL);\n\t\t} else {\n\t\t\tdst_release(dst);\n\t\t}\n\t\tdst = NULL;\n\t}\n\nout:\n\tdst_release(dst);\n\tfl6_sock_release(flowlabel);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint transhdrlen = 4; /* zero session-id */\n\tint ulen = len + transhdrlen;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (lsa) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (lsa->l2tp_family && lsa->l2tp_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\tdaddr = &lsa->l2tp_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = lsa->l2tp_flowinfo & IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (flowlabel == NULL)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    lsa->l2tp_scope_id &&\n\t\t    ipv6_addr_type(daddr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tfl6.flowi6_oif = lsa->l2tp_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel & IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\n\tif (opt == NULL)\n\t\topt = np->opt;\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tlock_sock(sk);\n\terr = ip6_append_data(sk, ip_generic_getfrag, msg,\n\t\t\t      ulen, transhdrlen, hlimit, tclass, opt,\n\t\t\t      &fl6, (struct rt6_info *)dst,\n\t\t\t      msg->msg_flags, dontfrag);\n\tif (err)\n\t\tip6_flush_pending_frames(sk);\n\telse if (!(msg->msg_flags & MSG_MORE))\n\t\terr = l2tp_ip6_push_pending_frames(sk);\n\trelease_sock(sk);\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\n\treturn err < 0 ? err : len;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint transhdrlen = 4; /* zero session-id */\n\tint ulen = len + transhdrlen;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (lsa) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (lsa->l2tp_family && lsa->l2tp_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\tdaddr = &lsa->l2tp_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = lsa->l2tp_flowinfo & IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (flowlabel == NULL)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    lsa->l2tp_scope_id &&\n\t\t    ipv6_addr_type(daddr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tfl6.flowi6_oif = lsa->l2tp_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel & IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tlock_sock(sk);\n\terr = ip6_append_data(sk, ip_generic_getfrag, msg,\n\t\t\t      ulen, transhdrlen, hlimit, tclass, opt,\n\t\t\t      &fl6, (struct rt6_info *)dst,\n\t\t\t      msg->msg_flags, dontfrag);\n\tif (err)\n\t\tip6_flush_pending_frames(sk);\n\telse if (!(msg->msg_flags & MSG_MORE))\n\t\terr = l2tp_ip6_push_pending_frames(sk);\n\trelease_sock(sk);\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\n\treturn err < 0 ? err : len;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t*daddr, *final_p, final;\n\tstruct dst_entry\t*dst;\n\tstruct flowi6\t\tfl6;\n\tstruct ip6_flowlabel\t*flowlabel = NULL;\n\tstruct ipv6_txoptions\t*opt;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tusin->sin6_addr.s6_addr[15] = 0x01;\n\t}\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6.flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet->inet_dport;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tif (!fl6.flowi6_oif && (addr_type&IPV6_ADDR_MULTICAST))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\topt = flowlabel ? flowlabel->opt : np->opt;\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\terr = 0;\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\t/* source address lookup done in ip6_dst_lookup */\n\n\tif (ipv6_addr_any(&np->saddr))\n\t\tnp->saddr = fl6.saddr;\n\n\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tsk->sk_v6_rcv_saddr = fl6.saddr;\n\t\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\t\tif (sk->sk_prot->rehash)\n\t\t\tsk->sk_prot->rehash(sk);\n\t}\n\n\tip6_dst_store(sk, dst,\n\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t      &np->saddr :\n#endif\n\t\t      NULL);\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err;\n}",
                        "code_after_change": "static int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t*daddr, *final_p, final;\n\tstruct dst_entry\t*dst;\n\tstruct flowi6\t\tfl6;\n\tstruct ip6_flowlabel\t*flowlabel = NULL;\n\tstruct ipv6_txoptions\t*opt;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tusin->sin6_addr.s6_addr[15] = 0x01;\n\t}\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6.flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet->inet_dport;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tif (!fl6.flowi6_oif && (addr_type&IPV6_ADDR_MULTICAST))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\trcu_read_lock();\n\topt = flowlabel ? flowlabel->opt : rcu_dereference(np->opt);\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\trcu_read_unlock();\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\terr = 0;\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\t/* source address lookup done in ip6_dst_lookup */\n\n\tif (ipv6_addr_any(&np->saddr))\n\t\tnp->saddr = fl6.saddr;\n\n\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tsk->sk_v6_rcv_saddr = fl6.saddr;\n\t\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\t\tif (sk->sk_prot->rehash)\n\t\t\tsk->sk_prot->rehash(sk);\n\t}\n\n\tip6_dst_store(sk, dst,\n\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t      &np->saddr :\n#endif\n\t\t      NULL);\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err;\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nint udpv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_len = msg->msg_namelen;\n\tint ulen = len;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint err;\n\tint connected = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\n\t/* destination address check */\n\tif (sin6) {\n\t\tif (addr_len < offsetof(struct sockaddr, sa_data))\n\t\t\treturn -EINVAL;\n\n\t\tswitch (sin6->sin6_family) {\n\t\tcase AF_INET6:\n\t\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\t\treturn -EINVAL;\n\t\t\tdaddr = &sin6->sin6_addr;\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tgoto do_udp_sendmsg;\n\t\tcase AF_UNSPEC:\n\t\t\tmsg->msg_name = sin6 = NULL;\n\t\t\tmsg->msg_namelen = addr_len = 0;\n\t\t\tdaddr = NULL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (!up->pending) {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t} else\n\t\tdaddr = NULL;\n\n\tif (daddr) {\n\t\tif (ipv6_addr_v4mapped(daddr)) {\n\t\t\tstruct sockaddr_in sin;\n\t\t\tsin.sin_family = AF_INET;\n\t\t\tsin.sin_port = sin6 ? sin6->sin6_port : inet->inet_dport;\n\t\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\t\tmsg->msg_name = &sin;\n\t\t\tmsg->msg_namelen = sizeof(sin);\ndo_udp_sendmsg:\n\t\t\tif (__ipv6_only_sock(sk))\n\t\t\t\treturn -ENETUNREACH;\n\t\t\treturn udp_sendmsg(sk, msg, len);\n\t\t}\n\t}\n\n\tif (up->pending == AF_INET)\n\t\treturn udp_sendmsg(sk, msg, len);\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t   */\n\tif (len > INT_MAX - sizeof(struct udphdr))\n\t\treturn -EMSGSIZE;\n\n\tgetfrag  =  is_udplite ?  udplite_getfrag : ip_generic_getfrag;\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET6)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t\t}\n\t\t\tdst = NULL;\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (sin6) {\n\t\tif (sin6->sin6_port == 0)\n\t\t\treturn -EINVAL;\n\n\t\tfl6.fl6_dport = sin6->sin6_port;\n\t\tdaddr = &sin6->sin6_addr;\n\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tconnected = 1;\n\t}\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->sticky_pktinfo.ipi6_ifindex;\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(*opt);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t\tconnected = 0;\n\t}\n\tif (!opt)\n\t\topt = np->opt;\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\tif (final_p)\n\t\tconnected = 0;\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr)) {\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\t\tconnected = 0;\n\t} else if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_sk_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\t/* Lockless fast path for the non-corking case */\n\tif (!corkreq) {\n\t\tstruct sk_buff *skb;\n\n\t\tskb = ip6_make_skb(sk, getfrag, msg, ulen,\n\t\t\t\t   sizeof(struct udphdr), hlimit, tclass, opt,\n\t\t\t\t   &fl6, (struct rt6_info *)dst,\n\t\t\t\t   msg->msg_flags, dontfrag);\n\t\terr = PTR_ERR(skb);\n\t\tif (!IS_ERR_OR_NULL(skb))\n\t\t\terr = udp_v6_send_skb(skb, &fl6);\n\t\tgoto release_dst;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tnet_dbg_ratelimited(\"udp cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tup->pending = AF_INET6;\n\ndo_append_data:\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\tup->len += ulen;\n\terr = ip6_append_data(sk, getfrag, msg, ulen,\n\t\tsizeof(struct udphdr), hlimit, tclass, opt, &fl6,\n\t\t(struct rt6_info *)dst,\n\t\tcorkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags, dontfrag);\n\tif (err)\n\t\tudp_v6_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_v6_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\n\tif (err > 0)\n\t\terr = np->recverr ? net_xmit_errno(err) : 0;\n\trelease_sock(sk);\n\nrelease_dst:\n\tif (dst) {\n\t\tif (connected) {\n\t\t\tip6_dst_store(sk, dst,\n\t\t\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t\t\t      &np->saddr :\n#endif\n\t\t\t\t      NULL);\n\t\t} else {\n\t\t\tdst_release(dst);\n\t\t}\n\t\tdst = NULL;\n\t}\n\nout:\n\tdst_release(dst);\n\tfl6_sock_release(flowlabel);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t*daddr, *final_p, final;\n\tstruct dst_entry\t*dst;\n\tstruct flowi6\t\tfl6;\n\tstruct ip6_flowlabel\t*flowlabel = NULL;\n\tstruct ipv6_txoptions\t*opt;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tusin->sin6_addr.s6_addr[15] = 0x01;\n\t}\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6.flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet->inet_dport;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tif (!fl6.flowi6_oif && (addr_type&IPV6_ADDR_MULTICAST))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\topt = flowlabel ? flowlabel->opt : np->opt;\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\terr = 0;\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\t/* source address lookup done in ip6_dst_lookup */\n\n\tif (ipv6_addr_any(&np->saddr))\n\t\tnp->saddr = fl6.saddr;\n\n\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tsk->sk_v6_rcv_saddr = fl6.saddr;\n\t\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\t\tif (sk->sk_prot->rehash)\n\t\t\tsk->sk_prot->rehash(sk);\n\t}\n\n\tip6_dst_store(sk, dst,\n\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t      &np->saddr :\n#endif\n\t\t      NULL);\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t*daddr, *final_p, final;\n\tstruct dst_entry\t*dst;\n\tstruct flowi6\t\tfl6;\n\tstruct ip6_flowlabel\t*flowlabel = NULL;\n\tstruct ipv6_txoptions\t*opt;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tusin->sin6_addr.s6_addr[15] = 0x01;\n\t}\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6.flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet->inet_dport;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tif (!fl6.flowi6_oif && (addr_type&IPV6_ADDR_MULTICAST))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\trcu_read_lock();\n\topt = flowlabel ? flowlabel->opt : rcu_dereference(np->opt);\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\trcu_read_unlock();\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\terr = 0;\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\t/* source address lookup done in ip6_dst_lookup */\n\n\tif (ipv6_addr_any(&np->saddr))\n\t\tnp->saddr = fl6.saddr;\n\n\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tsk->sk_v6_rcv_saddr = fl6.saddr;\n\t\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\t\tif (sk->sk_prot->rehash)\n\t\t\tsk->sk_prot->rehash(sk);\n\t}\n\n\tip6_dst_store(sk, dst,\n\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t      &np->saddr :\n#endif\n\t\t      NULL);\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t  int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\n\t/*\n\t *\tconnect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 0x1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type&IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp &&\n\t    !ipv6_addr_equal(&sk->sk_v6_daddr, &usin->sin6_addr)) {\n\t\ttp->rx_opt.ts_recent = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq = 0;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t *\tTCP over IPv4\n\t */\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &ipv6_mapped;\n\t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\ttp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\terr = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &ipv6_specific;\n\t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (!saddr) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp &&\n\t    ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr))\n\t\ttcp_fetch_timewait_stamp(sk, dst);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (np->opt)\n\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n\t\t\t\t\t  np->opt->opt_nflen);\n\n\ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet6_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tsk_set_txhash(sk);\n\n\tif (!tp->write_seq && likely(!tp->repair))\n\t\ttp->write_seq = secure_tcpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t\t     sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t\t     inet->inet_sport,\n\t\t\t\t\t\t\t     inet->inet_dport);\n\n\terr = tcp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\ttcp_set_state(sk, TCP_CLOSE);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
                        "code_after_change": "static int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t  int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct ipv6_txoptions *opt;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\n\t/*\n\t *\tconnect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 0x1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type&IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp &&\n\t    !ipv6_addr_equal(&sk->sk_v6_daddr, &usin->sin6_addr)) {\n\t\ttp->rx_opt.ts_recent = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq = 0;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t *\tTCP over IPv4\n\t */\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &ipv6_mapped;\n\t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\ttp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\terr = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &ipv6_specific;\n\t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (!saddr) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp &&\n\t    ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr))\n\t\ttcp_fetch_timewait_stamp(sk, dst);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len = opt->opt_flen +\n\t\t\t\t\t opt->opt_nflen;\n\n\ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet6_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tsk_set_txhash(sk);\n\n\tif (!tp->write_seq && likely(!tp->repair))\n\t\ttp->write_seq = secure_tcpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t\t     sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t\t     inet->inet_sport,\n\t\t\t\t\t\t\t     inet->inet_dport);\n\n\terr = tcp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\ttcp_set_state(sk, TCP_CLOSE);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nint udpv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_len = msg->msg_namelen;\n\tint ulen = len;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint err;\n\tint connected = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\n\t/* destination address check */\n\tif (sin6) {\n\t\tif (addr_len < offsetof(struct sockaddr, sa_data))\n\t\t\treturn -EINVAL;\n\n\t\tswitch (sin6->sin6_family) {\n\t\tcase AF_INET6:\n\t\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\t\treturn -EINVAL;\n\t\t\tdaddr = &sin6->sin6_addr;\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tgoto do_udp_sendmsg;\n\t\tcase AF_UNSPEC:\n\t\t\tmsg->msg_name = sin6 = NULL;\n\t\t\tmsg->msg_namelen = addr_len = 0;\n\t\t\tdaddr = NULL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (!up->pending) {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t} else\n\t\tdaddr = NULL;\n\n\tif (daddr) {\n\t\tif (ipv6_addr_v4mapped(daddr)) {\n\t\t\tstruct sockaddr_in sin;\n\t\t\tsin.sin_family = AF_INET;\n\t\t\tsin.sin_port = sin6 ? sin6->sin6_port : inet->inet_dport;\n\t\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\t\tmsg->msg_name = &sin;\n\t\t\tmsg->msg_namelen = sizeof(sin);\ndo_udp_sendmsg:\n\t\t\tif (__ipv6_only_sock(sk))\n\t\t\t\treturn -ENETUNREACH;\n\t\t\treturn udp_sendmsg(sk, msg, len);\n\t\t}\n\t}\n\n\tif (up->pending == AF_INET)\n\t\treturn udp_sendmsg(sk, msg, len);\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t   */\n\tif (len > INT_MAX - sizeof(struct udphdr))\n\t\treturn -EMSGSIZE;\n\n\tgetfrag  =  is_udplite ?  udplite_getfrag : ip_generic_getfrag;\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET6)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t\t}\n\t\t\tdst = NULL;\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (sin6) {\n\t\tif (sin6->sin6_port == 0)\n\t\t\treturn -EINVAL;\n\n\t\tfl6.fl6_dport = sin6->sin6_port;\n\t\tdaddr = &sin6->sin6_addr;\n\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tconnected = 1;\n\t}\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->sticky_pktinfo.ipi6_ifindex;\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(*opt);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t\tconnected = 0;\n\t}\n\tif (!opt)\n\t\topt = np->opt;\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\tif (final_p)\n\t\tconnected = 0;\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr)) {\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\t\tconnected = 0;\n\t} else if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_sk_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\t/* Lockless fast path for the non-corking case */\n\tif (!corkreq) {\n\t\tstruct sk_buff *skb;\n\n\t\tskb = ip6_make_skb(sk, getfrag, msg, ulen,\n\t\t\t\t   sizeof(struct udphdr), hlimit, tclass, opt,\n\t\t\t\t   &fl6, (struct rt6_info *)dst,\n\t\t\t\t   msg->msg_flags, dontfrag);\n\t\terr = PTR_ERR(skb);\n\t\tif (!IS_ERR_OR_NULL(skb))\n\t\t\terr = udp_v6_send_skb(skb, &fl6);\n\t\tgoto release_dst;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tnet_dbg_ratelimited(\"udp cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tup->pending = AF_INET6;\n\ndo_append_data:\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\tup->len += ulen;\n\terr = ip6_append_data(sk, getfrag, msg, ulen,\n\t\tsizeof(struct udphdr), hlimit, tclass, opt, &fl6,\n\t\t(struct rt6_info *)dst,\n\t\tcorkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags, dontfrag);\n\tif (err)\n\t\tudp_v6_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_v6_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\n\tif (err > 0)\n\t\terr = np->recverr ? net_xmit_errno(err) : 0;\n\trelease_sock(sk);\n\nrelease_dst:\n\tif (dst) {\n\t\tif (connected) {\n\t\t\tip6_dst_store(sk, dst,\n\t\t\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t\t\t      &np->saddr :\n#endif\n\t\t\t\t      NULL);\n\t\t} else {\n\t\t\tdst_release(dst);\n\t\t}\n\t\tdst = NULL;\n\t}\n\nout:\n\tdst_release(dst);\n\tfl6_sock_release(flowlabel);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t  int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\n\t/*\n\t *\tconnect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 0x1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type&IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp &&\n\t    !ipv6_addr_equal(&sk->sk_v6_daddr, &usin->sin6_addr)) {\n\t\ttp->rx_opt.ts_recent = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq = 0;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t *\tTCP over IPv4\n\t */\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &ipv6_mapped;\n\t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\ttp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\terr = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &ipv6_specific;\n\t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (!saddr) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp &&\n\t    ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr))\n\t\ttcp_fetch_timewait_stamp(sk, dst);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (np->opt)\n\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n\t\t\t\t\t  np->opt->opt_nflen);\n\n\ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet6_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tsk_set_txhash(sk);\n\n\tif (!tp->write_seq && likely(!tp->repair))\n\t\ttp->write_seq = secure_tcpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t\t     sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t\t     inet->inet_sport,\n\t\t\t\t\t\t\t     inet->inet_dport);\n\n\terr = tcp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\ttcp_set_state(sk, TCP_CLOSE);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t  int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct ipv6_txoptions *opt;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\n\t/*\n\t *\tconnect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 0x1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type&IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp &&\n\t    !ipv6_addr_equal(&sk->sk_v6_daddr, &usin->sin6_addr)) {\n\t\ttp->rx_opt.ts_recent = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq = 0;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t *\tTCP over IPv4\n\t */\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &ipv6_mapped;\n\t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\ttp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\terr = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &ipv6_specific;\n\t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (!saddr) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp &&\n\t    ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr))\n\t\ttcp_fetch_timewait_stamp(sk, dst);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len = opt->opt_flen +\n\t\t\t\t\t opt->opt_nflen;\n\n\ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet6_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tsk_set_txhash(sk);\n\n\tif (!tp->write_seq && likely(!tp->repair))\n\t\ttp->write_seq = secure_tcpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t\t     sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t\t     inet->inet_sport,\n\t\t\t\t\t\t\t     inet->inet_dport);\n\n\terr = tcp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\ttcp_set_state(sk, TCP_CLOSE);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 452,
            "cve_id": "CVE-2014-0181",
            "code_snippet": "static int addr_doit(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct ifaddrmsg *ifm;\n\tint err;\n\tu8 pnaddr;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tASSERT_RTNL();\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, ifa_phonet_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tifm = nlmsg_data(nlh);\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\tpnaddr = nla_get_u8(tb[IFA_LOCAL]);\n\tif (pnaddr & 3)\n\t\t/* Phonet addresses only have 6 high-order bits */\n\t\treturn -EINVAL;\n\n\tdev = __dev_get_by_index(net, ifm->ifa_index);\n\tif (dev == NULL)\n\t\treturn -ENODEV;\n\n\tif (nlh->nlmsg_type == RTM_NEWADDR)\n\t\terr = phonet_address_add(dev, pnaddr);\n\telse\n\t\terr = phonet_address_del(dev, pnaddr);\n\tif (!err)\n\t\tphonet_address_notify(nlh->nlmsg_type, dev, pnaddr);\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}",
                        "code_after_change": "static int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int addr_doit(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct ifaddrmsg *ifm;\n\tint err;\n\tu8 pnaddr;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tASSERT_RTNL();\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, ifa_phonet_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tifm = nlmsg_data(nlh);\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\tpnaddr = nla_get_u8(tb[IFA_LOCAL]);\n\tif (pnaddr & 3)\n\t\t/* Phonet addresses only have 6 high-order bits */\n\t\treturn -EINVAL;\n\n\tdev = __dev_get_by_index(net, ifm->ifa_index);\n\tif (dev == NULL)\n\t\treturn -ENODEV;\n\n\tif (nlh->nlmsg_type == RTM_NEWADDR)\n\t\terr = phonet_address_add(dev, pnaddr);\n\telse\n\t\terr = phonet_address_del(dev, pnaddr);\n\tif (!err)\n\t\tphonet_address_notify(nlh->nlmsg_type, dev, pnaddr);\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}",
                        "code_after_change": "static int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int addr_doit(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct ifaddrmsg *ifm;\n\tint err;\n\tu8 pnaddr;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tASSERT_RTNL();\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, ifa_phonet_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tifm = nlmsg_data(nlh);\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\tpnaddr = nla_get_u8(tb[IFA_LOCAL]);\n\tif (pnaddr & 3)\n\t\t/* Phonet addresses only have 6 high-order bits */\n\t\treturn -EINVAL;\n\n\tdev = __dev_get_by_index(net, ifm->ifa_index);\n\tif (dev == NULL)\n\t\treturn -ENODEV;\n\n\tif (nlh->nlmsg_type == RTM_NEWADDR)\n\t\terr = phonet_address_add(dev, pnaddr);\n\telse\n\t\terr = phonet_address_del(dev, pnaddr);\n\tif (!err)\n\t\tphonet_address_notify(nlh->nlmsg_type, dev, pnaddr);\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}",
                        "code_after_change": "static int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(skb, net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int addr_doit(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct ifaddrmsg *ifm;\n\tint err;\n\tu8 pnaddr;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tASSERT_RTNL();\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, ifa_phonet_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tifm = nlmsg_data(nlh);\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\tpnaddr = nla_get_u8(tb[IFA_LOCAL]);\n\tif (pnaddr & 3)\n\t\t/* Phonet addresses only have 6 high-order bits */\n\t\treturn -EINVAL;\n\n\tdev = __dev_get_by_index(net, ifm->ifa_index);\n\tif (dev == NULL)\n\t\treturn -ENODEV;\n\n\tif (nlh->nlmsg_type == RTM_NEWADDR)\n\t\terr = phonet_address_add(dev, pnaddr);\n\telse\n\t\terr = phonet_address_del(dev, pnaddr);\n\tif (!err)\n\t\tphonet_address_notify(nlh->nlmsg_type, dev, pnaddr);\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(skb, net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 441,
            "cve_id": "CVE-2014-0181",
            "code_snippet": "static int rtnl_fdb_del(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct ndmsg *ndm;\n\tstruct nlattr *tb[NDA_MAX+1];\n\tstruct net_device *dev;\n\tint err = -EINVAL;\n\t__u8 *addr;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(nlh, sizeof(*ndm), tb, NDA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tndm = nlmsg_data(nlh);\n\tif (ndm->ndm_ifindex == 0) {\n\t\tpr_info(\"PF_BRIDGE: RTM_DELNEIGH with invalid ifindex\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tdev = __dev_get_by_index(net, ndm->ndm_ifindex);\n\tif (dev == NULL) {\n\t\tpr_info(\"PF_BRIDGE: RTM_DELNEIGH with unknown ifindex\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (!tb[NDA_LLADDR] || nla_len(tb[NDA_LLADDR]) != ETH_ALEN) {\n\t\tpr_info(\"PF_BRIDGE: RTM_DELNEIGH with invalid address\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\taddr = nla_data(tb[NDA_LLADDR]);\n\n\terr = -EOPNOTSUPP;\n\n\t/* Support fdb on master device the net/bridge default case */\n\tif ((!ndm->ndm_flags || ndm->ndm_flags & NTF_MASTER) &&\n\t    (dev->priv_flags & IFF_BRIDGE_PORT)) {\n\t\tstruct net_device *br_dev = netdev_master_upper_dev_get(dev);\n\t\tconst struct net_device_ops *ops = br_dev->netdev_ops;\n\n\t\tif (ops->ndo_fdb_del)\n\t\t\terr = ops->ndo_fdb_del(ndm, tb, dev, addr);\n\n\t\tif (err)\n\t\t\tgoto out;\n\t\telse\n\t\t\tndm->ndm_flags &= ~NTF_MASTER;\n\t}\n\n\t/* Embedded bridge, macvlan, and any other device support */\n\tif (ndm->ndm_flags & NTF_SELF) {\n\t\tif (dev->netdev_ops->ndo_fdb_del)\n\t\t\terr = dev->netdev_ops->ndo_fdb_del(ndm, tb, dev, addr);\n\t\telse\n\t\t\terr = ndo_dflt_fdb_del(ndm, tb, dev, addr);\n\n\t\tif (!err) {\n\t\t\trtnl_fdb_notify(dev, addr, RTM_DELNEIGH);\n\t\t\tndm->ndm_flags &= ~NTF_SELF;\n\t\t}\n\t}\nout:\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}",
                        "code_after_change": "static int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(skb, net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int rtnl_fdb_del(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct ndmsg *ndm;\n\tstruct nlattr *tb[NDA_MAX+1];\n\tstruct net_device *dev;\n\tint err = -EINVAL;\n\t__u8 *addr;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(nlh, sizeof(*ndm), tb, NDA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tndm = nlmsg_data(nlh);\n\tif (ndm->ndm_ifindex == 0) {\n\t\tpr_info(\"PF_BRIDGE: RTM_DELNEIGH with invalid ifindex\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tdev = __dev_get_by_index(net, ndm->ndm_ifindex);\n\tif (dev == NULL) {\n\t\tpr_info(\"PF_BRIDGE: RTM_DELNEIGH with unknown ifindex\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (!tb[NDA_LLADDR] || nla_len(tb[NDA_LLADDR]) != ETH_ALEN) {\n\t\tpr_info(\"PF_BRIDGE: RTM_DELNEIGH with invalid address\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\taddr = nla_data(tb[NDA_LLADDR]);\n\n\terr = -EOPNOTSUPP;\n\n\t/* Support fdb on master device the net/bridge default case */\n\tif ((!ndm->ndm_flags || ndm->ndm_flags & NTF_MASTER) &&\n\t    (dev->priv_flags & IFF_BRIDGE_PORT)) {\n\t\tstruct net_device *br_dev = netdev_master_upper_dev_get(dev);\n\t\tconst struct net_device_ops *ops = br_dev->netdev_ops;\n\n\t\tif (ops->ndo_fdb_del)\n\t\t\terr = ops->ndo_fdb_del(ndm, tb, dev, addr);\n\n\t\tif (err)\n\t\t\tgoto out;\n\t\telse\n\t\t\tndm->ndm_flags &= ~NTF_MASTER;\n\t}\n\n\t/* Embedded bridge, macvlan, and any other device support */\n\tif (ndm->ndm_flags & NTF_SELF) {\n\t\tif (dev->netdev_ops->ndo_fdb_del)\n\t\t\terr = dev->netdev_ops->ndo_fdb_del(ndm, tb, dev, addr);\n\t\telse\n\t\t\terr = ndo_dflt_fdb_del(ndm, tb, dev, addr);\n\n\t\tif (!err) {\n\t\t\trtnl_fdb_notify(dev, addr, RTM_DELNEIGH);\n\t\t\tndm->ndm_flags &= ~NTF_SELF;\n\t\t}\n\t}\nout:\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(skb, net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}",
                        "code_after_change": "static int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int rtnl_fdb_del(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct ndmsg *ndm;\n\tstruct nlattr *tb[NDA_MAX+1];\n\tstruct net_device *dev;\n\tint err = -EINVAL;\n\t__u8 *addr;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(nlh, sizeof(*ndm), tb, NDA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tndm = nlmsg_data(nlh);\n\tif (ndm->ndm_ifindex == 0) {\n\t\tpr_info(\"PF_BRIDGE: RTM_DELNEIGH with invalid ifindex\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tdev = __dev_get_by_index(net, ndm->ndm_ifindex);\n\tif (dev == NULL) {\n\t\tpr_info(\"PF_BRIDGE: RTM_DELNEIGH with unknown ifindex\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (!tb[NDA_LLADDR] || nla_len(tb[NDA_LLADDR]) != ETH_ALEN) {\n\t\tpr_info(\"PF_BRIDGE: RTM_DELNEIGH with invalid address\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\taddr = nla_data(tb[NDA_LLADDR]);\n\n\terr = -EOPNOTSUPP;\n\n\t/* Support fdb on master device the net/bridge default case */\n\tif ((!ndm->ndm_flags || ndm->ndm_flags & NTF_MASTER) &&\n\t    (dev->priv_flags & IFF_BRIDGE_PORT)) {\n\t\tstruct net_device *br_dev = netdev_master_upper_dev_get(dev);\n\t\tconst struct net_device_ops *ops = br_dev->netdev_ops;\n\n\t\tif (ops->ndo_fdb_del)\n\t\t\terr = ops->ndo_fdb_del(ndm, tb, dev, addr);\n\n\t\tif (err)\n\t\t\tgoto out;\n\t\telse\n\t\t\tndm->ndm_flags &= ~NTF_MASTER;\n\t}\n\n\t/* Embedded bridge, macvlan, and any other device support */\n\tif (ndm->ndm_flags & NTF_SELF) {\n\t\tif (dev->netdev_ops->ndo_fdb_del)\n\t\t\terr = dev->netdev_ops->ndo_fdb_del(ndm, tb, dev, addr);\n\t\telse\n\t\t\terr = ndo_dflt_fdb_del(ndm, tb, dev, addr);\n\n\t\tif (!err) {\n\t\t\trtnl_fdb_notify(dev, addr, RTM_DELNEIGH);\n\t\t\tndm->ndm_flags &= ~NTF_SELF;\n\t\t}\n\t}\nout:\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}",
                        "code_after_change": "static int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int rtnl_fdb_del(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct ndmsg *ndm;\n\tstruct nlattr *tb[NDA_MAX+1];\n\tstruct net_device *dev;\n\tint err = -EINVAL;\n\t__u8 *addr;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(nlh, sizeof(*ndm), tb, NDA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tndm = nlmsg_data(nlh);\n\tif (ndm->ndm_ifindex == 0) {\n\t\tpr_info(\"PF_BRIDGE: RTM_DELNEIGH with invalid ifindex\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tdev = __dev_get_by_index(net, ndm->ndm_ifindex);\n\tif (dev == NULL) {\n\t\tpr_info(\"PF_BRIDGE: RTM_DELNEIGH with unknown ifindex\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (!tb[NDA_LLADDR] || nla_len(tb[NDA_LLADDR]) != ETH_ALEN) {\n\t\tpr_info(\"PF_BRIDGE: RTM_DELNEIGH with invalid address\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\taddr = nla_data(tb[NDA_LLADDR]);\n\n\terr = -EOPNOTSUPP;\n\n\t/* Support fdb on master device the net/bridge default case */\n\tif ((!ndm->ndm_flags || ndm->ndm_flags & NTF_MASTER) &&\n\t    (dev->priv_flags & IFF_BRIDGE_PORT)) {\n\t\tstruct net_device *br_dev = netdev_master_upper_dev_get(dev);\n\t\tconst struct net_device_ops *ops = br_dev->netdev_ops;\n\n\t\tif (ops->ndo_fdb_del)\n\t\t\terr = ops->ndo_fdb_del(ndm, tb, dev, addr);\n\n\t\tif (err)\n\t\t\tgoto out;\n\t\telse\n\t\t\tndm->ndm_flags &= ~NTF_MASTER;\n\t}\n\n\t/* Embedded bridge, macvlan, and any other device support */\n\tif (ndm->ndm_flags & NTF_SELF) {\n\t\tif (dev->netdev_ops->ndo_fdb_del)\n\t\t\terr = dev->netdev_ops->ndo_fdb_del(ndm, tb, dev, addr);\n\t\telse\n\t\t\terr = ndo_dflt_fdb_del(ndm, tb, dev, addr);\n\n\t\tif (!err) {\n\t\t\trtnl_fdb_notify(dev, addr, RTM_DELNEIGH);\n\t\t\tndm->ndm_flags &= ~NTF_SELF;\n\t\t}\n\t}\nout:\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 443,
            "cve_id": "CVE-2014-0181",
            "code_snippet": "static int dcb_doit(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct net_device *netdev;\n\tstruct dcbmsg *dcb = nlmsg_data(nlh);\n\tstruct nlattr *tb[DCB_ATTR_MAX + 1];\n\tu32 portid = skb ? NETLINK_CB(skb).portid : 0;\n\tint ret = -EINVAL;\n\tstruct sk_buff *reply_skb;\n\tstruct nlmsghdr *reply_nlh = NULL;\n\tconst struct reply_func *fn;\n\n\tif ((nlh->nlmsg_type == RTM_SETDCB) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tret = nlmsg_parse(nlh, sizeof(*dcb), tb, DCB_ATTR_MAX,\n\t\t\t  dcbnl_rtnl_policy);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (dcb->cmd > DCB_CMD_MAX)\n\t\treturn -EINVAL;\n\n\t/* check if a reply function has been defined for the command */\n\tfn = &reply_funcs[dcb->cmd];\n\tif (!fn->cb)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!tb[DCB_ATTR_IFNAME])\n\t\treturn -EINVAL;\n\n\tnetdev = __dev_get_by_name(net, nla_data(tb[DCB_ATTR_IFNAME]));\n\tif (!netdev)\n\t\treturn -ENODEV;\n\n\tif (!netdev->dcbnl_ops)\n\t\treturn -EOPNOTSUPP;\n\n\treply_skb = dcbnl_newmsg(fn->type, dcb->cmd, portid, nlh->nlmsg_seq,\n\t\t\t\t nlh->nlmsg_flags, &reply_nlh);\n\tif (!reply_skb)\n\t\treturn -ENOBUFS;\n\n\tret = fn->cb(netdev, nlh, nlh->nlmsg_seq, tb, reply_skb);\n\tif (ret < 0) {\n\t\tnlmsg_free(reply_skb);\n\t\tgoto out;\n\t}\n\n\tnlmsg_end(reply_skb, reply_nlh);\n\n\tret = rtnl_unicast(reply_skb, net, portid);\nout:\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
                        "code_after_change": "static int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int dcb_doit(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct net_device *netdev;\n\tstruct dcbmsg *dcb = nlmsg_data(nlh);\n\tstruct nlattr *tb[DCB_ATTR_MAX + 1];\n\tu32 portid = skb ? NETLINK_CB(skb).portid : 0;\n\tint ret = -EINVAL;\n\tstruct sk_buff *reply_skb;\n\tstruct nlmsghdr *reply_nlh = NULL;\n\tconst struct reply_func *fn;\n\n\tif ((nlh->nlmsg_type == RTM_SETDCB) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tret = nlmsg_parse(nlh, sizeof(*dcb), tb, DCB_ATTR_MAX,\n\t\t\t  dcbnl_rtnl_policy);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (dcb->cmd > DCB_CMD_MAX)\n\t\treturn -EINVAL;\n\n\t/* check if a reply function has been defined for the command */\n\tfn = &reply_funcs[dcb->cmd];\n\tif (!fn->cb)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!tb[DCB_ATTR_IFNAME])\n\t\treturn -EINVAL;\n\n\tnetdev = __dev_get_by_name(net, nla_data(tb[DCB_ATTR_IFNAME]));\n\tif (!netdev)\n\t\treturn -ENODEV;\n\n\tif (!netdev->dcbnl_ops)\n\t\treturn -EOPNOTSUPP;\n\n\treply_skb = dcbnl_newmsg(fn->type, dcb->cmd, portid, nlh->nlmsg_seq,\n\t\t\t\t nlh->nlmsg_flags, &reply_nlh);\n\tif (!reply_skb)\n\t\treturn -ENOBUFS;\n\n\tret = fn->cb(netdev, nlh, nlh->nlmsg_seq, tb, reply_skb);\n\tif (ret < 0) {\n\t\tnlmsg_free(reply_skb);\n\t\tgoto out;\n\t}\n\n\tnlmsg_end(reply_skb, reply_nlh);\n\n\tret = rtnl_unicast(reply_skb, net, portid);\nout:\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}",
                        "code_after_change": "static int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int dcb_doit(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct net_device *netdev;\n\tstruct dcbmsg *dcb = nlmsg_data(nlh);\n\tstruct nlattr *tb[DCB_ATTR_MAX + 1];\n\tu32 portid = skb ? NETLINK_CB(skb).portid : 0;\n\tint ret = -EINVAL;\n\tstruct sk_buff *reply_skb;\n\tstruct nlmsghdr *reply_nlh = NULL;\n\tconst struct reply_func *fn;\n\n\tif ((nlh->nlmsg_type == RTM_SETDCB) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tret = nlmsg_parse(nlh, sizeof(*dcb), tb, DCB_ATTR_MAX,\n\t\t\t  dcbnl_rtnl_policy);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (dcb->cmd > DCB_CMD_MAX)\n\t\treturn -EINVAL;\n\n\t/* check if a reply function has been defined for the command */\n\tfn = &reply_funcs[dcb->cmd];\n\tif (!fn->cb)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!tb[DCB_ATTR_IFNAME])\n\t\treturn -EINVAL;\n\n\tnetdev = __dev_get_by_name(net, nla_data(tb[DCB_ATTR_IFNAME]));\n\tif (!netdev)\n\t\treturn -ENODEV;\n\n\tif (!netdev->dcbnl_ops)\n\t\treturn -EOPNOTSUPP;\n\n\treply_skb = dcbnl_newmsg(fn->type, dcb->cmd, portid, nlh->nlmsg_seq,\n\t\t\t\t nlh->nlmsg_flags, &reply_nlh);\n\tif (!reply_skb)\n\t\treturn -ENOBUFS;\n\n\tret = fn->cb(netdev, nlh, nlh->nlmsg_seq, tb, reply_skb);\n\tif (ret < 0) {\n\t\tnlmsg_free(reply_skb);\n\t\tgoto out;\n\t}\n\n\tnlmsg_end(reply_skb, reply_nlh);\n\n\tret = rtnl_unicast(reply_skb, net, portid);\nout:\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}",
                        "code_after_change": "static int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int dcb_doit(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct net_device *netdev;\n\tstruct dcbmsg *dcb = nlmsg_data(nlh);\n\tstruct nlattr *tb[DCB_ATTR_MAX + 1];\n\tu32 portid = skb ? NETLINK_CB(skb).portid : 0;\n\tint ret = -EINVAL;\n\tstruct sk_buff *reply_skb;\n\tstruct nlmsghdr *reply_nlh = NULL;\n\tconst struct reply_func *fn;\n\n\tif ((nlh->nlmsg_type == RTM_SETDCB) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tret = nlmsg_parse(nlh, sizeof(*dcb), tb, DCB_ATTR_MAX,\n\t\t\t  dcbnl_rtnl_policy);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (dcb->cmd > DCB_CMD_MAX)\n\t\treturn -EINVAL;\n\n\t/* check if a reply function has been defined for the command */\n\tfn = &reply_funcs[dcb->cmd];\n\tif (!fn->cb)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!tb[DCB_ATTR_IFNAME])\n\t\treturn -EINVAL;\n\n\tnetdev = __dev_get_by_name(net, nla_data(tb[DCB_ATTR_IFNAME]));\n\tif (!netdev)\n\t\treturn -ENODEV;\n\n\tif (!netdev->dcbnl_ops)\n\t\treturn -EOPNOTSUPP;\n\n\treply_skb = dcbnl_newmsg(fn->type, dcb->cmd, portid, nlh->nlmsg_seq,\n\t\t\t\t nlh->nlmsg_flags, &reply_nlh);\n\tif (!reply_skb)\n\t\treturn -ENOBUFS;\n\n\tret = fn->cb(netdev, nlh, nlh->nlmsg_seq, tb, reply_skb);\n\tif (ret < 0) {\n\t\tnlmsg_free(reply_skb);\n\t\tgoto out;\n\t}\n\n\tnlmsg_end(reply_skb, reply_nlh);\n\n\tret = rtnl_unicast(reply_skb, net, portid);\nout:\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 453,
            "cve_id": "CVE-2014-0181",
            "code_snippet": "static int route_doit(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[RTA_MAX+1];\n\tstruct net_device *dev;\n\tstruct rtmsg *rtm;\n\tint err;\n\tu8 dst;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tASSERT_RTNL();\n\n\terr = nlmsg_parse(nlh, sizeof(*rtm), tb, RTA_MAX, rtm_phonet_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\trtm = nlmsg_data(nlh);\n\tif (rtm->rtm_table != RT_TABLE_MAIN || rtm->rtm_type != RTN_UNICAST)\n\t\treturn -EINVAL;\n\tif (tb[RTA_DST] == NULL || tb[RTA_OIF] == NULL)\n\t\treturn -EINVAL;\n\tdst = nla_get_u8(tb[RTA_DST]);\n\tif (dst & 3) /* Phonet addresses only have 6 high-order bits */\n\t\treturn -EINVAL;\n\n\tdev = __dev_get_by_index(net, nla_get_u32(tb[RTA_OIF]));\n\tif (dev == NULL)\n\t\treturn -ENODEV;\n\n\tif (nlh->nlmsg_type == RTM_NEWROUTE)\n\t\terr = phonet_route_add(dev, dst);\n\telse\n\t\terr = phonet_route_del(dev, dst);\n\tif (!err)\n\t\trtm_phonet_notify(nlh->nlmsg_type, dev, dst);\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}",
                        "code_after_change": "static int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int route_doit(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[RTA_MAX+1];\n\tstruct net_device *dev;\n\tstruct rtmsg *rtm;\n\tint err;\n\tu8 dst;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tASSERT_RTNL();\n\n\terr = nlmsg_parse(nlh, sizeof(*rtm), tb, RTA_MAX, rtm_phonet_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\trtm = nlmsg_data(nlh);\n\tif (rtm->rtm_table != RT_TABLE_MAIN || rtm->rtm_type != RTN_UNICAST)\n\t\treturn -EINVAL;\n\tif (tb[RTA_DST] == NULL || tb[RTA_OIF] == NULL)\n\t\treturn -EINVAL;\n\tdst = nla_get_u8(tb[RTA_DST]);\n\tif (dst & 3) /* Phonet addresses only have 6 high-order bits */\n\t\treturn -EINVAL;\n\n\tdev = __dev_get_by_index(net, nla_get_u32(tb[RTA_OIF]));\n\tif (dev == NULL)\n\t\treturn -ENODEV;\n\n\tif (nlh->nlmsg_type == RTM_NEWROUTE)\n\t\terr = phonet_route_add(dev, dst);\n\telse\n\t\terr = phonet_route_del(dev, dst);\n\tif (!err)\n\t\trtm_phonet_notify(nlh->nlmsg_type, dev, dst);\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
                        "code_after_change": "static int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int route_doit(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[RTA_MAX+1];\n\tstruct net_device *dev;\n\tstruct rtmsg *rtm;\n\tint err;\n\tu8 dst;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tASSERT_RTNL();\n\n\terr = nlmsg_parse(nlh, sizeof(*rtm), tb, RTA_MAX, rtm_phonet_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\trtm = nlmsg_data(nlh);\n\tif (rtm->rtm_table != RT_TABLE_MAIN || rtm->rtm_type != RTN_UNICAST)\n\t\treturn -EINVAL;\n\tif (tb[RTA_DST] == NULL || tb[RTA_OIF] == NULL)\n\t\treturn -EINVAL;\n\tdst = nla_get_u8(tb[RTA_DST]);\n\tif (dst & 3) /* Phonet addresses only have 6 high-order bits */\n\t\treturn -EINVAL;\n\n\tdev = __dev_get_by_index(net, nla_get_u32(tb[RTA_OIF]));\n\tif (dev == NULL)\n\t\treturn -ENODEV;\n\n\tif (nlh->nlmsg_type == RTM_NEWROUTE)\n\t\terr = phonet_route_add(dev, dst);\n\telse\n\t\terr = phonet_route_del(dev, dst);\n\tif (!err)\n\t\trtm_phonet_notify(nlh->nlmsg_type, dev, dst);\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}",
                        "code_after_change": "static int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(skb, net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int route_doit(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[RTA_MAX+1];\n\tstruct net_device *dev;\n\tstruct rtmsg *rtm;\n\tint err;\n\tu8 dst;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tASSERT_RTNL();\n\n\terr = nlmsg_parse(nlh, sizeof(*rtm), tb, RTA_MAX, rtm_phonet_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\trtm = nlmsg_data(nlh);\n\tif (rtm->rtm_table != RT_TABLE_MAIN || rtm->rtm_type != RTN_UNICAST)\n\t\treturn -EINVAL;\n\tif (tb[RTA_DST] == NULL || tb[RTA_OIF] == NULL)\n\t\treturn -EINVAL;\n\tdst = nla_get_u8(tb[RTA_DST]);\n\tif (dst & 3) /* Phonet addresses only have 6 high-order bits */\n\t\treturn -EINVAL;\n\n\tdev = __dev_get_by_index(net, nla_get_u32(tb[RTA_OIF]));\n\tif (dev == NULL)\n\t\treturn -ENODEV;\n\n\tif (nlh->nlmsg_type == RTM_NEWROUTE)\n\t\terr = phonet_route_add(dev, dst);\n\telse\n\t\terr = phonet_route_del(dev, dst);\n\tif (!err)\n\t\trtm_phonet_notify(nlh->nlmsg_type, dev, dst);\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(skb, net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 436,
            "cve_id": "CVE-2014-0181",
            "code_snippet": "static int audit_netlink_ok(struct sk_buff *skb, u16 msg_type)\n{\n\tint err = 0;\n\n\t/* Only support initial user namespace for now. */\n\t/*\n\t * We return ECONNREFUSED because it tricks userspace into thinking\n\t * that audit was not configured into the kernel.  Lots of users\n\t * configure their PAM stack (because that's what the distro does)\n\t * to reject login if unable to send messages to audit.  If we return\n\t * ECONNREFUSED the PAM stack thinks the kernel does not have audit\n\t * configured in and will let login proceed.  If we return EPERM\n\t * userspace will reject all logins.  This should be removed when we\n\t * support non init namespaces!!\n\t */\n\tif (current_user_ns() != &init_user_ns)\n\t\treturn -ECONNREFUSED;\n\n\tswitch (msg_type) {\n\tcase AUDIT_LIST:\n\tcase AUDIT_ADD:\n\tcase AUDIT_DEL:\n\t\treturn -EOPNOTSUPP;\n\tcase AUDIT_GET:\n\tcase AUDIT_SET:\n\tcase AUDIT_GET_FEATURE:\n\tcase AUDIT_SET_FEATURE:\n\tcase AUDIT_LIST_RULES:\n\tcase AUDIT_ADD_RULE:\n\tcase AUDIT_DEL_RULE:\n\tcase AUDIT_SIGNAL_INFO:\n\tcase AUDIT_TTY_GET:\n\tcase AUDIT_TTY_SET:\n\tcase AUDIT_TRIM:\n\tcase AUDIT_MAKE_EQUIV:\n\t\t/* Only support auditd and auditctl in initial pid namespace\n\t\t * for now. */\n\t\tif ((task_active_pid_ns(current) != &init_pid_ns))\n\t\t\treturn -EPERM;\n\n\t\tif (!capable(CAP_AUDIT_CONTROL))\n\t\t\terr = -EPERM;\n\t\tbreak;\n\tcase AUDIT_USER:\n\tcase AUDIT_FIRST_USER_MSG ... AUDIT_LAST_USER_MSG:\n\tcase AUDIT_FIRST_USER_MSG2 ... AUDIT_LAST_USER_MSG2:\n\t\tif (!capable(CAP_AUDIT_WRITE))\n\t\t\terr = -EPERM;\n\t\tbreak;\n\tdefault:  /* bad msg */\n\t\terr = -EINVAL;\n\t}\n\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static void cn_proc_mcast_ctl(struct cn_msg *msg,\n\t\t\t      struct netlink_skb_parms *nsp)\n{\n\tenum proc_cn_mcast_op *mc_op = NULL;\n\tint err = 0;\n\n\tif (msg->len != sizeof(*mc_op))\n\t\treturn;\n\n\t/* \n\t * Events are reported with respect to the initial pid\n\t * and user namespaces so ignore requestors from\n\t * other namespaces.\n\t */\n\tif ((current_user_ns() != &init_user_ns) ||\n\t    (task_active_pid_ns(current) != &init_pid_ns))\n\t\treturn;\n\n\t/* Can only change if privileged. */\n\tif (!capable(CAP_NET_ADMIN)) {\n\t\terr = EPERM;\n\t\tgoto out;\n\t}\n\n\tmc_op = (enum proc_cn_mcast_op *)msg->data;\n\tswitch (*mc_op) {\n\tcase PROC_CN_MCAST_LISTEN:\n\t\tatomic_inc(&proc_event_num_listeners);\n\t\tbreak;\n\tcase PROC_CN_MCAST_IGNORE:\n\t\tatomic_dec(&proc_event_num_listeners);\n\t\tbreak;\n\tdefault:\n\t\terr = EINVAL;\n\t\tbreak;\n\t}\n\nout:\n\tcn_proc_ack(err, msg->seq, msg->ack);\n}",
                        "code_after_change": "static void cn_proc_mcast_ctl(struct cn_msg *msg,\n\t\t\t      struct netlink_skb_parms *nsp)\n{\n\tenum proc_cn_mcast_op *mc_op = NULL;\n\tint err = 0;\n\n\tif (msg->len != sizeof(*mc_op))\n\t\treturn;\n\n\t/* \n\t * Events are reported with respect to the initial pid\n\t * and user namespaces so ignore requestors from\n\t * other namespaces.\n\t */\n\tif ((current_user_ns() != &init_user_ns) ||\n\t    (task_active_pid_ns(current) != &init_pid_ns))\n\t\treturn;\n\n\t/* Can only change if privileged. */\n\tif (!__netlink_ns_capable(nsp, &init_user_ns, CAP_NET_ADMIN)) {\n\t\terr = EPERM;\n\t\tgoto out;\n\t}\n\n\tmc_op = (enum proc_cn_mcast_op *)msg->data;\n\tswitch (*mc_op) {\n\tcase PROC_CN_MCAST_LISTEN:\n\t\tatomic_inc(&proc_event_num_listeners);\n\t\tbreak;\n\tcase PROC_CN_MCAST_IGNORE:\n\t\tatomic_dec(&proc_event_num_listeners);\n\t\tbreak;\n\tdefault:\n\t\terr = EINVAL;\n\t\tbreak;\n\t}\n\nout:\n\tcn_proc_ack(err, msg->seq, msg->ack);\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int audit_netlink_ok(struct sk_buff *skb, u16 msg_type)\n{\n\tint err = 0;\n\n\t/* Only support initial user namespace for now. */\n\t/*\n\t * We return ECONNREFUSED because it tricks userspace into thinking\n\t * that audit was not configured into the kernel.  Lots of users\n\t * configure their PAM stack (because that's what the distro does)\n\t * to reject login if unable to send messages to audit.  If we return\n\t * ECONNREFUSED the PAM stack thinks the kernel does not have audit\n\t * configured in and will let login proceed.  If we return EPERM\n\t * userspace will reject all logins.  This should be removed when we\n\t * support non init namespaces!!\n\t */\n\tif (current_user_ns() != &init_user_ns)\n\t\treturn -ECONNREFUSED;\n\n\tswitch (msg_type) {\n\tcase AUDIT_LIST:\n\tcase AUDIT_ADD:\n\tcase AUDIT_DEL:\n\t\treturn -EOPNOTSUPP;\n\tcase AUDIT_GET:\n\tcase AUDIT_SET:\n\tcase AUDIT_GET_FEATURE:\n\tcase AUDIT_SET_FEATURE:\n\tcase AUDIT_LIST_RULES:\n\tcase AUDIT_ADD_RULE:\n\tcase AUDIT_DEL_RULE:\n\tcase AUDIT_SIGNAL_INFO:\n\tcase AUDIT_TTY_GET:\n\tcase AUDIT_TTY_SET:\n\tcase AUDIT_TRIM:\n\tcase AUDIT_MAKE_EQUIV:\n\t\t/* Only support auditd and auditctl in initial pid namespace\n\t\t * for now. */\n\t\tif ((task_active_pid_ns(current) != &init_pid_ns))\n\t\t\treturn -EPERM;\n\n\t\tif (!capable(CAP_AUDIT_CONTROL))\n\t\t\terr = -EPERM;\n\t\tbreak;\n\tcase AUDIT_USER:\n\tcase AUDIT_FIRST_USER_MSG ... AUDIT_LAST_USER_MSG:\n\tcase AUDIT_FIRST_USER_MSG2 ... AUDIT_LAST_USER_MSG2:\n\t\tif (!capable(CAP_AUDIT_WRITE))\n\t\t\terr = -EPERM;\n\t\tbreak;\n\tdefault:  /* bad msg */\n\t\terr = -EINVAL;\n\t}\n\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic void cn_proc_mcast_ctl(struct cn_msg *msg,\n\t\t\t      struct netlink_skb_parms *nsp)\n{\n\tenum proc_cn_mcast_op *mc_op = NULL;\n\tint err = 0;\n\n\tif (msg->len != sizeof(*mc_op))\n\t\treturn;\n\n\t/* \n\t * Events are reported with respect to the initial pid\n\t * and user namespaces so ignore requestors from\n\t * other namespaces.\n\t */\n\tif ((current_user_ns() != &init_user_ns) ||\n\t    (task_active_pid_ns(current) != &init_pid_ns))\n\t\treturn;\n\n\t/* Can only change if privileged. */\n\tif (!capable(CAP_NET_ADMIN)) {\n\t\terr = EPERM;\n\t\tgoto out;\n\t}\n\n\tmc_op = (enum proc_cn_mcast_op *)msg->data;\n\tswitch (*mc_op) {\n\tcase PROC_CN_MCAST_LISTEN:\n\t\tatomic_inc(&proc_event_num_listeners);\n\t\tbreak;\n\tcase PROC_CN_MCAST_IGNORE:\n\t\tatomic_dec(&proc_event_num_listeners);\n\t\tbreak;\n\tdefault:\n\t\terr = EINVAL;\n\t\tbreak;\n\t}\n\nout:\n\tcn_proc_ack(err, msg->seq, msg->ack);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic void cn_proc_mcast_ctl(struct cn_msg *msg,\n\t\t\t      struct netlink_skb_parms *nsp)\n{\n\tenum proc_cn_mcast_op *mc_op = NULL;\n\tint err = 0;\n\n\tif (msg->len != sizeof(*mc_op))\n\t\treturn;\n\n\t/* \n\t * Events are reported with respect to the initial pid\n\t * and user namespaces so ignore requestors from\n\t * other namespaces.\n\t */\n\tif ((current_user_ns() != &init_user_ns) ||\n\t    (task_active_pid_ns(current) != &init_pid_ns))\n\t\treturn;\n\n\t/* Can only change if privileged. */\n\tif (!__netlink_ns_capable(nsp, &init_user_ns, CAP_NET_ADMIN)) {\n\t\terr = EPERM;\n\t\tgoto out;\n\t}\n\n\tmc_op = (enum proc_cn_mcast_op *)msg->data;\n\tswitch (*mc_op) {\n\tcase PROC_CN_MCAST_LISTEN:\n\t\tatomic_inc(&proc_event_num_listeners);\n\t\tbreak;\n\tcase PROC_CN_MCAST_IGNORE:\n\t\tatomic_dec(&proc_event_num_listeners);\n\t\tbreak;\n\tdefault:\n\t\terr = EINVAL;\n\t\tbreak;\n\t}\n\nout:\n\tcn_proc_ack(err, msg->seq, msg->ack);\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "asmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}",
                        "code_after_change": "asmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value[0] = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}",
                        "cve_id": "CVE-2014-9870"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int audit_netlink_ok(struct sk_buff *skb, u16 msg_type)\n{\n\tint err = 0;\n\n\t/* Only support initial user namespace for now. */\n\t/*\n\t * We return ECONNREFUSED because it tricks userspace into thinking\n\t * that audit was not configured into the kernel.  Lots of users\n\t * configure their PAM stack (because that's what the distro does)\n\t * to reject login if unable to send messages to audit.  If we return\n\t * ECONNREFUSED the PAM stack thinks the kernel does not have audit\n\t * configured in and will let login proceed.  If we return EPERM\n\t * userspace will reject all logins.  This should be removed when we\n\t * support non init namespaces!!\n\t */\n\tif (current_user_ns() != &init_user_ns)\n\t\treturn -ECONNREFUSED;\n\n\tswitch (msg_type) {\n\tcase AUDIT_LIST:\n\tcase AUDIT_ADD:\n\tcase AUDIT_DEL:\n\t\treturn -EOPNOTSUPP;\n\tcase AUDIT_GET:\n\tcase AUDIT_SET:\n\tcase AUDIT_GET_FEATURE:\n\tcase AUDIT_SET_FEATURE:\n\tcase AUDIT_LIST_RULES:\n\tcase AUDIT_ADD_RULE:\n\tcase AUDIT_DEL_RULE:\n\tcase AUDIT_SIGNAL_INFO:\n\tcase AUDIT_TTY_GET:\n\tcase AUDIT_TTY_SET:\n\tcase AUDIT_TRIM:\n\tcase AUDIT_MAKE_EQUIV:\n\t\t/* Only support auditd and auditctl in initial pid namespace\n\t\t * for now. */\n\t\tif ((task_active_pid_ns(current) != &init_pid_ns))\n\t\t\treturn -EPERM;\n\n\t\tif (!capable(CAP_AUDIT_CONTROL))\n\t\t\terr = -EPERM;\n\t\tbreak;\n\tcase AUDIT_USER:\n\tcase AUDIT_FIRST_USER_MSG ... AUDIT_LAST_USER_MSG:\n\tcase AUDIT_FIRST_USER_MSG2 ... AUDIT_LAST_USER_MSG2:\n\t\tif (!capable(CAP_AUDIT_WRITE))\n\t\t\terr = -EPERM;\n\t\tbreak;\n\tdefault:  /* bad msg */\n\t\terr = -EINVAL;\n\t}\n\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nasmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nasmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value[0] = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static noinline void\nno_context(struct pt_regs *regs, unsigned long error_code,\n\t   unsigned long address, int signal, int si_code)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long flags;\n\tint sig;\n\n\t/* Are we prepared to handle this kernel fault? */\n\tif (fixup_exception(regs)) {\n\t\t/*\n\t\t * Any interrupt that takes a fault gets the fixup. This makes\n\t\t * the below recursive fault logic only apply to a faults from\n\t\t * task context.\n\t\t */\n\t\tif (in_interrupt())\n\t\t\treturn;\n\n\t\t/*\n\t\t * Per the above we're !in_interrupt(), aka. task context.\n\t\t *\n\t\t * In this case we need to make sure we're not recursively\n\t\t * faulting through the emulate_vsyscall() logic.\n\t\t */\n\t\tif (current_thread_info()->sig_on_uaccess_error && signal) {\n\t\t\ttsk->thread.trap_nr = X86_TRAP_PF;\n\t\t\ttsk->thread.error_code = error_code | PF_USER;\n\t\t\ttsk->thread.cr2 = address;\n\n\t\t\t/* XXX: hwpoison faults will set the wrong code. */\n\t\t\tforce_sig_info_fault(signal, si_code, address, tsk, 0);\n\t\t}\n\n\t\t/*\n\t\t * Barring that, we can do the fixup and be happy.\n\t\t */\n\t\treturn;\n\t}\n\n\t/*\n\t * 32-bit:\n\t *\n\t *   Valid to do another page fault here, because if this fault\n\t *   had been triggered by is_prefetch fixup_exception would have\n\t *   handled it.\n\t *\n\t * 64-bit:\n\t *\n\t *   Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n\t/*\n\t * Oops. The kernel tried to access some bad page. We'll have to\n\t * terminate things with extreme prejudice:\n\t */\n\tflags = oops_begin();\n\n\tshow_fault_oops(regs, error_code, address);\n\n\tif (task_stack_end_corrupted(tsk))\n\t\tprintk(KERN_EMERG \"Thread overran stack, or stack corrupted\\n\");\n\n\ttsk->thread.cr2\t\t= address;\n\ttsk->thread.trap_nr\t= X86_TRAP_PF;\n\ttsk->thread.error_code\t= error_code;\n\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_DEFAULT \"CR2: %016lx\\n\", address);\n\n\toops_end(flags, regs, sig);\n}",
                        "code_after_change": "static noinline void\nno_context(struct pt_regs *regs, unsigned long error_code,\n\t   unsigned long address, int signal, int si_code)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long flags;\n\tint sig;\n\n\t/* Are we prepared to handle this kernel fault? */\n\tif (fixup_exception(regs, X86_TRAP_PF)) {\n\t\t/*\n\t\t * Any interrupt that takes a fault gets the fixup. This makes\n\t\t * the below recursive fault logic only apply to a faults from\n\t\t * task context.\n\t\t */\n\t\tif (in_interrupt())\n\t\t\treturn;\n\n\t\t/*\n\t\t * Per the above we're !in_interrupt(), aka. task context.\n\t\t *\n\t\t * In this case we need to make sure we're not recursively\n\t\t * faulting through the emulate_vsyscall() logic.\n\t\t */\n\t\tif (current_thread_info()->sig_on_uaccess_error && signal) {\n\t\t\ttsk->thread.trap_nr = X86_TRAP_PF;\n\t\t\ttsk->thread.error_code = error_code | PF_USER;\n\t\t\ttsk->thread.cr2 = address;\n\n\t\t\t/* XXX: hwpoison faults will set the wrong code. */\n\t\t\tforce_sig_info_fault(signal, si_code, address, tsk, 0);\n\t\t}\n\n\t\t/*\n\t\t * Barring that, we can do the fixup and be happy.\n\t\t */\n\t\treturn;\n\t}\n\n\t/*\n\t * 32-bit:\n\t *\n\t *   Valid to do another page fault here, because if this fault\n\t *   had been triggered by is_prefetch fixup_exception would have\n\t *   handled it.\n\t *\n\t * 64-bit:\n\t *\n\t *   Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n\t/*\n\t * Oops. The kernel tried to access some bad page. We'll have to\n\t * terminate things with extreme prejudice:\n\t */\n\tflags = oops_begin();\n\n\tshow_fault_oops(regs, error_code, address);\n\n\tif (task_stack_end_corrupted(tsk))\n\t\tprintk(KERN_EMERG \"Thread overran stack, or stack corrupted\\n\");\n\n\ttsk->thread.cr2\t\t= address;\n\ttsk->thread.trap_nr\t= X86_TRAP_PF;\n\ttsk->thread.error_code\t= error_code;\n\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_DEFAULT \"CR2: %016lx\\n\", address);\n\n\toops_end(flags, regs, sig);\n}",
                        "cve_id": "CVE-2016-9644"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int audit_netlink_ok(struct sk_buff *skb, u16 msg_type)\n{\n\tint err = 0;\n\n\t/* Only support initial user namespace for now. */\n\t/*\n\t * We return ECONNREFUSED because it tricks userspace into thinking\n\t * that audit was not configured into the kernel.  Lots of users\n\t * configure their PAM stack (because that's what the distro does)\n\t * to reject login if unable to send messages to audit.  If we return\n\t * ECONNREFUSED the PAM stack thinks the kernel does not have audit\n\t * configured in and will let login proceed.  If we return EPERM\n\t * userspace will reject all logins.  This should be removed when we\n\t * support non init namespaces!!\n\t */\n\tif (current_user_ns() != &init_user_ns)\n\t\treturn -ECONNREFUSED;\n\n\tswitch (msg_type) {\n\tcase AUDIT_LIST:\n\tcase AUDIT_ADD:\n\tcase AUDIT_DEL:\n\t\treturn -EOPNOTSUPP;\n\tcase AUDIT_GET:\n\tcase AUDIT_SET:\n\tcase AUDIT_GET_FEATURE:\n\tcase AUDIT_SET_FEATURE:\n\tcase AUDIT_LIST_RULES:\n\tcase AUDIT_ADD_RULE:\n\tcase AUDIT_DEL_RULE:\n\tcase AUDIT_SIGNAL_INFO:\n\tcase AUDIT_TTY_GET:\n\tcase AUDIT_TTY_SET:\n\tcase AUDIT_TRIM:\n\tcase AUDIT_MAKE_EQUIV:\n\t\t/* Only support auditd and auditctl in initial pid namespace\n\t\t * for now. */\n\t\tif ((task_active_pid_ns(current) != &init_pid_ns))\n\t\t\treturn -EPERM;\n\n\t\tif (!capable(CAP_AUDIT_CONTROL))\n\t\t\terr = -EPERM;\n\t\tbreak;\n\tcase AUDIT_USER:\n\tcase AUDIT_FIRST_USER_MSG ... AUDIT_LAST_USER_MSG:\n\tcase AUDIT_FIRST_USER_MSG2 ... AUDIT_LAST_USER_MSG2:\n\t\tif (!capable(CAP_AUDIT_WRITE))\n\t\t\terr = -EPERM;\n\t\tbreak;\n\tdefault:  /* bad msg */\n\t\terr = -EINVAL;\n\t}\n\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic noinline void\nno_context(struct pt_regs *regs, unsigned long error_code,\n\t   unsigned long address, int signal, int si_code)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long flags;\n\tint sig;\n\n\t/* Are we prepared to handle this kernel fault? */\n\tif (fixup_exception(regs)) {\n\t\t/*\n\t\t * Any interrupt that takes a fault gets the fixup. This makes\n\t\t * the below recursive fault logic only apply to a faults from\n\t\t * task context.\n\t\t */\n\t\tif (in_interrupt())\n\t\t\treturn;\n\n\t\t/*\n\t\t * Per the above we're !in_interrupt(), aka. task context.\n\t\t *\n\t\t * In this case we need to make sure we're not recursively\n\t\t * faulting through the emulate_vsyscall() logic.\n\t\t */\n\t\tif (current_thread_info()->sig_on_uaccess_error && signal) {\n\t\t\ttsk->thread.trap_nr = X86_TRAP_PF;\n\t\t\ttsk->thread.error_code = error_code | PF_USER;\n\t\t\ttsk->thread.cr2 = address;\n\n\t\t\t/* XXX: hwpoison faults will set the wrong code. */\n\t\t\tforce_sig_info_fault(signal, si_code, address, tsk, 0);\n\t\t}\n\n\t\t/*\n\t\t * Barring that, we can do the fixup and be happy.\n\t\t */\n\t\treturn;\n\t}\n\n\t/*\n\t * 32-bit:\n\t *\n\t *   Valid to do another page fault here, because if this fault\n\t *   had been triggered by is_prefetch fixup_exception would have\n\t *   handled it.\n\t *\n\t * 64-bit:\n\t *\n\t *   Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n\t/*\n\t * Oops. The kernel tried to access some bad page. We'll have to\n\t * terminate things with extreme prejudice:\n\t */\n\tflags = oops_begin();\n\n\tshow_fault_oops(regs, error_code, address);\n\n\tif (task_stack_end_corrupted(tsk))\n\t\tprintk(KERN_EMERG \"Thread overran stack, or stack corrupted\\n\");\n\n\ttsk->thread.cr2\t\t= address;\n\ttsk->thread.trap_nr\t= X86_TRAP_PF;\n\ttsk->thread.error_code\t= error_code;\n\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_DEFAULT \"CR2: %016lx\\n\", address);\n\n\toops_end(flags, regs, sig);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic noinline void\nno_context(struct pt_regs *regs, unsigned long error_code,\n\t   unsigned long address, int signal, int si_code)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long flags;\n\tint sig;\n\n\t/* Are we prepared to handle this kernel fault? */\n\tif (fixup_exception(regs, X86_TRAP_PF)) {\n\t\t/*\n\t\t * Any interrupt that takes a fault gets the fixup. This makes\n\t\t * the below recursive fault logic only apply to a faults from\n\t\t * task context.\n\t\t */\n\t\tif (in_interrupt())\n\t\t\treturn;\n\n\t\t/*\n\t\t * Per the above we're !in_interrupt(), aka. task context.\n\t\t *\n\t\t * In this case we need to make sure we're not recursively\n\t\t * faulting through the emulate_vsyscall() logic.\n\t\t */\n\t\tif (current_thread_info()->sig_on_uaccess_error && signal) {\n\t\t\ttsk->thread.trap_nr = X86_TRAP_PF;\n\t\t\ttsk->thread.error_code = error_code | PF_USER;\n\t\t\ttsk->thread.cr2 = address;\n\n\t\t\t/* XXX: hwpoison faults will set the wrong code. */\n\t\t\tforce_sig_info_fault(signal, si_code, address, tsk, 0);\n\t\t}\n\n\t\t/*\n\t\t * Barring that, we can do the fixup and be happy.\n\t\t */\n\t\treturn;\n\t}\n\n\t/*\n\t * 32-bit:\n\t *\n\t *   Valid to do another page fault here, because if this fault\n\t *   had been triggered by is_prefetch fixup_exception would have\n\t *   handled it.\n\t *\n\t * 64-bit:\n\t *\n\t *   Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n\t/*\n\t * Oops. The kernel tried to access some bad page. We'll have to\n\t * terminate things with extreme prejudice:\n\t */\n\tflags = oops_begin();\n\n\tshow_fault_oops(regs, error_code, address);\n\n\tif (task_stack_end_corrupted(tsk))\n\t\tprintk(KERN_EMERG \"Thread overran stack, or stack corrupted\\n\");\n\n\ttsk->thread.cr2\t\t= address;\n\ttsk->thread.trap_nr\t= X86_TRAP_PF;\n\ttsk->thread.error_code\t= error_code;\n\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_DEFAULT \"CR2: %016lx\\n\", address);\n\n\toops_end(flags, regs, sig);\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 136,
            "cve_id": "CVE-2012-6657",
            "code_snippet": "int sock_setsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tint val;\n\tint valbool;\n\tstruct linger ling;\n\tint ret = 0;\n\n\t/*\n\t *\tOptions without arguments\n\t */\n\n\tif (optname == SO_BINDTODEVICE)\n\t\treturn sock_bindtodevice(sk, optval, optlen);\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\n\tvalbool = val ? 1 : 0;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase SO_DEBUG:\n\t\tif (val && !capable(CAP_NET_ADMIN))\n\t\t\tret = -EACCES;\n\t\telse\n\t\t\tsock_valbool_flag(sk, SOCK_DBG, valbool);\n\t\tbreak;\n\tcase SO_REUSEADDR:\n\t\tsk->sk_reuse = (valbool ? SK_CAN_REUSE : SK_NO_REUSE);\n\t\tbreak;\n\tcase SO_TYPE:\n\tcase SO_PROTOCOL:\n\tcase SO_DOMAIN:\n\tcase SO_ERROR:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\tcase SO_DONTROUTE:\n\t\tsock_valbool_flag(sk, SOCK_LOCALROUTE, valbool);\n\t\tbreak;\n\tcase SO_BROADCAST:\n\t\tsock_valbool_flag(sk, SOCK_BROADCAST, valbool);\n\t\tbreak;\n\tcase SO_SNDBUF:\n\t\t/* Don't error on this BSD doesn't and if you think\n\t\t * about it this is right. Otherwise apps have to\n\t\t * play 'guess the biggest size' games. RCVBUF/SNDBUF\n\t\t * are treated in BSD as hints\n\t\t */\n\t\tval = min_t(u32, val, sysctl_wmem_max);\nset_sndbuf:\n\t\tsk->sk_userlocks |= SOCK_SNDBUF_LOCK;\n\t\tsk->sk_sndbuf = max_t(u32, val * 2, SOCK_MIN_SNDBUF);\n\t\t/* Wake up sending tasks if we upped the value. */\n\t\tsk->sk_write_space(sk);\n\t\tbreak;\n\n\tcase SO_SNDBUFFORCE:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tgoto set_sndbuf;\n\n\tcase SO_RCVBUF:\n\t\t/* Don't error on this BSD doesn't and if you think\n\t\t * about it this is right. Otherwise apps have to\n\t\t * play 'guess the biggest size' games. RCVBUF/SNDBUF\n\t\t * are treated in BSD as hints\n\t\t */\n\t\tval = min_t(u32, val, sysctl_rmem_max);\nset_rcvbuf:\n\t\tsk->sk_userlocks |= SOCK_RCVBUF_LOCK;\n\t\t/*\n\t\t * We double it on the way in to account for\n\t\t * \"struct sk_buff\" etc. overhead.   Applications\n\t\t * assume that the SO_RCVBUF setting they make will\n\t\t * allow that much actual data to be received on that\n\t\t * socket.\n\t\t *\n\t\t * Applications are unaware that \"struct sk_buff\" and\n\t\t * other overheads allocate from the receive buffer\n\t\t * during socket buffer allocation.\n\t\t *\n\t\t * And after considering the possible alternatives,\n\t\t * returning the value we actually used in getsockopt\n\t\t * is the most desirable behavior.\n\t\t */\n\t\tsk->sk_rcvbuf = max_t(u32, val * 2, SOCK_MIN_RCVBUF);\n\t\tbreak;\n\n\tcase SO_RCVBUFFORCE:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tgoto set_rcvbuf;\n\n\tcase SO_KEEPALIVE:\n#ifdef CONFIG_INET\n\t\tif (sk->sk_protocol == IPPROTO_TCP)\n\t\t\ttcp_set_keepalive(sk, valbool);\n#endif\n\t\tsock_valbool_flag(sk, SOCK_KEEPOPEN, valbool);\n\t\tbreak;\n\n\tcase SO_OOBINLINE:\n\t\tsock_valbool_flag(sk, SOCK_URGINLINE, valbool);\n\t\tbreak;\n\n\tcase SO_NO_CHECK:\n\t\tsk->sk_no_check = valbool;\n\t\tbreak;\n\n\tcase SO_PRIORITY:\n\t\tif ((val >= 0 && val <= 6) || capable(CAP_NET_ADMIN))\n\t\t\tsk->sk_priority = val;\n\t\telse\n\t\t\tret = -EPERM;\n\t\tbreak;\n\n\tcase SO_LINGER:\n\t\tif (optlen < sizeof(ling)) {\n\t\t\tret = -EINVAL;\t/* 1003.1g */\n\t\t\tbreak;\n\t\t}\n\t\tif (copy_from_user(&ling, optval, sizeof(ling))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!ling.l_onoff)\n\t\t\tsock_reset_flag(sk, SOCK_LINGER);\n\t\telse {\n#if (BITS_PER_LONG == 32)\n\t\t\tif ((unsigned int)ling.l_linger >= MAX_SCHEDULE_TIMEOUT/HZ)\n\t\t\t\tsk->sk_lingertime = MAX_SCHEDULE_TIMEOUT;\n\t\t\telse\n#endif\n\t\t\t\tsk->sk_lingertime = (unsigned int)ling.l_linger * HZ;\n\t\t\tsock_set_flag(sk, SOCK_LINGER);\n\t\t}\n\t\tbreak;\n\n\tcase SO_BSDCOMPAT:\n\t\tsock_warn_obsolete_bsdism(\"setsockopt\");\n\t\tbreak;\n\n\tcase SO_PASSCRED:\n\t\tif (valbool)\n\t\t\tset_bit(SOCK_PASSCRED, &sock->flags);\n\t\telse\n\t\t\tclear_bit(SOCK_PASSCRED, &sock->flags);\n\t\tbreak;\n\n\tcase SO_TIMESTAMP:\n\tcase SO_TIMESTAMPNS:\n\t\tif (valbool)  {\n\t\t\tif (optname == SO_TIMESTAMP)\n\t\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t\telse\n\t\t\t\tsock_set_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t\tsock_set_flag(sk, SOCK_RCVTSTAMP);\n\t\t\tsock_enable_timestamp(sk, SOCK_TIMESTAMP);\n\t\t} else {\n\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMP);\n\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t}\n\t\tbreak;\n\n\tcase SO_TIMESTAMPING:\n\t\tif (val & ~SOF_TIMESTAMPING_MASK) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_TX_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_TX_HARDWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_TX_SOFTWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_TX_SOFTWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_RX_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_RX_HARDWARE);\n\t\tif (val & SOF_TIMESTAMPING_RX_SOFTWARE)\n\t\t\tsock_enable_timestamp(sk,\n\t\t\t\t\t      SOCK_TIMESTAMPING_RX_SOFTWARE);\n\t\telse\n\t\t\tsock_disable_timestamp(sk,\n\t\t\t\t\t       (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE));\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_SOFTWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_SOFTWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_SYS_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_SYS_HARDWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_RAW_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_RAW_HARDWARE);\n\t\tbreak;\n\n\tcase SO_RCVLOWAT:\n\t\tif (val < 0)\n\t\t\tval = INT_MAX;\n\t\tsk->sk_rcvlowat = val ? : 1;\n\t\tbreak;\n\n\tcase SO_RCVTIMEO:\n\t\tret = sock_set_timeout(&sk->sk_rcvtimeo, optval, optlen);\n\t\tbreak;\n\n\tcase SO_SNDTIMEO:\n\t\tret = sock_set_timeout(&sk->sk_sndtimeo, optval, optlen);\n\t\tbreak;\n\n\tcase SO_ATTACH_FILTER:\n\t\tret = -EINVAL;\n\t\tif (optlen == sizeof(struct sock_fprog)) {\n\t\t\tstruct sock_fprog fprog;\n\n\t\t\tret = -EFAULT;\n\t\t\tif (copy_from_user(&fprog, optval, sizeof(fprog)))\n\t\t\t\tbreak;\n\n\t\t\tret = sk_attach_filter(&fprog, sk);\n\t\t}\n\t\tbreak;\n\n\tcase SO_DETACH_FILTER:\n\t\tret = sk_detach_filter(sk);\n\t\tbreak;\n\n\tcase SO_PASSSEC:\n\t\tif (valbool)\n\t\t\tset_bit(SOCK_PASSSEC, &sock->flags);\n\t\telse\n\t\t\tclear_bit(SOCK_PASSSEC, &sock->flags);\n\t\tbreak;\n\tcase SO_MARK:\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tsk->sk_mark = val;\n\t\tbreak;\n\n\t\t/* We implement the SO_SNDLOWAT etc to\n\t\t   not be settable (1003.1g 5.3) */\n\tcase SO_RXQ_OVFL:\n\t\tsock_valbool_flag(sk, SOCK_RXQ_OVFL, valbool);\n\t\tbreak;\n\n\tcase SO_WIFI_STATUS:\n\t\tsock_valbool_flag(sk, SOCK_WIFI_STATUS, valbool);\n\t\tbreak;\n\n\tcase SO_PEEK_OFF:\n\t\tif (sock->ops->set_peek_off)\n\t\t\tsock->ops->set_peek_off(sk, val);\n\t\telse\n\t\t\tret = -EOPNOTSUPP;\n\t\tbreak;\n\n\tcase SO_NOFCS:\n\t\tsock_valbool_flag(sk, SOCK_NOFCS, valbool);\n\t\tbreak;\n\n\tdefault:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\trelease_sock(sk);\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, valbool;\n\tint retv = -ENOPROTOOPT;\n\tbool needs_rtnl = setsockopt_needs_rtnl(optname);\n\n\tif (!optval)\n\t\tval = 0;\n\telse {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (get_user(val, (int __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\tval = 0;\n\t}\n\n\tvalbool = (val != 0);\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_setsockopt(sk, optname, optval, optlen);\n\n\tif (needs_rtnl)\n\t\trtnl_lock();\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\n\tcase IPV6_ADDRFORM:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val == PF_INET) {\n\t\t\tstruct ipv6_txoptions *opt;\n\t\t\tstruct sk_buff *pktopt;\n\n\t\t\tif (sk->sk_type == SOCK_RAW)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_protocol == IPPROTO_UDP ||\n\t\t\t    sk->sk_protocol == IPPROTO_UDPLITE) {\n\t\t\t\tstruct udp_sock *up = udp_sk(sk);\n\t\t\t\tif (up->pending == AF_INET6) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (sk->sk_protocol != IPPROTO_TCP)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t\tretv = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (ipv6_only_sock(sk) ||\n\t\t\t    !ipv6_addr_v4mapped(&sk->sk_v6_daddr)) {\n\t\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tfl6_free_socklist(sk);\n\t\t\tipv6_sock_mc_close(sk);\n\n\t\t\t/*\n\t\t\t * Sock is moving from IPv6 to IPv4 (sk_prot), so\n\t\t\t * remove it from the refcnt debug socks count in the\n\t\t\t * original family...\n\t\t\t */\n\t\t\tsk_refcnt_debug_dec(sk);\n\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = &tcp_prot;\n\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;\n\t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t\t} else {\n\t\t\t\tstruct proto *prot = &udp_prot;\n\n\t\t\t\tif (sk->sk_protocol == IPPROTO_UDPLITE)\n\t\t\t\t\tprot = &udplite_prot;\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = prot;\n\t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t}\n\t\t\topt = xchg(&np->opt, NULL);\n\t\t\tif (opt)\n\t\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t\t\tpktopt = xchg(&np->pktoptions, NULL);\n\t\t\tkfree_skb(pktopt);\n\n\t\t\tsk->sk_destruct = inet_sock_destruct;\n\t\t\t/*\n\t\t\t * ... and add it to the refcnt debug socks count\n\t\t\t * in the new family. -acme\n\t\t\t */\n\t\t\tsk_refcnt_debug_inc(sk);\n\t\t\tmodule_put(THIS_MODULE);\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\t\tgoto e_inval;\n\n\tcase IPV6_V6ONLY:\n\t\tif (optlen < sizeof(int) ||\n\t\t    inet_sk(sk)->inet_num)\n\t\t\tgoto e_inval;\n\t\tsk->sk_ipv6only = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxoinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxhlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxohlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.srcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.osrcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.hopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.ohopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.dstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.odstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < -1 || val > 0xff)\n\t\t\tgoto e_inval;\n\t\t/* RFC 3542, 6.5: default traffic class of 0x0 */\n\t\tif (val == -1)\n\t\t\tval = 0;\n\t\tnp->tclass = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxtclass = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxpmtu = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TRANSPARENT:\n\t\tif (valbool && !ns_capable(net->user_ns, CAP_NET_ADMIN) &&\n\t\t    !ns_capable(net->user_ns, CAP_NET_RAW)) {\n\t\t\tretv = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we don't have a separate transparent bit for IPV6 we use the one in the IPv4 socket */\n\t\tinet_sk(sk)->transparent = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxorigdstaddr = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\t/* remove any sticky options header with a zero option\n\t\t * length, per RFC3542.\n\t\t */\n\t\tif (optlen == 0)\n\t\t\toptval = NULL;\n\t\telse if (!optval)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct ipv6_opt_hdr) ||\n\t\t\t optlen & 0x7 || optlen > 8 * 255)\n\t\t\tgoto e_inval;\n\n\t\t/* hop-by-hop / destination options are privileged option */\n\t\tretv = -EPERM;\n\t\tif (optname != IPV6_RTHDR && !ns_capable(net->user_ns, CAP_NET_RAW))\n\t\t\tbreak;\n\n\t\topt = ipv6_renew_options(sk, np->opt, optname,\n\t\t\t\t\t (struct ipv6_opt_hdr __user *)optval,\n\t\t\t\t\t optlen);\n\t\tif (IS_ERR(opt)) {\n\t\t\tretv = PTR_ERR(opt);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* routing header option needs extra check */\n\t\tretv = -EINVAL;\n\t\tif (optname == IPV6_RTHDR && opt && opt->srcrt) {\n\t\t\tstruct ipv6_rt_hdr *rthdr = opt->srcrt;\n\t\t\tswitch (rthdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\t\tcase IPV6_SRCRT_TYPE_2:\n\t\t\t\tif (rthdr->hdrlen != 2 ||\n\t\t\t\t    rthdr->segments_left != 1)\n\t\t\t\t\tgoto sticky_done;\n\n\t\t\t\tbreak;\n#endif\n\t\t\tdefault:\n\t\t\t\tgoto sticky_done;\n\t\t\t}\n\t\t}\n\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\nsticky_done:\n\t\tif (opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t\tbreak;\n\t}\n\n\tcase IPV6_PKTINFO:\n\t{\n\t\tstruct in6_pktinfo pkt;\n\n\t\tif (optlen == 0)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct in6_pktinfo) || !optval)\n\t\t\tgoto e_inval;\n\n\t\tif (copy_from_user(&pkt, optval, sizeof(struct in6_pktinfo))) {\n\t\t\t\tretv = -EFAULT;\n\t\t\t\tbreak;\n\t\t}\n\t\tif (sk->sk_bound_dev_if && pkt.ipi6_ifindex != sk->sk_bound_dev_if)\n\t\t\tgoto e_inval;\n\n\t\tnp->sticky_pktinfo.ipi6_ifindex = pkt.ipi6_ifindex;\n\t\tnp->sticky_pktinfo.ipi6_addr = pkt.ipi6_addr;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct ipv6_txoptions *opt = NULL;\n\t\tstruct msghdr msg;\n\t\tstruct flowi6 fl6;\n\t\tint junk;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\n\t\tif (optlen == 0)\n\t\t\tgoto update;\n\n\t\t/* 1K is probably excessive\n\t\t * 1K is surely not enough, 2K per standard header is 16K.\n\t\t */\n\t\tretv = -EINVAL;\n\t\tif (optlen > 64*1024)\n\t\t\tbreak;\n\n\t\topt = sock_kmalloc(sk, sizeof(*opt) + optlen, GFP_KERNEL);\n\t\tretv = -ENOBUFS;\n\t\tif (!opt)\n\t\t\tbreak;\n\n\t\tmemset(opt, 0, sizeof(*opt));\n\t\topt->tot_len = sizeof(*opt) + optlen;\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(opt+1, optval, optlen))\n\t\t\tgoto done;\n\n\t\tmsg.msg_controllen = optlen;\n\t\tmsg.msg_control = (void *)(opt+1);\n\n\t\tretv = ip6_datagram_send_ctl(net, sk, &msg, &fl6, opt, &junk,\n\t\t\t\t\t     &junk, &junk);\n\t\tif (retv)\n\t\t\tgoto done;\nupdate:\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\ndone:\n\t\tif (opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t\tbreak;\n\t}\n\tcase IPV6_UNICAST_HOPS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->hop_limit = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_HOPS:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->mcast_hops = (val == -1 ? IPV6_DEFAULT_MCASTHOPS : val);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val != valbool)\n\t\t\tgoto e_inval;\n\t\tnp->mc_loop = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t{\n\t\tstruct net_device *dev = NULL;\n\t\tint ifindex;\n\n\t\tif (optlen != sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tifindex = (__force int)ntohl((__force __be32)val);\n\t\tif (ifindex == 0) {\n\t\t\tnp->ucast_oif = 0;\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tdev = dev_get_by_index(net, ifindex);\n\t\tretv = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\tretv = -EINVAL;\n\t\tif (sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tnp->ucast_oif = ifindex;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_IF:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tif (val) {\n\t\t\tstruct net_device *dev;\n\n\t\t\tif (sk->sk_bound_dev_if && sk->sk_bound_dev_if != val)\n\t\t\t\tgoto e_inval;\n\n\t\t\tdev = dev_get_by_index(net, val);\n\t\t\tif (!dev) {\n\t\t\t\tretv = -ENODEV;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdev_put(dev);\n\t\t}\n\t\tnp->mcast_oif = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_ADD_MEMBERSHIP)\n\t\t\tretv = ipv6_sock_mc_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_JOIN_ANYCAST)\n\t\t\tretv = ipv6_sock_ac_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_ac_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t{\n\t\tstruct group_req greq;\n\t\tstruct sockaddr_in6 *psin6;\n\n\t\tif (optlen < sizeof(struct group_req))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&greq, optval, sizeof(struct group_req)))\n\t\t\tbreak;\n\t\tif (greq.gr_group.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tpsin6 = (struct sockaddr_in6 *)&greq.gr_group;\n\t\tif (optname == MCAST_JOIN_GROUP)\n\t\t\tretv = ipv6_sock_mc_join(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t{\n\t\tstruct group_source_req greqs;\n\t\tint omode, add;\n\n\t\tif (optlen < sizeof(struct group_source_req))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&greqs, optval, sizeof(greqs))) {\n\t\t\tretv = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (greqs.gsr_group.ss_family != AF_INET6 ||\n\t\t    greqs.gsr_source.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tif (optname == MCAST_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == MCAST_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == MCAST_JOIN_SOURCE_GROUP) {\n\t\t\tstruct sockaddr_in6 *psin6;\n\n\t\t\tpsin6 = (struct sockaddr_in6 *)&greqs.gsr_group;\n\t\t\tretv = ipv6_sock_mc_join(sk, greqs.gsr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\t\t/* prior join w/ different source is ok */\n\t\t\tif (retv && retv != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* MCAST_LEAVE_SOURCE_GROUP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\tretv = ip6_mc_source(add, omode, sk, &greqs);\n\t\tbreak;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter *gsf;\n\n\t\tif (optlen < GROUP_FILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tgsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!gsf) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(gsf, optval, optlen)) {\n\t\t\tkfree(gsf);\n\t\t\tbreak;\n\t\t}\n\t\t/* numsrc >= (4G-140)/128 overflow in 32 bits */\n\t\tif (gsf->gf_numsrc >= 0x1ffffffU ||\n\t\t    gsf->gf_numsrc > sysctl_mld_max_msf) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tif (GROUP_FILTER_SIZE(gsf->gf_numsrc) > optlen) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tretv = ip6_mc_msfilter(sk, gsf);\n\t\tkfree(gsf);\n\n\t\tbreak;\n\t}\n\tcase IPV6_ROUTER_ALERT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = ip6_ra_control(sk, val);\n\t\tbreak;\n\tcase IPV6_MTU_DISCOVER:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < IPV6_PMTUDISC_DONT || val > IPV6_PMTUDISC_OMIT)\n\t\t\tgoto e_inval;\n\t\tnp->pmtudisc = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val && val < IPV6_MIN_MTU)\n\t\t\tgoto e_inval;\n\t\tnp->frag_size = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->recverr = valbool;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWINFO_SEND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->sndflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWLABEL_MGR:\n\t\tretv = ipv6_flowlabel_opt(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_IPSEC_POLICY:\n\tcase IPV6_XFRM_POLICY:\n\t\tretv = -EPERM;\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\tretv = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t    {\n\t\tunsigned int pref = 0;\n\t\tunsigned int prefmask = ~0;\n\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EINVAL;\n\n\t\t/* check PUBLIC/TMP/PUBTMP_DEFAULT conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_PUBLIC|\n\t\t\t       IPV6_PREFER_SRC_TMP|\n\t\t\t       IPV6_PREFER_SRC_PUBTMP_DEFAULT)) {\n\t\tcase IPV6_PREFER_SRC_PUBLIC:\n\t\t\tpref |= IPV6_PREFER_SRC_PUBLIC;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_TMP:\n\t\t\tpref |= IPV6_PREFER_SRC_TMP;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_PUBTMP_DEFAULT:\n\t\t\tbreak;\n\t\tcase 0:\n\t\t\tgoto pref_skip_pubtmp;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~(IPV6_PREFER_SRC_PUBLIC|\n\t\t\t      IPV6_PREFER_SRC_TMP);\npref_skip_pubtmp:\n\n\t\t/* check HOME/COA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_HOME|IPV6_PREFER_SRC_COA)) {\n\t\tcase IPV6_PREFER_SRC_HOME:\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_COA:\n\t\t\tpref |= IPV6_PREFER_SRC_COA;\n\t\tcase 0:\n\t\t\tgoto pref_skip_coa;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~IPV6_PREFER_SRC_COA;\npref_skip_coa:\n\n\t\t/* check CGA/NONCGA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_CGA|IPV6_PREFER_SRC_NONCGA)) {\n\t\tcase IPV6_PREFER_SRC_CGA:\n\t\tcase IPV6_PREFER_SRC_NONCGA:\n\t\tcase 0:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tnp->srcprefs = (np->srcprefs & prefmask) | pref;\n\t\tretv = 0;\n\n\t\tbreak;\n\t    }\n\tcase IPV6_MINHOPCOUNT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tnp->min_hopcount = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_DONTFRAG:\n\t\tnp->dontfrag = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tnp->autoflowlabel = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\n\treturn retv;\n\ne_inval:\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\treturn -EINVAL;\n}",
                        "code_after_change": "static int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, valbool;\n\tint retv = -ENOPROTOOPT;\n\tbool needs_rtnl = setsockopt_needs_rtnl(optname);\n\n\tif (!optval)\n\t\tval = 0;\n\telse {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (get_user(val, (int __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\tval = 0;\n\t}\n\n\tvalbool = (val != 0);\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_setsockopt(sk, optname, optval, optlen);\n\n\tif (needs_rtnl)\n\t\trtnl_lock();\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\n\tcase IPV6_ADDRFORM:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val == PF_INET) {\n\t\t\tstruct ipv6_txoptions *opt;\n\t\t\tstruct sk_buff *pktopt;\n\n\t\t\tif (sk->sk_type == SOCK_RAW)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_protocol == IPPROTO_UDP ||\n\t\t\t    sk->sk_protocol == IPPROTO_UDPLITE) {\n\t\t\t\tstruct udp_sock *up = udp_sk(sk);\n\t\t\t\tif (up->pending == AF_INET6) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (sk->sk_protocol != IPPROTO_TCP)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t\tretv = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (ipv6_only_sock(sk) ||\n\t\t\t    !ipv6_addr_v4mapped(&sk->sk_v6_daddr)) {\n\t\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tfl6_free_socklist(sk);\n\t\t\tipv6_sock_mc_close(sk);\n\n\t\t\t/*\n\t\t\t * Sock is moving from IPv6 to IPv4 (sk_prot), so\n\t\t\t * remove it from the refcnt debug socks count in the\n\t\t\t * original family...\n\t\t\t */\n\t\t\tsk_refcnt_debug_dec(sk);\n\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = &tcp_prot;\n\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;\n\t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t\t} else {\n\t\t\t\tstruct proto *prot = &udp_prot;\n\n\t\t\t\tif (sk->sk_protocol == IPPROTO_UDPLITE)\n\t\t\t\t\tprot = &udplite_prot;\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = prot;\n\t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t}\n\t\t\topt = xchg((__force struct ipv6_txoptions **)&np->opt,\n\t\t\t\t   NULL);\n\t\t\tif (opt) {\n\t\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\t\ttxopt_put(opt);\n\t\t\t}\n\t\t\tpktopt = xchg(&np->pktoptions, NULL);\n\t\t\tkfree_skb(pktopt);\n\n\t\t\tsk->sk_destruct = inet_sock_destruct;\n\t\t\t/*\n\t\t\t * ... and add it to the refcnt debug socks count\n\t\t\t * in the new family. -acme\n\t\t\t */\n\t\t\tsk_refcnt_debug_inc(sk);\n\t\t\tmodule_put(THIS_MODULE);\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\t\tgoto e_inval;\n\n\tcase IPV6_V6ONLY:\n\t\tif (optlen < sizeof(int) ||\n\t\t    inet_sk(sk)->inet_num)\n\t\t\tgoto e_inval;\n\t\tsk->sk_ipv6only = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxoinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxhlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxohlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.srcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.osrcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.hopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.ohopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.dstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.odstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < -1 || val > 0xff)\n\t\t\tgoto e_inval;\n\t\t/* RFC 3542, 6.5: default traffic class of 0x0 */\n\t\tif (val == -1)\n\t\t\tval = 0;\n\t\tnp->tclass = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxtclass = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxpmtu = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TRANSPARENT:\n\t\tif (valbool && !ns_capable(net->user_ns, CAP_NET_ADMIN) &&\n\t\t    !ns_capable(net->user_ns, CAP_NET_RAW)) {\n\t\t\tretv = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we don't have a separate transparent bit for IPV6 we use the one in the IPv4 socket */\n\t\tinet_sk(sk)->transparent = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxorigdstaddr = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\t/* remove any sticky options header with a zero option\n\t\t * length, per RFC3542.\n\t\t */\n\t\tif (optlen == 0)\n\t\t\toptval = NULL;\n\t\telse if (!optval)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct ipv6_opt_hdr) ||\n\t\t\t optlen & 0x7 || optlen > 8 * 255)\n\t\t\tgoto e_inval;\n\n\t\t/* hop-by-hop / destination options are privileged option */\n\t\tretv = -EPERM;\n\t\tif (optname != IPV6_RTHDR && !ns_capable(net->user_ns, CAP_NET_RAW))\n\t\t\tbreak;\n\n\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\t\topt = ipv6_renew_options(sk, opt, optname,\n\t\t\t\t\t (struct ipv6_opt_hdr __user *)optval,\n\t\t\t\t\t optlen);\n\t\tif (IS_ERR(opt)) {\n\t\t\tretv = PTR_ERR(opt);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* routing header option needs extra check */\n\t\tretv = -EINVAL;\n\t\tif (optname == IPV6_RTHDR && opt && opt->srcrt) {\n\t\t\tstruct ipv6_rt_hdr *rthdr = opt->srcrt;\n\t\t\tswitch (rthdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\t\tcase IPV6_SRCRT_TYPE_2:\n\t\t\t\tif (rthdr->hdrlen != 2 ||\n\t\t\t\t    rthdr->segments_left != 1)\n\t\t\t\t\tgoto sticky_done;\n\n\t\t\t\tbreak;\n#endif\n\t\t\tdefault:\n\t\t\t\tgoto sticky_done;\n\t\t\t}\n\t\t}\n\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\nsticky_done:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase IPV6_PKTINFO:\n\t{\n\t\tstruct in6_pktinfo pkt;\n\n\t\tif (optlen == 0)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct in6_pktinfo) || !optval)\n\t\t\tgoto e_inval;\n\n\t\tif (copy_from_user(&pkt, optval, sizeof(struct in6_pktinfo))) {\n\t\t\t\tretv = -EFAULT;\n\t\t\t\tbreak;\n\t\t}\n\t\tif (sk->sk_bound_dev_if && pkt.ipi6_ifindex != sk->sk_bound_dev_if)\n\t\t\tgoto e_inval;\n\n\t\tnp->sticky_pktinfo.ipi6_ifindex = pkt.ipi6_ifindex;\n\t\tnp->sticky_pktinfo.ipi6_addr = pkt.ipi6_addr;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct ipv6_txoptions *opt = NULL;\n\t\tstruct msghdr msg;\n\t\tstruct flowi6 fl6;\n\t\tint junk;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\n\t\tif (optlen == 0)\n\t\t\tgoto update;\n\n\t\t/* 1K is probably excessive\n\t\t * 1K is surely not enough, 2K per standard header is 16K.\n\t\t */\n\t\tretv = -EINVAL;\n\t\tif (optlen > 64*1024)\n\t\t\tbreak;\n\n\t\topt = sock_kmalloc(sk, sizeof(*opt) + optlen, GFP_KERNEL);\n\t\tretv = -ENOBUFS;\n\t\tif (!opt)\n\t\t\tbreak;\n\n\t\tmemset(opt, 0, sizeof(*opt));\n\t\tatomic_set(&opt->refcnt, 1);\n\t\topt->tot_len = sizeof(*opt) + optlen;\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(opt+1, optval, optlen))\n\t\t\tgoto done;\n\n\t\tmsg.msg_controllen = optlen;\n\t\tmsg.msg_control = (void *)(opt+1);\n\n\t\tretv = ip6_datagram_send_ctl(net, sk, &msg, &fl6, opt, &junk,\n\t\t\t\t\t     &junk, &junk);\n\t\tif (retv)\n\t\t\tgoto done;\nupdate:\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\ndone:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\tcase IPV6_UNICAST_HOPS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->hop_limit = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_HOPS:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->mcast_hops = (val == -1 ? IPV6_DEFAULT_MCASTHOPS : val);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val != valbool)\n\t\t\tgoto e_inval;\n\t\tnp->mc_loop = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t{\n\t\tstruct net_device *dev = NULL;\n\t\tint ifindex;\n\n\t\tif (optlen != sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tifindex = (__force int)ntohl((__force __be32)val);\n\t\tif (ifindex == 0) {\n\t\t\tnp->ucast_oif = 0;\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tdev = dev_get_by_index(net, ifindex);\n\t\tretv = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\tretv = -EINVAL;\n\t\tif (sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tnp->ucast_oif = ifindex;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_IF:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tif (val) {\n\t\t\tstruct net_device *dev;\n\n\t\t\tif (sk->sk_bound_dev_if && sk->sk_bound_dev_if != val)\n\t\t\t\tgoto e_inval;\n\n\t\t\tdev = dev_get_by_index(net, val);\n\t\t\tif (!dev) {\n\t\t\t\tretv = -ENODEV;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdev_put(dev);\n\t\t}\n\t\tnp->mcast_oif = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_ADD_MEMBERSHIP)\n\t\t\tretv = ipv6_sock_mc_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_JOIN_ANYCAST)\n\t\t\tretv = ipv6_sock_ac_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_ac_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t{\n\t\tstruct group_req greq;\n\t\tstruct sockaddr_in6 *psin6;\n\n\t\tif (optlen < sizeof(struct group_req))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&greq, optval, sizeof(struct group_req)))\n\t\t\tbreak;\n\t\tif (greq.gr_group.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tpsin6 = (struct sockaddr_in6 *)&greq.gr_group;\n\t\tif (optname == MCAST_JOIN_GROUP)\n\t\t\tretv = ipv6_sock_mc_join(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t{\n\t\tstruct group_source_req greqs;\n\t\tint omode, add;\n\n\t\tif (optlen < sizeof(struct group_source_req))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&greqs, optval, sizeof(greqs))) {\n\t\t\tretv = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (greqs.gsr_group.ss_family != AF_INET6 ||\n\t\t    greqs.gsr_source.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tif (optname == MCAST_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == MCAST_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == MCAST_JOIN_SOURCE_GROUP) {\n\t\t\tstruct sockaddr_in6 *psin6;\n\n\t\t\tpsin6 = (struct sockaddr_in6 *)&greqs.gsr_group;\n\t\t\tretv = ipv6_sock_mc_join(sk, greqs.gsr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\t\t/* prior join w/ different source is ok */\n\t\t\tif (retv && retv != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* MCAST_LEAVE_SOURCE_GROUP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\tretv = ip6_mc_source(add, omode, sk, &greqs);\n\t\tbreak;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter *gsf;\n\n\t\tif (optlen < GROUP_FILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tgsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!gsf) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(gsf, optval, optlen)) {\n\t\t\tkfree(gsf);\n\t\t\tbreak;\n\t\t}\n\t\t/* numsrc >= (4G-140)/128 overflow in 32 bits */\n\t\tif (gsf->gf_numsrc >= 0x1ffffffU ||\n\t\t    gsf->gf_numsrc > sysctl_mld_max_msf) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tif (GROUP_FILTER_SIZE(gsf->gf_numsrc) > optlen) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tretv = ip6_mc_msfilter(sk, gsf);\n\t\tkfree(gsf);\n\n\t\tbreak;\n\t}\n\tcase IPV6_ROUTER_ALERT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = ip6_ra_control(sk, val);\n\t\tbreak;\n\tcase IPV6_MTU_DISCOVER:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < IPV6_PMTUDISC_DONT || val > IPV6_PMTUDISC_OMIT)\n\t\t\tgoto e_inval;\n\t\tnp->pmtudisc = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val && val < IPV6_MIN_MTU)\n\t\t\tgoto e_inval;\n\t\tnp->frag_size = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->recverr = valbool;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWINFO_SEND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->sndflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWLABEL_MGR:\n\t\tretv = ipv6_flowlabel_opt(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_IPSEC_POLICY:\n\tcase IPV6_XFRM_POLICY:\n\t\tretv = -EPERM;\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\tretv = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t    {\n\t\tunsigned int pref = 0;\n\t\tunsigned int prefmask = ~0;\n\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EINVAL;\n\n\t\t/* check PUBLIC/TMP/PUBTMP_DEFAULT conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_PUBLIC|\n\t\t\t       IPV6_PREFER_SRC_TMP|\n\t\t\t       IPV6_PREFER_SRC_PUBTMP_DEFAULT)) {\n\t\tcase IPV6_PREFER_SRC_PUBLIC:\n\t\t\tpref |= IPV6_PREFER_SRC_PUBLIC;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_TMP:\n\t\t\tpref |= IPV6_PREFER_SRC_TMP;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_PUBTMP_DEFAULT:\n\t\t\tbreak;\n\t\tcase 0:\n\t\t\tgoto pref_skip_pubtmp;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~(IPV6_PREFER_SRC_PUBLIC|\n\t\t\t      IPV6_PREFER_SRC_TMP);\npref_skip_pubtmp:\n\n\t\t/* check HOME/COA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_HOME|IPV6_PREFER_SRC_COA)) {\n\t\tcase IPV6_PREFER_SRC_HOME:\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_COA:\n\t\t\tpref |= IPV6_PREFER_SRC_COA;\n\t\tcase 0:\n\t\t\tgoto pref_skip_coa;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~IPV6_PREFER_SRC_COA;\npref_skip_coa:\n\n\t\t/* check CGA/NONCGA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_CGA|IPV6_PREFER_SRC_NONCGA)) {\n\t\tcase IPV6_PREFER_SRC_CGA:\n\t\tcase IPV6_PREFER_SRC_NONCGA:\n\t\tcase 0:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tnp->srcprefs = (np->srcprefs & prefmask) | pref;\n\t\tretv = 0;\n\n\t\tbreak;\n\t    }\n\tcase IPV6_MINHOPCOUNT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tnp->min_hopcount = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_DONTFRAG:\n\t\tnp->dontfrag = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tnp->autoflowlabel = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\n\treturn retv;\n\ne_inval:\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\treturn -EINVAL;\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nint sock_setsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tint val;\n\tint valbool;\n\tstruct linger ling;\n\tint ret = 0;\n\n\t/*\n\t *\tOptions without arguments\n\t */\n\n\tif (optname == SO_BINDTODEVICE)\n\t\treturn sock_bindtodevice(sk, optval, optlen);\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\n\tvalbool = val ? 1 : 0;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase SO_DEBUG:\n\t\tif (val && !capable(CAP_NET_ADMIN))\n\t\t\tret = -EACCES;\n\t\telse\n\t\t\tsock_valbool_flag(sk, SOCK_DBG, valbool);\n\t\tbreak;\n\tcase SO_REUSEADDR:\n\t\tsk->sk_reuse = (valbool ? SK_CAN_REUSE : SK_NO_REUSE);\n\t\tbreak;\n\tcase SO_TYPE:\n\tcase SO_PROTOCOL:\n\tcase SO_DOMAIN:\n\tcase SO_ERROR:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\tcase SO_DONTROUTE:\n\t\tsock_valbool_flag(sk, SOCK_LOCALROUTE, valbool);\n\t\tbreak;\n\tcase SO_BROADCAST:\n\t\tsock_valbool_flag(sk, SOCK_BROADCAST, valbool);\n\t\tbreak;\n\tcase SO_SNDBUF:\n\t\t/* Don't error on this BSD doesn't and if you think\n\t\t * about it this is right. Otherwise apps have to\n\t\t * play 'guess the biggest size' games. RCVBUF/SNDBUF\n\t\t * are treated in BSD as hints\n\t\t */\n\t\tval = min_t(u32, val, sysctl_wmem_max);\nset_sndbuf:\n\t\tsk->sk_userlocks |= SOCK_SNDBUF_LOCK;\n\t\tsk->sk_sndbuf = max_t(u32, val * 2, SOCK_MIN_SNDBUF);\n\t\t/* Wake up sending tasks if we upped the value. */\n\t\tsk->sk_write_space(sk);\n\t\tbreak;\n\n\tcase SO_SNDBUFFORCE:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tgoto set_sndbuf;\n\n\tcase SO_RCVBUF:\n\t\t/* Don't error on this BSD doesn't and if you think\n\t\t * about it this is right. Otherwise apps have to\n\t\t * play 'guess the biggest size' games. RCVBUF/SNDBUF\n\t\t * are treated in BSD as hints\n\t\t */\n\t\tval = min_t(u32, val, sysctl_rmem_max);\nset_rcvbuf:\n\t\tsk->sk_userlocks |= SOCK_RCVBUF_LOCK;\n\t\t/*\n\t\t * We double it on the way in to account for\n\t\t * \"struct sk_buff\" etc. overhead.   Applications\n\t\t * assume that the SO_RCVBUF setting they make will\n\t\t * allow that much actual data to be received on that\n\t\t * socket.\n\t\t *\n\t\t * Applications are unaware that \"struct sk_buff\" and\n\t\t * other overheads allocate from the receive buffer\n\t\t * during socket buffer allocation.\n\t\t *\n\t\t * And after considering the possible alternatives,\n\t\t * returning the value we actually used in getsockopt\n\t\t * is the most desirable behavior.\n\t\t */\n\t\tsk->sk_rcvbuf = max_t(u32, val * 2, SOCK_MIN_RCVBUF);\n\t\tbreak;\n\n\tcase SO_RCVBUFFORCE:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tgoto set_rcvbuf;\n\n\tcase SO_KEEPALIVE:\n#ifdef CONFIG_INET\n\t\tif (sk->sk_protocol == IPPROTO_TCP)\n\t\t\ttcp_set_keepalive(sk, valbool);\n#endif\n\t\tsock_valbool_flag(sk, SOCK_KEEPOPEN, valbool);\n\t\tbreak;\n\n\tcase SO_OOBINLINE:\n\t\tsock_valbool_flag(sk, SOCK_URGINLINE, valbool);\n\t\tbreak;\n\n\tcase SO_NO_CHECK:\n\t\tsk->sk_no_check = valbool;\n\t\tbreak;\n\n\tcase SO_PRIORITY:\n\t\tif ((val >= 0 && val <= 6) || capable(CAP_NET_ADMIN))\n\t\t\tsk->sk_priority = val;\n\t\telse\n\t\t\tret = -EPERM;\n\t\tbreak;\n\n\tcase SO_LINGER:\n\t\tif (optlen < sizeof(ling)) {\n\t\t\tret = -EINVAL;\t/* 1003.1g */\n\t\t\tbreak;\n\t\t}\n\t\tif (copy_from_user(&ling, optval, sizeof(ling))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!ling.l_onoff)\n\t\t\tsock_reset_flag(sk, SOCK_LINGER);\n\t\telse {\n#if (BITS_PER_LONG == 32)\n\t\t\tif ((unsigned int)ling.l_linger >= MAX_SCHEDULE_TIMEOUT/HZ)\n\t\t\t\tsk->sk_lingertime = MAX_SCHEDULE_TIMEOUT;\n\t\t\telse\n#endif\n\t\t\t\tsk->sk_lingertime = (unsigned int)ling.l_linger * HZ;\n\t\t\tsock_set_flag(sk, SOCK_LINGER);\n\t\t}\n\t\tbreak;\n\n\tcase SO_BSDCOMPAT:\n\t\tsock_warn_obsolete_bsdism(\"setsockopt\");\n\t\tbreak;\n\n\tcase SO_PASSCRED:\n\t\tif (valbool)\n\t\t\tset_bit(SOCK_PASSCRED, &sock->flags);\n\t\telse\n\t\t\tclear_bit(SOCK_PASSCRED, &sock->flags);\n\t\tbreak;\n\n\tcase SO_TIMESTAMP:\n\tcase SO_TIMESTAMPNS:\n\t\tif (valbool)  {\n\t\t\tif (optname == SO_TIMESTAMP)\n\t\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t\telse\n\t\t\t\tsock_set_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t\tsock_set_flag(sk, SOCK_RCVTSTAMP);\n\t\t\tsock_enable_timestamp(sk, SOCK_TIMESTAMP);\n\t\t} else {\n\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMP);\n\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t}\n\t\tbreak;\n\n\tcase SO_TIMESTAMPING:\n\t\tif (val & ~SOF_TIMESTAMPING_MASK) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_TX_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_TX_HARDWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_TX_SOFTWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_TX_SOFTWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_RX_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_RX_HARDWARE);\n\t\tif (val & SOF_TIMESTAMPING_RX_SOFTWARE)\n\t\t\tsock_enable_timestamp(sk,\n\t\t\t\t\t      SOCK_TIMESTAMPING_RX_SOFTWARE);\n\t\telse\n\t\t\tsock_disable_timestamp(sk,\n\t\t\t\t\t       (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE));\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_SOFTWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_SOFTWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_SYS_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_SYS_HARDWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_RAW_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_RAW_HARDWARE);\n\t\tbreak;\n\n\tcase SO_RCVLOWAT:\n\t\tif (val < 0)\n\t\t\tval = INT_MAX;\n\t\tsk->sk_rcvlowat = val ? : 1;\n\t\tbreak;\n\n\tcase SO_RCVTIMEO:\n\t\tret = sock_set_timeout(&sk->sk_rcvtimeo, optval, optlen);\n\t\tbreak;\n\n\tcase SO_SNDTIMEO:\n\t\tret = sock_set_timeout(&sk->sk_sndtimeo, optval, optlen);\n\t\tbreak;\n\n\tcase SO_ATTACH_FILTER:\n\t\tret = -EINVAL;\n\t\tif (optlen == sizeof(struct sock_fprog)) {\n\t\t\tstruct sock_fprog fprog;\n\n\t\t\tret = -EFAULT;\n\t\t\tif (copy_from_user(&fprog, optval, sizeof(fprog)))\n\t\t\t\tbreak;\n\n\t\t\tret = sk_attach_filter(&fprog, sk);\n\t\t}\n\t\tbreak;\n\n\tcase SO_DETACH_FILTER:\n\t\tret = sk_detach_filter(sk);\n\t\tbreak;\n\n\tcase SO_PASSSEC:\n\t\tif (valbool)\n\t\t\tset_bit(SOCK_PASSSEC, &sock->flags);\n\t\telse\n\t\t\tclear_bit(SOCK_PASSSEC, &sock->flags);\n\t\tbreak;\n\tcase SO_MARK:\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tsk->sk_mark = val;\n\t\tbreak;\n\n\t\t/* We implement the SO_SNDLOWAT etc to\n\t\t   not be settable (1003.1g 5.3) */\n\tcase SO_RXQ_OVFL:\n\t\tsock_valbool_flag(sk, SOCK_RXQ_OVFL, valbool);\n\t\tbreak;\n\n\tcase SO_WIFI_STATUS:\n\t\tsock_valbool_flag(sk, SOCK_WIFI_STATUS, valbool);\n\t\tbreak;\n\n\tcase SO_PEEK_OFF:\n\t\tif (sock->ops->set_peek_off)\n\t\t\tsock->ops->set_peek_off(sk, val);\n\t\telse\n\t\t\tret = -EOPNOTSUPP;\n\t\tbreak;\n\n\tcase SO_NOFCS:\n\t\tsock_valbool_flag(sk, SOCK_NOFCS, valbool);\n\t\tbreak;\n\n\tdefault:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\trelease_sock(sk);\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, valbool;\n\tint retv = -ENOPROTOOPT;\n\tbool needs_rtnl = setsockopt_needs_rtnl(optname);\n\n\tif (!optval)\n\t\tval = 0;\n\telse {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (get_user(val, (int __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\tval = 0;\n\t}\n\n\tvalbool = (val != 0);\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_setsockopt(sk, optname, optval, optlen);\n\n\tif (needs_rtnl)\n\t\trtnl_lock();\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\n\tcase IPV6_ADDRFORM:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val == PF_INET) {\n\t\t\tstruct ipv6_txoptions *opt;\n\t\t\tstruct sk_buff *pktopt;\n\n\t\t\tif (sk->sk_type == SOCK_RAW)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_protocol == IPPROTO_UDP ||\n\t\t\t    sk->sk_protocol == IPPROTO_UDPLITE) {\n\t\t\t\tstruct udp_sock *up = udp_sk(sk);\n\t\t\t\tif (up->pending == AF_INET6) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (sk->sk_protocol != IPPROTO_TCP)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t\tretv = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (ipv6_only_sock(sk) ||\n\t\t\t    !ipv6_addr_v4mapped(&sk->sk_v6_daddr)) {\n\t\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tfl6_free_socklist(sk);\n\t\t\tipv6_sock_mc_close(sk);\n\n\t\t\t/*\n\t\t\t * Sock is moving from IPv6 to IPv4 (sk_prot), so\n\t\t\t * remove it from the refcnt debug socks count in the\n\t\t\t * original family...\n\t\t\t */\n\t\t\tsk_refcnt_debug_dec(sk);\n\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = &tcp_prot;\n\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;\n\t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t\t} else {\n\t\t\t\tstruct proto *prot = &udp_prot;\n\n\t\t\t\tif (sk->sk_protocol == IPPROTO_UDPLITE)\n\t\t\t\t\tprot = &udplite_prot;\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = prot;\n\t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t}\n\t\t\topt = xchg(&np->opt, NULL);\n\t\t\tif (opt)\n\t\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t\t\tpktopt = xchg(&np->pktoptions, NULL);\n\t\t\tkfree_skb(pktopt);\n\n\t\t\tsk->sk_destruct = inet_sock_destruct;\n\t\t\t/*\n\t\t\t * ... and add it to the refcnt debug socks count\n\t\t\t * in the new family. -acme\n\t\t\t */\n\t\t\tsk_refcnt_debug_inc(sk);\n\t\t\tmodule_put(THIS_MODULE);\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\t\tgoto e_inval;\n\n\tcase IPV6_V6ONLY:\n\t\tif (optlen < sizeof(int) ||\n\t\t    inet_sk(sk)->inet_num)\n\t\t\tgoto e_inval;\n\t\tsk->sk_ipv6only = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxoinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxhlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxohlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.srcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.osrcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.hopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.ohopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.dstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.odstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < -1 || val > 0xff)\n\t\t\tgoto e_inval;\n\t\t/* RFC 3542, 6.5: default traffic class of 0x0 */\n\t\tif (val == -1)\n\t\t\tval = 0;\n\t\tnp->tclass = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxtclass = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxpmtu = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TRANSPARENT:\n\t\tif (valbool && !ns_capable(net->user_ns, CAP_NET_ADMIN) &&\n\t\t    !ns_capable(net->user_ns, CAP_NET_RAW)) {\n\t\t\tretv = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we don't have a separate transparent bit for IPV6 we use the one in the IPv4 socket */\n\t\tinet_sk(sk)->transparent = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxorigdstaddr = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\t/* remove any sticky options header with a zero option\n\t\t * length, per RFC3542.\n\t\t */\n\t\tif (optlen == 0)\n\t\t\toptval = NULL;\n\t\telse if (!optval)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct ipv6_opt_hdr) ||\n\t\t\t optlen & 0x7 || optlen > 8 * 255)\n\t\t\tgoto e_inval;\n\n\t\t/* hop-by-hop / destination options are privileged option */\n\t\tretv = -EPERM;\n\t\tif (optname != IPV6_RTHDR && !ns_capable(net->user_ns, CAP_NET_RAW))\n\t\t\tbreak;\n\n\t\topt = ipv6_renew_options(sk, np->opt, optname,\n\t\t\t\t\t (struct ipv6_opt_hdr __user *)optval,\n\t\t\t\t\t optlen);\n\t\tif (IS_ERR(opt)) {\n\t\t\tretv = PTR_ERR(opt);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* routing header option needs extra check */\n\t\tretv = -EINVAL;\n\t\tif (optname == IPV6_RTHDR && opt && opt->srcrt) {\n\t\t\tstruct ipv6_rt_hdr *rthdr = opt->srcrt;\n\t\t\tswitch (rthdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\t\tcase IPV6_SRCRT_TYPE_2:\n\t\t\t\tif (rthdr->hdrlen != 2 ||\n\t\t\t\t    rthdr->segments_left != 1)\n\t\t\t\t\tgoto sticky_done;\n\n\t\t\t\tbreak;\n#endif\n\t\t\tdefault:\n\t\t\t\tgoto sticky_done;\n\t\t\t}\n\t\t}\n\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\nsticky_done:\n\t\tif (opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t\tbreak;\n\t}\n\n\tcase IPV6_PKTINFO:\n\t{\n\t\tstruct in6_pktinfo pkt;\n\n\t\tif (optlen == 0)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct in6_pktinfo) || !optval)\n\t\t\tgoto e_inval;\n\n\t\tif (copy_from_user(&pkt, optval, sizeof(struct in6_pktinfo))) {\n\t\t\t\tretv = -EFAULT;\n\t\t\t\tbreak;\n\t\t}\n\t\tif (sk->sk_bound_dev_if && pkt.ipi6_ifindex != sk->sk_bound_dev_if)\n\t\t\tgoto e_inval;\n\n\t\tnp->sticky_pktinfo.ipi6_ifindex = pkt.ipi6_ifindex;\n\t\tnp->sticky_pktinfo.ipi6_addr = pkt.ipi6_addr;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct ipv6_txoptions *opt = NULL;\n\t\tstruct msghdr msg;\n\t\tstruct flowi6 fl6;\n\t\tint junk;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\n\t\tif (optlen == 0)\n\t\t\tgoto update;\n\n\t\t/* 1K is probably excessive\n\t\t * 1K is surely not enough, 2K per standard header is 16K.\n\t\t */\n\t\tretv = -EINVAL;\n\t\tif (optlen > 64*1024)\n\t\t\tbreak;\n\n\t\topt = sock_kmalloc(sk, sizeof(*opt) + optlen, GFP_KERNEL);\n\t\tretv = -ENOBUFS;\n\t\tif (!opt)\n\t\t\tbreak;\n\n\t\tmemset(opt, 0, sizeof(*opt));\n\t\topt->tot_len = sizeof(*opt) + optlen;\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(opt+1, optval, optlen))\n\t\t\tgoto done;\n\n\t\tmsg.msg_controllen = optlen;\n\t\tmsg.msg_control = (void *)(opt+1);\n\n\t\tretv = ip6_datagram_send_ctl(net, sk, &msg, &fl6, opt, &junk,\n\t\t\t\t\t     &junk, &junk);\n\t\tif (retv)\n\t\t\tgoto done;\nupdate:\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\ndone:\n\t\tif (opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t\tbreak;\n\t}\n\tcase IPV6_UNICAST_HOPS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->hop_limit = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_HOPS:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->mcast_hops = (val == -1 ? IPV6_DEFAULT_MCASTHOPS : val);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val != valbool)\n\t\t\tgoto e_inval;\n\t\tnp->mc_loop = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t{\n\t\tstruct net_device *dev = NULL;\n\t\tint ifindex;\n\n\t\tif (optlen != sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tifindex = (__force int)ntohl((__force __be32)val);\n\t\tif (ifindex == 0) {\n\t\t\tnp->ucast_oif = 0;\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tdev = dev_get_by_index(net, ifindex);\n\t\tretv = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\tretv = -EINVAL;\n\t\tif (sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tnp->ucast_oif = ifindex;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_IF:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tif (val) {\n\t\t\tstruct net_device *dev;\n\n\t\t\tif (sk->sk_bound_dev_if && sk->sk_bound_dev_if != val)\n\t\t\t\tgoto e_inval;\n\n\t\t\tdev = dev_get_by_index(net, val);\n\t\t\tif (!dev) {\n\t\t\t\tretv = -ENODEV;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdev_put(dev);\n\t\t}\n\t\tnp->mcast_oif = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_ADD_MEMBERSHIP)\n\t\t\tretv = ipv6_sock_mc_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_JOIN_ANYCAST)\n\t\t\tretv = ipv6_sock_ac_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_ac_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t{\n\t\tstruct group_req greq;\n\t\tstruct sockaddr_in6 *psin6;\n\n\t\tif (optlen < sizeof(struct group_req))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&greq, optval, sizeof(struct group_req)))\n\t\t\tbreak;\n\t\tif (greq.gr_group.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tpsin6 = (struct sockaddr_in6 *)&greq.gr_group;\n\t\tif (optname == MCAST_JOIN_GROUP)\n\t\t\tretv = ipv6_sock_mc_join(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t{\n\t\tstruct group_source_req greqs;\n\t\tint omode, add;\n\n\t\tif (optlen < sizeof(struct group_source_req))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&greqs, optval, sizeof(greqs))) {\n\t\t\tretv = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (greqs.gsr_group.ss_family != AF_INET6 ||\n\t\t    greqs.gsr_source.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tif (optname == MCAST_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == MCAST_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == MCAST_JOIN_SOURCE_GROUP) {\n\t\t\tstruct sockaddr_in6 *psin6;\n\n\t\t\tpsin6 = (struct sockaddr_in6 *)&greqs.gsr_group;\n\t\t\tretv = ipv6_sock_mc_join(sk, greqs.gsr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\t\t/* prior join w/ different source is ok */\n\t\t\tif (retv && retv != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* MCAST_LEAVE_SOURCE_GROUP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\tretv = ip6_mc_source(add, omode, sk, &greqs);\n\t\tbreak;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter *gsf;\n\n\t\tif (optlen < GROUP_FILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tgsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!gsf) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(gsf, optval, optlen)) {\n\t\t\tkfree(gsf);\n\t\t\tbreak;\n\t\t}\n\t\t/* numsrc >= (4G-140)/128 overflow in 32 bits */\n\t\tif (gsf->gf_numsrc >= 0x1ffffffU ||\n\t\t    gsf->gf_numsrc > sysctl_mld_max_msf) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tif (GROUP_FILTER_SIZE(gsf->gf_numsrc) > optlen) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tretv = ip6_mc_msfilter(sk, gsf);\n\t\tkfree(gsf);\n\n\t\tbreak;\n\t}\n\tcase IPV6_ROUTER_ALERT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = ip6_ra_control(sk, val);\n\t\tbreak;\n\tcase IPV6_MTU_DISCOVER:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < IPV6_PMTUDISC_DONT || val > IPV6_PMTUDISC_OMIT)\n\t\t\tgoto e_inval;\n\t\tnp->pmtudisc = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val && val < IPV6_MIN_MTU)\n\t\t\tgoto e_inval;\n\t\tnp->frag_size = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->recverr = valbool;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWINFO_SEND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->sndflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWLABEL_MGR:\n\t\tretv = ipv6_flowlabel_opt(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_IPSEC_POLICY:\n\tcase IPV6_XFRM_POLICY:\n\t\tretv = -EPERM;\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\tretv = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t    {\n\t\tunsigned int pref = 0;\n\t\tunsigned int prefmask = ~0;\n\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EINVAL;\n\n\t\t/* check PUBLIC/TMP/PUBTMP_DEFAULT conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_PUBLIC|\n\t\t\t       IPV6_PREFER_SRC_TMP|\n\t\t\t       IPV6_PREFER_SRC_PUBTMP_DEFAULT)) {\n\t\tcase IPV6_PREFER_SRC_PUBLIC:\n\t\t\tpref |= IPV6_PREFER_SRC_PUBLIC;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_TMP:\n\t\t\tpref |= IPV6_PREFER_SRC_TMP;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_PUBTMP_DEFAULT:\n\t\t\tbreak;\n\t\tcase 0:\n\t\t\tgoto pref_skip_pubtmp;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~(IPV6_PREFER_SRC_PUBLIC|\n\t\t\t      IPV6_PREFER_SRC_TMP);\npref_skip_pubtmp:\n\n\t\t/* check HOME/COA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_HOME|IPV6_PREFER_SRC_COA)) {\n\t\tcase IPV6_PREFER_SRC_HOME:\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_COA:\n\t\t\tpref |= IPV6_PREFER_SRC_COA;\n\t\tcase 0:\n\t\t\tgoto pref_skip_coa;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~IPV6_PREFER_SRC_COA;\npref_skip_coa:\n\n\t\t/* check CGA/NONCGA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_CGA|IPV6_PREFER_SRC_NONCGA)) {\n\t\tcase IPV6_PREFER_SRC_CGA:\n\t\tcase IPV6_PREFER_SRC_NONCGA:\n\t\tcase 0:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tnp->srcprefs = (np->srcprefs & prefmask) | pref;\n\t\tretv = 0;\n\n\t\tbreak;\n\t    }\n\tcase IPV6_MINHOPCOUNT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tnp->min_hopcount = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_DONTFRAG:\n\t\tnp->dontfrag = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tnp->autoflowlabel = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\n\treturn retv;\n\ne_inval:\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\treturn -EINVAL;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, valbool;\n\tint retv = -ENOPROTOOPT;\n\tbool needs_rtnl = setsockopt_needs_rtnl(optname);\n\n\tif (!optval)\n\t\tval = 0;\n\telse {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (get_user(val, (int __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\tval = 0;\n\t}\n\n\tvalbool = (val != 0);\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_setsockopt(sk, optname, optval, optlen);\n\n\tif (needs_rtnl)\n\t\trtnl_lock();\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\n\tcase IPV6_ADDRFORM:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val == PF_INET) {\n\t\t\tstruct ipv6_txoptions *opt;\n\t\t\tstruct sk_buff *pktopt;\n\n\t\t\tif (sk->sk_type == SOCK_RAW)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_protocol == IPPROTO_UDP ||\n\t\t\t    sk->sk_protocol == IPPROTO_UDPLITE) {\n\t\t\t\tstruct udp_sock *up = udp_sk(sk);\n\t\t\t\tif (up->pending == AF_INET6) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (sk->sk_protocol != IPPROTO_TCP)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t\tretv = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (ipv6_only_sock(sk) ||\n\t\t\t    !ipv6_addr_v4mapped(&sk->sk_v6_daddr)) {\n\t\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tfl6_free_socklist(sk);\n\t\t\tipv6_sock_mc_close(sk);\n\n\t\t\t/*\n\t\t\t * Sock is moving from IPv6 to IPv4 (sk_prot), so\n\t\t\t * remove it from the refcnt debug socks count in the\n\t\t\t * original family...\n\t\t\t */\n\t\t\tsk_refcnt_debug_dec(sk);\n\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = &tcp_prot;\n\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;\n\t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t\t} else {\n\t\t\t\tstruct proto *prot = &udp_prot;\n\n\t\t\t\tif (sk->sk_protocol == IPPROTO_UDPLITE)\n\t\t\t\t\tprot = &udplite_prot;\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = prot;\n\t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t}\n\t\t\topt = xchg((__force struct ipv6_txoptions **)&np->opt,\n\t\t\t\t   NULL);\n\t\t\tif (opt) {\n\t\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\t\ttxopt_put(opt);\n\t\t\t}\n\t\t\tpktopt = xchg(&np->pktoptions, NULL);\n\t\t\tkfree_skb(pktopt);\n\n\t\t\tsk->sk_destruct = inet_sock_destruct;\n\t\t\t/*\n\t\t\t * ... and add it to the refcnt debug socks count\n\t\t\t * in the new family. -acme\n\t\t\t */\n\t\t\tsk_refcnt_debug_inc(sk);\n\t\t\tmodule_put(THIS_MODULE);\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\t\tgoto e_inval;\n\n\tcase IPV6_V6ONLY:\n\t\tif (optlen < sizeof(int) ||\n\t\t    inet_sk(sk)->inet_num)\n\t\t\tgoto e_inval;\n\t\tsk->sk_ipv6only = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxoinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxhlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxohlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.srcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.osrcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.hopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.ohopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.dstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.odstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < -1 || val > 0xff)\n\t\t\tgoto e_inval;\n\t\t/* RFC 3542, 6.5: default traffic class of 0x0 */\n\t\tif (val == -1)\n\t\t\tval = 0;\n\t\tnp->tclass = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxtclass = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxpmtu = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TRANSPARENT:\n\t\tif (valbool && !ns_capable(net->user_ns, CAP_NET_ADMIN) &&\n\t\t    !ns_capable(net->user_ns, CAP_NET_RAW)) {\n\t\t\tretv = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we don't have a separate transparent bit for IPV6 we use the one in the IPv4 socket */\n\t\tinet_sk(sk)->transparent = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxorigdstaddr = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\t/* remove any sticky options header with a zero option\n\t\t * length, per RFC3542.\n\t\t */\n\t\tif (optlen == 0)\n\t\t\toptval = NULL;\n\t\telse if (!optval)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct ipv6_opt_hdr) ||\n\t\t\t optlen & 0x7 || optlen > 8 * 255)\n\t\t\tgoto e_inval;\n\n\t\t/* hop-by-hop / destination options are privileged option */\n\t\tretv = -EPERM;\n\t\tif (optname != IPV6_RTHDR && !ns_capable(net->user_ns, CAP_NET_RAW))\n\t\t\tbreak;\n\n\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\t\topt = ipv6_renew_options(sk, opt, optname,\n\t\t\t\t\t (struct ipv6_opt_hdr __user *)optval,\n\t\t\t\t\t optlen);\n\t\tif (IS_ERR(opt)) {\n\t\t\tretv = PTR_ERR(opt);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* routing header option needs extra check */\n\t\tretv = -EINVAL;\n\t\tif (optname == IPV6_RTHDR && opt && opt->srcrt) {\n\t\t\tstruct ipv6_rt_hdr *rthdr = opt->srcrt;\n\t\t\tswitch (rthdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\t\tcase IPV6_SRCRT_TYPE_2:\n\t\t\t\tif (rthdr->hdrlen != 2 ||\n\t\t\t\t    rthdr->segments_left != 1)\n\t\t\t\t\tgoto sticky_done;\n\n\t\t\t\tbreak;\n#endif\n\t\t\tdefault:\n\t\t\t\tgoto sticky_done;\n\t\t\t}\n\t\t}\n\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\nsticky_done:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase IPV6_PKTINFO:\n\t{\n\t\tstruct in6_pktinfo pkt;\n\n\t\tif (optlen == 0)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct in6_pktinfo) || !optval)\n\t\t\tgoto e_inval;\n\n\t\tif (copy_from_user(&pkt, optval, sizeof(struct in6_pktinfo))) {\n\t\t\t\tretv = -EFAULT;\n\t\t\t\tbreak;\n\t\t}\n\t\tif (sk->sk_bound_dev_if && pkt.ipi6_ifindex != sk->sk_bound_dev_if)\n\t\t\tgoto e_inval;\n\n\t\tnp->sticky_pktinfo.ipi6_ifindex = pkt.ipi6_ifindex;\n\t\tnp->sticky_pktinfo.ipi6_addr = pkt.ipi6_addr;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct ipv6_txoptions *opt = NULL;\n\t\tstruct msghdr msg;\n\t\tstruct flowi6 fl6;\n\t\tint junk;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\n\t\tif (optlen == 0)\n\t\t\tgoto update;\n\n\t\t/* 1K is probably excessive\n\t\t * 1K is surely not enough, 2K per standard header is 16K.\n\t\t */\n\t\tretv = -EINVAL;\n\t\tif (optlen > 64*1024)\n\t\t\tbreak;\n\n\t\topt = sock_kmalloc(sk, sizeof(*opt) + optlen, GFP_KERNEL);\n\t\tretv = -ENOBUFS;\n\t\tif (!opt)\n\t\t\tbreak;\n\n\t\tmemset(opt, 0, sizeof(*opt));\n\t\tatomic_set(&opt->refcnt, 1);\n\t\topt->tot_len = sizeof(*opt) + optlen;\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(opt+1, optval, optlen))\n\t\t\tgoto done;\n\n\t\tmsg.msg_controllen = optlen;\n\t\tmsg.msg_control = (void *)(opt+1);\n\n\t\tretv = ip6_datagram_send_ctl(net, sk, &msg, &fl6, opt, &junk,\n\t\t\t\t\t     &junk, &junk);\n\t\tif (retv)\n\t\t\tgoto done;\nupdate:\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\ndone:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\tcase IPV6_UNICAST_HOPS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->hop_limit = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_HOPS:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->mcast_hops = (val == -1 ? IPV6_DEFAULT_MCASTHOPS : val);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val != valbool)\n\t\t\tgoto e_inval;\n\t\tnp->mc_loop = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t{\n\t\tstruct net_device *dev = NULL;\n\t\tint ifindex;\n\n\t\tif (optlen != sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tifindex = (__force int)ntohl((__force __be32)val);\n\t\tif (ifindex == 0) {\n\t\t\tnp->ucast_oif = 0;\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tdev = dev_get_by_index(net, ifindex);\n\t\tretv = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\tretv = -EINVAL;\n\t\tif (sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tnp->ucast_oif = ifindex;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_IF:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tif (val) {\n\t\t\tstruct net_device *dev;\n\n\t\t\tif (sk->sk_bound_dev_if && sk->sk_bound_dev_if != val)\n\t\t\t\tgoto e_inval;\n\n\t\t\tdev = dev_get_by_index(net, val);\n\t\t\tif (!dev) {\n\t\t\t\tretv = -ENODEV;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdev_put(dev);\n\t\t}\n\t\tnp->mcast_oif = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_ADD_MEMBERSHIP)\n\t\t\tretv = ipv6_sock_mc_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_JOIN_ANYCAST)\n\t\t\tretv = ipv6_sock_ac_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_ac_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t{\n\t\tstruct group_req greq;\n\t\tstruct sockaddr_in6 *psin6;\n\n\t\tif (optlen < sizeof(struct group_req))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&greq, optval, sizeof(struct group_req)))\n\t\t\tbreak;\n\t\tif (greq.gr_group.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tpsin6 = (struct sockaddr_in6 *)&greq.gr_group;\n\t\tif (optname == MCAST_JOIN_GROUP)\n\t\t\tretv = ipv6_sock_mc_join(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t{\n\t\tstruct group_source_req greqs;\n\t\tint omode, add;\n\n\t\tif (optlen < sizeof(struct group_source_req))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&greqs, optval, sizeof(greqs))) {\n\t\t\tretv = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (greqs.gsr_group.ss_family != AF_INET6 ||\n\t\t    greqs.gsr_source.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tif (optname == MCAST_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == MCAST_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == MCAST_JOIN_SOURCE_GROUP) {\n\t\t\tstruct sockaddr_in6 *psin6;\n\n\t\t\tpsin6 = (struct sockaddr_in6 *)&greqs.gsr_group;\n\t\t\tretv = ipv6_sock_mc_join(sk, greqs.gsr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\t\t/* prior join w/ different source is ok */\n\t\t\tif (retv && retv != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* MCAST_LEAVE_SOURCE_GROUP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\tretv = ip6_mc_source(add, omode, sk, &greqs);\n\t\tbreak;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter *gsf;\n\n\t\tif (optlen < GROUP_FILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tgsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!gsf) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(gsf, optval, optlen)) {\n\t\t\tkfree(gsf);\n\t\t\tbreak;\n\t\t}\n\t\t/* numsrc >= (4G-140)/128 overflow in 32 bits */\n\t\tif (gsf->gf_numsrc >= 0x1ffffffU ||\n\t\t    gsf->gf_numsrc > sysctl_mld_max_msf) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tif (GROUP_FILTER_SIZE(gsf->gf_numsrc) > optlen) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tretv = ip6_mc_msfilter(sk, gsf);\n\t\tkfree(gsf);\n\n\t\tbreak;\n\t}\n\tcase IPV6_ROUTER_ALERT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = ip6_ra_control(sk, val);\n\t\tbreak;\n\tcase IPV6_MTU_DISCOVER:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < IPV6_PMTUDISC_DONT || val > IPV6_PMTUDISC_OMIT)\n\t\t\tgoto e_inval;\n\t\tnp->pmtudisc = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val && val < IPV6_MIN_MTU)\n\t\t\tgoto e_inval;\n\t\tnp->frag_size = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->recverr = valbool;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWINFO_SEND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->sndflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWLABEL_MGR:\n\t\tretv = ipv6_flowlabel_opt(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_IPSEC_POLICY:\n\tcase IPV6_XFRM_POLICY:\n\t\tretv = -EPERM;\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\tretv = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t    {\n\t\tunsigned int pref = 0;\n\t\tunsigned int prefmask = ~0;\n\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EINVAL;\n\n\t\t/* check PUBLIC/TMP/PUBTMP_DEFAULT conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_PUBLIC|\n\t\t\t       IPV6_PREFER_SRC_TMP|\n\t\t\t       IPV6_PREFER_SRC_PUBTMP_DEFAULT)) {\n\t\tcase IPV6_PREFER_SRC_PUBLIC:\n\t\t\tpref |= IPV6_PREFER_SRC_PUBLIC;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_TMP:\n\t\t\tpref |= IPV6_PREFER_SRC_TMP;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_PUBTMP_DEFAULT:\n\t\t\tbreak;\n\t\tcase 0:\n\t\t\tgoto pref_skip_pubtmp;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~(IPV6_PREFER_SRC_PUBLIC|\n\t\t\t      IPV6_PREFER_SRC_TMP);\npref_skip_pubtmp:\n\n\t\t/* check HOME/COA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_HOME|IPV6_PREFER_SRC_COA)) {\n\t\tcase IPV6_PREFER_SRC_HOME:\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_COA:\n\t\t\tpref |= IPV6_PREFER_SRC_COA;\n\t\tcase 0:\n\t\t\tgoto pref_skip_coa;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~IPV6_PREFER_SRC_COA;\npref_skip_coa:\n\n\t\t/* check CGA/NONCGA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_CGA|IPV6_PREFER_SRC_NONCGA)) {\n\t\tcase IPV6_PREFER_SRC_CGA:\n\t\tcase IPV6_PREFER_SRC_NONCGA:\n\t\tcase 0:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tnp->srcprefs = (np->srcprefs & prefmask) | pref;\n\t\tretv = 0;\n\n\t\tbreak;\n\t    }\n\tcase IPV6_MINHOPCOUNT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tnp->min_hopcount = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_DONTFRAG:\n\t\tnp->dontfrag = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tnp->autoflowlabel = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\n\treturn retv;\n\ne_inval:\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\treturn -EINVAL;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int do_ipv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, int __user *optlen, unsigned int flags)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tint len;\n\tint val;\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_getsockopt(sk, optname, optval, optlen);\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tswitch (optname) {\n\tcase IPV6_ADDRFORM:\n\t\tif (sk->sk_protocol != IPPROTO_UDP &&\n\t\t    sk->sk_protocol != IPPROTO_UDPLITE &&\n\t\t    sk->sk_protocol != IPPROTO_TCP)\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -ENOTCONN;\n\t\tval = sk->sk_family;\n\t\tbreak;\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter gsf;\n\t\tint err;\n\n\t\tif (len < GROUP_FILTER_SIZE(0))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&gsf, optval, GROUP_FILTER_SIZE(0)))\n\t\t\treturn -EFAULT;\n\t\tif (gsf.gf_group.ss_family != AF_INET6)\n\t\t\treturn -EADDRNOTAVAIL;\n\t\tlock_sock(sk);\n\t\terr = ip6_mc_msfget(sk, &gsf,\n\t\t\t(struct group_filter __user *)optval, optlen);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct msghdr msg;\n\t\tstruct sk_buff *skb;\n\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\treturn -ENOPROTOOPT;\n\n\t\tmsg.msg_control = optval;\n\t\tmsg.msg_controllen = len;\n\t\tmsg.msg_flags = flags;\n\n\t\tlock_sock(sk);\n\t\tskb = np->pktoptions;\n\t\tif (skb)\n\t\t\tip6_datagram_recv_ctl(sk, &msg, skb);\n\t\trelease_sock(sk);\n\t\tif (!skb) {\n\t\t\tif (np->rxopt.bits.rxinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr : np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxhlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxtclass) {\n\t\t\t\tint tclass = (int)ip6_tclass(np->rcv_flowinfo);\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_TCLASS, sizeof(tclass), &tclass);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxoinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr :\n\t\t\t\t\t\t\t\t     np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxohlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxflow) {\n\t\t\t\t__be32 flowinfo = np->rcv_flowinfo;\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_FLOWINFO, sizeof(flowinfo), &flowinfo);\n\t\t\t}\n\t\t}\n\t\tlen -= msg.msg_controllen;\n\t\treturn put_user(len, optlen);\n\t}\n\tcase IPV6_MTU:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tval = 0;\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tval = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!val)\n\t\t\treturn -ENOTCONN;\n\t\tbreak;\n\t}\n\n\tcase IPV6_V6ONLY:\n\t\tval = sk->sk_ipv6only;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tval = np->rxopt.bits.rxinfo;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tval = np->rxopt.bits.rxoinfo;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tval = np->rxopt.bits.rxhlim;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tval = np->rxopt.bits.rxohlim;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tval = np->rxopt.bits.srcrt;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tval = np->rxopt.bits.osrcrt;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\n\t\tlock_sock(sk);\n\t\tlen = ipv6_getsockopt_sticky(sk, np->opt,\n\t\t\t\t\t     optname, optval, len);\n\t\trelease_sock(sk);\n\t\t/* check if ipv6_getsockopt_sticky() returns err code */\n\t\tif (len < 0)\n\t\t\treturn len;\n\t\treturn put_user(len, optlen);\n\t}\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tval = np->rxopt.bits.hopopts;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tval = np->rxopt.bits.ohopopts;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tval = np->rxopt.bits.dstopts;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tval = np->rxopt.bits.odstopts;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tval = np->tclass;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tval = np->rxopt.bits.rxtclass;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tval = np->rxopt.bits.rxflow;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tval = np->rxopt.bits.rxpmtu;\n\t\tbreak;\n\n\tcase IPV6_PATHMTU:\n\t{\n\t\tstruct dst_entry *dst;\n\t\tstruct ip6_mtuinfo mtuinfo;\n\n\t\tif (len < sizeof(mtuinfo))\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(mtuinfo);\n\t\tmemset(&mtuinfo, 0, sizeof(mtuinfo));\n\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tmtuinfo.ip6m_mtu = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!mtuinfo.ip6m_mtu)\n\t\t\treturn -ENOTCONN;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &mtuinfo, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_TRANSPARENT:\n\t\tval = inet_sk(sk)->transparent;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tval = np->rxopt.bits.rxorigdstaddr;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_HOPS:\n\tcase IPV6_MULTICAST_HOPS:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tif (optname == IPV6_UNICAST_HOPS)\n\t\t\tval = np->hop_limit;\n\t\telse\n\t\t\tval = np->mcast_hops;\n\n\t\tif (val < 0) {\n\t\t\trcu_read_lock();\n\t\t\tdst = __sk_dst_get(sk);\n\t\t\tif (dst)\n\t\t\t\tval = ip6_dst_hoplimit(dst);\n\t\t\trcu_read_unlock();\n\t\t}\n\n\t\tif (val < 0)\n\t\t\tval = sock_net(sk)->ipv6.devconf_all->hop_limit;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tval = np->mc_loop;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_IF:\n\t\tval = np->mcast_oif;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t\tval = (__force int)htonl((__u32) np->ucast_oif);\n\t\tbreak;\n\n\tcase IPV6_MTU_DISCOVER:\n\t\tval = np->pmtudisc;\n\t\tbreak;\n\n\tcase IPV6_RECVERR:\n\t\tval = np->recverr;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO_SEND:\n\t\tval = np->sndflow;\n\t\tbreak;\n\n\tcase IPV6_FLOWLABEL_MGR:\n\t{\n\t\tstruct in6_flowlabel_req freq;\n\t\tint flags;\n\n\t\tif (len < sizeof(freq))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_user(&freq, optval, sizeof(freq)))\n\t\t\treturn -EFAULT;\n\n\t\tif (freq.flr_action != IPV6_FL_A_GET)\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(freq);\n\t\tflags = freq.flr_flags;\n\n\t\tmemset(&freq, 0, sizeof(freq));\n\n\t\tval = ipv6_flowlabel_opt_get(sk, &freq, flags);\n\t\tif (val < 0)\n\t\t\treturn val;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &freq, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t\tval = 0;\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_TMP)\n\t\t\tval |= IPV6_PREFER_SRC_TMP;\n\t\telse if (np->srcprefs & IPV6_PREFER_SRC_PUBLIC)\n\t\t\tval |= IPV6_PREFER_SRC_PUBLIC;\n\t\telse {\n\t\t\t/* XXX: should we return system default? */\n\t\t\tval |= IPV6_PREFER_SRC_PUBTMP_DEFAULT;\n\t\t}\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_COA)\n\t\t\tval |= IPV6_PREFER_SRC_COA;\n\t\telse\n\t\t\tval |= IPV6_PREFER_SRC_HOME;\n\t\tbreak;\n\n\tcase IPV6_MINHOPCOUNT:\n\t\tval = np->min_hopcount;\n\t\tbreak;\n\n\tcase IPV6_DONTFRAG:\n\t\tval = np->dontfrag;\n\t\tbreak;\n\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tval = np->autoflowlabel;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\tlen = min_t(unsigned int, sizeof(int), len);\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}",
                        "code_after_change": "static int do_ipv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, int __user *optlen, unsigned int flags)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tint len;\n\tint val;\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_getsockopt(sk, optname, optval, optlen);\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tswitch (optname) {\n\tcase IPV6_ADDRFORM:\n\t\tif (sk->sk_protocol != IPPROTO_UDP &&\n\t\t    sk->sk_protocol != IPPROTO_UDPLITE &&\n\t\t    sk->sk_protocol != IPPROTO_TCP)\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -ENOTCONN;\n\t\tval = sk->sk_family;\n\t\tbreak;\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter gsf;\n\t\tint err;\n\n\t\tif (len < GROUP_FILTER_SIZE(0))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&gsf, optval, GROUP_FILTER_SIZE(0)))\n\t\t\treturn -EFAULT;\n\t\tif (gsf.gf_group.ss_family != AF_INET6)\n\t\t\treturn -EADDRNOTAVAIL;\n\t\tlock_sock(sk);\n\t\terr = ip6_mc_msfget(sk, &gsf,\n\t\t\t(struct group_filter __user *)optval, optlen);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct msghdr msg;\n\t\tstruct sk_buff *skb;\n\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\treturn -ENOPROTOOPT;\n\n\t\tmsg.msg_control = optval;\n\t\tmsg.msg_controllen = len;\n\t\tmsg.msg_flags = flags;\n\n\t\tlock_sock(sk);\n\t\tskb = np->pktoptions;\n\t\tif (skb)\n\t\t\tip6_datagram_recv_ctl(sk, &msg, skb);\n\t\trelease_sock(sk);\n\t\tif (!skb) {\n\t\t\tif (np->rxopt.bits.rxinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr : np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxhlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxtclass) {\n\t\t\t\tint tclass = (int)ip6_tclass(np->rcv_flowinfo);\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_TCLASS, sizeof(tclass), &tclass);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxoinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr :\n\t\t\t\t\t\t\t\t     np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxohlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxflow) {\n\t\t\t\t__be32 flowinfo = np->rcv_flowinfo;\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_FLOWINFO, sizeof(flowinfo), &flowinfo);\n\t\t\t}\n\t\t}\n\t\tlen -= msg.msg_controllen;\n\t\treturn put_user(len, optlen);\n\t}\n\tcase IPV6_MTU:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tval = 0;\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tval = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!val)\n\t\t\treturn -ENOTCONN;\n\t\tbreak;\n\t}\n\n\tcase IPV6_V6ONLY:\n\t\tval = sk->sk_ipv6only;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tval = np->rxopt.bits.rxinfo;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tval = np->rxopt.bits.rxoinfo;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tval = np->rxopt.bits.rxhlim;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tval = np->rxopt.bits.rxohlim;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tval = np->rxopt.bits.srcrt;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tval = np->rxopt.bits.osrcrt;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\tlock_sock(sk);\n\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\t\tlen = ipv6_getsockopt_sticky(sk, opt, optname, optval, len);\n\t\trelease_sock(sk);\n\t\t/* check if ipv6_getsockopt_sticky() returns err code */\n\t\tif (len < 0)\n\t\t\treturn len;\n\t\treturn put_user(len, optlen);\n\t}\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tval = np->rxopt.bits.hopopts;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tval = np->rxopt.bits.ohopopts;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tval = np->rxopt.bits.dstopts;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tval = np->rxopt.bits.odstopts;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tval = np->tclass;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tval = np->rxopt.bits.rxtclass;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tval = np->rxopt.bits.rxflow;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tval = np->rxopt.bits.rxpmtu;\n\t\tbreak;\n\n\tcase IPV6_PATHMTU:\n\t{\n\t\tstruct dst_entry *dst;\n\t\tstruct ip6_mtuinfo mtuinfo;\n\n\t\tif (len < sizeof(mtuinfo))\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(mtuinfo);\n\t\tmemset(&mtuinfo, 0, sizeof(mtuinfo));\n\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tmtuinfo.ip6m_mtu = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!mtuinfo.ip6m_mtu)\n\t\t\treturn -ENOTCONN;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &mtuinfo, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_TRANSPARENT:\n\t\tval = inet_sk(sk)->transparent;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tval = np->rxopt.bits.rxorigdstaddr;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_HOPS:\n\tcase IPV6_MULTICAST_HOPS:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tif (optname == IPV6_UNICAST_HOPS)\n\t\t\tval = np->hop_limit;\n\t\telse\n\t\t\tval = np->mcast_hops;\n\n\t\tif (val < 0) {\n\t\t\trcu_read_lock();\n\t\t\tdst = __sk_dst_get(sk);\n\t\t\tif (dst)\n\t\t\t\tval = ip6_dst_hoplimit(dst);\n\t\t\trcu_read_unlock();\n\t\t}\n\n\t\tif (val < 0)\n\t\t\tval = sock_net(sk)->ipv6.devconf_all->hop_limit;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tval = np->mc_loop;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_IF:\n\t\tval = np->mcast_oif;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t\tval = (__force int)htonl((__u32) np->ucast_oif);\n\t\tbreak;\n\n\tcase IPV6_MTU_DISCOVER:\n\t\tval = np->pmtudisc;\n\t\tbreak;\n\n\tcase IPV6_RECVERR:\n\t\tval = np->recverr;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO_SEND:\n\t\tval = np->sndflow;\n\t\tbreak;\n\n\tcase IPV6_FLOWLABEL_MGR:\n\t{\n\t\tstruct in6_flowlabel_req freq;\n\t\tint flags;\n\n\t\tif (len < sizeof(freq))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_user(&freq, optval, sizeof(freq)))\n\t\t\treturn -EFAULT;\n\n\t\tif (freq.flr_action != IPV6_FL_A_GET)\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(freq);\n\t\tflags = freq.flr_flags;\n\n\t\tmemset(&freq, 0, sizeof(freq));\n\n\t\tval = ipv6_flowlabel_opt_get(sk, &freq, flags);\n\t\tif (val < 0)\n\t\t\treturn val;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &freq, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t\tval = 0;\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_TMP)\n\t\t\tval |= IPV6_PREFER_SRC_TMP;\n\t\telse if (np->srcprefs & IPV6_PREFER_SRC_PUBLIC)\n\t\t\tval |= IPV6_PREFER_SRC_PUBLIC;\n\t\telse {\n\t\t\t/* XXX: should we return system default? */\n\t\t\tval |= IPV6_PREFER_SRC_PUBTMP_DEFAULT;\n\t\t}\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_COA)\n\t\t\tval |= IPV6_PREFER_SRC_COA;\n\t\telse\n\t\t\tval |= IPV6_PREFER_SRC_HOME;\n\t\tbreak;\n\n\tcase IPV6_MINHOPCOUNT:\n\t\tval = np->min_hopcount;\n\t\tbreak;\n\n\tcase IPV6_DONTFRAG:\n\t\tval = np->dontfrag;\n\t\tbreak;\n\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tval = np->autoflowlabel;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\tlen = min_t(unsigned int, sizeof(int), len);\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nint sock_setsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tint val;\n\tint valbool;\n\tstruct linger ling;\n\tint ret = 0;\n\n\t/*\n\t *\tOptions without arguments\n\t */\n\n\tif (optname == SO_BINDTODEVICE)\n\t\treturn sock_bindtodevice(sk, optval, optlen);\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\n\tvalbool = val ? 1 : 0;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase SO_DEBUG:\n\t\tif (val && !capable(CAP_NET_ADMIN))\n\t\t\tret = -EACCES;\n\t\telse\n\t\t\tsock_valbool_flag(sk, SOCK_DBG, valbool);\n\t\tbreak;\n\tcase SO_REUSEADDR:\n\t\tsk->sk_reuse = (valbool ? SK_CAN_REUSE : SK_NO_REUSE);\n\t\tbreak;\n\tcase SO_TYPE:\n\tcase SO_PROTOCOL:\n\tcase SO_DOMAIN:\n\tcase SO_ERROR:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\tcase SO_DONTROUTE:\n\t\tsock_valbool_flag(sk, SOCK_LOCALROUTE, valbool);\n\t\tbreak;\n\tcase SO_BROADCAST:\n\t\tsock_valbool_flag(sk, SOCK_BROADCAST, valbool);\n\t\tbreak;\n\tcase SO_SNDBUF:\n\t\t/* Don't error on this BSD doesn't and if you think\n\t\t * about it this is right. Otherwise apps have to\n\t\t * play 'guess the biggest size' games. RCVBUF/SNDBUF\n\t\t * are treated in BSD as hints\n\t\t */\n\t\tval = min_t(u32, val, sysctl_wmem_max);\nset_sndbuf:\n\t\tsk->sk_userlocks |= SOCK_SNDBUF_LOCK;\n\t\tsk->sk_sndbuf = max_t(u32, val * 2, SOCK_MIN_SNDBUF);\n\t\t/* Wake up sending tasks if we upped the value. */\n\t\tsk->sk_write_space(sk);\n\t\tbreak;\n\n\tcase SO_SNDBUFFORCE:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tgoto set_sndbuf;\n\n\tcase SO_RCVBUF:\n\t\t/* Don't error on this BSD doesn't and if you think\n\t\t * about it this is right. Otherwise apps have to\n\t\t * play 'guess the biggest size' games. RCVBUF/SNDBUF\n\t\t * are treated in BSD as hints\n\t\t */\n\t\tval = min_t(u32, val, sysctl_rmem_max);\nset_rcvbuf:\n\t\tsk->sk_userlocks |= SOCK_RCVBUF_LOCK;\n\t\t/*\n\t\t * We double it on the way in to account for\n\t\t * \"struct sk_buff\" etc. overhead.   Applications\n\t\t * assume that the SO_RCVBUF setting they make will\n\t\t * allow that much actual data to be received on that\n\t\t * socket.\n\t\t *\n\t\t * Applications are unaware that \"struct sk_buff\" and\n\t\t * other overheads allocate from the receive buffer\n\t\t * during socket buffer allocation.\n\t\t *\n\t\t * And after considering the possible alternatives,\n\t\t * returning the value we actually used in getsockopt\n\t\t * is the most desirable behavior.\n\t\t */\n\t\tsk->sk_rcvbuf = max_t(u32, val * 2, SOCK_MIN_RCVBUF);\n\t\tbreak;\n\n\tcase SO_RCVBUFFORCE:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tgoto set_rcvbuf;\n\n\tcase SO_KEEPALIVE:\n#ifdef CONFIG_INET\n\t\tif (sk->sk_protocol == IPPROTO_TCP)\n\t\t\ttcp_set_keepalive(sk, valbool);\n#endif\n\t\tsock_valbool_flag(sk, SOCK_KEEPOPEN, valbool);\n\t\tbreak;\n\n\tcase SO_OOBINLINE:\n\t\tsock_valbool_flag(sk, SOCK_URGINLINE, valbool);\n\t\tbreak;\n\n\tcase SO_NO_CHECK:\n\t\tsk->sk_no_check = valbool;\n\t\tbreak;\n\n\tcase SO_PRIORITY:\n\t\tif ((val >= 0 && val <= 6) || capable(CAP_NET_ADMIN))\n\t\t\tsk->sk_priority = val;\n\t\telse\n\t\t\tret = -EPERM;\n\t\tbreak;\n\n\tcase SO_LINGER:\n\t\tif (optlen < sizeof(ling)) {\n\t\t\tret = -EINVAL;\t/* 1003.1g */\n\t\t\tbreak;\n\t\t}\n\t\tif (copy_from_user(&ling, optval, sizeof(ling))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!ling.l_onoff)\n\t\t\tsock_reset_flag(sk, SOCK_LINGER);\n\t\telse {\n#if (BITS_PER_LONG == 32)\n\t\t\tif ((unsigned int)ling.l_linger >= MAX_SCHEDULE_TIMEOUT/HZ)\n\t\t\t\tsk->sk_lingertime = MAX_SCHEDULE_TIMEOUT;\n\t\t\telse\n#endif\n\t\t\t\tsk->sk_lingertime = (unsigned int)ling.l_linger * HZ;\n\t\t\tsock_set_flag(sk, SOCK_LINGER);\n\t\t}\n\t\tbreak;\n\n\tcase SO_BSDCOMPAT:\n\t\tsock_warn_obsolete_bsdism(\"setsockopt\");\n\t\tbreak;\n\n\tcase SO_PASSCRED:\n\t\tif (valbool)\n\t\t\tset_bit(SOCK_PASSCRED, &sock->flags);\n\t\telse\n\t\t\tclear_bit(SOCK_PASSCRED, &sock->flags);\n\t\tbreak;\n\n\tcase SO_TIMESTAMP:\n\tcase SO_TIMESTAMPNS:\n\t\tif (valbool)  {\n\t\t\tif (optname == SO_TIMESTAMP)\n\t\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t\telse\n\t\t\t\tsock_set_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t\tsock_set_flag(sk, SOCK_RCVTSTAMP);\n\t\t\tsock_enable_timestamp(sk, SOCK_TIMESTAMP);\n\t\t} else {\n\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMP);\n\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t}\n\t\tbreak;\n\n\tcase SO_TIMESTAMPING:\n\t\tif (val & ~SOF_TIMESTAMPING_MASK) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_TX_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_TX_HARDWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_TX_SOFTWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_TX_SOFTWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_RX_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_RX_HARDWARE);\n\t\tif (val & SOF_TIMESTAMPING_RX_SOFTWARE)\n\t\t\tsock_enable_timestamp(sk,\n\t\t\t\t\t      SOCK_TIMESTAMPING_RX_SOFTWARE);\n\t\telse\n\t\t\tsock_disable_timestamp(sk,\n\t\t\t\t\t       (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE));\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_SOFTWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_SOFTWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_SYS_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_SYS_HARDWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_RAW_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_RAW_HARDWARE);\n\t\tbreak;\n\n\tcase SO_RCVLOWAT:\n\t\tif (val < 0)\n\t\t\tval = INT_MAX;\n\t\tsk->sk_rcvlowat = val ? : 1;\n\t\tbreak;\n\n\tcase SO_RCVTIMEO:\n\t\tret = sock_set_timeout(&sk->sk_rcvtimeo, optval, optlen);\n\t\tbreak;\n\n\tcase SO_SNDTIMEO:\n\t\tret = sock_set_timeout(&sk->sk_sndtimeo, optval, optlen);\n\t\tbreak;\n\n\tcase SO_ATTACH_FILTER:\n\t\tret = -EINVAL;\n\t\tif (optlen == sizeof(struct sock_fprog)) {\n\t\t\tstruct sock_fprog fprog;\n\n\t\t\tret = -EFAULT;\n\t\t\tif (copy_from_user(&fprog, optval, sizeof(fprog)))\n\t\t\t\tbreak;\n\n\t\t\tret = sk_attach_filter(&fprog, sk);\n\t\t}\n\t\tbreak;\n\n\tcase SO_DETACH_FILTER:\n\t\tret = sk_detach_filter(sk);\n\t\tbreak;\n\n\tcase SO_PASSSEC:\n\t\tif (valbool)\n\t\t\tset_bit(SOCK_PASSSEC, &sock->flags);\n\t\telse\n\t\t\tclear_bit(SOCK_PASSSEC, &sock->flags);\n\t\tbreak;\n\tcase SO_MARK:\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tsk->sk_mark = val;\n\t\tbreak;\n\n\t\t/* We implement the SO_SNDLOWAT etc to\n\t\t   not be settable (1003.1g 5.3) */\n\tcase SO_RXQ_OVFL:\n\t\tsock_valbool_flag(sk, SOCK_RXQ_OVFL, valbool);\n\t\tbreak;\n\n\tcase SO_WIFI_STATUS:\n\t\tsock_valbool_flag(sk, SOCK_WIFI_STATUS, valbool);\n\t\tbreak;\n\n\tcase SO_PEEK_OFF:\n\t\tif (sock->ops->set_peek_off)\n\t\t\tsock->ops->set_peek_off(sk, val);\n\t\telse\n\t\t\tret = -EOPNOTSUPP;\n\t\tbreak;\n\n\tcase SO_NOFCS:\n\t\tsock_valbool_flag(sk, SOCK_NOFCS, valbool);\n\t\tbreak;\n\n\tdefault:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\trelease_sock(sk);\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int do_ipv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, int __user *optlen, unsigned int flags)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tint len;\n\tint val;\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_getsockopt(sk, optname, optval, optlen);\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tswitch (optname) {\n\tcase IPV6_ADDRFORM:\n\t\tif (sk->sk_protocol != IPPROTO_UDP &&\n\t\t    sk->sk_protocol != IPPROTO_UDPLITE &&\n\t\t    sk->sk_protocol != IPPROTO_TCP)\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -ENOTCONN;\n\t\tval = sk->sk_family;\n\t\tbreak;\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter gsf;\n\t\tint err;\n\n\t\tif (len < GROUP_FILTER_SIZE(0))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&gsf, optval, GROUP_FILTER_SIZE(0)))\n\t\t\treturn -EFAULT;\n\t\tif (gsf.gf_group.ss_family != AF_INET6)\n\t\t\treturn -EADDRNOTAVAIL;\n\t\tlock_sock(sk);\n\t\terr = ip6_mc_msfget(sk, &gsf,\n\t\t\t(struct group_filter __user *)optval, optlen);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct msghdr msg;\n\t\tstruct sk_buff *skb;\n\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\treturn -ENOPROTOOPT;\n\n\t\tmsg.msg_control = optval;\n\t\tmsg.msg_controllen = len;\n\t\tmsg.msg_flags = flags;\n\n\t\tlock_sock(sk);\n\t\tskb = np->pktoptions;\n\t\tif (skb)\n\t\t\tip6_datagram_recv_ctl(sk, &msg, skb);\n\t\trelease_sock(sk);\n\t\tif (!skb) {\n\t\t\tif (np->rxopt.bits.rxinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr : np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxhlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxtclass) {\n\t\t\t\tint tclass = (int)ip6_tclass(np->rcv_flowinfo);\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_TCLASS, sizeof(tclass), &tclass);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxoinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr :\n\t\t\t\t\t\t\t\t     np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxohlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxflow) {\n\t\t\t\t__be32 flowinfo = np->rcv_flowinfo;\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_FLOWINFO, sizeof(flowinfo), &flowinfo);\n\t\t\t}\n\t\t}\n\t\tlen -= msg.msg_controllen;\n\t\treturn put_user(len, optlen);\n\t}\n\tcase IPV6_MTU:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tval = 0;\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tval = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!val)\n\t\t\treturn -ENOTCONN;\n\t\tbreak;\n\t}\n\n\tcase IPV6_V6ONLY:\n\t\tval = sk->sk_ipv6only;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tval = np->rxopt.bits.rxinfo;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tval = np->rxopt.bits.rxoinfo;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tval = np->rxopt.bits.rxhlim;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tval = np->rxopt.bits.rxohlim;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tval = np->rxopt.bits.srcrt;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tval = np->rxopt.bits.osrcrt;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\n\t\tlock_sock(sk);\n\t\tlen = ipv6_getsockopt_sticky(sk, np->opt,\n\t\t\t\t\t     optname, optval, len);\n\t\trelease_sock(sk);\n\t\t/* check if ipv6_getsockopt_sticky() returns err code */\n\t\tif (len < 0)\n\t\t\treturn len;\n\t\treturn put_user(len, optlen);\n\t}\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tval = np->rxopt.bits.hopopts;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tval = np->rxopt.bits.ohopopts;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tval = np->rxopt.bits.dstopts;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tval = np->rxopt.bits.odstopts;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tval = np->tclass;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tval = np->rxopt.bits.rxtclass;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tval = np->rxopt.bits.rxflow;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tval = np->rxopt.bits.rxpmtu;\n\t\tbreak;\n\n\tcase IPV6_PATHMTU:\n\t{\n\t\tstruct dst_entry *dst;\n\t\tstruct ip6_mtuinfo mtuinfo;\n\n\t\tif (len < sizeof(mtuinfo))\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(mtuinfo);\n\t\tmemset(&mtuinfo, 0, sizeof(mtuinfo));\n\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tmtuinfo.ip6m_mtu = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!mtuinfo.ip6m_mtu)\n\t\t\treturn -ENOTCONN;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &mtuinfo, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_TRANSPARENT:\n\t\tval = inet_sk(sk)->transparent;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tval = np->rxopt.bits.rxorigdstaddr;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_HOPS:\n\tcase IPV6_MULTICAST_HOPS:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tif (optname == IPV6_UNICAST_HOPS)\n\t\t\tval = np->hop_limit;\n\t\telse\n\t\t\tval = np->mcast_hops;\n\n\t\tif (val < 0) {\n\t\t\trcu_read_lock();\n\t\t\tdst = __sk_dst_get(sk);\n\t\t\tif (dst)\n\t\t\t\tval = ip6_dst_hoplimit(dst);\n\t\t\trcu_read_unlock();\n\t\t}\n\n\t\tif (val < 0)\n\t\t\tval = sock_net(sk)->ipv6.devconf_all->hop_limit;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tval = np->mc_loop;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_IF:\n\t\tval = np->mcast_oif;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t\tval = (__force int)htonl((__u32) np->ucast_oif);\n\t\tbreak;\n\n\tcase IPV6_MTU_DISCOVER:\n\t\tval = np->pmtudisc;\n\t\tbreak;\n\n\tcase IPV6_RECVERR:\n\t\tval = np->recverr;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO_SEND:\n\t\tval = np->sndflow;\n\t\tbreak;\n\n\tcase IPV6_FLOWLABEL_MGR:\n\t{\n\t\tstruct in6_flowlabel_req freq;\n\t\tint flags;\n\n\t\tif (len < sizeof(freq))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_user(&freq, optval, sizeof(freq)))\n\t\t\treturn -EFAULT;\n\n\t\tif (freq.flr_action != IPV6_FL_A_GET)\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(freq);\n\t\tflags = freq.flr_flags;\n\n\t\tmemset(&freq, 0, sizeof(freq));\n\n\t\tval = ipv6_flowlabel_opt_get(sk, &freq, flags);\n\t\tif (val < 0)\n\t\t\treturn val;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &freq, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t\tval = 0;\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_TMP)\n\t\t\tval |= IPV6_PREFER_SRC_TMP;\n\t\telse if (np->srcprefs & IPV6_PREFER_SRC_PUBLIC)\n\t\t\tval |= IPV6_PREFER_SRC_PUBLIC;\n\t\telse {\n\t\t\t/* XXX: should we return system default? */\n\t\t\tval |= IPV6_PREFER_SRC_PUBTMP_DEFAULT;\n\t\t}\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_COA)\n\t\t\tval |= IPV6_PREFER_SRC_COA;\n\t\telse\n\t\t\tval |= IPV6_PREFER_SRC_HOME;\n\t\tbreak;\n\n\tcase IPV6_MINHOPCOUNT:\n\t\tval = np->min_hopcount;\n\t\tbreak;\n\n\tcase IPV6_DONTFRAG:\n\t\tval = np->dontfrag;\n\t\tbreak;\n\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tval = np->autoflowlabel;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\tlen = min_t(unsigned int, sizeof(int), len);\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int do_ipv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, int __user *optlen, unsigned int flags)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tint len;\n\tint val;\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_getsockopt(sk, optname, optval, optlen);\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tswitch (optname) {\n\tcase IPV6_ADDRFORM:\n\t\tif (sk->sk_protocol != IPPROTO_UDP &&\n\t\t    sk->sk_protocol != IPPROTO_UDPLITE &&\n\t\t    sk->sk_protocol != IPPROTO_TCP)\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -ENOTCONN;\n\t\tval = sk->sk_family;\n\t\tbreak;\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter gsf;\n\t\tint err;\n\n\t\tif (len < GROUP_FILTER_SIZE(0))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&gsf, optval, GROUP_FILTER_SIZE(0)))\n\t\t\treturn -EFAULT;\n\t\tif (gsf.gf_group.ss_family != AF_INET6)\n\t\t\treturn -EADDRNOTAVAIL;\n\t\tlock_sock(sk);\n\t\terr = ip6_mc_msfget(sk, &gsf,\n\t\t\t(struct group_filter __user *)optval, optlen);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct msghdr msg;\n\t\tstruct sk_buff *skb;\n\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\treturn -ENOPROTOOPT;\n\n\t\tmsg.msg_control = optval;\n\t\tmsg.msg_controllen = len;\n\t\tmsg.msg_flags = flags;\n\n\t\tlock_sock(sk);\n\t\tskb = np->pktoptions;\n\t\tif (skb)\n\t\t\tip6_datagram_recv_ctl(sk, &msg, skb);\n\t\trelease_sock(sk);\n\t\tif (!skb) {\n\t\t\tif (np->rxopt.bits.rxinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr : np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxhlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxtclass) {\n\t\t\t\tint tclass = (int)ip6_tclass(np->rcv_flowinfo);\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_TCLASS, sizeof(tclass), &tclass);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxoinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr :\n\t\t\t\t\t\t\t\t     np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxohlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxflow) {\n\t\t\t\t__be32 flowinfo = np->rcv_flowinfo;\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_FLOWINFO, sizeof(flowinfo), &flowinfo);\n\t\t\t}\n\t\t}\n\t\tlen -= msg.msg_controllen;\n\t\treturn put_user(len, optlen);\n\t}\n\tcase IPV6_MTU:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tval = 0;\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tval = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!val)\n\t\t\treturn -ENOTCONN;\n\t\tbreak;\n\t}\n\n\tcase IPV6_V6ONLY:\n\t\tval = sk->sk_ipv6only;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tval = np->rxopt.bits.rxinfo;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tval = np->rxopt.bits.rxoinfo;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tval = np->rxopt.bits.rxhlim;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tval = np->rxopt.bits.rxohlim;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tval = np->rxopt.bits.srcrt;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tval = np->rxopt.bits.osrcrt;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\tlock_sock(sk);\n\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\t\tlen = ipv6_getsockopt_sticky(sk, opt, optname, optval, len);\n\t\trelease_sock(sk);\n\t\t/* check if ipv6_getsockopt_sticky() returns err code */\n\t\tif (len < 0)\n\t\t\treturn len;\n\t\treturn put_user(len, optlen);\n\t}\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tval = np->rxopt.bits.hopopts;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tval = np->rxopt.bits.ohopopts;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tval = np->rxopt.bits.dstopts;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tval = np->rxopt.bits.odstopts;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tval = np->tclass;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tval = np->rxopt.bits.rxtclass;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tval = np->rxopt.bits.rxflow;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tval = np->rxopt.bits.rxpmtu;\n\t\tbreak;\n\n\tcase IPV6_PATHMTU:\n\t{\n\t\tstruct dst_entry *dst;\n\t\tstruct ip6_mtuinfo mtuinfo;\n\n\t\tif (len < sizeof(mtuinfo))\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(mtuinfo);\n\t\tmemset(&mtuinfo, 0, sizeof(mtuinfo));\n\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tmtuinfo.ip6m_mtu = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!mtuinfo.ip6m_mtu)\n\t\t\treturn -ENOTCONN;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &mtuinfo, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_TRANSPARENT:\n\t\tval = inet_sk(sk)->transparent;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tval = np->rxopt.bits.rxorigdstaddr;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_HOPS:\n\tcase IPV6_MULTICAST_HOPS:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tif (optname == IPV6_UNICAST_HOPS)\n\t\t\tval = np->hop_limit;\n\t\telse\n\t\t\tval = np->mcast_hops;\n\n\t\tif (val < 0) {\n\t\t\trcu_read_lock();\n\t\t\tdst = __sk_dst_get(sk);\n\t\t\tif (dst)\n\t\t\t\tval = ip6_dst_hoplimit(dst);\n\t\t\trcu_read_unlock();\n\t\t}\n\n\t\tif (val < 0)\n\t\t\tval = sock_net(sk)->ipv6.devconf_all->hop_limit;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tval = np->mc_loop;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_IF:\n\t\tval = np->mcast_oif;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t\tval = (__force int)htonl((__u32) np->ucast_oif);\n\t\tbreak;\n\n\tcase IPV6_MTU_DISCOVER:\n\t\tval = np->pmtudisc;\n\t\tbreak;\n\n\tcase IPV6_RECVERR:\n\t\tval = np->recverr;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO_SEND:\n\t\tval = np->sndflow;\n\t\tbreak;\n\n\tcase IPV6_FLOWLABEL_MGR:\n\t{\n\t\tstruct in6_flowlabel_req freq;\n\t\tint flags;\n\n\t\tif (len < sizeof(freq))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_user(&freq, optval, sizeof(freq)))\n\t\t\treturn -EFAULT;\n\n\t\tif (freq.flr_action != IPV6_FL_A_GET)\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(freq);\n\t\tflags = freq.flr_flags;\n\n\t\tmemset(&freq, 0, sizeof(freq));\n\n\t\tval = ipv6_flowlabel_opt_get(sk, &freq, flags);\n\t\tif (val < 0)\n\t\t\treturn val;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &freq, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t\tval = 0;\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_TMP)\n\t\t\tval |= IPV6_PREFER_SRC_TMP;\n\t\telse if (np->srcprefs & IPV6_PREFER_SRC_PUBLIC)\n\t\t\tval |= IPV6_PREFER_SRC_PUBLIC;\n\t\telse {\n\t\t\t/* XXX: should we return system default? */\n\t\t\tval |= IPV6_PREFER_SRC_PUBTMP_DEFAULT;\n\t\t}\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_COA)\n\t\t\tval |= IPV6_PREFER_SRC_COA;\n\t\telse\n\t\t\tval |= IPV6_PREFER_SRC_HOME;\n\t\tbreak;\n\n\tcase IPV6_MINHOPCOUNT:\n\t\tval = np->min_hopcount;\n\t\tbreak;\n\n\tcase IPV6_DONTFRAG:\n\t\tval = np->dontfrag;\n\t\tbreak;\n\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tval = np->autoflowlabel;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\tlen = min_t(unsigned int, sizeof(int), len);\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "asmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}",
                        "code_after_change": "asmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value[0] = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}",
                        "cve_id": "CVE-2014-9870"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nint sock_setsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tint val;\n\tint valbool;\n\tstruct linger ling;\n\tint ret = 0;\n\n\t/*\n\t *\tOptions without arguments\n\t */\n\n\tif (optname == SO_BINDTODEVICE)\n\t\treturn sock_bindtodevice(sk, optval, optlen);\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\n\tvalbool = val ? 1 : 0;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase SO_DEBUG:\n\t\tif (val && !capable(CAP_NET_ADMIN))\n\t\t\tret = -EACCES;\n\t\telse\n\t\t\tsock_valbool_flag(sk, SOCK_DBG, valbool);\n\t\tbreak;\n\tcase SO_REUSEADDR:\n\t\tsk->sk_reuse = (valbool ? SK_CAN_REUSE : SK_NO_REUSE);\n\t\tbreak;\n\tcase SO_TYPE:\n\tcase SO_PROTOCOL:\n\tcase SO_DOMAIN:\n\tcase SO_ERROR:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\tcase SO_DONTROUTE:\n\t\tsock_valbool_flag(sk, SOCK_LOCALROUTE, valbool);\n\t\tbreak;\n\tcase SO_BROADCAST:\n\t\tsock_valbool_flag(sk, SOCK_BROADCAST, valbool);\n\t\tbreak;\n\tcase SO_SNDBUF:\n\t\t/* Don't error on this BSD doesn't and if you think\n\t\t * about it this is right. Otherwise apps have to\n\t\t * play 'guess the biggest size' games. RCVBUF/SNDBUF\n\t\t * are treated in BSD as hints\n\t\t */\n\t\tval = min_t(u32, val, sysctl_wmem_max);\nset_sndbuf:\n\t\tsk->sk_userlocks |= SOCK_SNDBUF_LOCK;\n\t\tsk->sk_sndbuf = max_t(u32, val * 2, SOCK_MIN_SNDBUF);\n\t\t/* Wake up sending tasks if we upped the value. */\n\t\tsk->sk_write_space(sk);\n\t\tbreak;\n\n\tcase SO_SNDBUFFORCE:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tgoto set_sndbuf;\n\n\tcase SO_RCVBUF:\n\t\t/* Don't error on this BSD doesn't and if you think\n\t\t * about it this is right. Otherwise apps have to\n\t\t * play 'guess the biggest size' games. RCVBUF/SNDBUF\n\t\t * are treated in BSD as hints\n\t\t */\n\t\tval = min_t(u32, val, sysctl_rmem_max);\nset_rcvbuf:\n\t\tsk->sk_userlocks |= SOCK_RCVBUF_LOCK;\n\t\t/*\n\t\t * We double it on the way in to account for\n\t\t * \"struct sk_buff\" etc. overhead.   Applications\n\t\t * assume that the SO_RCVBUF setting they make will\n\t\t * allow that much actual data to be received on that\n\t\t * socket.\n\t\t *\n\t\t * Applications are unaware that \"struct sk_buff\" and\n\t\t * other overheads allocate from the receive buffer\n\t\t * during socket buffer allocation.\n\t\t *\n\t\t * And after considering the possible alternatives,\n\t\t * returning the value we actually used in getsockopt\n\t\t * is the most desirable behavior.\n\t\t */\n\t\tsk->sk_rcvbuf = max_t(u32, val * 2, SOCK_MIN_RCVBUF);\n\t\tbreak;\n\n\tcase SO_RCVBUFFORCE:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tgoto set_rcvbuf;\n\n\tcase SO_KEEPALIVE:\n#ifdef CONFIG_INET\n\t\tif (sk->sk_protocol == IPPROTO_TCP)\n\t\t\ttcp_set_keepalive(sk, valbool);\n#endif\n\t\tsock_valbool_flag(sk, SOCK_KEEPOPEN, valbool);\n\t\tbreak;\n\n\tcase SO_OOBINLINE:\n\t\tsock_valbool_flag(sk, SOCK_URGINLINE, valbool);\n\t\tbreak;\n\n\tcase SO_NO_CHECK:\n\t\tsk->sk_no_check = valbool;\n\t\tbreak;\n\n\tcase SO_PRIORITY:\n\t\tif ((val >= 0 && val <= 6) || capable(CAP_NET_ADMIN))\n\t\t\tsk->sk_priority = val;\n\t\telse\n\t\t\tret = -EPERM;\n\t\tbreak;\n\n\tcase SO_LINGER:\n\t\tif (optlen < sizeof(ling)) {\n\t\t\tret = -EINVAL;\t/* 1003.1g */\n\t\t\tbreak;\n\t\t}\n\t\tif (copy_from_user(&ling, optval, sizeof(ling))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!ling.l_onoff)\n\t\t\tsock_reset_flag(sk, SOCK_LINGER);\n\t\telse {\n#if (BITS_PER_LONG == 32)\n\t\t\tif ((unsigned int)ling.l_linger >= MAX_SCHEDULE_TIMEOUT/HZ)\n\t\t\t\tsk->sk_lingertime = MAX_SCHEDULE_TIMEOUT;\n\t\t\telse\n#endif\n\t\t\t\tsk->sk_lingertime = (unsigned int)ling.l_linger * HZ;\n\t\t\tsock_set_flag(sk, SOCK_LINGER);\n\t\t}\n\t\tbreak;\n\n\tcase SO_BSDCOMPAT:\n\t\tsock_warn_obsolete_bsdism(\"setsockopt\");\n\t\tbreak;\n\n\tcase SO_PASSCRED:\n\t\tif (valbool)\n\t\t\tset_bit(SOCK_PASSCRED, &sock->flags);\n\t\telse\n\t\t\tclear_bit(SOCK_PASSCRED, &sock->flags);\n\t\tbreak;\n\n\tcase SO_TIMESTAMP:\n\tcase SO_TIMESTAMPNS:\n\t\tif (valbool)  {\n\t\t\tif (optname == SO_TIMESTAMP)\n\t\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t\telse\n\t\t\t\tsock_set_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t\tsock_set_flag(sk, SOCK_RCVTSTAMP);\n\t\t\tsock_enable_timestamp(sk, SOCK_TIMESTAMP);\n\t\t} else {\n\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMP);\n\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t}\n\t\tbreak;\n\n\tcase SO_TIMESTAMPING:\n\t\tif (val & ~SOF_TIMESTAMPING_MASK) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_TX_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_TX_HARDWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_TX_SOFTWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_TX_SOFTWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_RX_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_RX_HARDWARE);\n\t\tif (val & SOF_TIMESTAMPING_RX_SOFTWARE)\n\t\t\tsock_enable_timestamp(sk,\n\t\t\t\t\t      SOCK_TIMESTAMPING_RX_SOFTWARE);\n\t\telse\n\t\t\tsock_disable_timestamp(sk,\n\t\t\t\t\t       (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE));\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_SOFTWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_SOFTWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_SYS_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_SYS_HARDWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_RAW_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_RAW_HARDWARE);\n\t\tbreak;\n\n\tcase SO_RCVLOWAT:\n\t\tif (val < 0)\n\t\t\tval = INT_MAX;\n\t\tsk->sk_rcvlowat = val ? : 1;\n\t\tbreak;\n\n\tcase SO_RCVTIMEO:\n\t\tret = sock_set_timeout(&sk->sk_rcvtimeo, optval, optlen);\n\t\tbreak;\n\n\tcase SO_SNDTIMEO:\n\t\tret = sock_set_timeout(&sk->sk_sndtimeo, optval, optlen);\n\t\tbreak;\n\n\tcase SO_ATTACH_FILTER:\n\t\tret = -EINVAL;\n\t\tif (optlen == sizeof(struct sock_fprog)) {\n\t\t\tstruct sock_fprog fprog;\n\n\t\t\tret = -EFAULT;\n\t\t\tif (copy_from_user(&fprog, optval, sizeof(fprog)))\n\t\t\t\tbreak;\n\n\t\t\tret = sk_attach_filter(&fprog, sk);\n\t\t}\n\t\tbreak;\n\n\tcase SO_DETACH_FILTER:\n\t\tret = sk_detach_filter(sk);\n\t\tbreak;\n\n\tcase SO_PASSSEC:\n\t\tif (valbool)\n\t\t\tset_bit(SOCK_PASSSEC, &sock->flags);\n\t\telse\n\t\t\tclear_bit(SOCK_PASSSEC, &sock->flags);\n\t\tbreak;\n\tcase SO_MARK:\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tsk->sk_mark = val;\n\t\tbreak;\n\n\t\t/* We implement the SO_SNDLOWAT etc to\n\t\t   not be settable (1003.1g 5.3) */\n\tcase SO_RXQ_OVFL:\n\t\tsock_valbool_flag(sk, SOCK_RXQ_OVFL, valbool);\n\t\tbreak;\n\n\tcase SO_WIFI_STATUS:\n\t\tsock_valbool_flag(sk, SOCK_WIFI_STATUS, valbool);\n\t\tbreak;\n\n\tcase SO_PEEK_OFF:\n\t\tif (sock->ops->set_peek_off)\n\t\t\tsock->ops->set_peek_off(sk, val);\n\t\telse\n\t\t\tret = -EOPNOTSUPP;\n\t\tbreak;\n\n\tcase SO_NOFCS:\n\t\tsock_valbool_flag(sk, SOCK_NOFCS, valbool);\n\t\tbreak;\n\n\tdefault:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\trelease_sock(sk);\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nasmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nasmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value[0] = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 437,
            "cve_id": "CVE-2014-0181",
            "code_snippet": "static int cgw_create_job(struct sk_buff *skb,  struct nlmsghdr *nlh)\n{\n\tstruct rtcanmsg *r;\n\tstruct cgw_job *gwj;\n\tu8 limhops = 0;\n\tint err = 0;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (nlmsg_len(nlh) < sizeof(*r))\n\t\treturn -EINVAL;\n\n\tr = nlmsg_data(nlh);\n\tif (r->can_family != AF_CAN)\n\t\treturn -EPFNOSUPPORT;\n\n\t/* so far we only support CAN -> CAN routings */\n\tif (r->gwtype != CGW_TYPE_CAN_CAN)\n\t\treturn -EINVAL;\n\n\tgwj = kmem_cache_alloc(cgw_cache, GFP_KERNEL);\n\tif (!gwj)\n\t\treturn -ENOMEM;\n\n\tgwj->handled_frames = 0;\n\tgwj->dropped_frames = 0;\n\tgwj->deleted_frames = 0;\n\tgwj->flags = r->flags;\n\tgwj->gwtype = r->gwtype;\n\n\terr = cgw_parse_attr(nlh, &gwj->mod, CGW_TYPE_CAN_CAN, &gwj->ccgw,\n\t\t\t     &limhops);\n\tif (err < 0)\n\t\tgoto out;\n\n\terr = -ENODEV;\n\n\t/* ifindex == 0 is not allowed for job creation */\n\tif (!gwj->ccgw.src_idx || !gwj->ccgw.dst_idx)\n\t\tgoto out;\n\n\tgwj->src.dev = __dev_get_by_index(&init_net, gwj->ccgw.src_idx);\n\n\tif (!gwj->src.dev)\n\t\tgoto out;\n\n\tif (gwj->src.dev->type != ARPHRD_CAN)\n\t\tgoto out;\n\n\tgwj->dst.dev = __dev_get_by_index(&init_net, gwj->ccgw.dst_idx);\n\n\tif (!gwj->dst.dev)\n\t\tgoto out;\n\n\tif (gwj->dst.dev->type != ARPHRD_CAN)\n\t\tgoto out;\n\n\tgwj->limit_hops = limhops;\n\n\tASSERT_RTNL();\n\n\terr = cgw_register_filter(gwj);\n\tif (!err)\n\t\thlist_add_head_rcu(&gwj->list, &cgw_list);\nout:\n\tif (err)\n\t\tkmem_cache_free(cgw_cache, gwj);\n\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
                        "code_after_change": "static int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int cgw_create_job(struct sk_buff *skb,  struct nlmsghdr *nlh)\n{\n\tstruct rtcanmsg *r;\n\tstruct cgw_job *gwj;\n\tu8 limhops = 0;\n\tint err = 0;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (nlmsg_len(nlh) < sizeof(*r))\n\t\treturn -EINVAL;\n\n\tr = nlmsg_data(nlh);\n\tif (r->can_family != AF_CAN)\n\t\treturn -EPFNOSUPPORT;\n\n\t/* so far we only support CAN -> CAN routings */\n\tif (r->gwtype != CGW_TYPE_CAN_CAN)\n\t\treturn -EINVAL;\n\n\tgwj = kmem_cache_alloc(cgw_cache, GFP_KERNEL);\n\tif (!gwj)\n\t\treturn -ENOMEM;\n\n\tgwj->handled_frames = 0;\n\tgwj->dropped_frames = 0;\n\tgwj->deleted_frames = 0;\n\tgwj->flags = r->flags;\n\tgwj->gwtype = r->gwtype;\n\n\terr = cgw_parse_attr(nlh, &gwj->mod, CGW_TYPE_CAN_CAN, &gwj->ccgw,\n\t\t\t     &limhops);\n\tif (err < 0)\n\t\tgoto out;\n\n\terr = -ENODEV;\n\n\t/* ifindex == 0 is not allowed for job creation */\n\tif (!gwj->ccgw.src_idx || !gwj->ccgw.dst_idx)\n\t\tgoto out;\n\n\tgwj->src.dev = __dev_get_by_index(&init_net, gwj->ccgw.src_idx);\n\n\tif (!gwj->src.dev)\n\t\tgoto out;\n\n\tif (gwj->src.dev->type != ARPHRD_CAN)\n\t\tgoto out;\n\n\tgwj->dst.dev = __dev_get_by_index(&init_net, gwj->ccgw.dst_idx);\n\n\tif (!gwj->dst.dev)\n\t\tgoto out;\n\n\tif (gwj->dst.dev->type != ARPHRD_CAN)\n\t\tgoto out;\n\n\tgwj->limit_hops = limhops;\n\n\tASSERT_RTNL();\n\n\terr = cgw_register_filter(gwj);\n\tif (!err)\n\t\thlist_add_head_rcu(&gwj->list, &cgw_list);\nout:\n\tif (err)\n\t\tkmem_cache_free(cgw_cache, gwj);\n\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}",
                        "code_after_change": "static int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int cgw_create_job(struct sk_buff *skb,  struct nlmsghdr *nlh)\n{\n\tstruct rtcanmsg *r;\n\tstruct cgw_job *gwj;\n\tu8 limhops = 0;\n\tint err = 0;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (nlmsg_len(nlh) < sizeof(*r))\n\t\treturn -EINVAL;\n\n\tr = nlmsg_data(nlh);\n\tif (r->can_family != AF_CAN)\n\t\treturn -EPFNOSUPPORT;\n\n\t/* so far we only support CAN -> CAN routings */\n\tif (r->gwtype != CGW_TYPE_CAN_CAN)\n\t\treturn -EINVAL;\n\n\tgwj = kmem_cache_alloc(cgw_cache, GFP_KERNEL);\n\tif (!gwj)\n\t\treturn -ENOMEM;\n\n\tgwj->handled_frames = 0;\n\tgwj->dropped_frames = 0;\n\tgwj->deleted_frames = 0;\n\tgwj->flags = r->flags;\n\tgwj->gwtype = r->gwtype;\n\n\terr = cgw_parse_attr(nlh, &gwj->mod, CGW_TYPE_CAN_CAN, &gwj->ccgw,\n\t\t\t     &limhops);\n\tif (err < 0)\n\t\tgoto out;\n\n\terr = -ENODEV;\n\n\t/* ifindex == 0 is not allowed for job creation */\n\tif (!gwj->ccgw.src_idx || !gwj->ccgw.dst_idx)\n\t\tgoto out;\n\n\tgwj->src.dev = __dev_get_by_index(&init_net, gwj->ccgw.src_idx);\n\n\tif (!gwj->src.dev)\n\t\tgoto out;\n\n\tif (gwj->src.dev->type != ARPHRD_CAN)\n\t\tgoto out;\n\n\tgwj->dst.dev = __dev_get_by_index(&init_net, gwj->ccgw.dst_idx);\n\n\tif (!gwj->dst.dev)\n\t\tgoto out;\n\n\tif (gwj->dst.dev->type != ARPHRD_CAN)\n\t\tgoto out;\n\n\tgwj->limit_hops = limhops;\n\n\tASSERT_RTNL();\n\n\terr = cgw_register_filter(gwj);\n\tif (!err)\n\t\thlist_add_head_rcu(&gwj->list, &cgw_list);\nout:\n\tif (err)\n\t\tkmem_cache_free(cgw_cache, gwj);\n\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int ovl_fill_super(struct super_block *sb, void *data, int silent)\n{\n\tstruct path lowerpath;\n\tstruct path upperpath;\n\tstruct path workpath;\n\tstruct inode *root_inode;\n\tstruct dentry *root_dentry;\n\tstruct ovl_entry *oe;\n\tstruct ovl_fs *ufs;\n\tstruct kstatfs statfs;\n\tint err;\n\n\terr = -ENOMEM;\n\tufs = kzalloc(sizeof(struct ovl_fs), GFP_KERNEL);\n\tif (!ufs)\n\t\tgoto out;\n\n\terr = ovl_parse_opt((char *) data, &ufs->config);\n\tif (err)\n\t\tgoto out_free_config;\n\n\t/* FIXME: workdir is not needed for a R/O mount */\n\terr = -EINVAL;\n\tif (!ufs->config.upperdir || !ufs->config.lowerdir ||\n\t    !ufs->config.workdir) {\n\t\tpr_err(\"overlayfs: missing upperdir or lowerdir or workdir\\n\");\n\t\tgoto out_free_config;\n\t}\n\n\terr = -ENOMEM;\n\toe = ovl_alloc_entry();\n\tif (oe == NULL)\n\t\tgoto out_free_config;\n\n\terr = ovl_mount_dir(ufs->config.upperdir, &upperpath);\n\tif (err)\n\t\tgoto out_free_oe;\n\n\terr = ovl_mount_dir(ufs->config.lowerdir, &lowerpath);\n\tif (err)\n\t\tgoto out_put_upperpath;\n\n\terr = ovl_mount_dir(ufs->config.workdir, &workpath);\n\tif (err)\n\t\tgoto out_put_lowerpath;\n\n\terr = -EINVAL;\n\tif (!S_ISDIR(upperpath.dentry->d_inode->i_mode) ||\n\t    !S_ISDIR(lowerpath.dentry->d_inode->i_mode) ||\n\t    !S_ISDIR(workpath.dentry->d_inode->i_mode)) {\n\t\tpr_err(\"overlayfs: upperdir or lowerdir or workdir not a directory\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (upperpath.mnt != workpath.mnt) {\n\t\tpr_err(\"overlayfs: workdir and upperdir must reside under the same mount\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\tif (!ovl_workdir_ok(workpath.dentry, upperpath.dentry)) {\n\t\tpr_err(\"overlayfs: workdir and upperdir must be separate subtrees\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (!ovl_is_allowed_fs_type(upperpath.dentry)) {\n\t\tpr_err(\"overlayfs: filesystem of upperdir is not supported\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (!ovl_is_allowed_fs_type(lowerpath.dentry)) {\n\t\tpr_err(\"overlayfs: filesystem of lowerdir is not supported\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\terr = vfs_statfs(&lowerpath, &statfs);\n\tif (err) {\n\t\tpr_err(\"overlayfs: statfs failed on lowerpath\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\tufs->lower_namelen = statfs.f_namelen;\n\n\tufs->upper_mnt = clone_private_mount(&upperpath);\n\terr = PTR_ERR(ufs->upper_mnt);\n\tif (IS_ERR(ufs->upper_mnt)) {\n\t\tpr_err(\"overlayfs: failed to clone upperpath\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tufs->lower_mnt = clone_private_mount(&lowerpath);\n\terr = PTR_ERR(ufs->lower_mnt);\n\tif (IS_ERR(ufs->lower_mnt)) {\n\t\tpr_err(\"overlayfs: failed to clone lowerpath\\n\");\n\t\tgoto out_put_upper_mnt;\n\t}\n\n\tufs->workdir = ovl_workdir_create(ufs->upper_mnt, workpath.dentry);\n\terr = PTR_ERR(ufs->workdir);\n\tif (IS_ERR(ufs->workdir)) {\n\t\tpr_err(\"overlayfs: failed to create directory %s/%s\\n\",\n\t\t       ufs->config.workdir, OVL_WORKDIR_NAME);\n\t\tgoto out_put_lower_mnt;\n\t}\n\n\t/*\n\t * Make lower_mnt R/O.  That way fchmod/fchown on lower file\n\t * will fail instead of modifying lower fs.\n\t */\n\tufs->lower_mnt->mnt_flags |= MNT_READONLY;\n\n\t/* If the upper fs is r/o, we mark overlayfs r/o too */\n\tif (ufs->upper_mnt->mnt_sb->s_flags & MS_RDONLY)\n\t\tsb->s_flags |= MS_RDONLY;\n\n\tsb->s_d_op = &ovl_dentry_operations;\n\n\terr = -ENOMEM;\n\troot_inode = ovl_new_inode(sb, S_IFDIR, oe);\n\tif (!root_inode)\n\t\tgoto out_put_workdir;\n\n\troot_dentry = d_make_root(root_inode);\n\tif (!root_dentry)\n\t\tgoto out_put_workdir;\n\n\tmntput(upperpath.mnt);\n\tmntput(lowerpath.mnt);\n\tpath_put(&workpath);\n\n\toe->__upperdentry = upperpath.dentry;\n\toe->lowerdentry = lowerpath.dentry;\n\n\troot_dentry->d_fsdata = oe;\n\n\tsb->s_magic = OVERLAYFS_SUPER_MAGIC;\n\tsb->s_op = &ovl_super_operations;\n\tsb->s_root = root_dentry;\n\tsb->s_fs_info = ufs;\n\n\treturn 0;\n\nout_put_workdir:\n\tdput(ufs->workdir);\nout_put_lower_mnt:\n\tmntput(ufs->lower_mnt);\nout_put_upper_mnt:\n\tmntput(ufs->upper_mnt);\nout_put_workpath:\n\tpath_put(&workpath);\nout_put_lowerpath:\n\tpath_put(&lowerpath);\nout_put_upperpath:\n\tpath_put(&upperpath);\nout_free_oe:\n\tkfree(oe);\nout_free_config:\n\tkfree(ufs->config.lowerdir);\n\tkfree(ufs->config.upperdir);\n\tkfree(ufs->config.workdir);\n\tkfree(ufs);\nout:\n\treturn err;\n}",
                        "code_after_change": "static int ovl_fill_super(struct super_block *sb, void *data, int silent)\n{\n\tstruct path lowerpath;\n\tstruct path upperpath;\n\tstruct path workpath;\n\tstruct inode *root_inode;\n\tstruct dentry *root_dentry;\n\tstruct ovl_entry *oe;\n\tstruct ovl_fs *ufs;\n\tstruct kstatfs statfs;\n\tint err;\n\n\terr = -ENOMEM;\n\tufs = kzalloc(sizeof(struct ovl_fs), GFP_KERNEL);\n\tif (!ufs)\n\t\tgoto out;\n\n\terr = ovl_parse_opt((char *) data, &ufs->config);\n\tif (err)\n\t\tgoto out_free_config;\n\n\t/* FIXME: workdir is not needed for a R/O mount */\n\terr = -EINVAL;\n\tif (!ufs->config.upperdir || !ufs->config.lowerdir ||\n\t    !ufs->config.workdir) {\n\t\tpr_err(\"overlayfs: missing upperdir or lowerdir or workdir\\n\");\n\t\tgoto out_free_config;\n\t}\n\n\terr = -ENOMEM;\n\toe = ovl_alloc_entry();\n\tif (oe == NULL)\n\t\tgoto out_free_config;\n\n\terr = ovl_mount_dir(ufs->config.upperdir, &upperpath);\n\tif (err)\n\t\tgoto out_free_oe;\n\n\terr = ovl_mount_dir(ufs->config.lowerdir, &lowerpath);\n\tif (err)\n\t\tgoto out_put_upperpath;\n\n\terr = ovl_mount_dir(ufs->config.workdir, &workpath);\n\tif (err)\n\t\tgoto out_put_lowerpath;\n\n\terr = -EINVAL;\n\tif (!S_ISDIR(upperpath.dentry->d_inode->i_mode) ||\n\t    !S_ISDIR(lowerpath.dentry->d_inode->i_mode) ||\n\t    !S_ISDIR(workpath.dentry->d_inode->i_mode)) {\n\t\tpr_err(\"overlayfs: upperdir or lowerdir or workdir not a directory\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (upperpath.mnt != workpath.mnt) {\n\t\tpr_err(\"overlayfs: workdir and upperdir must reside under the same mount\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\tif (!ovl_workdir_ok(workpath.dentry, upperpath.dentry)) {\n\t\tpr_err(\"overlayfs: workdir and upperdir must be separate subtrees\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (!ovl_is_allowed_fs_type(upperpath.dentry)) {\n\t\tpr_err(\"overlayfs: filesystem of upperdir is not supported\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (!ovl_is_allowed_fs_type(lowerpath.dentry)) {\n\t\tpr_err(\"overlayfs: filesystem of lowerdir is not supported\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\terr = vfs_statfs(&lowerpath, &statfs);\n\tif (err) {\n\t\tpr_err(\"overlayfs: statfs failed on lowerpath\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\tufs->lower_namelen = statfs.f_namelen;\n\n\tsb->s_stack_depth = max(upperpath.mnt->mnt_sb->s_stack_depth,\n\t\t\t\tlowerpath.mnt->mnt_sb->s_stack_depth) + 1;\n\n\terr = -EINVAL;\n\tif (sb->s_stack_depth > FILESYSTEM_MAX_STACK_DEPTH) {\n\t\tpr_err(\"overlayfs: maximum fs stacking depth exceeded\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tufs->upper_mnt = clone_private_mount(&upperpath);\n\terr = PTR_ERR(ufs->upper_mnt);\n\tif (IS_ERR(ufs->upper_mnt)) {\n\t\tpr_err(\"overlayfs: failed to clone upperpath\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tufs->lower_mnt = clone_private_mount(&lowerpath);\n\terr = PTR_ERR(ufs->lower_mnt);\n\tif (IS_ERR(ufs->lower_mnt)) {\n\t\tpr_err(\"overlayfs: failed to clone lowerpath\\n\");\n\t\tgoto out_put_upper_mnt;\n\t}\n\n\tufs->workdir = ovl_workdir_create(ufs->upper_mnt, workpath.dentry);\n\terr = PTR_ERR(ufs->workdir);\n\tif (IS_ERR(ufs->workdir)) {\n\t\tpr_err(\"overlayfs: failed to create directory %s/%s\\n\",\n\t\t       ufs->config.workdir, OVL_WORKDIR_NAME);\n\t\tgoto out_put_lower_mnt;\n\t}\n\n\t/*\n\t * Make lower_mnt R/O.  That way fchmod/fchown on lower file\n\t * will fail instead of modifying lower fs.\n\t */\n\tufs->lower_mnt->mnt_flags |= MNT_READONLY;\n\n\t/* If the upper fs is r/o, we mark overlayfs r/o too */\n\tif (ufs->upper_mnt->mnt_sb->s_flags & MS_RDONLY)\n\t\tsb->s_flags |= MS_RDONLY;\n\n\tsb->s_d_op = &ovl_dentry_operations;\n\n\terr = -ENOMEM;\n\troot_inode = ovl_new_inode(sb, S_IFDIR, oe);\n\tif (!root_inode)\n\t\tgoto out_put_workdir;\n\n\troot_dentry = d_make_root(root_inode);\n\tif (!root_dentry)\n\t\tgoto out_put_workdir;\n\n\tmntput(upperpath.mnt);\n\tmntput(lowerpath.mnt);\n\tpath_put(&workpath);\n\n\toe->__upperdentry = upperpath.dentry;\n\toe->lowerdentry = lowerpath.dentry;\n\n\troot_dentry->d_fsdata = oe;\n\n\tsb->s_magic = OVERLAYFS_SUPER_MAGIC;\n\tsb->s_op = &ovl_super_operations;\n\tsb->s_root = root_dentry;\n\tsb->s_fs_info = ufs;\n\n\treturn 0;\n\nout_put_workdir:\n\tdput(ufs->workdir);\nout_put_lower_mnt:\n\tmntput(ufs->lower_mnt);\nout_put_upper_mnt:\n\tmntput(ufs->upper_mnt);\nout_put_workpath:\n\tpath_put(&workpath);\nout_put_lowerpath:\n\tpath_put(&lowerpath);\nout_put_upperpath:\n\tpath_put(&upperpath);\nout_free_oe:\n\tkfree(oe);\nout_free_config:\n\tkfree(ufs->config.lowerdir);\n\tkfree(ufs->config.upperdir);\n\tkfree(ufs->config.workdir);\n\tkfree(ufs);\nout:\n\treturn err;\n}",
                        "cve_id": "CVE-2014-9922"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int cgw_create_job(struct sk_buff *skb,  struct nlmsghdr *nlh)\n{\n\tstruct rtcanmsg *r;\n\tstruct cgw_job *gwj;\n\tu8 limhops = 0;\n\tint err = 0;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (nlmsg_len(nlh) < sizeof(*r))\n\t\treturn -EINVAL;\n\n\tr = nlmsg_data(nlh);\n\tif (r->can_family != AF_CAN)\n\t\treturn -EPFNOSUPPORT;\n\n\t/* so far we only support CAN -> CAN routings */\n\tif (r->gwtype != CGW_TYPE_CAN_CAN)\n\t\treturn -EINVAL;\n\n\tgwj = kmem_cache_alloc(cgw_cache, GFP_KERNEL);\n\tif (!gwj)\n\t\treturn -ENOMEM;\n\n\tgwj->handled_frames = 0;\n\tgwj->dropped_frames = 0;\n\tgwj->deleted_frames = 0;\n\tgwj->flags = r->flags;\n\tgwj->gwtype = r->gwtype;\n\n\terr = cgw_parse_attr(nlh, &gwj->mod, CGW_TYPE_CAN_CAN, &gwj->ccgw,\n\t\t\t     &limhops);\n\tif (err < 0)\n\t\tgoto out;\n\n\terr = -ENODEV;\n\n\t/* ifindex == 0 is not allowed for job creation */\n\tif (!gwj->ccgw.src_idx || !gwj->ccgw.dst_idx)\n\t\tgoto out;\n\n\tgwj->src.dev = __dev_get_by_index(&init_net, gwj->ccgw.src_idx);\n\n\tif (!gwj->src.dev)\n\t\tgoto out;\n\n\tif (gwj->src.dev->type != ARPHRD_CAN)\n\t\tgoto out;\n\n\tgwj->dst.dev = __dev_get_by_index(&init_net, gwj->ccgw.dst_idx);\n\n\tif (!gwj->dst.dev)\n\t\tgoto out;\n\n\tif (gwj->dst.dev->type != ARPHRD_CAN)\n\t\tgoto out;\n\n\tgwj->limit_hops = limhops;\n\n\tASSERT_RTNL();\n\n\terr = cgw_register_filter(gwj);\n\tif (!err)\n\t\thlist_add_head_rcu(&gwj->list, &cgw_list);\nout:\n\tif (err)\n\t\tkmem_cache_free(cgw_cache, gwj);\n\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int ovl_fill_super(struct super_block *sb, void *data, int silent)\n{\n\tstruct path lowerpath;\n\tstruct path upperpath;\n\tstruct path workpath;\n\tstruct inode *root_inode;\n\tstruct dentry *root_dentry;\n\tstruct ovl_entry *oe;\n\tstruct ovl_fs *ufs;\n\tstruct kstatfs statfs;\n\tint err;\n\n\terr = -ENOMEM;\n\tufs = kzalloc(sizeof(struct ovl_fs), GFP_KERNEL);\n\tif (!ufs)\n\t\tgoto out;\n\n\terr = ovl_parse_opt((char *) data, &ufs->config);\n\tif (err)\n\t\tgoto out_free_config;\n\n\t/* FIXME: workdir is not needed for a R/O mount */\n\terr = -EINVAL;\n\tif (!ufs->config.upperdir || !ufs->config.lowerdir ||\n\t    !ufs->config.workdir) {\n\t\tpr_err(\"overlayfs: missing upperdir or lowerdir or workdir\\n\");\n\t\tgoto out_free_config;\n\t}\n\n\terr = -ENOMEM;\n\toe = ovl_alloc_entry();\n\tif (oe == NULL)\n\t\tgoto out_free_config;\n\n\terr = ovl_mount_dir(ufs->config.upperdir, &upperpath);\n\tif (err)\n\t\tgoto out_free_oe;\n\n\terr = ovl_mount_dir(ufs->config.lowerdir, &lowerpath);\n\tif (err)\n\t\tgoto out_put_upperpath;\n\n\terr = ovl_mount_dir(ufs->config.workdir, &workpath);\n\tif (err)\n\t\tgoto out_put_lowerpath;\n\n\terr = -EINVAL;\n\tif (!S_ISDIR(upperpath.dentry->d_inode->i_mode) ||\n\t    !S_ISDIR(lowerpath.dentry->d_inode->i_mode) ||\n\t    !S_ISDIR(workpath.dentry->d_inode->i_mode)) {\n\t\tpr_err(\"overlayfs: upperdir or lowerdir or workdir not a directory\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (upperpath.mnt != workpath.mnt) {\n\t\tpr_err(\"overlayfs: workdir and upperdir must reside under the same mount\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\tif (!ovl_workdir_ok(workpath.dentry, upperpath.dentry)) {\n\t\tpr_err(\"overlayfs: workdir and upperdir must be separate subtrees\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (!ovl_is_allowed_fs_type(upperpath.dentry)) {\n\t\tpr_err(\"overlayfs: filesystem of upperdir is not supported\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (!ovl_is_allowed_fs_type(lowerpath.dentry)) {\n\t\tpr_err(\"overlayfs: filesystem of lowerdir is not supported\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\terr = vfs_statfs(&lowerpath, &statfs);\n\tif (err) {\n\t\tpr_err(\"overlayfs: statfs failed on lowerpath\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\tufs->lower_namelen = statfs.f_namelen;\n\n\tufs->upper_mnt = clone_private_mount(&upperpath);\n\terr = PTR_ERR(ufs->upper_mnt);\n\tif (IS_ERR(ufs->upper_mnt)) {\n\t\tpr_err(\"overlayfs: failed to clone upperpath\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tufs->lower_mnt = clone_private_mount(&lowerpath);\n\terr = PTR_ERR(ufs->lower_mnt);\n\tif (IS_ERR(ufs->lower_mnt)) {\n\t\tpr_err(\"overlayfs: failed to clone lowerpath\\n\");\n\t\tgoto out_put_upper_mnt;\n\t}\n\n\tufs->workdir = ovl_workdir_create(ufs->upper_mnt, workpath.dentry);\n\terr = PTR_ERR(ufs->workdir);\n\tif (IS_ERR(ufs->workdir)) {\n\t\tpr_err(\"overlayfs: failed to create directory %s/%s\\n\",\n\t\t       ufs->config.workdir, OVL_WORKDIR_NAME);\n\t\tgoto out_put_lower_mnt;\n\t}\n\n\t/*\n\t * Make lower_mnt R/O.  That way fchmod/fchown on lower file\n\t * will fail instead of modifying lower fs.\n\t */\n\tufs->lower_mnt->mnt_flags |= MNT_READONLY;\n\n\t/* If the upper fs is r/o, we mark overlayfs r/o too */\n\tif (ufs->upper_mnt->mnt_sb->s_flags & MS_RDONLY)\n\t\tsb->s_flags |= MS_RDONLY;\n\n\tsb->s_d_op = &ovl_dentry_operations;\n\n\terr = -ENOMEM;\n\troot_inode = ovl_new_inode(sb, S_IFDIR, oe);\n\tif (!root_inode)\n\t\tgoto out_put_workdir;\n\n\troot_dentry = d_make_root(root_inode);\n\tif (!root_dentry)\n\t\tgoto out_put_workdir;\n\n\tmntput(upperpath.mnt);\n\tmntput(lowerpath.mnt);\n\tpath_put(&workpath);\n\n\toe->__upperdentry = upperpath.dentry;\n\toe->lowerdentry = lowerpath.dentry;\n\n\troot_dentry->d_fsdata = oe;\n\n\tsb->s_magic = OVERLAYFS_SUPER_MAGIC;\n\tsb->s_op = &ovl_super_operations;\n\tsb->s_root = root_dentry;\n\tsb->s_fs_info = ufs;\n\n\treturn 0;\n\nout_put_workdir:\n\tdput(ufs->workdir);\nout_put_lower_mnt:\n\tmntput(ufs->lower_mnt);\nout_put_upper_mnt:\n\tmntput(ufs->upper_mnt);\nout_put_workpath:\n\tpath_put(&workpath);\nout_put_lowerpath:\n\tpath_put(&lowerpath);\nout_put_upperpath:\n\tpath_put(&upperpath);\nout_free_oe:\n\tkfree(oe);\nout_free_config:\n\tkfree(ufs->config.lowerdir);\n\tkfree(ufs->config.upperdir);\n\tkfree(ufs->config.workdir);\n\tkfree(ufs);\nout:\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int ovl_fill_super(struct super_block *sb, void *data, int silent)\n{\n\tstruct path lowerpath;\n\tstruct path upperpath;\n\tstruct path workpath;\n\tstruct inode *root_inode;\n\tstruct dentry *root_dentry;\n\tstruct ovl_entry *oe;\n\tstruct ovl_fs *ufs;\n\tstruct kstatfs statfs;\n\tint err;\n\n\terr = -ENOMEM;\n\tufs = kzalloc(sizeof(struct ovl_fs), GFP_KERNEL);\n\tif (!ufs)\n\t\tgoto out;\n\n\terr = ovl_parse_opt((char *) data, &ufs->config);\n\tif (err)\n\t\tgoto out_free_config;\n\n\t/* FIXME: workdir is not needed for a R/O mount */\n\terr = -EINVAL;\n\tif (!ufs->config.upperdir || !ufs->config.lowerdir ||\n\t    !ufs->config.workdir) {\n\t\tpr_err(\"overlayfs: missing upperdir or lowerdir or workdir\\n\");\n\t\tgoto out_free_config;\n\t}\n\n\terr = -ENOMEM;\n\toe = ovl_alloc_entry();\n\tif (oe == NULL)\n\t\tgoto out_free_config;\n\n\terr = ovl_mount_dir(ufs->config.upperdir, &upperpath);\n\tif (err)\n\t\tgoto out_free_oe;\n\n\terr = ovl_mount_dir(ufs->config.lowerdir, &lowerpath);\n\tif (err)\n\t\tgoto out_put_upperpath;\n\n\terr = ovl_mount_dir(ufs->config.workdir, &workpath);\n\tif (err)\n\t\tgoto out_put_lowerpath;\n\n\terr = -EINVAL;\n\tif (!S_ISDIR(upperpath.dentry->d_inode->i_mode) ||\n\t    !S_ISDIR(lowerpath.dentry->d_inode->i_mode) ||\n\t    !S_ISDIR(workpath.dentry->d_inode->i_mode)) {\n\t\tpr_err(\"overlayfs: upperdir or lowerdir or workdir not a directory\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (upperpath.mnt != workpath.mnt) {\n\t\tpr_err(\"overlayfs: workdir and upperdir must reside under the same mount\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\tif (!ovl_workdir_ok(workpath.dentry, upperpath.dentry)) {\n\t\tpr_err(\"overlayfs: workdir and upperdir must be separate subtrees\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (!ovl_is_allowed_fs_type(upperpath.dentry)) {\n\t\tpr_err(\"overlayfs: filesystem of upperdir is not supported\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (!ovl_is_allowed_fs_type(lowerpath.dentry)) {\n\t\tpr_err(\"overlayfs: filesystem of lowerdir is not supported\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\terr = vfs_statfs(&lowerpath, &statfs);\n\tif (err) {\n\t\tpr_err(\"overlayfs: statfs failed on lowerpath\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\tufs->lower_namelen = statfs.f_namelen;\n\n\tsb->s_stack_depth = max(upperpath.mnt->mnt_sb->s_stack_depth,\n\t\t\t\tlowerpath.mnt->mnt_sb->s_stack_depth) + 1;\n\n\terr = -EINVAL;\n\tif (sb->s_stack_depth > FILESYSTEM_MAX_STACK_DEPTH) {\n\t\tpr_err(\"overlayfs: maximum fs stacking depth exceeded\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tufs->upper_mnt = clone_private_mount(&upperpath);\n\terr = PTR_ERR(ufs->upper_mnt);\n\tif (IS_ERR(ufs->upper_mnt)) {\n\t\tpr_err(\"overlayfs: failed to clone upperpath\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tufs->lower_mnt = clone_private_mount(&lowerpath);\n\terr = PTR_ERR(ufs->lower_mnt);\n\tif (IS_ERR(ufs->lower_mnt)) {\n\t\tpr_err(\"overlayfs: failed to clone lowerpath\\n\");\n\t\tgoto out_put_upper_mnt;\n\t}\n\n\tufs->workdir = ovl_workdir_create(ufs->upper_mnt, workpath.dentry);\n\terr = PTR_ERR(ufs->workdir);\n\tif (IS_ERR(ufs->workdir)) {\n\t\tpr_err(\"overlayfs: failed to create directory %s/%s\\n\",\n\t\t       ufs->config.workdir, OVL_WORKDIR_NAME);\n\t\tgoto out_put_lower_mnt;\n\t}\n\n\t/*\n\t * Make lower_mnt R/O.  That way fchmod/fchown on lower file\n\t * will fail instead of modifying lower fs.\n\t */\n\tufs->lower_mnt->mnt_flags |= MNT_READONLY;\n\n\t/* If the upper fs is r/o, we mark overlayfs r/o too */\n\tif (ufs->upper_mnt->mnt_sb->s_flags & MS_RDONLY)\n\t\tsb->s_flags |= MS_RDONLY;\n\n\tsb->s_d_op = &ovl_dentry_operations;\n\n\terr = -ENOMEM;\n\troot_inode = ovl_new_inode(sb, S_IFDIR, oe);\n\tif (!root_inode)\n\t\tgoto out_put_workdir;\n\n\troot_dentry = d_make_root(root_inode);\n\tif (!root_dentry)\n\t\tgoto out_put_workdir;\n\n\tmntput(upperpath.mnt);\n\tmntput(lowerpath.mnt);\n\tpath_put(&workpath);\n\n\toe->__upperdentry = upperpath.dentry;\n\toe->lowerdentry = lowerpath.dentry;\n\n\troot_dentry->d_fsdata = oe;\n\n\tsb->s_magic = OVERLAYFS_SUPER_MAGIC;\n\tsb->s_op = &ovl_super_operations;\n\tsb->s_root = root_dentry;\n\tsb->s_fs_info = ufs;\n\n\treturn 0;\n\nout_put_workdir:\n\tdput(ufs->workdir);\nout_put_lower_mnt:\n\tmntput(ufs->lower_mnt);\nout_put_upper_mnt:\n\tmntput(ufs->upper_mnt);\nout_put_workpath:\n\tpath_put(&workpath);\nout_put_lowerpath:\n\tpath_put(&lowerpath);\nout_put_upperpath:\n\tpath_put(&upperpath);\nout_free_oe:\n\tkfree(oe);\nout_free_config:\n\tkfree(ufs->config.lowerdir);\n\tkfree(ufs->config.upperdir);\n\tkfree(ufs->config.workdir);\n\tkfree(ufs);\nout:\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 456,
            "cve_id": "CVE-2014-0181",
            "code_snippet": "static int tc_modify_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm;\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q, *p;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\t/* Reinit, just in case something touches this. */\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\ttcm = nlmsg_data(n);\n\tclid = tcm->tcm_parent;\n\tq = p = NULL;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (clid != TC_H_INGRESS) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue_create(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\n\t\t/* It may be default qdisc, ignore it */\n\t\tif (q && q->handle == 0)\n\t\t\tq = NULL;\n\n\t\tif (!q || !tcm->tcm_handle || q->handle != tcm->tcm_handle) {\n\t\t\tif (tcm->tcm_handle) {\n\t\t\t\tif (q && !(n->nlmsg_flags & NLM_F_REPLACE))\n\t\t\t\t\treturn -EEXIST;\n\t\t\t\tif (TC_H_MIN(tcm->tcm_handle))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\t\t\tif (!q)\n\t\t\t\t\tgoto create_n_graft;\n\t\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\t\treturn -EEXIST;\n\t\t\t\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tif (q == p ||\n\t\t\t\t    (p && check_loop(q, p, 0)))\n\t\t\t\t\treturn -ELOOP;\n\t\t\t\tatomic_inc(&q->refcnt);\n\t\t\t\tgoto graft;\n\t\t\t} else {\n\t\t\t\tif (!q)\n\t\t\t\t\tgoto create_n_graft;\n\n\t\t\t\t/* This magic test requires explanation.\n\t\t\t\t *\n\t\t\t\t *   We know, that some child q is already\n\t\t\t\t *   attached to this parent and have choice:\n\t\t\t\t *   either to change it or to create/graft new one.\n\t\t\t\t *\n\t\t\t\t *   1. We are allowed to create/graft only\n\t\t\t\t *   if CREATE and REPLACE flags are set.\n\t\t\t\t *\n\t\t\t\t *   2. If EXCL is set, requestor wanted to say,\n\t\t\t\t *   that qdisc tcm_handle is not expected\n\t\t\t\t *   to exist, so that we choose create/graft too.\n\t\t\t\t *\n\t\t\t\t *   3. The last case is when no flags are set.\n\t\t\t\t *   Alas, it is sort of hole in API, we\n\t\t\t\t *   cannot decide what to do unambiguously.\n\t\t\t\t *   For now we select create/graft, if\n\t\t\t\t *   user gave KIND, which does not match existing.\n\t\t\t\t */\n\t\t\t\tif ((n->nlmsg_flags & NLM_F_CREATE) &&\n\t\t\t\t    (n->nlmsg_flags & NLM_F_REPLACE) &&\n\t\t\t\t    ((n->nlmsg_flags & NLM_F_EXCL) ||\n\t\t\t\t     (tca[TCA_KIND] &&\n\t\t\t\t      nla_strcmp(tca[TCA_KIND], q->ops->id))))\n\t\t\t\t\tgoto create_n_graft;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (!tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t}\n\n\t/* Change qdisc parameters */\n\tif (q == NULL)\n\t\treturn -ENOENT;\n\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\treturn -EEXIST;\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\terr = qdisc_change(q, tca);\n\tif (err == 0)\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\treturn err;\n\ncreate_n_graft:\n\tif (!(n->nlmsg_flags & NLM_F_CREATE))\n\t\treturn -ENOENT;\n\tif (clid == TC_H_INGRESS) {\n\t\tif (dev_ingress_queue(dev))\n\t\t\tq = qdisc_create(dev, dev_ingress_queue(dev), p,\n\t\t\t\t\t tcm->tcm_parent, tcm->tcm_parent,\n\t\t\t\t\t tca, &err);\n\t\telse\n\t\t\terr = -ENOENT;\n\t} else {\n\t\tstruct netdev_queue *dev_queue;\n\n\t\tif (p && p->ops->cl_ops && p->ops->cl_ops->select_queue)\n\t\t\tdev_queue = p->ops->cl_ops->select_queue(p, tcm);\n\t\telse if (p)\n\t\t\tdev_queue = p->dev_queue;\n\t\telse\n\t\t\tdev_queue = netdev_get_tx_queue(dev, 0);\n\n\t\tq = qdisc_create(dev, dev_queue, p,\n\t\t\t\t tcm->tcm_parent, tcm->tcm_handle,\n\t\t\t\t tca, &err);\n\t}\n\tif (q == NULL) {\n\t\tif (err == -EAGAIN)\n\t\t\tgoto replay;\n\t\treturn err;\n\t}\n\ngraft:\n\terr = qdisc_graft(dev, p, skb, n, clid, q, NULL);\n\tif (err) {\n\t\tif (q)\n\t\t\tqdisc_destroy(q);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}",
                        "code_after_change": "static int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int tc_modify_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm;\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q, *p;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\t/* Reinit, just in case something touches this. */\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\ttcm = nlmsg_data(n);\n\tclid = tcm->tcm_parent;\n\tq = p = NULL;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (clid != TC_H_INGRESS) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue_create(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\n\t\t/* It may be default qdisc, ignore it */\n\t\tif (q && q->handle == 0)\n\t\t\tq = NULL;\n\n\t\tif (!q || !tcm->tcm_handle || q->handle != tcm->tcm_handle) {\n\t\t\tif (tcm->tcm_handle) {\n\t\t\t\tif (q && !(n->nlmsg_flags & NLM_F_REPLACE))\n\t\t\t\t\treturn -EEXIST;\n\t\t\t\tif (TC_H_MIN(tcm->tcm_handle))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\t\t\tif (!q)\n\t\t\t\t\tgoto create_n_graft;\n\t\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\t\treturn -EEXIST;\n\t\t\t\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tif (q == p ||\n\t\t\t\t    (p && check_loop(q, p, 0)))\n\t\t\t\t\treturn -ELOOP;\n\t\t\t\tatomic_inc(&q->refcnt);\n\t\t\t\tgoto graft;\n\t\t\t} else {\n\t\t\t\tif (!q)\n\t\t\t\t\tgoto create_n_graft;\n\n\t\t\t\t/* This magic test requires explanation.\n\t\t\t\t *\n\t\t\t\t *   We know, that some child q is already\n\t\t\t\t *   attached to this parent and have choice:\n\t\t\t\t *   either to change it or to create/graft new one.\n\t\t\t\t *\n\t\t\t\t *   1. We are allowed to create/graft only\n\t\t\t\t *   if CREATE and REPLACE flags are set.\n\t\t\t\t *\n\t\t\t\t *   2. If EXCL is set, requestor wanted to say,\n\t\t\t\t *   that qdisc tcm_handle is not expected\n\t\t\t\t *   to exist, so that we choose create/graft too.\n\t\t\t\t *\n\t\t\t\t *   3. The last case is when no flags are set.\n\t\t\t\t *   Alas, it is sort of hole in API, we\n\t\t\t\t *   cannot decide what to do unambiguously.\n\t\t\t\t *   For now we select create/graft, if\n\t\t\t\t *   user gave KIND, which does not match existing.\n\t\t\t\t */\n\t\t\t\tif ((n->nlmsg_flags & NLM_F_CREATE) &&\n\t\t\t\t    (n->nlmsg_flags & NLM_F_REPLACE) &&\n\t\t\t\t    ((n->nlmsg_flags & NLM_F_EXCL) ||\n\t\t\t\t     (tca[TCA_KIND] &&\n\t\t\t\t      nla_strcmp(tca[TCA_KIND], q->ops->id))))\n\t\t\t\t\tgoto create_n_graft;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (!tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t}\n\n\t/* Change qdisc parameters */\n\tif (q == NULL)\n\t\treturn -ENOENT;\n\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\treturn -EEXIST;\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\terr = qdisc_change(q, tca);\n\tif (err == 0)\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\treturn err;\n\ncreate_n_graft:\n\tif (!(n->nlmsg_flags & NLM_F_CREATE))\n\t\treturn -ENOENT;\n\tif (clid == TC_H_INGRESS) {\n\t\tif (dev_ingress_queue(dev))\n\t\t\tq = qdisc_create(dev, dev_ingress_queue(dev), p,\n\t\t\t\t\t tcm->tcm_parent, tcm->tcm_parent,\n\t\t\t\t\t tca, &err);\n\t\telse\n\t\t\terr = -ENOENT;\n\t} else {\n\t\tstruct netdev_queue *dev_queue;\n\n\t\tif (p && p->ops->cl_ops && p->ops->cl_ops->select_queue)\n\t\t\tdev_queue = p->ops->cl_ops->select_queue(p, tcm);\n\t\telse if (p)\n\t\t\tdev_queue = p->dev_queue;\n\t\telse\n\t\t\tdev_queue = netdev_get_tx_queue(dev, 0);\n\n\t\tq = qdisc_create(dev, dev_queue, p,\n\t\t\t\t tcm->tcm_parent, tcm->tcm_handle,\n\t\t\t\t tca, &err);\n\t}\n\tif (q == NULL) {\n\t\tif (err == -EAGAIN)\n\t\t\tgoto replay;\n\t\treturn err;\n\t}\n\ngraft:\n\terr = qdisc_graft(dev, p, skb, n, clid, q, NULL);\n\tif (err) {\n\t\tif (q)\n\t\t\tqdisc_destroy(q);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}",
                        "code_after_change": "static int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int tc_modify_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm;\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q, *p;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\t/* Reinit, just in case something touches this. */\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\ttcm = nlmsg_data(n);\n\tclid = tcm->tcm_parent;\n\tq = p = NULL;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (clid != TC_H_INGRESS) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue_create(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\n\t\t/* It may be default qdisc, ignore it */\n\t\tif (q && q->handle == 0)\n\t\t\tq = NULL;\n\n\t\tif (!q || !tcm->tcm_handle || q->handle != tcm->tcm_handle) {\n\t\t\tif (tcm->tcm_handle) {\n\t\t\t\tif (q && !(n->nlmsg_flags & NLM_F_REPLACE))\n\t\t\t\t\treturn -EEXIST;\n\t\t\t\tif (TC_H_MIN(tcm->tcm_handle))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\t\t\tif (!q)\n\t\t\t\t\tgoto create_n_graft;\n\t\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\t\treturn -EEXIST;\n\t\t\t\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tif (q == p ||\n\t\t\t\t    (p && check_loop(q, p, 0)))\n\t\t\t\t\treturn -ELOOP;\n\t\t\t\tatomic_inc(&q->refcnt);\n\t\t\t\tgoto graft;\n\t\t\t} else {\n\t\t\t\tif (!q)\n\t\t\t\t\tgoto create_n_graft;\n\n\t\t\t\t/* This magic test requires explanation.\n\t\t\t\t *\n\t\t\t\t *   We know, that some child q is already\n\t\t\t\t *   attached to this parent and have choice:\n\t\t\t\t *   either to change it or to create/graft new one.\n\t\t\t\t *\n\t\t\t\t *   1. We are allowed to create/graft only\n\t\t\t\t *   if CREATE and REPLACE flags are set.\n\t\t\t\t *\n\t\t\t\t *   2. If EXCL is set, requestor wanted to say,\n\t\t\t\t *   that qdisc tcm_handle is not expected\n\t\t\t\t *   to exist, so that we choose create/graft too.\n\t\t\t\t *\n\t\t\t\t *   3. The last case is when no flags are set.\n\t\t\t\t *   Alas, it is sort of hole in API, we\n\t\t\t\t *   cannot decide what to do unambiguously.\n\t\t\t\t *   For now we select create/graft, if\n\t\t\t\t *   user gave KIND, which does not match existing.\n\t\t\t\t */\n\t\t\t\tif ((n->nlmsg_flags & NLM_F_CREATE) &&\n\t\t\t\t    (n->nlmsg_flags & NLM_F_REPLACE) &&\n\t\t\t\t    ((n->nlmsg_flags & NLM_F_EXCL) ||\n\t\t\t\t     (tca[TCA_KIND] &&\n\t\t\t\t      nla_strcmp(tca[TCA_KIND], q->ops->id))))\n\t\t\t\t\tgoto create_n_graft;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (!tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t}\n\n\t/* Change qdisc parameters */\n\tif (q == NULL)\n\t\treturn -ENOENT;\n\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\treturn -EEXIST;\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\terr = qdisc_change(q, tca);\n\tif (err == 0)\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\treturn err;\n\ncreate_n_graft:\n\tif (!(n->nlmsg_flags & NLM_F_CREATE))\n\t\treturn -ENOENT;\n\tif (clid == TC_H_INGRESS) {\n\t\tif (dev_ingress_queue(dev))\n\t\t\tq = qdisc_create(dev, dev_ingress_queue(dev), p,\n\t\t\t\t\t tcm->tcm_parent, tcm->tcm_parent,\n\t\t\t\t\t tca, &err);\n\t\telse\n\t\t\terr = -ENOENT;\n\t} else {\n\t\tstruct netdev_queue *dev_queue;\n\n\t\tif (p && p->ops->cl_ops && p->ops->cl_ops->select_queue)\n\t\t\tdev_queue = p->ops->cl_ops->select_queue(p, tcm);\n\t\telse if (p)\n\t\t\tdev_queue = p->dev_queue;\n\t\telse\n\t\t\tdev_queue = netdev_get_tx_queue(dev, 0);\n\n\t\tq = qdisc_create(dev, dev_queue, p,\n\t\t\t\t tcm->tcm_parent, tcm->tcm_handle,\n\t\t\t\t tca, &err);\n\t}\n\tif (q == NULL) {\n\t\tif (err == -EAGAIN)\n\t\t\tgoto replay;\n\t\treturn err;\n\t}\n\ngraft:\n\terr = qdisc_graft(dev, p, skb, n, clid, q, NULL);\n\tif (err) {\n\t\tif (q)\n\t\t\tqdisc_destroy(q);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}",
                        "code_after_change": "static int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(skb, net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int tc_modify_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm;\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q, *p;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\t/* Reinit, just in case something touches this. */\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\ttcm = nlmsg_data(n);\n\tclid = tcm->tcm_parent;\n\tq = p = NULL;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (clid != TC_H_INGRESS) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue_create(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\n\t\t/* It may be default qdisc, ignore it */\n\t\tif (q && q->handle == 0)\n\t\t\tq = NULL;\n\n\t\tif (!q || !tcm->tcm_handle || q->handle != tcm->tcm_handle) {\n\t\t\tif (tcm->tcm_handle) {\n\t\t\t\tif (q && !(n->nlmsg_flags & NLM_F_REPLACE))\n\t\t\t\t\treturn -EEXIST;\n\t\t\t\tif (TC_H_MIN(tcm->tcm_handle))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\t\t\tif (!q)\n\t\t\t\t\tgoto create_n_graft;\n\t\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\t\treturn -EEXIST;\n\t\t\t\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tif (q == p ||\n\t\t\t\t    (p && check_loop(q, p, 0)))\n\t\t\t\t\treturn -ELOOP;\n\t\t\t\tatomic_inc(&q->refcnt);\n\t\t\t\tgoto graft;\n\t\t\t} else {\n\t\t\t\tif (!q)\n\t\t\t\t\tgoto create_n_graft;\n\n\t\t\t\t/* This magic test requires explanation.\n\t\t\t\t *\n\t\t\t\t *   We know, that some child q is already\n\t\t\t\t *   attached to this parent and have choice:\n\t\t\t\t *   either to change it or to create/graft new one.\n\t\t\t\t *\n\t\t\t\t *   1. We are allowed to create/graft only\n\t\t\t\t *   if CREATE and REPLACE flags are set.\n\t\t\t\t *\n\t\t\t\t *   2. If EXCL is set, requestor wanted to say,\n\t\t\t\t *   that qdisc tcm_handle is not expected\n\t\t\t\t *   to exist, so that we choose create/graft too.\n\t\t\t\t *\n\t\t\t\t *   3. The last case is when no flags are set.\n\t\t\t\t *   Alas, it is sort of hole in API, we\n\t\t\t\t *   cannot decide what to do unambiguously.\n\t\t\t\t *   For now we select create/graft, if\n\t\t\t\t *   user gave KIND, which does not match existing.\n\t\t\t\t */\n\t\t\t\tif ((n->nlmsg_flags & NLM_F_CREATE) &&\n\t\t\t\t    (n->nlmsg_flags & NLM_F_REPLACE) &&\n\t\t\t\t    ((n->nlmsg_flags & NLM_F_EXCL) ||\n\t\t\t\t     (tca[TCA_KIND] &&\n\t\t\t\t      nla_strcmp(tca[TCA_KIND], q->ops->id))))\n\t\t\t\t\tgoto create_n_graft;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (!tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t}\n\n\t/* Change qdisc parameters */\n\tif (q == NULL)\n\t\treturn -ENOENT;\n\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\treturn -EEXIST;\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\terr = qdisc_change(q, tca);\n\tif (err == 0)\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\treturn err;\n\ncreate_n_graft:\n\tif (!(n->nlmsg_flags & NLM_F_CREATE))\n\t\treturn -ENOENT;\n\tif (clid == TC_H_INGRESS) {\n\t\tif (dev_ingress_queue(dev))\n\t\t\tq = qdisc_create(dev, dev_ingress_queue(dev), p,\n\t\t\t\t\t tcm->tcm_parent, tcm->tcm_parent,\n\t\t\t\t\t tca, &err);\n\t\telse\n\t\t\terr = -ENOENT;\n\t} else {\n\t\tstruct netdev_queue *dev_queue;\n\n\t\tif (p && p->ops->cl_ops && p->ops->cl_ops->select_queue)\n\t\t\tdev_queue = p->ops->cl_ops->select_queue(p, tcm);\n\t\telse if (p)\n\t\t\tdev_queue = p->dev_queue;\n\t\telse\n\t\t\tdev_queue = netdev_get_tx_queue(dev, 0);\n\n\t\tq = qdisc_create(dev, dev_queue, p,\n\t\t\t\t tcm->tcm_parent, tcm->tcm_handle,\n\t\t\t\t tca, &err);\n\t}\n\tif (q == NULL) {\n\t\tif (err == -EAGAIN)\n\t\t\tgoto replay;\n\t\treturn err;\n\t}\n\ngraft:\n\terr = qdisc_graft(dev, p, skb, n, clid, q, NULL);\n\tif (err) {\n\t\tif (q)\n\t\t\tqdisc_destroy(q);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(skb, net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 442,
            "cve_id": "CVE-2014-0181",
            "code_snippet": "static int rtnl_setlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct ifinfomsg *ifm;\n\tstruct net_device *dev;\n\tint err;\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tchar ifname[IFNAMSIZ];\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\terr = -EINVAL;\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse if (tb[IFLA_IFNAME])\n\t\tdev = __dev_get_by_name(net, ifname);\n\telse\n\t\tgoto errout;\n\n\tif (dev == NULL) {\n\t\terr = -ENODEV;\n\t\tgoto errout;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = do_setlink(dev, ifm, tb, ifname, 0);\nerrout:\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}",
                        "code_after_change": "static int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(skb, net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int rtnl_setlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct ifinfomsg *ifm;\n\tstruct net_device *dev;\n\tint err;\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tchar ifname[IFNAMSIZ];\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\terr = -EINVAL;\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse if (tb[IFLA_IFNAME])\n\t\tdev = __dev_get_by_name(net, ifname);\n\telse\n\t\tgoto errout;\n\n\tif (dev == NULL) {\n\t\terr = -ENODEV;\n\t\tgoto errout;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = do_setlink(dev, ifm, tb, ifname, 0);\nerrout:\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(skb, net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}",
                        "code_after_change": "static int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int rtnl_setlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct ifinfomsg *ifm;\n\tstruct net_device *dev;\n\tint err;\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tchar ifname[IFNAMSIZ];\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\terr = -EINVAL;\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse if (tb[IFLA_IFNAME])\n\t\tdev = __dev_get_by_name(net, ifname);\n\telse\n\t\tgoto errout;\n\n\tif (dev == NULL) {\n\t\terr = -ENODEV;\n\t\tgoto errout;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = do_setlink(dev, ifm, tb, ifname, 0);\nerrout:\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}",
                        "code_after_change": "static int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int rtnl_setlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct ifinfomsg *ifm;\n\tstruct net_device *dev;\n\tint err;\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tchar ifname[IFNAMSIZ];\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\terr = -EINVAL;\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse if (tb[IFLA_IFNAME])\n\t\tdev = __dev_get_by_name(net, ifname);\n\telse\n\t\tgoto errout;\n\n\tif (dev == NULL) {\n\t\terr = -ENODEV;\n\t\tgoto errout;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = do_setlink(dev, ifm, tb, ifname, 0);\nerrout:\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 447,
            "cve_id": "CVE-2014-0181",
            "code_snippet": "static int dn_fib_rtm_delroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 0);\n\tif (!tb)\n\t\treturn -ESRCH;\n\n\treturn tb->delete(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
                        "code_after_change": "static int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int dn_fib_rtm_delroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 0);\n\tif (!tb)\n\t\treturn -ESRCH;\n\n\treturn tb->delete(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}",
                        "code_after_change": "static int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int dn_fib_rtm_delroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 0);\n\tif (!tb)\n\t\treturn -ESRCH;\n\n\treturn tb->delete(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}",
                        "code_after_change": "static int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int dn_fib_rtm_delroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 0);\n\tif (!tb)\n\t\treturn -ESRCH;\n\n\treturn tb->delete(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 455,
            "cve_id": "CVE-2014-0181",
            "code_snippet": "static int tc_ctl_tfilter(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tspinlock_t *root_lock;\n\tstruct tcmsg *t;\n\tu32 protocol;\n\tu32 prio;\n\tu32 nprio;\n\tu32 parent;\n\tstruct net_device *dev;\n\tstruct Qdisc  *q;\n\tstruct tcf_proto **back, **chain;\n\tstruct tcf_proto *tp;\n\tconst struct tcf_proto_ops *tp_ops;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl;\n\tunsigned long fh;\n\tint err;\n\tint tp_created = 0;\n\n\tif ((n->nlmsg_type != RTM_GETTFILTER) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\terr = nlmsg_parse(n, sizeof(*t), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tt = nlmsg_data(n);\n\tprotocol = TC_H_MIN(t->tcm_info);\n\tprio = TC_H_MAJ(t->tcm_info);\n\tnprio = prio;\n\tparent = t->tcm_parent;\n\tcl = 0;\n\n\tif (prio == 0) {\n\t\t/* If no priority is given, user wants we allocated it. */\n\t\tif (n->nlmsg_type != RTM_NEWTFILTER ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\treturn -ENOENT;\n\t\tprio = TC_H_MAKE(0x80000000U, 0U);\n\t}\n\n\t/* Find head of filter chain. */\n\n\t/* Find link */\n\tdev = __dev_get_by_index(net, t->tcm_ifindex);\n\tif (dev == NULL)\n\t\treturn -ENODEV;\n\n\t/* Find qdisc */\n\tif (!parent) {\n\t\tq = dev->qdisc;\n\t\tparent = q->handle;\n\t} else {\n\t\tq = qdisc_lookup(dev, TC_H_MAJ(t->tcm_parent));\n\t\tif (q == NULL)\n\t\t\treturn -EINVAL;\n\t}\n\n\t/* Is it classful? */\n\tcops = q->ops->cl_ops;\n\tif (!cops)\n\t\treturn -EINVAL;\n\n\tif (cops->tcf_chain == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\t/* Do we search for filter, attached to class? */\n\tif (TC_H_MIN(parent)) {\n\t\tcl = cops->get(q, parent);\n\t\tif (cl == 0)\n\t\t\treturn -ENOENT;\n\t}\n\n\t/* And the last stroke */\n\tchain = cops->tcf_chain(q, cl);\n\terr = -EINVAL;\n\tif (chain == NULL)\n\t\tgoto errout;\n\n\t/* Check the chain for existence of proto-tcf with this priority */\n\tfor (back = chain; (tp = *back) != NULL; back = &tp->next) {\n\t\tif (tp->prio >= prio) {\n\t\t\tif (tp->prio == prio) {\n\t\t\t\tif (!nprio ||\n\t\t\t\t    (tp->protocol != protocol && protocol))\n\t\t\t\t\tgoto errout;\n\t\t\t} else\n\t\t\t\ttp = NULL;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\troot_lock = qdisc_root_sleeping_lock(q);\n\n\tif (tp == NULL) {\n\t\t/* Proto-tcf does not exist, create new one */\n\n\t\tif (tca[TCA_KIND] == NULL || !protocol)\n\t\t\tgoto errout;\n\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTFILTER ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto errout;\n\n\n\t\t/* Create new proto tcf */\n\n\t\terr = -ENOBUFS;\n\t\ttp = kzalloc(sizeof(*tp), GFP_KERNEL);\n\t\tif (tp == NULL)\n\t\t\tgoto errout;\n\t\terr = -ENOENT;\n\t\ttp_ops = tcf_proto_lookup_ops(tca[TCA_KIND]);\n\t\tif (tp_ops == NULL) {\n#ifdef CONFIG_MODULES\n\t\t\tstruct nlattr *kind = tca[TCA_KIND];\n\t\t\tchar name[IFNAMSIZ];\n\n\t\t\tif (kind != NULL &&\n\t\t\t    nla_strlcpy(name, kind, IFNAMSIZ) < IFNAMSIZ) {\n\t\t\t\trtnl_unlock();\n\t\t\t\trequest_module(\"cls_%s\", name);\n\t\t\t\trtnl_lock();\n\t\t\t\ttp_ops = tcf_proto_lookup_ops(kind);\n\t\t\t\t/* We dropped the RTNL semaphore in order to\n\t\t\t\t * perform the module load.  So, even if we\n\t\t\t\t * succeeded in loading the module we have to\n\t\t\t\t * replay the request.  We indicate this using\n\t\t\t\t * -EAGAIN.\n\t\t\t\t */\n\t\t\t\tif (tp_ops != NULL) {\n\t\t\t\t\tmodule_put(tp_ops->owner);\n\t\t\t\t\terr = -EAGAIN;\n\t\t\t\t}\n\t\t\t}\n#endif\n\t\t\tkfree(tp);\n\t\t\tgoto errout;\n\t\t}\n\t\ttp->ops = tp_ops;\n\t\ttp->protocol = protocol;\n\t\ttp->prio = nprio ? : TC_H_MAJ(tcf_auto_prio(*back));\n\t\ttp->q = q;\n\t\ttp->classify = tp_ops->classify;\n\t\ttp->classid = parent;\n\n\t\terr = tp_ops->init(tp);\n\t\tif (err != 0) {\n\t\t\tmodule_put(tp_ops->owner);\n\t\t\tkfree(tp);\n\t\t\tgoto errout;\n\t\t}\n\n\t\ttp_created = 1;\n\n\t} else if (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], tp->ops->kind))\n\t\tgoto errout;\n\n\tfh = tp->ops->get(tp, t->tcm_handle);\n\n\tif (fh == 0) {\n\t\tif (n->nlmsg_type == RTM_DELTFILTER && t->tcm_handle == 0) {\n\t\t\tspin_lock_bh(root_lock);\n\t\t\t*back = tp->next;\n\t\t\tspin_unlock_bh(root_lock);\n\n\t\t\ttfilter_notify(net, skb, n, tp, fh, RTM_DELTFILTER);\n\t\t\ttcf_destroy(tp);\n\t\t\terr = 0;\n\t\t\tgoto errout;\n\t\t}\n\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTFILTER ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto errout;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTFILTER:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL) {\n\t\t\t\tif (tp_created)\n\t\t\t\t\ttcf_destroy(tp);\n\t\t\t\tgoto errout;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase RTM_DELTFILTER:\n\t\t\terr = tp->ops->delete(tp, fh);\n\t\t\tif (err == 0)\n\t\t\t\ttfilter_notify(net, skb, n, tp, fh, RTM_DELTFILTER);\n\t\t\tgoto errout;\n\t\tcase RTM_GETTFILTER:\n\t\t\terr = tfilter_notify(net, skb, n, tp, fh, RTM_NEWTFILTER);\n\t\t\tgoto errout;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout;\n\t\t}\n\t}\n\n\terr = tp->ops->change(net, skb, tp, cl, t->tcm_handle, tca, &fh);\n\tif (err == 0) {\n\t\tif (tp_created) {\n\t\t\tspin_lock_bh(root_lock);\n\t\t\ttp->next = *back;\n\t\t\t*back = tp;\n\t\t\tspin_unlock_bh(root_lock);\n\t\t}\n\t\ttfilter_notify(net, skb, n, tp, fh, RTM_NEWTFILTER);\n\t} else {\n\t\tif (tp_created)\n\t\t\ttcf_destroy(tp);\n\t}\n\nerrout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\tif (err == -EAGAIN)\n\t\t/* Replay the request. */\n\t\tgoto replay;\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}",
                        "code_after_change": "static int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int tc_ctl_tfilter(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tspinlock_t *root_lock;\n\tstruct tcmsg *t;\n\tu32 protocol;\n\tu32 prio;\n\tu32 nprio;\n\tu32 parent;\n\tstruct net_device *dev;\n\tstruct Qdisc  *q;\n\tstruct tcf_proto **back, **chain;\n\tstruct tcf_proto *tp;\n\tconst struct tcf_proto_ops *tp_ops;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl;\n\tunsigned long fh;\n\tint err;\n\tint tp_created = 0;\n\n\tif ((n->nlmsg_type != RTM_GETTFILTER) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\terr = nlmsg_parse(n, sizeof(*t), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tt = nlmsg_data(n);\n\tprotocol = TC_H_MIN(t->tcm_info);\n\tprio = TC_H_MAJ(t->tcm_info);\n\tnprio = prio;\n\tparent = t->tcm_parent;\n\tcl = 0;\n\n\tif (prio == 0) {\n\t\t/* If no priority is given, user wants we allocated it. */\n\t\tif (n->nlmsg_type != RTM_NEWTFILTER ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\treturn -ENOENT;\n\t\tprio = TC_H_MAKE(0x80000000U, 0U);\n\t}\n\n\t/* Find head of filter chain. */\n\n\t/* Find link */\n\tdev = __dev_get_by_index(net, t->tcm_ifindex);\n\tif (dev == NULL)\n\t\treturn -ENODEV;\n\n\t/* Find qdisc */\n\tif (!parent) {\n\t\tq = dev->qdisc;\n\t\tparent = q->handle;\n\t} else {\n\t\tq = qdisc_lookup(dev, TC_H_MAJ(t->tcm_parent));\n\t\tif (q == NULL)\n\t\t\treturn -EINVAL;\n\t}\n\n\t/* Is it classful? */\n\tcops = q->ops->cl_ops;\n\tif (!cops)\n\t\treturn -EINVAL;\n\n\tif (cops->tcf_chain == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\t/* Do we search for filter, attached to class? */\n\tif (TC_H_MIN(parent)) {\n\t\tcl = cops->get(q, parent);\n\t\tif (cl == 0)\n\t\t\treturn -ENOENT;\n\t}\n\n\t/* And the last stroke */\n\tchain = cops->tcf_chain(q, cl);\n\terr = -EINVAL;\n\tif (chain == NULL)\n\t\tgoto errout;\n\n\t/* Check the chain for existence of proto-tcf with this priority */\n\tfor (back = chain; (tp = *back) != NULL; back = &tp->next) {\n\t\tif (tp->prio >= prio) {\n\t\t\tif (tp->prio == prio) {\n\t\t\t\tif (!nprio ||\n\t\t\t\t    (tp->protocol != protocol && protocol))\n\t\t\t\t\tgoto errout;\n\t\t\t} else\n\t\t\t\ttp = NULL;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\troot_lock = qdisc_root_sleeping_lock(q);\n\n\tif (tp == NULL) {\n\t\t/* Proto-tcf does not exist, create new one */\n\n\t\tif (tca[TCA_KIND] == NULL || !protocol)\n\t\t\tgoto errout;\n\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTFILTER ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto errout;\n\n\n\t\t/* Create new proto tcf */\n\n\t\terr = -ENOBUFS;\n\t\ttp = kzalloc(sizeof(*tp), GFP_KERNEL);\n\t\tif (tp == NULL)\n\t\t\tgoto errout;\n\t\terr = -ENOENT;\n\t\ttp_ops = tcf_proto_lookup_ops(tca[TCA_KIND]);\n\t\tif (tp_ops == NULL) {\n#ifdef CONFIG_MODULES\n\t\t\tstruct nlattr *kind = tca[TCA_KIND];\n\t\t\tchar name[IFNAMSIZ];\n\n\t\t\tif (kind != NULL &&\n\t\t\t    nla_strlcpy(name, kind, IFNAMSIZ) < IFNAMSIZ) {\n\t\t\t\trtnl_unlock();\n\t\t\t\trequest_module(\"cls_%s\", name);\n\t\t\t\trtnl_lock();\n\t\t\t\ttp_ops = tcf_proto_lookup_ops(kind);\n\t\t\t\t/* We dropped the RTNL semaphore in order to\n\t\t\t\t * perform the module load.  So, even if we\n\t\t\t\t * succeeded in loading the module we have to\n\t\t\t\t * replay the request.  We indicate this using\n\t\t\t\t * -EAGAIN.\n\t\t\t\t */\n\t\t\t\tif (tp_ops != NULL) {\n\t\t\t\t\tmodule_put(tp_ops->owner);\n\t\t\t\t\terr = -EAGAIN;\n\t\t\t\t}\n\t\t\t}\n#endif\n\t\t\tkfree(tp);\n\t\t\tgoto errout;\n\t\t}\n\t\ttp->ops = tp_ops;\n\t\ttp->protocol = protocol;\n\t\ttp->prio = nprio ? : TC_H_MAJ(tcf_auto_prio(*back));\n\t\ttp->q = q;\n\t\ttp->classify = tp_ops->classify;\n\t\ttp->classid = parent;\n\n\t\terr = tp_ops->init(tp);\n\t\tif (err != 0) {\n\t\t\tmodule_put(tp_ops->owner);\n\t\t\tkfree(tp);\n\t\t\tgoto errout;\n\t\t}\n\n\t\ttp_created = 1;\n\n\t} else if (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], tp->ops->kind))\n\t\tgoto errout;\n\n\tfh = tp->ops->get(tp, t->tcm_handle);\n\n\tif (fh == 0) {\n\t\tif (n->nlmsg_type == RTM_DELTFILTER && t->tcm_handle == 0) {\n\t\t\tspin_lock_bh(root_lock);\n\t\t\t*back = tp->next;\n\t\t\tspin_unlock_bh(root_lock);\n\n\t\t\ttfilter_notify(net, skb, n, tp, fh, RTM_DELTFILTER);\n\t\t\ttcf_destroy(tp);\n\t\t\terr = 0;\n\t\t\tgoto errout;\n\t\t}\n\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTFILTER ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto errout;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTFILTER:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL) {\n\t\t\t\tif (tp_created)\n\t\t\t\t\ttcf_destroy(tp);\n\t\t\t\tgoto errout;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase RTM_DELTFILTER:\n\t\t\terr = tp->ops->delete(tp, fh);\n\t\t\tif (err == 0)\n\t\t\t\ttfilter_notify(net, skb, n, tp, fh, RTM_DELTFILTER);\n\t\t\tgoto errout;\n\t\tcase RTM_GETTFILTER:\n\t\t\terr = tfilter_notify(net, skb, n, tp, fh, RTM_NEWTFILTER);\n\t\t\tgoto errout;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout;\n\t\t}\n\t}\n\n\terr = tp->ops->change(net, skb, tp, cl, t->tcm_handle, tca, &fh);\n\tif (err == 0) {\n\t\tif (tp_created) {\n\t\t\tspin_lock_bh(root_lock);\n\t\t\ttp->next = *back;\n\t\t\t*back = tp;\n\t\t\tspin_unlock_bh(root_lock);\n\t\t}\n\t\ttfilter_notify(net, skb, n, tp, fh, RTM_NEWTFILTER);\n\t} else {\n\t\tif (tp_created)\n\t\t\ttcf_destroy(tp);\n\t}\n\nerrout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\tif (err == -EAGAIN)\n\t\t/* Replay the request. */\n\t\tgoto replay;\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "YES"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 1
        },
        {
            "id": 438,
            "cve_id": "CVE-2014-0181",
            "code_snippet": "static int cgw_remove_job(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct cgw_job *gwj = NULL;\n\tstruct hlist_node *nx;\n\tstruct rtcanmsg *r;\n\tstruct cf_mod mod;\n\tstruct can_can_gw ccgw;\n\tu8 limhops = 0;\n\tint err = 0;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (nlmsg_len(nlh) < sizeof(*r))\n\t\treturn -EINVAL;\n\n\tr = nlmsg_data(nlh);\n\tif (r->can_family != AF_CAN)\n\t\treturn -EPFNOSUPPORT;\n\n\t/* so far we only support CAN -> CAN routings */\n\tif (r->gwtype != CGW_TYPE_CAN_CAN)\n\t\treturn -EINVAL;\n\n\terr = cgw_parse_attr(nlh, &mod, CGW_TYPE_CAN_CAN, &ccgw, &limhops);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* two interface indices both set to 0 => remove all entries */\n\tif (!ccgw.src_idx && !ccgw.dst_idx) {\n\t\tcgw_remove_all_jobs();\n\t\treturn 0;\n\t}\n\n\terr = -EINVAL;\n\n\tASSERT_RTNL();\n\n\t/* remove only the first matching entry */\n\thlist_for_each_entry_safe(gwj, nx, &cgw_list, list) {\n\n\t\tif (gwj->flags != r->flags)\n\t\t\tcontinue;\n\n\t\tif (gwj->limit_hops != limhops)\n\t\t\tcontinue;\n\n\t\tif (memcmp(&gwj->mod, &mod, sizeof(mod)))\n\t\t\tcontinue;\n\n\t\t/* if (r->gwtype == CGW_TYPE_CAN_CAN) - is made sure here */\n\t\tif (memcmp(&gwj->ccgw, &ccgw, sizeof(ccgw)))\n\t\t\tcontinue;\n\n\t\thlist_del(&gwj->list);\n\t\tcgw_unregister_filter(gwj);\n\t\tkmem_cache_free(cgw_cache, gwj);\n\t\terr = 0;\n\t\tbreak;\n\t}\n\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
                        "code_after_change": "static int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int cgw_remove_job(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct cgw_job *gwj = NULL;\n\tstruct hlist_node *nx;\n\tstruct rtcanmsg *r;\n\tstruct cf_mod mod;\n\tstruct can_can_gw ccgw;\n\tu8 limhops = 0;\n\tint err = 0;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (nlmsg_len(nlh) < sizeof(*r))\n\t\treturn -EINVAL;\n\n\tr = nlmsg_data(nlh);\n\tif (r->can_family != AF_CAN)\n\t\treturn -EPFNOSUPPORT;\n\n\t/* so far we only support CAN -> CAN routings */\n\tif (r->gwtype != CGW_TYPE_CAN_CAN)\n\t\treturn -EINVAL;\n\n\terr = cgw_parse_attr(nlh, &mod, CGW_TYPE_CAN_CAN, &ccgw, &limhops);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* two interface indices both set to 0 => remove all entries */\n\tif (!ccgw.src_idx && !ccgw.dst_idx) {\n\t\tcgw_remove_all_jobs();\n\t\treturn 0;\n\t}\n\n\terr = -EINVAL;\n\n\tASSERT_RTNL();\n\n\t/* remove only the first matching entry */\n\thlist_for_each_entry_safe(gwj, nx, &cgw_list, list) {\n\n\t\tif (gwj->flags != r->flags)\n\t\t\tcontinue;\n\n\t\tif (gwj->limit_hops != limhops)\n\t\t\tcontinue;\n\n\t\tif (memcmp(&gwj->mod, &mod, sizeof(mod)))\n\t\t\tcontinue;\n\n\t\t/* if (r->gwtype == CGW_TYPE_CAN_CAN) - is made sure here */\n\t\tif (memcmp(&gwj->ccgw, &ccgw, sizeof(ccgw)))\n\t\t\tcontinue;\n\n\t\thlist_del(&gwj->list);\n\t\tcgw_unregister_filter(gwj);\n\t\tkmem_cache_free(cgw_cache, gwj);\n\t\terr = 0;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}",
                        "code_after_change": "static int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int cgw_remove_job(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct cgw_job *gwj = NULL;\n\tstruct hlist_node *nx;\n\tstruct rtcanmsg *r;\n\tstruct cf_mod mod;\n\tstruct can_can_gw ccgw;\n\tu8 limhops = 0;\n\tint err = 0;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (nlmsg_len(nlh) < sizeof(*r))\n\t\treturn -EINVAL;\n\n\tr = nlmsg_data(nlh);\n\tif (r->can_family != AF_CAN)\n\t\treturn -EPFNOSUPPORT;\n\n\t/* so far we only support CAN -> CAN routings */\n\tif (r->gwtype != CGW_TYPE_CAN_CAN)\n\t\treturn -EINVAL;\n\n\terr = cgw_parse_attr(nlh, &mod, CGW_TYPE_CAN_CAN, &ccgw, &limhops);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* two interface indices both set to 0 => remove all entries */\n\tif (!ccgw.src_idx && !ccgw.dst_idx) {\n\t\tcgw_remove_all_jobs();\n\t\treturn 0;\n\t}\n\n\terr = -EINVAL;\n\n\tASSERT_RTNL();\n\n\t/* remove only the first matching entry */\n\thlist_for_each_entry_safe(gwj, nx, &cgw_list, list) {\n\n\t\tif (gwj->flags != r->flags)\n\t\t\tcontinue;\n\n\t\tif (gwj->limit_hops != limhops)\n\t\t\tcontinue;\n\n\t\tif (memcmp(&gwj->mod, &mod, sizeof(mod)))\n\t\t\tcontinue;\n\n\t\t/* if (r->gwtype == CGW_TYPE_CAN_CAN) - is made sure here */\n\t\tif (memcmp(&gwj->ccgw, &ccgw, sizeof(ccgw)))\n\t\t\tcontinue;\n\n\t\thlist_del(&gwj->list);\n\t\tcgw_unregister_filter(gwj);\n\t\tkmem_cache_free(cgw_cache, gwj);\n\t\terr = 0;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void\nscsi_nl_rcv_msg(struct sk_buff *skb)\n{\n\tstruct nlmsghdr *nlh;\n\tstruct scsi_nl_hdr *hdr;\n\tu32 rlen;\n\tint err, tport;\n\n\twhile (skb->len >= NLMSG_HDRLEN) {\n\t\terr = 0;\n\n\t\tnlh = nlmsg_hdr(skb);\n\t\tif ((nlh->nlmsg_len < (sizeof(*nlh) + sizeof(*hdr))) ||\n\t\t    (skb->len < nlh->nlmsg_len)) {\n\t\t\tprintk(KERN_WARNING \"%s: discarding partial skb\\n\",\n\t\t\t\t __func__);\n\t\t\treturn;\n\t\t}\n\n\t\trlen = NLMSG_ALIGN(nlh->nlmsg_len);\n\t\tif (rlen > skb->len)\n\t\t\trlen = skb->len;\n\n\t\tif (nlh->nlmsg_type != SCSI_TRANSPORT_MSG) {\n\t\t\terr = -EBADMSG;\n\t\t\tgoto next_msg;\n\t\t}\n\n\t\thdr = nlmsg_data(nlh);\n\t\tif ((hdr->version != SCSI_NL_VERSION) ||\n\t\t    (hdr->magic != SCSI_NL_MAGIC)) {\n\t\t\terr = -EPROTOTYPE;\n\t\t\tgoto next_msg;\n\t\t}\n\n\t\tif (!capable(CAP_SYS_ADMIN)) {\n\t\t\terr = -EPERM;\n\t\t\tgoto next_msg;\n\t\t}\n\n\t\tif (nlh->nlmsg_len < (sizeof(*nlh) + hdr->msglen)) {\n\t\t\tprintk(KERN_WARNING \"%s: discarding partial message\\n\",\n\t\t\t\t __func__);\n\t\t\tgoto next_msg;\n\t\t}\n\n\t\t/*\n\t\t * Deliver message to the appropriate transport\n\t\t */\n\t\ttport = hdr->transport;\n\t\tif (tport == SCSI_NL_TRANSPORT) {\n\t\t\tswitch (hdr->msgtype) {\n\t\t\tcase SCSI_NL_SHOST_VENDOR:\n\t\t\t\t/* Locate the driver that corresponds to the message */\n\t\t\t\terr = -ESRCH;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\terr = -EBADR;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (err)\n\t\t\t\tprintk(KERN_WARNING \"%s: Msgtype %d failed - err %d\\n\",\n\t\t\t\t       __func__, hdr->msgtype, err);\n\t\t}\n\t\telse\n\t\t\terr = -ENOENT;\n\nnext_msg:\n\t\tif ((err) || (nlh->nlmsg_flags & NLM_F_ACK))\n\t\t\tnetlink_ack(skb, nlh, err);\n\n\t\tskb_pull(skb, rlen);\n\t}\n}",
                        "code_after_change": "static int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int cgw_remove_job(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct cgw_job *gwj = NULL;\n\tstruct hlist_node *nx;\n\tstruct rtcanmsg *r;\n\tstruct cf_mod mod;\n\tstruct can_can_gw ccgw;\n\tu8 limhops = 0;\n\tint err = 0;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (nlmsg_len(nlh) < sizeof(*r))\n\t\treturn -EINVAL;\n\n\tr = nlmsg_data(nlh);\n\tif (r->can_family != AF_CAN)\n\t\treturn -EPFNOSUPPORT;\n\n\t/* so far we only support CAN -> CAN routings */\n\tif (r->gwtype != CGW_TYPE_CAN_CAN)\n\t\treturn -EINVAL;\n\n\terr = cgw_parse_attr(nlh, &mod, CGW_TYPE_CAN_CAN, &ccgw, &limhops);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* two interface indices both set to 0 => remove all entries */\n\tif (!ccgw.src_idx && !ccgw.dst_idx) {\n\t\tcgw_remove_all_jobs();\n\t\treturn 0;\n\t}\n\n\terr = -EINVAL;\n\n\tASSERT_RTNL();\n\n\t/* remove only the first matching entry */\n\thlist_for_each_entry_safe(gwj, nx, &cgw_list, list) {\n\n\t\tif (gwj->flags != r->flags)\n\t\t\tcontinue;\n\n\t\tif (gwj->limit_hops != limhops)\n\t\t\tcontinue;\n\n\t\tif (memcmp(&gwj->mod, &mod, sizeof(mod)))\n\t\t\tcontinue;\n\n\t\t/* if (r->gwtype == CGW_TYPE_CAN_CAN) - is made sure here */\n\t\tif (memcmp(&gwj->ccgw, &ccgw, sizeof(ccgw)))\n\t\t\tcontinue;\n\n\t\thlist_del(&gwj->list);\n\t\tcgw_unregister_filter(gwj);\n\t\tkmem_cache_free(cgw_cache, gwj);\n\t\terr = 0;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic void\nscsi_nl_rcv_msg(struct sk_buff *skb)\n{\n\tstruct nlmsghdr *nlh;\n\tstruct scsi_nl_hdr *hdr;\n\tu32 rlen;\n\tint err, tport;\n\n\twhile (skb->len >= NLMSG_HDRLEN) {\n\t\terr = 0;\n\n\t\tnlh = nlmsg_hdr(skb);\n\t\tif ((nlh->nlmsg_len < (sizeof(*nlh) + sizeof(*hdr))) ||\n\t\t    (skb->len < nlh->nlmsg_len)) {\n\t\t\tprintk(KERN_WARNING \"%s: discarding partial skb\\n\",\n\t\t\t\t __func__);\n\t\t\treturn;\n\t\t}\n\n\t\trlen = NLMSG_ALIGN(nlh->nlmsg_len);\n\t\tif (rlen > skb->len)\n\t\t\trlen = skb->len;\n\n\t\tif (nlh->nlmsg_type != SCSI_TRANSPORT_MSG) {\n\t\t\terr = -EBADMSG;\n\t\t\tgoto next_msg;\n\t\t}\n\n\t\thdr = nlmsg_data(nlh);\n\t\tif ((hdr->version != SCSI_NL_VERSION) ||\n\t\t    (hdr->magic != SCSI_NL_MAGIC)) {\n\t\t\terr = -EPROTOTYPE;\n\t\t\tgoto next_msg;\n\t\t}\n\n\t\tif (!capable(CAP_SYS_ADMIN)) {\n\t\t\terr = -EPERM;\n\t\t\tgoto next_msg;\n\t\t}\n\n\t\tif (nlh->nlmsg_len < (sizeof(*nlh) + hdr->msglen)) {\n\t\t\tprintk(KERN_WARNING \"%s: discarding partial message\\n\",\n\t\t\t\t __func__);\n\t\t\tgoto next_msg;\n\t\t}\n\n\t\t/*\n\t\t * Deliver message to the appropriate transport\n\t\t */\n\t\ttport = hdr->transport;\n\t\tif (tport == SCSI_NL_TRANSPORT) {\n\t\t\tswitch (hdr->msgtype) {\n\t\t\tcase SCSI_NL_SHOST_VENDOR:\n\t\t\t\t/* Locate the driver that corresponds to the message */\n\t\t\t\terr = -ESRCH;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\terr = -EBADR;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (err)\n\t\t\t\tprintk(KERN_WARNING \"%s: Msgtype %d failed - err %d\\n\",\n\t\t\t\t       __func__, hdr->msgtype, err);\n\t\t}\n\t\telse\n\t\t\terr = -ENOENT;\n\nnext_msg:\n\t\tif ((err) || (nlh->nlmsg_flags & NLM_F_ACK))\n\t\t\tnetlink_ack(skb, nlh, err);\n\n\t\tskb_pull(skb, rlen);\n\t}\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 1089,
            "cve_id": "CVE-2016-6786",
            "code_snippet": "static long perf_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct perf_event *event = file->private_data;\n\tvoid (*func)(struct perf_event *);\n\tu32 flags = arg;\n\n\tswitch (cmd) {\n\tcase PERF_EVENT_IOC_ENABLE:\n\t\tfunc = perf_event_enable;\n\t\tbreak;\n\tcase PERF_EVENT_IOC_DISABLE:\n\t\tfunc = perf_event_disable;\n\t\tbreak;\n\tcase PERF_EVENT_IOC_RESET:\n\t\tfunc = perf_event_reset;\n\t\tbreak;\n\n\tcase PERF_EVENT_IOC_REFRESH:\n\t\treturn perf_event_refresh(event, arg);\n\n\tcase PERF_EVENT_IOC_PERIOD:\n\t\treturn perf_event_period(event, (u64 __user *)arg);\n\n\tcase PERF_EVENT_IOC_ID:\n\t{\n\t\tu64 id = primary_event_id(event);\n\n\t\tif (copy_to_user((void __user *)arg, &id, sizeof(id)))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\n\tcase PERF_EVENT_IOC_SET_OUTPUT:\n\t{\n\t\tint ret;\n\t\tif (arg != -1) {\n\t\t\tstruct perf_event *output_event;\n\t\t\tstruct fd output;\n\t\t\tret = perf_fget_light(arg, &output);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\toutput_event = output.file->private_data;\n\t\t\tret = perf_event_set_output(event, output_event);\n\t\t\tfdput(output);\n\t\t} else {\n\t\t\tret = perf_event_set_output(event, NULL);\n\t\t}\n\t\treturn ret;\n\t}\n\n\tcase PERF_EVENT_IOC_SET_FILTER:\n\t\treturn perf_event_set_filter(event, (void __user *)arg);\n\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (flags & PERF_IOC_FLAG_GROUP)\n\t\tperf_event_for_each(event, func);\n\telse\n\t\tperf_event_for_each_child(event, func);\n\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tion_free(client, handle);\n\t\tion_handle_put(handle);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}",
                        "code_after_change": "static long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\tmutex_lock(&client->lock);\n\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n\t\tif (IS_ERR(handle)) {\n\t\t\tmutex_unlock(&client->lock);\n\t\t\treturn PTR_ERR(handle);\n\t\t}\n\t\tion_free_nolock(client, handle);\n\t\tion_handle_put_nolock(handle);\n\t\tmutex_unlock(&client->lock);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}",
                        "cve_id": "CVE-2016-9120"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic long perf_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct perf_event *event = file->private_data;\n\tvoid (*func)(struct perf_event *);\n\tu32 flags = arg;\n\n\tswitch (cmd) {\n\tcase PERF_EVENT_IOC_ENABLE:\n\t\tfunc = perf_event_enable;\n\t\tbreak;\n\tcase PERF_EVENT_IOC_DISABLE:\n\t\tfunc = perf_event_disable;\n\t\tbreak;\n\tcase PERF_EVENT_IOC_RESET:\n\t\tfunc = perf_event_reset;\n\t\tbreak;\n\n\tcase PERF_EVENT_IOC_REFRESH:\n\t\treturn perf_event_refresh(event, arg);\n\n\tcase PERF_EVENT_IOC_PERIOD:\n\t\treturn perf_event_period(event, (u64 __user *)arg);\n\n\tcase PERF_EVENT_IOC_ID:\n\t{\n\t\tu64 id = primary_event_id(event);\n\n\t\tif (copy_to_user((void __user *)arg, &id, sizeof(id)))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\n\tcase PERF_EVENT_IOC_SET_OUTPUT:\n\t{\n\t\tint ret;\n\t\tif (arg != -1) {\n\t\t\tstruct perf_event *output_event;\n\t\t\tstruct fd output;\n\t\t\tret = perf_fget_light(arg, &output);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\toutput_event = output.file->private_data;\n\t\t\tret = perf_event_set_output(event, output_event);\n\t\t\tfdput(output);\n\t\t} else {\n\t\t\tret = perf_event_set_output(event, NULL);\n\t\t}\n\t\treturn ret;\n\t}\n\n\tcase PERF_EVENT_IOC_SET_FILTER:\n\t\treturn perf_event_set_filter(event, (void __user *)arg);\n\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (flags & PERF_IOC_FLAG_GROUP)\n\t\tperf_event_for_each(event, func);\n\telse\n\t\tperf_event_for_each_child(event, func);\n\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tion_free(client, handle);\n\t\tion_handle_put(handle);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\tmutex_lock(&client->lock);\n\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n\t\tif (IS_ERR(handle)) {\n\t\t\tmutex_unlock(&client->lock);\n\t\t\treturn PTR_ERR(handle);\n\t\t}\n\t\tion_free_nolock(client, handle);\n\t\tion_handle_put_nolock(handle);\n\t\tmutex_unlock(&client->lock);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "asmlinkage long sys_oabi_fcntl64(unsigned int fd, unsigned int cmd,\n\t\t\t\t unsigned long arg)\n{\n\tstruct oabi_flock64 user;\n\tstruct flock64 kernel;\n\tmm_segment_t fs = USER_DS; /* initialized to kill a warning */\n\tunsigned long local_arg = arg;\n\tint ret;\n\n\tswitch (cmd) {\n\tcase F_OFD_GETLK:\n\tcase F_OFD_SETLK:\n\tcase F_OFD_SETLKW:\n\tcase F_GETLK64:\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\tif (copy_from_user(&user, (struct oabi_flock64 __user *)arg,\n\t\t\t\t   sizeof(user)))\n\t\t\treturn -EFAULT;\n\t\tkernel.l_type\t= user.l_type;\n\t\tkernel.l_whence\t= user.l_whence;\n\t\tkernel.l_start\t= user.l_start;\n\t\tkernel.l_len\t= user.l_len;\n\t\tkernel.l_pid\t= user.l_pid;\n\t\tlocal_arg = (unsigned long)&kernel;\n\t\tfs = get_fs();\n\t\tset_fs(KERNEL_DS);\n\t}\n\n\tret = sys_fcntl64(fd, cmd, local_arg);\n\n\tswitch (cmd) {\n\tcase F_GETLK64:\n\t\tif (!ret) {\n\t\t\tuser.l_type\t= kernel.l_type;\n\t\t\tuser.l_whence\t= kernel.l_whence;\n\t\t\tuser.l_start\t= kernel.l_start;\n\t\t\tuser.l_len\t= kernel.l_len;\n\t\t\tuser.l_pid\t= kernel.l_pid;\n\t\t\tif (copy_to_user((struct oabi_flock64 __user *)arg,\n\t\t\t\t\t &user, sizeof(user)))\n\t\t\t\tret = -EFAULT;\n\t\t}\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\tset_fs(fs);\n\t}\n\n\treturn ret;\n}",
                        "code_after_change": "asmlinkage long sys_oabi_fcntl64(unsigned int fd, unsigned int cmd,\n\t\t\t\t unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase F_OFD_GETLK:\n\tcase F_OFD_SETLK:\n\tcase F_OFD_SETLKW:\n\tcase F_GETLK64:\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\treturn do_locks(fd, cmd, arg);\n\n\tdefault:\n\t\treturn sys_fcntl64(fd, cmd, arg);\n\t}\n}",
                        "cve_id": "CVE-2015-8966"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic long perf_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct perf_event *event = file->private_data;\n\tvoid (*func)(struct perf_event *);\n\tu32 flags = arg;\n\n\tswitch (cmd) {\n\tcase PERF_EVENT_IOC_ENABLE:\n\t\tfunc = perf_event_enable;\n\t\tbreak;\n\tcase PERF_EVENT_IOC_DISABLE:\n\t\tfunc = perf_event_disable;\n\t\tbreak;\n\tcase PERF_EVENT_IOC_RESET:\n\t\tfunc = perf_event_reset;\n\t\tbreak;\n\n\tcase PERF_EVENT_IOC_REFRESH:\n\t\treturn perf_event_refresh(event, arg);\n\n\tcase PERF_EVENT_IOC_PERIOD:\n\t\treturn perf_event_period(event, (u64 __user *)arg);\n\n\tcase PERF_EVENT_IOC_ID:\n\t{\n\t\tu64 id = primary_event_id(event);\n\n\t\tif (copy_to_user((void __user *)arg, &id, sizeof(id)))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\n\tcase PERF_EVENT_IOC_SET_OUTPUT:\n\t{\n\t\tint ret;\n\t\tif (arg != -1) {\n\t\t\tstruct perf_event *output_event;\n\t\t\tstruct fd output;\n\t\t\tret = perf_fget_light(arg, &output);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\toutput_event = output.file->private_data;\n\t\t\tret = perf_event_set_output(event, output_event);\n\t\t\tfdput(output);\n\t\t} else {\n\t\t\tret = perf_event_set_output(event, NULL);\n\t\t}\n\t\treturn ret;\n\t}\n\n\tcase PERF_EVENT_IOC_SET_FILTER:\n\t\treturn perf_event_set_filter(event, (void __user *)arg);\n\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (flags & PERF_IOC_FLAG_GROUP)\n\t\tperf_event_for_each(event, func);\n\telse\n\t\tperf_event_for_each_child(event, func);\n\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nasmlinkage long sys_oabi_fcntl64(unsigned int fd, unsigned int cmd,\n\t\t\t\t unsigned long arg)\n{\n\tstruct oabi_flock64 user;\n\tstruct flock64 kernel;\n\tmm_segment_t fs = USER_DS; /* initialized to kill a warning */\n\tunsigned long local_arg = arg;\n\tint ret;\n\n\tswitch (cmd) {\n\tcase F_OFD_GETLK:\n\tcase F_OFD_SETLK:\n\tcase F_OFD_SETLKW:\n\tcase F_GETLK64:\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\tif (copy_from_user(&user, (struct oabi_flock64 __user *)arg,\n\t\t\t\t   sizeof(user)))\n\t\t\treturn -EFAULT;\n\t\tkernel.l_type\t= user.l_type;\n\t\tkernel.l_whence\t= user.l_whence;\n\t\tkernel.l_start\t= user.l_start;\n\t\tkernel.l_len\t= user.l_len;\n\t\tkernel.l_pid\t= user.l_pid;\n\t\tlocal_arg = (unsigned long)&kernel;\n\t\tfs = get_fs();\n\t\tset_fs(KERNEL_DS);\n\t}\n\n\tret = sys_fcntl64(fd, cmd, local_arg);\n\n\tswitch (cmd) {\n\tcase F_GETLK64:\n\t\tif (!ret) {\n\t\t\tuser.l_type\t= kernel.l_type;\n\t\t\tuser.l_whence\t= kernel.l_whence;\n\t\t\tuser.l_start\t= kernel.l_start;\n\t\t\tuser.l_len\t= kernel.l_len;\n\t\t\tuser.l_pid\t= kernel.l_pid;\n\t\t\tif (copy_to_user((struct oabi_flock64 __user *)arg,\n\t\t\t\t\t &user, sizeof(user)))\n\t\t\t\tret = -EFAULT;\n\t\t}\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\tset_fs(fs);\n\t}\n\n\treturn ret;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nasmlinkage long sys_oabi_fcntl64(unsigned int fd, unsigned int cmd,\n\t\t\t\t unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase F_OFD_GETLK:\n\tcase F_OFD_SETLK:\n\tcase F_OFD_SETLKW:\n\tcase F_GETLK64:\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\treturn do_locks(fd, cmd, arg);\n\n\tdefault:\n\t\treturn sys_fcntl64(fd, cmd, arg);\n\t}\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int perf_event_read_group(struct perf_event *event,\n\t\t\t\t   u64 read_format, char __user *buf)\n{\n\tstruct perf_event *leader = event->group_leader, *sub;\n\tint n = 0, size = 0, ret = -EFAULT;\n\tstruct perf_event_context *ctx = leader->ctx;\n\tu64 values[5];\n\tu64 count, enabled, running;\n\n\tmutex_lock(&ctx->mutex);\n\tcount = perf_event_read_value(leader, &enabled, &running);\n\n\tvalues[n++] = 1 + leader->nr_siblings;\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_ENABLED)\n\t\tvalues[n++] = enabled;\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_RUNNING)\n\t\tvalues[n++] = running;\n\tvalues[n++] = count;\n\tif (read_format & PERF_FORMAT_ID)\n\t\tvalues[n++] = primary_event_id(leader);\n\n\tsize = n * sizeof(u64);\n\n\tif (copy_to_user(buf, values, size))\n\t\tgoto unlock;\n\n\tret = size;\n\n\tlist_for_each_entry(sub, &leader->sibling_list, group_entry) {\n\t\tn = 0;\n\n\t\tvalues[n++] = perf_event_read_value(sub, &enabled, &running);\n\t\tif (read_format & PERF_FORMAT_ID)\n\t\t\tvalues[n++] = primary_event_id(sub);\n\n\t\tsize = n * sizeof(u64);\n\n\t\tif (copy_to_user(buf + ret, values, size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tret += size;\n\t}\nunlock:\n\tmutex_unlock(&ctx->mutex);\n\n\treturn ret;\n}",
                        "code_after_change": "static int perf_event_read_group(struct perf_event *event,\n\t\t\t\t   u64 read_format, char __user *buf)\n{\n\tstruct perf_event *leader = event->group_leader, *sub;\n\tstruct perf_event_context *ctx = leader->ctx;\n\tint n = 0, size = 0, ret;\n\tu64 count, enabled, running;\n\tu64 values[5];\n\n\tlockdep_assert_held(&ctx->mutex);\n\n\tcount = perf_event_read_value(leader, &enabled, &running);\n\n\tvalues[n++] = 1 + leader->nr_siblings;\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_ENABLED)\n\t\tvalues[n++] = enabled;\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_RUNNING)\n\t\tvalues[n++] = running;\n\tvalues[n++] = count;\n\tif (read_format & PERF_FORMAT_ID)\n\t\tvalues[n++] = primary_event_id(leader);\n\n\tsize = n * sizeof(u64);\n\n\tif (copy_to_user(buf, values, size))\n\t\treturn -EFAULT;\n\n\tret = size;\n\n\tlist_for_each_entry(sub, &leader->sibling_list, group_entry) {\n\t\tn = 0;\n\n\t\tvalues[n++] = perf_event_read_value(sub, &enabled, &running);\n\t\tif (read_format & PERF_FORMAT_ID)\n\t\t\tvalues[n++] = primary_event_id(sub);\n\n\t\tsize = n * sizeof(u64);\n\n\t\tif (copy_to_user(buf + ret, values, size)) {\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tret += size;\n\t}\n\n\treturn ret;\n}",
                        "cve_id": "CVE-2016-6786"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic long perf_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct perf_event *event = file->private_data;\n\tvoid (*func)(struct perf_event *);\n\tu32 flags = arg;\n\n\tswitch (cmd) {\n\tcase PERF_EVENT_IOC_ENABLE:\n\t\tfunc = perf_event_enable;\n\t\tbreak;\n\tcase PERF_EVENT_IOC_DISABLE:\n\t\tfunc = perf_event_disable;\n\t\tbreak;\n\tcase PERF_EVENT_IOC_RESET:\n\t\tfunc = perf_event_reset;\n\t\tbreak;\n\n\tcase PERF_EVENT_IOC_REFRESH:\n\t\treturn perf_event_refresh(event, arg);\n\n\tcase PERF_EVENT_IOC_PERIOD:\n\t\treturn perf_event_period(event, (u64 __user *)arg);\n\n\tcase PERF_EVENT_IOC_ID:\n\t{\n\t\tu64 id = primary_event_id(event);\n\n\t\tif (copy_to_user((void __user *)arg, &id, sizeof(id)))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\n\tcase PERF_EVENT_IOC_SET_OUTPUT:\n\t{\n\t\tint ret;\n\t\tif (arg != -1) {\n\t\t\tstruct perf_event *output_event;\n\t\t\tstruct fd output;\n\t\t\tret = perf_fget_light(arg, &output);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\toutput_event = output.file->private_data;\n\t\t\tret = perf_event_set_output(event, output_event);\n\t\t\tfdput(output);\n\t\t} else {\n\t\t\tret = perf_event_set_output(event, NULL);\n\t\t}\n\t\treturn ret;\n\t}\n\n\tcase PERF_EVENT_IOC_SET_FILTER:\n\t\treturn perf_event_set_filter(event, (void __user *)arg);\n\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (flags & PERF_IOC_FLAG_GROUP)\n\t\tperf_event_for_each(event, func);\n\telse\n\t\tperf_event_for_each_child(event, func);\n\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int perf_event_read_group(struct perf_event *event,\n\t\t\t\t   u64 read_format, char __user *buf)\n{\n\tstruct perf_event *leader = event->group_leader, *sub;\n\tint n = 0, size = 0, ret = -EFAULT;\n\tstruct perf_event_context *ctx = leader->ctx;\n\tu64 values[5];\n\tu64 count, enabled, running;\n\n\tmutex_lock(&ctx->mutex);\n\tcount = perf_event_read_value(leader, &enabled, &running);\n\n\tvalues[n++] = 1 + leader->nr_siblings;\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_ENABLED)\n\t\tvalues[n++] = enabled;\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_RUNNING)\n\t\tvalues[n++] = running;\n\tvalues[n++] = count;\n\tif (read_format & PERF_FORMAT_ID)\n\t\tvalues[n++] = primary_event_id(leader);\n\n\tsize = n * sizeof(u64);\n\n\tif (copy_to_user(buf, values, size))\n\t\tgoto unlock;\n\n\tret = size;\n\n\tlist_for_each_entry(sub, &leader->sibling_list, group_entry) {\n\t\tn = 0;\n\n\t\tvalues[n++] = perf_event_read_value(sub, &enabled, &running);\n\t\tif (read_format & PERF_FORMAT_ID)\n\t\t\tvalues[n++] = primary_event_id(sub);\n\n\t\tsize = n * sizeof(u64);\n\n\t\tif (copy_to_user(buf + ret, values, size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tret += size;\n\t}\nunlock:\n\tmutex_unlock(&ctx->mutex);\n\n\treturn ret;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int perf_event_read_group(struct perf_event *event,\n\t\t\t\t   u64 read_format, char __user *buf)\n{\n\tstruct perf_event *leader = event->group_leader, *sub;\n\tstruct perf_event_context *ctx = leader->ctx;\n\tint n = 0, size = 0, ret;\n\tu64 count, enabled, running;\n\tu64 values[5];\n\n\tlockdep_assert_held(&ctx->mutex);\n\n\tcount = perf_event_read_value(leader, &enabled, &running);\n\n\tvalues[n++] = 1 + leader->nr_siblings;\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_ENABLED)\n\t\tvalues[n++] = enabled;\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_RUNNING)\n\t\tvalues[n++] = running;\n\tvalues[n++] = count;\n\tif (read_format & PERF_FORMAT_ID)\n\t\tvalues[n++] = primary_event_id(leader);\n\n\tsize = n * sizeof(u64);\n\n\tif (copy_to_user(buf, values, size))\n\t\treturn -EFAULT;\n\n\tret = size;\n\n\tlist_for_each_entry(sub, &leader->sibling_list, group_entry) {\n\t\tn = 0;\n\n\t\tvalues[n++] = perf_event_read_value(sub, &enabled, &running);\n\t\tif (read_format & PERF_FORMAT_ID)\n\t\t\tvalues[n++] = primary_event_id(sub);\n\n\t\tsize = n * sizeof(u64);\n\n\t\tif (copy_to_user(buf + ret, values, size)) {\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tret += size;\n\t}\n\n\treturn ret;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 1081,
            "cve_id": "CVE-2016-6786",
            "code_snippet": "void perf_pmu_migrate_context(struct pmu *pmu, int src_cpu, int dst_cpu)\n{\n\tstruct perf_event_context *src_ctx;\n\tstruct perf_event_context *dst_ctx;\n\tstruct perf_event *event, *tmp;\n\tLIST_HEAD(events);\n\n\tsrc_ctx = &per_cpu_ptr(pmu->pmu_cpu_context, src_cpu)->ctx;\n\tdst_ctx = &per_cpu_ptr(pmu->pmu_cpu_context, dst_cpu)->ctx;\n\n\tmutex_lock(&src_ctx->mutex);\n\tlist_for_each_entry_safe(event, tmp, &src_ctx->event_list,\n\t\t\t\t event_entry) {\n\t\tperf_remove_from_context(event, false);\n\t\tunaccount_event_cpu(event, src_cpu);\n\t\tput_ctx(src_ctx);\n\t\tlist_add(&event->migrate_entry, &events);\n\t}\n\tmutex_unlock(&src_ctx->mutex);\n\n\tsynchronize_rcu();\n\n\tmutex_lock(&dst_ctx->mutex);\n\tlist_for_each_entry_safe(event, tmp, &events, migrate_entry) {\n\t\tlist_del(&event->migrate_entry);\n\t\tif (event->state >= PERF_EVENT_STATE_OFF)\n\t\t\tevent->state = PERF_EVENT_STATE_INACTIVE;\n\t\taccount_event_cpu(event, dst_cpu);\n\t\tperf_install_in_context(dst_ctx, event, dst_cpu);\n\t\tget_ctx(dst_ctx);\n\t}\n\tmutex_unlock(&dst_ctx->mutex);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": " */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx;\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tstruct perf_event_context *gctx = group_leader->ctx;\n\n\t\tmutex_lock(&gctx->mutex);\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\n\tif (move_group) {\n\t\tsynchronize_rcu();\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}",
                        "code_after_change": " */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *uninitialized_var(gctx);\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tgctx = group_leader->ctx;\n\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t} else {\n\t\tmutex_lock(&ctx->mutex);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\tif (move_group) {\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\n\tif (move_group) {\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}",
                        "cve_id": "CVE-2016-6786"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nvoid perf_pmu_migrate_context(struct pmu *pmu, int src_cpu, int dst_cpu)\n{\n\tstruct perf_event_context *src_ctx;\n\tstruct perf_event_context *dst_ctx;\n\tstruct perf_event *event, *tmp;\n\tLIST_HEAD(events);\n\n\tsrc_ctx = &per_cpu_ptr(pmu->pmu_cpu_context, src_cpu)->ctx;\n\tdst_ctx = &per_cpu_ptr(pmu->pmu_cpu_context, dst_cpu)->ctx;\n\n\tmutex_lock(&src_ctx->mutex);\n\tlist_for_each_entry_safe(event, tmp, &src_ctx->event_list,\n\t\t\t\t event_entry) {\n\t\tperf_remove_from_context(event, false);\n\t\tunaccount_event_cpu(event, src_cpu);\n\t\tput_ctx(src_ctx);\n\t\tlist_add(&event->migrate_entry, &events);\n\t}\n\tmutex_unlock(&src_ctx->mutex);\n\n\tsynchronize_rcu();\n\n\tmutex_lock(&dst_ctx->mutex);\n\tlist_for_each_entry_safe(event, tmp, &events, migrate_entry) {\n\t\tlist_del(&event->migrate_entry);\n\t\tif (event->state >= PERF_EVENT_STATE_OFF)\n\t\t\tevent->state = PERF_EVENT_STATE_INACTIVE;\n\t\taccount_event_cpu(event, dst_cpu);\n\t\tperf_install_in_context(dst_ctx, event, dst_cpu);\n\t\tget_ctx(dst_ctx);\n\t}\n\tmutex_unlock(&dst_ctx->mutex);\n}\n'''\nRelated vulnerability code snippet:\n'''\n */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx;\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tstruct perf_event_context *gctx = group_leader->ctx;\n\n\t\tmutex_lock(&gctx->mutex);\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\n\tif (move_group) {\n\t\tsynchronize_rcu();\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\n */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *uninitialized_var(gctx);\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tgctx = group_leader->ctx;\n\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t} else {\n\t\tmutex_lock(&ctx->mutex);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\tif (move_group) {\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\n\tif (move_group) {\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void perf_event_for_each(struct perf_event *event,\n\t\t\t\t  void (*func)(struct perf_event *))\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_event *sibling;\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\tevent = event->group_leader;\n\n\tperf_event_for_each_child(event, func);\n\tlist_for_each_entry(sibling, &event->sibling_list, group_entry)\n\t\tperf_event_for_each_child(sibling, func);\n\tmutex_unlock(&ctx->mutex);\n}",
                        "code_after_change": "static void perf_event_for_each(struct perf_event *event,\n\t\t\t\t  void (*func)(struct perf_event *))\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_event *sibling;\n\n\tlockdep_assert_held(&ctx->mutex);\n\n\tevent = event->group_leader;\n\n\tperf_event_for_each_child(event, func);\n\tlist_for_each_entry(sibling, &event->sibling_list, group_entry)\n\t\tperf_event_for_each_child(sibling, func);\n}",
                        "cve_id": "CVE-2016-6786"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nvoid perf_pmu_migrate_context(struct pmu *pmu, int src_cpu, int dst_cpu)\n{\n\tstruct perf_event_context *src_ctx;\n\tstruct perf_event_context *dst_ctx;\n\tstruct perf_event *event, *tmp;\n\tLIST_HEAD(events);\n\n\tsrc_ctx = &per_cpu_ptr(pmu->pmu_cpu_context, src_cpu)->ctx;\n\tdst_ctx = &per_cpu_ptr(pmu->pmu_cpu_context, dst_cpu)->ctx;\n\n\tmutex_lock(&src_ctx->mutex);\n\tlist_for_each_entry_safe(event, tmp, &src_ctx->event_list,\n\t\t\t\t event_entry) {\n\t\tperf_remove_from_context(event, false);\n\t\tunaccount_event_cpu(event, src_cpu);\n\t\tput_ctx(src_ctx);\n\t\tlist_add(&event->migrate_entry, &events);\n\t}\n\tmutex_unlock(&src_ctx->mutex);\n\n\tsynchronize_rcu();\n\n\tmutex_lock(&dst_ctx->mutex);\n\tlist_for_each_entry_safe(event, tmp, &events, migrate_entry) {\n\t\tlist_del(&event->migrate_entry);\n\t\tif (event->state >= PERF_EVENT_STATE_OFF)\n\t\t\tevent->state = PERF_EVENT_STATE_INACTIVE;\n\t\taccount_event_cpu(event, dst_cpu);\n\t\tperf_install_in_context(dst_ctx, event, dst_cpu);\n\t\tget_ctx(dst_ctx);\n\t}\n\tmutex_unlock(&dst_ctx->mutex);\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic void perf_event_for_each(struct perf_event *event,\n\t\t\t\t  void (*func)(struct perf_event *))\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_event *sibling;\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\tevent = event->group_leader;\n\n\tperf_event_for_each_child(event, func);\n\tlist_for_each_entry(sibling, &event->sibling_list, group_entry)\n\t\tperf_event_for_each_child(sibling, func);\n\tmutex_unlock(&ctx->mutex);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic void perf_event_for_each(struct perf_event *event,\n\t\t\t\t  void (*func)(struct perf_event *))\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_event *sibling;\n\n\tlockdep_assert_held(&ctx->mutex);\n\n\tevent = event->group_leader;\n\n\tperf_event_for_each_child(event, func);\n\tlist_for_each_entry(sibling, &event->sibling_list, group_entry)\n\t\tperf_event_for_each_child(sibling, func);\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "void perf_event_disable(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct task_struct *task = ctx->task;\n\n\tif (!task) {\n\t\t/*\n\t\t * Disable the event on the cpu that it's on\n\t\t */\n\t\tcpu_function_call(event->cpu, __perf_event_disable, event);\n\t\treturn;\n\t}\n\nretry:\n\tif (!task_function_call(task, __perf_event_disable, event))\n\t\treturn;\n\n\traw_spin_lock_irq(&ctx->lock);\n\t/*\n\t * If the event is still active, we need to retry the cross-call.\n\t */\n\tif (event->state == PERF_EVENT_STATE_ACTIVE) {\n\t\traw_spin_unlock_irq(&ctx->lock);\n\t\t/*\n\t\t * Reload the task pointer, it might have been changed by\n\t\t * a concurrent perf_event_context_sched_out().\n\t\t */\n\t\ttask = ctx->task;\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * Since we have the lock this context can't be scheduled\n\t * in, so we can change the state safely.\n\t */\n\tif (event->state == PERF_EVENT_STATE_INACTIVE) {\n\t\tupdate_group_times(event);\n\t\tevent->state = PERF_EVENT_STATE_OFF;\n\t}\n\traw_spin_unlock_irq(&ctx->lock);\n}",
                        "code_after_change": "int perf_event_task_disable(void)\n{\n\tstruct perf_event_context *ctx;\n\tstruct perf_event *event;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_for_each_entry(event, &current->perf_event_list, owner_entry) {\n\t\tctx = perf_event_ctx_lock(event);\n\t\tperf_event_for_each_child(event, _perf_event_disable);\n\t\tperf_event_ctx_unlock(event, ctx);\n\t}\n\tmutex_unlock(&current->perf_event_mutex);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2016-6786"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nvoid perf_pmu_migrate_context(struct pmu *pmu, int src_cpu, int dst_cpu)\n{\n\tstruct perf_event_context *src_ctx;\n\tstruct perf_event_context *dst_ctx;\n\tstruct perf_event *event, *tmp;\n\tLIST_HEAD(events);\n\n\tsrc_ctx = &per_cpu_ptr(pmu->pmu_cpu_context, src_cpu)->ctx;\n\tdst_ctx = &per_cpu_ptr(pmu->pmu_cpu_context, dst_cpu)->ctx;\n\n\tmutex_lock(&src_ctx->mutex);\n\tlist_for_each_entry_safe(event, tmp, &src_ctx->event_list,\n\t\t\t\t event_entry) {\n\t\tperf_remove_from_context(event, false);\n\t\tunaccount_event_cpu(event, src_cpu);\n\t\tput_ctx(src_ctx);\n\t\tlist_add(&event->migrate_entry, &events);\n\t}\n\tmutex_unlock(&src_ctx->mutex);\n\n\tsynchronize_rcu();\n\n\tmutex_lock(&dst_ctx->mutex);\n\tlist_for_each_entry_safe(event, tmp, &events, migrate_entry) {\n\t\tlist_del(&event->migrate_entry);\n\t\tif (event->state >= PERF_EVENT_STATE_OFF)\n\t\t\tevent->state = PERF_EVENT_STATE_INACTIVE;\n\t\taccount_event_cpu(event, dst_cpu);\n\t\tperf_install_in_context(dst_ctx, event, dst_cpu);\n\t\tget_ctx(dst_ctx);\n\t}\n\tmutex_unlock(&dst_ctx->mutex);\n}\n'''\nRelated vulnerability code snippet:\n'''\nvoid perf_event_disable(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct task_struct *task = ctx->task;\n\n\tif (!task) {\n\t\t/*\n\t\t * Disable the event on the cpu that it's on\n\t\t */\n\t\tcpu_function_call(event->cpu, __perf_event_disable, event);\n\t\treturn;\n\t}\n\nretry:\n\tif (!task_function_call(task, __perf_event_disable, event))\n\t\treturn;\n\n\traw_spin_lock_irq(&ctx->lock);\n\t/*\n\t * If the event is still active, we need to retry the cross-call.\n\t */\n\tif (event->state == PERF_EVENT_STATE_ACTIVE) {\n\t\traw_spin_unlock_irq(&ctx->lock);\n\t\t/*\n\t\t * Reload the task pointer, it might have been changed by\n\t\t * a concurrent perf_event_context_sched_out().\n\t\t */\n\t\ttask = ctx->task;\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * Since we have the lock this context can't be scheduled\n\t * in, so we can change the state safely.\n\t */\n\tif (event->state == PERF_EVENT_STATE_INACTIVE) {\n\t\tupdate_group_times(event);\n\t\tevent->state = PERF_EVENT_STATE_OFF;\n\t}\n\traw_spin_unlock_irq(&ctx->lock);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nint perf_event_task_disable(void)\n{\n\tstruct perf_event_context *ctx;\n\tstruct perf_event *event;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_for_each_entry(event, &current->perf_event_list, owner_entry) {\n\t\tctx = perf_event_ctx_lock(event);\n\t\tperf_event_for_each_child(event, _perf_event_disable);\n\t\tperf_event_ctx_unlock(event, ctx);\n\t}\n\tmutex_unlock(&current->perf_event_mutex);\n\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 1078,
            "cve_id": "CVE-2016-6786",
            "code_snippet": "static void perf_remove_from_owner(struct perf_event *event)\n{\n\tstruct task_struct *owner;\n\n\trcu_read_lock();\n\towner = ACCESS_ONCE(event->owner);\n\t/*\n\t * Matches the smp_wmb() in perf_event_exit_task(). If we observe\n\t * !owner it means the list deletion is complete and we can indeed\n\t * free this event, otherwise we need to serialize on\n\t * owner->perf_event_mutex.\n\t */\n\tsmp_read_barrier_depends();\n\tif (owner) {\n\t\t/*\n\t\t * Since delayed_put_task_struct() also drops the last\n\t\t * task reference we can safely take a new reference\n\t\t * while holding the rcu_read_lock().\n\t\t */\n\t\tget_task_struct(owner);\n\t}\n\trcu_read_unlock();\n\n\tif (owner) {\n\t\tmutex_lock(&owner->perf_event_mutex);\n\t\t/*\n\t\t * We have to re-check the event->owner field, if it is cleared\n\t\t * we raced with perf_event_exit_task(), acquiring the mutex\n\t\t * ensured they're done, and we can proceed with freeing the\n\t\t * event.\n\t\t */\n\t\tif (event->owner)\n\t\t\tlist_del_init(&event->owner_entry);\n\t\tmutex_unlock(&owner->perf_event_mutex);\n\t\tput_task_struct(owner);\n\t}\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": " */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx;\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tstruct perf_event_context *gctx = group_leader->ctx;\n\n\t\tmutex_lock(&gctx->mutex);\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\n\tif (move_group) {\n\t\tsynchronize_rcu();\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}",
                        "code_after_change": "int create_user_ns(struct cred *new)\n{\n\tstruct user_namespace *ns, *parent_ns = new->user_ns;\n\tkuid_t owner = new->euid;\n\tkgid_t group = new->egid;\n\tint ret;\n\n\t/*\n\t * Verify that we can not violate the policy of which files\n\t * may be accessed that is specified by the root directory,\n\t * by verifing that the root directory is at the root of the\n\t * mount namespace which allows all files to be accessed.\n\t */\n\tif (current_chrooted())\n\t\treturn -EPERM;\n\n\t/* The creator needs a mapping in the parent user namespace\n\t * or else we won't be able to reasonably tell userspace who\n\t * created a user_namespace.\n\t */\n\tif (!kuid_has_mapping(parent_ns, owner) ||\n\t    !kgid_has_mapping(parent_ns, group))\n\t\treturn -EPERM;\n\n\tns = kmem_cache_zalloc(user_ns_cachep, GFP_KERNEL);\n\tif (!ns)\n\t\treturn -ENOMEM;\n\n\tret = proc_alloc_inum(&ns->proc_inum);\n\tif (ret) {\n\t\tkmem_cache_free(user_ns_cachep, ns);\n\t\treturn ret;\n\t}\n\n\tatomic_set(&ns->count, 1);\n\t/* Leave the new->user_ns reference with the new user namespace. */\n\tns->parent = parent_ns;\n\tns->owner = owner;\n\tns->group = group;\n\n\tset_cred_user_ns(new, ns);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2016-6786"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic void perf_remove_from_owner(struct perf_event *event)\n{\n\tstruct task_struct *owner;\n\n\trcu_read_lock();\n\towner = ACCESS_ONCE(event->owner);\n\t/*\n\t * Matches the smp_wmb() in perf_event_exit_task(). If we observe\n\t * !owner it means the list deletion is complete and we can indeed\n\t * free this event, otherwise we need to serialize on\n\t * owner->perf_event_mutex.\n\t */\n\tsmp_read_barrier_depends();\n\tif (owner) {\n\t\t/*\n\t\t * Since delayed_put_task_struct() also drops the last\n\t\t * task reference we can safely take a new reference\n\t\t * while holding the rcu_read_lock().\n\t\t */\n\t\tget_task_struct(owner);\n\t}\n\trcu_read_unlock();\n\n\tif (owner) {\n\t\tmutex_lock(&owner->perf_event_mutex);\n\t\t/*\n\t\t * We have to re-check the event->owner field, if it is cleared\n\t\t * we raced with perf_event_exit_task(), acquiring the mutex\n\t\t * ensured they're done, and we can proceed with freeing the\n\t\t * event.\n\t\t */\n\t\tif (event->owner)\n\t\t\tlist_del_init(&event->owner_entry);\n\t\tmutex_unlock(&owner->perf_event_mutex);\n\t\tput_task_struct(owner);\n\t}\n}\n'''\nRelated vulnerability code snippet:\n'''\n */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx;\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tstruct perf_event_context *gctx = group_leader->ctx;\n\n\t\tmutex_lock(&gctx->mutex);\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\n\tif (move_group) {\n\t\tsynchronize_rcu();\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nint create_user_ns(struct cred *new)\n{\n\tstruct user_namespace *ns, *parent_ns = new->user_ns;\n\tkuid_t owner = new->euid;\n\tkgid_t group = new->egid;\n\tint ret;\n\n\t/*\n\t * Verify that we can not violate the policy of which files\n\t * may be accessed that is specified by the root directory,\n\t * by verifing that the root directory is at the root of the\n\t * mount namespace which allows all files to be accessed.\n\t */\n\tif (current_chrooted())\n\t\treturn -EPERM;\n\n\t/* The creator needs a mapping in the parent user namespace\n\t * or else we won't be able to reasonably tell userspace who\n\t * created a user_namespace.\n\t */\n\tif (!kuid_has_mapping(parent_ns, owner) ||\n\t    !kgid_has_mapping(parent_ns, group))\n\t\treturn -EPERM;\n\n\tns = kmem_cache_zalloc(user_ns_cachep, GFP_KERNEL);\n\tif (!ns)\n\t\treturn -ENOMEM;\n\n\tret = proc_alloc_inum(&ns->proc_inum);\n\tif (ret) {\n\t\tkmem_cache_free(user_ns_cachep, ns);\n\t\treturn ret;\n\t}\n\n\tatomic_set(&ns->count, 1);\n\t/* Leave the new->user_ns reference with the new user namespace. */\n\tns->parent = parent_ns;\n\tns->owner = owner;\n\tns->group = group;\n\n\tset_cred_user_ns(new, ns);\n\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int create_user_ns(struct cred *new)\n{\n\tstruct user_namespace *ns, *parent_ns = new->user_ns;\n\tkuid_t owner = new->euid;\n\tkgid_t group = new->egid;\n\tint ret;\n\n\t/* The creator needs a mapping in the parent user namespace\n\t * or else we won't be able to reasonably tell userspace who\n\t * created a user_namespace.\n\t */\n\tif (!kuid_has_mapping(parent_ns, owner) ||\n\t    !kgid_has_mapping(parent_ns, group))\n\t\treturn -EPERM;\n\n\tns = kmem_cache_zalloc(user_ns_cachep, GFP_KERNEL);\n\tif (!ns)\n\t\treturn -ENOMEM;\n\n\tret = proc_alloc_inum(&ns->proc_inum);\n\tif (ret) {\n\t\tkmem_cache_free(user_ns_cachep, ns);\n\t\treturn ret;\n\t}\n\n\tatomic_set(&ns->count, 1);\n\t/* Leave the new->user_ns reference with the new user namespace. */\n\tns->parent = parent_ns;\n\tns->owner = owner;\n\tns->group = group;\n\n\tset_cred_user_ns(new, ns);\n\n\treturn 0;\n}",
                        "code_after_change": " */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *uninitialized_var(gctx);\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tgctx = group_leader->ctx;\n\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t} else {\n\t\tmutex_lock(&ctx->mutex);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\tif (move_group) {\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\n\tif (move_group) {\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}",
                        "cve_id": "CVE-2013-1956"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic void perf_remove_from_owner(struct perf_event *event)\n{\n\tstruct task_struct *owner;\n\n\trcu_read_lock();\n\towner = ACCESS_ONCE(event->owner);\n\t/*\n\t * Matches the smp_wmb() in perf_event_exit_task(). If we observe\n\t * !owner it means the list deletion is complete and we can indeed\n\t * free this event, otherwise we need to serialize on\n\t * owner->perf_event_mutex.\n\t */\n\tsmp_read_barrier_depends();\n\tif (owner) {\n\t\t/*\n\t\t * Since delayed_put_task_struct() also drops the last\n\t\t * task reference we can safely take a new reference\n\t\t * while holding the rcu_read_lock().\n\t\t */\n\t\tget_task_struct(owner);\n\t}\n\trcu_read_unlock();\n\n\tif (owner) {\n\t\tmutex_lock(&owner->perf_event_mutex);\n\t\t/*\n\t\t * We have to re-check the event->owner field, if it is cleared\n\t\t * we raced with perf_event_exit_task(), acquiring the mutex\n\t\t * ensured they're done, and we can proceed with freeing the\n\t\t * event.\n\t\t */\n\t\tif (event->owner)\n\t\t\tlist_del_init(&event->owner_entry);\n\t\tmutex_unlock(&owner->perf_event_mutex);\n\t\tput_task_struct(owner);\n\t}\n}\n'''\nRelated vulnerability code snippet:\n'''\nint create_user_ns(struct cred *new)\n{\n\tstruct user_namespace *ns, *parent_ns = new->user_ns;\n\tkuid_t owner = new->euid;\n\tkgid_t group = new->egid;\n\tint ret;\n\n\t/* The creator needs a mapping in the parent user namespace\n\t * or else we won't be able to reasonably tell userspace who\n\t * created a user_namespace.\n\t */\n\tif (!kuid_has_mapping(parent_ns, owner) ||\n\t    !kgid_has_mapping(parent_ns, group))\n\t\treturn -EPERM;\n\n\tns = kmem_cache_zalloc(user_ns_cachep, GFP_KERNEL);\n\tif (!ns)\n\t\treturn -ENOMEM;\n\n\tret = proc_alloc_inum(&ns->proc_inum);\n\tif (ret) {\n\t\tkmem_cache_free(user_ns_cachep, ns);\n\t\treturn ret;\n\t}\n\n\tatomic_set(&ns->count, 1);\n\t/* Leave the new->user_ns reference with the new user namespace. */\n\tns->parent = parent_ns;\n\tns->owner = owner;\n\tns->group = group;\n\n\tset_cred_user_ns(new, ns);\n\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\n */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *uninitialized_var(gctx);\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tgctx = group_leader->ctx;\n\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t} else {\n\t\tmutex_lock(&ctx->mutex);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\tif (move_group) {\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\n\tif (move_group) {\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "void perf_event_disable(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct task_struct *task = ctx->task;\n\n\tif (!task) {\n\t\t/*\n\t\t * Disable the event on the cpu that it's on\n\t\t */\n\t\tcpu_function_call(event->cpu, __perf_event_disable, event);\n\t\treturn;\n\t}\n\nretry:\n\tif (!task_function_call(task, __perf_event_disable, event))\n\t\treturn;\n\n\traw_spin_lock_irq(&ctx->lock);\n\t/*\n\t * If the event is still active, we need to retry the cross-call.\n\t */\n\tif (event->state == PERF_EVENT_STATE_ACTIVE) {\n\t\traw_spin_unlock_irq(&ctx->lock);\n\t\t/*\n\t\t * Reload the task pointer, it might have been changed by\n\t\t * a concurrent perf_event_context_sched_out().\n\t\t */\n\t\ttask = ctx->task;\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * Since we have the lock this context can't be scheduled\n\t * in, so we can change the state safely.\n\t */\n\tif (event->state == PERF_EVENT_STATE_INACTIVE) {\n\t\tupdate_group_times(event);\n\t\tevent->state = PERF_EVENT_STATE_OFF;\n\t}\n\traw_spin_unlock_irq(&ctx->lock);\n}",
                        "code_after_change": "STATIC int\nxfs_ioctl_setattr(\n\txfs_inode_t\t\t*ip,\n\tstruct fsxattr\t\t*fa,\n\tint\t\t\tmask)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tstruct xfs_trans\t*tp;\n\tunsigned int\t\tlock_flags = 0;\n\tstruct xfs_dquot\t*udqp = NULL;\n\tstruct xfs_dquot\t*pdqp = NULL;\n\tstruct xfs_dquot\t*olddquot = NULL;\n\tint\t\t\tcode;\n\n\ttrace_xfs_ioctl_setattr(ip);\n\n\tif (mp->m_flags & XFS_MOUNT_RDONLY)\n\t\treturn XFS_ERROR(EROFS);\n\tif (XFS_FORCED_SHUTDOWN(mp))\n\t\treturn XFS_ERROR(EIO);\n\n\t/*\n\t * Disallow 32bit project ids when projid32bit feature is not enabled.\n\t */\n\tif ((mask & FSX_PROJID) && (fa->fsx_projid > (__uint16_t)-1) &&\n\t\t\t!xfs_sb_version_hasprojid32bit(&ip->i_mount->m_sb))\n\t\treturn XFS_ERROR(EINVAL);\n\n\t/*\n\t * If disk quotas is on, we make sure that the dquots do exist on disk,\n\t * before we start any other transactions. Trying to do this later\n\t * is messy. We don't care to take a readlock to look at the ids\n\t * in inode here, because we can't hold it across the trans_reserve.\n\t * If the IDs do change before we take the ilock, we're covered\n\t * because the i_*dquot fields will get updated anyway.\n\t */\n\tif (XFS_IS_QUOTA_ON(mp) && (mask & FSX_PROJID)) {\n\t\tcode = xfs_qm_vop_dqalloc(ip, ip->i_d.di_uid,\n\t\t\t\t\t ip->i_d.di_gid, fa->fsx_projid,\n\t\t\t\t\t XFS_QMOPT_PQUOTA, &udqp, NULL, &pdqp);\n\t\tif (code)\n\t\t\treturn code;\n\t}\n\n\t/*\n\t * For the other attributes, we acquire the inode lock and\n\t * first do an error checking pass.\n\t */\n\ttp = xfs_trans_alloc(mp, XFS_TRANS_SETATTR_NOT_SIZE);\n\tcode = xfs_trans_reserve(tp, &M_RES(mp)->tr_ichange, 0, 0);\n\tif (code)\n\t\tgoto error_return;\n\n\tlock_flags = XFS_ILOCK_EXCL;\n\txfs_ilock(ip, lock_flags);\n\n\t/*\n\t * CAP_FOWNER overrides the following restrictions:\n\t *\n\t * The user ID of the calling process must be equal\n\t * to the file owner ID, except in cases where the\n\t * CAP_FSETID capability is applicable.\n\t */\n\tif (!inode_owner_or_capable(VFS_I(ip))) {\n\t\tcode = XFS_ERROR(EPERM);\n\t\tgoto error_return;\n\t}\n\n\t/*\n\t * Do a quota reservation only if projid is actually going to change.\n\t * Only allow changing of projid from init_user_ns since it is a\n\t * non user namespace aware identifier.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\tif (current_user_ns() != &init_user_ns) {\n\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\tgoto error_return;\n\t\t}\n\n\t\tif (XFS_IS_QUOTA_RUNNING(mp) &&\n\t\t    XFS_IS_PQUOTA_ON(mp) &&\n\t\t    xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tASSERT(tp);\n\t\t\tcode = xfs_qm_vop_chown_reserve(tp, ip, udqp, NULL,\n\t\t\t\t\t\tpdqp, capable(CAP_FOWNER) ?\n\t\t\t\t\t\tXFS_QMOPT_FORCE_RES : 0);\n\t\t\tif (code)\t/* out of quota */\n\t\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\tif (mask & FSX_EXTSIZE) {\n\t\t/*\n\t\t * Can't change extent size if any extents are allocated.\n\t\t */\n\t\tif (ip->i_d.di_nextents &&\n\t\t    ((ip->i_d.di_extsize << mp->m_sb.sb_blocklog) !=\n\t\t     fa->fsx_extsize)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * Extent size must be a multiple of the appropriate block\n\t\t * size, if set at all. It must also be smaller than the\n\t\t * maximum extent size supported by the filesystem.\n\t\t *\n\t\t * Also, for non-realtime files, limit the extent size hint to\n\t\t * half the size of the AGs in the filesystem so alignment\n\t\t * doesn't result in extents larger than an AG.\n\t\t */\n\t\tif (fa->fsx_extsize != 0) {\n\t\t\txfs_extlen_t    size;\n\t\t\txfs_fsblock_t   extsize_fsb;\n\n\t\t\textsize_fsb = XFS_B_TO_FSB(mp, fa->fsx_extsize);\n\t\t\tif (extsize_fsb > MAXEXTLEN) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\n\t\t\tif (XFS_IS_REALTIME_INODE(ip) ||\n\t\t\t    ((mask & FSX_XFLAGS) &&\n\t\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME))) {\n\t\t\t\tsize = mp->m_sb.sb_rextsize <<\n\t\t\t\t       mp->m_sb.sb_blocklog;\n\t\t\t} else {\n\t\t\t\tsize = mp->m_sb.sb_blocksize;\n\t\t\t\tif (extsize_fsb > mp->m_sb.sb_agblocks / 2) {\n\t\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\t\tgoto error_return;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (fa->fsx_extsize % size) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\t}\n\n\n\tif (mask & FSX_XFLAGS) {\n\t\t/*\n\t\t * Can't change realtime flag if any extents are allocated.\n\t\t */\n\t\tif ((ip->i_d.di_nextents || ip->i_delayed_blks) &&\n\t\t    (XFS_IS_REALTIME_INODE(ip)) !=\n\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * If realtime flag is set then must have realtime data.\n\t\t */\n\t\tif ((fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tif ((mp->m_sb.sb_rblocks == 0) ||\n\t\t\t    (mp->m_sb.sb_rextsize == 0) ||\n\t\t\t    (ip->i_d.di_extsize % mp->m_sb.sb_rextsize)) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Can't modify an immutable/append-only file unless\n\t\t * we have appropriate permission.\n\t\t */\n\t\tif ((ip->i_d.di_flags &\n\t\t\t\t(XFS_DIFLAG_IMMUTABLE|XFS_DIFLAG_APPEND) ||\n\t\t     (fa->fsx_xflags &\n\t\t\t\t(XFS_XFLAG_IMMUTABLE | XFS_XFLAG_APPEND))) &&\n\t\t    !capable(CAP_LINUX_IMMUTABLE)) {\n\t\t\tcode = XFS_ERROR(EPERM);\n\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\txfs_trans_ijoin(tp, ip, 0);\n\n\t/*\n\t * Change file ownership.  Must be the owner or privileged.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\t/*\n\t\t * CAP_FSETID overrides the following restrictions:\n\t\t *\n\t\t * The set-user-ID and set-group-ID bits of a file will be\n\t\t * cleared upon successful return from chown()\n\t\t */\n\t\tif ((ip->i_d.di_mode & (S_ISUID|S_ISGID)) &&\n\t\t    !capable_wrt_inode_uidgid(VFS_I(ip), CAP_FSETID))\n\t\t\tip->i_d.di_mode &= ~(S_ISUID|S_ISGID);\n\n\t\t/*\n\t\t * Change the ownerships and register quota modifications\n\t\t * in the transaction.\n\t\t */\n\t\tif (xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tif (XFS_IS_QUOTA_RUNNING(mp) && XFS_IS_PQUOTA_ON(mp)) {\n\t\t\t\tolddquot = xfs_qm_vop_chown(tp, ip,\n\t\t\t\t\t\t\t&ip->i_pdquot, pdqp);\n\t\t\t}\n\t\t\txfs_set_projid(ip, fa->fsx_projid);\n\n\t\t\t/*\n\t\t\t * We may have to rev the inode as well as\n\t\t\t * the superblock version number since projids didn't\n\t\t\t * exist before DINODE_VERSION_2 and SB_VERSION_NLINK.\n\t\t\t */\n\t\t\tif (ip->i_d.di_version == 1)\n\t\t\t\txfs_bump_ino_vers2(tp, ip);\n\t\t}\n\n\t}\n\n\tif (mask & FSX_EXTSIZE)\n\t\tip->i_d.di_extsize = fa->fsx_extsize >> mp->m_sb.sb_blocklog;\n\tif (mask & FSX_XFLAGS) {\n\t\txfs_set_diflags(ip, fa->fsx_xflags);\n\t\txfs_diflags_to_linux(ip);\n\t}\n\n\txfs_trans_ichgtime(tp, ip, XFS_ICHGTIME_CHG);\n\txfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);\n\n\tXFS_STATS_INC(xs_ig_attrchg);\n\n\t/*\n\t * If this is a synchronous mount, make sure that the\n\t * transaction goes to disk before returning to the user.\n\t * This is slightly sub-optimal in that truncates require\n\t * two sync transactions instead of one for wsync filesystems.\n\t * One for the truncate and one for the timestamps since we\n\t * don't want to change the timestamps unless we're sure the\n\t * truncate worked.  Truncates are less than 1% of the laddis\n\t * mix so this probably isn't worth the trouble to optimize.\n\t */\n\tif (mp->m_flags & XFS_MOUNT_WSYNC)\n\t\txfs_trans_set_sync(tp);\n\tcode = xfs_trans_commit(tp, 0);\n\txfs_iunlock(ip, lock_flags);\n\n\t/*\n\t * Release any dquot(s) the inode had kept before chown.\n\t */\n\txfs_qm_dqrele(olddquot);\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\n\treturn code;\n\n error_return:\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\txfs_trans_cancel(tp, 0);\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\n\treturn code;\n}",
                        "cve_id": "CVE-2016-6786"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic void perf_remove_from_owner(struct perf_event *event)\n{\n\tstruct task_struct *owner;\n\n\trcu_read_lock();\n\towner = ACCESS_ONCE(event->owner);\n\t/*\n\t * Matches the smp_wmb() in perf_event_exit_task(). If we observe\n\t * !owner it means the list deletion is complete and we can indeed\n\t * free this event, otherwise we need to serialize on\n\t * owner->perf_event_mutex.\n\t */\n\tsmp_read_barrier_depends();\n\tif (owner) {\n\t\t/*\n\t\t * Since delayed_put_task_struct() also drops the last\n\t\t * task reference we can safely take a new reference\n\t\t * while holding the rcu_read_lock().\n\t\t */\n\t\tget_task_struct(owner);\n\t}\n\trcu_read_unlock();\n\n\tif (owner) {\n\t\tmutex_lock(&owner->perf_event_mutex);\n\t\t/*\n\t\t * We have to re-check the event->owner field, if it is cleared\n\t\t * we raced with perf_event_exit_task(), acquiring the mutex\n\t\t * ensured they're done, and we can proceed with freeing the\n\t\t * event.\n\t\t */\n\t\tif (event->owner)\n\t\t\tlist_del_init(&event->owner_entry);\n\t\tmutex_unlock(&owner->perf_event_mutex);\n\t\tput_task_struct(owner);\n\t}\n}\n'''\nRelated vulnerability code snippet:\n'''\nvoid perf_event_disable(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct task_struct *task = ctx->task;\n\n\tif (!task) {\n\t\t/*\n\t\t * Disable the event on the cpu that it's on\n\t\t */\n\t\tcpu_function_call(event->cpu, __perf_event_disable, event);\n\t\treturn;\n\t}\n\nretry:\n\tif (!task_function_call(task, __perf_event_disable, event))\n\t\treturn;\n\n\traw_spin_lock_irq(&ctx->lock);\n\t/*\n\t * If the event is still active, we need to retry the cross-call.\n\t */\n\tif (event->state == PERF_EVENT_STATE_ACTIVE) {\n\t\traw_spin_unlock_irq(&ctx->lock);\n\t\t/*\n\t\t * Reload the task pointer, it might have been changed by\n\t\t * a concurrent perf_event_context_sched_out().\n\t\t */\n\t\ttask = ctx->task;\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * Since we have the lock this context can't be scheduled\n\t * in, so we can change the state safely.\n\t */\n\tif (event->state == PERF_EVENT_STATE_INACTIVE) {\n\t\tupdate_group_times(event);\n\t\tevent->state = PERF_EVENT_STATE_OFF;\n\t}\n\traw_spin_unlock_irq(&ctx->lock);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nSTATIC int\nxfs_ioctl_setattr(\n\txfs_inode_t\t\t*ip,\n\tstruct fsxattr\t\t*fa,\n\tint\t\t\tmask)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tstruct xfs_trans\t*tp;\n\tunsigned int\t\tlock_flags = 0;\n\tstruct xfs_dquot\t*udqp = NULL;\n\tstruct xfs_dquot\t*pdqp = NULL;\n\tstruct xfs_dquot\t*olddquot = NULL;\n\tint\t\t\tcode;\n\n\ttrace_xfs_ioctl_setattr(ip);\n\n\tif (mp->m_flags & XFS_MOUNT_RDONLY)\n\t\treturn XFS_ERROR(EROFS);\n\tif (XFS_FORCED_SHUTDOWN(mp))\n\t\treturn XFS_ERROR(EIO);\n\n\t/*\n\t * Disallow 32bit project ids when projid32bit feature is not enabled.\n\t */\n\tif ((mask & FSX_PROJID) && (fa->fsx_projid > (__uint16_t)-1) &&\n\t\t\t!xfs_sb_version_hasprojid32bit(&ip->i_mount->m_sb))\n\t\treturn XFS_ERROR(EINVAL);\n\n\t/*\n\t * If disk quotas is on, we make sure that the dquots do exist on disk,\n\t * before we start any other transactions. Trying to do this later\n\t * is messy. We don't care to take a readlock to look at the ids\n\t * in inode here, because we can't hold it across the trans_reserve.\n\t * If the IDs do change before we take the ilock, we're covered\n\t * because the i_*dquot fields will get updated anyway.\n\t */\n\tif (XFS_IS_QUOTA_ON(mp) && (mask & FSX_PROJID)) {\n\t\tcode = xfs_qm_vop_dqalloc(ip, ip->i_d.di_uid,\n\t\t\t\t\t ip->i_d.di_gid, fa->fsx_projid,\n\t\t\t\t\t XFS_QMOPT_PQUOTA, &udqp, NULL, &pdqp);\n\t\tif (code)\n\t\t\treturn code;\n\t}\n\n\t/*\n\t * For the other attributes, we acquire the inode lock and\n\t * first do an error checking pass.\n\t */\n\ttp = xfs_trans_alloc(mp, XFS_TRANS_SETATTR_NOT_SIZE);\n\tcode = xfs_trans_reserve(tp, &M_RES(mp)->tr_ichange, 0, 0);\n\tif (code)\n\t\tgoto error_return;\n\n\tlock_flags = XFS_ILOCK_EXCL;\n\txfs_ilock(ip, lock_flags);\n\n\t/*\n\t * CAP_FOWNER overrides the following restrictions:\n\t *\n\t * The user ID of the calling process must be equal\n\t * to the file owner ID, except in cases where the\n\t * CAP_FSETID capability is applicable.\n\t */\n\tif (!inode_owner_or_capable(VFS_I(ip))) {\n\t\tcode = XFS_ERROR(EPERM);\n\t\tgoto error_return;\n\t}\n\n\t/*\n\t * Do a quota reservation only if projid is actually going to change.\n\t * Only allow changing of projid from init_user_ns since it is a\n\t * non user namespace aware identifier.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\tif (current_user_ns() != &init_user_ns) {\n\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\tgoto error_return;\n\t\t}\n\n\t\tif (XFS_IS_QUOTA_RUNNING(mp) &&\n\t\t    XFS_IS_PQUOTA_ON(mp) &&\n\t\t    xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tASSERT(tp);\n\t\t\tcode = xfs_qm_vop_chown_reserve(tp, ip, udqp, NULL,\n\t\t\t\t\t\tpdqp, capable(CAP_FOWNER) ?\n\t\t\t\t\t\tXFS_QMOPT_FORCE_RES : 0);\n\t\t\tif (code)\t/* out of quota */\n\t\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\tif (mask & FSX_EXTSIZE) {\n\t\t/*\n\t\t * Can't change extent size if any extents are allocated.\n\t\t */\n\t\tif (ip->i_d.di_nextents &&\n\t\t    ((ip->i_d.di_extsize << mp->m_sb.sb_blocklog) !=\n\t\t     fa->fsx_extsize)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * Extent size must be a multiple of the appropriate block\n\t\t * size, if set at all. It must also be smaller than the\n\t\t * maximum extent size supported by the filesystem.\n\t\t *\n\t\t * Also, for non-realtime files, limit the extent size hint to\n\t\t * half the size of the AGs in the filesystem so alignment\n\t\t * doesn't result in extents larger than an AG.\n\t\t */\n\t\tif (fa->fsx_extsize != 0) {\n\t\t\txfs_extlen_t    size;\n\t\t\txfs_fsblock_t   extsize_fsb;\n\n\t\t\textsize_fsb = XFS_B_TO_FSB(mp, fa->fsx_extsize);\n\t\t\tif (extsize_fsb > MAXEXTLEN) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\n\t\t\tif (XFS_IS_REALTIME_INODE(ip) ||\n\t\t\t    ((mask & FSX_XFLAGS) &&\n\t\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME))) {\n\t\t\t\tsize = mp->m_sb.sb_rextsize <<\n\t\t\t\t       mp->m_sb.sb_blocklog;\n\t\t\t} else {\n\t\t\t\tsize = mp->m_sb.sb_blocksize;\n\t\t\t\tif (extsize_fsb > mp->m_sb.sb_agblocks / 2) {\n\t\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\t\tgoto error_return;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (fa->fsx_extsize % size) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\t}\n\n\n\tif (mask & FSX_XFLAGS) {\n\t\t/*\n\t\t * Can't change realtime flag if any extents are allocated.\n\t\t */\n\t\tif ((ip->i_d.di_nextents || ip->i_delayed_blks) &&\n\t\t    (XFS_IS_REALTIME_INODE(ip)) !=\n\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * If realtime flag is set then must have realtime data.\n\t\t */\n\t\tif ((fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tif ((mp->m_sb.sb_rblocks == 0) ||\n\t\t\t    (mp->m_sb.sb_rextsize == 0) ||\n\t\t\t    (ip->i_d.di_extsize % mp->m_sb.sb_rextsize)) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Can't modify an immutable/append-only file unless\n\t\t * we have appropriate permission.\n\t\t */\n\t\tif ((ip->i_d.di_flags &\n\t\t\t\t(XFS_DIFLAG_IMMUTABLE|XFS_DIFLAG_APPEND) ||\n\t\t     (fa->fsx_xflags &\n\t\t\t\t(XFS_XFLAG_IMMUTABLE | XFS_XFLAG_APPEND))) &&\n\t\t    !capable(CAP_LINUX_IMMUTABLE)) {\n\t\t\tcode = XFS_ERROR(EPERM);\n\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\txfs_trans_ijoin(tp, ip, 0);\n\n\t/*\n\t * Change file ownership.  Must be the owner or privileged.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\t/*\n\t\t * CAP_FSETID overrides the following restrictions:\n\t\t *\n\t\t * The set-user-ID and set-group-ID bits of a file will be\n\t\t * cleared upon successful return from chown()\n\t\t */\n\t\tif ((ip->i_d.di_mode & (S_ISUID|S_ISGID)) &&\n\t\t    !capable_wrt_inode_uidgid(VFS_I(ip), CAP_FSETID))\n\t\t\tip->i_d.di_mode &= ~(S_ISUID|S_ISGID);\n\n\t\t/*\n\t\t * Change the ownerships and register quota modifications\n\t\t * in the transaction.\n\t\t */\n\t\tif (xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tif (XFS_IS_QUOTA_RUNNING(mp) && XFS_IS_PQUOTA_ON(mp)) {\n\t\t\t\tolddquot = xfs_qm_vop_chown(tp, ip,\n\t\t\t\t\t\t\t&ip->i_pdquot, pdqp);\n\t\t\t}\n\t\t\txfs_set_projid(ip, fa->fsx_projid);\n\n\t\t\t/*\n\t\t\t * We may have to rev the inode as well as\n\t\t\t * the superblock version number since projids didn't\n\t\t\t * exist before DINODE_VERSION_2 and SB_VERSION_NLINK.\n\t\t\t */\n\t\t\tif (ip->i_d.di_version == 1)\n\t\t\t\txfs_bump_ino_vers2(tp, ip);\n\t\t}\n\n\t}\n\n\tif (mask & FSX_EXTSIZE)\n\t\tip->i_d.di_extsize = fa->fsx_extsize >> mp->m_sb.sb_blocklog;\n\tif (mask & FSX_XFLAGS) {\n\t\txfs_set_diflags(ip, fa->fsx_xflags);\n\t\txfs_diflags_to_linux(ip);\n\t}\n\n\txfs_trans_ichgtime(tp, ip, XFS_ICHGTIME_CHG);\n\txfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);\n\n\tXFS_STATS_INC(xs_ig_attrchg);\n\n\t/*\n\t * If this is a synchronous mount, make sure that the\n\t * transaction goes to disk before returning to the user.\n\t * This is slightly sub-optimal in that truncates require\n\t * two sync transactions instead of one for wsync filesystems.\n\t * One for the truncate and one for the timestamps since we\n\t * don't want to change the timestamps unless we're sure the\n\t * truncate worked.  Truncates are less than 1% of the laddis\n\t * mix so this probably isn't worth the trouble to optimize.\n\t */\n\tif (mp->m_flags & XFS_MOUNT_WSYNC)\n\t\txfs_trans_set_sync(tp);\n\tcode = xfs_trans_commit(tp, 0);\n\txfs_iunlock(ip, lock_flags);\n\n\t/*\n\t * Release any dquot(s) the inode had kept before chown.\n\t */\n\txfs_qm_dqrele(olddquot);\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\n\treturn code;\n\n error_return:\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\txfs_trans_cancel(tp, 0);\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\n\treturn code;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 836,
            "cve_id": "CVE-2015-8709",
            "code_snippet": "struct mm_struct *mm_alloc(void)\n{\n\tstruct mm_struct *mm;\n\n\tmm = allocate_mm();\n\tif (!mm)\n\t\treturn NULL;\n\n\tmemset(mm, 0, sizeof(*mm));\n\treturn mm_init(mm, current);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static struct mm_struct *dup_mm(struct task_struct *tsk)\n{\n\tstruct mm_struct *mm, *oldmm = current->mm;\n\tint err;\n\n\tmm = allocate_mm();\n\tif (!mm)\n\t\tgoto fail_nomem;\n\n\tmemcpy(mm, oldmm, sizeof(*mm));\n\n\tif (!mm_init(mm, tsk))\n\t\tgoto fail_nomem;\n\n\terr = dup_mmap(mm, oldmm);\n\tif (err)\n\t\tgoto free_pt;\n\n\tmm->hiwater_rss = get_mm_rss(mm);\n\tmm->hiwater_vm = mm->total_vm;\n\n\tif (mm->binfmt && !try_module_get(mm->binfmt->module))\n\t\tgoto free_pt;\n\n\treturn mm;\n\nfree_pt:\n\t/* don't put binfmt in mmput, we haven't got module yet */\n\tmm->binfmt = NULL;\n\tmmput(mm);\n\nfail_nomem:\n\treturn NULL;\n}",
                        "code_after_change": "static struct mm_struct *dup_mm(struct task_struct *tsk)\n{\n\tstruct mm_struct *mm, *oldmm = current->mm;\n\tint err;\n\n\tmm = allocate_mm();\n\tif (!mm)\n\t\tgoto fail_nomem;\n\n\tmemcpy(mm, oldmm, sizeof(*mm));\n\n\tif (!mm_init(mm, tsk, mm->user_ns))\n\t\tgoto fail_nomem;\n\n\terr = dup_mmap(mm, oldmm);\n\tif (err)\n\t\tgoto free_pt;\n\n\tmm->hiwater_rss = get_mm_rss(mm);\n\tmm->hiwater_vm = mm->total_vm;\n\n\tif (mm->binfmt && !try_module_get(mm->binfmt->module))\n\t\tgoto free_pt;\n\n\treturn mm;\n\nfree_pt:\n\t/* don't put binfmt in mmput, we haven't got module yet */\n\tmm->binfmt = NULL;\n\tmmput(mm);\n\nfail_nomem:\n\treturn NULL;\n}",
                        "cve_id": "CVE-2015-8709"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstruct mm_struct *mm_alloc(void)\n{\n\tstruct mm_struct *mm;\n\n\tmm = allocate_mm();\n\tif (!mm)\n\t\treturn NULL;\n\n\tmemset(mm, 0, sizeof(*mm));\n\treturn mm_init(mm, current);\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic struct mm_struct *dup_mm(struct task_struct *tsk)\n{\n\tstruct mm_struct *mm, *oldmm = current->mm;\n\tint err;\n\n\tmm = allocate_mm();\n\tif (!mm)\n\t\tgoto fail_nomem;\n\n\tmemcpy(mm, oldmm, sizeof(*mm));\n\n\tif (!mm_init(mm, tsk))\n\t\tgoto fail_nomem;\n\n\terr = dup_mmap(mm, oldmm);\n\tif (err)\n\t\tgoto free_pt;\n\n\tmm->hiwater_rss = get_mm_rss(mm);\n\tmm->hiwater_vm = mm->total_vm;\n\n\tif (mm->binfmt && !try_module_get(mm->binfmt->module))\n\t\tgoto free_pt;\n\n\treturn mm;\n\nfree_pt:\n\t/* don't put binfmt in mmput, we haven't got module yet */\n\tmm->binfmt = NULL;\n\tmmput(mm);\n\nfail_nomem:\n\treturn NULL;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic struct mm_struct *dup_mm(struct task_struct *tsk)\n{\n\tstruct mm_struct *mm, *oldmm = current->mm;\n\tint err;\n\n\tmm = allocate_mm();\n\tif (!mm)\n\t\tgoto fail_nomem;\n\n\tmemcpy(mm, oldmm, sizeof(*mm));\n\n\tif (!mm_init(mm, tsk, mm->user_ns))\n\t\tgoto fail_nomem;\n\n\terr = dup_mmap(mm, oldmm);\n\tif (err)\n\t\tgoto free_pt;\n\n\tmm->hiwater_rss = get_mm_rss(mm);\n\tmm->hiwater_vm = mm->total_vm;\n\n\tif (mm->binfmt && !try_module_get(mm->binfmt->module))\n\t\tgoto free_pt;\n\n\treturn mm;\n\nfree_pt:\n\t/* don't put binfmt in mmput, we haven't got module yet */\n\tmm->binfmt = NULL;\n\tmmput(mm);\n\nfail_nomem:\n\treturn NULL;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "void __mmdrop(struct mm_struct *mm)\n{\n\tBUG_ON(mm == &init_mm);\n\tmm_free_pgd(mm);\n\tdestroy_context(mm);\n\tmmu_notifier_mm_destroy(mm);\n\tcheck_mm(mm);\n\tfree_mm(mm);\n}",
                        "code_after_change": "void __mmdrop(struct mm_struct *mm)\n{\n\tBUG_ON(mm == &init_mm);\n\tmm_free_pgd(mm);\n\tdestroy_context(mm);\n\tmmu_notifier_mm_destroy(mm);\n\tcheck_mm(mm);\n\tput_user_ns(mm->user_ns);\n\tfree_mm(mm);\n}",
                        "cve_id": "CVE-2015-8709"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstruct mm_struct *mm_alloc(void)\n{\n\tstruct mm_struct *mm;\n\n\tmm = allocate_mm();\n\tif (!mm)\n\t\treturn NULL;\n\n\tmemset(mm, 0, sizeof(*mm));\n\treturn mm_init(mm, current);\n}\n'''\nRelated vulnerability code snippet:\n'''\nvoid __mmdrop(struct mm_struct *mm)\n{\n\tBUG_ON(mm == &init_mm);\n\tmm_free_pgd(mm);\n\tdestroy_context(mm);\n\tmmu_notifier_mm_destroy(mm);\n\tcheck_mm(mm);\n\tfree_mm(mm);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nvoid __mmdrop(struct mm_struct *mm)\n{\n\tBUG_ON(mm == &init_mm);\n\tmm_free_pgd(mm);\n\tdestroy_context(mm);\n\tmmu_notifier_mm_destroy(mm);\n\tcheck_mm(mm);\n\tput_user_ns(mm->user_ns);\n\tfree_mm(mm);\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "asmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}",
                        "code_after_change": "static int __ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tconst struct cred *cred = current_cred(), *tcred;\n\tstruct mm_struct *mm;\n\tkuid_t caller_uid;\n\tkgid_t caller_gid;\n\n\tif (!(mode & PTRACE_MODE_FSCREDS) == !(mode & PTRACE_MODE_REALCREDS)) {\n\t\tWARN(1, \"denying ptrace access check without PTRACE_MODE_*CREDS\\n\");\n\t\treturn -EPERM;\n\t}\n\n\t/* May we inspect the given task?\n\t * This check is used both for attaching with ptrace\n\t * and for allowing access to sensitive information in /proc.\n\t *\n\t * ptrace_attach denies several cases that /proc allows\n\t * because setting up the necessary parent/child relationship\n\t * or halting the specified task is impossible.\n\t */\n\n\t/* Don't let security modules deny introspection */\n\tif (same_thread_group(task, current))\n\t\treturn 0;\n\trcu_read_lock();\n\tif (mode & PTRACE_MODE_FSCREDS) {\n\t\tcaller_uid = cred->fsuid;\n\t\tcaller_gid = cred->fsgid;\n\t} else {\n\t\t/*\n\t\t * Using the euid would make more sense here, but something\n\t\t * in userland might rely on the old behavior, and this\n\t\t * shouldn't be a security problem since\n\t\t * PTRACE_MODE_REALCREDS implies that the caller explicitly\n\t\t * used a syscall that requests access to another process\n\t\t * (and not a filesystem syscall to procfs).\n\t\t */\n\t\tcaller_uid = cred->uid;\n\t\tcaller_gid = cred->gid;\n\t}\n\ttcred = __task_cred(task);\n\tif (uid_eq(caller_uid, tcred->euid) &&\n\t    uid_eq(caller_uid, tcred->suid) &&\n\t    uid_eq(caller_uid, tcred->uid)  &&\n\t    gid_eq(caller_gid, tcred->egid) &&\n\t    gid_eq(caller_gid, tcred->sgid) &&\n\t    gid_eq(caller_gid, tcred->gid))\n\t\tgoto ok;\n\tif (ptrace_has_cap(tcred->user_ns, mode))\n\t\tgoto ok;\n\trcu_read_unlock();\n\treturn -EPERM;\nok:\n\trcu_read_unlock();\n\tmm = task->mm;\n\tif (mm &&\n\t    ((get_dumpable(mm) != SUID_DUMP_USER) &&\n\t     !ptrace_has_cap(mm->user_ns, mode)))\n\t    return -EPERM;\n\n\treturn security_ptrace_access_check(task, mode);\n}",
                        "cve_id": "CVE-2014-9870"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstruct mm_struct *mm_alloc(void)\n{\n\tstruct mm_struct *mm;\n\n\tmm = allocate_mm();\n\tif (!mm)\n\t\treturn NULL;\n\n\tmemset(mm, 0, sizeof(*mm));\n\treturn mm_init(mm, current);\n}\n'''\nRelated vulnerability code snippet:\n'''\nasmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int __ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tconst struct cred *cred = current_cred(), *tcred;\n\tstruct mm_struct *mm;\n\tkuid_t caller_uid;\n\tkgid_t caller_gid;\n\n\tif (!(mode & PTRACE_MODE_FSCREDS) == !(mode & PTRACE_MODE_REALCREDS)) {\n\t\tWARN(1, \"denying ptrace access check without PTRACE_MODE_*CREDS\\n\");\n\t\treturn -EPERM;\n\t}\n\n\t/* May we inspect the given task?\n\t * This check is used both for attaching with ptrace\n\t * and for allowing access to sensitive information in /proc.\n\t *\n\t * ptrace_attach denies several cases that /proc allows\n\t * because setting up the necessary parent/child relationship\n\t * or halting the specified task is impossible.\n\t */\n\n\t/* Don't let security modules deny introspection */\n\tif (same_thread_group(task, current))\n\t\treturn 0;\n\trcu_read_lock();\n\tif (mode & PTRACE_MODE_FSCREDS) {\n\t\tcaller_uid = cred->fsuid;\n\t\tcaller_gid = cred->fsgid;\n\t} else {\n\t\t/*\n\t\t * Using the euid would make more sense here, but something\n\t\t * in userland might rely on the old behavior, and this\n\t\t * shouldn't be a security problem since\n\t\t * PTRACE_MODE_REALCREDS implies that the caller explicitly\n\t\t * used a syscall that requests access to another process\n\t\t * (and not a filesystem syscall to procfs).\n\t\t */\n\t\tcaller_uid = cred->uid;\n\t\tcaller_gid = cred->gid;\n\t}\n\ttcred = __task_cred(task);\n\tif (uid_eq(caller_uid, tcred->euid) &&\n\t    uid_eq(caller_uid, tcred->suid) &&\n\t    uid_eq(caller_uid, tcred->uid)  &&\n\t    gid_eq(caller_gid, tcred->egid) &&\n\t    gid_eq(caller_gid, tcred->sgid) &&\n\t    gid_eq(caller_gid, tcred->gid))\n\t\tgoto ok;\n\tif (ptrace_has_cap(tcred->user_ns, mode))\n\t\tgoto ok;\n\trcu_read_unlock();\n\treturn -EPERM;\nok:\n\trcu_read_unlock();\n\tmm = task->mm;\n\tif (mm &&\n\t    ((get_dumpable(mm) != SUID_DUMP_USER) &&\n\t     !ptrace_has_cap(mm->user_ns, mode)))\n\t    return -EPERM;\n\n\treturn security_ptrace_access_check(task, mode);\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 260,
            "cve_id": "CVE-2013-2930",
            "code_snippet": "static int perf_trace_event_perm(struct ftrace_event_call *tp_event,\n\t\t\t\t struct perf_event *p_event)\n{\n\t/* The ftrace function trace is allowed only for root. */\n\tif (ftrace_event_is_function(tp_event) &&\n\t    perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\t/* No tracing, just counting, so no obvious leak */\n\tif (!(p_event->attr.sample_type & PERF_SAMPLE_RAW))\n\t\treturn 0;\n\n\t/* Some events are ok to be traced by non-root users... */\n\tif (p_event->attach_state == PERF_ATTACH_TASK) {\n\t\tif (tp_event->flags & TRACE_EVENT_FL_CAP_ANY)\n\t\t\treturn 0;\n\t}\n\n\t/*\n\t * ...otherwise raw tracepoint data can be a severe data leak,\n\t * only allow root to have these.\n\t */\n\tif (perf_paranoid_tracepoint_raw() && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "asmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}",
                        "code_after_change": "asmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value[0] = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}",
                        "cve_id": "CVE-2014-9870"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int perf_trace_event_perm(struct ftrace_event_call *tp_event,\n\t\t\t\t struct perf_event *p_event)\n{\n\t/* The ftrace function trace is allowed only for root. */\n\tif (ftrace_event_is_function(tp_event) &&\n\t    perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\t/* No tracing, just counting, so no obvious leak */\n\tif (!(p_event->attr.sample_type & PERF_SAMPLE_RAW))\n\t\treturn 0;\n\n\t/* Some events are ok to be traced by non-root users... */\n\tif (p_event->attach_state == PERF_ATTACH_TASK) {\n\t\tif (tp_event->flags & TRACE_EVENT_FL_CAP_ANY)\n\t\t\treturn 0;\n\t}\n\n\t/*\n\t * ...otherwise raw tracepoint data can be a severe data leak,\n\t * only allow root to have these.\n\t */\n\tif (perf_paranoid_tracepoint_raw() && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nasmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nasmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value[0] = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": " */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx;\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tstruct perf_event_context *gctx = group_leader->ctx;\n\n\t\tmutex_lock(&gctx->mutex);\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\n\tif (move_group) {\n\t\tsynchronize_rcu();\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}",
                        "code_after_change": "int create_user_ns(struct cred *new)\n{\n\tstruct user_namespace *ns, *parent_ns = new->user_ns;\n\tkuid_t owner = new->euid;\n\tkgid_t group = new->egid;\n\tint ret;\n\n\t/*\n\t * Verify that we can not violate the policy of which files\n\t * may be accessed that is specified by the root directory,\n\t * by verifing that the root directory is at the root of the\n\t * mount namespace which allows all files to be accessed.\n\t */\n\tif (current_chrooted())\n\t\treturn -EPERM;\n\n\t/* The creator needs a mapping in the parent user namespace\n\t * or else we won't be able to reasonably tell userspace who\n\t * created a user_namespace.\n\t */\n\tif (!kuid_has_mapping(parent_ns, owner) ||\n\t    !kgid_has_mapping(parent_ns, group))\n\t\treturn -EPERM;\n\n\tns = kmem_cache_zalloc(user_ns_cachep, GFP_KERNEL);\n\tif (!ns)\n\t\treturn -ENOMEM;\n\n\tret = proc_alloc_inum(&ns->proc_inum);\n\tif (ret) {\n\t\tkmem_cache_free(user_ns_cachep, ns);\n\t\treturn ret;\n\t}\n\n\tatomic_set(&ns->count, 1);\n\t/* Leave the new->user_ns reference with the new user namespace. */\n\tns->parent = parent_ns;\n\tns->owner = owner;\n\tns->group = group;\n\n\tset_cred_user_ns(new, ns);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2016-6786"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int perf_trace_event_perm(struct ftrace_event_call *tp_event,\n\t\t\t\t struct perf_event *p_event)\n{\n\t/* The ftrace function trace is allowed only for root. */\n\tif (ftrace_event_is_function(tp_event) &&\n\t    perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\t/* No tracing, just counting, so no obvious leak */\n\tif (!(p_event->attr.sample_type & PERF_SAMPLE_RAW))\n\t\treturn 0;\n\n\t/* Some events are ok to be traced by non-root users... */\n\tif (p_event->attach_state == PERF_ATTACH_TASK) {\n\t\tif (tp_event->flags & TRACE_EVENT_FL_CAP_ANY)\n\t\t\treturn 0;\n\t}\n\n\t/*\n\t * ...otherwise raw tracepoint data can be a severe data leak,\n\t * only allow root to have these.\n\t */\n\tif (perf_paranoid_tracepoint_raw() && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\n */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx;\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tstruct perf_event_context *gctx = group_leader->ctx;\n\n\t\tmutex_lock(&gctx->mutex);\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\n\tif (move_group) {\n\t\tsynchronize_rcu();\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nint create_user_ns(struct cred *new)\n{\n\tstruct user_namespace *ns, *parent_ns = new->user_ns;\n\tkuid_t owner = new->euid;\n\tkgid_t group = new->egid;\n\tint ret;\n\n\t/*\n\t * Verify that we can not violate the policy of which files\n\t * may be accessed that is specified by the root directory,\n\t * by verifing that the root directory is at the root of the\n\t * mount namespace which allows all files to be accessed.\n\t */\n\tif (current_chrooted())\n\t\treturn -EPERM;\n\n\t/* The creator needs a mapping in the parent user namespace\n\t * or else we won't be able to reasonably tell userspace who\n\t * created a user_namespace.\n\t */\n\tif (!kuid_has_mapping(parent_ns, owner) ||\n\t    !kgid_has_mapping(parent_ns, group))\n\t\treturn -EPERM;\n\n\tns = kmem_cache_zalloc(user_ns_cachep, GFP_KERNEL);\n\tif (!ns)\n\t\treturn -ENOMEM;\n\n\tret = proc_alloc_inum(&ns->proc_inum);\n\tif (ret) {\n\t\tkmem_cache_free(user_ns_cachep, ns);\n\t\treturn ret;\n\t}\n\n\tatomic_set(&ns->count, 1);\n\t/* Leave the new->user_ns reference with the new user namespace. */\n\tns->parent = parent_ns;\n\tns->owner = owner;\n\tns->group = group;\n\n\tset_cred_user_ns(new, ns);\n\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void cn_proc_mcast_ctl(struct cn_msg *msg,\n\t\t\t      struct netlink_skb_parms *nsp)\n{\n\tenum proc_cn_mcast_op *mc_op = NULL;\n\tint err = 0;\n\n\tif (msg->len != sizeof(*mc_op))\n\t\treturn;\n\n\t/* \n\t * Events are reported with respect to the initial pid\n\t * and user namespaces so ignore requestors from\n\t * other namespaces.\n\t */\n\tif ((current_user_ns() != &init_user_ns) ||\n\t    (task_active_pid_ns(current) != &init_pid_ns))\n\t\treturn;\n\n\t/* Can only change if privileged. */\n\tif (!capable(CAP_NET_ADMIN)) {\n\t\terr = EPERM;\n\t\tgoto out;\n\t}\n\n\tmc_op = (enum proc_cn_mcast_op *)msg->data;\n\tswitch (*mc_op) {\n\tcase PROC_CN_MCAST_LISTEN:\n\t\tatomic_inc(&proc_event_num_listeners);\n\t\tbreak;\n\tcase PROC_CN_MCAST_IGNORE:\n\t\tatomic_dec(&proc_event_num_listeners);\n\t\tbreak;\n\tdefault:\n\t\terr = EINVAL;\n\t\tbreak;\n\t}\n\nout:\n\tcn_proc_ack(err, msg->seq, msg->ack);\n}",
                        "code_after_change": " */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *uninitialized_var(gctx);\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tgctx = group_leader->ctx;\n\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t} else {\n\t\tmutex_lock(&ctx->mutex);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\tif (move_group) {\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\n\tif (move_group) {\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int perf_trace_event_perm(struct ftrace_event_call *tp_event,\n\t\t\t\t struct perf_event *p_event)\n{\n\t/* The ftrace function trace is allowed only for root. */\n\tif (ftrace_event_is_function(tp_event) &&\n\t    perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\t/* No tracing, just counting, so no obvious leak */\n\tif (!(p_event->attr.sample_type & PERF_SAMPLE_RAW))\n\t\treturn 0;\n\n\t/* Some events are ok to be traced by non-root users... */\n\tif (p_event->attach_state == PERF_ATTACH_TASK) {\n\t\tif (tp_event->flags & TRACE_EVENT_FL_CAP_ANY)\n\t\t\treturn 0;\n\t}\n\n\t/*\n\t * ...otherwise raw tracepoint data can be a severe data leak,\n\t * only allow root to have these.\n\t */\n\tif (perf_paranoid_tracepoint_raw() && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic void cn_proc_mcast_ctl(struct cn_msg *msg,\n\t\t\t      struct netlink_skb_parms *nsp)\n{\n\tenum proc_cn_mcast_op *mc_op = NULL;\n\tint err = 0;\n\n\tif (msg->len != sizeof(*mc_op))\n\t\treturn;\n\n\t/* \n\t * Events are reported with respect to the initial pid\n\t * and user namespaces so ignore requestors from\n\t * other namespaces.\n\t */\n\tif ((current_user_ns() != &init_user_ns) ||\n\t    (task_active_pid_ns(current) != &init_pid_ns))\n\t\treturn;\n\n\t/* Can only change if privileged. */\n\tif (!capable(CAP_NET_ADMIN)) {\n\t\terr = EPERM;\n\t\tgoto out;\n\t}\n\n\tmc_op = (enum proc_cn_mcast_op *)msg->data;\n\tswitch (*mc_op) {\n\tcase PROC_CN_MCAST_LISTEN:\n\t\tatomic_inc(&proc_event_num_listeners);\n\t\tbreak;\n\tcase PROC_CN_MCAST_IGNORE:\n\t\tatomic_dec(&proc_event_num_listeners);\n\t\tbreak;\n\tdefault:\n\t\terr = EINVAL;\n\t\tbreak;\n\t}\n\nout:\n\tcn_proc_ack(err, msg->seq, msg->ack);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\n */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *uninitialized_var(gctx);\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tgctx = group_leader->ctx;\n\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t} else {\n\t\tmutex_lock(&ctx->mutex);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\tif (move_group) {\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\n\tif (move_group) {\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 1041,
            "cve_id": "CVE-2016-4997",
            "code_snippet": "static int\ncheck_entry_size_and_hooks(struct ipt_entry *e,\n\t\t\t   struct xt_table_info *newinfo,\n\t\t\t   const unsigned char *base,\n\t\t\t   const unsigned char *limit,\n\t\t\t   const unsigned int *hook_entries,\n\t\t\t   const unsigned int *underflows,\n\t\t\t   unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct ipt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int\ncheck_entry_size_and_hooks(struct ip6t_entry *e,\n\t\t\t   struct xt_table_info *newinfo,\n\t\t\t   const unsigned char *base,\n\t\t\t   const unsigned char *limit,\n\t\t\t   const unsigned int *hook_entries,\n\t\t\t   const unsigned int *underflows,\n\t\t\t   unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct ip6t_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct ip6t_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct ip6t_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip6_checkentry(&e->ipv6))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}",
                        "code_after_change": "static int\ncheck_entry_size_and_hooks(struct ip6t_entry *e,\n\t\t\t   struct xt_table_info *newinfo,\n\t\t\t   const unsigned char *base,\n\t\t\t   const unsigned char *limit,\n\t\t\t   const unsigned int *hook_entries,\n\t\t\t   const unsigned int *underflows,\n\t\t\t   unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct ip6t_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct ip6t_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct ip6t_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip6_checkentry(&e->ipv6))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,\n\t\t\t\t     e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}",
                        "cve_id": "CVE-2016-4997"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int\ncheck_entry_size_and_hooks(struct ipt_entry *e,\n\t\t\t   struct xt_table_info *newinfo,\n\t\t\t   const unsigned char *base,\n\t\t\t   const unsigned char *limit,\n\t\t\t   const unsigned int *hook_entries,\n\t\t\t   const unsigned int *underflows,\n\t\t\t   unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct ipt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int\ncheck_entry_size_and_hooks(struct ip6t_entry *e,\n\t\t\t   struct xt_table_info *newinfo,\n\t\t\t   const unsigned char *base,\n\t\t\t   const unsigned char *limit,\n\t\t\t   const unsigned int *hook_entries,\n\t\t\t   const unsigned int *underflows,\n\t\t\t   unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct ip6t_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct ip6t_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct ip6t_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip6_checkentry(&e->ipv6))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int\ncheck_entry_size_and_hooks(struct ip6t_entry *e,\n\t\t\t   struct xt_table_info *newinfo,\n\t\t\t   const unsigned char *base,\n\t\t\t   const unsigned char *limit,\n\t\t\t   const unsigned int *hook_entries,\n\t\t\t   const unsigned int *underflows,\n\t\t\t   unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct ip6t_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct ip6t_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct ip6t_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip6_checkentry(&e->ipv6))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,\n\t\t\t\t     e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static inline int check_entry_size_and_hooks(struct arpt_entry *e,\n\t\t\t\t\t     struct xt_table_info *newinfo,\n\t\t\t\t\t     const unsigned char *base,\n\t\t\t\t\t     const unsigned char *limit,\n\t\t\t\t\t     const unsigned int *hook_entries,\n\t\t\t\t\t     const unsigned int *underflows,\n\t\t\t\t\t     unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct arpt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}",
                        "code_after_change": "static inline int check_entry_size_and_hooks(struct arpt_entry *e,\n\t\t\t\t\t     struct xt_table_info *newinfo,\n\t\t\t\t\t     const unsigned char *base,\n\t\t\t\t\t     const unsigned char *limit,\n\t\t\t\t\t     const unsigned int *hook_entries,\n\t\t\t\t\t     const unsigned int *underflows,\n\t\t\t\t\t     unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct arpt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,\n\t\t\t\t     e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}",
                        "cve_id": "CVE-2016-4997"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int\ncheck_entry_size_and_hooks(struct ipt_entry *e,\n\t\t\t   struct xt_table_info *newinfo,\n\t\t\t   const unsigned char *base,\n\t\t\t   const unsigned char *limit,\n\t\t\t   const unsigned int *hook_entries,\n\t\t\t   const unsigned int *underflows,\n\t\t\t   unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct ipt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic inline int check_entry_size_and_hooks(struct arpt_entry *e,\n\t\t\t\t\t     struct xt_table_info *newinfo,\n\t\t\t\t\t     const unsigned char *base,\n\t\t\t\t\t     const unsigned char *limit,\n\t\t\t\t\t     const unsigned int *hook_entries,\n\t\t\t\t\t     const unsigned int *underflows,\n\t\t\t\t\t     unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct arpt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic inline int check_entry_size_and_hooks(struct arpt_entry *e,\n\t\t\t\t\t     struct xt_table_info *newinfo,\n\t\t\t\t\t     const unsigned char *base,\n\t\t\t\t\t     const unsigned char *limit,\n\t\t\t\t\t     const unsigned int *hook_entries,\n\t\t\t\t\t     const unsigned int *underflows,\n\t\t\t\t\t     unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct arpt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,\n\t\t\t\t     e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int\ncheck_compat_entry_size_and_hooks(struct compat_ipt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ipt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ipt_entry) - sizeof(struct compat_ipt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ip, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ipt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV4, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}",
                        "code_after_change": "static int\ncheck_compat_entry_size_and_hooks(struct compat_ipt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ipt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e, e->elems,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ipt_entry) - sizeof(struct compat_ipt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ip, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ipt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV4, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}",
                        "cve_id": "CVE-2016-4997"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int\ncheck_entry_size_and_hooks(struct ipt_entry *e,\n\t\t\t   struct xt_table_info *newinfo,\n\t\t\t   const unsigned char *base,\n\t\t\t   const unsigned char *limit,\n\t\t\t   const unsigned int *hook_entries,\n\t\t\t   const unsigned int *underflows,\n\t\t\t   unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct ipt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int\ncheck_compat_entry_size_and_hooks(struct compat_ipt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ipt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ipt_entry) - sizeof(struct compat_ipt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ip, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ipt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV4, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int\ncheck_compat_entry_size_and_hooks(struct compat_ipt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ipt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e, e->elems,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ipt_entry) - sizeof(struct compat_ipt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ip, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ipt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV4, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 1154,
            "cve_id": "CVE-2016-9644",
            "code_snippet": "static void math_error(struct pt_regs *regs, int error_code, int trapnr)\n{\n\tstruct task_struct *task = current;\n\tstruct fpu *fpu = &task->thread.fpu;\n\tsiginfo_t info;\n\tchar *str = (trapnr == X86_TRAP_MF) ? \"fpu exception\" :\n\t\t\t\t\t\t\"simd exception\";\n\n\tif (notify_die(DIE_TRAP, str, regs, error_code, trapnr, SIGFPE) == NOTIFY_STOP)\n\t\treturn;\n\tconditional_sti(regs);\n\n\tif (!user_mode(regs)) {\n\t\tif (!fixup_exception(regs)) {\n\t\t\ttask->thread.error_code = error_code;\n\t\t\ttask->thread.trap_nr = trapnr;\n\t\t\tdie(str, regs, error_code);\n\t\t}\n\t\treturn;\n\t}\n\n\t/*\n\t * Save the info for the exception handler and clear the error.\n\t */\n\tfpu__save(fpu);\n\n\ttask->thread.trap_nr\t= trapnr;\n\ttask->thread.error_code = error_code;\n\tinfo.si_signo\t\t= SIGFPE;\n\tinfo.si_errno\t\t= 0;\n\tinfo.si_addr\t\t= (void __user *)uprobe_get_trap_addr(regs);\n\n\tinfo.si_code = fpu__exception_code(fpu, trapnr);\n\n\t/* Retry when we get spurious exceptions: */\n\tif (!info.si_code)\n\t\treturn;\n\n\tforce_sig_info(SIGFPE, &info, task);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static nokprobe_inline int\ndo_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,\n\t\t  struct pt_regs *regs,\tlong error_code)\n{\n\tif (v8086_mode(regs)) {\n\t\t/*\n\t\t * Traps 0, 1, 3, 4, and 5 should be forwarded to vm86.\n\t\t * On nmi (interrupt 2), do_trap should not be called.\n\t\t */\n\t\tif (trapnr < X86_TRAP_UD) {\n\t\t\tif (!handle_vm86_trap((struct kernel_vm86_regs *) regs,\n\t\t\t\t\t\terror_code, trapnr))\n\t\t\t\treturn 0;\n\t\t}\n\t\treturn -1;\n\t}\n\n\tif (!user_mode(regs)) {\n\t\tif (!fixup_exception(regs)) {\n\t\t\ttsk->thread.error_code = error_code;\n\t\t\ttsk->thread.trap_nr = trapnr;\n\t\t\tdie(str, regs, error_code);\n\t\t}\n\t\treturn 0;\n\t}\n\n\treturn -1;\n}",
                        "code_after_change": "static nokprobe_inline int\ndo_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,\n\t\t  struct pt_regs *regs,\tlong error_code)\n{\n\tif (v8086_mode(regs)) {\n\t\t/*\n\t\t * Traps 0, 1, 3, 4, and 5 should be forwarded to vm86.\n\t\t * On nmi (interrupt 2), do_trap should not be called.\n\t\t */\n\t\tif (trapnr < X86_TRAP_UD) {\n\t\t\tif (!handle_vm86_trap((struct kernel_vm86_regs *) regs,\n\t\t\t\t\t\terror_code, trapnr))\n\t\t\t\treturn 0;\n\t\t}\n\t\treturn -1;\n\t}\n\n\tif (!user_mode(regs)) {\n\t\tif (!fixup_exception(regs, trapnr)) {\n\t\t\ttsk->thread.error_code = error_code;\n\t\t\ttsk->thread.trap_nr = trapnr;\n\t\t\tdie(str, regs, error_code);\n\t\t}\n\t\treturn 0;\n\t}\n\n\treturn -1;\n}",
                        "cve_id": "CVE-2016-9644"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic void math_error(struct pt_regs *regs, int error_code, int trapnr)\n{\n\tstruct task_struct *task = current;\n\tstruct fpu *fpu = &task->thread.fpu;\n\tsiginfo_t info;\n\tchar *str = (trapnr == X86_TRAP_MF) ? \"fpu exception\" :\n\t\t\t\t\t\t\"simd exception\";\n\n\tif (notify_die(DIE_TRAP, str, regs, error_code, trapnr, SIGFPE) == NOTIFY_STOP)\n\t\treturn;\n\tconditional_sti(regs);\n\n\tif (!user_mode(regs)) {\n\t\tif (!fixup_exception(regs)) {\n\t\t\ttask->thread.error_code = error_code;\n\t\t\ttask->thread.trap_nr = trapnr;\n\t\t\tdie(str, regs, error_code);\n\t\t}\n\t\treturn;\n\t}\n\n\t/*\n\t * Save the info for the exception handler and clear the error.\n\t */\n\tfpu__save(fpu);\n\n\ttask->thread.trap_nr\t= trapnr;\n\ttask->thread.error_code = error_code;\n\tinfo.si_signo\t\t= SIGFPE;\n\tinfo.si_errno\t\t= 0;\n\tinfo.si_addr\t\t= (void __user *)uprobe_get_trap_addr(regs);\n\n\tinfo.si_code = fpu__exception_code(fpu, trapnr);\n\n\t/* Retry when we get spurious exceptions: */\n\tif (!info.si_code)\n\t\treturn;\n\n\tforce_sig_info(SIGFPE, &info, task);\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic nokprobe_inline int\ndo_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,\n\t\t  struct pt_regs *regs,\tlong error_code)\n{\n\tif (v8086_mode(regs)) {\n\t\t/*\n\t\t * Traps 0, 1, 3, 4, and 5 should be forwarded to vm86.\n\t\t * On nmi (interrupt 2), do_trap should not be called.\n\t\t */\n\t\tif (trapnr < X86_TRAP_UD) {\n\t\t\tif (!handle_vm86_trap((struct kernel_vm86_regs *) regs,\n\t\t\t\t\t\terror_code, trapnr))\n\t\t\t\treturn 0;\n\t\t}\n\t\treturn -1;\n\t}\n\n\tif (!user_mode(regs)) {\n\t\tif (!fixup_exception(regs)) {\n\t\t\ttsk->thread.error_code = error_code;\n\t\t\ttsk->thread.trap_nr = trapnr;\n\t\t\tdie(str, regs, error_code);\n\t\t}\n\t\treturn 0;\n\t}\n\n\treturn -1;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic nokprobe_inline int\ndo_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,\n\t\t  struct pt_regs *regs,\tlong error_code)\n{\n\tif (v8086_mode(regs)) {\n\t\t/*\n\t\t * Traps 0, 1, 3, 4, and 5 should be forwarded to vm86.\n\t\t * On nmi (interrupt 2), do_trap should not be called.\n\t\t */\n\t\tif (trapnr < X86_TRAP_UD) {\n\t\t\tif (!handle_vm86_trap((struct kernel_vm86_regs *) regs,\n\t\t\t\t\t\terror_code, trapnr))\n\t\t\t\treturn 0;\n\t\t}\n\t\treturn -1;\n\t}\n\n\tif (!user_mode(regs)) {\n\t\tif (!fixup_exception(regs, trapnr)) {\n\t\t\ttsk->thread.error_code = error_code;\n\t\t\ttsk->thread.trap_nr = trapnr;\n\t\t\tdie(str, regs, error_code);\n\t\t}\n\t\treturn 0;\n\t}\n\n\treturn -1;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "dotraplinkage void\ndo_general_protection(struct pt_regs *regs, long error_code)\n{\n\tstruct task_struct *tsk;\n\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(), \"entry code didn't wake RCU\");\n\tconditional_sti(regs);\n\n\tif (v8086_mode(regs)) {\n\t\tlocal_irq_enable();\n\t\thandle_vm86_fault((struct kernel_vm86_regs *) regs, error_code);\n\t\treturn;\n\t}\n\n\ttsk = current;\n\tif (!user_mode(regs)) {\n\t\tif (fixup_exception(regs))\n\t\t\treturn;\n\n\t\ttsk->thread.error_code = error_code;\n\t\ttsk->thread.trap_nr = X86_TRAP_GP;\n\t\tif (notify_die(DIE_GPF, \"general protection fault\", regs, error_code,\n\t\t\t       X86_TRAP_GP, SIGSEGV) != NOTIFY_STOP)\n\t\t\tdie(\"general protection fault\", regs, error_code);\n\t\treturn;\n\t}\n\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_nr = X86_TRAP_GP;\n\n\tif (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&\n\t\t\tprintk_ratelimit()) {\n\t\tpr_info(\"%s[%d] general protection ip:%lx sp:%lx error:%lx\",\n\t\t\ttsk->comm, task_pid_nr(tsk),\n\t\t\tregs->ip, regs->sp, error_code);\n\t\tprint_vma_addr(\" in \", regs->ip);\n\t\tpr_cont(\"\\n\");\n\t}\n\n\tforce_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);\n}",
                        "code_after_change": "dotraplinkage void\ndo_general_protection(struct pt_regs *regs, long error_code)\n{\n\tstruct task_struct *tsk;\n\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(), \"entry code didn't wake RCU\");\n\tconditional_sti(regs);\n\n\tif (v8086_mode(regs)) {\n\t\tlocal_irq_enable();\n\t\thandle_vm86_fault((struct kernel_vm86_regs *) regs, error_code);\n\t\treturn;\n\t}\n\n\ttsk = current;\n\tif (!user_mode(regs)) {\n\t\tif (fixup_exception(regs, X86_TRAP_GP))\n\t\t\treturn;\n\n\t\ttsk->thread.error_code = error_code;\n\t\ttsk->thread.trap_nr = X86_TRAP_GP;\n\t\tif (notify_die(DIE_GPF, \"general protection fault\", regs, error_code,\n\t\t\t       X86_TRAP_GP, SIGSEGV) != NOTIFY_STOP)\n\t\t\tdie(\"general protection fault\", regs, error_code);\n\t\treturn;\n\t}\n\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_nr = X86_TRAP_GP;\n\n\tif (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&\n\t\t\tprintk_ratelimit()) {\n\t\tpr_info(\"%s[%d] general protection ip:%lx sp:%lx error:%lx\",\n\t\t\ttsk->comm, task_pid_nr(tsk),\n\t\t\tregs->ip, regs->sp, error_code);\n\t\tprint_vma_addr(\" in \", regs->ip);\n\t\tpr_cont(\"\\n\");\n\t}\n\n\tforce_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);\n}",
                        "cve_id": "CVE-2016-9644"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic void math_error(struct pt_regs *regs, int error_code, int trapnr)\n{\n\tstruct task_struct *task = current;\n\tstruct fpu *fpu = &task->thread.fpu;\n\tsiginfo_t info;\n\tchar *str = (trapnr == X86_TRAP_MF) ? \"fpu exception\" :\n\t\t\t\t\t\t\"simd exception\";\n\n\tif (notify_die(DIE_TRAP, str, regs, error_code, trapnr, SIGFPE) == NOTIFY_STOP)\n\t\treturn;\n\tconditional_sti(regs);\n\n\tif (!user_mode(regs)) {\n\t\tif (!fixup_exception(regs)) {\n\t\t\ttask->thread.error_code = error_code;\n\t\t\ttask->thread.trap_nr = trapnr;\n\t\t\tdie(str, regs, error_code);\n\t\t}\n\t\treturn;\n\t}\n\n\t/*\n\t * Save the info for the exception handler and clear the error.\n\t */\n\tfpu__save(fpu);\n\n\ttask->thread.trap_nr\t= trapnr;\n\ttask->thread.error_code = error_code;\n\tinfo.si_signo\t\t= SIGFPE;\n\tinfo.si_errno\t\t= 0;\n\tinfo.si_addr\t\t= (void __user *)uprobe_get_trap_addr(regs);\n\n\tinfo.si_code = fpu__exception_code(fpu, trapnr);\n\n\t/* Retry when we get spurious exceptions: */\n\tif (!info.si_code)\n\t\treturn;\n\n\tforce_sig_info(SIGFPE, &info, task);\n}\n'''\nRelated vulnerability code snippet:\n'''\ndotraplinkage void\ndo_general_protection(struct pt_regs *regs, long error_code)\n{\n\tstruct task_struct *tsk;\n\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(), \"entry code didn't wake RCU\");\n\tconditional_sti(regs);\n\n\tif (v8086_mode(regs)) {\n\t\tlocal_irq_enable();\n\t\thandle_vm86_fault((struct kernel_vm86_regs *) regs, error_code);\n\t\treturn;\n\t}\n\n\ttsk = current;\n\tif (!user_mode(regs)) {\n\t\tif (fixup_exception(regs))\n\t\t\treturn;\n\n\t\ttsk->thread.error_code = error_code;\n\t\ttsk->thread.trap_nr = X86_TRAP_GP;\n\t\tif (notify_die(DIE_GPF, \"general protection fault\", regs, error_code,\n\t\t\t       X86_TRAP_GP, SIGSEGV) != NOTIFY_STOP)\n\t\t\tdie(\"general protection fault\", regs, error_code);\n\t\treturn;\n\t}\n\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_nr = X86_TRAP_GP;\n\n\tif (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&\n\t\t\tprintk_ratelimit()) {\n\t\tpr_info(\"%s[%d] general protection ip:%lx sp:%lx error:%lx\",\n\t\t\ttsk->comm, task_pid_nr(tsk),\n\t\t\tregs->ip, regs->sp, error_code);\n\t\tprint_vma_addr(\" in \", regs->ip);\n\t\tpr_cont(\"\\n\");\n\t}\n\n\tforce_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\ndotraplinkage void\ndo_general_protection(struct pt_regs *regs, long error_code)\n{\n\tstruct task_struct *tsk;\n\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(), \"entry code didn't wake RCU\");\n\tconditional_sti(regs);\n\n\tif (v8086_mode(regs)) {\n\t\tlocal_irq_enable();\n\t\thandle_vm86_fault((struct kernel_vm86_regs *) regs, error_code);\n\t\treturn;\n\t}\n\n\ttsk = current;\n\tif (!user_mode(regs)) {\n\t\tif (fixup_exception(regs, X86_TRAP_GP))\n\t\t\treturn;\n\n\t\ttsk->thread.error_code = error_code;\n\t\ttsk->thread.trap_nr = X86_TRAP_GP;\n\t\tif (notify_die(DIE_GPF, \"general protection fault\", regs, error_code,\n\t\t\t       X86_TRAP_GP, SIGSEGV) != NOTIFY_STOP)\n\t\t\tdie(\"general protection fault\", regs, error_code);\n\t\treturn;\n\t}\n\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_nr = X86_TRAP_GP;\n\n\tif (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&\n\t\t\tprintk_ratelimit()) {\n\t\tpr_info(\"%s[%d] general protection ip:%lx sp:%lx error:%lx\",\n\t\t\ttsk->comm, task_pid_nr(tsk),\n\t\t\tregs->ip, regs->sp, error_code);\n\t\tprint_vma_addr(\" in \", regs->ip);\n\t\tpr_cont(\"\\n\");\n\t}\n\n\tforce_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static noinline void\nno_context(struct pt_regs *regs, unsigned long error_code,\n\t   unsigned long address, int signal, int si_code)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long flags;\n\tint sig;\n\n\t/* Are we prepared to handle this kernel fault? */\n\tif (fixup_exception(regs)) {\n\t\t/*\n\t\t * Any interrupt that takes a fault gets the fixup. This makes\n\t\t * the below recursive fault logic only apply to a faults from\n\t\t * task context.\n\t\t */\n\t\tif (in_interrupt())\n\t\t\treturn;\n\n\t\t/*\n\t\t * Per the above we're !in_interrupt(), aka. task context.\n\t\t *\n\t\t * In this case we need to make sure we're not recursively\n\t\t * faulting through the emulate_vsyscall() logic.\n\t\t */\n\t\tif (current_thread_info()->sig_on_uaccess_error && signal) {\n\t\t\ttsk->thread.trap_nr = X86_TRAP_PF;\n\t\t\ttsk->thread.error_code = error_code | PF_USER;\n\t\t\ttsk->thread.cr2 = address;\n\n\t\t\t/* XXX: hwpoison faults will set the wrong code. */\n\t\t\tforce_sig_info_fault(signal, si_code, address, tsk, 0);\n\t\t}\n\n\t\t/*\n\t\t * Barring that, we can do the fixup and be happy.\n\t\t */\n\t\treturn;\n\t}\n\n\t/*\n\t * 32-bit:\n\t *\n\t *   Valid to do another page fault here, because if this fault\n\t *   had been triggered by is_prefetch fixup_exception would have\n\t *   handled it.\n\t *\n\t * 64-bit:\n\t *\n\t *   Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n\t/*\n\t * Oops. The kernel tried to access some bad page. We'll have to\n\t * terminate things with extreme prejudice:\n\t */\n\tflags = oops_begin();\n\n\tshow_fault_oops(regs, error_code, address);\n\n\tif (task_stack_end_corrupted(tsk))\n\t\tprintk(KERN_EMERG \"Thread overran stack, or stack corrupted\\n\");\n\n\ttsk->thread.cr2\t\t= address;\n\ttsk->thread.trap_nr\t= X86_TRAP_PF;\n\ttsk->thread.error_code\t= error_code;\n\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_DEFAULT \"CR2: %016lx\\n\", address);\n\n\toops_end(flags, regs, sig);\n}",
                        "code_after_change": "static noinline void\nno_context(struct pt_regs *regs, unsigned long error_code,\n\t   unsigned long address, int signal, int si_code)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long flags;\n\tint sig;\n\n\t/* Are we prepared to handle this kernel fault? */\n\tif (fixup_exception(regs, X86_TRAP_PF)) {\n\t\t/*\n\t\t * Any interrupt that takes a fault gets the fixup. This makes\n\t\t * the below recursive fault logic only apply to a faults from\n\t\t * task context.\n\t\t */\n\t\tif (in_interrupt())\n\t\t\treturn;\n\n\t\t/*\n\t\t * Per the above we're !in_interrupt(), aka. task context.\n\t\t *\n\t\t * In this case we need to make sure we're not recursively\n\t\t * faulting through the emulate_vsyscall() logic.\n\t\t */\n\t\tif (current_thread_info()->sig_on_uaccess_error && signal) {\n\t\t\ttsk->thread.trap_nr = X86_TRAP_PF;\n\t\t\ttsk->thread.error_code = error_code | PF_USER;\n\t\t\ttsk->thread.cr2 = address;\n\n\t\t\t/* XXX: hwpoison faults will set the wrong code. */\n\t\t\tforce_sig_info_fault(signal, si_code, address, tsk, 0);\n\t\t}\n\n\t\t/*\n\t\t * Barring that, we can do the fixup and be happy.\n\t\t */\n\t\treturn;\n\t}\n\n\t/*\n\t * 32-bit:\n\t *\n\t *   Valid to do another page fault here, because if this fault\n\t *   had been triggered by is_prefetch fixup_exception would have\n\t *   handled it.\n\t *\n\t * 64-bit:\n\t *\n\t *   Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n\t/*\n\t * Oops. The kernel tried to access some bad page. We'll have to\n\t * terminate things with extreme prejudice:\n\t */\n\tflags = oops_begin();\n\n\tshow_fault_oops(regs, error_code, address);\n\n\tif (task_stack_end_corrupted(tsk))\n\t\tprintk(KERN_EMERG \"Thread overran stack, or stack corrupted\\n\");\n\n\ttsk->thread.cr2\t\t= address;\n\ttsk->thread.trap_nr\t= X86_TRAP_PF;\n\ttsk->thread.error_code\t= error_code;\n\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_DEFAULT \"CR2: %016lx\\n\", address);\n\n\toops_end(flags, regs, sig);\n}",
                        "cve_id": "CVE-2016-9644"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic void math_error(struct pt_regs *regs, int error_code, int trapnr)\n{\n\tstruct task_struct *task = current;\n\tstruct fpu *fpu = &task->thread.fpu;\n\tsiginfo_t info;\n\tchar *str = (trapnr == X86_TRAP_MF) ? \"fpu exception\" :\n\t\t\t\t\t\t\"simd exception\";\n\n\tif (notify_die(DIE_TRAP, str, regs, error_code, trapnr, SIGFPE) == NOTIFY_STOP)\n\t\treturn;\n\tconditional_sti(regs);\n\n\tif (!user_mode(regs)) {\n\t\tif (!fixup_exception(regs)) {\n\t\t\ttask->thread.error_code = error_code;\n\t\t\ttask->thread.trap_nr = trapnr;\n\t\t\tdie(str, regs, error_code);\n\t\t}\n\t\treturn;\n\t}\n\n\t/*\n\t * Save the info for the exception handler and clear the error.\n\t */\n\tfpu__save(fpu);\n\n\ttask->thread.trap_nr\t= trapnr;\n\ttask->thread.error_code = error_code;\n\tinfo.si_signo\t\t= SIGFPE;\n\tinfo.si_errno\t\t= 0;\n\tinfo.si_addr\t\t= (void __user *)uprobe_get_trap_addr(regs);\n\n\tinfo.si_code = fpu__exception_code(fpu, trapnr);\n\n\t/* Retry when we get spurious exceptions: */\n\tif (!info.si_code)\n\t\treturn;\n\n\tforce_sig_info(SIGFPE, &info, task);\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic noinline void\nno_context(struct pt_regs *regs, unsigned long error_code,\n\t   unsigned long address, int signal, int si_code)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long flags;\n\tint sig;\n\n\t/* Are we prepared to handle this kernel fault? */\n\tif (fixup_exception(regs)) {\n\t\t/*\n\t\t * Any interrupt that takes a fault gets the fixup. This makes\n\t\t * the below recursive fault logic only apply to a faults from\n\t\t * task context.\n\t\t */\n\t\tif (in_interrupt())\n\t\t\treturn;\n\n\t\t/*\n\t\t * Per the above we're !in_interrupt(), aka. task context.\n\t\t *\n\t\t * In this case we need to make sure we're not recursively\n\t\t * faulting through the emulate_vsyscall() logic.\n\t\t */\n\t\tif (current_thread_info()->sig_on_uaccess_error && signal) {\n\t\t\ttsk->thread.trap_nr = X86_TRAP_PF;\n\t\t\ttsk->thread.error_code = error_code | PF_USER;\n\t\t\ttsk->thread.cr2 = address;\n\n\t\t\t/* XXX: hwpoison faults will set the wrong code. */\n\t\t\tforce_sig_info_fault(signal, si_code, address, tsk, 0);\n\t\t}\n\n\t\t/*\n\t\t * Barring that, we can do the fixup and be happy.\n\t\t */\n\t\treturn;\n\t}\n\n\t/*\n\t * 32-bit:\n\t *\n\t *   Valid to do another page fault here, because if this fault\n\t *   had been triggered by is_prefetch fixup_exception would have\n\t *   handled it.\n\t *\n\t * 64-bit:\n\t *\n\t *   Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n\t/*\n\t * Oops. The kernel tried to access some bad page. We'll have to\n\t * terminate things with extreme prejudice:\n\t */\n\tflags = oops_begin();\n\n\tshow_fault_oops(regs, error_code, address);\n\n\tif (task_stack_end_corrupted(tsk))\n\t\tprintk(KERN_EMERG \"Thread overran stack, or stack corrupted\\n\");\n\n\ttsk->thread.cr2\t\t= address;\n\ttsk->thread.trap_nr\t= X86_TRAP_PF;\n\ttsk->thread.error_code\t= error_code;\n\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_DEFAULT \"CR2: %016lx\\n\", address);\n\n\toops_end(flags, regs, sig);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic noinline void\nno_context(struct pt_regs *regs, unsigned long error_code,\n\t   unsigned long address, int signal, int si_code)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long flags;\n\tint sig;\n\n\t/* Are we prepared to handle this kernel fault? */\n\tif (fixup_exception(regs, X86_TRAP_PF)) {\n\t\t/*\n\t\t * Any interrupt that takes a fault gets the fixup. This makes\n\t\t * the below recursive fault logic only apply to a faults from\n\t\t * task context.\n\t\t */\n\t\tif (in_interrupt())\n\t\t\treturn;\n\n\t\t/*\n\t\t * Per the above we're !in_interrupt(), aka. task context.\n\t\t *\n\t\t * In this case we need to make sure we're not recursively\n\t\t * faulting through the emulate_vsyscall() logic.\n\t\t */\n\t\tif (current_thread_info()->sig_on_uaccess_error && signal) {\n\t\t\ttsk->thread.trap_nr = X86_TRAP_PF;\n\t\t\ttsk->thread.error_code = error_code | PF_USER;\n\t\t\ttsk->thread.cr2 = address;\n\n\t\t\t/* XXX: hwpoison faults will set the wrong code. */\n\t\t\tforce_sig_info_fault(signal, si_code, address, tsk, 0);\n\t\t}\n\n\t\t/*\n\t\t * Barring that, we can do the fixup and be happy.\n\t\t */\n\t\treturn;\n\t}\n\n\t/*\n\t * 32-bit:\n\t *\n\t *   Valid to do another page fault here, because if this fault\n\t *   had been triggered by is_prefetch fixup_exception would have\n\t *   handled it.\n\t *\n\t * 64-bit:\n\t *\n\t *   Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n\t/*\n\t * Oops. The kernel tried to access some bad page. We'll have to\n\t * terminate things with extreme prejudice:\n\t */\n\tflags = oops_begin();\n\n\tshow_fault_oops(regs, error_code, address);\n\n\tif (task_stack_end_corrupted(tsk))\n\t\tprintk(KERN_EMERG \"Thread overran stack, or stack corrupted\\n\");\n\n\ttsk->thread.cr2\t\t= address;\n\ttsk->thread.trap_nr\t= X86_TRAP_PF;\n\ttsk->thread.error_code\t= error_code;\n\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_DEFAULT \"CR2: %016lx\\n\", address);\n\n\toops_end(flags, regs, sig);\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 1039,
            "cve_id": "CVE-2016-4997",
            "code_snippet": "static inline int\ncheck_compat_entry_size_and_hooks(struct compat_arpt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_arpt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e, e->target_offset,\n\t\t\t\t\t    e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct arpt_entry) - sizeof(struct compat_arpt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\n\tt = compat_arpt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_ARP, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto out;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(NFPROTO_ARP, entry_offset, off);\n\tif (ret)\n\t\tgoto release_target;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nrelease_target:\n\tmodule_put(t->u.kernel.target->me);\nout:\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int\ncheck_compat_entry_size_and_hooks(struct compat_ip6t_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ip6t_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ip6t_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ip6t_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip6_checkentry(&e->ipv6))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ip6t_entry) - sizeof(struct compat_ip6t_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ipv6, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ip6t_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV6, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET6, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}",
                        "code_after_change": "static int\ncheck_compat_entry_size_and_hooks(struct compat_ip6t_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ip6t_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ip6t_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ip6t_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip6_checkentry(&e->ipv6))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e, e->elems,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ip6t_entry) - sizeof(struct compat_ip6t_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ipv6, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ip6t_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV6, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET6, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}",
                        "cve_id": "CVE-2016-4997"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic inline int\ncheck_compat_entry_size_and_hooks(struct compat_arpt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_arpt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e, e->target_offset,\n\t\t\t\t\t    e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct arpt_entry) - sizeof(struct compat_arpt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\n\tt = compat_arpt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_ARP, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto out;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(NFPROTO_ARP, entry_offset, off);\n\tif (ret)\n\t\tgoto release_target;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nrelease_target:\n\tmodule_put(t->u.kernel.target->me);\nout:\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int\ncheck_compat_entry_size_and_hooks(struct compat_ip6t_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ip6t_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ip6t_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ip6t_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip6_checkentry(&e->ipv6))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ip6t_entry) - sizeof(struct compat_ip6t_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ipv6, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ip6t_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV6, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET6, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int\ncheck_compat_entry_size_and_hooks(struct compat_ip6t_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ip6t_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ip6t_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ip6t_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip6_checkentry(&e->ipv6))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e, e->elems,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ip6t_entry) - sizeof(struct compat_ip6t_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ipv6, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ip6t_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV6, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET6, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int\ncheck_compat_entry_size_and_hooks(struct compat_ipt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ipt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ipt_entry) - sizeof(struct compat_ipt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ip, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ipt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV4, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}",
                        "code_after_change": "static int\ncheck_compat_entry_size_and_hooks(struct compat_ipt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ipt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e, e->elems,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ipt_entry) - sizeof(struct compat_ipt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ip, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ipt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV4, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}",
                        "cve_id": "CVE-2016-4997"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic inline int\ncheck_compat_entry_size_and_hooks(struct compat_arpt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_arpt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e, e->target_offset,\n\t\t\t\t\t    e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct arpt_entry) - sizeof(struct compat_arpt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\n\tt = compat_arpt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_ARP, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto out;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(NFPROTO_ARP, entry_offset, off);\n\tif (ret)\n\t\tgoto release_target;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nrelease_target:\n\tmodule_put(t->u.kernel.target->me);\nout:\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int\ncheck_compat_entry_size_and_hooks(struct compat_ipt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ipt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ipt_entry) - sizeof(struct compat_ipt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ip, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ipt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV4, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int\ncheck_compat_entry_size_and_hooks(struct compat_ipt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ipt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e, e->elems,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ipt_entry) - sizeof(struct compat_ipt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ip, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ipt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV4, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static inline int check_entry_size_and_hooks(struct arpt_entry *e,\n\t\t\t\t\t     struct xt_table_info *newinfo,\n\t\t\t\t\t     const unsigned char *base,\n\t\t\t\t\t     const unsigned char *limit,\n\t\t\t\t\t     const unsigned int *hook_entries,\n\t\t\t\t\t     const unsigned int *underflows,\n\t\t\t\t\t     unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct arpt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}",
                        "code_after_change": "static inline int check_entry_size_and_hooks(struct arpt_entry *e,\n\t\t\t\t\t     struct xt_table_info *newinfo,\n\t\t\t\t\t     const unsigned char *base,\n\t\t\t\t\t     const unsigned char *limit,\n\t\t\t\t\t     const unsigned int *hook_entries,\n\t\t\t\t\t     const unsigned int *underflows,\n\t\t\t\t\t     unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct arpt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,\n\t\t\t\t     e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}",
                        "cve_id": "CVE-2016-4997"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic inline int\ncheck_compat_entry_size_and_hooks(struct compat_arpt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_arpt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e, e->target_offset,\n\t\t\t\t\t    e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct arpt_entry) - sizeof(struct compat_arpt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\n\tt = compat_arpt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_ARP, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto out;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(NFPROTO_ARP, entry_offset, off);\n\tif (ret)\n\t\tgoto release_target;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nrelease_target:\n\tmodule_put(t->u.kernel.target->me);\nout:\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic inline int check_entry_size_and_hooks(struct arpt_entry *e,\n\t\t\t\t\t     struct xt_table_info *newinfo,\n\t\t\t\t\t     const unsigned char *base,\n\t\t\t\t\t     const unsigned char *limit,\n\t\t\t\t\t     const unsigned int *hook_entries,\n\t\t\t\t\t     const unsigned int *underflows,\n\t\t\t\t\t     unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct arpt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic inline int check_entry_size_and_hooks(struct arpt_entry *e,\n\t\t\t\t\t     struct xt_table_info *newinfo,\n\t\t\t\t\t     const unsigned char *base,\n\t\t\t\t\t     const unsigned char *limit,\n\t\t\t\t\t     const unsigned int *hook_entries,\n\t\t\t\t\t     const unsigned int *underflows,\n\t\t\t\t\t     unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct arpt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,\n\t\t\t\t     e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 351,
            "cve_id": "CVE-2013-6383",
            "code_snippet": "static int aac_compat_ioctl(struct scsi_device *sdev, int cmd, void __user *arg)\n{\n\tstruct aac_dev *dev = (struct aac_dev *)sdev->host->hostdata;\n\treturn aac_compat_do_ioctl(dev, cmd, (unsigned long)arg);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tion_free(client, handle);\n\t\tion_handle_put(handle);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}",
                        "code_after_change": "static long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\tmutex_lock(&client->lock);\n\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n\t\tif (IS_ERR(handle)) {\n\t\t\tmutex_unlock(&client->lock);\n\t\t\treturn PTR_ERR(handle);\n\t\t}\n\t\tion_free_nolock(client, handle);\n\t\tion_handle_put_nolock(handle);\n\t\tmutex_unlock(&client->lock);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}",
                        "cve_id": "CVE-2016-9120"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int aac_compat_ioctl(struct scsi_device *sdev, int cmd, void __user *arg)\n{\n\tstruct aac_dev *dev = (struct aac_dev *)sdev->host->hostdata;\n\treturn aac_compat_do_ioctl(dev, cmd, (unsigned long)arg);\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tion_free(client, handle);\n\t\tion_handle_put(handle);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\tmutex_lock(&client->lock);\n\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n\t\tif (IS_ERR(handle)) {\n\t\t\tmutex_unlock(&client->lock);\n\t\t\treturn PTR_ERR(handle);\n\t\t}\n\t\tion_free_nolock(client, handle);\n\t\tion_handle_put_nolock(handle);\n\t\tmutex_unlock(&client->lock);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "asmlinkage long sys_oabi_fcntl64(unsigned int fd, unsigned int cmd,\n\t\t\t\t unsigned long arg)\n{\n\tstruct oabi_flock64 user;\n\tstruct flock64 kernel;\n\tmm_segment_t fs = USER_DS; /* initialized to kill a warning */\n\tunsigned long local_arg = arg;\n\tint ret;\n\n\tswitch (cmd) {\n\tcase F_OFD_GETLK:\n\tcase F_OFD_SETLK:\n\tcase F_OFD_SETLKW:\n\tcase F_GETLK64:\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\tif (copy_from_user(&user, (struct oabi_flock64 __user *)arg,\n\t\t\t\t   sizeof(user)))\n\t\t\treturn -EFAULT;\n\t\tkernel.l_type\t= user.l_type;\n\t\tkernel.l_whence\t= user.l_whence;\n\t\tkernel.l_start\t= user.l_start;\n\t\tkernel.l_len\t= user.l_len;\n\t\tkernel.l_pid\t= user.l_pid;\n\t\tlocal_arg = (unsigned long)&kernel;\n\t\tfs = get_fs();\n\t\tset_fs(KERNEL_DS);\n\t}\n\n\tret = sys_fcntl64(fd, cmd, local_arg);\n\n\tswitch (cmd) {\n\tcase F_GETLK64:\n\t\tif (!ret) {\n\t\t\tuser.l_type\t= kernel.l_type;\n\t\t\tuser.l_whence\t= kernel.l_whence;\n\t\t\tuser.l_start\t= kernel.l_start;\n\t\t\tuser.l_len\t= kernel.l_len;\n\t\t\tuser.l_pid\t= kernel.l_pid;\n\t\t\tif (copy_to_user((struct oabi_flock64 __user *)arg,\n\t\t\t\t\t &user, sizeof(user)))\n\t\t\t\tret = -EFAULT;\n\t\t}\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\tset_fs(fs);\n\t}\n\n\treturn ret;\n}",
                        "code_after_change": "asmlinkage long sys_oabi_fcntl64(unsigned int fd, unsigned int cmd,\n\t\t\t\t unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase F_OFD_GETLK:\n\tcase F_OFD_SETLK:\n\tcase F_OFD_SETLKW:\n\tcase F_GETLK64:\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\treturn do_locks(fd, cmd, arg);\n\n\tdefault:\n\t\treturn sys_fcntl64(fd, cmd, arg);\n\t}\n}",
                        "cve_id": "CVE-2015-8966"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int aac_compat_ioctl(struct scsi_device *sdev, int cmd, void __user *arg)\n{\n\tstruct aac_dev *dev = (struct aac_dev *)sdev->host->hostdata;\n\treturn aac_compat_do_ioctl(dev, cmd, (unsigned long)arg);\n}\n'''\nRelated vulnerability code snippet:\n'''\nasmlinkage long sys_oabi_fcntl64(unsigned int fd, unsigned int cmd,\n\t\t\t\t unsigned long arg)\n{\n\tstruct oabi_flock64 user;\n\tstruct flock64 kernel;\n\tmm_segment_t fs = USER_DS; /* initialized to kill a warning */\n\tunsigned long local_arg = arg;\n\tint ret;\n\n\tswitch (cmd) {\n\tcase F_OFD_GETLK:\n\tcase F_OFD_SETLK:\n\tcase F_OFD_SETLKW:\n\tcase F_GETLK64:\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\tif (copy_from_user(&user, (struct oabi_flock64 __user *)arg,\n\t\t\t\t   sizeof(user)))\n\t\t\treturn -EFAULT;\n\t\tkernel.l_type\t= user.l_type;\n\t\tkernel.l_whence\t= user.l_whence;\n\t\tkernel.l_start\t= user.l_start;\n\t\tkernel.l_len\t= user.l_len;\n\t\tkernel.l_pid\t= user.l_pid;\n\t\tlocal_arg = (unsigned long)&kernel;\n\t\tfs = get_fs();\n\t\tset_fs(KERNEL_DS);\n\t}\n\n\tret = sys_fcntl64(fd, cmd, local_arg);\n\n\tswitch (cmd) {\n\tcase F_GETLK64:\n\t\tif (!ret) {\n\t\t\tuser.l_type\t= kernel.l_type;\n\t\t\tuser.l_whence\t= kernel.l_whence;\n\t\t\tuser.l_start\t= kernel.l_start;\n\t\t\tuser.l_len\t= kernel.l_len;\n\t\t\tuser.l_pid\t= kernel.l_pid;\n\t\t\tif (copy_to_user((struct oabi_flock64 __user *)arg,\n\t\t\t\t\t &user, sizeof(user)))\n\t\t\t\tret = -EFAULT;\n\t\t}\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\tset_fs(fs);\n\t}\n\n\treturn ret;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nasmlinkage long sys_oabi_fcntl64(unsigned int fd, unsigned int cmd,\n\t\t\t\t unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase F_OFD_GETLK:\n\tcase F_OFD_SETLK:\n\tcase F_OFD_SETLKW:\n\tcase F_GETLK64:\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\treturn do_locks(fd, cmd, arg);\n\n\tdefault:\n\t\treturn sys_fcntl64(fd, cmd, arg);\n\t}\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int\nrio_ioctl (struct net_device *dev, struct ifreq *rq, int cmd)\n{\n\tint phy_addr;\n\tstruct netdev_private *np = netdev_priv(dev);\n\tstruct mii_data *miidata = (struct mii_data *) &rq->ifr_ifru;\n\n\tstruct netdev_desc *desc;\n\tint i;\n\n\tphy_addr = np->phy_addr;\n\tswitch (cmd) {\n\tcase SIOCDEVPRIVATE:\n\t\tbreak;\n\n\tcase SIOCDEVPRIVATE + 1:\n\t\tmiidata->out_value = mii_read (dev, phy_addr, miidata->reg_num);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 2:\n\t\tmii_write (dev, phy_addr, miidata->reg_num, miidata->in_value);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 3:\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 4:\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 5:\n\t\tnetif_stop_queue (dev);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 6:\n\t\tnetif_wake_queue (dev);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 7:\n\t\tprintk\n\t\t    (\"tx_full=%x cur_tx=%lx old_tx=%lx cur_rx=%lx old_rx=%lx\\n\",\n\t\t     netif_queue_stopped(dev), np->cur_tx, np->old_tx, np->cur_rx,\n\t\t     np->old_rx);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 8:\n\t\tprintk(\"TX ring:\\n\");\n\t\tfor (i = 0; i < TX_RING_SIZE; i++) {\n\t\t\tdesc = &np->tx_ring[i];\n\t\t\tprintk\n\t\t\t    (\"%02x:cur:%08x next:%08x status:%08x frag1:%08x frag0:%08x\",\n\t\t\t     i,\n\t\t\t     (u32) (np->tx_ring_dma + i * sizeof (*desc)),\n\t\t\t     (u32)le64_to_cpu(desc->next_desc),\n\t\t\t     (u32)le64_to_cpu(desc->status),\n\t\t\t     (u32)(le64_to_cpu(desc->fraginfo) >> 32),\n\t\t\t     (u32)le64_to_cpu(desc->fraginfo));\n\t\t\tprintk (\"\\n\");\n\t\t}\n\t\tprintk (\"\\n\");\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\treturn 0;\n}",
                        "code_after_change": "static int\nrio_ioctl (struct net_device *dev, struct ifreq *rq, int cmd)\n{\n\tint phy_addr;\n\tstruct netdev_private *np = netdev_priv(dev);\n\tstruct mii_ioctl_data *miidata = if_mii(rq);\n\n\tphy_addr = np->phy_addr;\n\tswitch (cmd) {\n\tcase SIOCGMIIPHY:\n\t\tmiidata->phy_id = phy_addr;\n\t\tbreak;\n\tcase SIOCGMIIREG:\n\t\tmiidata->val_out = mii_read (dev, phy_addr, miidata->reg_num);\n\t\tbreak;\n\tcase SIOCSMIIREG:\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\treturn -EPERM;\n\t\tmii_write (dev, phy_addr, miidata->reg_num, miidata->val_in);\n\t\tbreak;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\treturn 0;\n}",
                        "cve_id": "CVE-2012-2313"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int aac_compat_ioctl(struct scsi_device *sdev, int cmd, void __user *arg)\n{\n\tstruct aac_dev *dev = (struct aac_dev *)sdev->host->hostdata;\n\treturn aac_compat_do_ioctl(dev, cmd, (unsigned long)arg);\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int\nrio_ioctl (struct net_device *dev, struct ifreq *rq, int cmd)\n{\n\tint phy_addr;\n\tstruct netdev_private *np = netdev_priv(dev);\n\tstruct mii_data *miidata = (struct mii_data *) &rq->ifr_ifru;\n\n\tstruct netdev_desc *desc;\n\tint i;\n\n\tphy_addr = np->phy_addr;\n\tswitch (cmd) {\n\tcase SIOCDEVPRIVATE:\n\t\tbreak;\n\n\tcase SIOCDEVPRIVATE + 1:\n\t\tmiidata->out_value = mii_read (dev, phy_addr, miidata->reg_num);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 2:\n\t\tmii_write (dev, phy_addr, miidata->reg_num, miidata->in_value);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 3:\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 4:\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 5:\n\t\tnetif_stop_queue (dev);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 6:\n\t\tnetif_wake_queue (dev);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 7:\n\t\tprintk\n\t\t    (\"tx_full=%x cur_tx=%lx old_tx=%lx cur_rx=%lx old_rx=%lx\\n\",\n\t\t     netif_queue_stopped(dev), np->cur_tx, np->old_tx, np->cur_rx,\n\t\t     np->old_rx);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 8:\n\t\tprintk(\"TX ring:\\n\");\n\t\tfor (i = 0; i < TX_RING_SIZE; i++) {\n\t\t\tdesc = &np->tx_ring[i];\n\t\t\tprintk\n\t\t\t    (\"%02x:cur:%08x next:%08x status:%08x frag1:%08x frag0:%08x\",\n\t\t\t     i,\n\t\t\t     (u32) (np->tx_ring_dma + i * sizeof (*desc)),\n\t\t\t     (u32)le64_to_cpu(desc->next_desc),\n\t\t\t     (u32)le64_to_cpu(desc->status),\n\t\t\t     (u32)(le64_to_cpu(desc->fraginfo) >> 32),\n\t\t\t     (u32)le64_to_cpu(desc->fraginfo));\n\t\t\tprintk (\"\\n\");\n\t\t}\n\t\tprintk (\"\\n\");\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int\nrio_ioctl (struct net_device *dev, struct ifreq *rq, int cmd)\n{\n\tint phy_addr;\n\tstruct netdev_private *np = netdev_priv(dev);\n\tstruct mii_ioctl_data *miidata = if_mii(rq);\n\n\tphy_addr = np->phy_addr;\n\tswitch (cmd) {\n\tcase SIOCGMIIPHY:\n\t\tmiidata->phy_id = phy_addr;\n\t\tbreak;\n\tcase SIOCGMIIREG:\n\t\tmiidata->val_out = mii_read (dev, phy_addr, miidata->reg_num);\n\t\tbreak;\n\tcase SIOCSMIIREG:\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\treturn -EPERM;\n\t\tmii_write (dev, phy_addr, miidata->reg_num, miidata->val_in);\n\t\tbreak;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 839,
            "cve_id": "CVE-2015-8709",
            "code_snippet": "static int ptrace_attach(struct task_struct *task, long request,\n\t\t\t unsigned long addr,\n\t\t\t unsigned long flags)\n{\n\tbool seize = (request == PTRACE_SEIZE);\n\tint retval;\n\n\tretval = -EIO;\n\tif (seize) {\n\t\tif (addr != 0)\n\t\t\tgoto out;\n\t\tif (flags & ~(unsigned long)PTRACE_O_MASK)\n\t\t\tgoto out;\n\t\tflags = PT_PTRACED | PT_SEIZED | (flags << PT_OPT_FLAG_SHIFT);\n\t} else {\n\t\tflags = PT_PTRACED;\n\t}\n\n\taudit_ptrace(task);\n\n\tretval = -EPERM;\n\tif (unlikely(task->flags & PF_KTHREAD))\n\t\tgoto out;\n\tif (same_thread_group(task, current))\n\t\tgoto out;\n\n\t/*\n\t * Protect exec's credential calculations against our interference;\n\t * SUID, SGID and LSM creds get determined differently\n\t * under ptrace.\n\t */\n\tretval = -ERESTARTNOINTR;\n\tif (mutex_lock_interruptible(&task->signal->cred_guard_mutex))\n\t\tgoto out;\n\n\ttask_lock(task);\n\tretval = __ptrace_may_access(task, PTRACE_MODE_ATTACH_REALCREDS);\n\ttask_unlock(task);\n\tif (retval)\n\t\tgoto unlock_creds;\n\n\twrite_lock_irq(&tasklist_lock);\n\tretval = -EPERM;\n\tif (unlikely(task->exit_state))\n\t\tgoto unlock_tasklist;\n\tif (task->ptrace)\n\t\tgoto unlock_tasklist;\n\n\tif (seize)\n\t\tflags |= PT_SEIZED;\n\trcu_read_lock();\n\tif (ns_capable(__task_cred(task)->user_ns, CAP_SYS_PTRACE))\n\t\tflags |= PT_PTRACE_CAP;\n\trcu_read_unlock();\n\ttask->ptrace = flags;\n\n\t__ptrace_link(task, current);\n\n\t/* SEIZE doesn't trap tracee on attach */\n\tif (!seize)\n\t\tsend_sig_info(SIGSTOP, SEND_SIG_FORCED, task);\n\n\tspin_lock(&task->sighand->siglock);\n\n\t/*\n\t * If the task is already STOPPED, set JOBCTL_TRAP_STOP and\n\t * TRAPPING, and kick it so that it transits to TRACED.  TRAPPING\n\t * will be cleared if the child completes the transition or any\n\t * event which clears the group stop states happens.  We'll wait\n\t * for the transition to complete before returning from this\n\t * function.\n\t *\n\t * This hides STOPPED -> RUNNING -> TRACED transition from the\n\t * attaching thread but a different thread in the same group can\n\t * still observe the transient RUNNING state.  IOW, if another\n\t * thread's WNOHANG wait(2) on the stopped tracee races against\n\t * ATTACH, the wait(2) may fail due to the transient RUNNING.\n\t *\n\t * The following task_is_stopped() test is safe as both transitions\n\t * in and out of STOPPED are protected by siglock.\n\t */\n\tif (task_is_stopped(task) &&\n\t    task_set_jobctl_pending(task, JOBCTL_TRAP_STOP | JOBCTL_TRAPPING))\n\t\tsignal_wake_up_state(task, __TASK_STOPPED);\n\n\tspin_unlock(&task->sighand->siglock);\n\n\tretval = 0;\nunlock_tasklist:\n\twrite_unlock_irq(&tasklist_lock);\nunlock_creds:\n\tmutex_unlock(&task->signal->cred_guard_mutex);\nout:\n\tif (!retval) {\n\t\t/*\n\t\t * We do not bother to change retval or clear JOBCTL_TRAPPING\n\t\t * if wait_on_bit() was interrupted by SIGKILL. The tracer will\n\t\t * not return to user-mode, it will exit and clear this bit in\n\t\t * __ptrace_unlink() if it wasn't already cleared by the tracee;\n\t\t * and until then nobody can ptrace this task.\n\t\t */\n\t\twait_on_bit(&task->jobctl, JOBCTL_TRAPPING_BIT, TASK_KILLABLE);\n\t\tproc_ptrace_connector(task, PTRACE_ATTACH);\n\t}\n\n\treturn retval;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static struct task_struct *copy_process(unsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace)\n{\n\tint retval;\n\tstruct task_struct *p;\n\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid namespace\n\t * don't allow the creation of threads.\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_NEWPID)) &&\n\t    (task_active_pid_ns(current) != current->nsproxy->pid_ns))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tretval = security_task_create(clone_flags);\n\tif (retval)\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current);\n\tif (!p)\n\t\tgoto fork_out;\n\n\tftrace_graph_init_task(p);\n\tget_seccomp_filter(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&\n\t\t    p->real_cred->user != INIT_USER)\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tif (!try_module_get(task_thread_info(p)->exec_domain->module))\n\t\tgoto bad_fork_cleanup_count;\n\n\tp->did_exec = 0;\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tcopy_flags(clone_flags, p);\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n\tp->utimescaled = p->stimescaled = 0;\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING\n\tp->prev_cputime.utime = p->prev_cputime.stime = 0;\n#endif\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqlock_init(&p->vtime_seqlock);\n\tp->vtime_snap = 0;\n\tp->vtime_snap_whence = VTIME_SLEEPING;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tdo_posix_clock_monotonic_gettime(&p->start_time);\n\tp->real_start_time = p->start_time;\n\tmonotonic_to_bootbased(&p->real_start_time);\n\tp->io_context = NULL;\n\tp->audit_context = NULL;\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_begin(current);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_cgroup;\n\t}\n\tmpol_fix_fork_child_flag(p);\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_init(&p->mems_allowed_seq);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n\tp->hardirqs_enabled = 0;\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_MEMCG\n\tp->memcg_batch.do_batch = 0;\n\tp->memcg_batch.memcg = NULL;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tsched_fork(p);\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\t/* copy all the process information */\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(clone_flags, stack_start, stack_size, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tif (pid != &init_struct_pid) {\n\t\tretval = -ENOMEM;\n\t\tpid = alloc_pid(p->nsproxy->pid_ns);\n\t\tif (!pid)\n\t\t\tgoto bad_fork_cleanup_io;\n\t}\n\n\tp->pid = pid_nr(pid);\n\tp->tgid = p->pid;\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->tgid = current->tgid;\n\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\tuprobe_copy_process(p);\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tp->sas_ss_sp = p->sas_ss_size = 0;\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->exit_signal = -1;\n\telse if (clone_flags & CLONE_PARENT)\n\t\tp->exit_signal = current->group_leader->exit_signal;\n\telse\n\t\tp->exit_signal = (clone_flags & CSIGNAL);\n\n\tp->pdeath_signal = 0;\n\tp->exit_state = 0;\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\t/*\n\t * Ok, make it visible to the rest of the system.\n\t * We dont wake it up yet.\n\t */\n\tp->group_leader = p;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\t/* Need tasklist lock for parent etc handling! */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Process group and session signals need to be delivered to just the\n\t * parent before the fork or both the parent and the child after the\n\t * fork. Restart if a signal comes in before we add the new process to\n\t * it's process group.\n\t * A fatal signal pending means that current will exit, so the new\n\t * thread can't slip out of an OOM kill (or normal SIGKILL).\n\t*/\n\trecalc_sigpending();\n\tif (signal_pending(current)) {\n\t\tspin_unlock(&current->sighand->siglock);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t\tretval = -ERESTARTNOINTR;\n\t\tgoto bad_fork_free_pid;\n\t}\n\n\tif (clone_flags & CLONE_THREAD) {\n\t\tcurrent->signal->nr_threads++;\n\t\tatomic_inc(&current->signal->live);\n\t\tatomic_inc(&current->signal->sigcnt);\n\t\tp->group_leader = current->group_leader;\n\t\tlist_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);\n\t}\n\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tif (thread_group_leader(p)) {\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\n\t\t\tp->signal->leader_pid = pid;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\tattach_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tattach_pid(p, PIDTYPE_SID, task_session(current));\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID, pid);\n\t\tnr_threads++;\n\t}\n\n\ttotal_forks++;\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\n\treturn p;\n\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm)\n\t\tmmput(p->mm);\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_policy:\n\tperf_event_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_cgroup:\n#endif\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tcgroup_exit(p, 0);\n\tdelayacct_tsk_free(p);\n\tmodule_put(task_thread_info(p)->exec_domain->module);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tfree_task(p);\nfork_out:\n\treturn ERR_PTR(retval);\n}",
                        "code_after_change": " */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *uninitialized_var(gctx);\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tgctx = group_leader->ctx;\n\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t} else {\n\t\tmutex_lock(&ctx->mutex);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\tif (move_group) {\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\n\tif (move_group) {\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}",
                        "cve_id": "CVE-2013-1858"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int ptrace_attach(struct task_struct *task, long request,\n\t\t\t unsigned long addr,\n\t\t\t unsigned long flags)\n{\n\tbool seize = (request == PTRACE_SEIZE);\n\tint retval;\n\n\tretval = -EIO;\n\tif (seize) {\n\t\tif (addr != 0)\n\t\t\tgoto out;\n\t\tif (flags & ~(unsigned long)PTRACE_O_MASK)\n\t\t\tgoto out;\n\t\tflags = PT_PTRACED | PT_SEIZED | (flags << PT_OPT_FLAG_SHIFT);\n\t} else {\n\t\tflags = PT_PTRACED;\n\t}\n\n\taudit_ptrace(task);\n\n\tretval = -EPERM;\n\tif (unlikely(task->flags & PF_KTHREAD))\n\t\tgoto out;\n\tif (same_thread_group(task, current))\n\t\tgoto out;\n\n\t/*\n\t * Protect exec's credential calculations against our interference;\n\t * SUID, SGID and LSM creds get determined differently\n\t * under ptrace.\n\t */\n\tretval = -ERESTARTNOINTR;\n\tif (mutex_lock_interruptible(&task->signal->cred_guard_mutex))\n\t\tgoto out;\n\n\ttask_lock(task);\n\tretval = __ptrace_may_access(task, PTRACE_MODE_ATTACH_REALCREDS);\n\ttask_unlock(task);\n\tif (retval)\n\t\tgoto unlock_creds;\n\n\twrite_lock_irq(&tasklist_lock);\n\tretval = -EPERM;\n\tif (unlikely(task->exit_state))\n\t\tgoto unlock_tasklist;\n\tif (task->ptrace)\n\t\tgoto unlock_tasklist;\n\n\tif (seize)\n\t\tflags |= PT_SEIZED;\n\trcu_read_lock();\n\tif (ns_capable(__task_cred(task)->user_ns, CAP_SYS_PTRACE))\n\t\tflags |= PT_PTRACE_CAP;\n\trcu_read_unlock();\n\ttask->ptrace = flags;\n\n\t__ptrace_link(task, current);\n\n\t/* SEIZE doesn't trap tracee on attach */\n\tif (!seize)\n\t\tsend_sig_info(SIGSTOP, SEND_SIG_FORCED, task);\n\n\tspin_lock(&task->sighand->siglock);\n\n\t/*\n\t * If the task is already STOPPED, set JOBCTL_TRAP_STOP and\n\t * TRAPPING, and kick it so that it transits to TRACED.  TRAPPING\n\t * will be cleared if the child completes the transition or any\n\t * event which clears the group stop states happens.  We'll wait\n\t * for the transition to complete before returning from this\n\t * function.\n\t *\n\t * This hides STOPPED -> RUNNING -> TRACED transition from the\n\t * attaching thread but a different thread in the same group can\n\t * still observe the transient RUNNING state.  IOW, if another\n\t * thread's WNOHANG wait(2) on the stopped tracee races against\n\t * ATTACH, the wait(2) may fail due to the transient RUNNING.\n\t *\n\t * The following task_is_stopped() test is safe as both transitions\n\t * in and out of STOPPED are protected by siglock.\n\t */\n\tif (task_is_stopped(task) &&\n\t    task_set_jobctl_pending(task, JOBCTL_TRAP_STOP | JOBCTL_TRAPPING))\n\t\tsignal_wake_up_state(task, __TASK_STOPPED);\n\n\tspin_unlock(&task->sighand->siglock);\n\n\tretval = 0;\nunlock_tasklist:\n\twrite_unlock_irq(&tasklist_lock);\nunlock_creds:\n\tmutex_unlock(&task->signal->cred_guard_mutex);\nout:\n\tif (!retval) {\n\t\t/*\n\t\t * We do not bother to change retval or clear JOBCTL_TRAPPING\n\t\t * if wait_on_bit() was interrupted by SIGKILL. The tracer will\n\t\t * not return to user-mode, it will exit and clear this bit in\n\t\t * __ptrace_unlink() if it wasn't already cleared by the tracee;\n\t\t * and until then nobody can ptrace this task.\n\t\t */\n\t\twait_on_bit(&task->jobctl, JOBCTL_TRAPPING_BIT, TASK_KILLABLE);\n\t\tproc_ptrace_connector(task, PTRACE_ATTACH);\n\t}\n\n\treturn retval;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic struct task_struct *copy_process(unsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace)\n{\n\tint retval;\n\tstruct task_struct *p;\n\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid namespace\n\t * don't allow the creation of threads.\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_NEWPID)) &&\n\t    (task_active_pid_ns(current) != current->nsproxy->pid_ns))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tretval = security_task_create(clone_flags);\n\tif (retval)\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current);\n\tif (!p)\n\t\tgoto fork_out;\n\n\tftrace_graph_init_task(p);\n\tget_seccomp_filter(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&\n\t\t    p->real_cred->user != INIT_USER)\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tif (!try_module_get(task_thread_info(p)->exec_domain->module))\n\t\tgoto bad_fork_cleanup_count;\n\n\tp->did_exec = 0;\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tcopy_flags(clone_flags, p);\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n\tp->utimescaled = p->stimescaled = 0;\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING\n\tp->prev_cputime.utime = p->prev_cputime.stime = 0;\n#endif\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqlock_init(&p->vtime_seqlock);\n\tp->vtime_snap = 0;\n\tp->vtime_snap_whence = VTIME_SLEEPING;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tdo_posix_clock_monotonic_gettime(&p->start_time);\n\tp->real_start_time = p->start_time;\n\tmonotonic_to_bootbased(&p->real_start_time);\n\tp->io_context = NULL;\n\tp->audit_context = NULL;\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_begin(current);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_cgroup;\n\t}\n\tmpol_fix_fork_child_flag(p);\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_init(&p->mems_allowed_seq);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n\tp->hardirqs_enabled = 0;\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_MEMCG\n\tp->memcg_batch.do_batch = 0;\n\tp->memcg_batch.memcg = NULL;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tsched_fork(p);\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\t/* copy all the process information */\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(clone_flags, stack_start, stack_size, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tif (pid != &init_struct_pid) {\n\t\tretval = -ENOMEM;\n\t\tpid = alloc_pid(p->nsproxy->pid_ns);\n\t\tif (!pid)\n\t\t\tgoto bad_fork_cleanup_io;\n\t}\n\n\tp->pid = pid_nr(pid);\n\tp->tgid = p->pid;\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->tgid = current->tgid;\n\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\tuprobe_copy_process(p);\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tp->sas_ss_sp = p->sas_ss_size = 0;\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->exit_signal = -1;\n\telse if (clone_flags & CLONE_PARENT)\n\t\tp->exit_signal = current->group_leader->exit_signal;\n\telse\n\t\tp->exit_signal = (clone_flags & CSIGNAL);\n\n\tp->pdeath_signal = 0;\n\tp->exit_state = 0;\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\t/*\n\t * Ok, make it visible to the rest of the system.\n\t * We dont wake it up yet.\n\t */\n\tp->group_leader = p;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\t/* Need tasklist lock for parent etc handling! */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Process group and session signals need to be delivered to just the\n\t * parent before the fork or both the parent and the child after the\n\t * fork. Restart if a signal comes in before we add the new process to\n\t * it's process group.\n\t * A fatal signal pending means that current will exit, so the new\n\t * thread can't slip out of an OOM kill (or normal SIGKILL).\n\t*/\n\trecalc_sigpending();\n\tif (signal_pending(current)) {\n\t\tspin_unlock(&current->sighand->siglock);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t\tretval = -ERESTARTNOINTR;\n\t\tgoto bad_fork_free_pid;\n\t}\n\n\tif (clone_flags & CLONE_THREAD) {\n\t\tcurrent->signal->nr_threads++;\n\t\tatomic_inc(&current->signal->live);\n\t\tatomic_inc(&current->signal->sigcnt);\n\t\tp->group_leader = current->group_leader;\n\t\tlist_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);\n\t}\n\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tif (thread_group_leader(p)) {\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\n\t\t\tp->signal->leader_pid = pid;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\tattach_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tattach_pid(p, PIDTYPE_SID, task_session(current));\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID, pid);\n\t\tnr_threads++;\n\t}\n\n\ttotal_forks++;\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\n\treturn p;\n\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm)\n\t\tmmput(p->mm);\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_policy:\n\tperf_event_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_cgroup:\n#endif\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tcgroup_exit(p, 0);\n\tdelayacct_tsk_free(p);\n\tmodule_put(task_thread_info(p)->exec_domain->module);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tfree_task(p);\nfork_out:\n\treturn ERR_PTR(retval);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\n */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *uninitialized_var(gctx);\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tgctx = group_leader->ctx;\n\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t} else {\n\t\tmutex_lock(&ctx->mutex);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\tif (move_group) {\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\n\tif (move_group) {\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int __ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tconst struct cred *cred = current_cred(), *tcred;\n\tint dumpable = 0;\n\tkuid_t caller_uid;\n\tkgid_t caller_gid;\n\n\tif (!(mode & PTRACE_MODE_FSCREDS) == !(mode & PTRACE_MODE_REALCREDS)) {\n\t\tWARN(1, \"denying ptrace access check without PTRACE_MODE_*CREDS\\n\");\n\t\treturn -EPERM;\n\t}\n\n\t/* May we inspect the given task?\n\t * This check is used both for attaching with ptrace\n\t * and for allowing access to sensitive information in /proc.\n\t *\n\t * ptrace_attach denies several cases that /proc allows\n\t * because setting up the necessary parent/child relationship\n\t * or halting the specified task is impossible.\n\t */\n\n\t/* Don't let security modules deny introspection */\n\tif (same_thread_group(task, current))\n\t\treturn 0;\n\trcu_read_lock();\n\tif (mode & PTRACE_MODE_FSCREDS) {\n\t\tcaller_uid = cred->fsuid;\n\t\tcaller_gid = cred->fsgid;\n\t} else {\n\t\t/*\n\t\t * Using the euid would make more sense here, but something\n\t\t * in userland might rely on the old behavior, and this\n\t\t * shouldn't be a security problem since\n\t\t * PTRACE_MODE_REALCREDS implies that the caller explicitly\n\t\t * used a syscall that requests access to another process\n\t\t * (and not a filesystem syscall to procfs).\n\t\t */\n\t\tcaller_uid = cred->uid;\n\t\tcaller_gid = cred->gid;\n\t}\n\ttcred = __task_cred(task);\n\tif (uid_eq(caller_uid, tcred->euid) &&\n\t    uid_eq(caller_uid, tcred->suid) &&\n\t    uid_eq(caller_uid, tcred->uid)  &&\n\t    gid_eq(caller_gid, tcred->egid) &&\n\t    gid_eq(caller_gid, tcred->sgid) &&\n\t    gid_eq(caller_gid, tcred->gid))\n\t\tgoto ok;\n\tif (ptrace_has_cap(tcred->user_ns, mode))\n\t\tgoto ok;\n\trcu_read_unlock();\n\treturn -EPERM;\nok:\n\trcu_read_unlock();\n\tsmp_rmb();\n\tif (task->mm)\n\t\tdumpable = get_dumpable(task->mm);\n\trcu_read_lock();\n\tif (dumpable != SUID_DUMP_USER &&\n\t    !ptrace_has_cap(__task_cred(task)->user_ns, mode)) {\n\t\trcu_read_unlock();\n\t\treturn -EPERM;\n\t}\n\trcu_read_unlock();\n\n\treturn security_ptrace_access_check(task, mode);\n}",
                        "code_after_change": "static struct task_struct *copy_process(unsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace)\n{\n\tint retval;\n\tstruct task_struct *p;\n\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid namespace\n\t * don't allow the creation of threads.\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_NEWPID)) &&\n\t    (task_active_pid_ns(current) != current->nsproxy->pid_ns))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tretval = security_task_create(clone_flags);\n\tif (retval)\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current);\n\tif (!p)\n\t\tgoto fork_out;\n\n\tftrace_graph_init_task(p);\n\tget_seccomp_filter(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&\n\t\t    p->real_cred->user != INIT_USER)\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tif (!try_module_get(task_thread_info(p)->exec_domain->module))\n\t\tgoto bad_fork_cleanup_count;\n\n\tp->did_exec = 0;\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tcopy_flags(clone_flags, p);\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n\tp->utimescaled = p->stimescaled = 0;\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING\n\tp->prev_cputime.utime = p->prev_cputime.stime = 0;\n#endif\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqlock_init(&p->vtime_seqlock);\n\tp->vtime_snap = 0;\n\tp->vtime_snap_whence = VTIME_SLEEPING;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tdo_posix_clock_monotonic_gettime(&p->start_time);\n\tp->real_start_time = p->start_time;\n\tmonotonic_to_bootbased(&p->real_start_time);\n\tp->io_context = NULL;\n\tp->audit_context = NULL;\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_begin(current);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_cgroup;\n\t}\n\tmpol_fix_fork_child_flag(p);\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_init(&p->mems_allowed_seq);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n\tp->hardirqs_enabled = 0;\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_MEMCG\n\tp->memcg_batch.do_batch = 0;\n\tp->memcg_batch.memcg = NULL;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tsched_fork(p);\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\t/* copy all the process information */\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(clone_flags, stack_start, stack_size, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tif (pid != &init_struct_pid) {\n\t\tretval = -ENOMEM;\n\t\tpid = alloc_pid(p->nsproxy->pid_ns);\n\t\tif (!pid)\n\t\t\tgoto bad_fork_cleanup_io;\n\t}\n\n\tp->pid = pid_nr(pid);\n\tp->tgid = p->pid;\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->tgid = current->tgid;\n\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\tuprobe_copy_process(p);\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tp->sas_ss_sp = p->sas_ss_size = 0;\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->exit_signal = -1;\n\telse if (clone_flags & CLONE_PARENT)\n\t\tp->exit_signal = current->group_leader->exit_signal;\n\telse\n\t\tp->exit_signal = (clone_flags & CSIGNAL);\n\n\tp->pdeath_signal = 0;\n\tp->exit_state = 0;\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\t/*\n\t * Ok, make it visible to the rest of the system.\n\t * We dont wake it up yet.\n\t */\n\tp->group_leader = p;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\t/* Need tasklist lock for parent etc handling! */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Process group and session signals need to be delivered to just the\n\t * parent before the fork or both the parent and the child after the\n\t * fork. Restart if a signal comes in before we add the new process to\n\t * it's process group.\n\t * A fatal signal pending means that current will exit, so the new\n\t * thread can't slip out of an OOM kill (or normal SIGKILL).\n\t*/\n\trecalc_sigpending();\n\tif (signal_pending(current)) {\n\t\tspin_unlock(&current->sighand->siglock);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t\tretval = -ERESTARTNOINTR;\n\t\tgoto bad_fork_free_pid;\n\t}\n\n\tif (clone_flags & CLONE_THREAD) {\n\t\tcurrent->signal->nr_threads++;\n\t\tatomic_inc(&current->signal->live);\n\t\tatomic_inc(&current->signal->sigcnt);\n\t\tp->group_leader = current->group_leader;\n\t\tlist_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);\n\t}\n\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tif (thread_group_leader(p)) {\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\n\t\t\tp->signal->leader_pid = pid;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\tattach_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tattach_pid(p, PIDTYPE_SID, task_session(current));\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID, pid);\n\t\tnr_threads++;\n\t}\n\n\ttotal_forks++;\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\n\treturn p;\n\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm)\n\t\tmmput(p->mm);\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_policy:\n\tperf_event_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_cgroup:\n#endif\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tcgroup_exit(p, 0);\n\tdelayacct_tsk_free(p);\n\tmodule_put(task_thread_info(p)->exec_domain->module);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tfree_task(p);\nfork_out:\n\treturn ERR_PTR(retval);\n}",
                        "cve_id": "CVE-2015-8709"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int ptrace_attach(struct task_struct *task, long request,\n\t\t\t unsigned long addr,\n\t\t\t unsigned long flags)\n{\n\tbool seize = (request == PTRACE_SEIZE);\n\tint retval;\n\n\tretval = -EIO;\n\tif (seize) {\n\t\tif (addr != 0)\n\t\t\tgoto out;\n\t\tif (flags & ~(unsigned long)PTRACE_O_MASK)\n\t\t\tgoto out;\n\t\tflags = PT_PTRACED | PT_SEIZED | (flags << PT_OPT_FLAG_SHIFT);\n\t} else {\n\t\tflags = PT_PTRACED;\n\t}\n\n\taudit_ptrace(task);\n\n\tretval = -EPERM;\n\tif (unlikely(task->flags & PF_KTHREAD))\n\t\tgoto out;\n\tif (same_thread_group(task, current))\n\t\tgoto out;\n\n\t/*\n\t * Protect exec's credential calculations against our interference;\n\t * SUID, SGID and LSM creds get determined differently\n\t * under ptrace.\n\t */\n\tretval = -ERESTARTNOINTR;\n\tif (mutex_lock_interruptible(&task->signal->cred_guard_mutex))\n\t\tgoto out;\n\n\ttask_lock(task);\n\tretval = __ptrace_may_access(task, PTRACE_MODE_ATTACH_REALCREDS);\n\ttask_unlock(task);\n\tif (retval)\n\t\tgoto unlock_creds;\n\n\twrite_lock_irq(&tasklist_lock);\n\tretval = -EPERM;\n\tif (unlikely(task->exit_state))\n\t\tgoto unlock_tasklist;\n\tif (task->ptrace)\n\t\tgoto unlock_tasklist;\n\n\tif (seize)\n\t\tflags |= PT_SEIZED;\n\trcu_read_lock();\n\tif (ns_capable(__task_cred(task)->user_ns, CAP_SYS_PTRACE))\n\t\tflags |= PT_PTRACE_CAP;\n\trcu_read_unlock();\n\ttask->ptrace = flags;\n\n\t__ptrace_link(task, current);\n\n\t/* SEIZE doesn't trap tracee on attach */\n\tif (!seize)\n\t\tsend_sig_info(SIGSTOP, SEND_SIG_FORCED, task);\n\n\tspin_lock(&task->sighand->siglock);\n\n\t/*\n\t * If the task is already STOPPED, set JOBCTL_TRAP_STOP and\n\t * TRAPPING, and kick it so that it transits to TRACED.  TRAPPING\n\t * will be cleared if the child completes the transition or any\n\t * event which clears the group stop states happens.  We'll wait\n\t * for the transition to complete before returning from this\n\t * function.\n\t *\n\t * This hides STOPPED -> RUNNING -> TRACED transition from the\n\t * attaching thread but a different thread in the same group can\n\t * still observe the transient RUNNING state.  IOW, if another\n\t * thread's WNOHANG wait(2) on the stopped tracee races against\n\t * ATTACH, the wait(2) may fail due to the transient RUNNING.\n\t *\n\t * The following task_is_stopped() test is safe as both transitions\n\t * in and out of STOPPED are protected by siglock.\n\t */\n\tif (task_is_stopped(task) &&\n\t    task_set_jobctl_pending(task, JOBCTL_TRAP_STOP | JOBCTL_TRAPPING))\n\t\tsignal_wake_up_state(task, __TASK_STOPPED);\n\n\tspin_unlock(&task->sighand->siglock);\n\n\tretval = 0;\nunlock_tasklist:\n\twrite_unlock_irq(&tasklist_lock);\nunlock_creds:\n\tmutex_unlock(&task->signal->cred_guard_mutex);\nout:\n\tif (!retval) {\n\t\t/*\n\t\t * We do not bother to change retval or clear JOBCTL_TRAPPING\n\t\t * if wait_on_bit() was interrupted by SIGKILL. The tracer will\n\t\t * not return to user-mode, it will exit and clear this bit in\n\t\t * __ptrace_unlink() if it wasn't already cleared by the tracee;\n\t\t * and until then nobody can ptrace this task.\n\t\t */\n\t\twait_on_bit(&task->jobctl, JOBCTL_TRAPPING_BIT, TASK_KILLABLE);\n\t\tproc_ptrace_connector(task, PTRACE_ATTACH);\n\t}\n\n\treturn retval;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int __ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tconst struct cred *cred = current_cred(), *tcred;\n\tint dumpable = 0;\n\tkuid_t caller_uid;\n\tkgid_t caller_gid;\n\n\tif (!(mode & PTRACE_MODE_FSCREDS) == !(mode & PTRACE_MODE_REALCREDS)) {\n\t\tWARN(1, \"denying ptrace access check without PTRACE_MODE_*CREDS\\n\");\n\t\treturn -EPERM;\n\t}\n\n\t/* May we inspect the given task?\n\t * This check is used both for attaching with ptrace\n\t * and for allowing access to sensitive information in /proc.\n\t *\n\t * ptrace_attach denies several cases that /proc allows\n\t * because setting up the necessary parent/child relationship\n\t * or halting the specified task is impossible.\n\t */\n\n\t/* Don't let security modules deny introspection */\n\tif (same_thread_group(task, current))\n\t\treturn 0;\n\trcu_read_lock();\n\tif (mode & PTRACE_MODE_FSCREDS) {\n\t\tcaller_uid = cred->fsuid;\n\t\tcaller_gid = cred->fsgid;\n\t} else {\n\t\t/*\n\t\t * Using the euid would make more sense here, but something\n\t\t * in userland might rely on the old behavior, and this\n\t\t * shouldn't be a security problem since\n\t\t * PTRACE_MODE_REALCREDS implies that the caller explicitly\n\t\t * used a syscall that requests access to another process\n\t\t * (and not a filesystem syscall to procfs).\n\t\t */\n\t\tcaller_uid = cred->uid;\n\t\tcaller_gid = cred->gid;\n\t}\n\ttcred = __task_cred(task);\n\tif (uid_eq(caller_uid, tcred->euid) &&\n\t    uid_eq(caller_uid, tcred->suid) &&\n\t    uid_eq(caller_uid, tcred->uid)  &&\n\t    gid_eq(caller_gid, tcred->egid) &&\n\t    gid_eq(caller_gid, tcred->sgid) &&\n\t    gid_eq(caller_gid, tcred->gid))\n\t\tgoto ok;\n\tif (ptrace_has_cap(tcred->user_ns, mode))\n\t\tgoto ok;\n\trcu_read_unlock();\n\treturn -EPERM;\nok:\n\trcu_read_unlock();\n\tsmp_rmb();\n\tif (task->mm)\n\t\tdumpable = get_dumpable(task->mm);\n\trcu_read_lock();\n\tif (dumpable != SUID_DUMP_USER &&\n\t    !ptrace_has_cap(__task_cred(task)->user_ns, mode)) {\n\t\trcu_read_unlock();\n\t\treturn -EPERM;\n\t}\n\trcu_read_unlock();\n\n\treturn security_ptrace_access_check(task, mode);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic struct task_struct *copy_process(unsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace)\n{\n\tint retval;\n\tstruct task_struct *p;\n\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid namespace\n\t * don't allow the creation of threads.\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_NEWPID)) &&\n\t    (task_active_pid_ns(current) != current->nsproxy->pid_ns))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tretval = security_task_create(clone_flags);\n\tif (retval)\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current);\n\tif (!p)\n\t\tgoto fork_out;\n\n\tftrace_graph_init_task(p);\n\tget_seccomp_filter(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&\n\t\t    p->real_cred->user != INIT_USER)\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tif (!try_module_get(task_thread_info(p)->exec_domain->module))\n\t\tgoto bad_fork_cleanup_count;\n\n\tp->did_exec = 0;\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tcopy_flags(clone_flags, p);\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n\tp->utimescaled = p->stimescaled = 0;\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING\n\tp->prev_cputime.utime = p->prev_cputime.stime = 0;\n#endif\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqlock_init(&p->vtime_seqlock);\n\tp->vtime_snap = 0;\n\tp->vtime_snap_whence = VTIME_SLEEPING;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tdo_posix_clock_monotonic_gettime(&p->start_time);\n\tp->real_start_time = p->start_time;\n\tmonotonic_to_bootbased(&p->real_start_time);\n\tp->io_context = NULL;\n\tp->audit_context = NULL;\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_begin(current);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_cgroup;\n\t}\n\tmpol_fix_fork_child_flag(p);\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_init(&p->mems_allowed_seq);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n\tp->hardirqs_enabled = 0;\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_MEMCG\n\tp->memcg_batch.do_batch = 0;\n\tp->memcg_batch.memcg = NULL;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tsched_fork(p);\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\t/* copy all the process information */\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(clone_flags, stack_start, stack_size, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tif (pid != &init_struct_pid) {\n\t\tretval = -ENOMEM;\n\t\tpid = alloc_pid(p->nsproxy->pid_ns);\n\t\tif (!pid)\n\t\t\tgoto bad_fork_cleanup_io;\n\t}\n\n\tp->pid = pid_nr(pid);\n\tp->tgid = p->pid;\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->tgid = current->tgid;\n\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\tuprobe_copy_process(p);\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tp->sas_ss_sp = p->sas_ss_size = 0;\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->exit_signal = -1;\n\telse if (clone_flags & CLONE_PARENT)\n\t\tp->exit_signal = current->group_leader->exit_signal;\n\telse\n\t\tp->exit_signal = (clone_flags & CSIGNAL);\n\n\tp->pdeath_signal = 0;\n\tp->exit_state = 0;\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\t/*\n\t * Ok, make it visible to the rest of the system.\n\t * We dont wake it up yet.\n\t */\n\tp->group_leader = p;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\t/* Need tasklist lock for parent etc handling! */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Process group and session signals need to be delivered to just the\n\t * parent before the fork or both the parent and the child after the\n\t * fork. Restart if a signal comes in before we add the new process to\n\t * it's process group.\n\t * A fatal signal pending means that current will exit, so the new\n\t * thread can't slip out of an OOM kill (or normal SIGKILL).\n\t*/\n\trecalc_sigpending();\n\tif (signal_pending(current)) {\n\t\tspin_unlock(&current->sighand->siglock);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t\tretval = -ERESTARTNOINTR;\n\t\tgoto bad_fork_free_pid;\n\t}\n\n\tif (clone_flags & CLONE_THREAD) {\n\t\tcurrent->signal->nr_threads++;\n\t\tatomic_inc(&current->signal->live);\n\t\tatomic_inc(&current->signal->sigcnt);\n\t\tp->group_leader = current->group_leader;\n\t\tlist_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);\n\t}\n\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tif (thread_group_leader(p)) {\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\n\t\t\tp->signal->leader_pid = pid;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\tattach_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tattach_pid(p, PIDTYPE_SID, task_session(current));\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID, pid);\n\t\tnr_threads++;\n\t}\n\n\ttotal_forks++;\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\n\treturn p;\n\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm)\n\t\tmmput(p->mm);\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_policy:\n\tperf_event_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_cgroup:\n#endif\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tcgroup_exit(p, 0);\n\tdelayacct_tsk_free(p);\n\tmodule_put(task_thread_info(p)->exec_domain->module);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tfree_task(p);\nfork_out:\n\treturn ERR_PTR(retval);\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": " */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx;\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tstruct perf_event_context *gctx = group_leader->ctx;\n\n\t\tmutex_lock(&gctx->mutex);\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\n\tif (move_group) {\n\t\tsynchronize_rcu();\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}",
                        "code_after_change": "static int __ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tconst struct cred *cred = current_cred(), *tcred;\n\tstruct mm_struct *mm;\n\tkuid_t caller_uid;\n\tkgid_t caller_gid;\n\n\tif (!(mode & PTRACE_MODE_FSCREDS) == !(mode & PTRACE_MODE_REALCREDS)) {\n\t\tWARN(1, \"denying ptrace access check without PTRACE_MODE_*CREDS\\n\");\n\t\treturn -EPERM;\n\t}\n\n\t/* May we inspect the given task?\n\t * This check is used both for attaching with ptrace\n\t * and for allowing access to sensitive information in /proc.\n\t *\n\t * ptrace_attach denies several cases that /proc allows\n\t * because setting up the necessary parent/child relationship\n\t * or halting the specified task is impossible.\n\t */\n\n\t/* Don't let security modules deny introspection */\n\tif (same_thread_group(task, current))\n\t\treturn 0;\n\trcu_read_lock();\n\tif (mode & PTRACE_MODE_FSCREDS) {\n\t\tcaller_uid = cred->fsuid;\n\t\tcaller_gid = cred->fsgid;\n\t} else {\n\t\t/*\n\t\t * Using the euid would make more sense here, but something\n\t\t * in userland might rely on the old behavior, and this\n\t\t * shouldn't be a security problem since\n\t\t * PTRACE_MODE_REALCREDS implies that the caller explicitly\n\t\t * used a syscall that requests access to another process\n\t\t * (and not a filesystem syscall to procfs).\n\t\t */\n\t\tcaller_uid = cred->uid;\n\t\tcaller_gid = cred->gid;\n\t}\n\ttcred = __task_cred(task);\n\tif (uid_eq(caller_uid, tcred->euid) &&\n\t    uid_eq(caller_uid, tcred->suid) &&\n\t    uid_eq(caller_uid, tcred->uid)  &&\n\t    gid_eq(caller_gid, tcred->egid) &&\n\t    gid_eq(caller_gid, tcred->sgid) &&\n\t    gid_eq(caller_gid, tcred->gid))\n\t\tgoto ok;\n\tif (ptrace_has_cap(tcred->user_ns, mode))\n\t\tgoto ok;\n\trcu_read_unlock();\n\treturn -EPERM;\nok:\n\trcu_read_unlock();\n\tmm = task->mm;\n\tif (mm &&\n\t    ((get_dumpable(mm) != SUID_DUMP_USER) &&\n\t     !ptrace_has_cap(mm->user_ns, mode)))\n\t    return -EPERM;\n\n\treturn security_ptrace_access_check(task, mode);\n}",
                        "cve_id": "CVE-2016-6786"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int ptrace_attach(struct task_struct *task, long request,\n\t\t\t unsigned long addr,\n\t\t\t unsigned long flags)\n{\n\tbool seize = (request == PTRACE_SEIZE);\n\tint retval;\n\n\tretval = -EIO;\n\tif (seize) {\n\t\tif (addr != 0)\n\t\t\tgoto out;\n\t\tif (flags & ~(unsigned long)PTRACE_O_MASK)\n\t\t\tgoto out;\n\t\tflags = PT_PTRACED | PT_SEIZED | (flags << PT_OPT_FLAG_SHIFT);\n\t} else {\n\t\tflags = PT_PTRACED;\n\t}\n\n\taudit_ptrace(task);\n\n\tretval = -EPERM;\n\tif (unlikely(task->flags & PF_KTHREAD))\n\t\tgoto out;\n\tif (same_thread_group(task, current))\n\t\tgoto out;\n\n\t/*\n\t * Protect exec's credential calculations against our interference;\n\t * SUID, SGID and LSM creds get determined differently\n\t * under ptrace.\n\t */\n\tretval = -ERESTARTNOINTR;\n\tif (mutex_lock_interruptible(&task->signal->cred_guard_mutex))\n\t\tgoto out;\n\n\ttask_lock(task);\n\tretval = __ptrace_may_access(task, PTRACE_MODE_ATTACH_REALCREDS);\n\ttask_unlock(task);\n\tif (retval)\n\t\tgoto unlock_creds;\n\n\twrite_lock_irq(&tasklist_lock);\n\tretval = -EPERM;\n\tif (unlikely(task->exit_state))\n\t\tgoto unlock_tasklist;\n\tif (task->ptrace)\n\t\tgoto unlock_tasklist;\n\n\tif (seize)\n\t\tflags |= PT_SEIZED;\n\trcu_read_lock();\n\tif (ns_capable(__task_cred(task)->user_ns, CAP_SYS_PTRACE))\n\t\tflags |= PT_PTRACE_CAP;\n\trcu_read_unlock();\n\ttask->ptrace = flags;\n\n\t__ptrace_link(task, current);\n\n\t/* SEIZE doesn't trap tracee on attach */\n\tif (!seize)\n\t\tsend_sig_info(SIGSTOP, SEND_SIG_FORCED, task);\n\n\tspin_lock(&task->sighand->siglock);\n\n\t/*\n\t * If the task is already STOPPED, set JOBCTL_TRAP_STOP and\n\t * TRAPPING, and kick it so that it transits to TRACED.  TRAPPING\n\t * will be cleared if the child completes the transition or any\n\t * event which clears the group stop states happens.  We'll wait\n\t * for the transition to complete before returning from this\n\t * function.\n\t *\n\t * This hides STOPPED -> RUNNING -> TRACED transition from the\n\t * attaching thread but a different thread in the same group can\n\t * still observe the transient RUNNING state.  IOW, if another\n\t * thread's WNOHANG wait(2) on the stopped tracee races against\n\t * ATTACH, the wait(2) may fail due to the transient RUNNING.\n\t *\n\t * The following task_is_stopped() test is safe as both transitions\n\t * in and out of STOPPED are protected by siglock.\n\t */\n\tif (task_is_stopped(task) &&\n\t    task_set_jobctl_pending(task, JOBCTL_TRAP_STOP | JOBCTL_TRAPPING))\n\t\tsignal_wake_up_state(task, __TASK_STOPPED);\n\n\tspin_unlock(&task->sighand->siglock);\n\n\tretval = 0;\nunlock_tasklist:\n\twrite_unlock_irq(&tasklist_lock);\nunlock_creds:\n\tmutex_unlock(&task->signal->cred_guard_mutex);\nout:\n\tif (!retval) {\n\t\t/*\n\t\t * We do not bother to change retval or clear JOBCTL_TRAPPING\n\t\t * if wait_on_bit() was interrupted by SIGKILL. The tracer will\n\t\t * not return to user-mode, it will exit and clear this bit in\n\t\t * __ptrace_unlink() if it wasn't already cleared by the tracee;\n\t\t * and until then nobody can ptrace this task.\n\t\t */\n\t\twait_on_bit(&task->jobctl, JOBCTL_TRAPPING_BIT, TASK_KILLABLE);\n\t\tproc_ptrace_connector(task, PTRACE_ATTACH);\n\t}\n\n\treturn retval;\n}\n'''\nRelated vulnerability code snippet:\n'''\n */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx;\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tstruct perf_event_context *gctx = group_leader->ctx;\n\n\t\tmutex_lock(&gctx->mutex);\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\n\tif (move_group) {\n\t\tsynchronize_rcu();\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int __ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tconst struct cred *cred = current_cred(), *tcred;\n\tstruct mm_struct *mm;\n\tkuid_t caller_uid;\n\tkgid_t caller_gid;\n\n\tif (!(mode & PTRACE_MODE_FSCREDS) == !(mode & PTRACE_MODE_REALCREDS)) {\n\t\tWARN(1, \"denying ptrace access check without PTRACE_MODE_*CREDS\\n\");\n\t\treturn -EPERM;\n\t}\n\n\t/* May we inspect the given task?\n\t * This check is used both for attaching with ptrace\n\t * and for allowing access to sensitive information in /proc.\n\t *\n\t * ptrace_attach denies several cases that /proc allows\n\t * because setting up the necessary parent/child relationship\n\t * or halting the specified task is impossible.\n\t */\n\n\t/* Don't let security modules deny introspection */\n\tif (same_thread_group(task, current))\n\t\treturn 0;\n\trcu_read_lock();\n\tif (mode & PTRACE_MODE_FSCREDS) {\n\t\tcaller_uid = cred->fsuid;\n\t\tcaller_gid = cred->fsgid;\n\t} else {\n\t\t/*\n\t\t * Using the euid would make more sense here, but something\n\t\t * in userland might rely on the old behavior, and this\n\t\t * shouldn't be a security problem since\n\t\t * PTRACE_MODE_REALCREDS implies that the caller explicitly\n\t\t * used a syscall that requests access to another process\n\t\t * (and not a filesystem syscall to procfs).\n\t\t */\n\t\tcaller_uid = cred->uid;\n\t\tcaller_gid = cred->gid;\n\t}\n\ttcred = __task_cred(task);\n\tif (uid_eq(caller_uid, tcred->euid) &&\n\t    uid_eq(caller_uid, tcred->suid) &&\n\t    uid_eq(caller_uid, tcred->uid)  &&\n\t    gid_eq(caller_gid, tcred->egid) &&\n\t    gid_eq(caller_gid, tcred->sgid) &&\n\t    gid_eq(caller_gid, tcred->gid))\n\t\tgoto ok;\n\tif (ptrace_has_cap(tcred->user_ns, mode))\n\t\tgoto ok;\n\trcu_read_unlock();\n\treturn -EPERM;\nok:\n\trcu_read_unlock();\n\tmm = task->mm;\n\tif (mm &&\n\t    ((get_dumpable(mm) != SUID_DUMP_USER) &&\n\t     !ptrace_has_cap(mm->user_ns, mode)))\n\t    return -EPERM;\n\n\treturn security_ptrace_access_check(task, mode);\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 884,
            "cve_id": "CVE-2015-9016",
            "code_snippet": "static void bt_tags_for_each(struct blk_mq_tags *tags,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_tag_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tif (!tags->rqs)\n\t\treturn;\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t\trq = blk_mq_tag_to_rq(tags, off + bit);\n\t\t\tfn(rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static void bt_for_each(struct blk_mq_hw_ctx *hctx,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t     \trq = blk_mq_tag_to_rq(hctx->tags, off + bit);\n\t\t\tif (rq->q == hctx->queue)\n\t\t\t\tfn(hctx, rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}",
                        "code_after_change": "static void bt_for_each(struct blk_mq_hw_ctx *hctx,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t\trq = hctx->tags->rqs[off + bit];\n\t\t\tif (rq->q == hctx->queue)\n\t\t\t\tfn(hctx, rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}",
                        "cve_id": "CVE-2015-9016"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic void bt_tags_for_each(struct blk_mq_tags *tags,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_tag_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tif (!tags->rqs)\n\t\treturn;\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t\trq = blk_mq_tag_to_rq(tags, off + bit);\n\t\t\tfn(rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic void bt_for_each(struct blk_mq_hw_ctx *hctx,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t     \trq = blk_mq_tag_to_rq(hctx->tags, off + bit);\n\t\t\tif (rq->q == hctx->queue)\n\t\t\t\tfn(hctx, rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic void bt_for_each(struct blk_mq_hw_ctx *hctx,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t\trq = hctx->tags->rqs[off + bit];\n\t\t\tif (rq->q == hctx->queue)\n\t\t\t\tfn(hctx, rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n{\n\tstruct request *rq = tags->rqs[tag];\n\t/* mq_ctx of flush rq is always cloned from the corresponding req */\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(rq->q, rq->mq_ctx);\n\n\tif (!is_flush_request(rq, fq, tag))\n\t\treturn rq;\n\n\treturn fq->flush_rq;\n}",
                        "code_after_change": "struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n{\n\treturn tags->rqs[tag];\n}",
                        "cve_id": "CVE-2015-9016"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic void bt_tags_for_each(struct blk_mq_tags *tags,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_tag_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tif (!tags->rqs)\n\t\treturn;\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t\trq = blk_mq_tag_to_rq(tags, off + bit);\n\t\t\tfn(rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}\n'''\nRelated vulnerability code snippet:\n'''\nstruct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n{\n\tstruct request *rq = tags->rqs[tag];\n\t/* mq_ctx of flush rq is always cloned from the corresponding req */\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(rq->q, rq->mq_ctx);\n\n\tif (!is_flush_request(rq, fq, tag))\n\t\treturn rq;\n\n\treturn fq->flush_rq;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstruct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n{\n\treturn tags->rqs[tag];\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static __init int hardware_setup(void)\n{\n\tint r = -ENOMEM, i, msr;\n\n\trdmsrl_safe(MSR_EFER, &host_efer);\n\n\tfor (i = 0; i < ARRAY_SIZE(vmx_msr_index); ++i)\n\t\tkvm_define_shared_msr(i, vmx_msr_index[i]);\n\n\tvmx_io_bitmap_a = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_io_bitmap_a)\n\t\treturn r;\n\n\tvmx_io_bitmap_b = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_io_bitmap_b)\n\t\tgoto out;\n\n\tvmx_msr_bitmap_legacy = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_legacy)\n\t\tgoto out1;\n\n\tvmx_msr_bitmap_legacy_x2apic =\n\t\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_legacy_x2apic)\n\t\tgoto out2;\n\n\tvmx_msr_bitmap_longmode = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_longmode)\n\t\tgoto out3;\n\n\tvmx_msr_bitmap_longmode_x2apic =\n\t\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_longmode_x2apic)\n\t\tgoto out4;\n\n\tif (nested) {\n\t\tvmx_msr_bitmap_nested =\n\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\t\tif (!vmx_msr_bitmap_nested)\n\t\t\tgoto out5;\n\t}\n\n\tvmx_vmread_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_vmread_bitmap)\n\t\tgoto out6;\n\n\tvmx_vmwrite_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_vmwrite_bitmap)\n\t\tgoto out7;\n\n\tmemset(vmx_vmread_bitmap, 0xff, PAGE_SIZE);\n\tmemset(vmx_vmwrite_bitmap, 0xff, PAGE_SIZE);\n\n\t/*\n\t * Allow direct access to the PC debug port (it is often used for I/O\n\t * delays, but the vmexits simply slow things down).\n\t */\n\tmemset(vmx_io_bitmap_a, 0xff, PAGE_SIZE);\n\tclear_bit(0x80, vmx_io_bitmap_a);\n\n\tmemset(vmx_io_bitmap_b, 0xff, PAGE_SIZE);\n\n\tmemset(vmx_msr_bitmap_legacy, 0xff, PAGE_SIZE);\n\tmemset(vmx_msr_bitmap_longmode, 0xff, PAGE_SIZE);\n\tif (nested)\n\t\tmemset(vmx_msr_bitmap_nested, 0xff, PAGE_SIZE);\n\n\tif (setup_vmcs_config(&vmcs_config) < 0) {\n\t\tr = -EIO;\n\t\tgoto out8;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_NX))\n\t\tkvm_enable_efer_bits(EFER_NX);\n\n\tif (!cpu_has_vmx_vpid())\n\t\tenable_vpid = 0;\n\tif (!cpu_has_vmx_shadow_vmcs())\n\t\tenable_shadow_vmcs = 0;\n\tif (enable_shadow_vmcs)\n\t\tinit_vmcs_shadow_fields();\n\n\tif (!cpu_has_vmx_ept() ||\n\t    !cpu_has_vmx_ept_4levels()) {\n\t\tenable_ept = 0;\n\t\tenable_unrestricted_guest = 0;\n\t\tenable_ept_ad_bits = 0;\n\t}\n\n\tif (!cpu_has_vmx_ept_ad_bits())\n\t\tenable_ept_ad_bits = 0;\n\n\tif (!cpu_has_vmx_unrestricted_guest())\n\t\tenable_unrestricted_guest = 0;\n\n\tif (!cpu_has_vmx_flexpriority())\n\t\tflexpriority_enabled = 0;\n\n\t/*\n\t * set_apic_access_page_addr() is used to reload apic access\n\t * page upon invalidation.  No need to do anything if not\n\t * using the APIC_ACCESS_ADDR VMCS field.\n\t */\n\tif (!flexpriority_enabled)\n\t\tkvm_x86_ops->set_apic_access_page_addr = NULL;\n\n\tif (!cpu_has_vmx_tpr_shadow())\n\t\tkvm_x86_ops->update_cr8_intercept = NULL;\n\n\tif (enable_ept && !cpu_has_vmx_ept_2m_page())\n\t\tkvm_disable_largepages();\n\n\tif (!cpu_has_vmx_ple())\n\t\tple_gap = 0;\n\n\tif (!cpu_has_vmx_apicv())\n\t\tenable_apicv = 0;\n\n\tif (cpu_has_vmx_tsc_scaling()) {\n\t\tkvm_has_tsc_control = true;\n\t\tkvm_max_tsc_scaling_ratio = KVM_VMX_TSC_MULTIPLIER_MAX;\n\t\tkvm_tsc_scaling_ratio_frac_bits = 48;\n\t}\n\n\tvmx_disable_intercept_for_msr(MSR_FS_BASE, false);\n\tvmx_disable_intercept_for_msr(MSR_GS_BASE, false);\n\tvmx_disable_intercept_for_msr(MSR_KERNEL_GS_BASE, true);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_CS, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_ESP, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_EIP, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_BNDCFGS, true);\n\n\tmemcpy(vmx_msr_bitmap_legacy_x2apic,\n\t\t\tvmx_msr_bitmap_legacy, PAGE_SIZE);\n\tmemcpy(vmx_msr_bitmap_longmode_x2apic,\n\t\t\tvmx_msr_bitmap_longmode, PAGE_SIZE);\n\n\tset_bit(0, vmx_vpid_bitmap); /* 0 is reserved for host */\n\n\tif (enable_apicv) {\n\t\tfor (msr = 0x800; msr <= 0x8ff; msr++)\n\t\t\tvmx_disable_intercept_msr_read_x2apic(msr);\n\n\t\t/* According SDM, in x2apic mode, the whole id reg is used.\n\t\t * But in KVM, it only use the highest eight bits. Need to\n\t\t * intercept it */\n\t\tvmx_enable_intercept_msr_read_x2apic(0x802);\n\t\t/* TMCCT */\n\t\tvmx_enable_intercept_msr_read_x2apic(0x839);\n\t\t/* TPR */\n\t\tvmx_disable_intercept_msr_write_x2apic(0x808);\n\t\t/* EOI */\n\t\tvmx_disable_intercept_msr_write_x2apic(0x80b);\n\t\t/* SELF-IPI */\n\t\tvmx_disable_intercept_msr_write_x2apic(0x83f);\n\t}\n\n\tif (enable_ept) {\n\t\tkvm_mmu_set_mask_ptes(0ull,\n\t\t\t(enable_ept_ad_bits) ? VMX_EPT_ACCESS_BIT : 0ull,\n\t\t\t(enable_ept_ad_bits) ? VMX_EPT_DIRTY_BIT : 0ull,\n\t\t\t0ull, VMX_EPT_EXECUTABLE_MASK);\n\t\tept_set_mmio_spte_mask();\n\t\tkvm_enable_tdp();\n\t} else\n\t\tkvm_disable_tdp();\n\n\tupdate_ple_window_actual_max();\n\n\t/*\n\t * Only enable PML when hardware supports PML feature, and both EPT\n\t * and EPT A/D bit features are enabled -- PML depends on them to work.\n\t */\n\tif (!enable_ept || !enable_ept_ad_bits || !cpu_has_vmx_pml())\n\t\tenable_pml = 0;\n\n\tif (!enable_pml) {\n\t\tkvm_x86_ops->slot_enable_log_dirty = NULL;\n\t\tkvm_x86_ops->slot_disable_log_dirty = NULL;\n\t\tkvm_x86_ops->flush_log_dirty = NULL;\n\t\tkvm_x86_ops->enable_log_dirty_pt_masked = NULL;\n\t}\n\n\tkvm_set_posted_intr_wakeup_handler(wakeup_handler);\n\n\treturn alloc_kvm_area();\n\nout8:\n\tfree_page((unsigned long)vmx_vmwrite_bitmap);\nout7:\n\tfree_page((unsigned long)vmx_vmread_bitmap);\nout6:\n\tif (nested)\n\t\tfree_page((unsigned long)vmx_msr_bitmap_nested);\nout5:\n\tfree_page((unsigned long)vmx_msr_bitmap_longmode_x2apic);\nout4:\n\tfree_page((unsigned long)vmx_msr_bitmap_longmode);\nout3:\n\tfree_page((unsigned long)vmx_msr_bitmap_legacy_x2apic);\nout2:\n\tfree_page((unsigned long)vmx_msr_bitmap_legacy);\nout1:\n\tfree_page((unsigned long)vmx_io_bitmap_b);\nout:\n\tfree_page((unsigned long)vmx_io_bitmap_a);\n\n    return r;\n}",
                        "code_after_change": "static __init int hardware_setup(void)\n{\n\tint r = -ENOMEM, i, msr;\n\n\trdmsrl_safe(MSR_EFER, &host_efer);\n\n\tfor (i = 0; i < ARRAY_SIZE(vmx_msr_index); ++i)\n\t\tkvm_define_shared_msr(i, vmx_msr_index[i]);\n\n\tvmx_io_bitmap_a = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_io_bitmap_a)\n\t\treturn r;\n\n\tvmx_io_bitmap_b = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_io_bitmap_b)\n\t\tgoto out;\n\n\tvmx_msr_bitmap_legacy = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_legacy)\n\t\tgoto out1;\n\n\tvmx_msr_bitmap_legacy_x2apic =\n\t\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_legacy_x2apic)\n\t\tgoto out2;\n\n\tvmx_msr_bitmap_longmode = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_longmode)\n\t\tgoto out3;\n\n\tvmx_msr_bitmap_longmode_x2apic =\n\t\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_longmode_x2apic)\n\t\tgoto out4;\n\n\tif (nested) {\n\t\tvmx_msr_bitmap_nested =\n\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\t\tif (!vmx_msr_bitmap_nested)\n\t\t\tgoto out5;\n\t}\n\n\tvmx_vmread_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_vmread_bitmap)\n\t\tgoto out6;\n\n\tvmx_vmwrite_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_vmwrite_bitmap)\n\t\tgoto out7;\n\n\tmemset(vmx_vmread_bitmap, 0xff, PAGE_SIZE);\n\tmemset(vmx_vmwrite_bitmap, 0xff, PAGE_SIZE);\n\n\t/*\n\t * Allow direct access to the PC debug port (it is often used for I/O\n\t * delays, but the vmexits simply slow things down).\n\t */\n\tmemset(vmx_io_bitmap_a, 0xff, PAGE_SIZE);\n\tclear_bit(0x80, vmx_io_bitmap_a);\n\n\tmemset(vmx_io_bitmap_b, 0xff, PAGE_SIZE);\n\n\tmemset(vmx_msr_bitmap_legacy, 0xff, PAGE_SIZE);\n\tmemset(vmx_msr_bitmap_longmode, 0xff, PAGE_SIZE);\n\tif (nested)\n\t\tmemset(vmx_msr_bitmap_nested, 0xff, PAGE_SIZE);\n\n\tif (setup_vmcs_config(&vmcs_config) < 0) {\n\t\tr = -EIO;\n\t\tgoto out8;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_NX))\n\t\tkvm_enable_efer_bits(EFER_NX);\n\n\tif (!cpu_has_vmx_vpid())\n\t\tenable_vpid = 0;\n\tif (!cpu_has_vmx_shadow_vmcs())\n\t\tenable_shadow_vmcs = 0;\n\tif (enable_shadow_vmcs)\n\t\tinit_vmcs_shadow_fields();\n\n\tif (!cpu_has_vmx_ept() ||\n\t    !cpu_has_vmx_ept_4levels()) {\n\t\tenable_ept = 0;\n\t\tenable_unrestricted_guest = 0;\n\t\tenable_ept_ad_bits = 0;\n\t}\n\n\tif (!cpu_has_vmx_ept_ad_bits())\n\t\tenable_ept_ad_bits = 0;\n\n\tif (!cpu_has_vmx_unrestricted_guest())\n\t\tenable_unrestricted_guest = 0;\n\n\tif (!cpu_has_vmx_flexpriority())\n\t\tflexpriority_enabled = 0;\n\n\t/*\n\t * set_apic_access_page_addr() is used to reload apic access\n\t * page upon invalidation.  No need to do anything if not\n\t * using the APIC_ACCESS_ADDR VMCS field.\n\t */\n\tif (!flexpriority_enabled)\n\t\tkvm_x86_ops->set_apic_access_page_addr = NULL;\n\n\tif (!cpu_has_vmx_tpr_shadow())\n\t\tkvm_x86_ops->update_cr8_intercept = NULL;\n\n\tif (enable_ept && !cpu_has_vmx_ept_2m_page())\n\t\tkvm_disable_largepages();\n\n\tif (!cpu_has_vmx_ple())\n\t\tple_gap = 0;\n\n\tif (!cpu_has_vmx_apicv())\n\t\tenable_apicv = 0;\n\n\tif (cpu_has_vmx_tsc_scaling()) {\n\t\tkvm_has_tsc_control = true;\n\t\tkvm_max_tsc_scaling_ratio = KVM_VMX_TSC_MULTIPLIER_MAX;\n\t\tkvm_tsc_scaling_ratio_frac_bits = 48;\n\t}\n\n\tvmx_disable_intercept_for_msr(MSR_FS_BASE, false);\n\tvmx_disable_intercept_for_msr(MSR_GS_BASE, false);\n\tvmx_disable_intercept_for_msr(MSR_KERNEL_GS_BASE, true);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_CS, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_ESP, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_EIP, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_BNDCFGS, true);\n\n\tmemcpy(vmx_msr_bitmap_legacy_x2apic,\n\t\t\tvmx_msr_bitmap_legacy, PAGE_SIZE);\n\tmemcpy(vmx_msr_bitmap_longmode_x2apic,\n\t\t\tvmx_msr_bitmap_longmode, PAGE_SIZE);\n\n\tset_bit(0, vmx_vpid_bitmap); /* 0 is reserved for host */\n\n\tfor (msr = 0x800; msr <= 0x8ff; msr++)\n\t\tvmx_disable_intercept_msr_read_x2apic(msr);\n\n\t/* According SDM, in x2apic mode, the whole id reg is used.  But in\n\t * KVM, it only use the highest eight bits. Need to intercept it */\n\tvmx_enable_intercept_msr_read_x2apic(0x802);\n\t/* TMCCT */\n\tvmx_enable_intercept_msr_read_x2apic(0x839);\n\t/* TPR */\n\tvmx_disable_intercept_msr_write_x2apic(0x808);\n\t/* EOI */\n\tvmx_disable_intercept_msr_write_x2apic(0x80b);\n\t/* SELF-IPI */\n\tvmx_disable_intercept_msr_write_x2apic(0x83f);\n\n\tif (enable_ept) {\n\t\tkvm_mmu_set_mask_ptes(0ull,\n\t\t\t(enable_ept_ad_bits) ? VMX_EPT_ACCESS_BIT : 0ull,\n\t\t\t(enable_ept_ad_bits) ? VMX_EPT_DIRTY_BIT : 0ull,\n\t\t\t0ull, VMX_EPT_EXECUTABLE_MASK);\n\t\tept_set_mmio_spte_mask();\n\t\tkvm_enable_tdp();\n\t} else\n\t\tkvm_disable_tdp();\n\n\tupdate_ple_window_actual_max();\n\n\t/*\n\t * Only enable PML when hardware supports PML feature, and both EPT\n\t * and EPT A/D bit features are enabled -- PML depends on them to work.\n\t */\n\tif (!enable_ept || !enable_ept_ad_bits || !cpu_has_vmx_pml())\n\t\tenable_pml = 0;\n\n\tif (!enable_pml) {\n\t\tkvm_x86_ops->slot_enable_log_dirty = NULL;\n\t\tkvm_x86_ops->slot_disable_log_dirty = NULL;\n\t\tkvm_x86_ops->flush_log_dirty = NULL;\n\t\tkvm_x86_ops->enable_log_dirty_pt_masked = NULL;\n\t}\n\n\tkvm_set_posted_intr_wakeup_handler(wakeup_handler);\n\n\treturn alloc_kvm_area();\n\nout8:\n\tfree_page((unsigned long)vmx_vmwrite_bitmap);\nout7:\n\tfree_page((unsigned long)vmx_vmread_bitmap);\nout6:\n\tif (nested)\n\t\tfree_page((unsigned long)vmx_msr_bitmap_nested);\nout5:\n\tfree_page((unsigned long)vmx_msr_bitmap_longmode_x2apic);\nout4:\n\tfree_page((unsigned long)vmx_msr_bitmap_longmode);\nout3:\n\tfree_page((unsigned long)vmx_msr_bitmap_legacy_x2apic);\nout2:\n\tfree_page((unsigned long)vmx_msr_bitmap_legacy);\nout1:\n\tfree_page((unsigned long)vmx_io_bitmap_b);\nout:\n\tfree_page((unsigned long)vmx_io_bitmap_a);\n\n    return r;\n}",
                        "cve_id": "CVE-2016-4440"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic void bt_tags_for_each(struct blk_mq_tags *tags,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_tag_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tif (!tags->rqs)\n\t\treturn;\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t\trq = blk_mq_tag_to_rq(tags, off + bit);\n\t\t\tfn(rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic __init int hardware_setup(void)\n{\n\tint r = -ENOMEM, i, msr;\n\n\trdmsrl_safe(MSR_EFER, &host_efer);\n\n\tfor (i = 0; i < ARRAY_SIZE(vmx_msr_index); ++i)\n\t\tkvm_define_shared_msr(i, vmx_msr_index[i]);\n\n\tvmx_io_bitmap_a = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_io_bitmap_a)\n\t\treturn r;\n\n\tvmx_io_bitmap_b = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_io_bitmap_b)\n\t\tgoto out;\n\n\tvmx_msr_bitmap_legacy = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_legacy)\n\t\tgoto out1;\n\n\tvmx_msr_bitmap_legacy_x2apic =\n\t\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_legacy_x2apic)\n\t\tgoto out2;\n\n\tvmx_msr_bitmap_longmode = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_longmode)\n\t\tgoto out3;\n\n\tvmx_msr_bitmap_longmode_x2apic =\n\t\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_longmode_x2apic)\n\t\tgoto out4;\n\n\tif (nested) {\n\t\tvmx_msr_bitmap_nested =\n\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\t\tif (!vmx_msr_bitmap_nested)\n\t\t\tgoto out5;\n\t}\n\n\tvmx_vmread_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_vmread_bitmap)\n\t\tgoto out6;\n\n\tvmx_vmwrite_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_vmwrite_bitmap)\n\t\tgoto out7;\n\n\tmemset(vmx_vmread_bitmap, 0xff, PAGE_SIZE);\n\tmemset(vmx_vmwrite_bitmap, 0xff, PAGE_SIZE);\n\n\t/*\n\t * Allow direct access to the PC debug port (it is often used for I/O\n\t * delays, but the vmexits simply slow things down).\n\t */\n\tmemset(vmx_io_bitmap_a, 0xff, PAGE_SIZE);\n\tclear_bit(0x80, vmx_io_bitmap_a);\n\n\tmemset(vmx_io_bitmap_b, 0xff, PAGE_SIZE);\n\n\tmemset(vmx_msr_bitmap_legacy, 0xff, PAGE_SIZE);\n\tmemset(vmx_msr_bitmap_longmode, 0xff, PAGE_SIZE);\n\tif (nested)\n\t\tmemset(vmx_msr_bitmap_nested, 0xff, PAGE_SIZE);\n\n\tif (setup_vmcs_config(&vmcs_config) < 0) {\n\t\tr = -EIO;\n\t\tgoto out8;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_NX))\n\t\tkvm_enable_efer_bits(EFER_NX);\n\n\tif (!cpu_has_vmx_vpid())\n\t\tenable_vpid = 0;\n\tif (!cpu_has_vmx_shadow_vmcs())\n\t\tenable_shadow_vmcs = 0;\n\tif (enable_shadow_vmcs)\n\t\tinit_vmcs_shadow_fields();\n\n\tif (!cpu_has_vmx_ept() ||\n\t    !cpu_has_vmx_ept_4levels()) {\n\t\tenable_ept = 0;\n\t\tenable_unrestricted_guest = 0;\n\t\tenable_ept_ad_bits = 0;\n\t}\n\n\tif (!cpu_has_vmx_ept_ad_bits())\n\t\tenable_ept_ad_bits = 0;\n\n\tif (!cpu_has_vmx_unrestricted_guest())\n\t\tenable_unrestricted_guest = 0;\n\n\tif (!cpu_has_vmx_flexpriority())\n\t\tflexpriority_enabled = 0;\n\n\t/*\n\t * set_apic_access_page_addr() is used to reload apic access\n\t * page upon invalidation.  No need to do anything if not\n\t * using the APIC_ACCESS_ADDR VMCS field.\n\t */\n\tif (!flexpriority_enabled)\n\t\tkvm_x86_ops->set_apic_access_page_addr = NULL;\n\n\tif (!cpu_has_vmx_tpr_shadow())\n\t\tkvm_x86_ops->update_cr8_intercept = NULL;\n\n\tif (enable_ept && !cpu_has_vmx_ept_2m_page())\n\t\tkvm_disable_largepages();\n\n\tif (!cpu_has_vmx_ple())\n\t\tple_gap = 0;\n\n\tif (!cpu_has_vmx_apicv())\n\t\tenable_apicv = 0;\n\n\tif (cpu_has_vmx_tsc_scaling()) {\n\t\tkvm_has_tsc_control = true;\n\t\tkvm_max_tsc_scaling_ratio = KVM_VMX_TSC_MULTIPLIER_MAX;\n\t\tkvm_tsc_scaling_ratio_frac_bits = 48;\n\t}\n\n\tvmx_disable_intercept_for_msr(MSR_FS_BASE, false);\n\tvmx_disable_intercept_for_msr(MSR_GS_BASE, false);\n\tvmx_disable_intercept_for_msr(MSR_KERNEL_GS_BASE, true);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_CS, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_ESP, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_EIP, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_BNDCFGS, true);\n\n\tmemcpy(vmx_msr_bitmap_legacy_x2apic,\n\t\t\tvmx_msr_bitmap_legacy, PAGE_SIZE);\n\tmemcpy(vmx_msr_bitmap_longmode_x2apic,\n\t\t\tvmx_msr_bitmap_longmode, PAGE_SIZE);\n\n\tset_bit(0, vmx_vpid_bitmap); /* 0 is reserved for host */\n\n\tif (enable_apicv) {\n\t\tfor (msr = 0x800; msr <= 0x8ff; msr++)\n\t\t\tvmx_disable_intercept_msr_read_x2apic(msr);\n\n\t\t/* According SDM, in x2apic mode, the whole id reg is used.\n\t\t * But in KVM, it only use the highest eight bits. Need to\n\t\t * intercept it */\n\t\tvmx_enable_intercept_msr_read_x2apic(0x802);\n\t\t/* TMCCT */\n\t\tvmx_enable_intercept_msr_read_x2apic(0x839);\n\t\t/* TPR */\n\t\tvmx_disable_intercept_msr_write_x2apic(0x808);\n\t\t/* EOI */\n\t\tvmx_disable_intercept_msr_write_x2apic(0x80b);\n\t\t/* SELF-IPI */\n\t\tvmx_disable_intercept_msr_write_x2apic(0x83f);\n\t}\n\n\tif (enable_ept) {\n\t\tkvm_mmu_set_mask_ptes(0ull,\n\t\t\t(enable_ept_ad_bits) ? VMX_EPT_ACCESS_BIT : 0ull,\n\t\t\t(enable_ept_ad_bits) ? VMX_EPT_DIRTY_BIT : 0ull,\n\t\t\t0ull, VMX_EPT_EXECUTABLE_MASK);\n\t\tept_set_mmio_spte_mask();\n\t\tkvm_enable_tdp();\n\t} else\n\t\tkvm_disable_tdp();\n\n\tupdate_ple_window_actual_max();\n\n\t/*\n\t * Only enable PML when hardware supports PML feature, and both EPT\n\t * and EPT A/D bit features are enabled -- PML depends on them to work.\n\t */\n\tif (!enable_ept || !enable_ept_ad_bits || !cpu_has_vmx_pml())\n\t\tenable_pml = 0;\n\n\tif (!enable_pml) {\n\t\tkvm_x86_ops->slot_enable_log_dirty = NULL;\n\t\tkvm_x86_ops->slot_disable_log_dirty = NULL;\n\t\tkvm_x86_ops->flush_log_dirty = NULL;\n\t\tkvm_x86_ops->enable_log_dirty_pt_masked = NULL;\n\t}\n\n\tkvm_set_posted_intr_wakeup_handler(wakeup_handler);\n\n\treturn alloc_kvm_area();\n\nout8:\n\tfree_page((unsigned long)vmx_vmwrite_bitmap);\nout7:\n\tfree_page((unsigned long)vmx_vmread_bitmap);\nout6:\n\tif (nested)\n\t\tfree_page((unsigned long)vmx_msr_bitmap_nested);\nout5:\n\tfree_page((unsigned long)vmx_msr_bitmap_longmode_x2apic);\nout4:\n\tfree_page((unsigned long)vmx_msr_bitmap_longmode);\nout3:\n\tfree_page((unsigned long)vmx_msr_bitmap_legacy_x2apic);\nout2:\n\tfree_page((unsigned long)vmx_msr_bitmap_legacy);\nout1:\n\tfree_page((unsigned long)vmx_io_bitmap_b);\nout:\n\tfree_page((unsigned long)vmx_io_bitmap_a);\n\n    return r;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic __init int hardware_setup(void)\n{\n\tint r = -ENOMEM, i, msr;\n\n\trdmsrl_safe(MSR_EFER, &host_efer);\n\n\tfor (i = 0; i < ARRAY_SIZE(vmx_msr_index); ++i)\n\t\tkvm_define_shared_msr(i, vmx_msr_index[i]);\n\n\tvmx_io_bitmap_a = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_io_bitmap_a)\n\t\treturn r;\n\n\tvmx_io_bitmap_b = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_io_bitmap_b)\n\t\tgoto out;\n\n\tvmx_msr_bitmap_legacy = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_legacy)\n\t\tgoto out1;\n\n\tvmx_msr_bitmap_legacy_x2apic =\n\t\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_legacy_x2apic)\n\t\tgoto out2;\n\n\tvmx_msr_bitmap_longmode = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_longmode)\n\t\tgoto out3;\n\n\tvmx_msr_bitmap_longmode_x2apic =\n\t\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_longmode_x2apic)\n\t\tgoto out4;\n\n\tif (nested) {\n\t\tvmx_msr_bitmap_nested =\n\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\t\tif (!vmx_msr_bitmap_nested)\n\t\t\tgoto out5;\n\t}\n\n\tvmx_vmread_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_vmread_bitmap)\n\t\tgoto out6;\n\n\tvmx_vmwrite_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_vmwrite_bitmap)\n\t\tgoto out7;\n\n\tmemset(vmx_vmread_bitmap, 0xff, PAGE_SIZE);\n\tmemset(vmx_vmwrite_bitmap, 0xff, PAGE_SIZE);\n\n\t/*\n\t * Allow direct access to the PC debug port (it is often used for I/O\n\t * delays, but the vmexits simply slow things down).\n\t */\n\tmemset(vmx_io_bitmap_a, 0xff, PAGE_SIZE);\n\tclear_bit(0x80, vmx_io_bitmap_a);\n\n\tmemset(vmx_io_bitmap_b, 0xff, PAGE_SIZE);\n\n\tmemset(vmx_msr_bitmap_legacy, 0xff, PAGE_SIZE);\n\tmemset(vmx_msr_bitmap_longmode, 0xff, PAGE_SIZE);\n\tif (nested)\n\t\tmemset(vmx_msr_bitmap_nested, 0xff, PAGE_SIZE);\n\n\tif (setup_vmcs_config(&vmcs_config) < 0) {\n\t\tr = -EIO;\n\t\tgoto out8;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_NX))\n\t\tkvm_enable_efer_bits(EFER_NX);\n\n\tif (!cpu_has_vmx_vpid())\n\t\tenable_vpid = 0;\n\tif (!cpu_has_vmx_shadow_vmcs())\n\t\tenable_shadow_vmcs = 0;\n\tif (enable_shadow_vmcs)\n\t\tinit_vmcs_shadow_fields();\n\n\tif (!cpu_has_vmx_ept() ||\n\t    !cpu_has_vmx_ept_4levels()) {\n\t\tenable_ept = 0;\n\t\tenable_unrestricted_guest = 0;\n\t\tenable_ept_ad_bits = 0;\n\t}\n\n\tif (!cpu_has_vmx_ept_ad_bits())\n\t\tenable_ept_ad_bits = 0;\n\n\tif (!cpu_has_vmx_unrestricted_guest())\n\t\tenable_unrestricted_guest = 0;\n\n\tif (!cpu_has_vmx_flexpriority())\n\t\tflexpriority_enabled = 0;\n\n\t/*\n\t * set_apic_access_page_addr() is used to reload apic access\n\t * page upon invalidation.  No need to do anything if not\n\t * using the APIC_ACCESS_ADDR VMCS field.\n\t */\n\tif (!flexpriority_enabled)\n\t\tkvm_x86_ops->set_apic_access_page_addr = NULL;\n\n\tif (!cpu_has_vmx_tpr_shadow())\n\t\tkvm_x86_ops->update_cr8_intercept = NULL;\n\n\tif (enable_ept && !cpu_has_vmx_ept_2m_page())\n\t\tkvm_disable_largepages();\n\n\tif (!cpu_has_vmx_ple())\n\t\tple_gap = 0;\n\n\tif (!cpu_has_vmx_apicv())\n\t\tenable_apicv = 0;\n\n\tif (cpu_has_vmx_tsc_scaling()) {\n\t\tkvm_has_tsc_control = true;\n\t\tkvm_max_tsc_scaling_ratio = KVM_VMX_TSC_MULTIPLIER_MAX;\n\t\tkvm_tsc_scaling_ratio_frac_bits = 48;\n\t}\n\n\tvmx_disable_intercept_for_msr(MSR_FS_BASE, false);\n\tvmx_disable_intercept_for_msr(MSR_GS_BASE, false);\n\tvmx_disable_intercept_for_msr(MSR_KERNEL_GS_BASE, true);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_CS, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_ESP, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_EIP, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_BNDCFGS, true);\n\n\tmemcpy(vmx_msr_bitmap_legacy_x2apic,\n\t\t\tvmx_msr_bitmap_legacy, PAGE_SIZE);\n\tmemcpy(vmx_msr_bitmap_longmode_x2apic,\n\t\t\tvmx_msr_bitmap_longmode, PAGE_SIZE);\n\n\tset_bit(0, vmx_vpid_bitmap); /* 0 is reserved for host */\n\n\tfor (msr = 0x800; msr <= 0x8ff; msr++)\n\t\tvmx_disable_intercept_msr_read_x2apic(msr);\n\n\t/* According SDM, in x2apic mode, the whole id reg is used.  But in\n\t * KVM, it only use the highest eight bits. Need to intercept it */\n\tvmx_enable_intercept_msr_read_x2apic(0x802);\n\t/* TMCCT */\n\tvmx_enable_intercept_msr_read_x2apic(0x839);\n\t/* TPR */\n\tvmx_disable_intercept_msr_write_x2apic(0x808);\n\t/* EOI */\n\tvmx_disable_intercept_msr_write_x2apic(0x80b);\n\t/* SELF-IPI */\n\tvmx_disable_intercept_msr_write_x2apic(0x83f);\n\n\tif (enable_ept) {\n\t\tkvm_mmu_set_mask_ptes(0ull,\n\t\t\t(enable_ept_ad_bits) ? VMX_EPT_ACCESS_BIT : 0ull,\n\t\t\t(enable_ept_ad_bits) ? VMX_EPT_DIRTY_BIT : 0ull,\n\t\t\t0ull, VMX_EPT_EXECUTABLE_MASK);\n\t\tept_set_mmio_spte_mask();\n\t\tkvm_enable_tdp();\n\t} else\n\t\tkvm_disable_tdp();\n\n\tupdate_ple_window_actual_max();\n\n\t/*\n\t * Only enable PML when hardware supports PML feature, and both EPT\n\t * and EPT A/D bit features are enabled -- PML depends on them to work.\n\t */\n\tif (!enable_ept || !enable_ept_ad_bits || !cpu_has_vmx_pml())\n\t\tenable_pml = 0;\n\n\tif (!enable_pml) {\n\t\tkvm_x86_ops->slot_enable_log_dirty = NULL;\n\t\tkvm_x86_ops->slot_disable_log_dirty = NULL;\n\t\tkvm_x86_ops->flush_log_dirty = NULL;\n\t\tkvm_x86_ops->enable_log_dirty_pt_masked = NULL;\n\t}\n\n\tkvm_set_posted_intr_wakeup_handler(wakeup_handler);\n\n\treturn alloc_kvm_area();\n\nout8:\n\tfree_page((unsigned long)vmx_vmwrite_bitmap);\nout7:\n\tfree_page((unsigned long)vmx_vmread_bitmap);\nout6:\n\tif (nested)\n\t\tfree_page((unsigned long)vmx_msr_bitmap_nested);\nout5:\n\tfree_page((unsigned long)vmx_msr_bitmap_longmode_x2apic);\nout4:\n\tfree_page((unsigned long)vmx_msr_bitmap_longmode);\nout3:\n\tfree_page((unsigned long)vmx_msr_bitmap_legacy_x2apic);\nout2:\n\tfree_page((unsigned long)vmx_msr_bitmap_legacy);\nout1:\n\tfree_page((unsigned long)vmx_io_bitmap_b);\nout:\n\tfree_page((unsigned long)vmx_io_bitmap_a);\n\n    return r;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 153,
            "cve_id": "CVE-2013-0268",
            "code_snippet": "static int msr_open(struct inode *inode, struct file *file)\n{\n\tunsigned int cpu;\n\tstruct cpuinfo_x86 *c;\n\n\tcpu = iminor(file->f_path.dentry->d_inode);\n\tif (cpu >= nr_cpu_ids || !cpu_online(cpu))\n\t\treturn -ENXIO;\t/* No such CPU */\n\n\tc = &cpu_data(cpu);\n\tif (!cpu_has(c, X86_FEATURE_MSR))\n\t\treturn -EIO;\t/* MSR not supported */\n\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": " */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx;\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tstruct perf_event_context *gctx = group_leader->ctx;\n\n\t\tmutex_lock(&gctx->mutex);\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\n\tif (move_group) {\n\t\tsynchronize_rcu();\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}",
                        "code_after_change": " */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *uninitialized_var(gctx);\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tgctx = group_leader->ctx;\n\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t} else {\n\t\tmutex_lock(&ctx->mutex);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\tif (move_group) {\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\n\tif (move_group) {\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}",
                        "cve_id": "CVE-2016-6786"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int msr_open(struct inode *inode, struct file *file)\n{\n\tunsigned int cpu;\n\tstruct cpuinfo_x86 *c;\n\n\tcpu = iminor(file->f_path.dentry->d_inode);\n\tif (cpu >= nr_cpu_ids || !cpu_online(cpu))\n\t\treturn -ENXIO;\t/* No such CPU */\n\n\tc = &cpu_data(cpu);\n\tif (!cpu_has(c, X86_FEATURE_MSR))\n\t\treturn -EIO;\t/* MSR not supported */\n\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\n */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx;\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tstruct perf_event_context *gctx = group_leader->ctx;\n\n\t\tmutex_lock(&gctx->mutex);\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\n\tif (move_group) {\n\t\tsynchronize_rcu();\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\n */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *uninitialized_var(gctx);\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tgctx = group_leader->ctx;\n\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t} else {\n\t\tmutex_lock(&ctx->mutex);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\tif (move_group) {\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\n\tif (move_group) {\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int hfsplus_readdir(struct file *filp, void *dirent, filldir_t filldir)\n{\n\tstruct inode *inode = filp->f_path.dentry->d_inode;\n\tstruct super_block *sb = inode->i_sb;\n\tint len, err;\n\tchar strbuf[HFSPLUS_MAX_STRLEN + 1];\n\thfsplus_cat_entry entry;\n\tstruct hfs_find_data fd;\n\tstruct hfsplus_readdir_data *rd;\n\tu16 type;\n\n\tif (filp->f_pos >= inode->i_size)\n\t\treturn 0;\n\n\terr = hfs_find_init(HFSPLUS_SB(sb)->cat_tree, &fd);\n\tif (err)\n\t\treturn err;\n\thfsplus_cat_build_key(sb, fd.search_key, inode->i_ino, NULL);\n\terr = hfs_brec_find(&fd);\n\tif (err)\n\t\tgoto out;\n\n\tswitch ((u32)filp->f_pos) {\n\tcase 0:\n\t\t/* This is completely artificial... */\n\t\tif (filldir(dirent, \".\", 1, 0, inode->i_ino, DT_DIR))\n\t\t\tgoto out;\n\t\tfilp->f_pos++;\n\t\t/* fall through */\n\tcase 1:\n\t\thfs_bnode_read(fd.bnode, &entry, fd.entryoffset,\n\t\t\tfd.entrylength);\n\t\tif (be16_to_cpu(entry.type) != HFSPLUS_FOLDER_THREAD) {\n\t\t\tprintk(KERN_ERR \"hfs: bad catalog folder thread\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (fd.entrylength < HFSPLUS_MIN_THREAD_SZ) {\n\t\t\tprintk(KERN_ERR \"hfs: truncated catalog thread\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (filldir(dirent, \"..\", 2, 1,\n\t\t\t    be32_to_cpu(entry.thread.parentID), DT_DIR))\n\t\t\tgoto out;\n\t\tfilp->f_pos++;\n\t\t/* fall through */\n\tdefault:\n\t\tif (filp->f_pos >= inode->i_size)\n\t\t\tgoto out;\n\t\terr = hfs_brec_goto(&fd, filp->f_pos - 1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tfor (;;) {\n\t\tif (be32_to_cpu(fd.key->cat.parent) != inode->i_ino) {\n\t\t\tprintk(KERN_ERR \"hfs: walked past end of dir\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\thfs_bnode_read(fd.bnode, &entry, fd.entryoffset,\n\t\t\tfd.entrylength);\n\t\ttype = be16_to_cpu(entry.type);\n\t\tlen = HFSPLUS_MAX_STRLEN;\n\t\terr = hfsplus_uni2asc(sb, &fd.key->cat.name, strbuf, &len);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tif (type == HFSPLUS_FOLDER) {\n\t\t\tif (fd.entrylength <\n\t\t\t\t\tsizeof(struct hfsplus_cat_folder)) {\n\t\t\t\tprintk(KERN_ERR \"hfs: small dir entry\\n\");\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (HFSPLUS_SB(sb)->hidden_dir &&\n\t\t\t    HFSPLUS_SB(sb)->hidden_dir->i_ino ==\n\t\t\t\t\tbe32_to_cpu(entry.folder.id))\n\t\t\t\tgoto next;\n\t\t\tif (filldir(dirent, strbuf, len, filp->f_pos,\n\t\t\t\t    be32_to_cpu(entry.folder.id), DT_DIR))\n\t\t\t\tbreak;\n\t\t} else if (type == HFSPLUS_FILE) {\n\t\t\tif (fd.entrylength < sizeof(struct hfsplus_cat_file)) {\n\t\t\t\tprintk(KERN_ERR \"hfs: small file entry\\n\");\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (filldir(dirent, strbuf, len, filp->f_pos,\n\t\t\t\t    be32_to_cpu(entry.file.id), DT_REG))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tprintk(KERN_ERR \"hfs: bad catalog entry type\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\nnext:\n\t\tfilp->f_pos++;\n\t\tif (filp->f_pos >= inode->i_size)\n\t\t\tgoto out;\n\t\terr = hfs_brec_goto(&fd, 1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\trd = filp->private_data;\n\tif (!rd) {\n\t\trd = kmalloc(sizeof(struct hfsplus_readdir_data), GFP_KERNEL);\n\t\tif (!rd) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tfilp->private_data = rd;\n\t\trd->file = filp;\n\t\tlist_add(&rd->list, &HFSPLUS_I(inode)->open_dir_list);\n\t}\n\tmemcpy(&rd->key, fd.key, sizeof(struct hfsplus_cat_key));\nout:\n\thfs_find_exit(&fd);\n\treturn err;\n}",
                        "code_after_change": "static int hfsplus_readdir(struct file *filp, void *dirent, filldir_t filldir)\n{\n\tstruct inode *inode = filp->f_path.dentry->d_inode;\n\tstruct super_block *sb = inode->i_sb;\n\tint len, err;\n\tchar strbuf[HFSPLUS_MAX_STRLEN + 1];\n\thfsplus_cat_entry entry;\n\tstruct hfs_find_data fd;\n\tstruct hfsplus_readdir_data *rd;\n\tu16 type;\n\n\tif (filp->f_pos >= inode->i_size)\n\t\treturn 0;\n\n\terr = hfs_find_init(HFSPLUS_SB(sb)->cat_tree, &fd);\n\tif (err)\n\t\treturn err;\n\thfsplus_cat_build_key(sb, fd.search_key, inode->i_ino, NULL);\n\terr = hfs_brec_find(&fd);\n\tif (err)\n\t\tgoto out;\n\n\tswitch ((u32)filp->f_pos) {\n\tcase 0:\n\t\t/* This is completely artificial... */\n\t\tif (filldir(dirent, \".\", 1, 0, inode->i_ino, DT_DIR))\n\t\t\tgoto out;\n\t\tfilp->f_pos++;\n\t\t/* fall through */\n\tcase 1:\n\t\tif (fd.entrylength > sizeof(entry) || fd.entrylength < 0) {\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\n\t\thfs_bnode_read(fd.bnode, &entry, fd.entryoffset,\n\t\t\tfd.entrylength);\n\t\tif (be16_to_cpu(entry.type) != HFSPLUS_FOLDER_THREAD) {\n\t\t\tprintk(KERN_ERR \"hfs: bad catalog folder thread\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (fd.entrylength < HFSPLUS_MIN_THREAD_SZ) {\n\t\t\tprintk(KERN_ERR \"hfs: truncated catalog thread\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (filldir(dirent, \"..\", 2, 1,\n\t\t\t    be32_to_cpu(entry.thread.parentID), DT_DIR))\n\t\t\tgoto out;\n\t\tfilp->f_pos++;\n\t\t/* fall through */\n\tdefault:\n\t\tif (filp->f_pos >= inode->i_size)\n\t\t\tgoto out;\n\t\terr = hfs_brec_goto(&fd, filp->f_pos - 1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tfor (;;) {\n\t\tif (be32_to_cpu(fd.key->cat.parent) != inode->i_ino) {\n\t\t\tprintk(KERN_ERR \"hfs: walked past end of dir\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (fd.entrylength > sizeof(entry) || fd.entrylength < 0) {\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\n\t\thfs_bnode_read(fd.bnode, &entry, fd.entryoffset,\n\t\t\tfd.entrylength);\n\t\ttype = be16_to_cpu(entry.type);\n\t\tlen = HFSPLUS_MAX_STRLEN;\n\t\terr = hfsplus_uni2asc(sb, &fd.key->cat.name, strbuf, &len);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tif (type == HFSPLUS_FOLDER) {\n\t\t\tif (fd.entrylength <\n\t\t\t\t\tsizeof(struct hfsplus_cat_folder)) {\n\t\t\t\tprintk(KERN_ERR \"hfs: small dir entry\\n\");\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (HFSPLUS_SB(sb)->hidden_dir &&\n\t\t\t    HFSPLUS_SB(sb)->hidden_dir->i_ino ==\n\t\t\t\t\tbe32_to_cpu(entry.folder.id))\n\t\t\t\tgoto next;\n\t\t\tif (filldir(dirent, strbuf, len, filp->f_pos,\n\t\t\t\t    be32_to_cpu(entry.folder.id), DT_DIR))\n\t\t\t\tbreak;\n\t\t} else if (type == HFSPLUS_FILE) {\n\t\t\tif (fd.entrylength < sizeof(struct hfsplus_cat_file)) {\n\t\t\t\tprintk(KERN_ERR \"hfs: small file entry\\n\");\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (filldir(dirent, strbuf, len, filp->f_pos,\n\t\t\t\t    be32_to_cpu(entry.file.id), DT_REG))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tprintk(KERN_ERR \"hfs: bad catalog entry type\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\nnext:\n\t\tfilp->f_pos++;\n\t\tif (filp->f_pos >= inode->i_size)\n\t\t\tgoto out;\n\t\terr = hfs_brec_goto(&fd, 1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\trd = filp->private_data;\n\tif (!rd) {\n\t\trd = kmalloc(sizeof(struct hfsplus_readdir_data), GFP_KERNEL);\n\t\tif (!rd) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tfilp->private_data = rd;\n\t\trd->file = filp;\n\t\tlist_add(&rd->list, &HFSPLUS_I(inode)->open_dir_list);\n\t}\n\tmemcpy(&rd->key, fd.key, sizeof(struct hfsplus_cat_key));\nout:\n\thfs_find_exit(&fd);\n\treturn err;\n}",
                        "cve_id": "CVE-2012-2319"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int msr_open(struct inode *inode, struct file *file)\n{\n\tunsigned int cpu;\n\tstruct cpuinfo_x86 *c;\n\n\tcpu = iminor(file->f_path.dentry->d_inode);\n\tif (cpu >= nr_cpu_ids || !cpu_online(cpu))\n\t\treturn -ENXIO;\t/* No such CPU */\n\n\tc = &cpu_data(cpu);\n\tif (!cpu_has(c, X86_FEATURE_MSR))\n\t\treturn -EIO;\t/* MSR not supported */\n\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int hfsplus_readdir(struct file *filp, void *dirent, filldir_t filldir)\n{\n\tstruct inode *inode = filp->f_path.dentry->d_inode;\n\tstruct super_block *sb = inode->i_sb;\n\tint len, err;\n\tchar strbuf[HFSPLUS_MAX_STRLEN + 1];\n\thfsplus_cat_entry entry;\n\tstruct hfs_find_data fd;\n\tstruct hfsplus_readdir_data *rd;\n\tu16 type;\n\n\tif (filp->f_pos >= inode->i_size)\n\t\treturn 0;\n\n\terr = hfs_find_init(HFSPLUS_SB(sb)->cat_tree, &fd);\n\tif (err)\n\t\treturn err;\n\thfsplus_cat_build_key(sb, fd.search_key, inode->i_ino, NULL);\n\terr = hfs_brec_find(&fd);\n\tif (err)\n\t\tgoto out;\n\n\tswitch ((u32)filp->f_pos) {\n\tcase 0:\n\t\t/* This is completely artificial... */\n\t\tif (filldir(dirent, \".\", 1, 0, inode->i_ino, DT_DIR))\n\t\t\tgoto out;\n\t\tfilp->f_pos++;\n\t\t/* fall through */\n\tcase 1:\n\t\thfs_bnode_read(fd.bnode, &entry, fd.entryoffset,\n\t\t\tfd.entrylength);\n\t\tif (be16_to_cpu(entry.type) != HFSPLUS_FOLDER_THREAD) {\n\t\t\tprintk(KERN_ERR \"hfs: bad catalog folder thread\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (fd.entrylength < HFSPLUS_MIN_THREAD_SZ) {\n\t\t\tprintk(KERN_ERR \"hfs: truncated catalog thread\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (filldir(dirent, \"..\", 2, 1,\n\t\t\t    be32_to_cpu(entry.thread.parentID), DT_DIR))\n\t\t\tgoto out;\n\t\tfilp->f_pos++;\n\t\t/* fall through */\n\tdefault:\n\t\tif (filp->f_pos >= inode->i_size)\n\t\t\tgoto out;\n\t\terr = hfs_brec_goto(&fd, filp->f_pos - 1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tfor (;;) {\n\t\tif (be32_to_cpu(fd.key->cat.parent) != inode->i_ino) {\n\t\t\tprintk(KERN_ERR \"hfs: walked past end of dir\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\thfs_bnode_read(fd.bnode, &entry, fd.entryoffset,\n\t\t\tfd.entrylength);\n\t\ttype = be16_to_cpu(entry.type);\n\t\tlen = HFSPLUS_MAX_STRLEN;\n\t\terr = hfsplus_uni2asc(sb, &fd.key->cat.name, strbuf, &len);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tif (type == HFSPLUS_FOLDER) {\n\t\t\tif (fd.entrylength <\n\t\t\t\t\tsizeof(struct hfsplus_cat_folder)) {\n\t\t\t\tprintk(KERN_ERR \"hfs: small dir entry\\n\");\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (HFSPLUS_SB(sb)->hidden_dir &&\n\t\t\t    HFSPLUS_SB(sb)->hidden_dir->i_ino ==\n\t\t\t\t\tbe32_to_cpu(entry.folder.id))\n\t\t\t\tgoto next;\n\t\t\tif (filldir(dirent, strbuf, len, filp->f_pos,\n\t\t\t\t    be32_to_cpu(entry.folder.id), DT_DIR))\n\t\t\t\tbreak;\n\t\t} else if (type == HFSPLUS_FILE) {\n\t\t\tif (fd.entrylength < sizeof(struct hfsplus_cat_file)) {\n\t\t\t\tprintk(KERN_ERR \"hfs: small file entry\\n\");\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (filldir(dirent, strbuf, len, filp->f_pos,\n\t\t\t\t    be32_to_cpu(entry.file.id), DT_REG))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tprintk(KERN_ERR \"hfs: bad catalog entry type\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\nnext:\n\t\tfilp->f_pos++;\n\t\tif (filp->f_pos >= inode->i_size)\n\t\t\tgoto out;\n\t\terr = hfs_brec_goto(&fd, 1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\trd = filp->private_data;\n\tif (!rd) {\n\t\trd = kmalloc(sizeof(struct hfsplus_readdir_data), GFP_KERNEL);\n\t\tif (!rd) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tfilp->private_data = rd;\n\t\trd->file = filp;\n\t\tlist_add(&rd->list, &HFSPLUS_I(inode)->open_dir_list);\n\t}\n\tmemcpy(&rd->key, fd.key, sizeof(struct hfsplus_cat_key));\nout:\n\thfs_find_exit(&fd);\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int hfsplus_readdir(struct file *filp, void *dirent, filldir_t filldir)\n{\n\tstruct inode *inode = filp->f_path.dentry->d_inode;\n\tstruct super_block *sb = inode->i_sb;\n\tint len, err;\n\tchar strbuf[HFSPLUS_MAX_STRLEN + 1];\n\thfsplus_cat_entry entry;\n\tstruct hfs_find_data fd;\n\tstruct hfsplus_readdir_data *rd;\n\tu16 type;\n\n\tif (filp->f_pos >= inode->i_size)\n\t\treturn 0;\n\n\terr = hfs_find_init(HFSPLUS_SB(sb)->cat_tree, &fd);\n\tif (err)\n\t\treturn err;\n\thfsplus_cat_build_key(sb, fd.search_key, inode->i_ino, NULL);\n\terr = hfs_brec_find(&fd);\n\tif (err)\n\t\tgoto out;\n\n\tswitch ((u32)filp->f_pos) {\n\tcase 0:\n\t\t/* This is completely artificial... */\n\t\tif (filldir(dirent, \".\", 1, 0, inode->i_ino, DT_DIR))\n\t\t\tgoto out;\n\t\tfilp->f_pos++;\n\t\t/* fall through */\n\tcase 1:\n\t\tif (fd.entrylength > sizeof(entry) || fd.entrylength < 0) {\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\n\t\thfs_bnode_read(fd.bnode, &entry, fd.entryoffset,\n\t\t\tfd.entrylength);\n\t\tif (be16_to_cpu(entry.type) != HFSPLUS_FOLDER_THREAD) {\n\t\t\tprintk(KERN_ERR \"hfs: bad catalog folder thread\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (fd.entrylength < HFSPLUS_MIN_THREAD_SZ) {\n\t\t\tprintk(KERN_ERR \"hfs: truncated catalog thread\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (filldir(dirent, \"..\", 2, 1,\n\t\t\t    be32_to_cpu(entry.thread.parentID), DT_DIR))\n\t\t\tgoto out;\n\t\tfilp->f_pos++;\n\t\t/* fall through */\n\tdefault:\n\t\tif (filp->f_pos >= inode->i_size)\n\t\t\tgoto out;\n\t\terr = hfs_brec_goto(&fd, filp->f_pos - 1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tfor (;;) {\n\t\tif (be32_to_cpu(fd.key->cat.parent) != inode->i_ino) {\n\t\t\tprintk(KERN_ERR \"hfs: walked past end of dir\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (fd.entrylength > sizeof(entry) || fd.entrylength < 0) {\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\n\t\thfs_bnode_read(fd.bnode, &entry, fd.entryoffset,\n\t\t\tfd.entrylength);\n\t\ttype = be16_to_cpu(entry.type);\n\t\tlen = HFSPLUS_MAX_STRLEN;\n\t\terr = hfsplus_uni2asc(sb, &fd.key->cat.name, strbuf, &len);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tif (type == HFSPLUS_FOLDER) {\n\t\t\tif (fd.entrylength <\n\t\t\t\t\tsizeof(struct hfsplus_cat_folder)) {\n\t\t\t\tprintk(KERN_ERR \"hfs: small dir entry\\n\");\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (HFSPLUS_SB(sb)->hidden_dir &&\n\t\t\t    HFSPLUS_SB(sb)->hidden_dir->i_ino ==\n\t\t\t\t\tbe32_to_cpu(entry.folder.id))\n\t\t\t\tgoto next;\n\t\t\tif (filldir(dirent, strbuf, len, filp->f_pos,\n\t\t\t\t    be32_to_cpu(entry.folder.id), DT_DIR))\n\t\t\t\tbreak;\n\t\t} else if (type == HFSPLUS_FILE) {\n\t\t\tif (fd.entrylength < sizeof(struct hfsplus_cat_file)) {\n\t\t\t\tprintk(KERN_ERR \"hfs: small file entry\\n\");\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (filldir(dirent, strbuf, len, filp->f_pos,\n\t\t\t\t    be32_to_cpu(entry.file.id), DT_REG))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tprintk(KERN_ERR \"hfs: bad catalog entry type\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\nnext:\n\t\tfilp->f_pos++;\n\t\tif (filp->f_pos >= inode->i_size)\n\t\t\tgoto out;\n\t\terr = hfs_brec_goto(&fd, 1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\trd = filp->private_data;\n\tif (!rd) {\n\t\trd = kmalloc(sizeof(struct hfsplus_readdir_data), GFP_KERNEL);\n\t\tif (!rd) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tfilp->private_data = rd;\n\t\trd->file = filp;\n\t\tlist_add(&rd->list, &HFSPLUS_I(inode)->open_dir_list);\n\t}\n\tmemcpy(&rd->key, fd.key, sizeof(struct hfsplus_cat_key));\nout:\n\thfs_find_exit(&fd);\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int perf_pmu_register(struct pmu *pmu, const char *name, int type)\n{\n\tint cpu, ret;\n\n\tmutex_lock(&pmus_lock);\n\tret = -ENOMEM;\n\tpmu->pmu_disable_count = alloc_percpu(int);\n\tif (!pmu->pmu_disable_count)\n\t\tgoto unlock;\n\n\tpmu->type = -1;\n\tif (!name)\n\t\tgoto skip_type;\n\tpmu->name = name;\n\n\tif (type < 0) {\n\t\ttype = idr_alloc(&pmu_idr, pmu, PERF_TYPE_MAX, 0, GFP_KERNEL);\n\t\tif (type < 0) {\n\t\t\tret = type;\n\t\t\tgoto free_pdc;\n\t\t}\n\t}\n\tpmu->type = type;\n\n\tif (pmu_bus_running) {\n\t\tret = pmu_dev_alloc(pmu);\n\t\tif (ret)\n\t\t\tgoto free_idr;\n\t}\n\nskip_type:\n\tpmu->pmu_cpu_context = find_pmu_context(pmu->task_ctx_nr);\n\tif (pmu->pmu_cpu_context)\n\t\tgoto got_cpu_context;\n\n\tret = -ENOMEM;\n\tpmu->pmu_cpu_context = alloc_percpu(struct perf_cpu_context);\n\tif (!pmu->pmu_cpu_context)\n\t\tgoto free_dev;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct perf_cpu_context *cpuctx;\n\n\t\tcpuctx = per_cpu_ptr(pmu->pmu_cpu_context, cpu);\n\t\t__perf_event_init_context(&cpuctx->ctx);\n\t\tlockdep_set_class(&cpuctx->ctx.mutex, &cpuctx_mutex);\n\t\tlockdep_set_class(&cpuctx->ctx.lock, &cpuctx_lock);\n\t\tcpuctx->ctx.type = cpu_context;\n\t\tcpuctx->ctx.pmu = pmu;\n\n\t\t__perf_cpu_hrtimer_init(cpuctx, cpu);\n\n\t\tINIT_LIST_HEAD(&cpuctx->rotation_list);\n\t\tcpuctx->unique_pmu = pmu;\n\t}\n\ngot_cpu_context:\n\tif (!pmu->start_txn) {\n\t\tif (pmu->pmu_enable) {\n\t\t\t/*\n\t\t\t * If we have pmu_enable/pmu_disable calls, install\n\t\t\t * transaction stubs that use that to try and batch\n\t\t\t * hardware accesses.\n\t\t\t */\n\t\t\tpmu->start_txn  = perf_pmu_start_txn;\n\t\t\tpmu->commit_txn = perf_pmu_commit_txn;\n\t\t\tpmu->cancel_txn = perf_pmu_cancel_txn;\n\t\t} else {\n\t\t\tpmu->start_txn  = perf_pmu_nop_void;\n\t\t\tpmu->commit_txn = perf_pmu_nop_int;\n\t\t\tpmu->cancel_txn = perf_pmu_nop_void;\n\t\t}\n\t}\n\n\tif (!pmu->pmu_enable) {\n\t\tpmu->pmu_enable  = perf_pmu_nop_void;\n\t\tpmu->pmu_disable = perf_pmu_nop_void;\n\t}\n\n\tif (!pmu->event_idx)\n\t\tpmu->event_idx = perf_event_idx_default;\n\n\tlist_add_rcu(&pmu->entry, &pmus);\n\tret = 0;\nunlock:\n\tmutex_unlock(&pmus_lock);\n\n\treturn ret;\n\nfree_dev:\n\tdevice_del(pmu->dev);\n\tput_device(pmu->dev);\n\nfree_idr:\n\tif (pmu->type >= PERF_TYPE_MAX)\n\t\tidr_remove(&pmu_idr, pmu->type);\n\nfree_pdc:\n\tfree_percpu(pmu->pmu_disable_count);\n\tgoto unlock;\n}",
                        "code_after_change": "int perf_pmu_register(struct pmu *pmu, const char *name, int type)\n{\n\tint cpu, ret;\n\n\tmutex_lock(&pmus_lock);\n\tret = -ENOMEM;\n\tpmu->pmu_disable_count = alloc_percpu(int);\n\tif (!pmu->pmu_disable_count)\n\t\tgoto unlock;\n\n\tpmu->type = -1;\n\tif (!name)\n\t\tgoto skip_type;\n\tpmu->name = name;\n\n\tif (type < 0) {\n\t\ttype = idr_alloc(&pmu_idr, pmu, PERF_TYPE_MAX, 0, GFP_KERNEL);\n\t\tif (type < 0) {\n\t\t\tret = type;\n\t\t\tgoto free_pdc;\n\t\t}\n\t}\n\tpmu->type = type;\n\n\tif (pmu_bus_running) {\n\t\tret = pmu_dev_alloc(pmu);\n\t\tif (ret)\n\t\t\tgoto free_idr;\n\t}\n\nskip_type:\n\tpmu->pmu_cpu_context = find_pmu_context(pmu->task_ctx_nr);\n\tif (pmu->pmu_cpu_context)\n\t\tgoto got_cpu_context;\n\n\tret = -ENOMEM;\n\tpmu->pmu_cpu_context = alloc_percpu(struct perf_cpu_context);\n\tif (!pmu->pmu_cpu_context)\n\t\tgoto free_dev;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct perf_cpu_context *cpuctx;\n\n\t\tcpuctx = per_cpu_ptr(pmu->pmu_cpu_context, cpu);\n\t\t__perf_event_init_context(&cpuctx->ctx);\n\t\tlockdep_set_class(&cpuctx->ctx.mutex, &cpuctx_mutex);\n\t\tlockdep_set_class(&cpuctx->ctx.lock, &cpuctx_lock);\n\t\tcpuctx->ctx.pmu = pmu;\n\n\t\t__perf_cpu_hrtimer_init(cpuctx, cpu);\n\n\t\tINIT_LIST_HEAD(&cpuctx->rotation_list);\n\t\tcpuctx->unique_pmu = pmu;\n\t}\n\ngot_cpu_context:\n\tif (!pmu->start_txn) {\n\t\tif (pmu->pmu_enable) {\n\t\t\t/*\n\t\t\t * If we have pmu_enable/pmu_disable calls, install\n\t\t\t * transaction stubs that use that to try and batch\n\t\t\t * hardware accesses.\n\t\t\t */\n\t\t\tpmu->start_txn  = perf_pmu_start_txn;\n\t\t\tpmu->commit_txn = perf_pmu_commit_txn;\n\t\t\tpmu->cancel_txn = perf_pmu_cancel_txn;\n\t\t} else {\n\t\t\tpmu->start_txn  = perf_pmu_nop_void;\n\t\t\tpmu->commit_txn = perf_pmu_nop_int;\n\t\t\tpmu->cancel_txn = perf_pmu_nop_void;\n\t\t}\n\t}\n\n\tif (!pmu->pmu_enable) {\n\t\tpmu->pmu_enable  = perf_pmu_nop_void;\n\t\tpmu->pmu_disable = perf_pmu_nop_void;\n\t}\n\n\tif (!pmu->event_idx)\n\t\tpmu->event_idx = perf_event_idx_default;\n\n\tlist_add_rcu(&pmu->entry, &pmus);\n\tret = 0;\nunlock:\n\tmutex_unlock(&pmus_lock);\n\n\treturn ret;\n\nfree_dev:\n\tdevice_del(pmu->dev);\n\tput_device(pmu->dev);\n\nfree_idr:\n\tif (pmu->type >= PERF_TYPE_MAX)\n\t\tidr_remove(&pmu_idr, pmu->type);\n\nfree_pdc:\n\tfree_percpu(pmu->pmu_disable_count);\n\tgoto unlock;\n}",
                        "cve_id": "CVE-2015-9004"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int msr_open(struct inode *inode, struct file *file)\n{\n\tunsigned int cpu;\n\tstruct cpuinfo_x86 *c;\n\n\tcpu = iminor(file->f_path.dentry->d_inode);\n\tif (cpu >= nr_cpu_ids || !cpu_online(cpu))\n\t\treturn -ENXIO;\t/* No such CPU */\n\n\tc = &cpu_data(cpu);\n\tif (!cpu_has(c, X86_FEATURE_MSR))\n\t\treturn -EIO;\t/* MSR not supported */\n\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nint perf_pmu_register(struct pmu *pmu, const char *name, int type)\n{\n\tint cpu, ret;\n\n\tmutex_lock(&pmus_lock);\n\tret = -ENOMEM;\n\tpmu->pmu_disable_count = alloc_percpu(int);\n\tif (!pmu->pmu_disable_count)\n\t\tgoto unlock;\n\n\tpmu->type = -1;\n\tif (!name)\n\t\tgoto skip_type;\n\tpmu->name = name;\n\n\tif (type < 0) {\n\t\ttype = idr_alloc(&pmu_idr, pmu, PERF_TYPE_MAX, 0, GFP_KERNEL);\n\t\tif (type < 0) {\n\t\t\tret = type;\n\t\t\tgoto free_pdc;\n\t\t}\n\t}\n\tpmu->type = type;\n\n\tif (pmu_bus_running) {\n\t\tret = pmu_dev_alloc(pmu);\n\t\tif (ret)\n\t\t\tgoto free_idr;\n\t}\n\nskip_type:\n\tpmu->pmu_cpu_context = find_pmu_context(pmu->task_ctx_nr);\n\tif (pmu->pmu_cpu_context)\n\t\tgoto got_cpu_context;\n\n\tret = -ENOMEM;\n\tpmu->pmu_cpu_context = alloc_percpu(struct perf_cpu_context);\n\tif (!pmu->pmu_cpu_context)\n\t\tgoto free_dev;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct perf_cpu_context *cpuctx;\n\n\t\tcpuctx = per_cpu_ptr(pmu->pmu_cpu_context, cpu);\n\t\t__perf_event_init_context(&cpuctx->ctx);\n\t\tlockdep_set_class(&cpuctx->ctx.mutex, &cpuctx_mutex);\n\t\tlockdep_set_class(&cpuctx->ctx.lock, &cpuctx_lock);\n\t\tcpuctx->ctx.type = cpu_context;\n\t\tcpuctx->ctx.pmu = pmu;\n\n\t\t__perf_cpu_hrtimer_init(cpuctx, cpu);\n\n\t\tINIT_LIST_HEAD(&cpuctx->rotation_list);\n\t\tcpuctx->unique_pmu = pmu;\n\t}\n\ngot_cpu_context:\n\tif (!pmu->start_txn) {\n\t\tif (pmu->pmu_enable) {\n\t\t\t/*\n\t\t\t * If we have pmu_enable/pmu_disable calls, install\n\t\t\t * transaction stubs that use that to try and batch\n\t\t\t * hardware accesses.\n\t\t\t */\n\t\t\tpmu->start_txn  = perf_pmu_start_txn;\n\t\t\tpmu->commit_txn = perf_pmu_commit_txn;\n\t\t\tpmu->cancel_txn = perf_pmu_cancel_txn;\n\t\t} else {\n\t\t\tpmu->start_txn  = perf_pmu_nop_void;\n\t\t\tpmu->commit_txn = perf_pmu_nop_int;\n\t\t\tpmu->cancel_txn = perf_pmu_nop_void;\n\t\t}\n\t}\n\n\tif (!pmu->pmu_enable) {\n\t\tpmu->pmu_enable  = perf_pmu_nop_void;\n\t\tpmu->pmu_disable = perf_pmu_nop_void;\n\t}\n\n\tif (!pmu->event_idx)\n\t\tpmu->event_idx = perf_event_idx_default;\n\n\tlist_add_rcu(&pmu->entry, &pmus);\n\tret = 0;\nunlock:\n\tmutex_unlock(&pmus_lock);\n\n\treturn ret;\n\nfree_dev:\n\tdevice_del(pmu->dev);\n\tput_device(pmu->dev);\n\nfree_idr:\n\tif (pmu->type >= PERF_TYPE_MAX)\n\t\tidr_remove(&pmu_idr, pmu->type);\n\nfree_pdc:\n\tfree_percpu(pmu->pmu_disable_count);\n\tgoto unlock;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nint perf_pmu_register(struct pmu *pmu, const char *name, int type)\n{\n\tint cpu, ret;\n\n\tmutex_lock(&pmus_lock);\n\tret = -ENOMEM;\n\tpmu->pmu_disable_count = alloc_percpu(int);\n\tif (!pmu->pmu_disable_count)\n\t\tgoto unlock;\n\n\tpmu->type = -1;\n\tif (!name)\n\t\tgoto skip_type;\n\tpmu->name = name;\n\n\tif (type < 0) {\n\t\ttype = idr_alloc(&pmu_idr, pmu, PERF_TYPE_MAX, 0, GFP_KERNEL);\n\t\tif (type < 0) {\n\t\t\tret = type;\n\t\t\tgoto free_pdc;\n\t\t}\n\t}\n\tpmu->type = type;\n\n\tif (pmu_bus_running) {\n\t\tret = pmu_dev_alloc(pmu);\n\t\tif (ret)\n\t\t\tgoto free_idr;\n\t}\n\nskip_type:\n\tpmu->pmu_cpu_context = find_pmu_context(pmu->task_ctx_nr);\n\tif (pmu->pmu_cpu_context)\n\t\tgoto got_cpu_context;\n\n\tret = -ENOMEM;\n\tpmu->pmu_cpu_context = alloc_percpu(struct perf_cpu_context);\n\tif (!pmu->pmu_cpu_context)\n\t\tgoto free_dev;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct perf_cpu_context *cpuctx;\n\n\t\tcpuctx = per_cpu_ptr(pmu->pmu_cpu_context, cpu);\n\t\t__perf_event_init_context(&cpuctx->ctx);\n\t\tlockdep_set_class(&cpuctx->ctx.mutex, &cpuctx_mutex);\n\t\tlockdep_set_class(&cpuctx->ctx.lock, &cpuctx_lock);\n\t\tcpuctx->ctx.pmu = pmu;\n\n\t\t__perf_cpu_hrtimer_init(cpuctx, cpu);\n\n\t\tINIT_LIST_HEAD(&cpuctx->rotation_list);\n\t\tcpuctx->unique_pmu = pmu;\n\t}\n\ngot_cpu_context:\n\tif (!pmu->start_txn) {\n\t\tif (pmu->pmu_enable) {\n\t\t\t/*\n\t\t\t * If we have pmu_enable/pmu_disable calls, install\n\t\t\t * transaction stubs that use that to try and batch\n\t\t\t * hardware accesses.\n\t\t\t */\n\t\t\tpmu->start_txn  = perf_pmu_start_txn;\n\t\t\tpmu->commit_txn = perf_pmu_commit_txn;\n\t\t\tpmu->cancel_txn = perf_pmu_cancel_txn;\n\t\t} else {\n\t\t\tpmu->start_txn  = perf_pmu_nop_void;\n\t\t\tpmu->commit_txn = perf_pmu_nop_int;\n\t\t\tpmu->cancel_txn = perf_pmu_nop_void;\n\t\t}\n\t}\n\n\tif (!pmu->pmu_enable) {\n\t\tpmu->pmu_enable  = perf_pmu_nop_void;\n\t\tpmu->pmu_disable = perf_pmu_nop_void;\n\t}\n\n\tif (!pmu->event_idx)\n\t\tpmu->event_idx = perf_event_idx_default;\n\n\tlist_add_rcu(&pmu->entry, &pmus);\n\tret = 0;\nunlock:\n\tmutex_unlock(&pmus_lock);\n\n\treturn ret;\n\nfree_dev:\n\tdevice_del(pmu->dev);\n\tput_device(pmu->dev);\n\nfree_idr:\n\tif (pmu->type >= PERF_TYPE_MAX)\n\t\tidr_remove(&pmu_idr, pmu->type);\n\nfree_pdc:\n\tfree_percpu(pmu->pmu_disable_count);\n\tgoto unlock;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 293,
            "cve_id": "CVE-2013-4299",
            "code_snippet": "static int persistent_prepare_exception(struct dm_exception_store *store,\n\t\t\t\t\tstruct dm_exception *e)\n{\n\tstruct pstore *ps = get_info(store);\n\tuint32_t stride;\n\tchunk_t next_free;\n\tsector_t size = get_dev_size(dm_snap_cow(store->snap)->bdev);\n\n\t/* Is there enough room ? */\n\tif (size < ((ps->next_free + 1) * store->chunk_size))\n\t\treturn -ENOSPC;\n\n\te->new_chunk = ps->next_free;\n\n\t/*\n\t * Move onto the next free pending, making sure to take\n\t * into account the location of the metadata chunks.\n\t */\n\tstride = (ps->exceptions_per_area + 1);\n\tnext_free = ++ps->next_free;\n\tif (sector_div(next_free, stride) == 1)\n\t\tps->next_free++;\n\n\tatomic_inc(&ps->pending_count);\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int read_exceptions(struct pstore *ps,\n\t\t\t   int (*callback)(void *callback_context, chunk_t old,\n\t\t\t\t\t   chunk_t new),\n\t\t\t   void *callback_context)\n{\n\tint r, full = 1;\n\n\t/*\n\t * Keeping reading chunks and inserting exceptions until\n\t * we find a partially full area.\n\t */\n\tfor (ps->current_area = 0; full; ps->current_area++) {\n\t\tr = area_io(ps, READ);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = insert_exceptions(ps, callback, callback_context, &full);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tps->current_area--;\n\n\treturn 0;\n}",
                        "code_after_change": "static int read_exceptions(struct pstore *ps,\n\t\t\t   int (*callback)(void *callback_context, chunk_t old,\n\t\t\t\t\t   chunk_t new),\n\t\t\t   void *callback_context)\n{\n\tint r, full = 1;\n\n\t/*\n\t * Keeping reading chunks and inserting exceptions until\n\t * we find a partially full area.\n\t */\n\tfor (ps->current_area = 0; full; ps->current_area++) {\n\t\tr = area_io(ps, READ);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = insert_exceptions(ps, callback, callback_context, &full);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tps->current_area--;\n\n\tskip_metadata(ps);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2013-4299"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int persistent_prepare_exception(struct dm_exception_store *store,\n\t\t\t\t\tstruct dm_exception *e)\n{\n\tstruct pstore *ps = get_info(store);\n\tuint32_t stride;\n\tchunk_t next_free;\n\tsector_t size = get_dev_size(dm_snap_cow(store->snap)->bdev);\n\n\t/* Is there enough room ? */\n\tif (size < ((ps->next_free + 1) * store->chunk_size))\n\t\treturn -ENOSPC;\n\n\te->new_chunk = ps->next_free;\n\n\t/*\n\t * Move onto the next free pending, making sure to take\n\t * into account the location of the metadata chunks.\n\t */\n\tstride = (ps->exceptions_per_area + 1);\n\tnext_free = ++ps->next_free;\n\tif (sector_div(next_free, stride) == 1)\n\t\tps->next_free++;\n\n\tatomic_inc(&ps->pending_count);\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int read_exceptions(struct pstore *ps,\n\t\t\t   int (*callback)(void *callback_context, chunk_t old,\n\t\t\t\t\t   chunk_t new),\n\t\t\t   void *callback_context)\n{\n\tint r, full = 1;\n\n\t/*\n\t * Keeping reading chunks and inserting exceptions until\n\t * we find a partially full area.\n\t */\n\tfor (ps->current_area = 0; full; ps->current_area++) {\n\t\tr = area_io(ps, READ);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = insert_exceptions(ps, callback, callback_context, &full);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tps->current_area--;\n\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int read_exceptions(struct pstore *ps,\n\t\t\t   int (*callback)(void *callback_context, chunk_t old,\n\t\t\t\t\t   chunk_t new),\n\t\t\t   void *callback_context)\n{\n\tint r, full = 1;\n\n\t/*\n\t * Keeping reading chunks and inserting exceptions until\n\t * we find a partially full area.\n\t */\n\tfor (ps->current_area = 0; full; ps->current_area++) {\n\t\tr = area_io(ps, READ);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = insert_exceptions(ps, callback, callback_context, &full);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tps->current_area--;\n\n\tskip_metadata(ps);\n\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "STATIC int\nxfs_ioctl_setattr(\n\txfs_inode_t\t\t*ip,\n\tstruct fsxattr\t\t*fa,\n\tint\t\t\tmask)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tstruct xfs_trans\t*tp;\n\tunsigned int\t\tlock_flags = 0;\n\tstruct xfs_dquot\t*udqp = NULL;\n\tstruct xfs_dquot\t*pdqp = NULL;\n\tstruct xfs_dquot\t*olddquot = NULL;\n\tint\t\t\tcode;\n\n\ttrace_xfs_ioctl_setattr(ip);\n\n\tif (mp->m_flags & XFS_MOUNT_RDONLY)\n\t\treturn XFS_ERROR(EROFS);\n\tif (XFS_FORCED_SHUTDOWN(mp))\n\t\treturn XFS_ERROR(EIO);\n\n\t/*\n\t * Disallow 32bit project ids when projid32bit feature is not enabled.\n\t */\n\tif ((mask & FSX_PROJID) && (fa->fsx_projid > (__uint16_t)-1) &&\n\t\t\t!xfs_sb_version_hasprojid32bit(&ip->i_mount->m_sb))\n\t\treturn XFS_ERROR(EINVAL);\n\n\t/*\n\t * If disk quotas is on, we make sure that the dquots do exist on disk,\n\t * before we start any other transactions. Trying to do this later\n\t * is messy. We don't care to take a readlock to look at the ids\n\t * in inode here, because we can't hold it across the trans_reserve.\n\t * If the IDs do change before we take the ilock, we're covered\n\t * because the i_*dquot fields will get updated anyway.\n\t */\n\tif (XFS_IS_QUOTA_ON(mp) && (mask & FSX_PROJID)) {\n\t\tcode = xfs_qm_vop_dqalloc(ip, ip->i_d.di_uid,\n\t\t\t\t\t ip->i_d.di_gid, fa->fsx_projid,\n\t\t\t\t\t XFS_QMOPT_PQUOTA, &udqp, NULL, &pdqp);\n\t\tif (code)\n\t\t\treturn code;\n\t}\n\n\t/*\n\t * For the other attributes, we acquire the inode lock and\n\t * first do an error checking pass.\n\t */\n\ttp = xfs_trans_alloc(mp, XFS_TRANS_SETATTR_NOT_SIZE);\n\tcode = xfs_trans_reserve(tp, &M_RES(mp)->tr_ichange, 0, 0);\n\tif (code)\n\t\tgoto error_return;\n\n\tlock_flags = XFS_ILOCK_EXCL;\n\txfs_ilock(ip, lock_flags);\n\n\t/*\n\t * CAP_FOWNER overrides the following restrictions:\n\t *\n\t * The user ID of the calling process must be equal\n\t * to the file owner ID, except in cases where the\n\t * CAP_FSETID capability is applicable.\n\t */\n\tif (!inode_owner_or_capable(VFS_I(ip))) {\n\t\tcode = XFS_ERROR(EPERM);\n\t\tgoto error_return;\n\t}\n\n\t/*\n\t * Do a quota reservation only if projid is actually going to change.\n\t * Only allow changing of projid from init_user_ns since it is a\n\t * non user namespace aware identifier.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\tif (current_user_ns() != &init_user_ns) {\n\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\tgoto error_return;\n\t\t}\n\n\t\tif (XFS_IS_QUOTA_RUNNING(mp) &&\n\t\t    XFS_IS_PQUOTA_ON(mp) &&\n\t\t    xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tASSERT(tp);\n\t\t\tcode = xfs_qm_vop_chown_reserve(tp, ip, udqp, NULL,\n\t\t\t\t\t\tpdqp, capable(CAP_FOWNER) ?\n\t\t\t\t\t\tXFS_QMOPT_FORCE_RES : 0);\n\t\t\tif (code)\t/* out of quota */\n\t\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\tif (mask & FSX_EXTSIZE) {\n\t\t/*\n\t\t * Can't change extent size if any extents are allocated.\n\t\t */\n\t\tif (ip->i_d.di_nextents &&\n\t\t    ((ip->i_d.di_extsize << mp->m_sb.sb_blocklog) !=\n\t\t     fa->fsx_extsize)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * Extent size must be a multiple of the appropriate block\n\t\t * size, if set at all. It must also be smaller than the\n\t\t * maximum extent size supported by the filesystem.\n\t\t *\n\t\t * Also, for non-realtime files, limit the extent size hint to\n\t\t * half the size of the AGs in the filesystem so alignment\n\t\t * doesn't result in extents larger than an AG.\n\t\t */\n\t\tif (fa->fsx_extsize != 0) {\n\t\t\txfs_extlen_t    size;\n\t\t\txfs_fsblock_t   extsize_fsb;\n\n\t\t\textsize_fsb = XFS_B_TO_FSB(mp, fa->fsx_extsize);\n\t\t\tif (extsize_fsb > MAXEXTLEN) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\n\t\t\tif (XFS_IS_REALTIME_INODE(ip) ||\n\t\t\t    ((mask & FSX_XFLAGS) &&\n\t\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME))) {\n\t\t\t\tsize = mp->m_sb.sb_rextsize <<\n\t\t\t\t       mp->m_sb.sb_blocklog;\n\t\t\t} else {\n\t\t\t\tsize = mp->m_sb.sb_blocksize;\n\t\t\t\tif (extsize_fsb > mp->m_sb.sb_agblocks / 2) {\n\t\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\t\tgoto error_return;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (fa->fsx_extsize % size) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\t}\n\n\n\tif (mask & FSX_XFLAGS) {\n\t\t/*\n\t\t * Can't change realtime flag if any extents are allocated.\n\t\t */\n\t\tif ((ip->i_d.di_nextents || ip->i_delayed_blks) &&\n\t\t    (XFS_IS_REALTIME_INODE(ip)) !=\n\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * If realtime flag is set then must have realtime data.\n\t\t */\n\t\tif ((fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tif ((mp->m_sb.sb_rblocks == 0) ||\n\t\t\t    (mp->m_sb.sb_rextsize == 0) ||\n\t\t\t    (ip->i_d.di_extsize % mp->m_sb.sb_rextsize)) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Can't modify an immutable/append-only file unless\n\t\t * we have appropriate permission.\n\t\t */\n\t\tif ((ip->i_d.di_flags &\n\t\t\t\t(XFS_DIFLAG_IMMUTABLE|XFS_DIFLAG_APPEND) ||\n\t\t     (fa->fsx_xflags &\n\t\t\t\t(XFS_XFLAG_IMMUTABLE | XFS_XFLAG_APPEND))) &&\n\t\t    !capable(CAP_LINUX_IMMUTABLE)) {\n\t\t\tcode = XFS_ERROR(EPERM);\n\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\txfs_trans_ijoin(tp, ip, 0);\n\n\t/*\n\t * Change file ownership.  Must be the owner or privileged.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\t/*\n\t\t * CAP_FSETID overrides the following restrictions:\n\t\t *\n\t\t * The set-user-ID and set-group-ID bits of a file will be\n\t\t * cleared upon successful return from chown()\n\t\t */\n\t\tif ((ip->i_d.di_mode & (S_ISUID|S_ISGID)) &&\n\t\t    !inode_capable(VFS_I(ip), CAP_FSETID))\n\t\t\tip->i_d.di_mode &= ~(S_ISUID|S_ISGID);\n\n\t\t/*\n\t\t * Change the ownerships and register quota modifications\n\t\t * in the transaction.\n\t\t */\n\t\tif (xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tif (XFS_IS_QUOTA_RUNNING(mp) && XFS_IS_PQUOTA_ON(mp)) {\n\t\t\t\tolddquot = xfs_qm_vop_chown(tp, ip,\n\t\t\t\t\t\t\t&ip->i_pdquot, pdqp);\n\t\t\t}\n\t\t\txfs_set_projid(ip, fa->fsx_projid);\n\n\t\t\t/*\n\t\t\t * We may have to rev the inode as well as\n\t\t\t * the superblock version number since projids didn't\n\t\t\t * exist before DINODE_VERSION_2 and SB_VERSION_NLINK.\n\t\t\t */\n\t\t\tif (ip->i_d.di_version == 1)\n\t\t\t\txfs_bump_ino_vers2(tp, ip);\n\t\t}\n\n\t}\n\n\tif (mask & FSX_EXTSIZE)\n\t\tip->i_d.di_extsize = fa->fsx_extsize >> mp->m_sb.sb_blocklog;\n\tif (mask & FSX_XFLAGS) {\n\t\txfs_set_diflags(ip, fa->fsx_xflags);\n\t\txfs_diflags_to_linux(ip);\n\t}\n\n\txfs_trans_ichgtime(tp, ip, XFS_ICHGTIME_CHG);\n\txfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);\n\n\tXFS_STATS_INC(xs_ig_attrchg);\n\n\t/*\n\t * If this is a synchronous mount, make sure that the\n\t * transaction goes to disk before returning to the user.\n\t * This is slightly sub-optimal in that truncates require\n\t * two sync transactions instead of one for wsync filesystems.\n\t * One for the truncate and one for the timestamps since we\n\t * don't want to change the timestamps unless we're sure the\n\t * truncate worked.  Truncates are less than 1% of the laddis\n\t * mix so this probably isn't worth the trouble to optimize.\n\t */\n\tif (mp->m_flags & XFS_MOUNT_WSYNC)\n\t\txfs_trans_set_sync(tp);\n\tcode = xfs_trans_commit(tp, 0);\n\txfs_iunlock(ip, lock_flags);\n\n\t/*\n\t * Release any dquot(s) the inode had kept before chown.\n\t */\n\txfs_qm_dqrele(olddquot);\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\n\treturn code;\n\n error_return:\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\txfs_trans_cancel(tp, 0);\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\n\treturn code;\n}",
                        "code_after_change": "STATIC int\nxfs_ioctl_setattr(\n\txfs_inode_t\t\t*ip,\n\tstruct fsxattr\t\t*fa,\n\tint\t\t\tmask)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tstruct xfs_trans\t*tp;\n\tunsigned int\t\tlock_flags = 0;\n\tstruct xfs_dquot\t*udqp = NULL;\n\tstruct xfs_dquot\t*pdqp = NULL;\n\tstruct xfs_dquot\t*olddquot = NULL;\n\tint\t\t\tcode;\n\n\ttrace_xfs_ioctl_setattr(ip);\n\n\tif (mp->m_flags & XFS_MOUNT_RDONLY)\n\t\treturn XFS_ERROR(EROFS);\n\tif (XFS_FORCED_SHUTDOWN(mp))\n\t\treturn XFS_ERROR(EIO);\n\n\t/*\n\t * Disallow 32bit project ids when projid32bit feature is not enabled.\n\t */\n\tif ((mask & FSX_PROJID) && (fa->fsx_projid > (__uint16_t)-1) &&\n\t\t\t!xfs_sb_version_hasprojid32bit(&ip->i_mount->m_sb))\n\t\treturn XFS_ERROR(EINVAL);\n\n\t/*\n\t * If disk quotas is on, we make sure that the dquots do exist on disk,\n\t * before we start any other transactions. Trying to do this later\n\t * is messy. We don't care to take a readlock to look at the ids\n\t * in inode here, because we can't hold it across the trans_reserve.\n\t * If the IDs do change before we take the ilock, we're covered\n\t * because the i_*dquot fields will get updated anyway.\n\t */\n\tif (XFS_IS_QUOTA_ON(mp) && (mask & FSX_PROJID)) {\n\t\tcode = xfs_qm_vop_dqalloc(ip, ip->i_d.di_uid,\n\t\t\t\t\t ip->i_d.di_gid, fa->fsx_projid,\n\t\t\t\t\t XFS_QMOPT_PQUOTA, &udqp, NULL, &pdqp);\n\t\tif (code)\n\t\t\treturn code;\n\t}\n\n\t/*\n\t * For the other attributes, we acquire the inode lock and\n\t * first do an error checking pass.\n\t */\n\ttp = xfs_trans_alloc(mp, XFS_TRANS_SETATTR_NOT_SIZE);\n\tcode = xfs_trans_reserve(tp, &M_RES(mp)->tr_ichange, 0, 0);\n\tif (code)\n\t\tgoto error_return;\n\n\tlock_flags = XFS_ILOCK_EXCL;\n\txfs_ilock(ip, lock_flags);\n\n\t/*\n\t * CAP_FOWNER overrides the following restrictions:\n\t *\n\t * The user ID of the calling process must be equal\n\t * to the file owner ID, except in cases where the\n\t * CAP_FSETID capability is applicable.\n\t */\n\tif (!inode_owner_or_capable(VFS_I(ip))) {\n\t\tcode = XFS_ERROR(EPERM);\n\t\tgoto error_return;\n\t}\n\n\t/*\n\t * Do a quota reservation only if projid is actually going to change.\n\t * Only allow changing of projid from init_user_ns since it is a\n\t * non user namespace aware identifier.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\tif (current_user_ns() != &init_user_ns) {\n\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\tgoto error_return;\n\t\t}\n\n\t\tif (XFS_IS_QUOTA_RUNNING(mp) &&\n\t\t    XFS_IS_PQUOTA_ON(mp) &&\n\t\t    xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tASSERT(tp);\n\t\t\tcode = xfs_qm_vop_chown_reserve(tp, ip, udqp, NULL,\n\t\t\t\t\t\tpdqp, capable(CAP_FOWNER) ?\n\t\t\t\t\t\tXFS_QMOPT_FORCE_RES : 0);\n\t\t\tif (code)\t/* out of quota */\n\t\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\tif (mask & FSX_EXTSIZE) {\n\t\t/*\n\t\t * Can't change extent size if any extents are allocated.\n\t\t */\n\t\tif (ip->i_d.di_nextents &&\n\t\t    ((ip->i_d.di_extsize << mp->m_sb.sb_blocklog) !=\n\t\t     fa->fsx_extsize)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * Extent size must be a multiple of the appropriate block\n\t\t * size, if set at all. It must also be smaller than the\n\t\t * maximum extent size supported by the filesystem.\n\t\t *\n\t\t * Also, for non-realtime files, limit the extent size hint to\n\t\t * half the size of the AGs in the filesystem so alignment\n\t\t * doesn't result in extents larger than an AG.\n\t\t */\n\t\tif (fa->fsx_extsize != 0) {\n\t\t\txfs_extlen_t    size;\n\t\t\txfs_fsblock_t   extsize_fsb;\n\n\t\t\textsize_fsb = XFS_B_TO_FSB(mp, fa->fsx_extsize);\n\t\t\tif (extsize_fsb > MAXEXTLEN) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\n\t\t\tif (XFS_IS_REALTIME_INODE(ip) ||\n\t\t\t    ((mask & FSX_XFLAGS) &&\n\t\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME))) {\n\t\t\t\tsize = mp->m_sb.sb_rextsize <<\n\t\t\t\t       mp->m_sb.sb_blocklog;\n\t\t\t} else {\n\t\t\t\tsize = mp->m_sb.sb_blocksize;\n\t\t\t\tif (extsize_fsb > mp->m_sb.sb_agblocks / 2) {\n\t\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\t\tgoto error_return;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (fa->fsx_extsize % size) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\t}\n\n\n\tif (mask & FSX_XFLAGS) {\n\t\t/*\n\t\t * Can't change realtime flag if any extents are allocated.\n\t\t */\n\t\tif ((ip->i_d.di_nextents || ip->i_delayed_blks) &&\n\t\t    (XFS_IS_REALTIME_INODE(ip)) !=\n\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * If realtime flag is set then must have realtime data.\n\t\t */\n\t\tif ((fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tif ((mp->m_sb.sb_rblocks == 0) ||\n\t\t\t    (mp->m_sb.sb_rextsize == 0) ||\n\t\t\t    (ip->i_d.di_extsize % mp->m_sb.sb_rextsize)) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Can't modify an immutable/append-only file unless\n\t\t * we have appropriate permission.\n\t\t */\n\t\tif ((ip->i_d.di_flags &\n\t\t\t\t(XFS_DIFLAG_IMMUTABLE|XFS_DIFLAG_APPEND) ||\n\t\t     (fa->fsx_xflags &\n\t\t\t\t(XFS_XFLAG_IMMUTABLE | XFS_XFLAG_APPEND))) &&\n\t\t    !capable(CAP_LINUX_IMMUTABLE)) {\n\t\t\tcode = XFS_ERROR(EPERM);\n\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\txfs_trans_ijoin(tp, ip, 0);\n\n\t/*\n\t * Change file ownership.  Must be the owner or privileged.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\t/*\n\t\t * CAP_FSETID overrides the following restrictions:\n\t\t *\n\t\t * The set-user-ID and set-group-ID bits of a file will be\n\t\t * cleared upon successful return from chown()\n\t\t */\n\t\tif ((ip->i_d.di_mode & (S_ISUID|S_ISGID)) &&\n\t\t    !capable_wrt_inode_uidgid(VFS_I(ip), CAP_FSETID))\n\t\t\tip->i_d.di_mode &= ~(S_ISUID|S_ISGID);\n\n\t\t/*\n\t\t * Change the ownerships and register quota modifications\n\t\t * in the transaction.\n\t\t */\n\t\tif (xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tif (XFS_IS_QUOTA_RUNNING(mp) && XFS_IS_PQUOTA_ON(mp)) {\n\t\t\t\tolddquot = xfs_qm_vop_chown(tp, ip,\n\t\t\t\t\t\t\t&ip->i_pdquot, pdqp);\n\t\t\t}\n\t\t\txfs_set_projid(ip, fa->fsx_projid);\n\n\t\t\t/*\n\t\t\t * We may have to rev the inode as well as\n\t\t\t * the superblock version number since projids didn't\n\t\t\t * exist before DINODE_VERSION_2 and SB_VERSION_NLINK.\n\t\t\t */\n\t\t\tif (ip->i_d.di_version == 1)\n\t\t\t\txfs_bump_ino_vers2(tp, ip);\n\t\t}\n\n\t}\n\n\tif (mask & FSX_EXTSIZE)\n\t\tip->i_d.di_extsize = fa->fsx_extsize >> mp->m_sb.sb_blocklog;\n\tif (mask & FSX_XFLAGS) {\n\t\txfs_set_diflags(ip, fa->fsx_xflags);\n\t\txfs_diflags_to_linux(ip);\n\t}\n\n\txfs_trans_ichgtime(tp, ip, XFS_ICHGTIME_CHG);\n\txfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);\n\n\tXFS_STATS_INC(xs_ig_attrchg);\n\n\t/*\n\t * If this is a synchronous mount, make sure that the\n\t * transaction goes to disk before returning to the user.\n\t * This is slightly sub-optimal in that truncates require\n\t * two sync transactions instead of one for wsync filesystems.\n\t * One for the truncate and one for the timestamps since we\n\t * don't want to change the timestamps unless we're sure the\n\t * truncate worked.  Truncates are less than 1% of the laddis\n\t * mix so this probably isn't worth the trouble to optimize.\n\t */\n\tif (mp->m_flags & XFS_MOUNT_WSYNC)\n\t\txfs_trans_set_sync(tp);\n\tcode = xfs_trans_commit(tp, 0);\n\txfs_iunlock(ip, lock_flags);\n\n\t/*\n\t * Release any dquot(s) the inode had kept before chown.\n\t */\n\txfs_qm_dqrele(olddquot);\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\n\treturn code;\n\n error_return:\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\txfs_trans_cancel(tp, 0);\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\n\treturn code;\n}",
                        "cve_id": "CVE-2014-4014"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int persistent_prepare_exception(struct dm_exception_store *store,\n\t\t\t\t\tstruct dm_exception *e)\n{\n\tstruct pstore *ps = get_info(store);\n\tuint32_t stride;\n\tchunk_t next_free;\n\tsector_t size = get_dev_size(dm_snap_cow(store->snap)->bdev);\n\n\t/* Is there enough room ? */\n\tif (size < ((ps->next_free + 1) * store->chunk_size))\n\t\treturn -ENOSPC;\n\n\te->new_chunk = ps->next_free;\n\n\t/*\n\t * Move onto the next free pending, making sure to take\n\t * into account the location of the metadata chunks.\n\t */\n\tstride = (ps->exceptions_per_area + 1);\n\tnext_free = ++ps->next_free;\n\tif (sector_div(next_free, stride) == 1)\n\t\tps->next_free++;\n\n\tatomic_inc(&ps->pending_count);\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nSTATIC int\nxfs_ioctl_setattr(\n\txfs_inode_t\t\t*ip,\n\tstruct fsxattr\t\t*fa,\n\tint\t\t\tmask)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tstruct xfs_trans\t*tp;\n\tunsigned int\t\tlock_flags = 0;\n\tstruct xfs_dquot\t*udqp = NULL;\n\tstruct xfs_dquot\t*pdqp = NULL;\n\tstruct xfs_dquot\t*olddquot = NULL;\n\tint\t\t\tcode;\n\n\ttrace_xfs_ioctl_setattr(ip);\n\n\tif (mp->m_flags & XFS_MOUNT_RDONLY)\n\t\treturn XFS_ERROR(EROFS);\n\tif (XFS_FORCED_SHUTDOWN(mp))\n\t\treturn XFS_ERROR(EIO);\n\n\t/*\n\t * Disallow 32bit project ids when projid32bit feature is not enabled.\n\t */\n\tif ((mask & FSX_PROJID) && (fa->fsx_projid > (__uint16_t)-1) &&\n\t\t\t!xfs_sb_version_hasprojid32bit(&ip->i_mount->m_sb))\n\t\treturn XFS_ERROR(EINVAL);\n\n\t/*\n\t * If disk quotas is on, we make sure that the dquots do exist on disk,\n\t * before we start any other transactions. Trying to do this later\n\t * is messy. We don't care to take a readlock to look at the ids\n\t * in inode here, because we can't hold it across the trans_reserve.\n\t * If the IDs do change before we take the ilock, we're covered\n\t * because the i_*dquot fields will get updated anyway.\n\t */\n\tif (XFS_IS_QUOTA_ON(mp) && (mask & FSX_PROJID)) {\n\t\tcode = xfs_qm_vop_dqalloc(ip, ip->i_d.di_uid,\n\t\t\t\t\t ip->i_d.di_gid, fa->fsx_projid,\n\t\t\t\t\t XFS_QMOPT_PQUOTA, &udqp, NULL, &pdqp);\n\t\tif (code)\n\t\t\treturn code;\n\t}\n\n\t/*\n\t * For the other attributes, we acquire the inode lock and\n\t * first do an error checking pass.\n\t */\n\ttp = xfs_trans_alloc(mp, XFS_TRANS_SETATTR_NOT_SIZE);\n\tcode = xfs_trans_reserve(tp, &M_RES(mp)->tr_ichange, 0, 0);\n\tif (code)\n\t\tgoto error_return;\n\n\tlock_flags = XFS_ILOCK_EXCL;\n\txfs_ilock(ip, lock_flags);\n\n\t/*\n\t * CAP_FOWNER overrides the following restrictions:\n\t *\n\t * The user ID of the calling process must be equal\n\t * to the file owner ID, except in cases where the\n\t * CAP_FSETID capability is applicable.\n\t */\n\tif (!inode_owner_or_capable(VFS_I(ip))) {\n\t\tcode = XFS_ERROR(EPERM);\n\t\tgoto error_return;\n\t}\n\n\t/*\n\t * Do a quota reservation only if projid is actually going to change.\n\t * Only allow changing of projid from init_user_ns since it is a\n\t * non user namespace aware identifier.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\tif (current_user_ns() != &init_user_ns) {\n\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\tgoto error_return;\n\t\t}\n\n\t\tif (XFS_IS_QUOTA_RUNNING(mp) &&\n\t\t    XFS_IS_PQUOTA_ON(mp) &&\n\t\t    xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tASSERT(tp);\n\t\t\tcode = xfs_qm_vop_chown_reserve(tp, ip, udqp, NULL,\n\t\t\t\t\t\tpdqp, capable(CAP_FOWNER) ?\n\t\t\t\t\t\tXFS_QMOPT_FORCE_RES : 0);\n\t\t\tif (code)\t/* out of quota */\n\t\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\tif (mask & FSX_EXTSIZE) {\n\t\t/*\n\t\t * Can't change extent size if any extents are allocated.\n\t\t */\n\t\tif (ip->i_d.di_nextents &&\n\t\t    ((ip->i_d.di_extsize << mp->m_sb.sb_blocklog) !=\n\t\t     fa->fsx_extsize)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * Extent size must be a multiple of the appropriate block\n\t\t * size, if set at all. It must also be smaller than the\n\t\t * maximum extent size supported by the filesystem.\n\t\t *\n\t\t * Also, for non-realtime files, limit the extent size hint to\n\t\t * half the size of the AGs in the filesystem so alignment\n\t\t * doesn't result in extents larger than an AG.\n\t\t */\n\t\tif (fa->fsx_extsize != 0) {\n\t\t\txfs_extlen_t    size;\n\t\t\txfs_fsblock_t   extsize_fsb;\n\n\t\t\textsize_fsb = XFS_B_TO_FSB(mp, fa->fsx_extsize);\n\t\t\tif (extsize_fsb > MAXEXTLEN) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\n\t\t\tif (XFS_IS_REALTIME_INODE(ip) ||\n\t\t\t    ((mask & FSX_XFLAGS) &&\n\t\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME))) {\n\t\t\t\tsize = mp->m_sb.sb_rextsize <<\n\t\t\t\t       mp->m_sb.sb_blocklog;\n\t\t\t} else {\n\t\t\t\tsize = mp->m_sb.sb_blocksize;\n\t\t\t\tif (extsize_fsb > mp->m_sb.sb_agblocks / 2) {\n\t\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\t\tgoto error_return;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (fa->fsx_extsize % size) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\t}\n\n\n\tif (mask & FSX_XFLAGS) {\n\t\t/*\n\t\t * Can't change realtime flag if any extents are allocated.\n\t\t */\n\t\tif ((ip->i_d.di_nextents || ip->i_delayed_blks) &&\n\t\t    (XFS_IS_REALTIME_INODE(ip)) !=\n\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * If realtime flag is set then must have realtime data.\n\t\t */\n\t\tif ((fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tif ((mp->m_sb.sb_rblocks == 0) ||\n\t\t\t    (mp->m_sb.sb_rextsize == 0) ||\n\t\t\t    (ip->i_d.di_extsize % mp->m_sb.sb_rextsize)) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Can't modify an immutable/append-only file unless\n\t\t * we have appropriate permission.\n\t\t */\n\t\tif ((ip->i_d.di_flags &\n\t\t\t\t(XFS_DIFLAG_IMMUTABLE|XFS_DIFLAG_APPEND) ||\n\t\t     (fa->fsx_xflags &\n\t\t\t\t(XFS_XFLAG_IMMUTABLE | XFS_XFLAG_APPEND))) &&\n\t\t    !capable(CAP_LINUX_IMMUTABLE)) {\n\t\t\tcode = XFS_ERROR(EPERM);\n\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\txfs_trans_ijoin(tp, ip, 0);\n\n\t/*\n\t * Change file ownership.  Must be the owner or privileged.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\t/*\n\t\t * CAP_FSETID overrides the following restrictions:\n\t\t *\n\t\t * The set-user-ID and set-group-ID bits of a file will be\n\t\t * cleared upon successful return from chown()\n\t\t */\n\t\tif ((ip->i_d.di_mode & (S_ISUID|S_ISGID)) &&\n\t\t    !inode_capable(VFS_I(ip), CAP_FSETID))\n\t\t\tip->i_d.di_mode &= ~(S_ISUID|S_ISGID);\n\n\t\t/*\n\t\t * Change the ownerships and register quota modifications\n\t\t * in the transaction.\n\t\t */\n\t\tif (xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tif (XFS_IS_QUOTA_RUNNING(mp) && XFS_IS_PQUOTA_ON(mp)) {\n\t\t\t\tolddquot = xfs_qm_vop_chown(tp, ip,\n\t\t\t\t\t\t\t&ip->i_pdquot, pdqp);\n\t\t\t}\n\t\t\txfs_set_projid(ip, fa->fsx_projid);\n\n\t\t\t/*\n\t\t\t * We may have to rev the inode as well as\n\t\t\t * the superblock version number since projids didn't\n\t\t\t * exist before DINODE_VERSION_2 and SB_VERSION_NLINK.\n\t\t\t */\n\t\t\tif (ip->i_d.di_version == 1)\n\t\t\t\txfs_bump_ino_vers2(tp, ip);\n\t\t}\n\n\t}\n\n\tif (mask & FSX_EXTSIZE)\n\t\tip->i_d.di_extsize = fa->fsx_extsize >> mp->m_sb.sb_blocklog;\n\tif (mask & FSX_XFLAGS) {\n\t\txfs_set_diflags(ip, fa->fsx_xflags);\n\t\txfs_diflags_to_linux(ip);\n\t}\n\n\txfs_trans_ichgtime(tp, ip, XFS_ICHGTIME_CHG);\n\txfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);\n\n\tXFS_STATS_INC(xs_ig_attrchg);\n\n\t/*\n\t * If this is a synchronous mount, make sure that the\n\t * transaction goes to disk before returning to the user.\n\t * This is slightly sub-optimal in that truncates require\n\t * two sync transactions instead of one for wsync filesystems.\n\t * One for the truncate and one for the timestamps since we\n\t * don't want to change the timestamps unless we're sure the\n\t * truncate worked.  Truncates are less than 1% of the laddis\n\t * mix so this probably isn't worth the trouble to optimize.\n\t */\n\tif (mp->m_flags & XFS_MOUNT_WSYNC)\n\t\txfs_trans_set_sync(tp);\n\tcode = xfs_trans_commit(tp, 0);\n\txfs_iunlock(ip, lock_flags);\n\n\t/*\n\t * Release any dquot(s) the inode had kept before chown.\n\t */\n\txfs_qm_dqrele(olddquot);\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\n\treturn code;\n\n error_return:\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\txfs_trans_cancel(tp, 0);\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\n\treturn code;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nSTATIC int\nxfs_ioctl_setattr(\n\txfs_inode_t\t\t*ip,\n\tstruct fsxattr\t\t*fa,\n\tint\t\t\tmask)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tstruct xfs_trans\t*tp;\n\tunsigned int\t\tlock_flags = 0;\n\tstruct xfs_dquot\t*udqp = NULL;\n\tstruct xfs_dquot\t*pdqp = NULL;\n\tstruct xfs_dquot\t*olddquot = NULL;\n\tint\t\t\tcode;\n\n\ttrace_xfs_ioctl_setattr(ip);\n\n\tif (mp->m_flags & XFS_MOUNT_RDONLY)\n\t\treturn XFS_ERROR(EROFS);\n\tif (XFS_FORCED_SHUTDOWN(mp))\n\t\treturn XFS_ERROR(EIO);\n\n\t/*\n\t * Disallow 32bit project ids when projid32bit feature is not enabled.\n\t */\n\tif ((mask & FSX_PROJID) && (fa->fsx_projid > (__uint16_t)-1) &&\n\t\t\t!xfs_sb_version_hasprojid32bit(&ip->i_mount->m_sb))\n\t\treturn XFS_ERROR(EINVAL);\n\n\t/*\n\t * If disk quotas is on, we make sure that the dquots do exist on disk,\n\t * before we start any other transactions. Trying to do this later\n\t * is messy. We don't care to take a readlock to look at the ids\n\t * in inode here, because we can't hold it across the trans_reserve.\n\t * If the IDs do change before we take the ilock, we're covered\n\t * because the i_*dquot fields will get updated anyway.\n\t */\n\tif (XFS_IS_QUOTA_ON(mp) && (mask & FSX_PROJID)) {\n\t\tcode = xfs_qm_vop_dqalloc(ip, ip->i_d.di_uid,\n\t\t\t\t\t ip->i_d.di_gid, fa->fsx_projid,\n\t\t\t\t\t XFS_QMOPT_PQUOTA, &udqp, NULL, &pdqp);\n\t\tif (code)\n\t\t\treturn code;\n\t}\n\n\t/*\n\t * For the other attributes, we acquire the inode lock and\n\t * first do an error checking pass.\n\t */\n\ttp = xfs_trans_alloc(mp, XFS_TRANS_SETATTR_NOT_SIZE);\n\tcode = xfs_trans_reserve(tp, &M_RES(mp)->tr_ichange, 0, 0);\n\tif (code)\n\t\tgoto error_return;\n\n\tlock_flags = XFS_ILOCK_EXCL;\n\txfs_ilock(ip, lock_flags);\n\n\t/*\n\t * CAP_FOWNER overrides the following restrictions:\n\t *\n\t * The user ID of the calling process must be equal\n\t * to the file owner ID, except in cases where the\n\t * CAP_FSETID capability is applicable.\n\t */\n\tif (!inode_owner_or_capable(VFS_I(ip))) {\n\t\tcode = XFS_ERROR(EPERM);\n\t\tgoto error_return;\n\t}\n\n\t/*\n\t * Do a quota reservation only if projid is actually going to change.\n\t * Only allow changing of projid from init_user_ns since it is a\n\t * non user namespace aware identifier.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\tif (current_user_ns() != &init_user_ns) {\n\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\tgoto error_return;\n\t\t}\n\n\t\tif (XFS_IS_QUOTA_RUNNING(mp) &&\n\t\t    XFS_IS_PQUOTA_ON(mp) &&\n\t\t    xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tASSERT(tp);\n\t\t\tcode = xfs_qm_vop_chown_reserve(tp, ip, udqp, NULL,\n\t\t\t\t\t\tpdqp, capable(CAP_FOWNER) ?\n\t\t\t\t\t\tXFS_QMOPT_FORCE_RES : 0);\n\t\t\tif (code)\t/* out of quota */\n\t\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\tif (mask & FSX_EXTSIZE) {\n\t\t/*\n\t\t * Can't change extent size if any extents are allocated.\n\t\t */\n\t\tif (ip->i_d.di_nextents &&\n\t\t    ((ip->i_d.di_extsize << mp->m_sb.sb_blocklog) !=\n\t\t     fa->fsx_extsize)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * Extent size must be a multiple of the appropriate block\n\t\t * size, if set at all. It must also be smaller than the\n\t\t * maximum extent size supported by the filesystem.\n\t\t *\n\t\t * Also, for non-realtime files, limit the extent size hint to\n\t\t * half the size of the AGs in the filesystem so alignment\n\t\t * doesn't result in extents larger than an AG.\n\t\t */\n\t\tif (fa->fsx_extsize != 0) {\n\t\t\txfs_extlen_t    size;\n\t\t\txfs_fsblock_t   extsize_fsb;\n\n\t\t\textsize_fsb = XFS_B_TO_FSB(mp, fa->fsx_extsize);\n\t\t\tif (extsize_fsb > MAXEXTLEN) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\n\t\t\tif (XFS_IS_REALTIME_INODE(ip) ||\n\t\t\t    ((mask & FSX_XFLAGS) &&\n\t\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME))) {\n\t\t\t\tsize = mp->m_sb.sb_rextsize <<\n\t\t\t\t       mp->m_sb.sb_blocklog;\n\t\t\t} else {\n\t\t\t\tsize = mp->m_sb.sb_blocksize;\n\t\t\t\tif (extsize_fsb > mp->m_sb.sb_agblocks / 2) {\n\t\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\t\tgoto error_return;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (fa->fsx_extsize % size) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\t}\n\n\n\tif (mask & FSX_XFLAGS) {\n\t\t/*\n\t\t * Can't change realtime flag if any extents are allocated.\n\t\t */\n\t\tif ((ip->i_d.di_nextents || ip->i_delayed_blks) &&\n\t\t    (XFS_IS_REALTIME_INODE(ip)) !=\n\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * If realtime flag is set then must have realtime data.\n\t\t */\n\t\tif ((fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tif ((mp->m_sb.sb_rblocks == 0) ||\n\t\t\t    (mp->m_sb.sb_rextsize == 0) ||\n\t\t\t    (ip->i_d.di_extsize % mp->m_sb.sb_rextsize)) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Can't modify an immutable/append-only file unless\n\t\t * we have appropriate permission.\n\t\t */\n\t\tif ((ip->i_d.di_flags &\n\t\t\t\t(XFS_DIFLAG_IMMUTABLE|XFS_DIFLAG_APPEND) ||\n\t\t     (fa->fsx_xflags &\n\t\t\t\t(XFS_XFLAG_IMMUTABLE | XFS_XFLAG_APPEND))) &&\n\t\t    !capable(CAP_LINUX_IMMUTABLE)) {\n\t\t\tcode = XFS_ERROR(EPERM);\n\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\txfs_trans_ijoin(tp, ip, 0);\n\n\t/*\n\t * Change file ownership.  Must be the owner or privileged.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\t/*\n\t\t * CAP_FSETID overrides the following restrictions:\n\t\t *\n\t\t * The set-user-ID and set-group-ID bits of a file will be\n\t\t * cleared upon successful return from chown()\n\t\t */\n\t\tif ((ip->i_d.di_mode & (S_ISUID|S_ISGID)) &&\n\t\t    !capable_wrt_inode_uidgid(VFS_I(ip), CAP_FSETID))\n\t\t\tip->i_d.di_mode &= ~(S_ISUID|S_ISGID);\n\n\t\t/*\n\t\t * Change the ownerships and register quota modifications\n\t\t * in the transaction.\n\t\t */\n\t\tif (xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tif (XFS_IS_QUOTA_RUNNING(mp) && XFS_IS_PQUOTA_ON(mp)) {\n\t\t\t\tolddquot = xfs_qm_vop_chown(tp, ip,\n\t\t\t\t\t\t\t&ip->i_pdquot, pdqp);\n\t\t\t}\n\t\t\txfs_set_projid(ip, fa->fsx_projid);\n\n\t\t\t/*\n\t\t\t * We may have to rev the inode as well as\n\t\t\t * the superblock version number since projids didn't\n\t\t\t * exist before DINODE_VERSION_2 and SB_VERSION_NLINK.\n\t\t\t */\n\t\t\tif (ip->i_d.di_version == 1)\n\t\t\t\txfs_bump_ino_vers2(tp, ip);\n\t\t}\n\n\t}\n\n\tif (mask & FSX_EXTSIZE)\n\t\tip->i_d.di_extsize = fa->fsx_extsize >> mp->m_sb.sb_blocklog;\n\tif (mask & FSX_XFLAGS) {\n\t\txfs_set_diflags(ip, fa->fsx_xflags);\n\t\txfs_diflags_to_linux(ip);\n\t}\n\n\txfs_trans_ichgtime(tp, ip, XFS_ICHGTIME_CHG);\n\txfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);\n\n\tXFS_STATS_INC(xs_ig_attrchg);\n\n\t/*\n\t * If this is a synchronous mount, make sure that the\n\t * transaction goes to disk before returning to the user.\n\t * This is slightly sub-optimal in that truncates require\n\t * two sync transactions instead of one for wsync filesystems.\n\t * One for the truncate and one for the timestamps since we\n\t * don't want to change the timestamps unless we're sure the\n\t * truncate worked.  Truncates are less than 1% of the laddis\n\t * mix so this probably isn't worth the trouble to optimize.\n\t */\n\tif (mp->m_flags & XFS_MOUNT_WSYNC)\n\t\txfs_trans_set_sync(tp);\n\tcode = xfs_trans_commit(tp, 0);\n\txfs_iunlock(ip, lock_flags);\n\n\t/*\n\t * Release any dquot(s) the inode had kept before chown.\n\t */\n\txfs_qm_dqrele(olddquot);\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\n\treturn code;\n\n error_return:\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\txfs_trans_cancel(tp, 0);\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\n\treturn code;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void flush_end_io(struct request *flush_rq, int error)\n{\n\tstruct request_queue *q = flush_rq->q;\n\tstruct list_head *running;\n\tbool queued = false;\n\tstruct request *rq, *n;\n\tunsigned long flags = 0;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, flush_rq->mq_ctx);\n\n\tif (q->mq_ops) {\n\t\tspin_lock_irqsave(&fq->mq_flush_lock, flags);\n\t\tflush_rq->tag = -1;\n\t}\n\n\trunning = &fq->flush_queue[fq->flush_running_idx];\n\tBUG_ON(fq->flush_pending_idx == fq->flush_running_idx);\n\n\t/* account completion of the flush request */\n\tfq->flush_running_idx ^= 1;\n\n\tif (!q->mq_ops)\n\t\telv_completed_request(q, flush_rq);\n\n\t/* and push the waiting requests to the next stage */\n\tlist_for_each_entry_safe(rq, n, running, flush.list) {\n\t\tunsigned int seq = blk_flush_cur_seq(rq);\n\n\t\tBUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);\n\t\tqueued |= blk_flush_complete_seq(rq, fq, seq, error);\n\t}\n\n\t/*\n\t * Kick the queue to avoid stall for two cases:\n\t * 1. Moving a request silently to empty queue_head may stall the\n\t * queue.\n\t * 2. When flush request is running in non-queueable queue, the\n\t * queue is hold. Restart the queue after flush request is finished\n\t * to avoid stall.\n\t * This function is called from request completion path and calling\n\t * directly into request_fn may confuse the driver.  Always use\n\t * kblockd.\n\t */\n\tif (queued || fq->flush_queue_delayed) {\n\t\tWARN_ON(q->mq_ops);\n\t\tblk_run_queue_async(q);\n\t}\n\tfq->flush_queue_delayed = 0;\n\tif (q->mq_ops)\n\t\tspin_unlock_irqrestore(&fq->mq_flush_lock, flags);\n}",
                        "code_after_change": "static void flush_end_io(struct request *flush_rq, int error)\n{\n\tstruct request_queue *q = flush_rq->q;\n\tstruct list_head *running;\n\tbool queued = false;\n\tstruct request *rq, *n;\n\tunsigned long flags = 0;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, flush_rq->mq_ctx);\n\n\tif (q->mq_ops) {\n\t\tstruct blk_mq_hw_ctx *hctx;\n\n\t\t/* release the tag's ownership to the req cloned from */\n\t\tspin_lock_irqsave(&fq->mq_flush_lock, flags);\n\t\thctx = q->mq_ops->map_queue(q, flush_rq->mq_ctx->cpu);\n\t\tblk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);\n\t\tflush_rq->tag = -1;\n\t}\n\n\trunning = &fq->flush_queue[fq->flush_running_idx];\n\tBUG_ON(fq->flush_pending_idx == fq->flush_running_idx);\n\n\t/* account completion of the flush request */\n\tfq->flush_running_idx ^= 1;\n\n\tif (!q->mq_ops)\n\t\telv_completed_request(q, flush_rq);\n\n\t/* and push the waiting requests to the next stage */\n\tlist_for_each_entry_safe(rq, n, running, flush.list) {\n\t\tunsigned int seq = blk_flush_cur_seq(rq);\n\n\t\tBUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);\n\t\tqueued |= blk_flush_complete_seq(rq, fq, seq, error);\n\t}\n\n\t/*\n\t * Kick the queue to avoid stall for two cases:\n\t * 1. Moving a request silently to empty queue_head may stall the\n\t * queue.\n\t * 2. When flush request is running in non-queueable queue, the\n\t * queue is hold. Restart the queue after flush request is finished\n\t * to avoid stall.\n\t * This function is called from request completion path and calling\n\t * directly into request_fn may confuse the driver.  Always use\n\t * kblockd.\n\t */\n\tif (queued || fq->flush_queue_delayed) {\n\t\tWARN_ON(q->mq_ops);\n\t\tblk_run_queue_async(q);\n\t}\n\tfq->flush_queue_delayed = 0;\n\tif (q->mq_ops)\n\t\tspin_unlock_irqrestore(&fq->mq_flush_lock, flags);\n}",
                        "cve_id": "CVE-2015-9016"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int persistent_prepare_exception(struct dm_exception_store *store,\n\t\t\t\t\tstruct dm_exception *e)\n{\n\tstruct pstore *ps = get_info(store);\n\tuint32_t stride;\n\tchunk_t next_free;\n\tsector_t size = get_dev_size(dm_snap_cow(store->snap)->bdev);\n\n\t/* Is there enough room ? */\n\tif (size < ((ps->next_free + 1) * store->chunk_size))\n\t\treturn -ENOSPC;\n\n\te->new_chunk = ps->next_free;\n\n\t/*\n\t * Move onto the next free pending, making sure to take\n\t * into account the location of the metadata chunks.\n\t */\n\tstride = (ps->exceptions_per_area + 1);\n\tnext_free = ++ps->next_free;\n\tif (sector_div(next_free, stride) == 1)\n\t\tps->next_free++;\n\n\tatomic_inc(&ps->pending_count);\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic void flush_end_io(struct request *flush_rq, int error)\n{\n\tstruct request_queue *q = flush_rq->q;\n\tstruct list_head *running;\n\tbool queued = false;\n\tstruct request *rq, *n;\n\tunsigned long flags = 0;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, flush_rq->mq_ctx);\n\n\tif (q->mq_ops) {\n\t\tspin_lock_irqsave(&fq->mq_flush_lock, flags);\n\t\tflush_rq->tag = -1;\n\t}\n\n\trunning = &fq->flush_queue[fq->flush_running_idx];\n\tBUG_ON(fq->flush_pending_idx == fq->flush_running_idx);\n\n\t/* account completion of the flush request */\n\tfq->flush_running_idx ^= 1;\n\n\tif (!q->mq_ops)\n\t\telv_completed_request(q, flush_rq);\n\n\t/* and push the waiting requests to the next stage */\n\tlist_for_each_entry_safe(rq, n, running, flush.list) {\n\t\tunsigned int seq = blk_flush_cur_seq(rq);\n\n\t\tBUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);\n\t\tqueued |= blk_flush_complete_seq(rq, fq, seq, error);\n\t}\n\n\t/*\n\t * Kick the queue to avoid stall for two cases:\n\t * 1. Moving a request silently to empty queue_head may stall the\n\t * queue.\n\t * 2. When flush request is running in non-queueable queue, the\n\t * queue is hold. Restart the queue after flush request is finished\n\t * to avoid stall.\n\t * This function is called from request completion path and calling\n\t * directly into request_fn may confuse the driver.  Always use\n\t * kblockd.\n\t */\n\tif (queued || fq->flush_queue_delayed) {\n\t\tWARN_ON(q->mq_ops);\n\t\tblk_run_queue_async(q);\n\t}\n\tfq->flush_queue_delayed = 0;\n\tif (q->mq_ops)\n\t\tspin_unlock_irqrestore(&fq->mq_flush_lock, flags);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic void flush_end_io(struct request *flush_rq, int error)\n{\n\tstruct request_queue *q = flush_rq->q;\n\tstruct list_head *running;\n\tbool queued = false;\n\tstruct request *rq, *n;\n\tunsigned long flags = 0;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, flush_rq->mq_ctx);\n\n\tif (q->mq_ops) {\n\t\tstruct blk_mq_hw_ctx *hctx;\n\n\t\t/* release the tag's ownership to the req cloned from */\n\t\tspin_lock_irqsave(&fq->mq_flush_lock, flags);\n\t\thctx = q->mq_ops->map_queue(q, flush_rq->mq_ctx->cpu);\n\t\tblk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);\n\t\tflush_rq->tag = -1;\n\t}\n\n\trunning = &fq->flush_queue[fq->flush_running_idx];\n\tBUG_ON(fq->flush_pending_idx == fq->flush_running_idx);\n\n\t/* account completion of the flush request */\n\tfq->flush_running_idx ^= 1;\n\n\tif (!q->mq_ops)\n\t\telv_completed_request(q, flush_rq);\n\n\t/* and push the waiting requests to the next stage */\n\tlist_for_each_entry_safe(rq, n, running, flush.list) {\n\t\tunsigned int seq = blk_flush_cur_seq(rq);\n\n\t\tBUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);\n\t\tqueued |= blk_flush_complete_seq(rq, fq, seq, error);\n\t}\n\n\t/*\n\t * Kick the queue to avoid stall for two cases:\n\t * 1. Moving a request silently to empty queue_head may stall the\n\t * queue.\n\t * 2. When flush request is running in non-queueable queue, the\n\t * queue is hold. Restart the queue after flush request is finished\n\t * to avoid stall.\n\t * This function is called from request completion path and calling\n\t * directly into request_fn may confuse the driver.  Always use\n\t * kblockd.\n\t */\n\tif (queued || fq->flush_queue_delayed) {\n\t\tWARN_ON(q->mq_ops);\n\t\tblk_run_queue_async(q);\n\t}\n\tfq->flush_queue_delayed = 0;\n\tif (q->mq_ops)\n\t\tspin_unlock_irqrestore(&fq->mq_flush_lock, flags);\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        }
    ],
    "non_vul_data": [
        {
            "id": 995,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "struct ipv6_txoptions *\nipv6_dup_options(struct sock *sk, struct ipv6_txoptions *opt)\n{\n\tstruct ipv6_txoptions *opt2;\n\n\topt2 = sock_kmalloc(sk, opt->tot_len, GFP_ATOMIC);\n\tif (opt2) {\n\t\tlong dif = (char *)opt2 - (char *)opt;\n\t\tmemcpy(opt2, opt, opt->tot_len);\n\t\tif (opt2->hopopt)\n\t\t\t*((char **)&opt2->hopopt) += dif;\n\t\tif (opt2->dst0opt)\n\t\t\t*((char **)&opt2->dst0opt) += dif;\n\t\tif (opt2->dst1opt)\n\t\t\t*((char **)&opt2->dst1opt) += dif;\n\t\tif (opt2->srcrt)\n\t\t\t*((char **)&opt2->srcrt) += dif;\n\t\tatomic_set(&opt2->refcnt, 1);\n\t}\n\treturn opt2;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "struct ipv6_txoptions *\nipv6_renew_options(struct sock *sk, struct ipv6_txoptions *opt,\n\t\t   int newtype,\n\t\t   struct ipv6_opt_hdr __user *newopt, int newoptlen)\n{\n\tint tot_len = 0;\n\tchar *p;\n\tstruct ipv6_txoptions *opt2;\n\tint err;\n\n\tif (opt) {\n\t\tif (newtype != IPV6_HOPOPTS && opt->hopopt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->hopopt));\n\t\tif (newtype != IPV6_RTHDRDSTOPTS && opt->dst0opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst0opt));\n\t\tif (newtype != IPV6_RTHDR && opt->srcrt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->srcrt));\n\t\tif (newtype != IPV6_DSTOPTS && opt->dst1opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst1opt));\n\t}\n\n\tif (newopt && newoptlen)\n\t\ttot_len += CMSG_ALIGN(newoptlen);\n\n\tif (!tot_len)\n\t\treturn NULL;\n\n\ttot_len += sizeof(*opt2);\n\topt2 = sock_kmalloc(sk, tot_len, GFP_ATOMIC);\n\tif (!opt2)\n\t\treturn ERR_PTR(-ENOBUFS);\n\n\tmemset(opt2, 0, tot_len);\n\n\topt2->tot_len = tot_len;\n\tp = (char *)(opt2 + 1);\n\n\terr = ipv6_renew_option(opt ? opt->hopopt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_HOPOPTS,\n\t\t\t\t&opt2->hopopt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst0opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDRDSTOPTS,\n\t\t\t\t&opt2->dst0opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->srcrt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDR,\n\t\t\t\t(struct ipv6_opt_hdr **)&opt2->srcrt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst1opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_DSTOPTS,\n\t\t\t\t&opt2->dst1opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\topt2->opt_nflen = (opt2->hopopt ? ipv6_optlen(opt2->hopopt) : 0) +\n\t\t\t  (opt2->dst0opt ? ipv6_optlen(opt2->dst0opt) : 0) +\n\t\t\t  (opt2->srcrt ? ipv6_optlen(opt2->srcrt) : 0);\n\topt2->opt_flen = (opt2->dst1opt ? ipv6_optlen(opt2->dst1opt) : 0);\n\n\treturn opt2;\nout:\n\tsock_kfree_s(sk, opt2, opt2->tot_len);\n\treturn ERR_PTR(err);\n}",
                        "code_after_change": "struct ipv6_txoptions *\nipv6_renew_options(struct sock *sk, struct ipv6_txoptions *opt,\n\t\t   int newtype,\n\t\t   struct ipv6_opt_hdr __user *newopt, int newoptlen)\n{\n\tint tot_len = 0;\n\tchar *p;\n\tstruct ipv6_txoptions *opt2;\n\tint err;\n\n\tif (opt) {\n\t\tif (newtype != IPV6_HOPOPTS && opt->hopopt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->hopopt));\n\t\tif (newtype != IPV6_RTHDRDSTOPTS && opt->dst0opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst0opt));\n\t\tif (newtype != IPV6_RTHDR && opt->srcrt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->srcrt));\n\t\tif (newtype != IPV6_DSTOPTS && opt->dst1opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst1opt));\n\t}\n\n\tif (newopt && newoptlen)\n\t\ttot_len += CMSG_ALIGN(newoptlen);\n\n\tif (!tot_len)\n\t\treturn NULL;\n\n\ttot_len += sizeof(*opt2);\n\topt2 = sock_kmalloc(sk, tot_len, GFP_ATOMIC);\n\tif (!opt2)\n\t\treturn ERR_PTR(-ENOBUFS);\n\n\tmemset(opt2, 0, tot_len);\n\tatomic_set(&opt2->refcnt, 1);\n\topt2->tot_len = tot_len;\n\tp = (char *)(opt2 + 1);\n\n\terr = ipv6_renew_option(opt ? opt->hopopt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_HOPOPTS,\n\t\t\t\t&opt2->hopopt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst0opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDRDSTOPTS,\n\t\t\t\t&opt2->dst0opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->srcrt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDR,\n\t\t\t\t(struct ipv6_opt_hdr **)&opt2->srcrt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst1opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_DSTOPTS,\n\t\t\t\t&opt2->dst1opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\topt2->opt_nflen = (opt2->hopopt ? ipv6_optlen(opt2->hopopt) : 0) +\n\t\t\t  (opt2->dst0opt ? ipv6_optlen(opt2->dst0opt) : 0) +\n\t\t\t  (opt2->srcrt ? ipv6_optlen(opt2->srcrt) : 0);\n\topt2->opt_flen = (opt2->dst1opt ? ipv6_optlen(opt2->dst1opt) : 0);\n\n\treturn opt2;\nout:\n\tsock_kfree_s(sk, opt2, opt2->tot_len);\n\treturn ERR_PTR(err);\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstruct ipv6_txoptions *\nipv6_dup_options(struct sock *sk, struct ipv6_txoptions *opt)\n{\n\tstruct ipv6_txoptions *opt2;\n\n\topt2 = sock_kmalloc(sk, opt->tot_len, GFP_ATOMIC);\n\tif (opt2) {\n\t\tlong dif = (char *)opt2 - (char *)opt;\n\t\tmemcpy(opt2, opt, opt->tot_len);\n\t\tif (opt2->hopopt)\n\t\t\t*((char **)&opt2->hopopt) += dif;\n\t\tif (opt2->dst0opt)\n\t\t\t*((char **)&opt2->dst0opt) += dif;\n\t\tif (opt2->dst1opt)\n\t\t\t*((char **)&opt2->dst1opt) += dif;\n\t\tif (opt2->srcrt)\n\t\t\t*((char **)&opt2->srcrt) += dif;\n\t\tatomic_set(&opt2->refcnt, 1);\n\t}\n\treturn opt2;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstruct ipv6_txoptions *\nipv6_renew_options(struct sock *sk, struct ipv6_txoptions *opt,\n\t\t   int newtype,\n\t\t   struct ipv6_opt_hdr __user *newopt, int newoptlen)\n{\n\tint tot_len = 0;\n\tchar *p;\n\tstruct ipv6_txoptions *opt2;\n\tint err;\n\n\tif (opt) {\n\t\tif (newtype != IPV6_HOPOPTS && opt->hopopt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->hopopt));\n\t\tif (newtype != IPV6_RTHDRDSTOPTS && opt->dst0opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst0opt));\n\t\tif (newtype != IPV6_RTHDR && opt->srcrt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->srcrt));\n\t\tif (newtype != IPV6_DSTOPTS && opt->dst1opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst1opt));\n\t}\n\n\tif (newopt && newoptlen)\n\t\ttot_len += CMSG_ALIGN(newoptlen);\n\n\tif (!tot_len)\n\t\treturn NULL;\n\n\ttot_len += sizeof(*opt2);\n\topt2 = sock_kmalloc(sk, tot_len, GFP_ATOMIC);\n\tif (!opt2)\n\t\treturn ERR_PTR(-ENOBUFS);\n\n\tmemset(opt2, 0, tot_len);\n\n\topt2->tot_len = tot_len;\n\tp = (char *)(opt2 + 1);\n\n\terr = ipv6_renew_option(opt ? opt->hopopt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_HOPOPTS,\n\t\t\t\t&opt2->hopopt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst0opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDRDSTOPTS,\n\t\t\t\t&opt2->dst0opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->srcrt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDR,\n\t\t\t\t(struct ipv6_opt_hdr **)&opt2->srcrt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst1opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_DSTOPTS,\n\t\t\t\t&opt2->dst1opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\topt2->opt_nflen = (opt2->hopopt ? ipv6_optlen(opt2->hopopt) : 0) +\n\t\t\t  (opt2->dst0opt ? ipv6_optlen(opt2->dst0opt) : 0) +\n\t\t\t  (opt2->srcrt ? ipv6_optlen(opt2->srcrt) : 0);\n\topt2->opt_flen = (opt2->dst1opt ? ipv6_optlen(opt2->dst1opt) : 0);\n\n\treturn opt2;\nout:\n\tsock_kfree_s(sk, opt2, opt2->tot_len);\n\treturn ERR_PTR(err);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstruct ipv6_txoptions *\nipv6_renew_options(struct sock *sk, struct ipv6_txoptions *opt,\n\t\t   int newtype,\n\t\t   struct ipv6_opt_hdr __user *newopt, int newoptlen)\n{\n\tint tot_len = 0;\n\tchar *p;\n\tstruct ipv6_txoptions *opt2;\n\tint err;\n\n\tif (opt) {\n\t\tif (newtype != IPV6_HOPOPTS && opt->hopopt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->hopopt));\n\t\tif (newtype != IPV6_RTHDRDSTOPTS && opt->dst0opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst0opt));\n\t\tif (newtype != IPV6_RTHDR && opt->srcrt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->srcrt));\n\t\tif (newtype != IPV6_DSTOPTS && opt->dst1opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst1opt));\n\t}\n\n\tif (newopt && newoptlen)\n\t\ttot_len += CMSG_ALIGN(newoptlen);\n\n\tif (!tot_len)\n\t\treturn NULL;\n\n\ttot_len += sizeof(*opt2);\n\topt2 = sock_kmalloc(sk, tot_len, GFP_ATOMIC);\n\tif (!opt2)\n\t\treturn ERR_PTR(-ENOBUFS);\n\n\tmemset(opt2, 0, tot_len);\n\tatomic_set(&opt2->refcnt, 1);\n\topt2->tot_len = tot_len;\n\tp = (char *)(opt2 + 1);\n\n\terr = ipv6_renew_option(opt ? opt->hopopt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_HOPOPTS,\n\t\t\t\t&opt2->hopopt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst0opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDRDSTOPTS,\n\t\t\t\t&opt2->dst0opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->srcrt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDR,\n\t\t\t\t(struct ipv6_opt_hdr **)&opt2->srcrt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst1opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_DSTOPTS,\n\t\t\t\t&opt2->dst1opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\topt2->opt_nflen = (opt2->hopopt ? ipv6_optlen(opt2->hopopt) : 0) +\n\t\t\t  (opt2->dst0opt ? ipv6_optlen(opt2->dst0opt) : 0) +\n\t\t\t  (opt2->srcrt ? ipv6_optlen(opt2->srcrt) : 0);\n\topt2->opt_flen = (opt2->dst1opt ? ipv6_optlen(opt2->dst1opt) : 0);\n\n\treturn opt2;\nout:\n\tsock_kfree_s(sk, opt2, opt2->tot_len);\n\treturn ERR_PTR(err);\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "void inet6_destroy_sock(struct sock *sk)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct ipv6_txoptions *opt;\n\n\t/* Release rx options */\n\n\tskb = xchg(&np->pktoptions, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\tskb = xchg(&np->rxpmtu, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\t/* Free flowlabels */\n\tfl6_free_socklist(sk);\n\n\t/* Free tx options */\n\n\topt = xchg(&np->opt, NULL);\n\tif (opt)\n\t\tsock_kfree_s(sk, opt, opt->tot_len);\n}",
                        "code_after_change": "void inet6_destroy_sock(struct sock *sk)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct ipv6_txoptions *opt;\n\n\t/* Release rx options */\n\n\tskb = xchg(&np->pktoptions, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\tskb = xchg(&np->rxpmtu, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\t/* Free flowlabels */\n\tfl6_free_socklist(sk);\n\n\t/* Free tx options */\n\n\topt = xchg((__force struct ipv6_txoptions **)&np->opt, NULL);\n\tif (opt) {\n\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\ttxopt_put(opt);\n\t}\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstruct ipv6_txoptions *\nipv6_dup_options(struct sock *sk, struct ipv6_txoptions *opt)\n{\n\tstruct ipv6_txoptions *opt2;\n\n\topt2 = sock_kmalloc(sk, opt->tot_len, GFP_ATOMIC);\n\tif (opt2) {\n\t\tlong dif = (char *)opt2 - (char *)opt;\n\t\tmemcpy(opt2, opt, opt->tot_len);\n\t\tif (opt2->hopopt)\n\t\t\t*((char **)&opt2->hopopt) += dif;\n\t\tif (opt2->dst0opt)\n\t\t\t*((char **)&opt2->dst0opt) += dif;\n\t\tif (opt2->dst1opt)\n\t\t\t*((char **)&opt2->dst1opt) += dif;\n\t\tif (opt2->srcrt)\n\t\t\t*((char **)&opt2->srcrt) += dif;\n\t\tatomic_set(&opt2->refcnt, 1);\n\t}\n\treturn opt2;\n}\n'''\nRelated vulnerability code snippet:\n'''\nvoid inet6_destroy_sock(struct sock *sk)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct ipv6_txoptions *opt;\n\n\t/* Release rx options */\n\n\tskb = xchg(&np->pktoptions, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\tskb = xchg(&np->rxpmtu, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\t/* Free flowlabels */\n\tfl6_free_socklist(sk);\n\n\t/* Free tx options */\n\n\topt = xchg(&np->opt, NULL);\n\tif (opt)\n\t\tsock_kfree_s(sk, opt, opt->tot_len);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nvoid inet6_destroy_sock(struct sock *sk)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct ipv6_txoptions *opt;\n\n\t/* Release rx options */\n\n\tskb = xchg(&np->pktoptions, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\tskb = xchg(&np->rxpmtu, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\t/* Free flowlabels */\n\tfl6_free_socklist(sk);\n\n\t/* Free tx options */\n\n\topt = xchg((__force struct ipv6_txoptions **)&np->opt, NULL);\n\tif (opt) {\n\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\ttxopt_put(opt);\n\t}\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "struct ipv6_txoptions *ipv6_update_options(struct sock *sk,\n\t\t\t\t\t   struct ipv6_txoptions *opt)\n{\n\tif (inet_sk(sk)->is_icsk) {\n\t\tif (opt &&\n\t\t    !((1 << sk->sk_state) & (TCPF_LISTEN | TCPF_CLOSE)) &&\n\t\t    inet_sk(sk)->inet_daddr != LOOPBACK4_IPV6) {\n\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;\n\t\t\ticsk->icsk_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t}\n\t}\n\topt = xchg(&inet6_sk(sk)->opt, opt);\n\tsk_dst_reset(sk);\n\n\treturn opt;\n}",
                        "code_after_change": "static int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, valbool;\n\tint retv = -ENOPROTOOPT;\n\tbool needs_rtnl = setsockopt_needs_rtnl(optname);\n\n\tif (!optval)\n\t\tval = 0;\n\telse {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (get_user(val, (int __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\tval = 0;\n\t}\n\n\tvalbool = (val != 0);\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_setsockopt(sk, optname, optval, optlen);\n\n\tif (needs_rtnl)\n\t\trtnl_lock();\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\n\tcase IPV6_ADDRFORM:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val == PF_INET) {\n\t\t\tstruct ipv6_txoptions *opt;\n\t\t\tstruct sk_buff *pktopt;\n\n\t\t\tif (sk->sk_type == SOCK_RAW)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_protocol == IPPROTO_UDP ||\n\t\t\t    sk->sk_protocol == IPPROTO_UDPLITE) {\n\t\t\t\tstruct udp_sock *up = udp_sk(sk);\n\t\t\t\tif (up->pending == AF_INET6) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (sk->sk_protocol != IPPROTO_TCP)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t\tretv = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (ipv6_only_sock(sk) ||\n\t\t\t    !ipv6_addr_v4mapped(&sk->sk_v6_daddr)) {\n\t\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tfl6_free_socklist(sk);\n\t\t\tipv6_sock_mc_close(sk);\n\n\t\t\t/*\n\t\t\t * Sock is moving from IPv6 to IPv4 (sk_prot), so\n\t\t\t * remove it from the refcnt debug socks count in the\n\t\t\t * original family...\n\t\t\t */\n\t\t\tsk_refcnt_debug_dec(sk);\n\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = &tcp_prot;\n\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;\n\t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t\t} else {\n\t\t\t\tstruct proto *prot = &udp_prot;\n\n\t\t\t\tif (sk->sk_protocol == IPPROTO_UDPLITE)\n\t\t\t\t\tprot = &udplite_prot;\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = prot;\n\t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t}\n\t\t\topt = xchg((__force struct ipv6_txoptions **)&np->opt,\n\t\t\t\t   NULL);\n\t\t\tif (opt) {\n\t\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\t\ttxopt_put(opt);\n\t\t\t}\n\t\t\tpktopt = xchg(&np->pktoptions, NULL);\n\t\t\tkfree_skb(pktopt);\n\n\t\t\tsk->sk_destruct = inet_sock_destruct;\n\t\t\t/*\n\t\t\t * ... and add it to the refcnt debug socks count\n\t\t\t * in the new family. -acme\n\t\t\t */\n\t\t\tsk_refcnt_debug_inc(sk);\n\t\t\tmodule_put(THIS_MODULE);\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\t\tgoto e_inval;\n\n\tcase IPV6_V6ONLY:\n\t\tif (optlen < sizeof(int) ||\n\t\t    inet_sk(sk)->inet_num)\n\t\t\tgoto e_inval;\n\t\tsk->sk_ipv6only = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxoinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxhlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxohlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.srcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.osrcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.hopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.ohopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.dstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.odstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < -1 || val > 0xff)\n\t\t\tgoto e_inval;\n\t\t/* RFC 3542, 6.5: default traffic class of 0x0 */\n\t\tif (val == -1)\n\t\t\tval = 0;\n\t\tnp->tclass = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxtclass = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxpmtu = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TRANSPARENT:\n\t\tif (valbool && !ns_capable(net->user_ns, CAP_NET_ADMIN) &&\n\t\t    !ns_capable(net->user_ns, CAP_NET_RAW)) {\n\t\t\tretv = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we don't have a separate transparent bit for IPV6 we use the one in the IPv4 socket */\n\t\tinet_sk(sk)->transparent = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxorigdstaddr = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\t/* remove any sticky options header with a zero option\n\t\t * length, per RFC3542.\n\t\t */\n\t\tif (optlen == 0)\n\t\t\toptval = NULL;\n\t\telse if (!optval)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct ipv6_opt_hdr) ||\n\t\t\t optlen & 0x7 || optlen > 8 * 255)\n\t\t\tgoto e_inval;\n\n\t\t/* hop-by-hop / destination options are privileged option */\n\t\tretv = -EPERM;\n\t\tif (optname != IPV6_RTHDR && !ns_capable(net->user_ns, CAP_NET_RAW))\n\t\t\tbreak;\n\n\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\t\topt = ipv6_renew_options(sk, opt, optname,\n\t\t\t\t\t (struct ipv6_opt_hdr __user *)optval,\n\t\t\t\t\t optlen);\n\t\tif (IS_ERR(opt)) {\n\t\t\tretv = PTR_ERR(opt);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* routing header option needs extra check */\n\t\tretv = -EINVAL;\n\t\tif (optname == IPV6_RTHDR && opt && opt->srcrt) {\n\t\t\tstruct ipv6_rt_hdr *rthdr = opt->srcrt;\n\t\t\tswitch (rthdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\t\tcase IPV6_SRCRT_TYPE_2:\n\t\t\t\tif (rthdr->hdrlen != 2 ||\n\t\t\t\t    rthdr->segments_left != 1)\n\t\t\t\t\tgoto sticky_done;\n\n\t\t\t\tbreak;\n#endif\n\t\t\tdefault:\n\t\t\t\tgoto sticky_done;\n\t\t\t}\n\t\t}\n\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\nsticky_done:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase IPV6_PKTINFO:\n\t{\n\t\tstruct in6_pktinfo pkt;\n\n\t\tif (optlen == 0)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct in6_pktinfo) || !optval)\n\t\t\tgoto e_inval;\n\n\t\tif (copy_from_user(&pkt, optval, sizeof(struct in6_pktinfo))) {\n\t\t\t\tretv = -EFAULT;\n\t\t\t\tbreak;\n\t\t}\n\t\tif (sk->sk_bound_dev_if && pkt.ipi6_ifindex != sk->sk_bound_dev_if)\n\t\t\tgoto e_inval;\n\n\t\tnp->sticky_pktinfo.ipi6_ifindex = pkt.ipi6_ifindex;\n\t\tnp->sticky_pktinfo.ipi6_addr = pkt.ipi6_addr;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct ipv6_txoptions *opt = NULL;\n\t\tstruct msghdr msg;\n\t\tstruct flowi6 fl6;\n\t\tint junk;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\n\t\tif (optlen == 0)\n\t\t\tgoto update;\n\n\t\t/* 1K is probably excessive\n\t\t * 1K is surely not enough, 2K per standard header is 16K.\n\t\t */\n\t\tretv = -EINVAL;\n\t\tif (optlen > 64*1024)\n\t\t\tbreak;\n\n\t\topt = sock_kmalloc(sk, sizeof(*opt) + optlen, GFP_KERNEL);\n\t\tretv = -ENOBUFS;\n\t\tif (!opt)\n\t\t\tbreak;\n\n\t\tmemset(opt, 0, sizeof(*opt));\n\t\tatomic_set(&opt->refcnt, 1);\n\t\topt->tot_len = sizeof(*opt) + optlen;\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(opt+1, optval, optlen))\n\t\t\tgoto done;\n\n\t\tmsg.msg_controllen = optlen;\n\t\tmsg.msg_control = (void *)(opt+1);\n\n\t\tretv = ip6_datagram_send_ctl(net, sk, &msg, &fl6, opt, &junk,\n\t\t\t\t\t     &junk, &junk);\n\t\tif (retv)\n\t\t\tgoto done;\nupdate:\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\ndone:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\tcase IPV6_UNICAST_HOPS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->hop_limit = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_HOPS:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->mcast_hops = (val == -1 ? IPV6_DEFAULT_MCASTHOPS : val);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val != valbool)\n\t\t\tgoto e_inval;\n\t\tnp->mc_loop = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t{\n\t\tstruct net_device *dev = NULL;\n\t\tint ifindex;\n\n\t\tif (optlen != sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tifindex = (__force int)ntohl((__force __be32)val);\n\t\tif (ifindex == 0) {\n\t\t\tnp->ucast_oif = 0;\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tdev = dev_get_by_index(net, ifindex);\n\t\tretv = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\tretv = -EINVAL;\n\t\tif (sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tnp->ucast_oif = ifindex;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_IF:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tif (val) {\n\t\t\tstruct net_device *dev;\n\n\t\t\tif (sk->sk_bound_dev_if && sk->sk_bound_dev_if != val)\n\t\t\t\tgoto e_inval;\n\n\t\t\tdev = dev_get_by_index(net, val);\n\t\t\tif (!dev) {\n\t\t\t\tretv = -ENODEV;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdev_put(dev);\n\t\t}\n\t\tnp->mcast_oif = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_ADD_MEMBERSHIP)\n\t\t\tretv = ipv6_sock_mc_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_JOIN_ANYCAST)\n\t\t\tretv = ipv6_sock_ac_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_ac_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t{\n\t\tstruct group_req greq;\n\t\tstruct sockaddr_in6 *psin6;\n\n\t\tif (optlen < sizeof(struct group_req))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&greq, optval, sizeof(struct group_req)))\n\t\t\tbreak;\n\t\tif (greq.gr_group.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tpsin6 = (struct sockaddr_in6 *)&greq.gr_group;\n\t\tif (optname == MCAST_JOIN_GROUP)\n\t\t\tretv = ipv6_sock_mc_join(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t{\n\t\tstruct group_source_req greqs;\n\t\tint omode, add;\n\n\t\tif (optlen < sizeof(struct group_source_req))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&greqs, optval, sizeof(greqs))) {\n\t\t\tretv = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (greqs.gsr_group.ss_family != AF_INET6 ||\n\t\t    greqs.gsr_source.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tif (optname == MCAST_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == MCAST_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == MCAST_JOIN_SOURCE_GROUP) {\n\t\t\tstruct sockaddr_in6 *psin6;\n\n\t\t\tpsin6 = (struct sockaddr_in6 *)&greqs.gsr_group;\n\t\t\tretv = ipv6_sock_mc_join(sk, greqs.gsr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\t\t/* prior join w/ different source is ok */\n\t\t\tif (retv && retv != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* MCAST_LEAVE_SOURCE_GROUP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\tretv = ip6_mc_source(add, omode, sk, &greqs);\n\t\tbreak;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter *gsf;\n\n\t\tif (optlen < GROUP_FILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tgsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!gsf) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(gsf, optval, optlen)) {\n\t\t\tkfree(gsf);\n\t\t\tbreak;\n\t\t}\n\t\t/* numsrc >= (4G-140)/128 overflow in 32 bits */\n\t\tif (gsf->gf_numsrc >= 0x1ffffffU ||\n\t\t    gsf->gf_numsrc > sysctl_mld_max_msf) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tif (GROUP_FILTER_SIZE(gsf->gf_numsrc) > optlen) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tretv = ip6_mc_msfilter(sk, gsf);\n\t\tkfree(gsf);\n\n\t\tbreak;\n\t}\n\tcase IPV6_ROUTER_ALERT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = ip6_ra_control(sk, val);\n\t\tbreak;\n\tcase IPV6_MTU_DISCOVER:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < IPV6_PMTUDISC_DONT || val > IPV6_PMTUDISC_OMIT)\n\t\t\tgoto e_inval;\n\t\tnp->pmtudisc = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val && val < IPV6_MIN_MTU)\n\t\t\tgoto e_inval;\n\t\tnp->frag_size = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->recverr = valbool;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWINFO_SEND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->sndflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWLABEL_MGR:\n\t\tretv = ipv6_flowlabel_opt(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_IPSEC_POLICY:\n\tcase IPV6_XFRM_POLICY:\n\t\tretv = -EPERM;\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\tretv = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t    {\n\t\tunsigned int pref = 0;\n\t\tunsigned int prefmask = ~0;\n\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EINVAL;\n\n\t\t/* check PUBLIC/TMP/PUBTMP_DEFAULT conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_PUBLIC|\n\t\t\t       IPV6_PREFER_SRC_TMP|\n\t\t\t       IPV6_PREFER_SRC_PUBTMP_DEFAULT)) {\n\t\tcase IPV6_PREFER_SRC_PUBLIC:\n\t\t\tpref |= IPV6_PREFER_SRC_PUBLIC;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_TMP:\n\t\t\tpref |= IPV6_PREFER_SRC_TMP;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_PUBTMP_DEFAULT:\n\t\t\tbreak;\n\t\tcase 0:\n\t\t\tgoto pref_skip_pubtmp;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~(IPV6_PREFER_SRC_PUBLIC|\n\t\t\t      IPV6_PREFER_SRC_TMP);\npref_skip_pubtmp:\n\n\t\t/* check HOME/COA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_HOME|IPV6_PREFER_SRC_COA)) {\n\t\tcase IPV6_PREFER_SRC_HOME:\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_COA:\n\t\t\tpref |= IPV6_PREFER_SRC_COA;\n\t\tcase 0:\n\t\t\tgoto pref_skip_coa;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~IPV6_PREFER_SRC_COA;\npref_skip_coa:\n\n\t\t/* check CGA/NONCGA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_CGA|IPV6_PREFER_SRC_NONCGA)) {\n\t\tcase IPV6_PREFER_SRC_CGA:\n\t\tcase IPV6_PREFER_SRC_NONCGA:\n\t\tcase 0:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tnp->srcprefs = (np->srcprefs & prefmask) | pref;\n\t\tretv = 0;\n\n\t\tbreak;\n\t    }\n\tcase IPV6_MINHOPCOUNT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tnp->min_hopcount = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_DONTFRAG:\n\t\tnp->dontfrag = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tnp->autoflowlabel = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\n\treturn retv;\n\ne_inval:\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\treturn -EINVAL;\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstruct ipv6_txoptions *\nipv6_dup_options(struct sock *sk, struct ipv6_txoptions *opt)\n{\n\tstruct ipv6_txoptions *opt2;\n\n\topt2 = sock_kmalloc(sk, opt->tot_len, GFP_ATOMIC);\n\tif (opt2) {\n\t\tlong dif = (char *)opt2 - (char *)opt;\n\t\tmemcpy(opt2, opt, opt->tot_len);\n\t\tif (opt2->hopopt)\n\t\t\t*((char **)&opt2->hopopt) += dif;\n\t\tif (opt2->dst0opt)\n\t\t\t*((char **)&opt2->dst0opt) += dif;\n\t\tif (opt2->dst1opt)\n\t\t\t*((char **)&opt2->dst1opt) += dif;\n\t\tif (opt2->srcrt)\n\t\t\t*((char **)&opt2->srcrt) += dif;\n\t\tatomic_set(&opt2->refcnt, 1);\n\t}\n\treturn opt2;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstruct ipv6_txoptions *ipv6_update_options(struct sock *sk,\n\t\t\t\t\t   struct ipv6_txoptions *opt)\n{\n\tif (inet_sk(sk)->is_icsk) {\n\t\tif (opt &&\n\t\t    !((1 << sk->sk_state) & (TCPF_LISTEN | TCPF_CLOSE)) &&\n\t\t    inet_sk(sk)->inet_daddr != LOOPBACK4_IPV6) {\n\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;\n\t\t\ticsk->icsk_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t}\n\t}\n\topt = xchg(&inet6_sk(sk)->opt, opt);\n\tsk_dst_reset(sk);\n\n\treturn opt;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, valbool;\n\tint retv = -ENOPROTOOPT;\n\tbool needs_rtnl = setsockopt_needs_rtnl(optname);\n\n\tif (!optval)\n\t\tval = 0;\n\telse {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (get_user(val, (int __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\tval = 0;\n\t}\n\n\tvalbool = (val != 0);\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_setsockopt(sk, optname, optval, optlen);\n\n\tif (needs_rtnl)\n\t\trtnl_lock();\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\n\tcase IPV6_ADDRFORM:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val == PF_INET) {\n\t\t\tstruct ipv6_txoptions *opt;\n\t\t\tstruct sk_buff *pktopt;\n\n\t\t\tif (sk->sk_type == SOCK_RAW)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_protocol == IPPROTO_UDP ||\n\t\t\t    sk->sk_protocol == IPPROTO_UDPLITE) {\n\t\t\t\tstruct udp_sock *up = udp_sk(sk);\n\t\t\t\tif (up->pending == AF_INET6) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (sk->sk_protocol != IPPROTO_TCP)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t\tretv = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (ipv6_only_sock(sk) ||\n\t\t\t    !ipv6_addr_v4mapped(&sk->sk_v6_daddr)) {\n\t\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tfl6_free_socklist(sk);\n\t\t\tipv6_sock_mc_close(sk);\n\n\t\t\t/*\n\t\t\t * Sock is moving from IPv6 to IPv4 (sk_prot), so\n\t\t\t * remove it from the refcnt debug socks count in the\n\t\t\t * original family...\n\t\t\t */\n\t\t\tsk_refcnt_debug_dec(sk);\n\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = &tcp_prot;\n\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;\n\t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t\t} else {\n\t\t\t\tstruct proto *prot = &udp_prot;\n\n\t\t\t\tif (sk->sk_protocol == IPPROTO_UDPLITE)\n\t\t\t\t\tprot = &udplite_prot;\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = prot;\n\t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t}\n\t\t\topt = xchg((__force struct ipv6_txoptions **)&np->opt,\n\t\t\t\t   NULL);\n\t\t\tif (opt) {\n\t\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\t\ttxopt_put(opt);\n\t\t\t}\n\t\t\tpktopt = xchg(&np->pktoptions, NULL);\n\t\t\tkfree_skb(pktopt);\n\n\t\t\tsk->sk_destruct = inet_sock_destruct;\n\t\t\t/*\n\t\t\t * ... and add it to the refcnt debug socks count\n\t\t\t * in the new family. -acme\n\t\t\t */\n\t\t\tsk_refcnt_debug_inc(sk);\n\t\t\tmodule_put(THIS_MODULE);\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\t\tgoto e_inval;\n\n\tcase IPV6_V6ONLY:\n\t\tif (optlen < sizeof(int) ||\n\t\t    inet_sk(sk)->inet_num)\n\t\t\tgoto e_inval;\n\t\tsk->sk_ipv6only = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxoinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxhlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxohlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.srcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.osrcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.hopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.ohopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.dstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.odstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < -1 || val > 0xff)\n\t\t\tgoto e_inval;\n\t\t/* RFC 3542, 6.5: default traffic class of 0x0 */\n\t\tif (val == -1)\n\t\t\tval = 0;\n\t\tnp->tclass = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxtclass = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxpmtu = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TRANSPARENT:\n\t\tif (valbool && !ns_capable(net->user_ns, CAP_NET_ADMIN) &&\n\t\t    !ns_capable(net->user_ns, CAP_NET_RAW)) {\n\t\t\tretv = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we don't have a separate transparent bit for IPV6 we use the one in the IPv4 socket */\n\t\tinet_sk(sk)->transparent = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxorigdstaddr = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\t/* remove any sticky options header with a zero option\n\t\t * length, per RFC3542.\n\t\t */\n\t\tif (optlen == 0)\n\t\t\toptval = NULL;\n\t\telse if (!optval)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct ipv6_opt_hdr) ||\n\t\t\t optlen & 0x7 || optlen > 8 * 255)\n\t\t\tgoto e_inval;\n\n\t\t/* hop-by-hop / destination options are privileged option */\n\t\tretv = -EPERM;\n\t\tif (optname != IPV6_RTHDR && !ns_capable(net->user_ns, CAP_NET_RAW))\n\t\t\tbreak;\n\n\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\t\topt = ipv6_renew_options(sk, opt, optname,\n\t\t\t\t\t (struct ipv6_opt_hdr __user *)optval,\n\t\t\t\t\t optlen);\n\t\tif (IS_ERR(opt)) {\n\t\t\tretv = PTR_ERR(opt);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* routing header option needs extra check */\n\t\tretv = -EINVAL;\n\t\tif (optname == IPV6_RTHDR && opt && opt->srcrt) {\n\t\t\tstruct ipv6_rt_hdr *rthdr = opt->srcrt;\n\t\t\tswitch (rthdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\t\tcase IPV6_SRCRT_TYPE_2:\n\t\t\t\tif (rthdr->hdrlen != 2 ||\n\t\t\t\t    rthdr->segments_left != 1)\n\t\t\t\t\tgoto sticky_done;\n\n\t\t\t\tbreak;\n#endif\n\t\t\tdefault:\n\t\t\t\tgoto sticky_done;\n\t\t\t}\n\t\t}\n\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\nsticky_done:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase IPV6_PKTINFO:\n\t{\n\t\tstruct in6_pktinfo pkt;\n\n\t\tif (optlen == 0)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct in6_pktinfo) || !optval)\n\t\t\tgoto e_inval;\n\n\t\tif (copy_from_user(&pkt, optval, sizeof(struct in6_pktinfo))) {\n\t\t\t\tretv = -EFAULT;\n\t\t\t\tbreak;\n\t\t}\n\t\tif (sk->sk_bound_dev_if && pkt.ipi6_ifindex != sk->sk_bound_dev_if)\n\t\t\tgoto e_inval;\n\n\t\tnp->sticky_pktinfo.ipi6_ifindex = pkt.ipi6_ifindex;\n\t\tnp->sticky_pktinfo.ipi6_addr = pkt.ipi6_addr;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct ipv6_txoptions *opt = NULL;\n\t\tstruct msghdr msg;\n\t\tstruct flowi6 fl6;\n\t\tint junk;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\n\t\tif (optlen == 0)\n\t\t\tgoto update;\n\n\t\t/* 1K is probably excessive\n\t\t * 1K is surely not enough, 2K per standard header is 16K.\n\t\t */\n\t\tretv = -EINVAL;\n\t\tif (optlen > 64*1024)\n\t\t\tbreak;\n\n\t\topt = sock_kmalloc(sk, sizeof(*opt) + optlen, GFP_KERNEL);\n\t\tretv = -ENOBUFS;\n\t\tif (!opt)\n\t\t\tbreak;\n\n\t\tmemset(opt, 0, sizeof(*opt));\n\t\tatomic_set(&opt->refcnt, 1);\n\t\topt->tot_len = sizeof(*opt) + optlen;\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(opt+1, optval, optlen))\n\t\t\tgoto done;\n\n\t\tmsg.msg_controllen = optlen;\n\t\tmsg.msg_control = (void *)(opt+1);\n\n\t\tretv = ip6_datagram_send_ctl(net, sk, &msg, &fl6, opt, &junk,\n\t\t\t\t\t     &junk, &junk);\n\t\tif (retv)\n\t\t\tgoto done;\nupdate:\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\ndone:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\tcase IPV6_UNICAST_HOPS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->hop_limit = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_HOPS:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->mcast_hops = (val == -1 ? IPV6_DEFAULT_MCASTHOPS : val);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val != valbool)\n\t\t\tgoto e_inval;\n\t\tnp->mc_loop = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t{\n\t\tstruct net_device *dev = NULL;\n\t\tint ifindex;\n\n\t\tif (optlen != sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tifindex = (__force int)ntohl((__force __be32)val);\n\t\tif (ifindex == 0) {\n\t\t\tnp->ucast_oif = 0;\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tdev = dev_get_by_index(net, ifindex);\n\t\tretv = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\tretv = -EINVAL;\n\t\tif (sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tnp->ucast_oif = ifindex;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_IF:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tif (val) {\n\t\t\tstruct net_device *dev;\n\n\t\t\tif (sk->sk_bound_dev_if && sk->sk_bound_dev_if != val)\n\t\t\t\tgoto e_inval;\n\n\t\t\tdev = dev_get_by_index(net, val);\n\t\t\tif (!dev) {\n\t\t\t\tretv = -ENODEV;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdev_put(dev);\n\t\t}\n\t\tnp->mcast_oif = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_ADD_MEMBERSHIP)\n\t\t\tretv = ipv6_sock_mc_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_JOIN_ANYCAST)\n\t\t\tretv = ipv6_sock_ac_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_ac_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t{\n\t\tstruct group_req greq;\n\t\tstruct sockaddr_in6 *psin6;\n\n\t\tif (optlen < sizeof(struct group_req))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&greq, optval, sizeof(struct group_req)))\n\t\t\tbreak;\n\t\tif (greq.gr_group.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tpsin6 = (struct sockaddr_in6 *)&greq.gr_group;\n\t\tif (optname == MCAST_JOIN_GROUP)\n\t\t\tretv = ipv6_sock_mc_join(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t{\n\t\tstruct group_source_req greqs;\n\t\tint omode, add;\n\n\t\tif (optlen < sizeof(struct group_source_req))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&greqs, optval, sizeof(greqs))) {\n\t\t\tretv = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (greqs.gsr_group.ss_family != AF_INET6 ||\n\t\t    greqs.gsr_source.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tif (optname == MCAST_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == MCAST_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == MCAST_JOIN_SOURCE_GROUP) {\n\t\t\tstruct sockaddr_in6 *psin6;\n\n\t\t\tpsin6 = (struct sockaddr_in6 *)&greqs.gsr_group;\n\t\t\tretv = ipv6_sock_mc_join(sk, greqs.gsr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\t\t/* prior join w/ different source is ok */\n\t\t\tif (retv && retv != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* MCAST_LEAVE_SOURCE_GROUP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\tretv = ip6_mc_source(add, omode, sk, &greqs);\n\t\tbreak;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter *gsf;\n\n\t\tif (optlen < GROUP_FILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tgsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!gsf) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(gsf, optval, optlen)) {\n\t\t\tkfree(gsf);\n\t\t\tbreak;\n\t\t}\n\t\t/* numsrc >= (4G-140)/128 overflow in 32 bits */\n\t\tif (gsf->gf_numsrc >= 0x1ffffffU ||\n\t\t    gsf->gf_numsrc > sysctl_mld_max_msf) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tif (GROUP_FILTER_SIZE(gsf->gf_numsrc) > optlen) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tretv = ip6_mc_msfilter(sk, gsf);\n\t\tkfree(gsf);\n\n\t\tbreak;\n\t}\n\tcase IPV6_ROUTER_ALERT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = ip6_ra_control(sk, val);\n\t\tbreak;\n\tcase IPV6_MTU_DISCOVER:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < IPV6_PMTUDISC_DONT || val > IPV6_PMTUDISC_OMIT)\n\t\t\tgoto e_inval;\n\t\tnp->pmtudisc = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val && val < IPV6_MIN_MTU)\n\t\t\tgoto e_inval;\n\t\tnp->frag_size = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->recverr = valbool;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWINFO_SEND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->sndflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWLABEL_MGR:\n\t\tretv = ipv6_flowlabel_opt(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_IPSEC_POLICY:\n\tcase IPV6_XFRM_POLICY:\n\t\tretv = -EPERM;\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\tretv = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t    {\n\t\tunsigned int pref = 0;\n\t\tunsigned int prefmask = ~0;\n\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EINVAL;\n\n\t\t/* check PUBLIC/TMP/PUBTMP_DEFAULT conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_PUBLIC|\n\t\t\t       IPV6_PREFER_SRC_TMP|\n\t\t\t       IPV6_PREFER_SRC_PUBTMP_DEFAULT)) {\n\t\tcase IPV6_PREFER_SRC_PUBLIC:\n\t\t\tpref |= IPV6_PREFER_SRC_PUBLIC;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_TMP:\n\t\t\tpref |= IPV6_PREFER_SRC_TMP;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_PUBTMP_DEFAULT:\n\t\t\tbreak;\n\t\tcase 0:\n\t\t\tgoto pref_skip_pubtmp;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~(IPV6_PREFER_SRC_PUBLIC|\n\t\t\t      IPV6_PREFER_SRC_TMP);\npref_skip_pubtmp:\n\n\t\t/* check HOME/COA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_HOME|IPV6_PREFER_SRC_COA)) {\n\t\tcase IPV6_PREFER_SRC_HOME:\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_COA:\n\t\t\tpref |= IPV6_PREFER_SRC_COA;\n\t\tcase 0:\n\t\t\tgoto pref_skip_coa;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~IPV6_PREFER_SRC_COA;\npref_skip_coa:\n\n\t\t/* check CGA/NONCGA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_CGA|IPV6_PREFER_SRC_NONCGA)) {\n\t\tcase IPV6_PREFER_SRC_CGA:\n\t\tcase IPV6_PREFER_SRC_NONCGA:\n\t\tcase 0:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tnp->srcprefs = (np->srcprefs & prefmask) | pref;\n\t\tretv = 0;\n\n\t\tbreak;\n\t    }\n\tcase IPV6_MINHOPCOUNT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tnp->min_hopcount = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_DONTFRAG:\n\t\tnp->dontfrag = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tnp->autoflowlabel = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\n\treturn retv;\n\ne_inval:\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\treturn -EINVAL;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 1004,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_options_received tcp_opt;\n\tstruct inet_request_sock *ireq;\n\tstruct tcp_request_sock *treq;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\t__u32 cookie = ntohl(th->ack_seq) - 1;\n\tstruct sock *ret = sk;\n\tstruct request_sock *req;\n\tint mss;\n\tstruct dst_entry *dst;\n\t__u8 rcv_wscale;\n\n\tif (!sysctl_tcp_syncookies || !th->ack || th->rst)\n\t\tgoto out;\n\n\tif (tcp_synq_no_recent_overflow(sk))\n\t\tgoto out;\n\n\tmss = __cookie_v6_check(ipv6_hdr(skb), th, cookie);\n\tif (mss == 0) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESFAILED);\n\t\tgoto out;\n\t}\n\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESRECV);\n\n\t/* check for timestamp cookie support */\n\tmemset(&tcp_opt, 0, sizeof(tcp_opt));\n\ttcp_parse_options(skb, &tcp_opt, 0, NULL);\n\n\tif (!cookie_timestamp_decode(&tcp_opt))\n\t\tgoto out;\n\n\tret = NULL;\n\treq = inet_reqsk_alloc(&tcp6_request_sock_ops, sk, false);\n\tif (!req)\n\t\tgoto out;\n\n\tireq = inet_rsk(req);\n\ttreq = tcp_rsk(req);\n\ttreq->tfo_listener = false;\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\tgoto out_free;\n\n\treq->mss = mss;\n\tireq->ir_rmt_port = th->source;\n\tireq->ir_num = ntohs(th->dest);\n\tireq->ir_v6_rmt_addr = ipv6_hdr(skb)->saddr;\n\tireq->ir_v6_loc_addr = ipv6_hdr(skb)->daddr;\n\tif (ipv6_opt_accepted(sk, skb, &TCP_SKB_CB(skb)->header.h6) ||\n\t    np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo ||\n\t    np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) {\n\t\tatomic_inc(&skb->users);\n\t\tireq->pktopts = skb;\n\t}\n\n\tireq->ir_iif = sk->sk_bound_dev_if;\n\t/* So that link locals have meaning */\n\tif (!sk->sk_bound_dev_if &&\n\t    ipv6_addr_type(&ireq->ir_v6_rmt_addr) & IPV6_ADDR_LINKLOCAL)\n\t\tireq->ir_iif = tcp_v6_iif(skb);\n\n\tireq->ir_mark = inet_request_mark(sk, skb);\n\n\treq->num_retrans = 0;\n\tireq->snd_wscale\t= tcp_opt.snd_wscale;\n\tireq->sack_ok\t\t= tcp_opt.sack_ok;\n\tireq->wscale_ok\t\t= tcp_opt.wscale_ok;\n\tireq->tstamp_ok\t\t= tcp_opt.saw_tstamp;\n\treq->ts_recent\t\t= tcp_opt.saw_tstamp ? tcp_opt.rcv_tsval : 0;\n\ttreq->snt_synack.v64\t= 0;\n\ttreq->rcv_isn = ntohl(th->seq) - 1;\n\ttreq->snt_isn = cookie;\n\n\t/*\n\t * We need to lookup the dst_entry to get the correct window size.\n\t * This is taken from tcp_v6_syn_recv_sock.  Somebody please enlighten\n\t * me if there is a preferred way.\n\t */\n\t{\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = IPPROTO_TCP;\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);\n\t\tfl6.saddr = ireq->ir_v6_loc_addr;\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = ireq->ir_mark;\n\t\tfl6.fl6_dport = ireq->ir_rmt_port;\n\t\tfl6.fl6_sport = inet_sk(sk)->inet_sport;\n\t\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\t\tif (IS_ERR(dst))\n\t\t\tgoto out_free;\n\t}\n\n\treq->rsk_window_clamp = tp->window_clamp ? :dst_metric(dst, RTAX_WINDOW);\n\ttcp_select_initial_window(tcp_full_space(sk), req->mss,\n\t\t\t\t  &req->rsk_rcv_wnd, &req->rsk_window_clamp,\n\t\t\t\t  ireq->wscale_ok, &rcv_wscale,\n\t\t\t\t  dst_metric(dst, RTAX_INITRWND));\n\n\tireq->rcv_wscale = rcv_wscale;\n\tireq->ecn_ok = cookie_ecn_ok(&tcp_opt, sock_net(sk), dst);\n\n\tret = tcp_get_cookie_sock(sk, skb, req, dst);\nout:\n\treturn ret;\nout_free:\n\treqsk_free(req);\n\treturn NULL;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dccp_v6_send_response(const struct sock *sk, struct request_sock *req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct in6_addr *final_p, final;\n\tstruct flowi6 fl6;\n\tint err = -1;\n\tstruct dst_entry *dst;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\tfl6.saddr = ireq->ir_v6_loc_addr;\n\tfl6.flowlabel = 0;\n\tfl6.flowi6_oif = ireq->ir_iif;\n\tfl6.fl6_dport = ireq->ir_rmt_port;\n\tfl6.fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto done;\n\t}\n\n\tskb = dccp_make_response(sk, dst, req);\n\tif (skb != NULL) {\n\t\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\t\tdh->dccph_checksum = dccp_v6_csum_finish(skb,\n\t\t\t\t\t\t\t &ireq->ir_v6_loc_addr,\n\t\t\t\t\t\t\t &ireq->ir_v6_rmt_addr);\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\terr = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);\n\t\terr = net_xmit_eval(err);\n\t}\n\ndone:\n\tdst_release(dst);\n\treturn err;\n}",
                        "code_after_change": "static int dccp_v6_send_response(const struct sock *sk, struct request_sock *req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct in6_addr *final_p, final;\n\tstruct flowi6 fl6;\n\tint err = -1;\n\tstruct dst_entry *dst;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\tfl6.saddr = ireq->ir_v6_loc_addr;\n\tfl6.flowlabel = 0;\n\tfl6.flowi6_oif = ireq->ir_iif;\n\tfl6.fl6_dport = ireq->ir_rmt_port;\n\tfl6.fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\n\trcu_read_lock();\n\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);\n\trcu_read_unlock();\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto done;\n\t}\n\n\tskb = dccp_make_response(sk, dst, req);\n\tif (skb != NULL) {\n\t\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\t\tdh->dccph_checksum = dccp_v6_csum_finish(skb,\n\t\t\t\t\t\t\t &ireq->ir_v6_loc_addr,\n\t\t\t\t\t\t\t &ireq->ir_v6_rmt_addr);\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\trcu_read_lock();\n\t\terr = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),\n\t\t\t       np->tclass);\n\t\trcu_read_unlock();\n\t\terr = net_xmit_eval(err);\n\t}\n\ndone:\n\tdst_release(dst);\n\treturn err;\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstruct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_options_received tcp_opt;\n\tstruct inet_request_sock *ireq;\n\tstruct tcp_request_sock *treq;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\t__u32 cookie = ntohl(th->ack_seq) - 1;\n\tstruct sock *ret = sk;\n\tstruct request_sock *req;\n\tint mss;\n\tstruct dst_entry *dst;\n\t__u8 rcv_wscale;\n\n\tif (!sysctl_tcp_syncookies || !th->ack || th->rst)\n\t\tgoto out;\n\n\tif (tcp_synq_no_recent_overflow(sk))\n\t\tgoto out;\n\n\tmss = __cookie_v6_check(ipv6_hdr(skb), th, cookie);\n\tif (mss == 0) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESFAILED);\n\t\tgoto out;\n\t}\n\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESRECV);\n\n\t/* check for timestamp cookie support */\n\tmemset(&tcp_opt, 0, sizeof(tcp_opt));\n\ttcp_parse_options(skb, &tcp_opt, 0, NULL);\n\n\tif (!cookie_timestamp_decode(&tcp_opt))\n\t\tgoto out;\n\n\tret = NULL;\n\treq = inet_reqsk_alloc(&tcp6_request_sock_ops, sk, false);\n\tif (!req)\n\t\tgoto out;\n\n\tireq = inet_rsk(req);\n\ttreq = tcp_rsk(req);\n\ttreq->tfo_listener = false;\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\tgoto out_free;\n\n\treq->mss = mss;\n\tireq->ir_rmt_port = th->source;\n\tireq->ir_num = ntohs(th->dest);\n\tireq->ir_v6_rmt_addr = ipv6_hdr(skb)->saddr;\n\tireq->ir_v6_loc_addr = ipv6_hdr(skb)->daddr;\n\tif (ipv6_opt_accepted(sk, skb, &TCP_SKB_CB(skb)->header.h6) ||\n\t    np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo ||\n\t    np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) {\n\t\tatomic_inc(&skb->users);\n\t\tireq->pktopts = skb;\n\t}\n\n\tireq->ir_iif = sk->sk_bound_dev_if;\n\t/* So that link locals have meaning */\n\tif (!sk->sk_bound_dev_if &&\n\t    ipv6_addr_type(&ireq->ir_v6_rmt_addr) & IPV6_ADDR_LINKLOCAL)\n\t\tireq->ir_iif = tcp_v6_iif(skb);\n\n\tireq->ir_mark = inet_request_mark(sk, skb);\n\n\treq->num_retrans = 0;\n\tireq->snd_wscale\t= tcp_opt.snd_wscale;\n\tireq->sack_ok\t\t= tcp_opt.sack_ok;\n\tireq->wscale_ok\t\t= tcp_opt.wscale_ok;\n\tireq->tstamp_ok\t\t= tcp_opt.saw_tstamp;\n\treq->ts_recent\t\t= tcp_opt.saw_tstamp ? tcp_opt.rcv_tsval : 0;\n\ttreq->snt_synack.v64\t= 0;\n\ttreq->rcv_isn = ntohl(th->seq) - 1;\n\ttreq->snt_isn = cookie;\n\n\t/*\n\t * We need to lookup the dst_entry to get the correct window size.\n\t * This is taken from tcp_v6_syn_recv_sock.  Somebody please enlighten\n\t * me if there is a preferred way.\n\t */\n\t{\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = IPPROTO_TCP;\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);\n\t\tfl6.saddr = ireq->ir_v6_loc_addr;\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = ireq->ir_mark;\n\t\tfl6.fl6_dport = ireq->ir_rmt_port;\n\t\tfl6.fl6_sport = inet_sk(sk)->inet_sport;\n\t\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\t\tif (IS_ERR(dst))\n\t\t\tgoto out_free;\n\t}\n\n\treq->rsk_window_clamp = tp->window_clamp ? :dst_metric(dst, RTAX_WINDOW);\n\ttcp_select_initial_window(tcp_full_space(sk), req->mss,\n\t\t\t\t  &req->rsk_rcv_wnd, &req->rsk_window_clamp,\n\t\t\t\t  ireq->wscale_ok, &rcv_wscale,\n\t\t\t\t  dst_metric(dst, RTAX_INITRWND));\n\n\tireq->rcv_wscale = rcv_wscale;\n\tireq->ecn_ok = cookie_ecn_ok(&tcp_opt, sock_net(sk), dst);\n\n\tret = tcp_get_cookie_sock(sk, skb, req, dst);\nout:\n\treturn ret;\nout_free:\n\treqsk_free(req);\n\treturn NULL;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dccp_v6_send_response(const struct sock *sk, struct request_sock *req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct in6_addr *final_p, final;\n\tstruct flowi6 fl6;\n\tint err = -1;\n\tstruct dst_entry *dst;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\tfl6.saddr = ireq->ir_v6_loc_addr;\n\tfl6.flowlabel = 0;\n\tfl6.flowi6_oif = ireq->ir_iif;\n\tfl6.fl6_dport = ireq->ir_rmt_port;\n\tfl6.fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto done;\n\t}\n\n\tskb = dccp_make_response(sk, dst, req);\n\tif (skb != NULL) {\n\t\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\t\tdh->dccph_checksum = dccp_v6_csum_finish(skb,\n\t\t\t\t\t\t\t &ireq->ir_v6_loc_addr,\n\t\t\t\t\t\t\t &ireq->ir_v6_rmt_addr);\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\terr = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);\n\t\terr = net_xmit_eval(err);\n\t}\n\ndone:\n\tdst_release(dst);\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dccp_v6_send_response(const struct sock *sk, struct request_sock *req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct in6_addr *final_p, final;\n\tstruct flowi6 fl6;\n\tint err = -1;\n\tstruct dst_entry *dst;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\tfl6.saddr = ireq->ir_v6_loc_addr;\n\tfl6.flowlabel = 0;\n\tfl6.flowi6_oif = ireq->ir_iif;\n\tfl6.fl6_dport = ireq->ir_rmt_port;\n\tfl6.fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\n\trcu_read_lock();\n\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);\n\trcu_read_unlock();\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto done;\n\t}\n\n\tskb = dccp_make_response(sk, dst, req);\n\tif (skb != NULL) {\n\t\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\t\tdh->dccph_checksum = dccp_v6_csum_finish(skb,\n\t\t\t\t\t\t\t &ireq->ir_v6_loc_addr,\n\t\t\t\t\t\t\t &ireq->ir_v6_rmt_addr);\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\trcu_read_lock();\n\t\terr = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),\n\t\t\t       np->tclass);\n\t\trcu_read_unlock();\n\t\terr = net_xmit_eval(err);\n\t}\n\ndone:\n\tdst_release(dst);\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "struct dst_entry *inet6_csk_route_req(const struct sock *sk,\n\t\t\t\t      struct flowi6 *fl6,\n\t\t\t\t      const struct request_sock *req,\n\t\t\t\t      u8 proto)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = proto;\n\tfl6->daddr = ireq->ir_v6_rmt_addr;\n\tfinal_p = fl6_update_dst(fl6, np->opt, &final);\n\tfl6->saddr = ireq->ir_v6_loc_addr;\n\tfl6->flowi6_oif = ireq->ir_iif;\n\tfl6->flowi6_mark = ireq->ir_mark;\n\tfl6->fl6_dport = ireq->ir_rmt_port;\n\tfl6->fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, fl6, final_p);\n\tif (IS_ERR(dst))\n\t\treturn NULL;\n\n\treturn dst;\n}",
                        "code_after_change": "struct dst_entry *inet6_csk_route_req(const struct sock *sk,\n\t\t\t\t      struct flowi6 *fl6,\n\t\t\t\t      const struct request_sock *req,\n\t\t\t\t      u8 proto)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = proto;\n\tfl6->daddr = ireq->ir_v6_rmt_addr;\n\trcu_read_lock();\n\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);\n\trcu_read_unlock();\n\tfl6->saddr = ireq->ir_v6_loc_addr;\n\tfl6->flowi6_oif = ireq->ir_iif;\n\tfl6->flowi6_mark = ireq->ir_mark;\n\tfl6->fl6_dport = ireq->ir_rmt_port;\n\tfl6->fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, fl6, final_p);\n\tif (IS_ERR(dst))\n\t\treturn NULL;\n\n\treturn dst;\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstruct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_options_received tcp_opt;\n\tstruct inet_request_sock *ireq;\n\tstruct tcp_request_sock *treq;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\t__u32 cookie = ntohl(th->ack_seq) - 1;\n\tstruct sock *ret = sk;\n\tstruct request_sock *req;\n\tint mss;\n\tstruct dst_entry *dst;\n\t__u8 rcv_wscale;\n\n\tif (!sysctl_tcp_syncookies || !th->ack || th->rst)\n\t\tgoto out;\n\n\tif (tcp_synq_no_recent_overflow(sk))\n\t\tgoto out;\n\n\tmss = __cookie_v6_check(ipv6_hdr(skb), th, cookie);\n\tif (mss == 0) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESFAILED);\n\t\tgoto out;\n\t}\n\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESRECV);\n\n\t/* check for timestamp cookie support */\n\tmemset(&tcp_opt, 0, sizeof(tcp_opt));\n\ttcp_parse_options(skb, &tcp_opt, 0, NULL);\n\n\tif (!cookie_timestamp_decode(&tcp_opt))\n\t\tgoto out;\n\n\tret = NULL;\n\treq = inet_reqsk_alloc(&tcp6_request_sock_ops, sk, false);\n\tif (!req)\n\t\tgoto out;\n\n\tireq = inet_rsk(req);\n\ttreq = tcp_rsk(req);\n\ttreq->tfo_listener = false;\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\tgoto out_free;\n\n\treq->mss = mss;\n\tireq->ir_rmt_port = th->source;\n\tireq->ir_num = ntohs(th->dest);\n\tireq->ir_v6_rmt_addr = ipv6_hdr(skb)->saddr;\n\tireq->ir_v6_loc_addr = ipv6_hdr(skb)->daddr;\n\tif (ipv6_opt_accepted(sk, skb, &TCP_SKB_CB(skb)->header.h6) ||\n\t    np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo ||\n\t    np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) {\n\t\tatomic_inc(&skb->users);\n\t\tireq->pktopts = skb;\n\t}\n\n\tireq->ir_iif = sk->sk_bound_dev_if;\n\t/* So that link locals have meaning */\n\tif (!sk->sk_bound_dev_if &&\n\t    ipv6_addr_type(&ireq->ir_v6_rmt_addr) & IPV6_ADDR_LINKLOCAL)\n\t\tireq->ir_iif = tcp_v6_iif(skb);\n\n\tireq->ir_mark = inet_request_mark(sk, skb);\n\n\treq->num_retrans = 0;\n\tireq->snd_wscale\t= tcp_opt.snd_wscale;\n\tireq->sack_ok\t\t= tcp_opt.sack_ok;\n\tireq->wscale_ok\t\t= tcp_opt.wscale_ok;\n\tireq->tstamp_ok\t\t= tcp_opt.saw_tstamp;\n\treq->ts_recent\t\t= tcp_opt.saw_tstamp ? tcp_opt.rcv_tsval : 0;\n\ttreq->snt_synack.v64\t= 0;\n\ttreq->rcv_isn = ntohl(th->seq) - 1;\n\ttreq->snt_isn = cookie;\n\n\t/*\n\t * We need to lookup the dst_entry to get the correct window size.\n\t * This is taken from tcp_v6_syn_recv_sock.  Somebody please enlighten\n\t * me if there is a preferred way.\n\t */\n\t{\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = IPPROTO_TCP;\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);\n\t\tfl6.saddr = ireq->ir_v6_loc_addr;\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = ireq->ir_mark;\n\t\tfl6.fl6_dport = ireq->ir_rmt_port;\n\t\tfl6.fl6_sport = inet_sk(sk)->inet_sport;\n\t\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\t\tif (IS_ERR(dst))\n\t\t\tgoto out_free;\n\t}\n\n\treq->rsk_window_clamp = tp->window_clamp ? :dst_metric(dst, RTAX_WINDOW);\n\ttcp_select_initial_window(tcp_full_space(sk), req->mss,\n\t\t\t\t  &req->rsk_rcv_wnd, &req->rsk_window_clamp,\n\t\t\t\t  ireq->wscale_ok, &rcv_wscale,\n\t\t\t\t  dst_metric(dst, RTAX_INITRWND));\n\n\tireq->rcv_wscale = rcv_wscale;\n\tireq->ecn_ok = cookie_ecn_ok(&tcp_opt, sock_net(sk), dst);\n\n\tret = tcp_get_cookie_sock(sk, skb, req, dst);\nout:\n\treturn ret;\nout_free:\n\treqsk_free(req);\n\treturn NULL;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstruct dst_entry *inet6_csk_route_req(const struct sock *sk,\n\t\t\t\t      struct flowi6 *fl6,\n\t\t\t\t      const struct request_sock *req,\n\t\t\t\t      u8 proto)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = proto;\n\tfl6->daddr = ireq->ir_v6_rmt_addr;\n\tfinal_p = fl6_update_dst(fl6, np->opt, &final);\n\tfl6->saddr = ireq->ir_v6_loc_addr;\n\tfl6->flowi6_oif = ireq->ir_iif;\n\tfl6->flowi6_mark = ireq->ir_mark;\n\tfl6->fl6_dport = ireq->ir_rmt_port;\n\tfl6->fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, fl6, final_p);\n\tif (IS_ERR(dst))\n\t\treturn NULL;\n\n\treturn dst;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstruct dst_entry *inet6_csk_route_req(const struct sock *sk,\n\t\t\t\t      struct flowi6 *fl6,\n\t\t\t\t      const struct request_sock *req,\n\t\t\t\t      u8 proto)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = proto;\n\tfl6->daddr = ireq->ir_v6_rmt_addr;\n\trcu_read_lock();\n\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);\n\trcu_read_unlock();\n\tfl6->saddr = ireq->ir_v6_loc_addr;\n\tfl6->flowi6_oif = ireq->ir_iif;\n\tfl6->flowi6_mark = ireq->ir_mark;\n\tfl6->fl6_dport = ireq->ir_rmt_port;\n\tfl6->fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, fl6, final_p);\n\tif (IS_ERR(dst))\n\t\treturn NULL;\n\n\treturn dst;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "YES"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 1
        },
        {
            "id": 1121,
            "cve_id": "CVE-2016-8632",
            "code_snippet": "int tipc_enable_l2_media(struct net *net, struct tipc_bearer *b,\n\t\t\t struct nlattr *attr[])\n{\n\tstruct net_device *dev;\n\tchar *driver_name = strchr((const char *)b->name, ':') + 1;\n\n\t/* Find device with specified name */\n\tdev = dev_get_by_name(net, driver_name);\n\tif (!dev)\n\t\treturn -ENODEV;\n\tif (tipc_mtu_bad(dev, 0)) {\n\t\tdev_put(dev);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Associate TIPC bearer with L2 bearer */\n\trcu_assign_pointer(b->media_ptr, dev);\n\tmemset(&b->bcast_addr, 0, sizeof(b->bcast_addr));\n\tmemcpy(b->bcast_addr.value, dev->broadcast, b->media->hwaddr_len);\n\tb->bcast_addr.media_id = b->media->type_id;\n\tb->bcast_addr.broadcast = 1;\n\tb->mtu = dev->mtu;\n\tb->media->raw2addr(b, &b->addr, (char *)dev->dev_addr);\n\trcu_assign_pointer(dev->tipc_ptr, b);\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tipc_udp_enable(struct net *net, struct tipc_bearer *b,\n\t\t\t   struct nlattr *attrs[])\n{\n\tint err = -EINVAL;\n\tstruct udp_bearer *ub;\n\tstruct udp_media_addr remote = {0};\n\tstruct udp_media_addr local = {0};\n\tstruct udp_port_cfg udp_conf = {0};\n\tstruct udp_tunnel_sock_cfg tuncfg = {NULL};\n\tstruct nlattr *opts[TIPC_NLA_UDP_MAX + 1];\n\n\tub = kzalloc(sizeof(*ub), GFP_ATOMIC);\n\tif (!ub)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&ub->rcast.list);\n\n\tif (!attrs[TIPC_NLA_BEARER_UDP_OPTS])\n\t\tgoto err;\n\n\tif (nla_parse_nested(opts, TIPC_NLA_UDP_MAX,\n\t\t\t     attrs[TIPC_NLA_BEARER_UDP_OPTS],\n\t\t\t     tipc_nl_udp_policy))\n\t\tgoto err;\n\n\tif (!opts[TIPC_NLA_UDP_LOCAL] || !opts[TIPC_NLA_UDP_REMOTE]) {\n\t\tpr_err(\"Invalid UDP bearer configuration\");\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\terr = tipc_parse_udp_addr(opts[TIPC_NLA_UDP_LOCAL], &local,\n\t\t\t\t  &ub->ifindex);\n\tif (err)\n\t\tgoto err;\n\n\terr = tipc_parse_udp_addr(opts[TIPC_NLA_UDP_REMOTE], &remote, NULL);\n\tif (err)\n\t\tgoto err;\n\n\tb->bcast_addr.media_id = TIPC_MEDIA_TYPE_UDP;\n\tb->bcast_addr.broadcast = 1;\n\trcu_assign_pointer(b->media_ptr, ub);\n\trcu_assign_pointer(ub->bearer, b);\n\ttipc_udp_media_addr_set(&b->addr, &local);\n\tif (local.proto == htons(ETH_P_IP)) {\n\t\tstruct net_device *dev;\n\n\t\tdev = __ip_dev_find(net, local.ipv4.s_addr, false);\n\t\tif (!dev) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto err;\n\t\t}\n\t\tudp_conf.family = AF_INET;\n\t\tudp_conf.local_ip.s_addr = htonl(INADDR_ANY);\n\t\tudp_conf.use_udp_checksums = false;\n\t\tub->ifindex = dev->ifindex;\n\t\tb->mtu = dev->mtu - sizeof(struct iphdr)\n\t\t\t- sizeof(struct udphdr);\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else if (local.proto == htons(ETH_P_IPV6)) {\n\t\tudp_conf.family = AF_INET6;\n\t\tudp_conf.use_udp6_tx_checksums = true;\n\t\tudp_conf.use_udp6_rx_checksums = true;\n\t\tudp_conf.local_ip6 = in6addr_any;\n\t\tb->mtu = 1280;\n#endif\n\t} else {\n\t\terr = -EAFNOSUPPORT;\n\t\tgoto err;\n\t}\n\tudp_conf.local_udp_port = local.port;\n\terr = udp_sock_create(net, &udp_conf, &ub->ubsock);\n\tif (err)\n\t\tgoto err;\n\ttuncfg.sk_user_data = ub;\n\ttuncfg.encap_type = 1;\n\ttuncfg.encap_rcv = tipc_udp_recv;\n\ttuncfg.encap_destroy = NULL;\n\tsetup_udp_tunnel_sock(net, ub->ubsock, &tuncfg);\n\n\t/**\n\t * The bcast media address port is used for all peers and the ip\n\t * is used if it's a multicast address.\n\t */\n\tmemcpy(&b->bcast_addr.value, &remote, sizeof(remote));\n\tif (tipc_udp_is_mcast_addr(&remote))\n\t\terr = enable_mcast(ub, &remote);\n\telse\n\t\terr = tipc_udp_rcast_add(b, &remote);\n\tif (err)\n\t\tgoto err;\n\n\treturn 0;\nerr:\n\tif (ub->ubsock)\n\t\tudp_tunnel_sock_release(ub->ubsock);\n\tkfree(ub);\n\treturn err;\n}",
                        "code_after_change": "static int tipc_l2_device_event(struct notifier_block *nb, unsigned long evt,\n\t\t\t\tvoid *ptr)\n{\n\tstruct net_device *dev = netdev_notifier_info_to_dev(ptr);\n\tstruct net *net = dev_net(dev);\n\tstruct tipc_bearer *b;\n\n\tb = rtnl_dereference(dev->tipc_ptr);\n\tif (!b)\n\t\treturn NOTIFY_DONE;\n\n\tswitch (evt) {\n\tcase NETDEV_CHANGE:\n\t\tif (netif_carrier_ok(dev))\n\t\t\tbreak;\n\tcase NETDEV_UP:\n\t\ttest_and_set_bit_lock(0, &b->up);\n\t\tbreak;\n\tcase NETDEV_GOING_DOWN:\n\t\tclear_bit_unlock(0, &b->up);\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_CHANGEMTU:\n\t\tif (tipc_mtu_bad(dev, 0)) {\n\t\t\tbearer_disable(net, b);\n\t\t\tbreak;\n\t\t}\n\t\tb->mtu = dev->mtu;\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_CHANGEADDR:\n\t\tb->media->raw2addr(b, &b->addr,\n\t\t\t\t   (char *)dev->dev_addr);\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_UNREGISTER:\n\tcase NETDEV_CHANGENAME:\n\t\tbearer_disable(dev_net(dev), b);\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}",
                        "cve_id": "CVE-2016-8632"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nint tipc_enable_l2_media(struct net *net, struct tipc_bearer *b,\n\t\t\t struct nlattr *attr[])\n{\n\tstruct net_device *dev;\n\tchar *driver_name = strchr((const char *)b->name, ':') + 1;\n\n\t/* Find device with specified name */\n\tdev = dev_get_by_name(net, driver_name);\n\tif (!dev)\n\t\treturn -ENODEV;\n\tif (tipc_mtu_bad(dev, 0)) {\n\t\tdev_put(dev);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Associate TIPC bearer with L2 bearer */\n\trcu_assign_pointer(b->media_ptr, dev);\n\tmemset(&b->bcast_addr, 0, sizeof(b->bcast_addr));\n\tmemcpy(b->bcast_addr.value, dev->broadcast, b->media->hwaddr_len);\n\tb->bcast_addr.media_id = b->media->type_id;\n\tb->bcast_addr.broadcast = 1;\n\tb->mtu = dev->mtu;\n\tb->media->raw2addr(b, &b->addr, (char *)dev->dev_addr);\n\trcu_assign_pointer(dev->tipc_ptr, b);\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int tipc_udp_enable(struct net *net, struct tipc_bearer *b,\n\t\t\t   struct nlattr *attrs[])\n{\n\tint err = -EINVAL;\n\tstruct udp_bearer *ub;\n\tstruct udp_media_addr remote = {0};\n\tstruct udp_media_addr local = {0};\n\tstruct udp_port_cfg udp_conf = {0};\n\tstruct udp_tunnel_sock_cfg tuncfg = {NULL};\n\tstruct nlattr *opts[TIPC_NLA_UDP_MAX + 1];\n\n\tub = kzalloc(sizeof(*ub), GFP_ATOMIC);\n\tif (!ub)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&ub->rcast.list);\n\n\tif (!attrs[TIPC_NLA_BEARER_UDP_OPTS])\n\t\tgoto err;\n\n\tif (nla_parse_nested(opts, TIPC_NLA_UDP_MAX,\n\t\t\t     attrs[TIPC_NLA_BEARER_UDP_OPTS],\n\t\t\t     tipc_nl_udp_policy))\n\t\tgoto err;\n\n\tif (!opts[TIPC_NLA_UDP_LOCAL] || !opts[TIPC_NLA_UDP_REMOTE]) {\n\t\tpr_err(\"Invalid UDP bearer configuration\");\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\terr = tipc_parse_udp_addr(opts[TIPC_NLA_UDP_LOCAL], &local,\n\t\t\t\t  &ub->ifindex);\n\tif (err)\n\t\tgoto err;\n\n\terr = tipc_parse_udp_addr(opts[TIPC_NLA_UDP_REMOTE], &remote, NULL);\n\tif (err)\n\t\tgoto err;\n\n\tb->bcast_addr.media_id = TIPC_MEDIA_TYPE_UDP;\n\tb->bcast_addr.broadcast = 1;\n\trcu_assign_pointer(b->media_ptr, ub);\n\trcu_assign_pointer(ub->bearer, b);\n\ttipc_udp_media_addr_set(&b->addr, &local);\n\tif (local.proto == htons(ETH_P_IP)) {\n\t\tstruct net_device *dev;\n\n\t\tdev = __ip_dev_find(net, local.ipv4.s_addr, false);\n\t\tif (!dev) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto err;\n\t\t}\n\t\tudp_conf.family = AF_INET;\n\t\tudp_conf.local_ip.s_addr = htonl(INADDR_ANY);\n\t\tudp_conf.use_udp_checksums = false;\n\t\tub->ifindex = dev->ifindex;\n\t\tb->mtu = dev->mtu - sizeof(struct iphdr)\n\t\t\t- sizeof(struct udphdr);\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else if (local.proto == htons(ETH_P_IPV6)) {\n\t\tudp_conf.family = AF_INET6;\n\t\tudp_conf.use_udp6_tx_checksums = true;\n\t\tudp_conf.use_udp6_rx_checksums = true;\n\t\tudp_conf.local_ip6 = in6addr_any;\n\t\tb->mtu = 1280;\n#endif\n\t} else {\n\t\terr = -EAFNOSUPPORT;\n\t\tgoto err;\n\t}\n\tudp_conf.local_udp_port = local.port;\n\terr = udp_sock_create(net, &udp_conf, &ub->ubsock);\n\tif (err)\n\t\tgoto err;\n\ttuncfg.sk_user_data = ub;\n\ttuncfg.encap_type = 1;\n\ttuncfg.encap_rcv = tipc_udp_recv;\n\ttuncfg.encap_destroy = NULL;\n\tsetup_udp_tunnel_sock(net, ub->ubsock, &tuncfg);\n\n\t/**\n\t * The bcast media address port is used for all peers and the ip\n\t * is used if it's a multicast address.\n\t */\n\tmemcpy(&b->bcast_addr.value, &remote, sizeof(remote));\n\tif (tipc_udp_is_mcast_addr(&remote))\n\t\terr = enable_mcast(ub, &remote);\n\telse\n\t\terr = tipc_udp_rcast_add(b, &remote);\n\tif (err)\n\t\tgoto err;\n\n\treturn 0;\nerr:\n\tif (ub->ubsock)\n\t\tudp_tunnel_sock_release(ub->ubsock);\n\tkfree(ub);\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int tipc_l2_device_event(struct notifier_block *nb, unsigned long evt,\n\t\t\t\tvoid *ptr)\n{\n\tstruct net_device *dev = netdev_notifier_info_to_dev(ptr);\n\tstruct net *net = dev_net(dev);\n\tstruct tipc_bearer *b;\n\n\tb = rtnl_dereference(dev->tipc_ptr);\n\tif (!b)\n\t\treturn NOTIFY_DONE;\n\n\tswitch (evt) {\n\tcase NETDEV_CHANGE:\n\t\tif (netif_carrier_ok(dev))\n\t\t\tbreak;\n\tcase NETDEV_UP:\n\t\ttest_and_set_bit_lock(0, &b->up);\n\t\tbreak;\n\tcase NETDEV_GOING_DOWN:\n\t\tclear_bit_unlock(0, &b->up);\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_CHANGEMTU:\n\t\tif (tipc_mtu_bad(dev, 0)) {\n\t\t\tbearer_disable(net, b);\n\t\t\tbreak;\n\t\t}\n\t\tb->mtu = dev->mtu;\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_CHANGEADDR:\n\t\tb->media->raw2addr(b, &b->addr,\n\t\t\t\t   (char *)dev->dev_addr);\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_UNREGISTER:\n\tcase NETDEV_CHANGENAME:\n\t\tbearer_disable(dev_net(dev), b);\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tipc_l2_device_event(struct notifier_block *nb, unsigned long evt,\n\t\t\t\tvoid *ptr)\n{\n\tstruct net_device *dev = netdev_notifier_info_to_dev(ptr);\n\tstruct net *net = dev_net(dev);\n\tstruct tipc_bearer *b;\n\n\tb = rtnl_dereference(dev->tipc_ptr);\n\tif (!b)\n\t\treturn NOTIFY_DONE;\n\n\tb->mtu = dev->mtu;\n\n\tswitch (evt) {\n\tcase NETDEV_CHANGE:\n\t\tif (netif_carrier_ok(dev))\n\t\t\tbreak;\n\tcase NETDEV_UP:\n\t\ttest_and_set_bit_lock(0, &b->up);\n\t\tbreak;\n\tcase NETDEV_GOING_DOWN:\n\t\tclear_bit_unlock(0, &b->up);\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_CHANGEMTU:\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_CHANGEADDR:\n\t\tb->media->raw2addr(b, &b->addr,\n\t\t\t\t   (char *)dev->dev_addr);\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_UNREGISTER:\n\tcase NETDEV_CHANGENAME:\n\t\tbearer_disable(dev_net(dev), b);\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}",
                        "code_after_change": "static int tipc_udp_enable(struct net *net, struct tipc_bearer *b,\n\t\t\t   struct nlattr *attrs[])\n{\n\tint err = -EINVAL;\n\tstruct udp_bearer *ub;\n\tstruct udp_media_addr remote = {0};\n\tstruct udp_media_addr local = {0};\n\tstruct udp_port_cfg udp_conf = {0};\n\tstruct udp_tunnel_sock_cfg tuncfg = {NULL};\n\tstruct nlattr *opts[TIPC_NLA_UDP_MAX + 1];\n\n\tub = kzalloc(sizeof(*ub), GFP_ATOMIC);\n\tif (!ub)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&ub->rcast.list);\n\n\tif (!attrs[TIPC_NLA_BEARER_UDP_OPTS])\n\t\tgoto err;\n\n\tif (nla_parse_nested(opts, TIPC_NLA_UDP_MAX,\n\t\t\t     attrs[TIPC_NLA_BEARER_UDP_OPTS],\n\t\t\t     tipc_nl_udp_policy))\n\t\tgoto err;\n\n\tif (!opts[TIPC_NLA_UDP_LOCAL] || !opts[TIPC_NLA_UDP_REMOTE]) {\n\t\tpr_err(\"Invalid UDP bearer configuration\");\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\terr = tipc_parse_udp_addr(opts[TIPC_NLA_UDP_LOCAL], &local,\n\t\t\t\t  &ub->ifindex);\n\tif (err)\n\t\tgoto err;\n\n\terr = tipc_parse_udp_addr(opts[TIPC_NLA_UDP_REMOTE], &remote, NULL);\n\tif (err)\n\t\tgoto err;\n\n\tb->bcast_addr.media_id = TIPC_MEDIA_TYPE_UDP;\n\tb->bcast_addr.broadcast = 1;\n\trcu_assign_pointer(b->media_ptr, ub);\n\trcu_assign_pointer(ub->bearer, b);\n\ttipc_udp_media_addr_set(&b->addr, &local);\n\tif (local.proto == htons(ETH_P_IP)) {\n\t\tstruct net_device *dev;\n\n\t\tdev = __ip_dev_find(net, local.ipv4.s_addr, false);\n\t\tif (!dev) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto err;\n\t\t}\n\t\tudp_conf.family = AF_INET;\n\t\tudp_conf.local_ip.s_addr = htonl(INADDR_ANY);\n\t\tudp_conf.use_udp_checksums = false;\n\t\tub->ifindex = dev->ifindex;\n\t\tif (tipc_mtu_bad(dev, sizeof(struct iphdr) +\n\t\t\t\t      sizeof(struct udphdr))) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\t\tb->mtu = dev->mtu - sizeof(struct iphdr)\n\t\t\t- sizeof(struct udphdr);\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else if (local.proto == htons(ETH_P_IPV6)) {\n\t\tudp_conf.family = AF_INET6;\n\t\tudp_conf.use_udp6_tx_checksums = true;\n\t\tudp_conf.use_udp6_rx_checksums = true;\n\t\tudp_conf.local_ip6 = in6addr_any;\n\t\tb->mtu = 1280;\n#endif\n\t} else {\n\t\terr = -EAFNOSUPPORT;\n\t\tgoto err;\n\t}\n\tudp_conf.local_udp_port = local.port;\n\terr = udp_sock_create(net, &udp_conf, &ub->ubsock);\n\tif (err)\n\t\tgoto err;\n\ttuncfg.sk_user_data = ub;\n\ttuncfg.encap_type = 1;\n\ttuncfg.encap_rcv = tipc_udp_recv;\n\ttuncfg.encap_destroy = NULL;\n\tsetup_udp_tunnel_sock(net, ub->ubsock, &tuncfg);\n\n\t/**\n\t * The bcast media address port is used for all peers and the ip\n\t * is used if it's a multicast address.\n\t */\n\tmemcpy(&b->bcast_addr.value, &remote, sizeof(remote));\n\tif (tipc_udp_is_mcast_addr(&remote))\n\t\terr = enable_mcast(ub, &remote);\n\telse\n\t\terr = tipc_udp_rcast_add(b, &remote);\n\tif (err)\n\t\tgoto err;\n\n\treturn 0;\nerr:\n\tif (ub->ubsock)\n\t\tudp_tunnel_sock_release(ub->ubsock);\n\tkfree(ub);\n\treturn err;\n}",
                        "cve_id": "CVE-2016-8632"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nint tipc_enable_l2_media(struct net *net, struct tipc_bearer *b,\n\t\t\t struct nlattr *attr[])\n{\n\tstruct net_device *dev;\n\tchar *driver_name = strchr((const char *)b->name, ':') + 1;\n\n\t/* Find device with specified name */\n\tdev = dev_get_by_name(net, driver_name);\n\tif (!dev)\n\t\treturn -ENODEV;\n\tif (tipc_mtu_bad(dev, 0)) {\n\t\tdev_put(dev);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Associate TIPC bearer with L2 bearer */\n\trcu_assign_pointer(b->media_ptr, dev);\n\tmemset(&b->bcast_addr, 0, sizeof(b->bcast_addr));\n\tmemcpy(b->bcast_addr.value, dev->broadcast, b->media->hwaddr_len);\n\tb->bcast_addr.media_id = b->media->type_id;\n\tb->bcast_addr.broadcast = 1;\n\tb->mtu = dev->mtu;\n\tb->media->raw2addr(b, &b->addr, (char *)dev->dev_addr);\n\trcu_assign_pointer(dev->tipc_ptr, b);\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int tipc_l2_device_event(struct notifier_block *nb, unsigned long evt,\n\t\t\t\tvoid *ptr)\n{\n\tstruct net_device *dev = netdev_notifier_info_to_dev(ptr);\n\tstruct net *net = dev_net(dev);\n\tstruct tipc_bearer *b;\n\n\tb = rtnl_dereference(dev->tipc_ptr);\n\tif (!b)\n\t\treturn NOTIFY_DONE;\n\n\tb->mtu = dev->mtu;\n\n\tswitch (evt) {\n\tcase NETDEV_CHANGE:\n\t\tif (netif_carrier_ok(dev))\n\t\t\tbreak;\n\tcase NETDEV_UP:\n\t\ttest_and_set_bit_lock(0, &b->up);\n\t\tbreak;\n\tcase NETDEV_GOING_DOWN:\n\t\tclear_bit_unlock(0, &b->up);\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_CHANGEMTU:\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_CHANGEADDR:\n\t\tb->media->raw2addr(b, &b->addr,\n\t\t\t\t   (char *)dev->dev_addr);\n\t\ttipc_reset_bearer(net, b);\n\t\tbreak;\n\tcase NETDEV_UNREGISTER:\n\tcase NETDEV_CHANGENAME:\n\t\tbearer_disable(dev_net(dev), b);\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int tipc_udp_enable(struct net *net, struct tipc_bearer *b,\n\t\t\t   struct nlattr *attrs[])\n{\n\tint err = -EINVAL;\n\tstruct udp_bearer *ub;\n\tstruct udp_media_addr remote = {0};\n\tstruct udp_media_addr local = {0};\n\tstruct udp_port_cfg udp_conf = {0};\n\tstruct udp_tunnel_sock_cfg tuncfg = {NULL};\n\tstruct nlattr *opts[TIPC_NLA_UDP_MAX + 1];\n\n\tub = kzalloc(sizeof(*ub), GFP_ATOMIC);\n\tif (!ub)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&ub->rcast.list);\n\n\tif (!attrs[TIPC_NLA_BEARER_UDP_OPTS])\n\t\tgoto err;\n\n\tif (nla_parse_nested(opts, TIPC_NLA_UDP_MAX,\n\t\t\t     attrs[TIPC_NLA_BEARER_UDP_OPTS],\n\t\t\t     tipc_nl_udp_policy))\n\t\tgoto err;\n\n\tif (!opts[TIPC_NLA_UDP_LOCAL] || !opts[TIPC_NLA_UDP_REMOTE]) {\n\t\tpr_err(\"Invalid UDP bearer configuration\");\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\terr = tipc_parse_udp_addr(opts[TIPC_NLA_UDP_LOCAL], &local,\n\t\t\t\t  &ub->ifindex);\n\tif (err)\n\t\tgoto err;\n\n\terr = tipc_parse_udp_addr(opts[TIPC_NLA_UDP_REMOTE], &remote, NULL);\n\tif (err)\n\t\tgoto err;\n\n\tb->bcast_addr.media_id = TIPC_MEDIA_TYPE_UDP;\n\tb->bcast_addr.broadcast = 1;\n\trcu_assign_pointer(b->media_ptr, ub);\n\trcu_assign_pointer(ub->bearer, b);\n\ttipc_udp_media_addr_set(&b->addr, &local);\n\tif (local.proto == htons(ETH_P_IP)) {\n\t\tstruct net_device *dev;\n\n\t\tdev = __ip_dev_find(net, local.ipv4.s_addr, false);\n\t\tif (!dev) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto err;\n\t\t}\n\t\tudp_conf.family = AF_INET;\n\t\tudp_conf.local_ip.s_addr = htonl(INADDR_ANY);\n\t\tudp_conf.use_udp_checksums = false;\n\t\tub->ifindex = dev->ifindex;\n\t\tif (tipc_mtu_bad(dev, sizeof(struct iphdr) +\n\t\t\t\t      sizeof(struct udphdr))) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\t\tb->mtu = dev->mtu - sizeof(struct iphdr)\n\t\t\t- sizeof(struct udphdr);\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else if (local.proto == htons(ETH_P_IPV6)) {\n\t\tudp_conf.family = AF_INET6;\n\t\tudp_conf.use_udp6_tx_checksums = true;\n\t\tudp_conf.use_udp6_rx_checksums = true;\n\t\tudp_conf.local_ip6 = in6addr_any;\n\t\tb->mtu = 1280;\n#endif\n\t} else {\n\t\terr = -EAFNOSUPPORT;\n\t\tgoto err;\n\t}\n\tudp_conf.local_udp_port = local.port;\n\terr = udp_sock_create(net, &udp_conf, &ub->ubsock);\n\tif (err)\n\t\tgoto err;\n\ttuncfg.sk_user_data = ub;\n\ttuncfg.encap_type = 1;\n\ttuncfg.encap_rcv = tipc_udp_recv;\n\ttuncfg.encap_destroy = NULL;\n\tsetup_udp_tunnel_sock(net, ub->ubsock, &tuncfg);\n\n\t/**\n\t * The bcast media address port is used for all peers and the ip\n\t * is used if it's a multicast address.\n\t */\n\tmemcpy(&b->bcast_addr.value, &remote, sizeof(remote));\n\tif (tipc_udp_is_mcast_addr(&remote))\n\t\terr = enable_mcast(ub, &remote);\n\telse\n\t\terr = tipc_udp_rcast_add(b, &remote);\n\tif (err)\n\t\tgoto err;\n\n\treturn 0;\nerr:\n\tif (ub->ubsock)\n\t\tudp_tunnel_sock_release(ub->ubsock);\n\tkfree(ub);\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}",
                        "code_after_change": "static int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(skb, net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nint tipc_enable_l2_media(struct net *net, struct tipc_bearer *b,\n\t\t\t struct nlattr *attr[])\n{\n\tstruct net_device *dev;\n\tchar *driver_name = strchr((const char *)b->name, ':') + 1;\n\n\t/* Find device with specified name */\n\tdev = dev_get_by_name(net, driver_name);\n\tif (!dev)\n\t\treturn -ENODEV;\n\tif (tipc_mtu_bad(dev, 0)) {\n\t\tdev_put(dev);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Associate TIPC bearer with L2 bearer */\n\trcu_assign_pointer(b->media_ptr, dev);\n\tmemset(&b->bcast_addr, 0, sizeof(b->bcast_addr));\n\tmemcpy(b->bcast_addr.value, dev->broadcast, b->media->hwaddr_len);\n\tb->bcast_addr.media_id = b->media->type_id;\n\tb->bcast_addr.broadcast = 1;\n\tb->mtu = dev->mtu;\n\tb->media->raw2addr(b, &b->addr, (char *)dev->dev_addr);\n\trcu_assign_pointer(dev->tipc_ptr, b);\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(skb, net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 454,
            "cve_id": "CVE-2014-0181",
            "code_snippet": "static int tc_ctl_action(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_ACT_MAX + 1];\n\tu32 portid = skb ? NETLINK_CB(skb).portid : 0;\n\tint ret = 0, ovr = 0;\n\n\tif ((n->nlmsg_type != RTM_GETACTION) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tret = nlmsg_parse(n, sizeof(struct tcamsg), tca, TCA_ACT_MAX, NULL);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (tca[TCA_ACT_TAB] == NULL) {\n\t\tpr_notice(\"tc_ctl_action: received NO action attribs\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* n->nlmsg_flags & NLM_F_CREATE */\n\tswitch (n->nlmsg_type) {\n\tcase RTM_NEWACTION:\n\t\t/* we are going to assume all other flags\n\t\t * imply create only if it doesn't exist\n\t\t * Note that CREATE | EXCL implies that\n\t\t * but since we want avoid ambiguity (eg when flags\n\t\t * is zero) then just set this\n\t\t */\n\t\tif (n->nlmsg_flags & NLM_F_REPLACE)\n\t\t\tovr = 1;\nreplay:\n\t\tret = tcf_action_add(net, tca[TCA_ACT_TAB], n, portid, ovr);\n\t\tif (ret == -EAGAIN)\n\t\t\tgoto replay;\n\t\tbreak;\n\tcase RTM_DELACTION:\n\t\tret = tca_action_gd(net, tca[TCA_ACT_TAB], n,\n\t\t\t\t    portid, RTM_DELACTION);\n\t\tbreak;\n\tcase RTM_GETACTION:\n\t\tret = tca_action_gd(net, tca[TCA_ACT_TAB], n,\n\t\t\t\t    portid, RTM_GETACTION);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}",
                        "code_after_change": "static int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int tc_ctl_action(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_ACT_MAX + 1];\n\tu32 portid = skb ? NETLINK_CB(skb).portid : 0;\n\tint ret = 0, ovr = 0;\n\n\tif ((n->nlmsg_type != RTM_GETACTION) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tret = nlmsg_parse(n, sizeof(struct tcamsg), tca, TCA_ACT_MAX, NULL);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (tca[TCA_ACT_TAB] == NULL) {\n\t\tpr_notice(\"tc_ctl_action: received NO action attribs\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* n->nlmsg_flags & NLM_F_CREATE */\n\tswitch (n->nlmsg_type) {\n\tcase RTM_NEWACTION:\n\t\t/* we are going to assume all other flags\n\t\t * imply create only if it doesn't exist\n\t\t * Note that CREATE | EXCL implies that\n\t\t * but since we want avoid ambiguity (eg when flags\n\t\t * is zero) then just set this\n\t\t */\n\t\tif (n->nlmsg_flags & NLM_F_REPLACE)\n\t\t\tovr = 1;\nreplay:\n\t\tret = tcf_action_add(net, tca[TCA_ACT_TAB], n, portid, ovr);\n\t\tif (ret == -EAGAIN)\n\t\t\tgoto replay;\n\t\tbreak;\n\tcase RTM_DELACTION:\n\t\tret = tca_action_gd(net, tca[TCA_ACT_TAB], n,\n\t\t\t\t    portid, RTM_DELACTION);\n\t\tbreak;\n\tcase RTM_GETACTION:\n\t\tret = tca_action_gd(net, tca[TCA_ACT_TAB], n,\n\t\t\t\t    portid, RTM_GETACTION);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}",
                        "code_after_change": "static int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int tc_ctl_action(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_ACT_MAX + 1];\n\tu32 portid = skb ? NETLINK_CB(skb).portid : 0;\n\tint ret = 0, ovr = 0;\n\n\tif ((n->nlmsg_type != RTM_GETACTION) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tret = nlmsg_parse(n, sizeof(struct tcamsg), tca, TCA_ACT_MAX, NULL);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (tca[TCA_ACT_TAB] == NULL) {\n\t\tpr_notice(\"tc_ctl_action: received NO action attribs\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* n->nlmsg_flags & NLM_F_CREATE */\n\tswitch (n->nlmsg_type) {\n\tcase RTM_NEWACTION:\n\t\t/* we are going to assume all other flags\n\t\t * imply create only if it doesn't exist\n\t\t * Note that CREATE | EXCL implies that\n\t\t * but since we want avoid ambiguity (eg when flags\n\t\t * is zero) then just set this\n\t\t */\n\t\tif (n->nlmsg_flags & NLM_F_REPLACE)\n\t\t\tovr = 1;\nreplay:\n\t\tret = tcf_action_add(net, tca[TCA_ACT_TAB], n, portid, ovr);\n\t\tif (ret == -EAGAIN)\n\t\t\tgoto replay;\n\t\tbreak;\n\tcase RTM_DELACTION:\n\t\tret = tca_action_gd(net, tca[TCA_ACT_TAB], n,\n\t\t\t\t    portid, RTM_DELACTION);\n\t\tbreak;\n\tcase RTM_GETACTION:\n\t\tret = tca_action_gd(net, tca[TCA_ACT_TAB], n,\n\t\t\t\t    portid, RTM_GETACTION);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}",
                        "code_after_change": "static int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int tc_ctl_action(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_ACT_MAX + 1];\n\tu32 portid = skb ? NETLINK_CB(skb).portid : 0;\n\tint ret = 0, ovr = 0;\n\n\tif ((n->nlmsg_type != RTM_GETACTION) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tret = nlmsg_parse(n, sizeof(struct tcamsg), tca, TCA_ACT_MAX, NULL);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (tca[TCA_ACT_TAB] == NULL) {\n\t\tpr_notice(\"tc_ctl_action: received NO action attribs\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* n->nlmsg_flags & NLM_F_CREATE */\n\tswitch (n->nlmsg_type) {\n\tcase RTM_NEWACTION:\n\t\t/* we are going to assume all other flags\n\t\t * imply create only if it doesn't exist\n\t\t * Note that CREATE | EXCL implies that\n\t\t * but since we want avoid ambiguity (eg when flags\n\t\t * is zero) then just set this\n\t\t */\n\t\tif (n->nlmsg_flags & NLM_F_REPLACE)\n\t\t\tovr = 1;\nreplay:\n\t\tret = tcf_action_add(net, tca[TCA_ACT_TAB], n, portid, ovr);\n\t\tif (ret == -EAGAIN)\n\t\t\tgoto replay;\n\t\tbreak;\n\tcase RTM_DELACTION:\n\t\tret = tca_action_gd(net, tca[TCA_ACT_TAB], n,\n\t\t\t\t    portid, RTM_DELACTION);\n\t\tbreak;\n\tcase RTM_GETACTION:\n\t\tret = tca_action_gd(net, tca[TCA_ACT_TAB], n,\n\t\t\t\t    portid, RTM_GETACTION);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 1003,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "static int rawv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct raw6_frag_vec rfv;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tu16 proto;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (sin6) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (sin6->sin6_family && sin6->sin6_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\t/* port is the proto value [0..255] carried in nexthdr */\n\t\tproto = ntohs(sin6->sin6_port);\n\n\t\tif (!proto)\n\t\t\tproto = inet->inet_num;\n\t\telse if (proto != inet->inet_num)\n\t\t\treturn -EINVAL;\n\n\t\tif (proto > 255)\n\t\t\treturn -EINVAL;\n\n\t\tdaddr = &sin6->sin6_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tproto = inet->inet_num;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = proto;\n\trfv.msg = msg;\n\trfv.hlen = 0;\n\terr = rawv6_probe_proto_opt(&rfv, &fl6);\n\tif (err)\n\t\tgoto out;\n\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tif (inet->hdrincl)\n\t\tfl6.flowi6_flags |= FLOWI_FLAG_KNOWN_NH;\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tif (inet->hdrincl)\n\t\terr = rawv6_send_hdrinc(sk, msg, len, &fl6, &dst, msg->msg_flags);\n\telse {\n\t\tlock_sock(sk);\n\t\terr = ip6_append_data(sk, raw6_getfrag, &rfv,\n\t\t\tlen, 0, hlimit, tclass, opt, &fl6, (struct rt6_info *)dst,\n\t\t\tmsg->msg_flags, dontfrag);\n\n\t\tif (err)\n\t\t\tip6_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE))\n\t\t\terr = rawv6_push_pending_frames(sk, &fl6, rp);\n\t\trelease_sock(sk);\n\t}\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\treturn err < 0 ? err : len;\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint transhdrlen = 4; /* zero session-id */\n\tint ulen = len + transhdrlen;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (lsa) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (lsa->l2tp_family && lsa->l2tp_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\tdaddr = &lsa->l2tp_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = lsa->l2tp_flowinfo & IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (flowlabel == NULL)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    lsa->l2tp_scope_id &&\n\t\t    ipv6_addr_type(daddr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tfl6.flowi6_oif = lsa->l2tp_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel & IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\n\tif (opt == NULL)\n\t\topt = np->opt;\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tlock_sock(sk);\n\terr = ip6_append_data(sk, ip_generic_getfrag, msg,\n\t\t\t      ulen, transhdrlen, hlimit, tclass, opt,\n\t\t\t      &fl6, (struct rt6_info *)dst,\n\t\t\t      msg->msg_flags, dontfrag);\n\tif (err)\n\t\tip6_flush_pending_frames(sk);\n\telse if (!(msg->msg_flags & MSG_MORE))\n\t\terr = l2tp_ip6_push_pending_frames(sk);\n\trelease_sock(sk);\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\n\treturn err < 0 ? err : len;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
                        "code_after_change": "static int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint transhdrlen = 4; /* zero session-id */\n\tint ulen = len + transhdrlen;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (lsa) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (lsa->l2tp_family && lsa->l2tp_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\tdaddr = &lsa->l2tp_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = lsa->l2tp_flowinfo & IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (flowlabel == NULL)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    lsa->l2tp_scope_id &&\n\t\t    ipv6_addr_type(daddr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tfl6.flowi6_oif = lsa->l2tp_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel & IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tlock_sock(sk);\n\terr = ip6_append_data(sk, ip_generic_getfrag, msg,\n\t\t\t      ulen, transhdrlen, hlimit, tclass, opt,\n\t\t\t      &fl6, (struct rt6_info *)dst,\n\t\t\t      msg->msg_flags, dontfrag);\n\tif (err)\n\t\tip6_flush_pending_frames(sk);\n\telse if (!(msg->msg_flags & MSG_MORE))\n\t\terr = l2tp_ip6_push_pending_frames(sk);\n\trelease_sock(sk);\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\n\treturn err < 0 ? err : len;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int rawv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct raw6_frag_vec rfv;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tu16 proto;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (sin6) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (sin6->sin6_family && sin6->sin6_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\t/* port is the proto value [0..255] carried in nexthdr */\n\t\tproto = ntohs(sin6->sin6_port);\n\n\t\tif (!proto)\n\t\t\tproto = inet->inet_num;\n\t\telse if (proto != inet->inet_num)\n\t\t\treturn -EINVAL;\n\n\t\tif (proto > 255)\n\t\t\treturn -EINVAL;\n\n\t\tdaddr = &sin6->sin6_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tproto = inet->inet_num;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = proto;\n\trfv.msg = msg;\n\trfv.hlen = 0;\n\terr = rawv6_probe_proto_opt(&rfv, &fl6);\n\tif (err)\n\t\tgoto out;\n\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tif (inet->hdrincl)\n\t\tfl6.flowi6_flags |= FLOWI_FLAG_KNOWN_NH;\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tif (inet->hdrincl)\n\t\terr = rawv6_send_hdrinc(sk, msg, len, &fl6, &dst, msg->msg_flags);\n\telse {\n\t\tlock_sock(sk);\n\t\terr = ip6_append_data(sk, raw6_getfrag, &rfv,\n\t\t\tlen, 0, hlimit, tclass, opt, &fl6, (struct rt6_info *)dst,\n\t\t\tmsg->msg_flags, dontfrag);\n\n\t\tif (err)\n\t\t\tip6_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE))\n\t\t\terr = rawv6_push_pending_frames(sk, &fl6, rp);\n\t\trelease_sock(sk);\n\t}\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\treturn err < 0 ? err : len;\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint transhdrlen = 4; /* zero session-id */\n\tint ulen = len + transhdrlen;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (lsa) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (lsa->l2tp_family && lsa->l2tp_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\tdaddr = &lsa->l2tp_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = lsa->l2tp_flowinfo & IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (flowlabel == NULL)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    lsa->l2tp_scope_id &&\n\t\t    ipv6_addr_type(daddr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tfl6.flowi6_oif = lsa->l2tp_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel & IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\n\tif (opt == NULL)\n\t\topt = np->opt;\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tlock_sock(sk);\n\terr = ip6_append_data(sk, ip_generic_getfrag, msg,\n\t\t\t      ulen, transhdrlen, hlimit, tclass, opt,\n\t\t\t      &fl6, (struct rt6_info *)dst,\n\t\t\t      msg->msg_flags, dontfrag);\n\tif (err)\n\t\tip6_flush_pending_frames(sk);\n\telse if (!(msg->msg_flags & MSG_MORE))\n\t\terr = l2tp_ip6_push_pending_frames(sk);\n\trelease_sock(sk);\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\n\treturn err < 0 ? err : len;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint transhdrlen = 4; /* zero session-id */\n\tint ulen = len + transhdrlen;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (lsa) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (lsa->l2tp_family && lsa->l2tp_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\tdaddr = &lsa->l2tp_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = lsa->l2tp_flowinfo & IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (flowlabel == NULL)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    lsa->l2tp_scope_id &&\n\t\t    ipv6_addr_type(daddr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tfl6.flowi6_oif = lsa->l2tp_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel & IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tlock_sock(sk);\n\terr = ip6_append_data(sk, ip_generic_getfrag, msg,\n\t\t\t      ulen, transhdrlen, hlimit, tclass, opt,\n\t\t\t      &fl6, (struct rt6_info *)dst,\n\t\t\t      msg->msg_flags, dontfrag);\n\tif (err)\n\t\tip6_flush_pending_frames(sk);\n\telse if (!(msg->msg_flags & MSG_MORE))\n\t\terr = l2tp_ip6_push_pending_frames(sk);\n\trelease_sock(sk);\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\n\treturn err < 0 ? err : len;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t*daddr, *final_p, final;\n\tstruct dst_entry\t*dst;\n\tstruct flowi6\t\tfl6;\n\tstruct ip6_flowlabel\t*flowlabel = NULL;\n\tstruct ipv6_txoptions\t*opt;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tusin->sin6_addr.s6_addr[15] = 0x01;\n\t}\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6.flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet->inet_dport;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tif (!fl6.flowi6_oif && (addr_type&IPV6_ADDR_MULTICAST))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\topt = flowlabel ? flowlabel->opt : np->opt;\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\terr = 0;\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\t/* source address lookup done in ip6_dst_lookup */\n\n\tif (ipv6_addr_any(&np->saddr))\n\t\tnp->saddr = fl6.saddr;\n\n\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tsk->sk_v6_rcv_saddr = fl6.saddr;\n\t\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\t\tif (sk->sk_prot->rehash)\n\t\t\tsk->sk_prot->rehash(sk);\n\t}\n\n\tip6_dst_store(sk, dst,\n\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t      &np->saddr :\n#endif\n\t\t      NULL);\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err;\n}",
                        "code_after_change": "static int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t*daddr, *final_p, final;\n\tstruct dst_entry\t*dst;\n\tstruct flowi6\t\tfl6;\n\tstruct ip6_flowlabel\t*flowlabel = NULL;\n\tstruct ipv6_txoptions\t*opt;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tusin->sin6_addr.s6_addr[15] = 0x01;\n\t}\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6.flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet->inet_dport;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tif (!fl6.flowi6_oif && (addr_type&IPV6_ADDR_MULTICAST))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\trcu_read_lock();\n\topt = flowlabel ? flowlabel->opt : rcu_dereference(np->opt);\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\trcu_read_unlock();\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\terr = 0;\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\t/* source address lookup done in ip6_dst_lookup */\n\n\tif (ipv6_addr_any(&np->saddr))\n\t\tnp->saddr = fl6.saddr;\n\n\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tsk->sk_v6_rcv_saddr = fl6.saddr;\n\t\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\t\tif (sk->sk_prot->rehash)\n\t\t\tsk->sk_prot->rehash(sk);\n\t}\n\n\tip6_dst_store(sk, dst,\n\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t      &np->saddr :\n#endif\n\t\t      NULL);\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err;\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int rawv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct raw6_frag_vec rfv;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tu16 proto;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (sin6) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (sin6->sin6_family && sin6->sin6_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\t/* port is the proto value [0..255] carried in nexthdr */\n\t\tproto = ntohs(sin6->sin6_port);\n\n\t\tif (!proto)\n\t\t\tproto = inet->inet_num;\n\t\telse if (proto != inet->inet_num)\n\t\t\treturn -EINVAL;\n\n\t\tif (proto > 255)\n\t\t\treturn -EINVAL;\n\n\t\tdaddr = &sin6->sin6_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tproto = inet->inet_num;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = proto;\n\trfv.msg = msg;\n\trfv.hlen = 0;\n\terr = rawv6_probe_proto_opt(&rfv, &fl6);\n\tif (err)\n\t\tgoto out;\n\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tif (inet->hdrincl)\n\t\tfl6.flowi6_flags |= FLOWI_FLAG_KNOWN_NH;\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tif (inet->hdrincl)\n\t\terr = rawv6_send_hdrinc(sk, msg, len, &fl6, &dst, msg->msg_flags);\n\telse {\n\t\tlock_sock(sk);\n\t\terr = ip6_append_data(sk, raw6_getfrag, &rfv,\n\t\t\tlen, 0, hlimit, tclass, opt, &fl6, (struct rt6_info *)dst,\n\t\t\tmsg->msg_flags, dontfrag);\n\n\t\tif (err)\n\t\t\tip6_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE))\n\t\t\terr = rawv6_push_pending_frames(sk, &fl6, rp);\n\t\trelease_sock(sk);\n\t}\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\treturn err < 0 ? err : len;\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t*daddr, *final_p, final;\n\tstruct dst_entry\t*dst;\n\tstruct flowi6\t\tfl6;\n\tstruct ip6_flowlabel\t*flowlabel = NULL;\n\tstruct ipv6_txoptions\t*opt;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tusin->sin6_addr.s6_addr[15] = 0x01;\n\t}\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6.flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet->inet_dport;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tif (!fl6.flowi6_oif && (addr_type&IPV6_ADDR_MULTICAST))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\topt = flowlabel ? flowlabel->opt : np->opt;\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\terr = 0;\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\t/* source address lookup done in ip6_dst_lookup */\n\n\tif (ipv6_addr_any(&np->saddr))\n\t\tnp->saddr = fl6.saddr;\n\n\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tsk->sk_v6_rcv_saddr = fl6.saddr;\n\t\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\t\tif (sk->sk_prot->rehash)\n\t\t\tsk->sk_prot->rehash(sk);\n\t}\n\n\tip6_dst_store(sk, dst,\n\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t      &np->saddr :\n#endif\n\t\t      NULL);\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t*daddr, *final_p, final;\n\tstruct dst_entry\t*dst;\n\tstruct flowi6\t\tfl6;\n\tstruct ip6_flowlabel\t*flowlabel = NULL;\n\tstruct ipv6_txoptions\t*opt;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tusin->sin6_addr.s6_addr[15] = 0x01;\n\t}\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6.flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet->inet_dport;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tif (!fl6.flowi6_oif && (addr_type&IPV6_ADDR_MULTICAST))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\trcu_read_lock();\n\topt = flowlabel ? flowlabel->opt : rcu_dereference(np->opt);\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\trcu_read_unlock();\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\terr = 0;\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\t/* source address lookup done in ip6_dst_lookup */\n\n\tif (ipv6_addr_any(&np->saddr))\n\t\tnp->saddr = fl6.saddr;\n\n\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tsk->sk_v6_rcv_saddr = fl6.saddr;\n\t\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\t\tif (sk->sk_prot->rehash)\n\t\t\tsk->sk_prot->rehash(sk);\n\t}\n\n\tip6_dst_store(sk, dst,\n\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t      &np->saddr :\n#endif\n\t\t      NULL);\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *)uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel & IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\t/*\n\t * connect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t * DCCP over IPv4\n\t */\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\n\t\terr = dccp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &dccp_ipv6_af_ops;\n\t\t\tsk->sk_backlog_rcv = dccp_v6_do_rcv;\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (saddr == NULL) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (np->opt != NULL)\n\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n\t\t\t\t\t  np->opt->opt_nflen);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet6_hash_connect(&dccp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tdp->dccps_iss = secure_dccpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t      sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t      inet->inet_sport,\n\t\t\t\t\t\t      inet->inet_dport);\n\terr = dccp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\tdccp_set_state(sk, DCCP_CLOSED);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
                        "code_after_change": "static int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *)uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct ipv6_txoptions *opt;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel & IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\t/*\n\t * connect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t * DCCP over IPv4\n\t */\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\n\t\terr = dccp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &dccp_ipv6_af_ops;\n\t\t\tsk->sk_backlog_rcv = dccp_v6_do_rcv;\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (saddr == NULL) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet6_hash_connect(&dccp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tdp->dccps_iss = secure_dccpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t      sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t      inet->inet_sport,\n\t\t\t\t\t\t      inet->inet_dport);\n\terr = dccp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\tdccp_set_state(sk, DCCP_CLOSED);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int rawv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct raw6_frag_vec rfv;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tu16 proto;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (sin6) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (sin6->sin6_family && sin6->sin6_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\t/* port is the proto value [0..255] carried in nexthdr */\n\t\tproto = ntohs(sin6->sin6_port);\n\n\t\tif (!proto)\n\t\t\tproto = inet->inet_num;\n\t\telse if (proto != inet->inet_num)\n\t\t\treturn -EINVAL;\n\n\t\tif (proto > 255)\n\t\t\treturn -EINVAL;\n\n\t\tdaddr = &sin6->sin6_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tproto = inet->inet_num;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = proto;\n\trfv.msg = msg;\n\trfv.hlen = 0;\n\terr = rawv6_probe_proto_opt(&rfv, &fl6);\n\tif (err)\n\t\tgoto out;\n\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tif (inet->hdrincl)\n\t\tfl6.flowi6_flags |= FLOWI_FLAG_KNOWN_NH;\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tif (inet->hdrincl)\n\t\terr = rawv6_send_hdrinc(sk, msg, len, &fl6, &dst, msg->msg_flags);\n\telse {\n\t\tlock_sock(sk);\n\t\terr = ip6_append_data(sk, raw6_getfrag, &rfv,\n\t\t\tlen, 0, hlimit, tclass, opt, &fl6, (struct rt6_info *)dst,\n\t\t\tmsg->msg_flags, dontfrag);\n\n\t\tif (err)\n\t\t\tip6_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE))\n\t\t\terr = rawv6_push_pending_frames(sk, &fl6, rp);\n\t\trelease_sock(sk);\n\t}\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\treturn err < 0 ? err : len;\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *)uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel & IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\t/*\n\t * connect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t * DCCP over IPv4\n\t */\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\n\t\terr = dccp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &dccp_ipv6_af_ops;\n\t\t\tsk->sk_backlog_rcv = dccp_v6_do_rcv;\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (saddr == NULL) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (np->opt != NULL)\n\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n\t\t\t\t\t  np->opt->opt_nflen);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet6_hash_connect(&dccp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tdp->dccps_iss = secure_dccpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t      sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t      inet->inet_sport,\n\t\t\t\t\t\t      inet->inet_dport);\n\terr = dccp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\tdccp_set_state(sk, DCCP_CLOSED);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *)uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct ipv6_txoptions *opt;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel & IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\t/*\n\t * connect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t * DCCP over IPv4\n\t */\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\n\t\terr = dccp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &dccp_ipv6_af_ops;\n\t\t\tsk->sk_backlog_rcv = dccp_v6_do_rcv;\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (saddr == NULL) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet6_hash_connect(&dccp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tdp->dccps_iss = secure_dccpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t      sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t      inet->inet_sport,\n\t\t\t\t\t\t      inet->inet_dport);\n\terr = dccp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\tdccp_set_state(sk, DCCP_CLOSED);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 1008,
            "cve_id": "CVE-2016-3841",
            "code_snippet": "int udpv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_len = msg->msg_namelen;\n\tint ulen = len;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint err;\n\tint connected = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\n\t/* destination address check */\n\tif (sin6) {\n\t\tif (addr_len < offsetof(struct sockaddr, sa_data))\n\t\t\treturn -EINVAL;\n\n\t\tswitch (sin6->sin6_family) {\n\t\tcase AF_INET6:\n\t\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\t\treturn -EINVAL;\n\t\t\tdaddr = &sin6->sin6_addr;\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tgoto do_udp_sendmsg;\n\t\tcase AF_UNSPEC:\n\t\t\tmsg->msg_name = sin6 = NULL;\n\t\t\tmsg->msg_namelen = addr_len = 0;\n\t\t\tdaddr = NULL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (!up->pending) {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t} else\n\t\tdaddr = NULL;\n\n\tif (daddr) {\n\t\tif (ipv6_addr_v4mapped(daddr)) {\n\t\t\tstruct sockaddr_in sin;\n\t\t\tsin.sin_family = AF_INET;\n\t\t\tsin.sin_port = sin6 ? sin6->sin6_port : inet->inet_dport;\n\t\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\t\tmsg->msg_name = &sin;\n\t\t\tmsg->msg_namelen = sizeof(sin);\ndo_udp_sendmsg:\n\t\t\tif (__ipv6_only_sock(sk))\n\t\t\t\treturn -ENETUNREACH;\n\t\t\treturn udp_sendmsg(sk, msg, len);\n\t\t}\n\t}\n\n\tif (up->pending == AF_INET)\n\t\treturn udp_sendmsg(sk, msg, len);\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t   */\n\tif (len > INT_MAX - sizeof(struct udphdr))\n\t\treturn -EMSGSIZE;\n\n\tgetfrag  =  is_udplite ?  udplite_getfrag : ip_generic_getfrag;\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET6)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t\t}\n\t\t\tdst = NULL;\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (sin6) {\n\t\tif (sin6->sin6_port == 0)\n\t\t\treturn -EINVAL;\n\n\t\tfl6.fl6_dport = sin6->sin6_port;\n\t\tdaddr = &sin6->sin6_addr;\n\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tconnected = 1;\n\t}\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->sticky_pktinfo.ipi6_ifindex;\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(*opt);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t\tconnected = 0;\n\t}\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\tif (final_p)\n\t\tconnected = 0;\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr)) {\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\t\tconnected = 0;\n\t} else if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_sk_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\t/* Lockless fast path for the non-corking case */\n\tif (!corkreq) {\n\t\tstruct sk_buff *skb;\n\n\t\tskb = ip6_make_skb(sk, getfrag, msg, ulen,\n\t\t\t\t   sizeof(struct udphdr), hlimit, tclass, opt,\n\t\t\t\t   &fl6, (struct rt6_info *)dst,\n\t\t\t\t   msg->msg_flags, dontfrag);\n\t\terr = PTR_ERR(skb);\n\t\tif (!IS_ERR_OR_NULL(skb))\n\t\t\terr = udp_v6_send_skb(skb, &fl6);\n\t\tgoto release_dst;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tnet_dbg_ratelimited(\"udp cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tup->pending = AF_INET6;\n\ndo_append_data:\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\tup->len += ulen;\n\terr = ip6_append_data(sk, getfrag, msg, ulen,\n\t\tsizeof(struct udphdr), hlimit, tclass, opt, &fl6,\n\t\t(struct rt6_info *)dst,\n\t\tcorkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags, dontfrag);\n\tif (err)\n\t\tudp_v6_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_v6_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\n\tif (err > 0)\n\t\terr = np->recverr ? net_xmit_errno(err) : 0;\n\trelease_sock(sk);\n\nrelease_dst:\n\tif (dst) {\n\t\tif (connected) {\n\t\t\tip6_dst_store(sk, dst,\n\t\t\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t\t\t      &np->saddr :\n#endif\n\t\t\t\t      NULL);\n\t\t} else {\n\t\t\tdst_release(dst);\n\t\t}\n\t\tdst = NULL;\n\t}\n\nout:\n\tdst_release(dst);\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint transhdrlen = 4; /* zero session-id */\n\tint ulen = len + transhdrlen;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (lsa) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (lsa->l2tp_family && lsa->l2tp_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\tdaddr = &lsa->l2tp_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = lsa->l2tp_flowinfo & IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (flowlabel == NULL)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    lsa->l2tp_scope_id &&\n\t\t    ipv6_addr_type(daddr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tfl6.flowi6_oif = lsa->l2tp_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel & IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\n\tif (opt == NULL)\n\t\topt = np->opt;\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tlock_sock(sk);\n\terr = ip6_append_data(sk, ip_generic_getfrag, msg,\n\t\t\t      ulen, transhdrlen, hlimit, tclass, opt,\n\t\t\t      &fl6, (struct rt6_info *)dst,\n\t\t\t      msg->msg_flags, dontfrag);\n\tif (err)\n\t\tip6_flush_pending_frames(sk);\n\telse if (!(msg->msg_flags & MSG_MORE))\n\t\terr = l2tp_ip6_push_pending_frames(sk);\n\trelease_sock(sk);\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\n\treturn err < 0 ? err : len;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
                        "code_after_change": "static int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint transhdrlen = 4; /* zero session-id */\n\tint ulen = len + transhdrlen;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (lsa) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (lsa->l2tp_family && lsa->l2tp_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\tdaddr = &lsa->l2tp_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = lsa->l2tp_flowinfo & IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (flowlabel == NULL)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    lsa->l2tp_scope_id &&\n\t\t    ipv6_addr_type(daddr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tfl6.flowi6_oif = lsa->l2tp_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel & IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tlock_sock(sk);\n\terr = ip6_append_data(sk, ip_generic_getfrag, msg,\n\t\t\t      ulen, transhdrlen, hlimit, tclass, opt,\n\t\t\t      &fl6, (struct rt6_info *)dst,\n\t\t\t      msg->msg_flags, dontfrag);\n\tif (err)\n\t\tip6_flush_pending_frames(sk);\n\telse if (!(msg->msg_flags & MSG_MORE))\n\t\terr = l2tp_ip6_push_pending_frames(sk);\n\trelease_sock(sk);\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\n\treturn err < 0 ? err : len;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nint udpv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_len = msg->msg_namelen;\n\tint ulen = len;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint err;\n\tint connected = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\n\t/* destination address check */\n\tif (sin6) {\n\t\tif (addr_len < offsetof(struct sockaddr, sa_data))\n\t\t\treturn -EINVAL;\n\n\t\tswitch (sin6->sin6_family) {\n\t\tcase AF_INET6:\n\t\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\t\treturn -EINVAL;\n\t\t\tdaddr = &sin6->sin6_addr;\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tgoto do_udp_sendmsg;\n\t\tcase AF_UNSPEC:\n\t\t\tmsg->msg_name = sin6 = NULL;\n\t\t\tmsg->msg_namelen = addr_len = 0;\n\t\t\tdaddr = NULL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (!up->pending) {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t} else\n\t\tdaddr = NULL;\n\n\tif (daddr) {\n\t\tif (ipv6_addr_v4mapped(daddr)) {\n\t\t\tstruct sockaddr_in sin;\n\t\t\tsin.sin_family = AF_INET;\n\t\t\tsin.sin_port = sin6 ? sin6->sin6_port : inet->inet_dport;\n\t\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\t\tmsg->msg_name = &sin;\n\t\t\tmsg->msg_namelen = sizeof(sin);\ndo_udp_sendmsg:\n\t\t\tif (__ipv6_only_sock(sk))\n\t\t\t\treturn -ENETUNREACH;\n\t\t\treturn udp_sendmsg(sk, msg, len);\n\t\t}\n\t}\n\n\tif (up->pending == AF_INET)\n\t\treturn udp_sendmsg(sk, msg, len);\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t   */\n\tif (len > INT_MAX - sizeof(struct udphdr))\n\t\treturn -EMSGSIZE;\n\n\tgetfrag  =  is_udplite ?  udplite_getfrag : ip_generic_getfrag;\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET6)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t\t}\n\t\t\tdst = NULL;\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (sin6) {\n\t\tif (sin6->sin6_port == 0)\n\t\t\treturn -EINVAL;\n\n\t\tfl6.fl6_dport = sin6->sin6_port;\n\t\tdaddr = &sin6->sin6_addr;\n\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tconnected = 1;\n\t}\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->sticky_pktinfo.ipi6_ifindex;\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(*opt);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t\tconnected = 0;\n\t}\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\tif (final_p)\n\t\tconnected = 0;\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr)) {\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\t\tconnected = 0;\n\t} else if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_sk_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\t/* Lockless fast path for the non-corking case */\n\tif (!corkreq) {\n\t\tstruct sk_buff *skb;\n\n\t\tskb = ip6_make_skb(sk, getfrag, msg, ulen,\n\t\t\t\t   sizeof(struct udphdr), hlimit, tclass, opt,\n\t\t\t\t   &fl6, (struct rt6_info *)dst,\n\t\t\t\t   msg->msg_flags, dontfrag);\n\t\terr = PTR_ERR(skb);\n\t\tif (!IS_ERR_OR_NULL(skb))\n\t\t\terr = udp_v6_send_skb(skb, &fl6);\n\t\tgoto release_dst;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tnet_dbg_ratelimited(\"udp cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tup->pending = AF_INET6;\n\ndo_append_data:\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\tup->len += ulen;\n\terr = ip6_append_data(sk, getfrag, msg, ulen,\n\t\tsizeof(struct udphdr), hlimit, tclass, opt, &fl6,\n\t\t(struct rt6_info *)dst,\n\t\tcorkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags, dontfrag);\n\tif (err)\n\t\tudp_v6_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_v6_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\n\tif (err > 0)\n\t\terr = np->recverr ? net_xmit_errno(err) : 0;\n\trelease_sock(sk);\n\nrelease_dst:\n\tif (dst) {\n\t\tif (connected) {\n\t\t\tip6_dst_store(sk, dst,\n\t\t\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t\t\t      &np->saddr :\n#endif\n\t\t\t\t      NULL);\n\t\t} else {\n\t\t\tdst_release(dst);\n\t\t}\n\t\tdst = NULL;\n\t}\n\nout:\n\tdst_release(dst);\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint transhdrlen = 4; /* zero session-id */\n\tint ulen = len + transhdrlen;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (lsa) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (lsa->l2tp_family && lsa->l2tp_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\tdaddr = &lsa->l2tp_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = lsa->l2tp_flowinfo & IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (flowlabel == NULL)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    lsa->l2tp_scope_id &&\n\t\t    ipv6_addr_type(daddr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tfl6.flowi6_oif = lsa->l2tp_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel & IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\n\tif (opt == NULL)\n\t\topt = np->opt;\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tlock_sock(sk);\n\terr = ip6_append_data(sk, ip_generic_getfrag, msg,\n\t\t\t      ulen, transhdrlen, hlimit, tclass, opt,\n\t\t\t      &fl6, (struct rt6_info *)dst,\n\t\t\t      msg->msg_flags, dontfrag);\n\tif (err)\n\t\tip6_flush_pending_frames(sk);\n\telse if (!(msg->msg_flags & MSG_MORE))\n\t\terr = l2tp_ip6_push_pending_frames(sk);\n\trelease_sock(sk);\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\n\treturn err < 0 ? err : len;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint transhdrlen = 4; /* zero session-id */\n\tint ulen = len + transhdrlen;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (lsa) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (lsa->l2tp_family && lsa->l2tp_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\tdaddr = &lsa->l2tp_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = lsa->l2tp_flowinfo & IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (flowlabel == NULL)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    lsa->l2tp_scope_id &&\n\t\t    ipv6_addr_type(daddr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tfl6.flowi6_oif = lsa->l2tp_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel & IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tlock_sock(sk);\n\terr = ip6_append_data(sk, ip_generic_getfrag, msg,\n\t\t\t      ulen, transhdrlen, hlimit, tclass, opt,\n\t\t\t      &fl6, (struct rt6_info *)dst,\n\t\t\t      msg->msg_flags, dontfrag);\n\tif (err)\n\t\tip6_flush_pending_frames(sk);\n\telse if (!(msg->msg_flags & MSG_MORE))\n\t\terr = l2tp_ip6_push_pending_frames(sk);\n\trelease_sock(sk);\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\n\treturn err < 0 ? err : len;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t*daddr, *final_p, final;\n\tstruct dst_entry\t*dst;\n\tstruct flowi6\t\tfl6;\n\tstruct ip6_flowlabel\t*flowlabel = NULL;\n\tstruct ipv6_txoptions\t*opt;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tusin->sin6_addr.s6_addr[15] = 0x01;\n\t}\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6.flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet->inet_dport;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tif (!fl6.flowi6_oif && (addr_type&IPV6_ADDR_MULTICAST))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\topt = flowlabel ? flowlabel->opt : np->opt;\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\terr = 0;\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\t/* source address lookup done in ip6_dst_lookup */\n\n\tif (ipv6_addr_any(&np->saddr))\n\t\tnp->saddr = fl6.saddr;\n\n\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tsk->sk_v6_rcv_saddr = fl6.saddr;\n\t\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\t\tif (sk->sk_prot->rehash)\n\t\t\tsk->sk_prot->rehash(sk);\n\t}\n\n\tip6_dst_store(sk, dst,\n\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t      &np->saddr :\n#endif\n\t\t      NULL);\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err;\n}",
                        "code_after_change": "static int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t*daddr, *final_p, final;\n\tstruct dst_entry\t*dst;\n\tstruct flowi6\t\tfl6;\n\tstruct ip6_flowlabel\t*flowlabel = NULL;\n\tstruct ipv6_txoptions\t*opt;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tusin->sin6_addr.s6_addr[15] = 0x01;\n\t}\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6.flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet->inet_dport;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tif (!fl6.flowi6_oif && (addr_type&IPV6_ADDR_MULTICAST))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\trcu_read_lock();\n\topt = flowlabel ? flowlabel->opt : rcu_dereference(np->opt);\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\trcu_read_unlock();\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\terr = 0;\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\t/* source address lookup done in ip6_dst_lookup */\n\n\tif (ipv6_addr_any(&np->saddr))\n\t\tnp->saddr = fl6.saddr;\n\n\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tsk->sk_v6_rcv_saddr = fl6.saddr;\n\t\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\t\tif (sk->sk_prot->rehash)\n\t\t\tsk->sk_prot->rehash(sk);\n\t}\n\n\tip6_dst_store(sk, dst,\n\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t      &np->saddr :\n#endif\n\t\t      NULL);\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err;\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nint udpv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_len = msg->msg_namelen;\n\tint ulen = len;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint err;\n\tint connected = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\n\t/* destination address check */\n\tif (sin6) {\n\t\tif (addr_len < offsetof(struct sockaddr, sa_data))\n\t\t\treturn -EINVAL;\n\n\t\tswitch (sin6->sin6_family) {\n\t\tcase AF_INET6:\n\t\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\t\treturn -EINVAL;\n\t\t\tdaddr = &sin6->sin6_addr;\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tgoto do_udp_sendmsg;\n\t\tcase AF_UNSPEC:\n\t\t\tmsg->msg_name = sin6 = NULL;\n\t\t\tmsg->msg_namelen = addr_len = 0;\n\t\t\tdaddr = NULL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (!up->pending) {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t} else\n\t\tdaddr = NULL;\n\n\tif (daddr) {\n\t\tif (ipv6_addr_v4mapped(daddr)) {\n\t\t\tstruct sockaddr_in sin;\n\t\t\tsin.sin_family = AF_INET;\n\t\t\tsin.sin_port = sin6 ? sin6->sin6_port : inet->inet_dport;\n\t\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\t\tmsg->msg_name = &sin;\n\t\t\tmsg->msg_namelen = sizeof(sin);\ndo_udp_sendmsg:\n\t\t\tif (__ipv6_only_sock(sk))\n\t\t\t\treturn -ENETUNREACH;\n\t\t\treturn udp_sendmsg(sk, msg, len);\n\t\t}\n\t}\n\n\tif (up->pending == AF_INET)\n\t\treturn udp_sendmsg(sk, msg, len);\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t   */\n\tif (len > INT_MAX - sizeof(struct udphdr))\n\t\treturn -EMSGSIZE;\n\n\tgetfrag  =  is_udplite ?  udplite_getfrag : ip_generic_getfrag;\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET6)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t\t}\n\t\t\tdst = NULL;\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (sin6) {\n\t\tif (sin6->sin6_port == 0)\n\t\t\treturn -EINVAL;\n\n\t\tfl6.fl6_dport = sin6->sin6_port;\n\t\tdaddr = &sin6->sin6_addr;\n\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tconnected = 1;\n\t}\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->sticky_pktinfo.ipi6_ifindex;\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(*opt);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t\tconnected = 0;\n\t}\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\tif (final_p)\n\t\tconnected = 0;\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr)) {\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\t\tconnected = 0;\n\t} else if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_sk_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\t/* Lockless fast path for the non-corking case */\n\tif (!corkreq) {\n\t\tstruct sk_buff *skb;\n\n\t\tskb = ip6_make_skb(sk, getfrag, msg, ulen,\n\t\t\t\t   sizeof(struct udphdr), hlimit, tclass, opt,\n\t\t\t\t   &fl6, (struct rt6_info *)dst,\n\t\t\t\t   msg->msg_flags, dontfrag);\n\t\terr = PTR_ERR(skb);\n\t\tif (!IS_ERR_OR_NULL(skb))\n\t\t\terr = udp_v6_send_skb(skb, &fl6);\n\t\tgoto release_dst;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tnet_dbg_ratelimited(\"udp cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tup->pending = AF_INET6;\n\ndo_append_data:\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\tup->len += ulen;\n\terr = ip6_append_data(sk, getfrag, msg, ulen,\n\t\tsizeof(struct udphdr), hlimit, tclass, opt, &fl6,\n\t\t(struct rt6_info *)dst,\n\t\tcorkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags, dontfrag);\n\tif (err)\n\t\tudp_v6_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_v6_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\n\tif (err > 0)\n\t\terr = np->recverr ? net_xmit_errno(err) : 0;\n\trelease_sock(sk);\n\nrelease_dst:\n\tif (dst) {\n\t\tif (connected) {\n\t\t\tip6_dst_store(sk, dst,\n\t\t\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t\t\t      &np->saddr :\n#endif\n\t\t\t\t      NULL);\n\t\t} else {\n\t\t\tdst_release(dst);\n\t\t}\n\t\tdst = NULL;\n\t}\n\nout:\n\tdst_release(dst);\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t*daddr, *final_p, final;\n\tstruct dst_entry\t*dst;\n\tstruct flowi6\t\tfl6;\n\tstruct ip6_flowlabel\t*flowlabel = NULL;\n\tstruct ipv6_txoptions\t*opt;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tusin->sin6_addr.s6_addr[15] = 0x01;\n\t}\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6.flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet->inet_dport;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tif (!fl6.flowi6_oif && (addr_type&IPV6_ADDR_MULTICAST))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\topt = flowlabel ? flowlabel->opt : np->opt;\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\terr = 0;\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\t/* source address lookup done in ip6_dst_lookup */\n\n\tif (ipv6_addr_any(&np->saddr))\n\t\tnp->saddr = fl6.saddr;\n\n\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tsk->sk_v6_rcv_saddr = fl6.saddr;\n\t\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\t\tif (sk->sk_prot->rehash)\n\t\t\tsk->sk_prot->rehash(sk);\n\t}\n\n\tip6_dst_store(sk, dst,\n\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t      &np->saddr :\n#endif\n\t\t      NULL);\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t*daddr, *final_p, final;\n\tstruct dst_entry\t*dst;\n\tstruct flowi6\t\tfl6;\n\tstruct ip6_flowlabel\t*flowlabel = NULL;\n\tstruct ipv6_txoptions\t*opt;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tusin->sin6_addr.s6_addr[15] = 0x01;\n\t}\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6.flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet->inet_dport;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tif (!fl6.flowi6_oif && (addr_type&IPV6_ADDR_MULTICAST))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\trcu_read_lock();\n\topt = flowlabel ? flowlabel->opt : rcu_dereference(np->opt);\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\trcu_read_unlock();\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\terr = 0;\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\t/* source address lookup done in ip6_dst_lookup */\n\n\tif (ipv6_addr_any(&np->saddr))\n\t\tnp->saddr = fl6.saddr;\n\n\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tsk->sk_v6_rcv_saddr = fl6.saddr;\n\t\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\t\tif (sk->sk_prot->rehash)\n\t\t\tsk->sk_prot->rehash(sk);\n\t}\n\n\tip6_dst_store(sk, dst,\n\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t      &np->saddr :\n#endif\n\t\t      NULL);\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t  int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\n\t/*\n\t *\tconnect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 0x1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type&IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp &&\n\t    !ipv6_addr_equal(&sk->sk_v6_daddr, &usin->sin6_addr)) {\n\t\ttp->rx_opt.ts_recent = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq = 0;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t *\tTCP over IPv4\n\t */\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &ipv6_mapped;\n\t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\ttp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\terr = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &ipv6_specific;\n\t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (!saddr) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp &&\n\t    ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr))\n\t\ttcp_fetch_timewait_stamp(sk, dst);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (np->opt)\n\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n\t\t\t\t\t  np->opt->opt_nflen);\n\n\ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet6_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tsk_set_txhash(sk);\n\n\tif (!tp->write_seq && likely(!tp->repair))\n\t\ttp->write_seq = secure_tcpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t\t     sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t\t     inet->inet_sport,\n\t\t\t\t\t\t\t     inet->inet_dport);\n\n\terr = tcp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\ttcp_set_state(sk, TCP_CLOSE);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
                        "code_after_change": "static int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t  int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct ipv6_txoptions *opt;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\n\t/*\n\t *\tconnect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 0x1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type&IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp &&\n\t    !ipv6_addr_equal(&sk->sk_v6_daddr, &usin->sin6_addr)) {\n\t\ttp->rx_opt.ts_recent = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq = 0;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t *\tTCP over IPv4\n\t */\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &ipv6_mapped;\n\t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\ttp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\terr = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &ipv6_specific;\n\t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (!saddr) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp &&\n\t    ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr))\n\t\ttcp_fetch_timewait_stamp(sk, dst);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len = opt->opt_flen +\n\t\t\t\t\t opt->opt_nflen;\n\n\ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet6_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tsk_set_txhash(sk);\n\n\tif (!tp->write_seq && likely(!tp->repair))\n\t\ttp->write_seq = secure_tcpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t\t     sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t\t     inet->inet_sport,\n\t\t\t\t\t\t\t     inet->inet_dport);\n\n\terr = tcp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\ttcp_set_state(sk, TCP_CLOSE);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nint udpv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_len = msg->msg_namelen;\n\tint ulen = len;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint err;\n\tint connected = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\n\t/* destination address check */\n\tif (sin6) {\n\t\tif (addr_len < offsetof(struct sockaddr, sa_data))\n\t\t\treturn -EINVAL;\n\n\t\tswitch (sin6->sin6_family) {\n\t\tcase AF_INET6:\n\t\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\t\treturn -EINVAL;\n\t\t\tdaddr = &sin6->sin6_addr;\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tgoto do_udp_sendmsg;\n\t\tcase AF_UNSPEC:\n\t\t\tmsg->msg_name = sin6 = NULL;\n\t\t\tmsg->msg_namelen = addr_len = 0;\n\t\t\tdaddr = NULL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (!up->pending) {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t} else\n\t\tdaddr = NULL;\n\n\tif (daddr) {\n\t\tif (ipv6_addr_v4mapped(daddr)) {\n\t\t\tstruct sockaddr_in sin;\n\t\t\tsin.sin_family = AF_INET;\n\t\t\tsin.sin_port = sin6 ? sin6->sin6_port : inet->inet_dport;\n\t\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\t\tmsg->msg_name = &sin;\n\t\t\tmsg->msg_namelen = sizeof(sin);\ndo_udp_sendmsg:\n\t\t\tif (__ipv6_only_sock(sk))\n\t\t\t\treturn -ENETUNREACH;\n\t\t\treturn udp_sendmsg(sk, msg, len);\n\t\t}\n\t}\n\n\tif (up->pending == AF_INET)\n\t\treturn udp_sendmsg(sk, msg, len);\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t   */\n\tif (len > INT_MAX - sizeof(struct udphdr))\n\t\treturn -EMSGSIZE;\n\n\tgetfrag  =  is_udplite ?  udplite_getfrag : ip_generic_getfrag;\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET6)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t\t}\n\t\t\tdst = NULL;\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (sin6) {\n\t\tif (sin6->sin6_port == 0)\n\t\t\treturn -EINVAL;\n\n\t\tfl6.fl6_dport = sin6->sin6_port;\n\t\tdaddr = &sin6->sin6_addr;\n\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tconnected = 1;\n\t}\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->sticky_pktinfo.ipi6_ifindex;\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(*opt);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t\tconnected = 0;\n\t}\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\tif (final_p)\n\t\tconnected = 0;\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr)) {\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\t\tconnected = 0;\n\t} else if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_sk_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\t/* Lockless fast path for the non-corking case */\n\tif (!corkreq) {\n\t\tstruct sk_buff *skb;\n\n\t\tskb = ip6_make_skb(sk, getfrag, msg, ulen,\n\t\t\t\t   sizeof(struct udphdr), hlimit, tclass, opt,\n\t\t\t\t   &fl6, (struct rt6_info *)dst,\n\t\t\t\t   msg->msg_flags, dontfrag);\n\t\terr = PTR_ERR(skb);\n\t\tif (!IS_ERR_OR_NULL(skb))\n\t\t\terr = udp_v6_send_skb(skb, &fl6);\n\t\tgoto release_dst;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tnet_dbg_ratelimited(\"udp cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tup->pending = AF_INET6;\n\ndo_append_data:\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\tup->len += ulen;\n\terr = ip6_append_data(sk, getfrag, msg, ulen,\n\t\tsizeof(struct udphdr), hlimit, tclass, opt, &fl6,\n\t\t(struct rt6_info *)dst,\n\t\tcorkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags, dontfrag);\n\tif (err)\n\t\tudp_v6_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_v6_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\n\tif (err > 0)\n\t\terr = np->recverr ? net_xmit_errno(err) : 0;\n\trelease_sock(sk);\n\nrelease_dst:\n\tif (dst) {\n\t\tif (connected) {\n\t\t\tip6_dst_store(sk, dst,\n\t\t\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t\t\t      &np->saddr :\n#endif\n\t\t\t\t      NULL);\n\t\t} else {\n\t\t\tdst_release(dst);\n\t\t}\n\t\tdst = NULL;\n\t}\n\nout:\n\tdst_release(dst);\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t  int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\n\t/*\n\t *\tconnect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 0x1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type&IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp &&\n\t    !ipv6_addr_equal(&sk->sk_v6_daddr, &usin->sin6_addr)) {\n\t\ttp->rx_opt.ts_recent = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq = 0;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t *\tTCP over IPv4\n\t */\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &ipv6_mapped;\n\t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\ttp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\terr = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &ipv6_specific;\n\t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (!saddr) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp &&\n\t    ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr))\n\t\ttcp_fetch_timewait_stamp(sk, dst);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (np->opt)\n\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n\t\t\t\t\t  np->opt->opt_nflen);\n\n\ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet6_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tsk_set_txhash(sk);\n\n\tif (!tp->write_seq && likely(!tp->repair))\n\t\ttp->write_seq = secure_tcpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t\t     sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t\t     inet->inet_sport,\n\t\t\t\t\t\t\t     inet->inet_dport);\n\n\terr = tcp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\ttcp_set_state(sk, TCP_CLOSE);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t  int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct ipv6_txoptions *opt;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\n\t/*\n\t *\tconnect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 0x1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type&IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp &&\n\t    !ipv6_addr_equal(&sk->sk_v6_daddr, &usin->sin6_addr)) {\n\t\ttp->rx_opt.ts_recent = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq = 0;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t *\tTCP over IPv4\n\t */\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &ipv6_mapped;\n\t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\ttp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\terr = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &ipv6_specific;\n\t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (!saddr) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp &&\n\t    ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr))\n\t\ttcp_fetch_timewait_stamp(sk, dst);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len = opt->opt_flen +\n\t\t\t\t\t opt->opt_nflen;\n\n\ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet6_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tsk_set_txhash(sk);\n\n\tif (!tp->write_seq && likely(!tp->repair))\n\t\ttp->write_seq = secure_tcpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t\t     sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t\t     inet->inet_sport,\n\t\t\t\t\t\t\t     inet->inet_dport);\n\n\terr = tcp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\ttcp_set_state(sk, TCP_CLOSE);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 452,
            "cve_id": "CVE-2014-0181",
            "code_snippet": "static int addr_doit(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct ifaddrmsg *ifm;\n\tint err;\n\tu8 pnaddr;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!netlink_capable(skb, CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tASSERT_RTNL();\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, ifa_phonet_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tifm = nlmsg_data(nlh);\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\tpnaddr = nla_get_u8(tb[IFA_LOCAL]);\n\tif (pnaddr & 3)\n\t\t/* Phonet addresses only have 6 high-order bits */\n\t\treturn -EINVAL;\n\n\tdev = __dev_get_by_index(net, ifm->ifa_index);\n\tif (dev == NULL)\n\t\treturn -ENODEV;\n\n\tif (nlh->nlmsg_type == RTM_NEWADDR)\n\t\terr = phonet_address_add(dev, pnaddr);\n\telse\n\t\terr = phonet_address_del(dev, pnaddr);\n\tif (!err)\n\t\tphonet_address_notify(nlh->nlmsg_type, dev, pnaddr);\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}",
                        "code_after_change": "static int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int addr_doit(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct ifaddrmsg *ifm;\n\tint err;\n\tu8 pnaddr;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!netlink_capable(skb, CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tASSERT_RTNL();\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, ifa_phonet_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tifm = nlmsg_data(nlh);\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\tpnaddr = nla_get_u8(tb[IFA_LOCAL]);\n\tif (pnaddr & 3)\n\t\t/* Phonet addresses only have 6 high-order bits */\n\t\treturn -EINVAL;\n\n\tdev = __dev_get_by_index(net, ifm->ifa_index);\n\tif (dev == NULL)\n\t\treturn -ENODEV;\n\n\tif (nlh->nlmsg_type == RTM_NEWADDR)\n\t\terr = phonet_address_add(dev, pnaddr);\n\telse\n\t\terr = phonet_address_del(dev, pnaddr);\n\tif (!err)\n\t\tphonet_address_notify(nlh->nlmsg_type, dev, pnaddr);\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}",
                        "code_after_change": "static int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int addr_doit(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct ifaddrmsg *ifm;\n\tint err;\n\tu8 pnaddr;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!netlink_capable(skb, CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tASSERT_RTNL();\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, ifa_phonet_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tifm = nlmsg_data(nlh);\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\tpnaddr = nla_get_u8(tb[IFA_LOCAL]);\n\tif (pnaddr & 3)\n\t\t/* Phonet addresses only have 6 high-order bits */\n\t\treturn -EINVAL;\n\n\tdev = __dev_get_by_index(net, ifm->ifa_index);\n\tif (dev == NULL)\n\t\treturn -ENODEV;\n\n\tif (nlh->nlmsg_type == RTM_NEWADDR)\n\t\terr = phonet_address_add(dev, pnaddr);\n\telse\n\t\terr = phonet_address_del(dev, pnaddr);\n\tif (!err)\n\t\tphonet_address_notify(nlh->nlmsg_type, dev, pnaddr);\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}",
                        "code_after_change": "static int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(skb, net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int addr_doit(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct ifaddrmsg *ifm;\n\tint err;\n\tu8 pnaddr;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!netlink_capable(skb, CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tASSERT_RTNL();\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, ifa_phonet_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tifm = nlmsg_data(nlh);\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\tpnaddr = nla_get_u8(tb[IFA_LOCAL]);\n\tif (pnaddr & 3)\n\t\t/* Phonet addresses only have 6 high-order bits */\n\t\treturn -EINVAL;\n\n\tdev = __dev_get_by_index(net, ifm->ifa_index);\n\tif (dev == NULL)\n\t\treturn -ENODEV;\n\n\tif (nlh->nlmsg_type == RTM_NEWADDR)\n\t\terr = phonet_address_add(dev, pnaddr);\n\telse\n\t\terr = phonet_address_del(dev, pnaddr);\n\tif (!err)\n\t\tphonet_address_notify(nlh->nlmsg_type, dev, pnaddr);\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(skb, net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 441,
            "cve_id": "CVE-2014-0181",
            "code_snippet": "static int rtnl_fdb_del(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct ndmsg *ndm;\n\tstruct nlattr *tb[NDA_MAX+1];\n\tstruct net_device *dev;\n\tint err = -EINVAL;\n\t__u8 *addr;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(nlh, sizeof(*ndm), tb, NDA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tndm = nlmsg_data(nlh);\n\tif (ndm->ndm_ifindex == 0) {\n\t\tpr_info(\"PF_BRIDGE: RTM_DELNEIGH with invalid ifindex\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tdev = __dev_get_by_index(net, ndm->ndm_ifindex);\n\tif (dev == NULL) {\n\t\tpr_info(\"PF_BRIDGE: RTM_DELNEIGH with unknown ifindex\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (!tb[NDA_LLADDR] || nla_len(tb[NDA_LLADDR]) != ETH_ALEN) {\n\t\tpr_info(\"PF_BRIDGE: RTM_DELNEIGH with invalid address\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\taddr = nla_data(tb[NDA_LLADDR]);\n\n\terr = -EOPNOTSUPP;\n\n\t/* Support fdb on master device the net/bridge default case */\n\tif ((!ndm->ndm_flags || ndm->ndm_flags & NTF_MASTER) &&\n\t    (dev->priv_flags & IFF_BRIDGE_PORT)) {\n\t\tstruct net_device *br_dev = netdev_master_upper_dev_get(dev);\n\t\tconst struct net_device_ops *ops = br_dev->netdev_ops;\n\n\t\tif (ops->ndo_fdb_del)\n\t\t\terr = ops->ndo_fdb_del(ndm, tb, dev, addr);\n\n\t\tif (err)\n\t\t\tgoto out;\n\t\telse\n\t\t\tndm->ndm_flags &= ~NTF_MASTER;\n\t}\n\n\t/* Embedded bridge, macvlan, and any other device support */\n\tif (ndm->ndm_flags & NTF_SELF) {\n\t\tif (dev->netdev_ops->ndo_fdb_del)\n\t\t\terr = dev->netdev_ops->ndo_fdb_del(ndm, tb, dev, addr);\n\t\telse\n\t\t\terr = ndo_dflt_fdb_del(ndm, tb, dev, addr);\n\n\t\tif (!err) {\n\t\t\trtnl_fdb_notify(dev, addr, RTM_DELNEIGH);\n\t\t\tndm->ndm_flags &= ~NTF_SELF;\n\t\t}\n\t}\nout:\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}",
                        "code_after_change": "static int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(skb, net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int rtnl_fdb_del(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct ndmsg *ndm;\n\tstruct nlattr *tb[NDA_MAX+1];\n\tstruct net_device *dev;\n\tint err = -EINVAL;\n\t__u8 *addr;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(nlh, sizeof(*ndm), tb, NDA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tndm = nlmsg_data(nlh);\n\tif (ndm->ndm_ifindex == 0) {\n\t\tpr_info(\"PF_BRIDGE: RTM_DELNEIGH with invalid ifindex\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tdev = __dev_get_by_index(net, ndm->ndm_ifindex);\n\tif (dev == NULL) {\n\t\tpr_info(\"PF_BRIDGE: RTM_DELNEIGH with unknown ifindex\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (!tb[NDA_LLADDR] || nla_len(tb[NDA_LLADDR]) != ETH_ALEN) {\n\t\tpr_info(\"PF_BRIDGE: RTM_DELNEIGH with invalid address\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\taddr = nla_data(tb[NDA_LLADDR]);\n\n\terr = -EOPNOTSUPP;\n\n\t/* Support fdb on master device the net/bridge default case */\n\tif ((!ndm->ndm_flags || ndm->ndm_flags & NTF_MASTER) &&\n\t    (dev->priv_flags & IFF_BRIDGE_PORT)) {\n\t\tstruct net_device *br_dev = netdev_master_upper_dev_get(dev);\n\t\tconst struct net_device_ops *ops = br_dev->netdev_ops;\n\n\t\tif (ops->ndo_fdb_del)\n\t\t\terr = ops->ndo_fdb_del(ndm, tb, dev, addr);\n\n\t\tif (err)\n\t\t\tgoto out;\n\t\telse\n\t\t\tndm->ndm_flags &= ~NTF_MASTER;\n\t}\n\n\t/* Embedded bridge, macvlan, and any other device support */\n\tif (ndm->ndm_flags & NTF_SELF) {\n\t\tif (dev->netdev_ops->ndo_fdb_del)\n\t\t\terr = dev->netdev_ops->ndo_fdb_del(ndm, tb, dev, addr);\n\t\telse\n\t\t\terr = ndo_dflt_fdb_del(ndm, tb, dev, addr);\n\n\t\tif (!err) {\n\t\t\trtnl_fdb_notify(dev, addr, RTM_DELNEIGH);\n\t\t\tndm->ndm_flags &= ~NTF_SELF;\n\t\t}\n\t}\nout:\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(skb, net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}",
                        "code_after_change": "static int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int rtnl_fdb_del(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct ndmsg *ndm;\n\tstruct nlattr *tb[NDA_MAX+1];\n\tstruct net_device *dev;\n\tint err = -EINVAL;\n\t__u8 *addr;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(nlh, sizeof(*ndm), tb, NDA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tndm = nlmsg_data(nlh);\n\tif (ndm->ndm_ifindex == 0) {\n\t\tpr_info(\"PF_BRIDGE: RTM_DELNEIGH with invalid ifindex\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tdev = __dev_get_by_index(net, ndm->ndm_ifindex);\n\tif (dev == NULL) {\n\t\tpr_info(\"PF_BRIDGE: RTM_DELNEIGH with unknown ifindex\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (!tb[NDA_LLADDR] || nla_len(tb[NDA_LLADDR]) != ETH_ALEN) {\n\t\tpr_info(\"PF_BRIDGE: RTM_DELNEIGH with invalid address\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\taddr = nla_data(tb[NDA_LLADDR]);\n\n\terr = -EOPNOTSUPP;\n\n\t/* Support fdb on master device the net/bridge default case */\n\tif ((!ndm->ndm_flags || ndm->ndm_flags & NTF_MASTER) &&\n\t    (dev->priv_flags & IFF_BRIDGE_PORT)) {\n\t\tstruct net_device *br_dev = netdev_master_upper_dev_get(dev);\n\t\tconst struct net_device_ops *ops = br_dev->netdev_ops;\n\n\t\tif (ops->ndo_fdb_del)\n\t\t\terr = ops->ndo_fdb_del(ndm, tb, dev, addr);\n\n\t\tif (err)\n\t\t\tgoto out;\n\t\telse\n\t\t\tndm->ndm_flags &= ~NTF_MASTER;\n\t}\n\n\t/* Embedded bridge, macvlan, and any other device support */\n\tif (ndm->ndm_flags & NTF_SELF) {\n\t\tif (dev->netdev_ops->ndo_fdb_del)\n\t\t\terr = dev->netdev_ops->ndo_fdb_del(ndm, tb, dev, addr);\n\t\telse\n\t\t\terr = ndo_dflt_fdb_del(ndm, tb, dev, addr);\n\n\t\tif (!err) {\n\t\t\trtnl_fdb_notify(dev, addr, RTM_DELNEIGH);\n\t\t\tndm->ndm_flags &= ~NTF_SELF;\n\t\t}\n\t}\nout:\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}",
                        "code_after_change": "static int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int rtnl_fdb_del(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct ndmsg *ndm;\n\tstruct nlattr *tb[NDA_MAX+1];\n\tstruct net_device *dev;\n\tint err = -EINVAL;\n\t__u8 *addr;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(nlh, sizeof(*ndm), tb, NDA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tndm = nlmsg_data(nlh);\n\tif (ndm->ndm_ifindex == 0) {\n\t\tpr_info(\"PF_BRIDGE: RTM_DELNEIGH with invalid ifindex\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tdev = __dev_get_by_index(net, ndm->ndm_ifindex);\n\tif (dev == NULL) {\n\t\tpr_info(\"PF_BRIDGE: RTM_DELNEIGH with unknown ifindex\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (!tb[NDA_LLADDR] || nla_len(tb[NDA_LLADDR]) != ETH_ALEN) {\n\t\tpr_info(\"PF_BRIDGE: RTM_DELNEIGH with invalid address\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\taddr = nla_data(tb[NDA_LLADDR]);\n\n\terr = -EOPNOTSUPP;\n\n\t/* Support fdb on master device the net/bridge default case */\n\tif ((!ndm->ndm_flags || ndm->ndm_flags & NTF_MASTER) &&\n\t    (dev->priv_flags & IFF_BRIDGE_PORT)) {\n\t\tstruct net_device *br_dev = netdev_master_upper_dev_get(dev);\n\t\tconst struct net_device_ops *ops = br_dev->netdev_ops;\n\n\t\tif (ops->ndo_fdb_del)\n\t\t\terr = ops->ndo_fdb_del(ndm, tb, dev, addr);\n\n\t\tif (err)\n\t\t\tgoto out;\n\t\telse\n\t\t\tndm->ndm_flags &= ~NTF_MASTER;\n\t}\n\n\t/* Embedded bridge, macvlan, and any other device support */\n\tif (ndm->ndm_flags & NTF_SELF) {\n\t\tif (dev->netdev_ops->ndo_fdb_del)\n\t\t\terr = dev->netdev_ops->ndo_fdb_del(ndm, tb, dev, addr);\n\t\telse\n\t\t\terr = ndo_dflt_fdb_del(ndm, tb, dev, addr);\n\n\t\tif (!err) {\n\t\t\trtnl_fdb_notify(dev, addr, RTM_DELNEIGH);\n\t\t\tndm->ndm_flags &= ~NTF_SELF;\n\t\t}\n\t}\nout:\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 443,
            "cve_id": "CVE-2014-0181",
            "code_snippet": "static int dcb_doit(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct net_device *netdev;\n\tstruct dcbmsg *dcb = nlmsg_data(nlh);\n\tstruct nlattr *tb[DCB_ATTR_MAX + 1];\n\tu32 portid = skb ? NETLINK_CB(skb).portid : 0;\n\tint ret = -EINVAL;\n\tstruct sk_buff *reply_skb;\n\tstruct nlmsghdr *reply_nlh = NULL;\n\tconst struct reply_func *fn;\n\n\tif ((nlh->nlmsg_type == RTM_SETDCB) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tret = nlmsg_parse(nlh, sizeof(*dcb), tb, DCB_ATTR_MAX,\n\t\t\t  dcbnl_rtnl_policy);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (dcb->cmd > DCB_CMD_MAX)\n\t\treturn -EINVAL;\n\n\t/* check if a reply function has been defined for the command */\n\tfn = &reply_funcs[dcb->cmd];\n\tif (!fn->cb)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!tb[DCB_ATTR_IFNAME])\n\t\treturn -EINVAL;\n\n\tnetdev = __dev_get_by_name(net, nla_data(tb[DCB_ATTR_IFNAME]));\n\tif (!netdev)\n\t\treturn -ENODEV;\n\n\tif (!netdev->dcbnl_ops)\n\t\treturn -EOPNOTSUPP;\n\n\treply_skb = dcbnl_newmsg(fn->type, dcb->cmd, portid, nlh->nlmsg_seq,\n\t\t\t\t nlh->nlmsg_flags, &reply_nlh);\n\tif (!reply_skb)\n\t\treturn -ENOBUFS;\n\n\tret = fn->cb(netdev, nlh, nlh->nlmsg_seq, tb, reply_skb);\n\tif (ret < 0) {\n\t\tnlmsg_free(reply_skb);\n\t\tgoto out;\n\t}\n\n\tnlmsg_end(reply_skb, reply_nlh);\n\n\tret = rtnl_unicast(reply_skb, net, portid);\nout:\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}",
                        "code_after_change": "static int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int dcb_doit(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct net_device *netdev;\n\tstruct dcbmsg *dcb = nlmsg_data(nlh);\n\tstruct nlattr *tb[DCB_ATTR_MAX + 1];\n\tu32 portid = skb ? NETLINK_CB(skb).portid : 0;\n\tint ret = -EINVAL;\n\tstruct sk_buff *reply_skb;\n\tstruct nlmsghdr *reply_nlh = NULL;\n\tconst struct reply_func *fn;\n\n\tif ((nlh->nlmsg_type == RTM_SETDCB) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tret = nlmsg_parse(nlh, sizeof(*dcb), tb, DCB_ATTR_MAX,\n\t\t\t  dcbnl_rtnl_policy);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (dcb->cmd > DCB_CMD_MAX)\n\t\treturn -EINVAL;\n\n\t/* check if a reply function has been defined for the command */\n\tfn = &reply_funcs[dcb->cmd];\n\tif (!fn->cb)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!tb[DCB_ATTR_IFNAME])\n\t\treturn -EINVAL;\n\n\tnetdev = __dev_get_by_name(net, nla_data(tb[DCB_ATTR_IFNAME]));\n\tif (!netdev)\n\t\treturn -ENODEV;\n\n\tif (!netdev->dcbnl_ops)\n\t\treturn -EOPNOTSUPP;\n\n\treply_skb = dcbnl_newmsg(fn->type, dcb->cmd, portid, nlh->nlmsg_seq,\n\t\t\t\t nlh->nlmsg_flags, &reply_nlh);\n\tif (!reply_skb)\n\t\treturn -ENOBUFS;\n\n\tret = fn->cb(netdev, nlh, nlh->nlmsg_seq, tb, reply_skb);\n\tif (ret < 0) {\n\t\tnlmsg_free(reply_skb);\n\t\tgoto out;\n\t}\n\n\tnlmsg_end(reply_skb, reply_nlh);\n\n\tret = rtnl_unicast(reply_skb, net, portid);\nout:\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
                        "code_after_change": "static int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int dcb_doit(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct net_device *netdev;\n\tstruct dcbmsg *dcb = nlmsg_data(nlh);\n\tstruct nlattr *tb[DCB_ATTR_MAX + 1];\n\tu32 portid = skb ? NETLINK_CB(skb).portid : 0;\n\tint ret = -EINVAL;\n\tstruct sk_buff *reply_skb;\n\tstruct nlmsghdr *reply_nlh = NULL;\n\tconst struct reply_func *fn;\n\n\tif ((nlh->nlmsg_type == RTM_SETDCB) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tret = nlmsg_parse(nlh, sizeof(*dcb), tb, DCB_ATTR_MAX,\n\t\t\t  dcbnl_rtnl_policy);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (dcb->cmd > DCB_CMD_MAX)\n\t\treturn -EINVAL;\n\n\t/* check if a reply function has been defined for the command */\n\tfn = &reply_funcs[dcb->cmd];\n\tif (!fn->cb)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!tb[DCB_ATTR_IFNAME])\n\t\treturn -EINVAL;\n\n\tnetdev = __dev_get_by_name(net, nla_data(tb[DCB_ATTR_IFNAME]));\n\tif (!netdev)\n\t\treturn -ENODEV;\n\n\tif (!netdev->dcbnl_ops)\n\t\treturn -EOPNOTSUPP;\n\n\treply_skb = dcbnl_newmsg(fn->type, dcb->cmd, portid, nlh->nlmsg_seq,\n\t\t\t\t nlh->nlmsg_flags, &reply_nlh);\n\tif (!reply_skb)\n\t\treturn -ENOBUFS;\n\n\tret = fn->cb(netdev, nlh, nlh->nlmsg_seq, tb, reply_skb);\n\tif (ret < 0) {\n\t\tnlmsg_free(reply_skb);\n\t\tgoto out;\n\t}\n\n\tnlmsg_end(reply_skb, reply_nlh);\n\n\tret = rtnl_unicast(reply_skb, net, portid);\nout:\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}",
                        "code_after_change": "static int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int dcb_doit(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct net_device *netdev;\n\tstruct dcbmsg *dcb = nlmsg_data(nlh);\n\tstruct nlattr *tb[DCB_ATTR_MAX + 1];\n\tu32 portid = skb ? NETLINK_CB(skb).portid : 0;\n\tint ret = -EINVAL;\n\tstruct sk_buff *reply_skb;\n\tstruct nlmsghdr *reply_nlh = NULL;\n\tconst struct reply_func *fn;\n\n\tif ((nlh->nlmsg_type == RTM_SETDCB) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tret = nlmsg_parse(nlh, sizeof(*dcb), tb, DCB_ATTR_MAX,\n\t\t\t  dcbnl_rtnl_policy);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (dcb->cmd > DCB_CMD_MAX)\n\t\treturn -EINVAL;\n\n\t/* check if a reply function has been defined for the command */\n\tfn = &reply_funcs[dcb->cmd];\n\tif (!fn->cb)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!tb[DCB_ATTR_IFNAME])\n\t\treturn -EINVAL;\n\n\tnetdev = __dev_get_by_name(net, nla_data(tb[DCB_ATTR_IFNAME]));\n\tif (!netdev)\n\t\treturn -ENODEV;\n\n\tif (!netdev->dcbnl_ops)\n\t\treturn -EOPNOTSUPP;\n\n\treply_skb = dcbnl_newmsg(fn->type, dcb->cmd, portid, nlh->nlmsg_seq,\n\t\t\t\t nlh->nlmsg_flags, &reply_nlh);\n\tif (!reply_skb)\n\t\treturn -ENOBUFS;\n\n\tret = fn->cb(netdev, nlh, nlh->nlmsg_seq, tb, reply_skb);\n\tif (ret < 0) {\n\t\tnlmsg_free(reply_skb);\n\t\tgoto out;\n\t}\n\n\tnlmsg_end(reply_skb, reply_nlh);\n\n\tret = rtnl_unicast(reply_skb, net, portid);\nout:\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 453,
            "cve_id": "CVE-2014-0181",
            "code_snippet": "static int route_doit(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[RTA_MAX+1];\n\tstruct net_device *dev;\n\tstruct rtmsg *rtm;\n\tint err;\n\tu8 dst;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!netlink_capable(skb, CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tASSERT_RTNL();\n\n\terr = nlmsg_parse(nlh, sizeof(*rtm), tb, RTA_MAX, rtm_phonet_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\trtm = nlmsg_data(nlh);\n\tif (rtm->rtm_table != RT_TABLE_MAIN || rtm->rtm_type != RTN_UNICAST)\n\t\treturn -EINVAL;\n\tif (tb[RTA_DST] == NULL || tb[RTA_OIF] == NULL)\n\t\treturn -EINVAL;\n\tdst = nla_get_u8(tb[RTA_DST]);\n\tif (dst & 3) /* Phonet addresses only have 6 high-order bits */\n\t\treturn -EINVAL;\n\n\tdev = __dev_get_by_index(net, nla_get_u32(tb[RTA_OIF]));\n\tif (dev == NULL)\n\t\treturn -ENODEV;\n\n\tif (nlh->nlmsg_type == RTM_NEWROUTE)\n\t\terr = phonet_route_add(dev, dst);\n\telse\n\t\terr = phonet_route_del(dev, dst);\n\tif (!err)\n\t\trtm_phonet_notify(nlh->nlmsg_type, dev, dst);\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}",
                        "code_after_change": "static int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int route_doit(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[RTA_MAX+1];\n\tstruct net_device *dev;\n\tstruct rtmsg *rtm;\n\tint err;\n\tu8 dst;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!netlink_capable(skb, CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tASSERT_RTNL();\n\n\terr = nlmsg_parse(nlh, sizeof(*rtm), tb, RTA_MAX, rtm_phonet_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\trtm = nlmsg_data(nlh);\n\tif (rtm->rtm_table != RT_TABLE_MAIN || rtm->rtm_type != RTN_UNICAST)\n\t\treturn -EINVAL;\n\tif (tb[RTA_DST] == NULL || tb[RTA_OIF] == NULL)\n\t\treturn -EINVAL;\n\tdst = nla_get_u8(tb[RTA_DST]);\n\tif (dst & 3) /* Phonet addresses only have 6 high-order bits */\n\t\treturn -EINVAL;\n\n\tdev = __dev_get_by_index(net, nla_get_u32(tb[RTA_OIF]));\n\tif (dev == NULL)\n\t\treturn -ENODEV;\n\n\tif (nlh->nlmsg_type == RTM_NEWROUTE)\n\t\terr = phonet_route_add(dev, dst);\n\telse\n\t\terr = phonet_route_del(dev, dst);\n\tif (!err)\n\t\trtm_phonet_notify(nlh->nlmsg_type, dev, dst);\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
                        "code_after_change": "static int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int route_doit(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[RTA_MAX+1];\n\tstruct net_device *dev;\n\tstruct rtmsg *rtm;\n\tint err;\n\tu8 dst;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!netlink_capable(skb, CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tASSERT_RTNL();\n\n\terr = nlmsg_parse(nlh, sizeof(*rtm), tb, RTA_MAX, rtm_phonet_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\trtm = nlmsg_data(nlh);\n\tif (rtm->rtm_table != RT_TABLE_MAIN || rtm->rtm_type != RTN_UNICAST)\n\t\treturn -EINVAL;\n\tif (tb[RTA_DST] == NULL || tb[RTA_OIF] == NULL)\n\t\treturn -EINVAL;\n\tdst = nla_get_u8(tb[RTA_DST]);\n\tif (dst & 3) /* Phonet addresses only have 6 high-order bits */\n\t\treturn -EINVAL;\n\n\tdev = __dev_get_by_index(net, nla_get_u32(tb[RTA_OIF]));\n\tif (dev == NULL)\n\t\treturn -ENODEV;\n\n\tif (nlh->nlmsg_type == RTM_NEWROUTE)\n\t\terr = phonet_route_add(dev, dst);\n\telse\n\t\terr = phonet_route_del(dev, dst);\n\tif (!err)\n\t\trtm_phonet_notify(nlh->nlmsg_type, dev, dst);\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}",
                        "code_after_change": "static int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(skb, net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int route_doit(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[RTA_MAX+1];\n\tstruct net_device *dev;\n\tstruct rtmsg *rtm;\n\tint err;\n\tu8 dst;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!netlink_capable(skb, CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tASSERT_RTNL();\n\n\terr = nlmsg_parse(nlh, sizeof(*rtm), tb, RTA_MAX, rtm_phonet_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\trtm = nlmsg_data(nlh);\n\tif (rtm->rtm_table != RT_TABLE_MAIN || rtm->rtm_type != RTN_UNICAST)\n\t\treturn -EINVAL;\n\tif (tb[RTA_DST] == NULL || tb[RTA_OIF] == NULL)\n\t\treturn -EINVAL;\n\tdst = nla_get_u8(tb[RTA_DST]);\n\tif (dst & 3) /* Phonet addresses only have 6 high-order bits */\n\t\treturn -EINVAL;\n\n\tdev = __dev_get_by_index(net, nla_get_u32(tb[RTA_OIF]));\n\tif (dev == NULL)\n\t\treturn -ENODEV;\n\n\tif (nlh->nlmsg_type == RTM_NEWROUTE)\n\t\terr = phonet_route_add(dev, dst);\n\telse\n\t\terr = phonet_route_del(dev, dst);\n\tif (!err)\n\t\trtm_phonet_notify(nlh->nlmsg_type, dev, dst);\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(skb, net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 436,
            "cve_id": "CVE-2014-0181",
            "code_snippet": "static int audit_netlink_ok(struct sk_buff *skb, u16 msg_type)\n{\n\tint err = 0;\n\n\t/* Only support initial user namespace for now. */\n\t/*\n\t * We return ECONNREFUSED because it tricks userspace into thinking\n\t * that audit was not configured into the kernel.  Lots of users\n\t * configure their PAM stack (because that's what the distro does)\n\t * to reject login if unable to send messages to audit.  If we return\n\t * ECONNREFUSED the PAM stack thinks the kernel does not have audit\n\t * configured in and will let login proceed.  If we return EPERM\n\t * userspace will reject all logins.  This should be removed when we\n\t * support non init namespaces!!\n\t */\n\tif (current_user_ns() != &init_user_ns)\n\t\treturn -ECONNREFUSED;\n\n\tswitch (msg_type) {\n\tcase AUDIT_LIST:\n\tcase AUDIT_ADD:\n\tcase AUDIT_DEL:\n\t\treturn -EOPNOTSUPP;\n\tcase AUDIT_GET:\n\tcase AUDIT_SET:\n\tcase AUDIT_GET_FEATURE:\n\tcase AUDIT_SET_FEATURE:\n\tcase AUDIT_LIST_RULES:\n\tcase AUDIT_ADD_RULE:\n\tcase AUDIT_DEL_RULE:\n\tcase AUDIT_SIGNAL_INFO:\n\tcase AUDIT_TTY_GET:\n\tcase AUDIT_TTY_SET:\n\tcase AUDIT_TRIM:\n\tcase AUDIT_MAKE_EQUIV:\n\t\t/* Only support auditd and auditctl in initial pid namespace\n\t\t * for now. */\n\t\tif ((task_active_pid_ns(current) != &init_pid_ns))\n\t\t\treturn -EPERM;\n\n\t\tif (!netlink_capable(skb, CAP_AUDIT_CONTROL))\n\t\t\terr = -EPERM;\n\t\tbreak;\n\tcase AUDIT_USER:\n\tcase AUDIT_FIRST_USER_MSG ... AUDIT_LAST_USER_MSG:\n\tcase AUDIT_FIRST_USER_MSG2 ... AUDIT_LAST_USER_MSG2:\n\t\tif (!netlink_capable(skb, CAP_AUDIT_WRITE))\n\t\t\terr = -EPERM;\n\t\tbreak;\n\tdefault:  /* bad msg */\n\t\terr = -EINVAL;\n\t}\n\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static void cn_proc_mcast_ctl(struct cn_msg *msg,\n\t\t\t      struct netlink_skb_parms *nsp)\n{\n\tenum proc_cn_mcast_op *mc_op = NULL;\n\tint err = 0;\n\n\tif (msg->len != sizeof(*mc_op))\n\t\treturn;\n\n\t/* \n\t * Events are reported with respect to the initial pid\n\t * and user namespaces so ignore requestors from\n\t * other namespaces.\n\t */\n\tif ((current_user_ns() != &init_user_ns) ||\n\t    (task_active_pid_ns(current) != &init_pid_ns))\n\t\treturn;\n\n\t/* Can only change if privileged. */\n\tif (!capable(CAP_NET_ADMIN)) {\n\t\terr = EPERM;\n\t\tgoto out;\n\t}\n\n\tmc_op = (enum proc_cn_mcast_op *)msg->data;\n\tswitch (*mc_op) {\n\tcase PROC_CN_MCAST_LISTEN:\n\t\tatomic_inc(&proc_event_num_listeners);\n\t\tbreak;\n\tcase PROC_CN_MCAST_IGNORE:\n\t\tatomic_dec(&proc_event_num_listeners);\n\t\tbreak;\n\tdefault:\n\t\terr = EINVAL;\n\t\tbreak;\n\t}\n\nout:\n\tcn_proc_ack(err, msg->seq, msg->ack);\n}",
                        "code_after_change": "static void cn_proc_mcast_ctl(struct cn_msg *msg,\n\t\t\t      struct netlink_skb_parms *nsp)\n{\n\tenum proc_cn_mcast_op *mc_op = NULL;\n\tint err = 0;\n\n\tif (msg->len != sizeof(*mc_op))\n\t\treturn;\n\n\t/* \n\t * Events are reported with respect to the initial pid\n\t * and user namespaces so ignore requestors from\n\t * other namespaces.\n\t */\n\tif ((current_user_ns() != &init_user_ns) ||\n\t    (task_active_pid_ns(current) != &init_pid_ns))\n\t\treturn;\n\n\t/* Can only change if privileged. */\n\tif (!__netlink_ns_capable(nsp, &init_user_ns, CAP_NET_ADMIN)) {\n\t\terr = EPERM;\n\t\tgoto out;\n\t}\n\n\tmc_op = (enum proc_cn_mcast_op *)msg->data;\n\tswitch (*mc_op) {\n\tcase PROC_CN_MCAST_LISTEN:\n\t\tatomic_inc(&proc_event_num_listeners);\n\t\tbreak;\n\tcase PROC_CN_MCAST_IGNORE:\n\t\tatomic_dec(&proc_event_num_listeners);\n\t\tbreak;\n\tdefault:\n\t\terr = EINVAL;\n\t\tbreak;\n\t}\n\nout:\n\tcn_proc_ack(err, msg->seq, msg->ack);\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int audit_netlink_ok(struct sk_buff *skb, u16 msg_type)\n{\n\tint err = 0;\n\n\t/* Only support initial user namespace for now. */\n\t/*\n\t * We return ECONNREFUSED because it tricks userspace into thinking\n\t * that audit was not configured into the kernel.  Lots of users\n\t * configure their PAM stack (because that's what the distro does)\n\t * to reject login if unable to send messages to audit.  If we return\n\t * ECONNREFUSED the PAM stack thinks the kernel does not have audit\n\t * configured in and will let login proceed.  If we return EPERM\n\t * userspace will reject all logins.  This should be removed when we\n\t * support non init namespaces!!\n\t */\n\tif (current_user_ns() != &init_user_ns)\n\t\treturn -ECONNREFUSED;\n\n\tswitch (msg_type) {\n\tcase AUDIT_LIST:\n\tcase AUDIT_ADD:\n\tcase AUDIT_DEL:\n\t\treturn -EOPNOTSUPP;\n\tcase AUDIT_GET:\n\tcase AUDIT_SET:\n\tcase AUDIT_GET_FEATURE:\n\tcase AUDIT_SET_FEATURE:\n\tcase AUDIT_LIST_RULES:\n\tcase AUDIT_ADD_RULE:\n\tcase AUDIT_DEL_RULE:\n\tcase AUDIT_SIGNAL_INFO:\n\tcase AUDIT_TTY_GET:\n\tcase AUDIT_TTY_SET:\n\tcase AUDIT_TRIM:\n\tcase AUDIT_MAKE_EQUIV:\n\t\t/* Only support auditd and auditctl in initial pid namespace\n\t\t * for now. */\n\t\tif ((task_active_pid_ns(current) != &init_pid_ns))\n\t\t\treturn -EPERM;\n\n\t\tif (!netlink_capable(skb, CAP_AUDIT_CONTROL))\n\t\t\terr = -EPERM;\n\t\tbreak;\n\tcase AUDIT_USER:\n\tcase AUDIT_FIRST_USER_MSG ... AUDIT_LAST_USER_MSG:\n\tcase AUDIT_FIRST_USER_MSG2 ... AUDIT_LAST_USER_MSG2:\n\t\tif (!netlink_capable(skb, CAP_AUDIT_WRITE))\n\t\t\terr = -EPERM;\n\t\tbreak;\n\tdefault:  /* bad msg */\n\t\terr = -EINVAL;\n\t}\n\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic void cn_proc_mcast_ctl(struct cn_msg *msg,\n\t\t\t      struct netlink_skb_parms *nsp)\n{\n\tenum proc_cn_mcast_op *mc_op = NULL;\n\tint err = 0;\n\n\tif (msg->len != sizeof(*mc_op))\n\t\treturn;\n\n\t/* \n\t * Events are reported with respect to the initial pid\n\t * and user namespaces so ignore requestors from\n\t * other namespaces.\n\t */\n\tif ((current_user_ns() != &init_user_ns) ||\n\t    (task_active_pid_ns(current) != &init_pid_ns))\n\t\treturn;\n\n\t/* Can only change if privileged. */\n\tif (!capable(CAP_NET_ADMIN)) {\n\t\terr = EPERM;\n\t\tgoto out;\n\t}\n\n\tmc_op = (enum proc_cn_mcast_op *)msg->data;\n\tswitch (*mc_op) {\n\tcase PROC_CN_MCAST_LISTEN:\n\t\tatomic_inc(&proc_event_num_listeners);\n\t\tbreak;\n\tcase PROC_CN_MCAST_IGNORE:\n\t\tatomic_dec(&proc_event_num_listeners);\n\t\tbreak;\n\tdefault:\n\t\terr = EINVAL;\n\t\tbreak;\n\t}\n\nout:\n\tcn_proc_ack(err, msg->seq, msg->ack);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic void cn_proc_mcast_ctl(struct cn_msg *msg,\n\t\t\t      struct netlink_skb_parms *nsp)\n{\n\tenum proc_cn_mcast_op *mc_op = NULL;\n\tint err = 0;\n\n\tif (msg->len != sizeof(*mc_op))\n\t\treturn;\n\n\t/* \n\t * Events are reported with respect to the initial pid\n\t * and user namespaces so ignore requestors from\n\t * other namespaces.\n\t */\n\tif ((current_user_ns() != &init_user_ns) ||\n\t    (task_active_pid_ns(current) != &init_pid_ns))\n\t\treturn;\n\n\t/* Can only change if privileged. */\n\tif (!__netlink_ns_capable(nsp, &init_user_ns, CAP_NET_ADMIN)) {\n\t\terr = EPERM;\n\t\tgoto out;\n\t}\n\n\tmc_op = (enum proc_cn_mcast_op *)msg->data;\n\tswitch (*mc_op) {\n\tcase PROC_CN_MCAST_LISTEN:\n\t\tatomic_inc(&proc_event_num_listeners);\n\t\tbreak;\n\tcase PROC_CN_MCAST_IGNORE:\n\t\tatomic_dec(&proc_event_num_listeners);\n\t\tbreak;\n\tdefault:\n\t\terr = EINVAL;\n\t\tbreak;\n\t}\n\nout:\n\tcn_proc_ack(err, msg->seq, msg->ack);\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "asmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}",
                        "code_after_change": "asmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value[0] = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}",
                        "cve_id": "CVE-2014-9870"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int audit_netlink_ok(struct sk_buff *skb, u16 msg_type)\n{\n\tint err = 0;\n\n\t/* Only support initial user namespace for now. */\n\t/*\n\t * We return ECONNREFUSED because it tricks userspace into thinking\n\t * that audit was not configured into the kernel.  Lots of users\n\t * configure their PAM stack (because that's what the distro does)\n\t * to reject login if unable to send messages to audit.  If we return\n\t * ECONNREFUSED the PAM stack thinks the kernel does not have audit\n\t * configured in and will let login proceed.  If we return EPERM\n\t * userspace will reject all logins.  This should be removed when we\n\t * support non init namespaces!!\n\t */\n\tif (current_user_ns() != &init_user_ns)\n\t\treturn -ECONNREFUSED;\n\n\tswitch (msg_type) {\n\tcase AUDIT_LIST:\n\tcase AUDIT_ADD:\n\tcase AUDIT_DEL:\n\t\treturn -EOPNOTSUPP;\n\tcase AUDIT_GET:\n\tcase AUDIT_SET:\n\tcase AUDIT_GET_FEATURE:\n\tcase AUDIT_SET_FEATURE:\n\tcase AUDIT_LIST_RULES:\n\tcase AUDIT_ADD_RULE:\n\tcase AUDIT_DEL_RULE:\n\tcase AUDIT_SIGNAL_INFO:\n\tcase AUDIT_TTY_GET:\n\tcase AUDIT_TTY_SET:\n\tcase AUDIT_TRIM:\n\tcase AUDIT_MAKE_EQUIV:\n\t\t/* Only support auditd and auditctl in initial pid namespace\n\t\t * for now. */\n\t\tif ((task_active_pid_ns(current) != &init_pid_ns))\n\t\t\treturn -EPERM;\n\n\t\tif (!netlink_capable(skb, CAP_AUDIT_CONTROL))\n\t\t\terr = -EPERM;\n\t\tbreak;\n\tcase AUDIT_USER:\n\tcase AUDIT_FIRST_USER_MSG ... AUDIT_LAST_USER_MSG:\n\tcase AUDIT_FIRST_USER_MSG2 ... AUDIT_LAST_USER_MSG2:\n\t\tif (!netlink_capable(skb, CAP_AUDIT_WRITE))\n\t\t\terr = -EPERM;\n\t\tbreak;\n\tdefault:  /* bad msg */\n\t\terr = -EINVAL;\n\t}\n\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nasmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nasmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value[0] = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static noinline void\nno_context(struct pt_regs *regs, unsigned long error_code,\n\t   unsigned long address, int signal, int si_code)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long flags;\n\tint sig;\n\n\t/* Are we prepared to handle this kernel fault? */\n\tif (fixup_exception(regs)) {\n\t\t/*\n\t\t * Any interrupt that takes a fault gets the fixup. This makes\n\t\t * the below recursive fault logic only apply to a faults from\n\t\t * task context.\n\t\t */\n\t\tif (in_interrupt())\n\t\t\treturn;\n\n\t\t/*\n\t\t * Per the above we're !in_interrupt(), aka. task context.\n\t\t *\n\t\t * In this case we need to make sure we're not recursively\n\t\t * faulting through the emulate_vsyscall() logic.\n\t\t */\n\t\tif (current_thread_info()->sig_on_uaccess_error && signal) {\n\t\t\ttsk->thread.trap_nr = X86_TRAP_PF;\n\t\t\ttsk->thread.error_code = error_code | PF_USER;\n\t\t\ttsk->thread.cr2 = address;\n\n\t\t\t/* XXX: hwpoison faults will set the wrong code. */\n\t\t\tforce_sig_info_fault(signal, si_code, address, tsk, 0);\n\t\t}\n\n\t\t/*\n\t\t * Barring that, we can do the fixup and be happy.\n\t\t */\n\t\treturn;\n\t}\n\n\t/*\n\t * 32-bit:\n\t *\n\t *   Valid to do another page fault here, because if this fault\n\t *   had been triggered by is_prefetch fixup_exception would have\n\t *   handled it.\n\t *\n\t * 64-bit:\n\t *\n\t *   Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n\t/*\n\t * Oops. The kernel tried to access some bad page. We'll have to\n\t * terminate things with extreme prejudice:\n\t */\n\tflags = oops_begin();\n\n\tshow_fault_oops(regs, error_code, address);\n\n\tif (task_stack_end_corrupted(tsk))\n\t\tprintk(KERN_EMERG \"Thread overran stack, or stack corrupted\\n\");\n\n\ttsk->thread.cr2\t\t= address;\n\ttsk->thread.trap_nr\t= X86_TRAP_PF;\n\ttsk->thread.error_code\t= error_code;\n\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_DEFAULT \"CR2: %016lx\\n\", address);\n\n\toops_end(flags, regs, sig);\n}",
                        "code_after_change": "static noinline void\nno_context(struct pt_regs *regs, unsigned long error_code,\n\t   unsigned long address, int signal, int si_code)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long flags;\n\tint sig;\n\n\t/* Are we prepared to handle this kernel fault? */\n\tif (fixup_exception(regs, X86_TRAP_PF)) {\n\t\t/*\n\t\t * Any interrupt that takes a fault gets the fixup. This makes\n\t\t * the below recursive fault logic only apply to a faults from\n\t\t * task context.\n\t\t */\n\t\tif (in_interrupt())\n\t\t\treturn;\n\n\t\t/*\n\t\t * Per the above we're !in_interrupt(), aka. task context.\n\t\t *\n\t\t * In this case we need to make sure we're not recursively\n\t\t * faulting through the emulate_vsyscall() logic.\n\t\t */\n\t\tif (current_thread_info()->sig_on_uaccess_error && signal) {\n\t\t\ttsk->thread.trap_nr = X86_TRAP_PF;\n\t\t\ttsk->thread.error_code = error_code | PF_USER;\n\t\t\ttsk->thread.cr2 = address;\n\n\t\t\t/* XXX: hwpoison faults will set the wrong code. */\n\t\t\tforce_sig_info_fault(signal, si_code, address, tsk, 0);\n\t\t}\n\n\t\t/*\n\t\t * Barring that, we can do the fixup and be happy.\n\t\t */\n\t\treturn;\n\t}\n\n\t/*\n\t * 32-bit:\n\t *\n\t *   Valid to do another page fault here, because if this fault\n\t *   had been triggered by is_prefetch fixup_exception would have\n\t *   handled it.\n\t *\n\t * 64-bit:\n\t *\n\t *   Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n\t/*\n\t * Oops. The kernel tried to access some bad page. We'll have to\n\t * terminate things with extreme prejudice:\n\t */\n\tflags = oops_begin();\n\n\tshow_fault_oops(regs, error_code, address);\n\n\tif (task_stack_end_corrupted(tsk))\n\t\tprintk(KERN_EMERG \"Thread overran stack, or stack corrupted\\n\");\n\n\ttsk->thread.cr2\t\t= address;\n\ttsk->thread.trap_nr\t= X86_TRAP_PF;\n\ttsk->thread.error_code\t= error_code;\n\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_DEFAULT \"CR2: %016lx\\n\", address);\n\n\toops_end(flags, regs, sig);\n}",
                        "cve_id": "CVE-2016-9644"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int audit_netlink_ok(struct sk_buff *skb, u16 msg_type)\n{\n\tint err = 0;\n\n\t/* Only support initial user namespace for now. */\n\t/*\n\t * We return ECONNREFUSED because it tricks userspace into thinking\n\t * that audit was not configured into the kernel.  Lots of users\n\t * configure their PAM stack (because that's what the distro does)\n\t * to reject login if unable to send messages to audit.  If we return\n\t * ECONNREFUSED the PAM stack thinks the kernel does not have audit\n\t * configured in and will let login proceed.  If we return EPERM\n\t * userspace will reject all logins.  This should be removed when we\n\t * support non init namespaces!!\n\t */\n\tif (current_user_ns() != &init_user_ns)\n\t\treturn -ECONNREFUSED;\n\n\tswitch (msg_type) {\n\tcase AUDIT_LIST:\n\tcase AUDIT_ADD:\n\tcase AUDIT_DEL:\n\t\treturn -EOPNOTSUPP;\n\tcase AUDIT_GET:\n\tcase AUDIT_SET:\n\tcase AUDIT_GET_FEATURE:\n\tcase AUDIT_SET_FEATURE:\n\tcase AUDIT_LIST_RULES:\n\tcase AUDIT_ADD_RULE:\n\tcase AUDIT_DEL_RULE:\n\tcase AUDIT_SIGNAL_INFO:\n\tcase AUDIT_TTY_GET:\n\tcase AUDIT_TTY_SET:\n\tcase AUDIT_TRIM:\n\tcase AUDIT_MAKE_EQUIV:\n\t\t/* Only support auditd and auditctl in initial pid namespace\n\t\t * for now. */\n\t\tif ((task_active_pid_ns(current) != &init_pid_ns))\n\t\t\treturn -EPERM;\n\n\t\tif (!netlink_capable(skb, CAP_AUDIT_CONTROL))\n\t\t\terr = -EPERM;\n\t\tbreak;\n\tcase AUDIT_USER:\n\tcase AUDIT_FIRST_USER_MSG ... AUDIT_LAST_USER_MSG:\n\tcase AUDIT_FIRST_USER_MSG2 ... AUDIT_LAST_USER_MSG2:\n\t\tif (!netlink_capable(skb, CAP_AUDIT_WRITE))\n\t\t\terr = -EPERM;\n\t\tbreak;\n\tdefault:  /* bad msg */\n\t\terr = -EINVAL;\n\t}\n\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic noinline void\nno_context(struct pt_regs *regs, unsigned long error_code,\n\t   unsigned long address, int signal, int si_code)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long flags;\n\tint sig;\n\n\t/* Are we prepared to handle this kernel fault? */\n\tif (fixup_exception(regs)) {\n\t\t/*\n\t\t * Any interrupt that takes a fault gets the fixup. This makes\n\t\t * the below recursive fault logic only apply to a faults from\n\t\t * task context.\n\t\t */\n\t\tif (in_interrupt())\n\t\t\treturn;\n\n\t\t/*\n\t\t * Per the above we're !in_interrupt(), aka. task context.\n\t\t *\n\t\t * In this case we need to make sure we're not recursively\n\t\t * faulting through the emulate_vsyscall() logic.\n\t\t */\n\t\tif (current_thread_info()->sig_on_uaccess_error && signal) {\n\t\t\ttsk->thread.trap_nr = X86_TRAP_PF;\n\t\t\ttsk->thread.error_code = error_code | PF_USER;\n\t\t\ttsk->thread.cr2 = address;\n\n\t\t\t/* XXX: hwpoison faults will set the wrong code. */\n\t\t\tforce_sig_info_fault(signal, si_code, address, tsk, 0);\n\t\t}\n\n\t\t/*\n\t\t * Barring that, we can do the fixup and be happy.\n\t\t */\n\t\treturn;\n\t}\n\n\t/*\n\t * 32-bit:\n\t *\n\t *   Valid to do another page fault here, because if this fault\n\t *   had been triggered by is_prefetch fixup_exception would have\n\t *   handled it.\n\t *\n\t * 64-bit:\n\t *\n\t *   Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n\t/*\n\t * Oops. The kernel tried to access some bad page. We'll have to\n\t * terminate things with extreme prejudice:\n\t */\n\tflags = oops_begin();\n\n\tshow_fault_oops(regs, error_code, address);\n\n\tif (task_stack_end_corrupted(tsk))\n\t\tprintk(KERN_EMERG \"Thread overran stack, or stack corrupted\\n\");\n\n\ttsk->thread.cr2\t\t= address;\n\ttsk->thread.trap_nr\t= X86_TRAP_PF;\n\ttsk->thread.error_code\t= error_code;\n\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_DEFAULT \"CR2: %016lx\\n\", address);\n\n\toops_end(flags, regs, sig);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic noinline void\nno_context(struct pt_regs *regs, unsigned long error_code,\n\t   unsigned long address, int signal, int si_code)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long flags;\n\tint sig;\n\n\t/* Are we prepared to handle this kernel fault? */\n\tif (fixup_exception(regs, X86_TRAP_PF)) {\n\t\t/*\n\t\t * Any interrupt that takes a fault gets the fixup. This makes\n\t\t * the below recursive fault logic only apply to a faults from\n\t\t * task context.\n\t\t */\n\t\tif (in_interrupt())\n\t\t\treturn;\n\n\t\t/*\n\t\t * Per the above we're !in_interrupt(), aka. task context.\n\t\t *\n\t\t * In this case we need to make sure we're not recursively\n\t\t * faulting through the emulate_vsyscall() logic.\n\t\t */\n\t\tif (current_thread_info()->sig_on_uaccess_error && signal) {\n\t\t\ttsk->thread.trap_nr = X86_TRAP_PF;\n\t\t\ttsk->thread.error_code = error_code | PF_USER;\n\t\t\ttsk->thread.cr2 = address;\n\n\t\t\t/* XXX: hwpoison faults will set the wrong code. */\n\t\t\tforce_sig_info_fault(signal, si_code, address, tsk, 0);\n\t\t}\n\n\t\t/*\n\t\t * Barring that, we can do the fixup and be happy.\n\t\t */\n\t\treturn;\n\t}\n\n\t/*\n\t * 32-bit:\n\t *\n\t *   Valid to do another page fault here, because if this fault\n\t *   had been triggered by is_prefetch fixup_exception would have\n\t *   handled it.\n\t *\n\t * 64-bit:\n\t *\n\t *   Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n\t/*\n\t * Oops. The kernel tried to access some bad page. We'll have to\n\t * terminate things with extreme prejudice:\n\t */\n\tflags = oops_begin();\n\n\tshow_fault_oops(regs, error_code, address);\n\n\tif (task_stack_end_corrupted(tsk))\n\t\tprintk(KERN_EMERG \"Thread overran stack, or stack corrupted\\n\");\n\n\ttsk->thread.cr2\t\t= address;\n\ttsk->thread.trap_nr\t= X86_TRAP_PF;\n\ttsk->thread.error_code\t= error_code;\n\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_DEFAULT \"CR2: %016lx\\n\", address);\n\n\toops_end(flags, regs, sig);\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 136,
            "cve_id": "CVE-2012-6657",
            "code_snippet": "int sock_setsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tint val;\n\tint valbool;\n\tstruct linger ling;\n\tint ret = 0;\n\n\t/*\n\t *\tOptions without arguments\n\t */\n\n\tif (optname == SO_BINDTODEVICE)\n\t\treturn sock_bindtodevice(sk, optval, optlen);\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\n\tvalbool = val ? 1 : 0;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase SO_DEBUG:\n\t\tif (val && !capable(CAP_NET_ADMIN))\n\t\t\tret = -EACCES;\n\t\telse\n\t\t\tsock_valbool_flag(sk, SOCK_DBG, valbool);\n\t\tbreak;\n\tcase SO_REUSEADDR:\n\t\tsk->sk_reuse = (valbool ? SK_CAN_REUSE : SK_NO_REUSE);\n\t\tbreak;\n\tcase SO_TYPE:\n\tcase SO_PROTOCOL:\n\tcase SO_DOMAIN:\n\tcase SO_ERROR:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\tcase SO_DONTROUTE:\n\t\tsock_valbool_flag(sk, SOCK_LOCALROUTE, valbool);\n\t\tbreak;\n\tcase SO_BROADCAST:\n\t\tsock_valbool_flag(sk, SOCK_BROADCAST, valbool);\n\t\tbreak;\n\tcase SO_SNDBUF:\n\t\t/* Don't error on this BSD doesn't and if you think\n\t\t * about it this is right. Otherwise apps have to\n\t\t * play 'guess the biggest size' games. RCVBUF/SNDBUF\n\t\t * are treated in BSD as hints\n\t\t */\n\t\tval = min_t(u32, val, sysctl_wmem_max);\nset_sndbuf:\n\t\tsk->sk_userlocks |= SOCK_SNDBUF_LOCK;\n\t\tsk->sk_sndbuf = max_t(u32, val * 2, SOCK_MIN_SNDBUF);\n\t\t/* Wake up sending tasks if we upped the value. */\n\t\tsk->sk_write_space(sk);\n\t\tbreak;\n\n\tcase SO_SNDBUFFORCE:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tgoto set_sndbuf;\n\n\tcase SO_RCVBUF:\n\t\t/* Don't error on this BSD doesn't and if you think\n\t\t * about it this is right. Otherwise apps have to\n\t\t * play 'guess the biggest size' games. RCVBUF/SNDBUF\n\t\t * are treated in BSD as hints\n\t\t */\n\t\tval = min_t(u32, val, sysctl_rmem_max);\nset_rcvbuf:\n\t\tsk->sk_userlocks |= SOCK_RCVBUF_LOCK;\n\t\t/*\n\t\t * We double it on the way in to account for\n\t\t * \"struct sk_buff\" etc. overhead.   Applications\n\t\t * assume that the SO_RCVBUF setting they make will\n\t\t * allow that much actual data to be received on that\n\t\t * socket.\n\t\t *\n\t\t * Applications are unaware that \"struct sk_buff\" and\n\t\t * other overheads allocate from the receive buffer\n\t\t * during socket buffer allocation.\n\t\t *\n\t\t * And after considering the possible alternatives,\n\t\t * returning the value we actually used in getsockopt\n\t\t * is the most desirable behavior.\n\t\t */\n\t\tsk->sk_rcvbuf = max_t(u32, val * 2, SOCK_MIN_RCVBUF);\n\t\tbreak;\n\n\tcase SO_RCVBUFFORCE:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tgoto set_rcvbuf;\n\n\tcase SO_KEEPALIVE:\n#ifdef CONFIG_INET\n\t\tif (sk->sk_protocol == IPPROTO_TCP &&\n\t\t    sk->sk_type == SOCK_STREAM)\n\t\t\ttcp_set_keepalive(sk, valbool);\n#endif\n\t\tsock_valbool_flag(sk, SOCK_KEEPOPEN, valbool);\n\t\tbreak;\n\n\tcase SO_OOBINLINE:\n\t\tsock_valbool_flag(sk, SOCK_URGINLINE, valbool);\n\t\tbreak;\n\n\tcase SO_NO_CHECK:\n\t\tsk->sk_no_check = valbool;\n\t\tbreak;\n\n\tcase SO_PRIORITY:\n\t\tif ((val >= 0 && val <= 6) || capable(CAP_NET_ADMIN))\n\t\t\tsk->sk_priority = val;\n\t\telse\n\t\t\tret = -EPERM;\n\t\tbreak;\n\n\tcase SO_LINGER:\n\t\tif (optlen < sizeof(ling)) {\n\t\t\tret = -EINVAL;\t/* 1003.1g */\n\t\t\tbreak;\n\t\t}\n\t\tif (copy_from_user(&ling, optval, sizeof(ling))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!ling.l_onoff)\n\t\t\tsock_reset_flag(sk, SOCK_LINGER);\n\t\telse {\n#if (BITS_PER_LONG == 32)\n\t\t\tif ((unsigned int)ling.l_linger >= MAX_SCHEDULE_TIMEOUT/HZ)\n\t\t\t\tsk->sk_lingertime = MAX_SCHEDULE_TIMEOUT;\n\t\t\telse\n#endif\n\t\t\t\tsk->sk_lingertime = (unsigned int)ling.l_linger * HZ;\n\t\t\tsock_set_flag(sk, SOCK_LINGER);\n\t\t}\n\t\tbreak;\n\n\tcase SO_BSDCOMPAT:\n\t\tsock_warn_obsolete_bsdism(\"setsockopt\");\n\t\tbreak;\n\n\tcase SO_PASSCRED:\n\t\tif (valbool)\n\t\t\tset_bit(SOCK_PASSCRED, &sock->flags);\n\t\telse\n\t\t\tclear_bit(SOCK_PASSCRED, &sock->flags);\n\t\tbreak;\n\n\tcase SO_TIMESTAMP:\n\tcase SO_TIMESTAMPNS:\n\t\tif (valbool)  {\n\t\t\tif (optname == SO_TIMESTAMP)\n\t\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t\telse\n\t\t\t\tsock_set_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t\tsock_set_flag(sk, SOCK_RCVTSTAMP);\n\t\t\tsock_enable_timestamp(sk, SOCK_TIMESTAMP);\n\t\t} else {\n\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMP);\n\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t}\n\t\tbreak;\n\n\tcase SO_TIMESTAMPING:\n\t\tif (val & ~SOF_TIMESTAMPING_MASK) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_TX_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_TX_HARDWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_TX_SOFTWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_TX_SOFTWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_RX_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_RX_HARDWARE);\n\t\tif (val & SOF_TIMESTAMPING_RX_SOFTWARE)\n\t\t\tsock_enable_timestamp(sk,\n\t\t\t\t\t      SOCK_TIMESTAMPING_RX_SOFTWARE);\n\t\telse\n\t\t\tsock_disable_timestamp(sk,\n\t\t\t\t\t       (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE));\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_SOFTWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_SOFTWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_SYS_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_SYS_HARDWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_RAW_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_RAW_HARDWARE);\n\t\tbreak;\n\n\tcase SO_RCVLOWAT:\n\t\tif (val < 0)\n\t\t\tval = INT_MAX;\n\t\tsk->sk_rcvlowat = val ? : 1;\n\t\tbreak;\n\n\tcase SO_RCVTIMEO:\n\t\tret = sock_set_timeout(&sk->sk_rcvtimeo, optval, optlen);\n\t\tbreak;\n\n\tcase SO_SNDTIMEO:\n\t\tret = sock_set_timeout(&sk->sk_sndtimeo, optval, optlen);\n\t\tbreak;\n\n\tcase SO_ATTACH_FILTER:\n\t\tret = -EINVAL;\n\t\tif (optlen == sizeof(struct sock_fprog)) {\n\t\t\tstruct sock_fprog fprog;\n\n\t\t\tret = -EFAULT;\n\t\t\tif (copy_from_user(&fprog, optval, sizeof(fprog)))\n\t\t\t\tbreak;\n\n\t\t\tret = sk_attach_filter(&fprog, sk);\n\t\t}\n\t\tbreak;\n\n\tcase SO_DETACH_FILTER:\n\t\tret = sk_detach_filter(sk);\n\t\tbreak;\n\n\tcase SO_PASSSEC:\n\t\tif (valbool)\n\t\t\tset_bit(SOCK_PASSSEC, &sock->flags);\n\t\telse\n\t\t\tclear_bit(SOCK_PASSSEC, &sock->flags);\n\t\tbreak;\n\tcase SO_MARK:\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tsk->sk_mark = val;\n\t\tbreak;\n\n\t\t/* We implement the SO_SNDLOWAT etc to\n\t\t   not be settable (1003.1g 5.3) */\n\tcase SO_RXQ_OVFL:\n\t\tsock_valbool_flag(sk, SOCK_RXQ_OVFL, valbool);\n\t\tbreak;\n\n\tcase SO_WIFI_STATUS:\n\t\tsock_valbool_flag(sk, SOCK_WIFI_STATUS, valbool);\n\t\tbreak;\n\n\tcase SO_PEEK_OFF:\n\t\tif (sock->ops->set_peek_off)\n\t\t\tsock->ops->set_peek_off(sk, val);\n\t\telse\n\t\t\tret = -EOPNOTSUPP;\n\t\tbreak;\n\n\tcase SO_NOFCS:\n\t\tsock_valbool_flag(sk, SOCK_NOFCS, valbool);\n\t\tbreak;\n\n\tdefault:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\trelease_sock(sk);\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, valbool;\n\tint retv = -ENOPROTOOPT;\n\tbool needs_rtnl = setsockopt_needs_rtnl(optname);\n\n\tif (!optval)\n\t\tval = 0;\n\telse {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (get_user(val, (int __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\tval = 0;\n\t}\n\n\tvalbool = (val != 0);\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_setsockopt(sk, optname, optval, optlen);\n\n\tif (needs_rtnl)\n\t\trtnl_lock();\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\n\tcase IPV6_ADDRFORM:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val == PF_INET) {\n\t\t\tstruct ipv6_txoptions *opt;\n\t\t\tstruct sk_buff *pktopt;\n\n\t\t\tif (sk->sk_type == SOCK_RAW)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_protocol == IPPROTO_UDP ||\n\t\t\t    sk->sk_protocol == IPPROTO_UDPLITE) {\n\t\t\t\tstruct udp_sock *up = udp_sk(sk);\n\t\t\t\tif (up->pending == AF_INET6) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (sk->sk_protocol != IPPROTO_TCP)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t\tretv = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (ipv6_only_sock(sk) ||\n\t\t\t    !ipv6_addr_v4mapped(&sk->sk_v6_daddr)) {\n\t\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tfl6_free_socklist(sk);\n\t\t\tipv6_sock_mc_close(sk);\n\n\t\t\t/*\n\t\t\t * Sock is moving from IPv6 to IPv4 (sk_prot), so\n\t\t\t * remove it from the refcnt debug socks count in the\n\t\t\t * original family...\n\t\t\t */\n\t\t\tsk_refcnt_debug_dec(sk);\n\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = &tcp_prot;\n\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;\n\t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t\t} else {\n\t\t\t\tstruct proto *prot = &udp_prot;\n\n\t\t\t\tif (sk->sk_protocol == IPPROTO_UDPLITE)\n\t\t\t\t\tprot = &udplite_prot;\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = prot;\n\t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t}\n\t\t\topt = xchg(&np->opt, NULL);\n\t\t\tif (opt)\n\t\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t\t\tpktopt = xchg(&np->pktoptions, NULL);\n\t\t\tkfree_skb(pktopt);\n\n\t\t\tsk->sk_destruct = inet_sock_destruct;\n\t\t\t/*\n\t\t\t * ... and add it to the refcnt debug socks count\n\t\t\t * in the new family. -acme\n\t\t\t */\n\t\t\tsk_refcnt_debug_inc(sk);\n\t\t\tmodule_put(THIS_MODULE);\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\t\tgoto e_inval;\n\n\tcase IPV6_V6ONLY:\n\t\tif (optlen < sizeof(int) ||\n\t\t    inet_sk(sk)->inet_num)\n\t\t\tgoto e_inval;\n\t\tsk->sk_ipv6only = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxoinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxhlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxohlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.srcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.osrcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.hopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.ohopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.dstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.odstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < -1 || val > 0xff)\n\t\t\tgoto e_inval;\n\t\t/* RFC 3542, 6.5: default traffic class of 0x0 */\n\t\tif (val == -1)\n\t\t\tval = 0;\n\t\tnp->tclass = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxtclass = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxpmtu = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TRANSPARENT:\n\t\tif (valbool && !ns_capable(net->user_ns, CAP_NET_ADMIN) &&\n\t\t    !ns_capable(net->user_ns, CAP_NET_RAW)) {\n\t\t\tretv = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we don't have a separate transparent bit for IPV6 we use the one in the IPv4 socket */\n\t\tinet_sk(sk)->transparent = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxorigdstaddr = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\t/* remove any sticky options header with a zero option\n\t\t * length, per RFC3542.\n\t\t */\n\t\tif (optlen == 0)\n\t\t\toptval = NULL;\n\t\telse if (!optval)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct ipv6_opt_hdr) ||\n\t\t\t optlen & 0x7 || optlen > 8 * 255)\n\t\t\tgoto e_inval;\n\n\t\t/* hop-by-hop / destination options are privileged option */\n\t\tretv = -EPERM;\n\t\tif (optname != IPV6_RTHDR && !ns_capable(net->user_ns, CAP_NET_RAW))\n\t\t\tbreak;\n\n\t\topt = ipv6_renew_options(sk, np->opt, optname,\n\t\t\t\t\t (struct ipv6_opt_hdr __user *)optval,\n\t\t\t\t\t optlen);\n\t\tif (IS_ERR(opt)) {\n\t\t\tretv = PTR_ERR(opt);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* routing header option needs extra check */\n\t\tretv = -EINVAL;\n\t\tif (optname == IPV6_RTHDR && opt && opt->srcrt) {\n\t\t\tstruct ipv6_rt_hdr *rthdr = opt->srcrt;\n\t\t\tswitch (rthdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\t\tcase IPV6_SRCRT_TYPE_2:\n\t\t\t\tif (rthdr->hdrlen != 2 ||\n\t\t\t\t    rthdr->segments_left != 1)\n\t\t\t\t\tgoto sticky_done;\n\n\t\t\t\tbreak;\n#endif\n\t\t\tdefault:\n\t\t\t\tgoto sticky_done;\n\t\t\t}\n\t\t}\n\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\nsticky_done:\n\t\tif (opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t\tbreak;\n\t}\n\n\tcase IPV6_PKTINFO:\n\t{\n\t\tstruct in6_pktinfo pkt;\n\n\t\tif (optlen == 0)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct in6_pktinfo) || !optval)\n\t\t\tgoto e_inval;\n\n\t\tif (copy_from_user(&pkt, optval, sizeof(struct in6_pktinfo))) {\n\t\t\t\tretv = -EFAULT;\n\t\t\t\tbreak;\n\t\t}\n\t\tif (sk->sk_bound_dev_if && pkt.ipi6_ifindex != sk->sk_bound_dev_if)\n\t\t\tgoto e_inval;\n\n\t\tnp->sticky_pktinfo.ipi6_ifindex = pkt.ipi6_ifindex;\n\t\tnp->sticky_pktinfo.ipi6_addr = pkt.ipi6_addr;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct ipv6_txoptions *opt = NULL;\n\t\tstruct msghdr msg;\n\t\tstruct flowi6 fl6;\n\t\tint junk;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\n\t\tif (optlen == 0)\n\t\t\tgoto update;\n\n\t\t/* 1K is probably excessive\n\t\t * 1K is surely not enough, 2K per standard header is 16K.\n\t\t */\n\t\tretv = -EINVAL;\n\t\tif (optlen > 64*1024)\n\t\t\tbreak;\n\n\t\topt = sock_kmalloc(sk, sizeof(*opt) + optlen, GFP_KERNEL);\n\t\tretv = -ENOBUFS;\n\t\tif (!opt)\n\t\t\tbreak;\n\n\t\tmemset(opt, 0, sizeof(*opt));\n\t\topt->tot_len = sizeof(*opt) + optlen;\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(opt+1, optval, optlen))\n\t\t\tgoto done;\n\n\t\tmsg.msg_controllen = optlen;\n\t\tmsg.msg_control = (void *)(opt+1);\n\n\t\tretv = ip6_datagram_send_ctl(net, sk, &msg, &fl6, opt, &junk,\n\t\t\t\t\t     &junk, &junk);\n\t\tif (retv)\n\t\t\tgoto done;\nupdate:\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\ndone:\n\t\tif (opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t\tbreak;\n\t}\n\tcase IPV6_UNICAST_HOPS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->hop_limit = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_HOPS:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->mcast_hops = (val == -1 ? IPV6_DEFAULT_MCASTHOPS : val);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val != valbool)\n\t\t\tgoto e_inval;\n\t\tnp->mc_loop = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t{\n\t\tstruct net_device *dev = NULL;\n\t\tint ifindex;\n\n\t\tif (optlen != sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tifindex = (__force int)ntohl((__force __be32)val);\n\t\tif (ifindex == 0) {\n\t\t\tnp->ucast_oif = 0;\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tdev = dev_get_by_index(net, ifindex);\n\t\tretv = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\tretv = -EINVAL;\n\t\tif (sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tnp->ucast_oif = ifindex;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_IF:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tif (val) {\n\t\t\tstruct net_device *dev;\n\n\t\t\tif (sk->sk_bound_dev_if && sk->sk_bound_dev_if != val)\n\t\t\t\tgoto e_inval;\n\n\t\t\tdev = dev_get_by_index(net, val);\n\t\t\tif (!dev) {\n\t\t\t\tretv = -ENODEV;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdev_put(dev);\n\t\t}\n\t\tnp->mcast_oif = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_ADD_MEMBERSHIP)\n\t\t\tretv = ipv6_sock_mc_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_JOIN_ANYCAST)\n\t\t\tretv = ipv6_sock_ac_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_ac_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t{\n\t\tstruct group_req greq;\n\t\tstruct sockaddr_in6 *psin6;\n\n\t\tif (optlen < sizeof(struct group_req))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&greq, optval, sizeof(struct group_req)))\n\t\t\tbreak;\n\t\tif (greq.gr_group.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tpsin6 = (struct sockaddr_in6 *)&greq.gr_group;\n\t\tif (optname == MCAST_JOIN_GROUP)\n\t\t\tretv = ipv6_sock_mc_join(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t{\n\t\tstruct group_source_req greqs;\n\t\tint omode, add;\n\n\t\tif (optlen < sizeof(struct group_source_req))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&greqs, optval, sizeof(greqs))) {\n\t\t\tretv = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (greqs.gsr_group.ss_family != AF_INET6 ||\n\t\t    greqs.gsr_source.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tif (optname == MCAST_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == MCAST_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == MCAST_JOIN_SOURCE_GROUP) {\n\t\t\tstruct sockaddr_in6 *psin6;\n\n\t\t\tpsin6 = (struct sockaddr_in6 *)&greqs.gsr_group;\n\t\t\tretv = ipv6_sock_mc_join(sk, greqs.gsr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\t\t/* prior join w/ different source is ok */\n\t\t\tif (retv && retv != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* MCAST_LEAVE_SOURCE_GROUP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\tretv = ip6_mc_source(add, omode, sk, &greqs);\n\t\tbreak;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter *gsf;\n\n\t\tif (optlen < GROUP_FILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tgsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!gsf) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(gsf, optval, optlen)) {\n\t\t\tkfree(gsf);\n\t\t\tbreak;\n\t\t}\n\t\t/* numsrc >= (4G-140)/128 overflow in 32 bits */\n\t\tif (gsf->gf_numsrc >= 0x1ffffffU ||\n\t\t    gsf->gf_numsrc > sysctl_mld_max_msf) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tif (GROUP_FILTER_SIZE(gsf->gf_numsrc) > optlen) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tretv = ip6_mc_msfilter(sk, gsf);\n\t\tkfree(gsf);\n\n\t\tbreak;\n\t}\n\tcase IPV6_ROUTER_ALERT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = ip6_ra_control(sk, val);\n\t\tbreak;\n\tcase IPV6_MTU_DISCOVER:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < IPV6_PMTUDISC_DONT || val > IPV6_PMTUDISC_OMIT)\n\t\t\tgoto e_inval;\n\t\tnp->pmtudisc = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val && val < IPV6_MIN_MTU)\n\t\t\tgoto e_inval;\n\t\tnp->frag_size = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->recverr = valbool;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWINFO_SEND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->sndflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWLABEL_MGR:\n\t\tretv = ipv6_flowlabel_opt(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_IPSEC_POLICY:\n\tcase IPV6_XFRM_POLICY:\n\t\tretv = -EPERM;\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\tretv = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t    {\n\t\tunsigned int pref = 0;\n\t\tunsigned int prefmask = ~0;\n\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EINVAL;\n\n\t\t/* check PUBLIC/TMP/PUBTMP_DEFAULT conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_PUBLIC|\n\t\t\t       IPV6_PREFER_SRC_TMP|\n\t\t\t       IPV6_PREFER_SRC_PUBTMP_DEFAULT)) {\n\t\tcase IPV6_PREFER_SRC_PUBLIC:\n\t\t\tpref |= IPV6_PREFER_SRC_PUBLIC;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_TMP:\n\t\t\tpref |= IPV6_PREFER_SRC_TMP;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_PUBTMP_DEFAULT:\n\t\t\tbreak;\n\t\tcase 0:\n\t\t\tgoto pref_skip_pubtmp;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~(IPV6_PREFER_SRC_PUBLIC|\n\t\t\t      IPV6_PREFER_SRC_TMP);\npref_skip_pubtmp:\n\n\t\t/* check HOME/COA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_HOME|IPV6_PREFER_SRC_COA)) {\n\t\tcase IPV6_PREFER_SRC_HOME:\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_COA:\n\t\t\tpref |= IPV6_PREFER_SRC_COA;\n\t\tcase 0:\n\t\t\tgoto pref_skip_coa;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~IPV6_PREFER_SRC_COA;\npref_skip_coa:\n\n\t\t/* check CGA/NONCGA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_CGA|IPV6_PREFER_SRC_NONCGA)) {\n\t\tcase IPV6_PREFER_SRC_CGA:\n\t\tcase IPV6_PREFER_SRC_NONCGA:\n\t\tcase 0:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tnp->srcprefs = (np->srcprefs & prefmask) | pref;\n\t\tretv = 0;\n\n\t\tbreak;\n\t    }\n\tcase IPV6_MINHOPCOUNT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tnp->min_hopcount = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_DONTFRAG:\n\t\tnp->dontfrag = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tnp->autoflowlabel = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\n\treturn retv;\n\ne_inval:\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\treturn -EINVAL;\n}",
                        "code_after_change": "static int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, valbool;\n\tint retv = -ENOPROTOOPT;\n\tbool needs_rtnl = setsockopt_needs_rtnl(optname);\n\n\tif (!optval)\n\t\tval = 0;\n\telse {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (get_user(val, (int __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\tval = 0;\n\t}\n\n\tvalbool = (val != 0);\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_setsockopt(sk, optname, optval, optlen);\n\n\tif (needs_rtnl)\n\t\trtnl_lock();\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\n\tcase IPV6_ADDRFORM:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val == PF_INET) {\n\t\t\tstruct ipv6_txoptions *opt;\n\t\t\tstruct sk_buff *pktopt;\n\n\t\t\tif (sk->sk_type == SOCK_RAW)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_protocol == IPPROTO_UDP ||\n\t\t\t    sk->sk_protocol == IPPROTO_UDPLITE) {\n\t\t\t\tstruct udp_sock *up = udp_sk(sk);\n\t\t\t\tif (up->pending == AF_INET6) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (sk->sk_protocol != IPPROTO_TCP)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t\tretv = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (ipv6_only_sock(sk) ||\n\t\t\t    !ipv6_addr_v4mapped(&sk->sk_v6_daddr)) {\n\t\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tfl6_free_socklist(sk);\n\t\t\tipv6_sock_mc_close(sk);\n\n\t\t\t/*\n\t\t\t * Sock is moving from IPv6 to IPv4 (sk_prot), so\n\t\t\t * remove it from the refcnt debug socks count in the\n\t\t\t * original family...\n\t\t\t */\n\t\t\tsk_refcnt_debug_dec(sk);\n\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = &tcp_prot;\n\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;\n\t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t\t} else {\n\t\t\t\tstruct proto *prot = &udp_prot;\n\n\t\t\t\tif (sk->sk_protocol == IPPROTO_UDPLITE)\n\t\t\t\t\tprot = &udplite_prot;\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = prot;\n\t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t}\n\t\t\topt = xchg((__force struct ipv6_txoptions **)&np->opt,\n\t\t\t\t   NULL);\n\t\t\tif (opt) {\n\t\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\t\ttxopt_put(opt);\n\t\t\t}\n\t\t\tpktopt = xchg(&np->pktoptions, NULL);\n\t\t\tkfree_skb(pktopt);\n\n\t\t\tsk->sk_destruct = inet_sock_destruct;\n\t\t\t/*\n\t\t\t * ... and add it to the refcnt debug socks count\n\t\t\t * in the new family. -acme\n\t\t\t */\n\t\t\tsk_refcnt_debug_inc(sk);\n\t\t\tmodule_put(THIS_MODULE);\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\t\tgoto e_inval;\n\n\tcase IPV6_V6ONLY:\n\t\tif (optlen < sizeof(int) ||\n\t\t    inet_sk(sk)->inet_num)\n\t\t\tgoto e_inval;\n\t\tsk->sk_ipv6only = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxoinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxhlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxohlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.srcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.osrcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.hopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.ohopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.dstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.odstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < -1 || val > 0xff)\n\t\t\tgoto e_inval;\n\t\t/* RFC 3542, 6.5: default traffic class of 0x0 */\n\t\tif (val == -1)\n\t\t\tval = 0;\n\t\tnp->tclass = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxtclass = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxpmtu = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TRANSPARENT:\n\t\tif (valbool && !ns_capable(net->user_ns, CAP_NET_ADMIN) &&\n\t\t    !ns_capable(net->user_ns, CAP_NET_RAW)) {\n\t\t\tretv = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we don't have a separate transparent bit for IPV6 we use the one in the IPv4 socket */\n\t\tinet_sk(sk)->transparent = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxorigdstaddr = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\t/* remove any sticky options header with a zero option\n\t\t * length, per RFC3542.\n\t\t */\n\t\tif (optlen == 0)\n\t\t\toptval = NULL;\n\t\telse if (!optval)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct ipv6_opt_hdr) ||\n\t\t\t optlen & 0x7 || optlen > 8 * 255)\n\t\t\tgoto e_inval;\n\n\t\t/* hop-by-hop / destination options are privileged option */\n\t\tretv = -EPERM;\n\t\tif (optname != IPV6_RTHDR && !ns_capable(net->user_ns, CAP_NET_RAW))\n\t\t\tbreak;\n\n\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\t\topt = ipv6_renew_options(sk, opt, optname,\n\t\t\t\t\t (struct ipv6_opt_hdr __user *)optval,\n\t\t\t\t\t optlen);\n\t\tif (IS_ERR(opt)) {\n\t\t\tretv = PTR_ERR(opt);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* routing header option needs extra check */\n\t\tretv = -EINVAL;\n\t\tif (optname == IPV6_RTHDR && opt && opt->srcrt) {\n\t\t\tstruct ipv6_rt_hdr *rthdr = opt->srcrt;\n\t\t\tswitch (rthdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\t\tcase IPV6_SRCRT_TYPE_2:\n\t\t\t\tif (rthdr->hdrlen != 2 ||\n\t\t\t\t    rthdr->segments_left != 1)\n\t\t\t\t\tgoto sticky_done;\n\n\t\t\t\tbreak;\n#endif\n\t\t\tdefault:\n\t\t\t\tgoto sticky_done;\n\t\t\t}\n\t\t}\n\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\nsticky_done:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase IPV6_PKTINFO:\n\t{\n\t\tstruct in6_pktinfo pkt;\n\n\t\tif (optlen == 0)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct in6_pktinfo) || !optval)\n\t\t\tgoto e_inval;\n\n\t\tif (copy_from_user(&pkt, optval, sizeof(struct in6_pktinfo))) {\n\t\t\t\tretv = -EFAULT;\n\t\t\t\tbreak;\n\t\t}\n\t\tif (sk->sk_bound_dev_if && pkt.ipi6_ifindex != sk->sk_bound_dev_if)\n\t\t\tgoto e_inval;\n\n\t\tnp->sticky_pktinfo.ipi6_ifindex = pkt.ipi6_ifindex;\n\t\tnp->sticky_pktinfo.ipi6_addr = pkt.ipi6_addr;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct ipv6_txoptions *opt = NULL;\n\t\tstruct msghdr msg;\n\t\tstruct flowi6 fl6;\n\t\tint junk;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\n\t\tif (optlen == 0)\n\t\t\tgoto update;\n\n\t\t/* 1K is probably excessive\n\t\t * 1K is surely not enough, 2K per standard header is 16K.\n\t\t */\n\t\tretv = -EINVAL;\n\t\tif (optlen > 64*1024)\n\t\t\tbreak;\n\n\t\topt = sock_kmalloc(sk, sizeof(*opt) + optlen, GFP_KERNEL);\n\t\tretv = -ENOBUFS;\n\t\tif (!opt)\n\t\t\tbreak;\n\n\t\tmemset(opt, 0, sizeof(*opt));\n\t\tatomic_set(&opt->refcnt, 1);\n\t\topt->tot_len = sizeof(*opt) + optlen;\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(opt+1, optval, optlen))\n\t\t\tgoto done;\n\n\t\tmsg.msg_controllen = optlen;\n\t\tmsg.msg_control = (void *)(opt+1);\n\n\t\tretv = ip6_datagram_send_ctl(net, sk, &msg, &fl6, opt, &junk,\n\t\t\t\t\t     &junk, &junk);\n\t\tif (retv)\n\t\t\tgoto done;\nupdate:\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\ndone:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\tcase IPV6_UNICAST_HOPS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->hop_limit = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_HOPS:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->mcast_hops = (val == -1 ? IPV6_DEFAULT_MCASTHOPS : val);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val != valbool)\n\t\t\tgoto e_inval;\n\t\tnp->mc_loop = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t{\n\t\tstruct net_device *dev = NULL;\n\t\tint ifindex;\n\n\t\tif (optlen != sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tifindex = (__force int)ntohl((__force __be32)val);\n\t\tif (ifindex == 0) {\n\t\t\tnp->ucast_oif = 0;\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tdev = dev_get_by_index(net, ifindex);\n\t\tretv = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\tretv = -EINVAL;\n\t\tif (sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tnp->ucast_oif = ifindex;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_IF:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tif (val) {\n\t\t\tstruct net_device *dev;\n\n\t\t\tif (sk->sk_bound_dev_if && sk->sk_bound_dev_if != val)\n\t\t\t\tgoto e_inval;\n\n\t\t\tdev = dev_get_by_index(net, val);\n\t\t\tif (!dev) {\n\t\t\t\tretv = -ENODEV;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdev_put(dev);\n\t\t}\n\t\tnp->mcast_oif = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_ADD_MEMBERSHIP)\n\t\t\tretv = ipv6_sock_mc_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_JOIN_ANYCAST)\n\t\t\tretv = ipv6_sock_ac_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_ac_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t{\n\t\tstruct group_req greq;\n\t\tstruct sockaddr_in6 *psin6;\n\n\t\tif (optlen < sizeof(struct group_req))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&greq, optval, sizeof(struct group_req)))\n\t\t\tbreak;\n\t\tif (greq.gr_group.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tpsin6 = (struct sockaddr_in6 *)&greq.gr_group;\n\t\tif (optname == MCAST_JOIN_GROUP)\n\t\t\tretv = ipv6_sock_mc_join(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t{\n\t\tstruct group_source_req greqs;\n\t\tint omode, add;\n\n\t\tif (optlen < sizeof(struct group_source_req))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&greqs, optval, sizeof(greqs))) {\n\t\t\tretv = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (greqs.gsr_group.ss_family != AF_INET6 ||\n\t\t    greqs.gsr_source.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tif (optname == MCAST_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == MCAST_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == MCAST_JOIN_SOURCE_GROUP) {\n\t\t\tstruct sockaddr_in6 *psin6;\n\n\t\t\tpsin6 = (struct sockaddr_in6 *)&greqs.gsr_group;\n\t\t\tretv = ipv6_sock_mc_join(sk, greqs.gsr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\t\t/* prior join w/ different source is ok */\n\t\t\tif (retv && retv != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* MCAST_LEAVE_SOURCE_GROUP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\tretv = ip6_mc_source(add, omode, sk, &greqs);\n\t\tbreak;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter *gsf;\n\n\t\tif (optlen < GROUP_FILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tgsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!gsf) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(gsf, optval, optlen)) {\n\t\t\tkfree(gsf);\n\t\t\tbreak;\n\t\t}\n\t\t/* numsrc >= (4G-140)/128 overflow in 32 bits */\n\t\tif (gsf->gf_numsrc >= 0x1ffffffU ||\n\t\t    gsf->gf_numsrc > sysctl_mld_max_msf) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tif (GROUP_FILTER_SIZE(gsf->gf_numsrc) > optlen) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tretv = ip6_mc_msfilter(sk, gsf);\n\t\tkfree(gsf);\n\n\t\tbreak;\n\t}\n\tcase IPV6_ROUTER_ALERT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = ip6_ra_control(sk, val);\n\t\tbreak;\n\tcase IPV6_MTU_DISCOVER:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < IPV6_PMTUDISC_DONT || val > IPV6_PMTUDISC_OMIT)\n\t\t\tgoto e_inval;\n\t\tnp->pmtudisc = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val && val < IPV6_MIN_MTU)\n\t\t\tgoto e_inval;\n\t\tnp->frag_size = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->recverr = valbool;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWINFO_SEND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->sndflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWLABEL_MGR:\n\t\tretv = ipv6_flowlabel_opt(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_IPSEC_POLICY:\n\tcase IPV6_XFRM_POLICY:\n\t\tretv = -EPERM;\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\tretv = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t    {\n\t\tunsigned int pref = 0;\n\t\tunsigned int prefmask = ~0;\n\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EINVAL;\n\n\t\t/* check PUBLIC/TMP/PUBTMP_DEFAULT conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_PUBLIC|\n\t\t\t       IPV6_PREFER_SRC_TMP|\n\t\t\t       IPV6_PREFER_SRC_PUBTMP_DEFAULT)) {\n\t\tcase IPV6_PREFER_SRC_PUBLIC:\n\t\t\tpref |= IPV6_PREFER_SRC_PUBLIC;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_TMP:\n\t\t\tpref |= IPV6_PREFER_SRC_TMP;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_PUBTMP_DEFAULT:\n\t\t\tbreak;\n\t\tcase 0:\n\t\t\tgoto pref_skip_pubtmp;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~(IPV6_PREFER_SRC_PUBLIC|\n\t\t\t      IPV6_PREFER_SRC_TMP);\npref_skip_pubtmp:\n\n\t\t/* check HOME/COA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_HOME|IPV6_PREFER_SRC_COA)) {\n\t\tcase IPV6_PREFER_SRC_HOME:\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_COA:\n\t\t\tpref |= IPV6_PREFER_SRC_COA;\n\t\tcase 0:\n\t\t\tgoto pref_skip_coa;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~IPV6_PREFER_SRC_COA;\npref_skip_coa:\n\n\t\t/* check CGA/NONCGA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_CGA|IPV6_PREFER_SRC_NONCGA)) {\n\t\tcase IPV6_PREFER_SRC_CGA:\n\t\tcase IPV6_PREFER_SRC_NONCGA:\n\t\tcase 0:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tnp->srcprefs = (np->srcprefs & prefmask) | pref;\n\t\tretv = 0;\n\n\t\tbreak;\n\t    }\n\tcase IPV6_MINHOPCOUNT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tnp->min_hopcount = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_DONTFRAG:\n\t\tnp->dontfrag = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tnp->autoflowlabel = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\n\treturn retv;\n\ne_inval:\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\treturn -EINVAL;\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nint sock_setsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tint val;\n\tint valbool;\n\tstruct linger ling;\n\tint ret = 0;\n\n\t/*\n\t *\tOptions without arguments\n\t */\n\n\tif (optname == SO_BINDTODEVICE)\n\t\treturn sock_bindtodevice(sk, optval, optlen);\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\n\tvalbool = val ? 1 : 0;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase SO_DEBUG:\n\t\tif (val && !capable(CAP_NET_ADMIN))\n\t\t\tret = -EACCES;\n\t\telse\n\t\t\tsock_valbool_flag(sk, SOCK_DBG, valbool);\n\t\tbreak;\n\tcase SO_REUSEADDR:\n\t\tsk->sk_reuse = (valbool ? SK_CAN_REUSE : SK_NO_REUSE);\n\t\tbreak;\n\tcase SO_TYPE:\n\tcase SO_PROTOCOL:\n\tcase SO_DOMAIN:\n\tcase SO_ERROR:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\tcase SO_DONTROUTE:\n\t\tsock_valbool_flag(sk, SOCK_LOCALROUTE, valbool);\n\t\tbreak;\n\tcase SO_BROADCAST:\n\t\tsock_valbool_flag(sk, SOCK_BROADCAST, valbool);\n\t\tbreak;\n\tcase SO_SNDBUF:\n\t\t/* Don't error on this BSD doesn't and if you think\n\t\t * about it this is right. Otherwise apps have to\n\t\t * play 'guess the biggest size' games. RCVBUF/SNDBUF\n\t\t * are treated in BSD as hints\n\t\t */\n\t\tval = min_t(u32, val, sysctl_wmem_max);\nset_sndbuf:\n\t\tsk->sk_userlocks |= SOCK_SNDBUF_LOCK;\n\t\tsk->sk_sndbuf = max_t(u32, val * 2, SOCK_MIN_SNDBUF);\n\t\t/* Wake up sending tasks if we upped the value. */\n\t\tsk->sk_write_space(sk);\n\t\tbreak;\n\n\tcase SO_SNDBUFFORCE:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tgoto set_sndbuf;\n\n\tcase SO_RCVBUF:\n\t\t/* Don't error on this BSD doesn't and if you think\n\t\t * about it this is right. Otherwise apps have to\n\t\t * play 'guess the biggest size' games. RCVBUF/SNDBUF\n\t\t * are treated in BSD as hints\n\t\t */\n\t\tval = min_t(u32, val, sysctl_rmem_max);\nset_rcvbuf:\n\t\tsk->sk_userlocks |= SOCK_RCVBUF_LOCK;\n\t\t/*\n\t\t * We double it on the way in to account for\n\t\t * \"struct sk_buff\" etc. overhead.   Applications\n\t\t * assume that the SO_RCVBUF setting they make will\n\t\t * allow that much actual data to be received on that\n\t\t * socket.\n\t\t *\n\t\t * Applications are unaware that \"struct sk_buff\" and\n\t\t * other overheads allocate from the receive buffer\n\t\t * during socket buffer allocation.\n\t\t *\n\t\t * And after considering the possible alternatives,\n\t\t * returning the value we actually used in getsockopt\n\t\t * is the most desirable behavior.\n\t\t */\n\t\tsk->sk_rcvbuf = max_t(u32, val * 2, SOCK_MIN_RCVBUF);\n\t\tbreak;\n\n\tcase SO_RCVBUFFORCE:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tgoto set_rcvbuf;\n\n\tcase SO_KEEPALIVE:\n#ifdef CONFIG_INET\n\t\tif (sk->sk_protocol == IPPROTO_TCP &&\n\t\t    sk->sk_type == SOCK_STREAM)\n\t\t\ttcp_set_keepalive(sk, valbool);\n#endif\n\t\tsock_valbool_flag(sk, SOCK_KEEPOPEN, valbool);\n\t\tbreak;\n\n\tcase SO_OOBINLINE:\n\t\tsock_valbool_flag(sk, SOCK_URGINLINE, valbool);\n\t\tbreak;\n\n\tcase SO_NO_CHECK:\n\t\tsk->sk_no_check = valbool;\n\t\tbreak;\n\n\tcase SO_PRIORITY:\n\t\tif ((val >= 0 && val <= 6) || capable(CAP_NET_ADMIN))\n\t\t\tsk->sk_priority = val;\n\t\telse\n\t\t\tret = -EPERM;\n\t\tbreak;\n\n\tcase SO_LINGER:\n\t\tif (optlen < sizeof(ling)) {\n\t\t\tret = -EINVAL;\t/* 1003.1g */\n\t\t\tbreak;\n\t\t}\n\t\tif (copy_from_user(&ling, optval, sizeof(ling))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!ling.l_onoff)\n\t\t\tsock_reset_flag(sk, SOCK_LINGER);\n\t\telse {\n#if (BITS_PER_LONG == 32)\n\t\t\tif ((unsigned int)ling.l_linger >= MAX_SCHEDULE_TIMEOUT/HZ)\n\t\t\t\tsk->sk_lingertime = MAX_SCHEDULE_TIMEOUT;\n\t\t\telse\n#endif\n\t\t\t\tsk->sk_lingertime = (unsigned int)ling.l_linger * HZ;\n\t\t\tsock_set_flag(sk, SOCK_LINGER);\n\t\t}\n\t\tbreak;\n\n\tcase SO_BSDCOMPAT:\n\t\tsock_warn_obsolete_bsdism(\"setsockopt\");\n\t\tbreak;\n\n\tcase SO_PASSCRED:\n\t\tif (valbool)\n\t\t\tset_bit(SOCK_PASSCRED, &sock->flags);\n\t\telse\n\t\t\tclear_bit(SOCK_PASSCRED, &sock->flags);\n\t\tbreak;\n\n\tcase SO_TIMESTAMP:\n\tcase SO_TIMESTAMPNS:\n\t\tif (valbool)  {\n\t\t\tif (optname == SO_TIMESTAMP)\n\t\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t\telse\n\t\t\t\tsock_set_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t\tsock_set_flag(sk, SOCK_RCVTSTAMP);\n\t\t\tsock_enable_timestamp(sk, SOCK_TIMESTAMP);\n\t\t} else {\n\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMP);\n\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t}\n\t\tbreak;\n\n\tcase SO_TIMESTAMPING:\n\t\tif (val & ~SOF_TIMESTAMPING_MASK) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_TX_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_TX_HARDWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_TX_SOFTWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_TX_SOFTWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_RX_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_RX_HARDWARE);\n\t\tif (val & SOF_TIMESTAMPING_RX_SOFTWARE)\n\t\t\tsock_enable_timestamp(sk,\n\t\t\t\t\t      SOCK_TIMESTAMPING_RX_SOFTWARE);\n\t\telse\n\t\t\tsock_disable_timestamp(sk,\n\t\t\t\t\t       (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE));\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_SOFTWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_SOFTWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_SYS_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_SYS_HARDWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_RAW_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_RAW_HARDWARE);\n\t\tbreak;\n\n\tcase SO_RCVLOWAT:\n\t\tif (val < 0)\n\t\t\tval = INT_MAX;\n\t\tsk->sk_rcvlowat = val ? : 1;\n\t\tbreak;\n\n\tcase SO_RCVTIMEO:\n\t\tret = sock_set_timeout(&sk->sk_rcvtimeo, optval, optlen);\n\t\tbreak;\n\n\tcase SO_SNDTIMEO:\n\t\tret = sock_set_timeout(&sk->sk_sndtimeo, optval, optlen);\n\t\tbreak;\n\n\tcase SO_ATTACH_FILTER:\n\t\tret = -EINVAL;\n\t\tif (optlen == sizeof(struct sock_fprog)) {\n\t\t\tstruct sock_fprog fprog;\n\n\t\t\tret = -EFAULT;\n\t\t\tif (copy_from_user(&fprog, optval, sizeof(fprog)))\n\t\t\t\tbreak;\n\n\t\t\tret = sk_attach_filter(&fprog, sk);\n\t\t}\n\t\tbreak;\n\n\tcase SO_DETACH_FILTER:\n\t\tret = sk_detach_filter(sk);\n\t\tbreak;\n\n\tcase SO_PASSSEC:\n\t\tif (valbool)\n\t\t\tset_bit(SOCK_PASSSEC, &sock->flags);\n\t\telse\n\t\t\tclear_bit(SOCK_PASSSEC, &sock->flags);\n\t\tbreak;\n\tcase SO_MARK:\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tsk->sk_mark = val;\n\t\tbreak;\n\n\t\t/* We implement the SO_SNDLOWAT etc to\n\t\t   not be settable (1003.1g 5.3) */\n\tcase SO_RXQ_OVFL:\n\t\tsock_valbool_flag(sk, SOCK_RXQ_OVFL, valbool);\n\t\tbreak;\n\n\tcase SO_WIFI_STATUS:\n\t\tsock_valbool_flag(sk, SOCK_WIFI_STATUS, valbool);\n\t\tbreak;\n\n\tcase SO_PEEK_OFF:\n\t\tif (sock->ops->set_peek_off)\n\t\t\tsock->ops->set_peek_off(sk, val);\n\t\telse\n\t\t\tret = -EOPNOTSUPP;\n\t\tbreak;\n\n\tcase SO_NOFCS:\n\t\tsock_valbool_flag(sk, SOCK_NOFCS, valbool);\n\t\tbreak;\n\n\tdefault:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\trelease_sock(sk);\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, valbool;\n\tint retv = -ENOPROTOOPT;\n\tbool needs_rtnl = setsockopt_needs_rtnl(optname);\n\n\tif (!optval)\n\t\tval = 0;\n\telse {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (get_user(val, (int __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\tval = 0;\n\t}\n\n\tvalbool = (val != 0);\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_setsockopt(sk, optname, optval, optlen);\n\n\tif (needs_rtnl)\n\t\trtnl_lock();\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\n\tcase IPV6_ADDRFORM:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val == PF_INET) {\n\t\t\tstruct ipv6_txoptions *opt;\n\t\t\tstruct sk_buff *pktopt;\n\n\t\t\tif (sk->sk_type == SOCK_RAW)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_protocol == IPPROTO_UDP ||\n\t\t\t    sk->sk_protocol == IPPROTO_UDPLITE) {\n\t\t\t\tstruct udp_sock *up = udp_sk(sk);\n\t\t\t\tif (up->pending == AF_INET6) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (sk->sk_protocol != IPPROTO_TCP)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t\tretv = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (ipv6_only_sock(sk) ||\n\t\t\t    !ipv6_addr_v4mapped(&sk->sk_v6_daddr)) {\n\t\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tfl6_free_socklist(sk);\n\t\t\tipv6_sock_mc_close(sk);\n\n\t\t\t/*\n\t\t\t * Sock is moving from IPv6 to IPv4 (sk_prot), so\n\t\t\t * remove it from the refcnt debug socks count in the\n\t\t\t * original family...\n\t\t\t */\n\t\t\tsk_refcnt_debug_dec(sk);\n\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = &tcp_prot;\n\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;\n\t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t\t} else {\n\t\t\t\tstruct proto *prot = &udp_prot;\n\n\t\t\t\tif (sk->sk_protocol == IPPROTO_UDPLITE)\n\t\t\t\t\tprot = &udplite_prot;\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = prot;\n\t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t}\n\t\t\topt = xchg(&np->opt, NULL);\n\t\t\tif (opt)\n\t\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t\t\tpktopt = xchg(&np->pktoptions, NULL);\n\t\t\tkfree_skb(pktopt);\n\n\t\t\tsk->sk_destruct = inet_sock_destruct;\n\t\t\t/*\n\t\t\t * ... and add it to the refcnt debug socks count\n\t\t\t * in the new family. -acme\n\t\t\t */\n\t\t\tsk_refcnt_debug_inc(sk);\n\t\t\tmodule_put(THIS_MODULE);\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\t\tgoto e_inval;\n\n\tcase IPV6_V6ONLY:\n\t\tif (optlen < sizeof(int) ||\n\t\t    inet_sk(sk)->inet_num)\n\t\t\tgoto e_inval;\n\t\tsk->sk_ipv6only = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxoinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxhlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxohlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.srcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.osrcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.hopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.ohopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.dstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.odstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < -1 || val > 0xff)\n\t\t\tgoto e_inval;\n\t\t/* RFC 3542, 6.5: default traffic class of 0x0 */\n\t\tif (val == -1)\n\t\t\tval = 0;\n\t\tnp->tclass = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxtclass = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxpmtu = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TRANSPARENT:\n\t\tif (valbool && !ns_capable(net->user_ns, CAP_NET_ADMIN) &&\n\t\t    !ns_capable(net->user_ns, CAP_NET_RAW)) {\n\t\t\tretv = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we don't have a separate transparent bit for IPV6 we use the one in the IPv4 socket */\n\t\tinet_sk(sk)->transparent = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxorigdstaddr = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\t/* remove any sticky options header with a zero option\n\t\t * length, per RFC3542.\n\t\t */\n\t\tif (optlen == 0)\n\t\t\toptval = NULL;\n\t\telse if (!optval)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct ipv6_opt_hdr) ||\n\t\t\t optlen & 0x7 || optlen > 8 * 255)\n\t\t\tgoto e_inval;\n\n\t\t/* hop-by-hop / destination options are privileged option */\n\t\tretv = -EPERM;\n\t\tif (optname != IPV6_RTHDR && !ns_capable(net->user_ns, CAP_NET_RAW))\n\t\t\tbreak;\n\n\t\topt = ipv6_renew_options(sk, np->opt, optname,\n\t\t\t\t\t (struct ipv6_opt_hdr __user *)optval,\n\t\t\t\t\t optlen);\n\t\tif (IS_ERR(opt)) {\n\t\t\tretv = PTR_ERR(opt);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* routing header option needs extra check */\n\t\tretv = -EINVAL;\n\t\tif (optname == IPV6_RTHDR && opt && opt->srcrt) {\n\t\t\tstruct ipv6_rt_hdr *rthdr = opt->srcrt;\n\t\t\tswitch (rthdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\t\tcase IPV6_SRCRT_TYPE_2:\n\t\t\t\tif (rthdr->hdrlen != 2 ||\n\t\t\t\t    rthdr->segments_left != 1)\n\t\t\t\t\tgoto sticky_done;\n\n\t\t\t\tbreak;\n#endif\n\t\t\tdefault:\n\t\t\t\tgoto sticky_done;\n\t\t\t}\n\t\t}\n\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\nsticky_done:\n\t\tif (opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t\tbreak;\n\t}\n\n\tcase IPV6_PKTINFO:\n\t{\n\t\tstruct in6_pktinfo pkt;\n\n\t\tif (optlen == 0)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct in6_pktinfo) || !optval)\n\t\t\tgoto e_inval;\n\n\t\tif (copy_from_user(&pkt, optval, sizeof(struct in6_pktinfo))) {\n\t\t\t\tretv = -EFAULT;\n\t\t\t\tbreak;\n\t\t}\n\t\tif (sk->sk_bound_dev_if && pkt.ipi6_ifindex != sk->sk_bound_dev_if)\n\t\t\tgoto e_inval;\n\n\t\tnp->sticky_pktinfo.ipi6_ifindex = pkt.ipi6_ifindex;\n\t\tnp->sticky_pktinfo.ipi6_addr = pkt.ipi6_addr;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct ipv6_txoptions *opt = NULL;\n\t\tstruct msghdr msg;\n\t\tstruct flowi6 fl6;\n\t\tint junk;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\n\t\tif (optlen == 0)\n\t\t\tgoto update;\n\n\t\t/* 1K is probably excessive\n\t\t * 1K is surely not enough, 2K per standard header is 16K.\n\t\t */\n\t\tretv = -EINVAL;\n\t\tif (optlen > 64*1024)\n\t\t\tbreak;\n\n\t\topt = sock_kmalloc(sk, sizeof(*opt) + optlen, GFP_KERNEL);\n\t\tretv = -ENOBUFS;\n\t\tif (!opt)\n\t\t\tbreak;\n\n\t\tmemset(opt, 0, sizeof(*opt));\n\t\topt->tot_len = sizeof(*opt) + optlen;\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(opt+1, optval, optlen))\n\t\t\tgoto done;\n\n\t\tmsg.msg_controllen = optlen;\n\t\tmsg.msg_control = (void *)(opt+1);\n\n\t\tretv = ip6_datagram_send_ctl(net, sk, &msg, &fl6, opt, &junk,\n\t\t\t\t\t     &junk, &junk);\n\t\tif (retv)\n\t\t\tgoto done;\nupdate:\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\ndone:\n\t\tif (opt)\n\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n\t\tbreak;\n\t}\n\tcase IPV6_UNICAST_HOPS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->hop_limit = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_HOPS:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->mcast_hops = (val == -1 ? IPV6_DEFAULT_MCASTHOPS : val);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val != valbool)\n\t\t\tgoto e_inval;\n\t\tnp->mc_loop = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t{\n\t\tstruct net_device *dev = NULL;\n\t\tint ifindex;\n\n\t\tif (optlen != sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tifindex = (__force int)ntohl((__force __be32)val);\n\t\tif (ifindex == 0) {\n\t\t\tnp->ucast_oif = 0;\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tdev = dev_get_by_index(net, ifindex);\n\t\tretv = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\tretv = -EINVAL;\n\t\tif (sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tnp->ucast_oif = ifindex;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_IF:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tif (val) {\n\t\t\tstruct net_device *dev;\n\n\t\t\tif (sk->sk_bound_dev_if && sk->sk_bound_dev_if != val)\n\t\t\t\tgoto e_inval;\n\n\t\t\tdev = dev_get_by_index(net, val);\n\t\t\tif (!dev) {\n\t\t\t\tretv = -ENODEV;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdev_put(dev);\n\t\t}\n\t\tnp->mcast_oif = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_ADD_MEMBERSHIP)\n\t\t\tretv = ipv6_sock_mc_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_JOIN_ANYCAST)\n\t\t\tretv = ipv6_sock_ac_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_ac_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t{\n\t\tstruct group_req greq;\n\t\tstruct sockaddr_in6 *psin6;\n\n\t\tif (optlen < sizeof(struct group_req))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&greq, optval, sizeof(struct group_req)))\n\t\t\tbreak;\n\t\tif (greq.gr_group.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tpsin6 = (struct sockaddr_in6 *)&greq.gr_group;\n\t\tif (optname == MCAST_JOIN_GROUP)\n\t\t\tretv = ipv6_sock_mc_join(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t{\n\t\tstruct group_source_req greqs;\n\t\tint omode, add;\n\n\t\tif (optlen < sizeof(struct group_source_req))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&greqs, optval, sizeof(greqs))) {\n\t\t\tretv = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (greqs.gsr_group.ss_family != AF_INET6 ||\n\t\t    greqs.gsr_source.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tif (optname == MCAST_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == MCAST_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == MCAST_JOIN_SOURCE_GROUP) {\n\t\t\tstruct sockaddr_in6 *psin6;\n\n\t\t\tpsin6 = (struct sockaddr_in6 *)&greqs.gsr_group;\n\t\t\tretv = ipv6_sock_mc_join(sk, greqs.gsr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\t\t/* prior join w/ different source is ok */\n\t\t\tif (retv && retv != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* MCAST_LEAVE_SOURCE_GROUP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\tretv = ip6_mc_source(add, omode, sk, &greqs);\n\t\tbreak;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter *gsf;\n\n\t\tif (optlen < GROUP_FILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tgsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!gsf) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(gsf, optval, optlen)) {\n\t\t\tkfree(gsf);\n\t\t\tbreak;\n\t\t}\n\t\t/* numsrc >= (4G-140)/128 overflow in 32 bits */\n\t\tif (gsf->gf_numsrc >= 0x1ffffffU ||\n\t\t    gsf->gf_numsrc > sysctl_mld_max_msf) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tif (GROUP_FILTER_SIZE(gsf->gf_numsrc) > optlen) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tretv = ip6_mc_msfilter(sk, gsf);\n\t\tkfree(gsf);\n\n\t\tbreak;\n\t}\n\tcase IPV6_ROUTER_ALERT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = ip6_ra_control(sk, val);\n\t\tbreak;\n\tcase IPV6_MTU_DISCOVER:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < IPV6_PMTUDISC_DONT || val > IPV6_PMTUDISC_OMIT)\n\t\t\tgoto e_inval;\n\t\tnp->pmtudisc = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val && val < IPV6_MIN_MTU)\n\t\t\tgoto e_inval;\n\t\tnp->frag_size = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->recverr = valbool;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWINFO_SEND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->sndflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWLABEL_MGR:\n\t\tretv = ipv6_flowlabel_opt(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_IPSEC_POLICY:\n\tcase IPV6_XFRM_POLICY:\n\t\tretv = -EPERM;\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\tretv = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t    {\n\t\tunsigned int pref = 0;\n\t\tunsigned int prefmask = ~0;\n\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EINVAL;\n\n\t\t/* check PUBLIC/TMP/PUBTMP_DEFAULT conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_PUBLIC|\n\t\t\t       IPV6_PREFER_SRC_TMP|\n\t\t\t       IPV6_PREFER_SRC_PUBTMP_DEFAULT)) {\n\t\tcase IPV6_PREFER_SRC_PUBLIC:\n\t\t\tpref |= IPV6_PREFER_SRC_PUBLIC;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_TMP:\n\t\t\tpref |= IPV6_PREFER_SRC_TMP;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_PUBTMP_DEFAULT:\n\t\t\tbreak;\n\t\tcase 0:\n\t\t\tgoto pref_skip_pubtmp;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~(IPV6_PREFER_SRC_PUBLIC|\n\t\t\t      IPV6_PREFER_SRC_TMP);\npref_skip_pubtmp:\n\n\t\t/* check HOME/COA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_HOME|IPV6_PREFER_SRC_COA)) {\n\t\tcase IPV6_PREFER_SRC_HOME:\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_COA:\n\t\t\tpref |= IPV6_PREFER_SRC_COA;\n\t\tcase 0:\n\t\t\tgoto pref_skip_coa;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~IPV6_PREFER_SRC_COA;\npref_skip_coa:\n\n\t\t/* check CGA/NONCGA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_CGA|IPV6_PREFER_SRC_NONCGA)) {\n\t\tcase IPV6_PREFER_SRC_CGA:\n\t\tcase IPV6_PREFER_SRC_NONCGA:\n\t\tcase 0:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tnp->srcprefs = (np->srcprefs & prefmask) | pref;\n\t\tretv = 0;\n\n\t\tbreak;\n\t    }\n\tcase IPV6_MINHOPCOUNT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tnp->min_hopcount = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_DONTFRAG:\n\t\tnp->dontfrag = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tnp->autoflowlabel = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\n\treturn retv;\n\ne_inval:\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\treturn -EINVAL;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, valbool;\n\tint retv = -ENOPROTOOPT;\n\tbool needs_rtnl = setsockopt_needs_rtnl(optname);\n\n\tif (!optval)\n\t\tval = 0;\n\telse {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (get_user(val, (int __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\tval = 0;\n\t}\n\n\tvalbool = (val != 0);\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_setsockopt(sk, optname, optval, optlen);\n\n\tif (needs_rtnl)\n\t\trtnl_lock();\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\n\tcase IPV6_ADDRFORM:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val == PF_INET) {\n\t\t\tstruct ipv6_txoptions *opt;\n\t\t\tstruct sk_buff *pktopt;\n\n\t\t\tif (sk->sk_type == SOCK_RAW)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_protocol == IPPROTO_UDP ||\n\t\t\t    sk->sk_protocol == IPPROTO_UDPLITE) {\n\t\t\t\tstruct udp_sock *up = udp_sk(sk);\n\t\t\t\tif (up->pending == AF_INET6) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (sk->sk_protocol != IPPROTO_TCP)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t\tretv = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (ipv6_only_sock(sk) ||\n\t\t\t    !ipv6_addr_v4mapped(&sk->sk_v6_daddr)) {\n\t\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tfl6_free_socklist(sk);\n\t\t\tipv6_sock_mc_close(sk);\n\n\t\t\t/*\n\t\t\t * Sock is moving from IPv6 to IPv4 (sk_prot), so\n\t\t\t * remove it from the refcnt debug socks count in the\n\t\t\t * original family...\n\t\t\t */\n\t\t\tsk_refcnt_debug_dec(sk);\n\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = &tcp_prot;\n\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;\n\t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t\t} else {\n\t\t\t\tstruct proto *prot = &udp_prot;\n\n\t\t\t\tif (sk->sk_protocol == IPPROTO_UDPLITE)\n\t\t\t\t\tprot = &udplite_prot;\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = prot;\n\t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t}\n\t\t\topt = xchg((__force struct ipv6_txoptions **)&np->opt,\n\t\t\t\t   NULL);\n\t\t\tif (opt) {\n\t\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\t\ttxopt_put(opt);\n\t\t\t}\n\t\t\tpktopt = xchg(&np->pktoptions, NULL);\n\t\t\tkfree_skb(pktopt);\n\n\t\t\tsk->sk_destruct = inet_sock_destruct;\n\t\t\t/*\n\t\t\t * ... and add it to the refcnt debug socks count\n\t\t\t * in the new family. -acme\n\t\t\t */\n\t\t\tsk_refcnt_debug_inc(sk);\n\t\t\tmodule_put(THIS_MODULE);\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\t\tgoto e_inval;\n\n\tcase IPV6_V6ONLY:\n\t\tif (optlen < sizeof(int) ||\n\t\t    inet_sk(sk)->inet_num)\n\t\t\tgoto e_inval;\n\t\tsk->sk_ipv6only = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxoinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxhlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxohlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.srcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.osrcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.hopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.ohopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.dstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.odstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < -1 || val > 0xff)\n\t\t\tgoto e_inval;\n\t\t/* RFC 3542, 6.5: default traffic class of 0x0 */\n\t\tif (val == -1)\n\t\t\tval = 0;\n\t\tnp->tclass = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxtclass = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxpmtu = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TRANSPARENT:\n\t\tif (valbool && !ns_capable(net->user_ns, CAP_NET_ADMIN) &&\n\t\t    !ns_capable(net->user_ns, CAP_NET_RAW)) {\n\t\t\tretv = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we don't have a separate transparent bit for IPV6 we use the one in the IPv4 socket */\n\t\tinet_sk(sk)->transparent = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxorigdstaddr = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\t/* remove any sticky options header with a zero option\n\t\t * length, per RFC3542.\n\t\t */\n\t\tif (optlen == 0)\n\t\t\toptval = NULL;\n\t\telse if (!optval)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct ipv6_opt_hdr) ||\n\t\t\t optlen & 0x7 || optlen > 8 * 255)\n\t\t\tgoto e_inval;\n\n\t\t/* hop-by-hop / destination options are privileged option */\n\t\tretv = -EPERM;\n\t\tif (optname != IPV6_RTHDR && !ns_capable(net->user_ns, CAP_NET_RAW))\n\t\t\tbreak;\n\n\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\t\topt = ipv6_renew_options(sk, opt, optname,\n\t\t\t\t\t (struct ipv6_opt_hdr __user *)optval,\n\t\t\t\t\t optlen);\n\t\tif (IS_ERR(opt)) {\n\t\t\tretv = PTR_ERR(opt);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* routing header option needs extra check */\n\t\tretv = -EINVAL;\n\t\tif (optname == IPV6_RTHDR && opt && opt->srcrt) {\n\t\t\tstruct ipv6_rt_hdr *rthdr = opt->srcrt;\n\t\t\tswitch (rthdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\t\tcase IPV6_SRCRT_TYPE_2:\n\t\t\t\tif (rthdr->hdrlen != 2 ||\n\t\t\t\t    rthdr->segments_left != 1)\n\t\t\t\t\tgoto sticky_done;\n\n\t\t\t\tbreak;\n#endif\n\t\t\tdefault:\n\t\t\t\tgoto sticky_done;\n\t\t\t}\n\t\t}\n\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\nsticky_done:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase IPV6_PKTINFO:\n\t{\n\t\tstruct in6_pktinfo pkt;\n\n\t\tif (optlen == 0)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct in6_pktinfo) || !optval)\n\t\t\tgoto e_inval;\n\n\t\tif (copy_from_user(&pkt, optval, sizeof(struct in6_pktinfo))) {\n\t\t\t\tretv = -EFAULT;\n\t\t\t\tbreak;\n\t\t}\n\t\tif (sk->sk_bound_dev_if && pkt.ipi6_ifindex != sk->sk_bound_dev_if)\n\t\t\tgoto e_inval;\n\n\t\tnp->sticky_pktinfo.ipi6_ifindex = pkt.ipi6_ifindex;\n\t\tnp->sticky_pktinfo.ipi6_addr = pkt.ipi6_addr;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct ipv6_txoptions *opt = NULL;\n\t\tstruct msghdr msg;\n\t\tstruct flowi6 fl6;\n\t\tint junk;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\n\t\tif (optlen == 0)\n\t\t\tgoto update;\n\n\t\t/* 1K is probably excessive\n\t\t * 1K is surely not enough, 2K per standard header is 16K.\n\t\t */\n\t\tretv = -EINVAL;\n\t\tif (optlen > 64*1024)\n\t\t\tbreak;\n\n\t\topt = sock_kmalloc(sk, sizeof(*opt) + optlen, GFP_KERNEL);\n\t\tretv = -ENOBUFS;\n\t\tif (!opt)\n\t\t\tbreak;\n\n\t\tmemset(opt, 0, sizeof(*opt));\n\t\tatomic_set(&opt->refcnt, 1);\n\t\topt->tot_len = sizeof(*opt) + optlen;\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(opt+1, optval, optlen))\n\t\t\tgoto done;\n\n\t\tmsg.msg_controllen = optlen;\n\t\tmsg.msg_control = (void *)(opt+1);\n\n\t\tretv = ip6_datagram_send_ctl(net, sk, &msg, &fl6, opt, &junk,\n\t\t\t\t\t     &junk, &junk);\n\t\tif (retv)\n\t\t\tgoto done;\nupdate:\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\ndone:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\tcase IPV6_UNICAST_HOPS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->hop_limit = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_HOPS:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->mcast_hops = (val == -1 ? IPV6_DEFAULT_MCASTHOPS : val);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val != valbool)\n\t\t\tgoto e_inval;\n\t\tnp->mc_loop = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t{\n\t\tstruct net_device *dev = NULL;\n\t\tint ifindex;\n\n\t\tif (optlen != sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tifindex = (__force int)ntohl((__force __be32)val);\n\t\tif (ifindex == 0) {\n\t\t\tnp->ucast_oif = 0;\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tdev = dev_get_by_index(net, ifindex);\n\t\tretv = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\tretv = -EINVAL;\n\t\tif (sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tnp->ucast_oif = ifindex;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_IF:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tif (val) {\n\t\t\tstruct net_device *dev;\n\n\t\t\tif (sk->sk_bound_dev_if && sk->sk_bound_dev_if != val)\n\t\t\t\tgoto e_inval;\n\n\t\t\tdev = dev_get_by_index(net, val);\n\t\t\tif (!dev) {\n\t\t\t\tretv = -ENODEV;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdev_put(dev);\n\t\t}\n\t\tnp->mcast_oif = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_ADD_MEMBERSHIP)\n\t\t\tretv = ipv6_sock_mc_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_JOIN_ANYCAST)\n\t\t\tretv = ipv6_sock_ac_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_ac_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t{\n\t\tstruct group_req greq;\n\t\tstruct sockaddr_in6 *psin6;\n\n\t\tif (optlen < sizeof(struct group_req))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&greq, optval, sizeof(struct group_req)))\n\t\t\tbreak;\n\t\tif (greq.gr_group.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tpsin6 = (struct sockaddr_in6 *)&greq.gr_group;\n\t\tif (optname == MCAST_JOIN_GROUP)\n\t\t\tretv = ipv6_sock_mc_join(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t{\n\t\tstruct group_source_req greqs;\n\t\tint omode, add;\n\n\t\tif (optlen < sizeof(struct group_source_req))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&greqs, optval, sizeof(greqs))) {\n\t\t\tretv = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (greqs.gsr_group.ss_family != AF_INET6 ||\n\t\t    greqs.gsr_source.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tif (optname == MCAST_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == MCAST_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == MCAST_JOIN_SOURCE_GROUP) {\n\t\t\tstruct sockaddr_in6 *psin6;\n\n\t\t\tpsin6 = (struct sockaddr_in6 *)&greqs.gsr_group;\n\t\t\tretv = ipv6_sock_mc_join(sk, greqs.gsr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\t\t/* prior join w/ different source is ok */\n\t\t\tif (retv && retv != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* MCAST_LEAVE_SOURCE_GROUP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\tretv = ip6_mc_source(add, omode, sk, &greqs);\n\t\tbreak;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter *gsf;\n\n\t\tif (optlen < GROUP_FILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tgsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!gsf) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(gsf, optval, optlen)) {\n\t\t\tkfree(gsf);\n\t\t\tbreak;\n\t\t}\n\t\t/* numsrc >= (4G-140)/128 overflow in 32 bits */\n\t\tif (gsf->gf_numsrc >= 0x1ffffffU ||\n\t\t    gsf->gf_numsrc > sysctl_mld_max_msf) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tif (GROUP_FILTER_SIZE(gsf->gf_numsrc) > optlen) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tretv = ip6_mc_msfilter(sk, gsf);\n\t\tkfree(gsf);\n\n\t\tbreak;\n\t}\n\tcase IPV6_ROUTER_ALERT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = ip6_ra_control(sk, val);\n\t\tbreak;\n\tcase IPV6_MTU_DISCOVER:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < IPV6_PMTUDISC_DONT || val > IPV6_PMTUDISC_OMIT)\n\t\t\tgoto e_inval;\n\t\tnp->pmtudisc = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val && val < IPV6_MIN_MTU)\n\t\t\tgoto e_inval;\n\t\tnp->frag_size = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->recverr = valbool;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWINFO_SEND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->sndflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWLABEL_MGR:\n\t\tretv = ipv6_flowlabel_opt(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_IPSEC_POLICY:\n\tcase IPV6_XFRM_POLICY:\n\t\tretv = -EPERM;\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\tretv = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t    {\n\t\tunsigned int pref = 0;\n\t\tunsigned int prefmask = ~0;\n\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EINVAL;\n\n\t\t/* check PUBLIC/TMP/PUBTMP_DEFAULT conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_PUBLIC|\n\t\t\t       IPV6_PREFER_SRC_TMP|\n\t\t\t       IPV6_PREFER_SRC_PUBTMP_DEFAULT)) {\n\t\tcase IPV6_PREFER_SRC_PUBLIC:\n\t\t\tpref |= IPV6_PREFER_SRC_PUBLIC;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_TMP:\n\t\t\tpref |= IPV6_PREFER_SRC_TMP;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_PUBTMP_DEFAULT:\n\t\t\tbreak;\n\t\tcase 0:\n\t\t\tgoto pref_skip_pubtmp;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~(IPV6_PREFER_SRC_PUBLIC|\n\t\t\t      IPV6_PREFER_SRC_TMP);\npref_skip_pubtmp:\n\n\t\t/* check HOME/COA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_HOME|IPV6_PREFER_SRC_COA)) {\n\t\tcase IPV6_PREFER_SRC_HOME:\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_COA:\n\t\t\tpref |= IPV6_PREFER_SRC_COA;\n\t\tcase 0:\n\t\t\tgoto pref_skip_coa;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~IPV6_PREFER_SRC_COA;\npref_skip_coa:\n\n\t\t/* check CGA/NONCGA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_CGA|IPV6_PREFER_SRC_NONCGA)) {\n\t\tcase IPV6_PREFER_SRC_CGA:\n\t\tcase IPV6_PREFER_SRC_NONCGA:\n\t\tcase 0:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tnp->srcprefs = (np->srcprefs & prefmask) | pref;\n\t\tretv = 0;\n\n\t\tbreak;\n\t    }\n\tcase IPV6_MINHOPCOUNT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tnp->min_hopcount = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_DONTFRAG:\n\t\tnp->dontfrag = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tnp->autoflowlabel = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\n\treturn retv;\n\ne_inval:\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\treturn -EINVAL;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int do_ipv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, int __user *optlen, unsigned int flags)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tint len;\n\tint val;\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_getsockopt(sk, optname, optval, optlen);\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tswitch (optname) {\n\tcase IPV6_ADDRFORM:\n\t\tif (sk->sk_protocol != IPPROTO_UDP &&\n\t\t    sk->sk_protocol != IPPROTO_UDPLITE &&\n\t\t    sk->sk_protocol != IPPROTO_TCP)\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -ENOTCONN;\n\t\tval = sk->sk_family;\n\t\tbreak;\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter gsf;\n\t\tint err;\n\n\t\tif (len < GROUP_FILTER_SIZE(0))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&gsf, optval, GROUP_FILTER_SIZE(0)))\n\t\t\treturn -EFAULT;\n\t\tif (gsf.gf_group.ss_family != AF_INET6)\n\t\t\treturn -EADDRNOTAVAIL;\n\t\tlock_sock(sk);\n\t\terr = ip6_mc_msfget(sk, &gsf,\n\t\t\t(struct group_filter __user *)optval, optlen);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct msghdr msg;\n\t\tstruct sk_buff *skb;\n\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\treturn -ENOPROTOOPT;\n\n\t\tmsg.msg_control = optval;\n\t\tmsg.msg_controllen = len;\n\t\tmsg.msg_flags = flags;\n\n\t\tlock_sock(sk);\n\t\tskb = np->pktoptions;\n\t\tif (skb)\n\t\t\tip6_datagram_recv_ctl(sk, &msg, skb);\n\t\trelease_sock(sk);\n\t\tif (!skb) {\n\t\t\tif (np->rxopt.bits.rxinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr : np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxhlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxtclass) {\n\t\t\t\tint tclass = (int)ip6_tclass(np->rcv_flowinfo);\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_TCLASS, sizeof(tclass), &tclass);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxoinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr :\n\t\t\t\t\t\t\t\t     np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxohlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxflow) {\n\t\t\t\t__be32 flowinfo = np->rcv_flowinfo;\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_FLOWINFO, sizeof(flowinfo), &flowinfo);\n\t\t\t}\n\t\t}\n\t\tlen -= msg.msg_controllen;\n\t\treturn put_user(len, optlen);\n\t}\n\tcase IPV6_MTU:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tval = 0;\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tval = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!val)\n\t\t\treturn -ENOTCONN;\n\t\tbreak;\n\t}\n\n\tcase IPV6_V6ONLY:\n\t\tval = sk->sk_ipv6only;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tval = np->rxopt.bits.rxinfo;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tval = np->rxopt.bits.rxoinfo;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tval = np->rxopt.bits.rxhlim;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tval = np->rxopt.bits.rxohlim;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tval = np->rxopt.bits.srcrt;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tval = np->rxopt.bits.osrcrt;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\n\t\tlock_sock(sk);\n\t\tlen = ipv6_getsockopt_sticky(sk, np->opt,\n\t\t\t\t\t     optname, optval, len);\n\t\trelease_sock(sk);\n\t\t/* check if ipv6_getsockopt_sticky() returns err code */\n\t\tif (len < 0)\n\t\t\treturn len;\n\t\treturn put_user(len, optlen);\n\t}\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tval = np->rxopt.bits.hopopts;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tval = np->rxopt.bits.ohopopts;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tval = np->rxopt.bits.dstopts;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tval = np->rxopt.bits.odstopts;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tval = np->tclass;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tval = np->rxopt.bits.rxtclass;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tval = np->rxopt.bits.rxflow;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tval = np->rxopt.bits.rxpmtu;\n\t\tbreak;\n\n\tcase IPV6_PATHMTU:\n\t{\n\t\tstruct dst_entry *dst;\n\t\tstruct ip6_mtuinfo mtuinfo;\n\n\t\tif (len < sizeof(mtuinfo))\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(mtuinfo);\n\t\tmemset(&mtuinfo, 0, sizeof(mtuinfo));\n\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tmtuinfo.ip6m_mtu = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!mtuinfo.ip6m_mtu)\n\t\t\treturn -ENOTCONN;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &mtuinfo, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_TRANSPARENT:\n\t\tval = inet_sk(sk)->transparent;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tval = np->rxopt.bits.rxorigdstaddr;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_HOPS:\n\tcase IPV6_MULTICAST_HOPS:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tif (optname == IPV6_UNICAST_HOPS)\n\t\t\tval = np->hop_limit;\n\t\telse\n\t\t\tval = np->mcast_hops;\n\n\t\tif (val < 0) {\n\t\t\trcu_read_lock();\n\t\t\tdst = __sk_dst_get(sk);\n\t\t\tif (dst)\n\t\t\t\tval = ip6_dst_hoplimit(dst);\n\t\t\trcu_read_unlock();\n\t\t}\n\n\t\tif (val < 0)\n\t\t\tval = sock_net(sk)->ipv6.devconf_all->hop_limit;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tval = np->mc_loop;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_IF:\n\t\tval = np->mcast_oif;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t\tval = (__force int)htonl((__u32) np->ucast_oif);\n\t\tbreak;\n\n\tcase IPV6_MTU_DISCOVER:\n\t\tval = np->pmtudisc;\n\t\tbreak;\n\n\tcase IPV6_RECVERR:\n\t\tval = np->recverr;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO_SEND:\n\t\tval = np->sndflow;\n\t\tbreak;\n\n\tcase IPV6_FLOWLABEL_MGR:\n\t{\n\t\tstruct in6_flowlabel_req freq;\n\t\tint flags;\n\n\t\tif (len < sizeof(freq))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_user(&freq, optval, sizeof(freq)))\n\t\t\treturn -EFAULT;\n\n\t\tif (freq.flr_action != IPV6_FL_A_GET)\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(freq);\n\t\tflags = freq.flr_flags;\n\n\t\tmemset(&freq, 0, sizeof(freq));\n\n\t\tval = ipv6_flowlabel_opt_get(sk, &freq, flags);\n\t\tif (val < 0)\n\t\t\treturn val;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &freq, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t\tval = 0;\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_TMP)\n\t\t\tval |= IPV6_PREFER_SRC_TMP;\n\t\telse if (np->srcprefs & IPV6_PREFER_SRC_PUBLIC)\n\t\t\tval |= IPV6_PREFER_SRC_PUBLIC;\n\t\telse {\n\t\t\t/* XXX: should we return system default? */\n\t\t\tval |= IPV6_PREFER_SRC_PUBTMP_DEFAULT;\n\t\t}\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_COA)\n\t\t\tval |= IPV6_PREFER_SRC_COA;\n\t\telse\n\t\t\tval |= IPV6_PREFER_SRC_HOME;\n\t\tbreak;\n\n\tcase IPV6_MINHOPCOUNT:\n\t\tval = np->min_hopcount;\n\t\tbreak;\n\n\tcase IPV6_DONTFRAG:\n\t\tval = np->dontfrag;\n\t\tbreak;\n\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tval = np->autoflowlabel;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\tlen = min_t(unsigned int, sizeof(int), len);\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}",
                        "code_after_change": "static int do_ipv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, int __user *optlen, unsigned int flags)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tint len;\n\tint val;\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_getsockopt(sk, optname, optval, optlen);\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tswitch (optname) {\n\tcase IPV6_ADDRFORM:\n\t\tif (sk->sk_protocol != IPPROTO_UDP &&\n\t\t    sk->sk_protocol != IPPROTO_UDPLITE &&\n\t\t    sk->sk_protocol != IPPROTO_TCP)\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -ENOTCONN;\n\t\tval = sk->sk_family;\n\t\tbreak;\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter gsf;\n\t\tint err;\n\n\t\tif (len < GROUP_FILTER_SIZE(0))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&gsf, optval, GROUP_FILTER_SIZE(0)))\n\t\t\treturn -EFAULT;\n\t\tif (gsf.gf_group.ss_family != AF_INET6)\n\t\t\treturn -EADDRNOTAVAIL;\n\t\tlock_sock(sk);\n\t\terr = ip6_mc_msfget(sk, &gsf,\n\t\t\t(struct group_filter __user *)optval, optlen);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct msghdr msg;\n\t\tstruct sk_buff *skb;\n\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\treturn -ENOPROTOOPT;\n\n\t\tmsg.msg_control = optval;\n\t\tmsg.msg_controllen = len;\n\t\tmsg.msg_flags = flags;\n\n\t\tlock_sock(sk);\n\t\tskb = np->pktoptions;\n\t\tif (skb)\n\t\t\tip6_datagram_recv_ctl(sk, &msg, skb);\n\t\trelease_sock(sk);\n\t\tif (!skb) {\n\t\t\tif (np->rxopt.bits.rxinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr : np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxhlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxtclass) {\n\t\t\t\tint tclass = (int)ip6_tclass(np->rcv_flowinfo);\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_TCLASS, sizeof(tclass), &tclass);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxoinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr :\n\t\t\t\t\t\t\t\t     np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxohlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxflow) {\n\t\t\t\t__be32 flowinfo = np->rcv_flowinfo;\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_FLOWINFO, sizeof(flowinfo), &flowinfo);\n\t\t\t}\n\t\t}\n\t\tlen -= msg.msg_controllen;\n\t\treturn put_user(len, optlen);\n\t}\n\tcase IPV6_MTU:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tval = 0;\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tval = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!val)\n\t\t\treturn -ENOTCONN;\n\t\tbreak;\n\t}\n\n\tcase IPV6_V6ONLY:\n\t\tval = sk->sk_ipv6only;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tval = np->rxopt.bits.rxinfo;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tval = np->rxopt.bits.rxoinfo;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tval = np->rxopt.bits.rxhlim;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tval = np->rxopt.bits.rxohlim;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tval = np->rxopt.bits.srcrt;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tval = np->rxopt.bits.osrcrt;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\tlock_sock(sk);\n\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\t\tlen = ipv6_getsockopt_sticky(sk, opt, optname, optval, len);\n\t\trelease_sock(sk);\n\t\t/* check if ipv6_getsockopt_sticky() returns err code */\n\t\tif (len < 0)\n\t\t\treturn len;\n\t\treturn put_user(len, optlen);\n\t}\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tval = np->rxopt.bits.hopopts;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tval = np->rxopt.bits.ohopopts;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tval = np->rxopt.bits.dstopts;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tval = np->rxopt.bits.odstopts;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tval = np->tclass;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tval = np->rxopt.bits.rxtclass;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tval = np->rxopt.bits.rxflow;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tval = np->rxopt.bits.rxpmtu;\n\t\tbreak;\n\n\tcase IPV6_PATHMTU:\n\t{\n\t\tstruct dst_entry *dst;\n\t\tstruct ip6_mtuinfo mtuinfo;\n\n\t\tif (len < sizeof(mtuinfo))\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(mtuinfo);\n\t\tmemset(&mtuinfo, 0, sizeof(mtuinfo));\n\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tmtuinfo.ip6m_mtu = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!mtuinfo.ip6m_mtu)\n\t\t\treturn -ENOTCONN;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &mtuinfo, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_TRANSPARENT:\n\t\tval = inet_sk(sk)->transparent;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tval = np->rxopt.bits.rxorigdstaddr;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_HOPS:\n\tcase IPV6_MULTICAST_HOPS:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tif (optname == IPV6_UNICAST_HOPS)\n\t\t\tval = np->hop_limit;\n\t\telse\n\t\t\tval = np->mcast_hops;\n\n\t\tif (val < 0) {\n\t\t\trcu_read_lock();\n\t\t\tdst = __sk_dst_get(sk);\n\t\t\tif (dst)\n\t\t\t\tval = ip6_dst_hoplimit(dst);\n\t\t\trcu_read_unlock();\n\t\t}\n\n\t\tif (val < 0)\n\t\t\tval = sock_net(sk)->ipv6.devconf_all->hop_limit;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tval = np->mc_loop;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_IF:\n\t\tval = np->mcast_oif;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t\tval = (__force int)htonl((__u32) np->ucast_oif);\n\t\tbreak;\n\n\tcase IPV6_MTU_DISCOVER:\n\t\tval = np->pmtudisc;\n\t\tbreak;\n\n\tcase IPV6_RECVERR:\n\t\tval = np->recverr;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO_SEND:\n\t\tval = np->sndflow;\n\t\tbreak;\n\n\tcase IPV6_FLOWLABEL_MGR:\n\t{\n\t\tstruct in6_flowlabel_req freq;\n\t\tint flags;\n\n\t\tif (len < sizeof(freq))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_user(&freq, optval, sizeof(freq)))\n\t\t\treturn -EFAULT;\n\n\t\tif (freq.flr_action != IPV6_FL_A_GET)\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(freq);\n\t\tflags = freq.flr_flags;\n\n\t\tmemset(&freq, 0, sizeof(freq));\n\n\t\tval = ipv6_flowlabel_opt_get(sk, &freq, flags);\n\t\tif (val < 0)\n\t\t\treturn val;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &freq, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t\tval = 0;\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_TMP)\n\t\t\tval |= IPV6_PREFER_SRC_TMP;\n\t\telse if (np->srcprefs & IPV6_PREFER_SRC_PUBLIC)\n\t\t\tval |= IPV6_PREFER_SRC_PUBLIC;\n\t\telse {\n\t\t\t/* XXX: should we return system default? */\n\t\t\tval |= IPV6_PREFER_SRC_PUBTMP_DEFAULT;\n\t\t}\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_COA)\n\t\t\tval |= IPV6_PREFER_SRC_COA;\n\t\telse\n\t\t\tval |= IPV6_PREFER_SRC_HOME;\n\t\tbreak;\n\n\tcase IPV6_MINHOPCOUNT:\n\t\tval = np->min_hopcount;\n\t\tbreak;\n\n\tcase IPV6_DONTFRAG:\n\t\tval = np->dontfrag;\n\t\tbreak;\n\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tval = np->autoflowlabel;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\tlen = min_t(unsigned int, sizeof(int), len);\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nint sock_setsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tint val;\n\tint valbool;\n\tstruct linger ling;\n\tint ret = 0;\n\n\t/*\n\t *\tOptions without arguments\n\t */\n\n\tif (optname == SO_BINDTODEVICE)\n\t\treturn sock_bindtodevice(sk, optval, optlen);\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\n\tvalbool = val ? 1 : 0;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase SO_DEBUG:\n\t\tif (val && !capable(CAP_NET_ADMIN))\n\t\t\tret = -EACCES;\n\t\telse\n\t\t\tsock_valbool_flag(sk, SOCK_DBG, valbool);\n\t\tbreak;\n\tcase SO_REUSEADDR:\n\t\tsk->sk_reuse = (valbool ? SK_CAN_REUSE : SK_NO_REUSE);\n\t\tbreak;\n\tcase SO_TYPE:\n\tcase SO_PROTOCOL:\n\tcase SO_DOMAIN:\n\tcase SO_ERROR:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\tcase SO_DONTROUTE:\n\t\tsock_valbool_flag(sk, SOCK_LOCALROUTE, valbool);\n\t\tbreak;\n\tcase SO_BROADCAST:\n\t\tsock_valbool_flag(sk, SOCK_BROADCAST, valbool);\n\t\tbreak;\n\tcase SO_SNDBUF:\n\t\t/* Don't error on this BSD doesn't and if you think\n\t\t * about it this is right. Otherwise apps have to\n\t\t * play 'guess the biggest size' games. RCVBUF/SNDBUF\n\t\t * are treated in BSD as hints\n\t\t */\n\t\tval = min_t(u32, val, sysctl_wmem_max);\nset_sndbuf:\n\t\tsk->sk_userlocks |= SOCK_SNDBUF_LOCK;\n\t\tsk->sk_sndbuf = max_t(u32, val * 2, SOCK_MIN_SNDBUF);\n\t\t/* Wake up sending tasks if we upped the value. */\n\t\tsk->sk_write_space(sk);\n\t\tbreak;\n\n\tcase SO_SNDBUFFORCE:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tgoto set_sndbuf;\n\n\tcase SO_RCVBUF:\n\t\t/* Don't error on this BSD doesn't and if you think\n\t\t * about it this is right. Otherwise apps have to\n\t\t * play 'guess the biggest size' games. RCVBUF/SNDBUF\n\t\t * are treated in BSD as hints\n\t\t */\n\t\tval = min_t(u32, val, sysctl_rmem_max);\nset_rcvbuf:\n\t\tsk->sk_userlocks |= SOCK_RCVBUF_LOCK;\n\t\t/*\n\t\t * We double it on the way in to account for\n\t\t * \"struct sk_buff\" etc. overhead.   Applications\n\t\t * assume that the SO_RCVBUF setting they make will\n\t\t * allow that much actual data to be received on that\n\t\t * socket.\n\t\t *\n\t\t * Applications are unaware that \"struct sk_buff\" and\n\t\t * other overheads allocate from the receive buffer\n\t\t * during socket buffer allocation.\n\t\t *\n\t\t * And after considering the possible alternatives,\n\t\t * returning the value we actually used in getsockopt\n\t\t * is the most desirable behavior.\n\t\t */\n\t\tsk->sk_rcvbuf = max_t(u32, val * 2, SOCK_MIN_RCVBUF);\n\t\tbreak;\n\n\tcase SO_RCVBUFFORCE:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tgoto set_rcvbuf;\n\n\tcase SO_KEEPALIVE:\n#ifdef CONFIG_INET\n\t\tif (sk->sk_protocol == IPPROTO_TCP &&\n\t\t    sk->sk_type == SOCK_STREAM)\n\t\t\ttcp_set_keepalive(sk, valbool);\n#endif\n\t\tsock_valbool_flag(sk, SOCK_KEEPOPEN, valbool);\n\t\tbreak;\n\n\tcase SO_OOBINLINE:\n\t\tsock_valbool_flag(sk, SOCK_URGINLINE, valbool);\n\t\tbreak;\n\n\tcase SO_NO_CHECK:\n\t\tsk->sk_no_check = valbool;\n\t\tbreak;\n\n\tcase SO_PRIORITY:\n\t\tif ((val >= 0 && val <= 6) || capable(CAP_NET_ADMIN))\n\t\t\tsk->sk_priority = val;\n\t\telse\n\t\t\tret = -EPERM;\n\t\tbreak;\n\n\tcase SO_LINGER:\n\t\tif (optlen < sizeof(ling)) {\n\t\t\tret = -EINVAL;\t/* 1003.1g */\n\t\t\tbreak;\n\t\t}\n\t\tif (copy_from_user(&ling, optval, sizeof(ling))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!ling.l_onoff)\n\t\t\tsock_reset_flag(sk, SOCK_LINGER);\n\t\telse {\n#if (BITS_PER_LONG == 32)\n\t\t\tif ((unsigned int)ling.l_linger >= MAX_SCHEDULE_TIMEOUT/HZ)\n\t\t\t\tsk->sk_lingertime = MAX_SCHEDULE_TIMEOUT;\n\t\t\telse\n#endif\n\t\t\t\tsk->sk_lingertime = (unsigned int)ling.l_linger * HZ;\n\t\t\tsock_set_flag(sk, SOCK_LINGER);\n\t\t}\n\t\tbreak;\n\n\tcase SO_BSDCOMPAT:\n\t\tsock_warn_obsolete_bsdism(\"setsockopt\");\n\t\tbreak;\n\n\tcase SO_PASSCRED:\n\t\tif (valbool)\n\t\t\tset_bit(SOCK_PASSCRED, &sock->flags);\n\t\telse\n\t\t\tclear_bit(SOCK_PASSCRED, &sock->flags);\n\t\tbreak;\n\n\tcase SO_TIMESTAMP:\n\tcase SO_TIMESTAMPNS:\n\t\tif (valbool)  {\n\t\t\tif (optname == SO_TIMESTAMP)\n\t\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t\telse\n\t\t\t\tsock_set_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t\tsock_set_flag(sk, SOCK_RCVTSTAMP);\n\t\t\tsock_enable_timestamp(sk, SOCK_TIMESTAMP);\n\t\t} else {\n\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMP);\n\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t}\n\t\tbreak;\n\n\tcase SO_TIMESTAMPING:\n\t\tif (val & ~SOF_TIMESTAMPING_MASK) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_TX_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_TX_HARDWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_TX_SOFTWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_TX_SOFTWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_RX_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_RX_HARDWARE);\n\t\tif (val & SOF_TIMESTAMPING_RX_SOFTWARE)\n\t\t\tsock_enable_timestamp(sk,\n\t\t\t\t\t      SOCK_TIMESTAMPING_RX_SOFTWARE);\n\t\telse\n\t\t\tsock_disable_timestamp(sk,\n\t\t\t\t\t       (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE));\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_SOFTWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_SOFTWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_SYS_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_SYS_HARDWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_RAW_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_RAW_HARDWARE);\n\t\tbreak;\n\n\tcase SO_RCVLOWAT:\n\t\tif (val < 0)\n\t\t\tval = INT_MAX;\n\t\tsk->sk_rcvlowat = val ? : 1;\n\t\tbreak;\n\n\tcase SO_RCVTIMEO:\n\t\tret = sock_set_timeout(&sk->sk_rcvtimeo, optval, optlen);\n\t\tbreak;\n\n\tcase SO_SNDTIMEO:\n\t\tret = sock_set_timeout(&sk->sk_sndtimeo, optval, optlen);\n\t\tbreak;\n\n\tcase SO_ATTACH_FILTER:\n\t\tret = -EINVAL;\n\t\tif (optlen == sizeof(struct sock_fprog)) {\n\t\t\tstruct sock_fprog fprog;\n\n\t\t\tret = -EFAULT;\n\t\t\tif (copy_from_user(&fprog, optval, sizeof(fprog)))\n\t\t\t\tbreak;\n\n\t\t\tret = sk_attach_filter(&fprog, sk);\n\t\t}\n\t\tbreak;\n\n\tcase SO_DETACH_FILTER:\n\t\tret = sk_detach_filter(sk);\n\t\tbreak;\n\n\tcase SO_PASSSEC:\n\t\tif (valbool)\n\t\t\tset_bit(SOCK_PASSSEC, &sock->flags);\n\t\telse\n\t\t\tclear_bit(SOCK_PASSSEC, &sock->flags);\n\t\tbreak;\n\tcase SO_MARK:\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tsk->sk_mark = val;\n\t\tbreak;\n\n\t\t/* We implement the SO_SNDLOWAT etc to\n\t\t   not be settable (1003.1g 5.3) */\n\tcase SO_RXQ_OVFL:\n\t\tsock_valbool_flag(sk, SOCK_RXQ_OVFL, valbool);\n\t\tbreak;\n\n\tcase SO_WIFI_STATUS:\n\t\tsock_valbool_flag(sk, SOCK_WIFI_STATUS, valbool);\n\t\tbreak;\n\n\tcase SO_PEEK_OFF:\n\t\tif (sock->ops->set_peek_off)\n\t\t\tsock->ops->set_peek_off(sk, val);\n\t\telse\n\t\t\tret = -EOPNOTSUPP;\n\t\tbreak;\n\n\tcase SO_NOFCS:\n\t\tsock_valbool_flag(sk, SOCK_NOFCS, valbool);\n\t\tbreak;\n\n\tdefault:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\trelease_sock(sk);\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int do_ipv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, int __user *optlen, unsigned int flags)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tint len;\n\tint val;\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_getsockopt(sk, optname, optval, optlen);\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tswitch (optname) {\n\tcase IPV6_ADDRFORM:\n\t\tif (sk->sk_protocol != IPPROTO_UDP &&\n\t\t    sk->sk_protocol != IPPROTO_UDPLITE &&\n\t\t    sk->sk_protocol != IPPROTO_TCP)\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -ENOTCONN;\n\t\tval = sk->sk_family;\n\t\tbreak;\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter gsf;\n\t\tint err;\n\n\t\tif (len < GROUP_FILTER_SIZE(0))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&gsf, optval, GROUP_FILTER_SIZE(0)))\n\t\t\treturn -EFAULT;\n\t\tif (gsf.gf_group.ss_family != AF_INET6)\n\t\t\treturn -EADDRNOTAVAIL;\n\t\tlock_sock(sk);\n\t\terr = ip6_mc_msfget(sk, &gsf,\n\t\t\t(struct group_filter __user *)optval, optlen);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct msghdr msg;\n\t\tstruct sk_buff *skb;\n\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\treturn -ENOPROTOOPT;\n\n\t\tmsg.msg_control = optval;\n\t\tmsg.msg_controllen = len;\n\t\tmsg.msg_flags = flags;\n\n\t\tlock_sock(sk);\n\t\tskb = np->pktoptions;\n\t\tif (skb)\n\t\t\tip6_datagram_recv_ctl(sk, &msg, skb);\n\t\trelease_sock(sk);\n\t\tif (!skb) {\n\t\t\tif (np->rxopt.bits.rxinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr : np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxhlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxtclass) {\n\t\t\t\tint tclass = (int)ip6_tclass(np->rcv_flowinfo);\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_TCLASS, sizeof(tclass), &tclass);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxoinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr :\n\t\t\t\t\t\t\t\t     np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxohlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxflow) {\n\t\t\t\t__be32 flowinfo = np->rcv_flowinfo;\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_FLOWINFO, sizeof(flowinfo), &flowinfo);\n\t\t\t}\n\t\t}\n\t\tlen -= msg.msg_controllen;\n\t\treturn put_user(len, optlen);\n\t}\n\tcase IPV6_MTU:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tval = 0;\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tval = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!val)\n\t\t\treturn -ENOTCONN;\n\t\tbreak;\n\t}\n\n\tcase IPV6_V6ONLY:\n\t\tval = sk->sk_ipv6only;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tval = np->rxopt.bits.rxinfo;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tval = np->rxopt.bits.rxoinfo;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tval = np->rxopt.bits.rxhlim;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tval = np->rxopt.bits.rxohlim;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tval = np->rxopt.bits.srcrt;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tval = np->rxopt.bits.osrcrt;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\n\t\tlock_sock(sk);\n\t\tlen = ipv6_getsockopt_sticky(sk, np->opt,\n\t\t\t\t\t     optname, optval, len);\n\t\trelease_sock(sk);\n\t\t/* check if ipv6_getsockopt_sticky() returns err code */\n\t\tif (len < 0)\n\t\t\treturn len;\n\t\treturn put_user(len, optlen);\n\t}\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tval = np->rxopt.bits.hopopts;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tval = np->rxopt.bits.ohopopts;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tval = np->rxopt.bits.dstopts;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tval = np->rxopt.bits.odstopts;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tval = np->tclass;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tval = np->rxopt.bits.rxtclass;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tval = np->rxopt.bits.rxflow;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tval = np->rxopt.bits.rxpmtu;\n\t\tbreak;\n\n\tcase IPV6_PATHMTU:\n\t{\n\t\tstruct dst_entry *dst;\n\t\tstruct ip6_mtuinfo mtuinfo;\n\n\t\tif (len < sizeof(mtuinfo))\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(mtuinfo);\n\t\tmemset(&mtuinfo, 0, sizeof(mtuinfo));\n\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tmtuinfo.ip6m_mtu = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!mtuinfo.ip6m_mtu)\n\t\t\treturn -ENOTCONN;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &mtuinfo, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_TRANSPARENT:\n\t\tval = inet_sk(sk)->transparent;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tval = np->rxopt.bits.rxorigdstaddr;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_HOPS:\n\tcase IPV6_MULTICAST_HOPS:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tif (optname == IPV6_UNICAST_HOPS)\n\t\t\tval = np->hop_limit;\n\t\telse\n\t\t\tval = np->mcast_hops;\n\n\t\tif (val < 0) {\n\t\t\trcu_read_lock();\n\t\t\tdst = __sk_dst_get(sk);\n\t\t\tif (dst)\n\t\t\t\tval = ip6_dst_hoplimit(dst);\n\t\t\trcu_read_unlock();\n\t\t}\n\n\t\tif (val < 0)\n\t\t\tval = sock_net(sk)->ipv6.devconf_all->hop_limit;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tval = np->mc_loop;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_IF:\n\t\tval = np->mcast_oif;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t\tval = (__force int)htonl((__u32) np->ucast_oif);\n\t\tbreak;\n\n\tcase IPV6_MTU_DISCOVER:\n\t\tval = np->pmtudisc;\n\t\tbreak;\n\n\tcase IPV6_RECVERR:\n\t\tval = np->recverr;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO_SEND:\n\t\tval = np->sndflow;\n\t\tbreak;\n\n\tcase IPV6_FLOWLABEL_MGR:\n\t{\n\t\tstruct in6_flowlabel_req freq;\n\t\tint flags;\n\n\t\tif (len < sizeof(freq))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_user(&freq, optval, sizeof(freq)))\n\t\t\treturn -EFAULT;\n\n\t\tif (freq.flr_action != IPV6_FL_A_GET)\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(freq);\n\t\tflags = freq.flr_flags;\n\n\t\tmemset(&freq, 0, sizeof(freq));\n\n\t\tval = ipv6_flowlabel_opt_get(sk, &freq, flags);\n\t\tif (val < 0)\n\t\t\treturn val;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &freq, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t\tval = 0;\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_TMP)\n\t\t\tval |= IPV6_PREFER_SRC_TMP;\n\t\telse if (np->srcprefs & IPV6_PREFER_SRC_PUBLIC)\n\t\t\tval |= IPV6_PREFER_SRC_PUBLIC;\n\t\telse {\n\t\t\t/* XXX: should we return system default? */\n\t\t\tval |= IPV6_PREFER_SRC_PUBTMP_DEFAULT;\n\t\t}\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_COA)\n\t\t\tval |= IPV6_PREFER_SRC_COA;\n\t\telse\n\t\t\tval |= IPV6_PREFER_SRC_HOME;\n\t\tbreak;\n\n\tcase IPV6_MINHOPCOUNT:\n\t\tval = np->min_hopcount;\n\t\tbreak;\n\n\tcase IPV6_DONTFRAG:\n\t\tval = np->dontfrag;\n\t\tbreak;\n\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tval = np->autoflowlabel;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\tlen = min_t(unsigned int, sizeof(int), len);\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int do_ipv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, int __user *optlen, unsigned int flags)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tint len;\n\tint val;\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_getsockopt(sk, optname, optval, optlen);\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tswitch (optname) {\n\tcase IPV6_ADDRFORM:\n\t\tif (sk->sk_protocol != IPPROTO_UDP &&\n\t\t    sk->sk_protocol != IPPROTO_UDPLITE &&\n\t\t    sk->sk_protocol != IPPROTO_TCP)\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -ENOTCONN;\n\t\tval = sk->sk_family;\n\t\tbreak;\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter gsf;\n\t\tint err;\n\n\t\tif (len < GROUP_FILTER_SIZE(0))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&gsf, optval, GROUP_FILTER_SIZE(0)))\n\t\t\treturn -EFAULT;\n\t\tif (gsf.gf_group.ss_family != AF_INET6)\n\t\t\treturn -EADDRNOTAVAIL;\n\t\tlock_sock(sk);\n\t\terr = ip6_mc_msfget(sk, &gsf,\n\t\t\t(struct group_filter __user *)optval, optlen);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct msghdr msg;\n\t\tstruct sk_buff *skb;\n\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\treturn -ENOPROTOOPT;\n\n\t\tmsg.msg_control = optval;\n\t\tmsg.msg_controllen = len;\n\t\tmsg.msg_flags = flags;\n\n\t\tlock_sock(sk);\n\t\tskb = np->pktoptions;\n\t\tif (skb)\n\t\t\tip6_datagram_recv_ctl(sk, &msg, skb);\n\t\trelease_sock(sk);\n\t\tif (!skb) {\n\t\t\tif (np->rxopt.bits.rxinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr : np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxhlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxtclass) {\n\t\t\t\tint tclass = (int)ip6_tclass(np->rcv_flowinfo);\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_TCLASS, sizeof(tclass), &tclass);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxoinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr :\n\t\t\t\t\t\t\t\t     np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxohlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxflow) {\n\t\t\t\t__be32 flowinfo = np->rcv_flowinfo;\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_FLOWINFO, sizeof(flowinfo), &flowinfo);\n\t\t\t}\n\t\t}\n\t\tlen -= msg.msg_controllen;\n\t\treturn put_user(len, optlen);\n\t}\n\tcase IPV6_MTU:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tval = 0;\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tval = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!val)\n\t\t\treturn -ENOTCONN;\n\t\tbreak;\n\t}\n\n\tcase IPV6_V6ONLY:\n\t\tval = sk->sk_ipv6only;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tval = np->rxopt.bits.rxinfo;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tval = np->rxopt.bits.rxoinfo;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tval = np->rxopt.bits.rxhlim;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tval = np->rxopt.bits.rxohlim;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tval = np->rxopt.bits.srcrt;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tval = np->rxopt.bits.osrcrt;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\tlock_sock(sk);\n\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\t\tlen = ipv6_getsockopt_sticky(sk, opt, optname, optval, len);\n\t\trelease_sock(sk);\n\t\t/* check if ipv6_getsockopt_sticky() returns err code */\n\t\tif (len < 0)\n\t\t\treturn len;\n\t\treturn put_user(len, optlen);\n\t}\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tval = np->rxopt.bits.hopopts;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tval = np->rxopt.bits.ohopopts;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tval = np->rxopt.bits.dstopts;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tval = np->rxopt.bits.odstopts;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tval = np->tclass;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tval = np->rxopt.bits.rxtclass;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tval = np->rxopt.bits.rxflow;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tval = np->rxopt.bits.rxpmtu;\n\t\tbreak;\n\n\tcase IPV6_PATHMTU:\n\t{\n\t\tstruct dst_entry *dst;\n\t\tstruct ip6_mtuinfo mtuinfo;\n\n\t\tif (len < sizeof(mtuinfo))\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(mtuinfo);\n\t\tmemset(&mtuinfo, 0, sizeof(mtuinfo));\n\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tmtuinfo.ip6m_mtu = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!mtuinfo.ip6m_mtu)\n\t\t\treturn -ENOTCONN;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &mtuinfo, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_TRANSPARENT:\n\t\tval = inet_sk(sk)->transparent;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tval = np->rxopt.bits.rxorigdstaddr;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_HOPS:\n\tcase IPV6_MULTICAST_HOPS:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tif (optname == IPV6_UNICAST_HOPS)\n\t\t\tval = np->hop_limit;\n\t\telse\n\t\t\tval = np->mcast_hops;\n\n\t\tif (val < 0) {\n\t\t\trcu_read_lock();\n\t\t\tdst = __sk_dst_get(sk);\n\t\t\tif (dst)\n\t\t\t\tval = ip6_dst_hoplimit(dst);\n\t\t\trcu_read_unlock();\n\t\t}\n\n\t\tif (val < 0)\n\t\t\tval = sock_net(sk)->ipv6.devconf_all->hop_limit;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tval = np->mc_loop;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_IF:\n\t\tval = np->mcast_oif;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t\tval = (__force int)htonl((__u32) np->ucast_oif);\n\t\tbreak;\n\n\tcase IPV6_MTU_DISCOVER:\n\t\tval = np->pmtudisc;\n\t\tbreak;\n\n\tcase IPV6_RECVERR:\n\t\tval = np->recverr;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO_SEND:\n\t\tval = np->sndflow;\n\t\tbreak;\n\n\tcase IPV6_FLOWLABEL_MGR:\n\t{\n\t\tstruct in6_flowlabel_req freq;\n\t\tint flags;\n\n\t\tif (len < sizeof(freq))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_user(&freq, optval, sizeof(freq)))\n\t\t\treturn -EFAULT;\n\n\t\tif (freq.flr_action != IPV6_FL_A_GET)\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(freq);\n\t\tflags = freq.flr_flags;\n\n\t\tmemset(&freq, 0, sizeof(freq));\n\n\t\tval = ipv6_flowlabel_opt_get(sk, &freq, flags);\n\t\tif (val < 0)\n\t\t\treturn val;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &freq, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t\tval = 0;\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_TMP)\n\t\t\tval |= IPV6_PREFER_SRC_TMP;\n\t\telse if (np->srcprefs & IPV6_PREFER_SRC_PUBLIC)\n\t\t\tval |= IPV6_PREFER_SRC_PUBLIC;\n\t\telse {\n\t\t\t/* XXX: should we return system default? */\n\t\t\tval |= IPV6_PREFER_SRC_PUBTMP_DEFAULT;\n\t\t}\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_COA)\n\t\t\tval |= IPV6_PREFER_SRC_COA;\n\t\telse\n\t\t\tval |= IPV6_PREFER_SRC_HOME;\n\t\tbreak;\n\n\tcase IPV6_MINHOPCOUNT:\n\t\tval = np->min_hopcount;\n\t\tbreak;\n\n\tcase IPV6_DONTFRAG:\n\t\tval = np->dontfrag;\n\t\tbreak;\n\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tval = np->autoflowlabel;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\tlen = min_t(unsigned int, sizeof(int), len);\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "asmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}",
                        "code_after_change": "asmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value[0] = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}",
                        "cve_id": "CVE-2014-9870"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nint sock_setsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tint val;\n\tint valbool;\n\tstruct linger ling;\n\tint ret = 0;\n\n\t/*\n\t *\tOptions without arguments\n\t */\n\n\tif (optname == SO_BINDTODEVICE)\n\t\treturn sock_bindtodevice(sk, optval, optlen);\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\n\tvalbool = val ? 1 : 0;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase SO_DEBUG:\n\t\tif (val && !capable(CAP_NET_ADMIN))\n\t\t\tret = -EACCES;\n\t\telse\n\t\t\tsock_valbool_flag(sk, SOCK_DBG, valbool);\n\t\tbreak;\n\tcase SO_REUSEADDR:\n\t\tsk->sk_reuse = (valbool ? SK_CAN_REUSE : SK_NO_REUSE);\n\t\tbreak;\n\tcase SO_TYPE:\n\tcase SO_PROTOCOL:\n\tcase SO_DOMAIN:\n\tcase SO_ERROR:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\tcase SO_DONTROUTE:\n\t\tsock_valbool_flag(sk, SOCK_LOCALROUTE, valbool);\n\t\tbreak;\n\tcase SO_BROADCAST:\n\t\tsock_valbool_flag(sk, SOCK_BROADCAST, valbool);\n\t\tbreak;\n\tcase SO_SNDBUF:\n\t\t/* Don't error on this BSD doesn't and if you think\n\t\t * about it this is right. Otherwise apps have to\n\t\t * play 'guess the biggest size' games. RCVBUF/SNDBUF\n\t\t * are treated in BSD as hints\n\t\t */\n\t\tval = min_t(u32, val, sysctl_wmem_max);\nset_sndbuf:\n\t\tsk->sk_userlocks |= SOCK_SNDBUF_LOCK;\n\t\tsk->sk_sndbuf = max_t(u32, val * 2, SOCK_MIN_SNDBUF);\n\t\t/* Wake up sending tasks if we upped the value. */\n\t\tsk->sk_write_space(sk);\n\t\tbreak;\n\n\tcase SO_SNDBUFFORCE:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tgoto set_sndbuf;\n\n\tcase SO_RCVBUF:\n\t\t/* Don't error on this BSD doesn't and if you think\n\t\t * about it this is right. Otherwise apps have to\n\t\t * play 'guess the biggest size' games. RCVBUF/SNDBUF\n\t\t * are treated in BSD as hints\n\t\t */\n\t\tval = min_t(u32, val, sysctl_rmem_max);\nset_rcvbuf:\n\t\tsk->sk_userlocks |= SOCK_RCVBUF_LOCK;\n\t\t/*\n\t\t * We double it on the way in to account for\n\t\t * \"struct sk_buff\" etc. overhead.   Applications\n\t\t * assume that the SO_RCVBUF setting they make will\n\t\t * allow that much actual data to be received on that\n\t\t * socket.\n\t\t *\n\t\t * Applications are unaware that \"struct sk_buff\" and\n\t\t * other overheads allocate from the receive buffer\n\t\t * during socket buffer allocation.\n\t\t *\n\t\t * And after considering the possible alternatives,\n\t\t * returning the value we actually used in getsockopt\n\t\t * is the most desirable behavior.\n\t\t */\n\t\tsk->sk_rcvbuf = max_t(u32, val * 2, SOCK_MIN_RCVBUF);\n\t\tbreak;\n\n\tcase SO_RCVBUFFORCE:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tgoto set_rcvbuf;\n\n\tcase SO_KEEPALIVE:\n#ifdef CONFIG_INET\n\t\tif (sk->sk_protocol == IPPROTO_TCP &&\n\t\t    sk->sk_type == SOCK_STREAM)\n\t\t\ttcp_set_keepalive(sk, valbool);\n#endif\n\t\tsock_valbool_flag(sk, SOCK_KEEPOPEN, valbool);\n\t\tbreak;\n\n\tcase SO_OOBINLINE:\n\t\tsock_valbool_flag(sk, SOCK_URGINLINE, valbool);\n\t\tbreak;\n\n\tcase SO_NO_CHECK:\n\t\tsk->sk_no_check = valbool;\n\t\tbreak;\n\n\tcase SO_PRIORITY:\n\t\tif ((val >= 0 && val <= 6) || capable(CAP_NET_ADMIN))\n\t\t\tsk->sk_priority = val;\n\t\telse\n\t\t\tret = -EPERM;\n\t\tbreak;\n\n\tcase SO_LINGER:\n\t\tif (optlen < sizeof(ling)) {\n\t\t\tret = -EINVAL;\t/* 1003.1g */\n\t\t\tbreak;\n\t\t}\n\t\tif (copy_from_user(&ling, optval, sizeof(ling))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!ling.l_onoff)\n\t\t\tsock_reset_flag(sk, SOCK_LINGER);\n\t\telse {\n#if (BITS_PER_LONG == 32)\n\t\t\tif ((unsigned int)ling.l_linger >= MAX_SCHEDULE_TIMEOUT/HZ)\n\t\t\t\tsk->sk_lingertime = MAX_SCHEDULE_TIMEOUT;\n\t\t\telse\n#endif\n\t\t\t\tsk->sk_lingertime = (unsigned int)ling.l_linger * HZ;\n\t\t\tsock_set_flag(sk, SOCK_LINGER);\n\t\t}\n\t\tbreak;\n\n\tcase SO_BSDCOMPAT:\n\t\tsock_warn_obsolete_bsdism(\"setsockopt\");\n\t\tbreak;\n\n\tcase SO_PASSCRED:\n\t\tif (valbool)\n\t\t\tset_bit(SOCK_PASSCRED, &sock->flags);\n\t\telse\n\t\t\tclear_bit(SOCK_PASSCRED, &sock->flags);\n\t\tbreak;\n\n\tcase SO_TIMESTAMP:\n\tcase SO_TIMESTAMPNS:\n\t\tif (valbool)  {\n\t\t\tif (optname == SO_TIMESTAMP)\n\t\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t\telse\n\t\t\t\tsock_set_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t\tsock_set_flag(sk, SOCK_RCVTSTAMP);\n\t\t\tsock_enable_timestamp(sk, SOCK_TIMESTAMP);\n\t\t} else {\n\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMP);\n\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t}\n\t\tbreak;\n\n\tcase SO_TIMESTAMPING:\n\t\tif (val & ~SOF_TIMESTAMPING_MASK) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_TX_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_TX_HARDWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_TX_SOFTWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_TX_SOFTWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_RX_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_RX_HARDWARE);\n\t\tif (val & SOF_TIMESTAMPING_RX_SOFTWARE)\n\t\t\tsock_enable_timestamp(sk,\n\t\t\t\t\t      SOCK_TIMESTAMPING_RX_SOFTWARE);\n\t\telse\n\t\t\tsock_disable_timestamp(sk,\n\t\t\t\t\t       (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE));\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_SOFTWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_SOFTWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_SYS_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_SYS_HARDWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_RAW_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_RAW_HARDWARE);\n\t\tbreak;\n\n\tcase SO_RCVLOWAT:\n\t\tif (val < 0)\n\t\t\tval = INT_MAX;\n\t\tsk->sk_rcvlowat = val ? : 1;\n\t\tbreak;\n\n\tcase SO_RCVTIMEO:\n\t\tret = sock_set_timeout(&sk->sk_rcvtimeo, optval, optlen);\n\t\tbreak;\n\n\tcase SO_SNDTIMEO:\n\t\tret = sock_set_timeout(&sk->sk_sndtimeo, optval, optlen);\n\t\tbreak;\n\n\tcase SO_ATTACH_FILTER:\n\t\tret = -EINVAL;\n\t\tif (optlen == sizeof(struct sock_fprog)) {\n\t\t\tstruct sock_fprog fprog;\n\n\t\t\tret = -EFAULT;\n\t\t\tif (copy_from_user(&fprog, optval, sizeof(fprog)))\n\t\t\t\tbreak;\n\n\t\t\tret = sk_attach_filter(&fprog, sk);\n\t\t}\n\t\tbreak;\n\n\tcase SO_DETACH_FILTER:\n\t\tret = sk_detach_filter(sk);\n\t\tbreak;\n\n\tcase SO_PASSSEC:\n\t\tif (valbool)\n\t\t\tset_bit(SOCK_PASSSEC, &sock->flags);\n\t\telse\n\t\t\tclear_bit(SOCK_PASSSEC, &sock->flags);\n\t\tbreak;\n\tcase SO_MARK:\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tsk->sk_mark = val;\n\t\tbreak;\n\n\t\t/* We implement the SO_SNDLOWAT etc to\n\t\t   not be settable (1003.1g 5.3) */\n\tcase SO_RXQ_OVFL:\n\t\tsock_valbool_flag(sk, SOCK_RXQ_OVFL, valbool);\n\t\tbreak;\n\n\tcase SO_WIFI_STATUS:\n\t\tsock_valbool_flag(sk, SOCK_WIFI_STATUS, valbool);\n\t\tbreak;\n\n\tcase SO_PEEK_OFF:\n\t\tif (sock->ops->set_peek_off)\n\t\t\tsock->ops->set_peek_off(sk, val);\n\t\telse\n\t\t\tret = -EOPNOTSUPP;\n\t\tbreak;\n\n\tcase SO_NOFCS:\n\t\tsock_valbool_flag(sk, SOCK_NOFCS, valbool);\n\t\tbreak;\n\n\tdefault:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\trelease_sock(sk);\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nasmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nasmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value[0] = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 437,
            "cve_id": "CVE-2014-0181",
            "code_snippet": "static int cgw_create_job(struct sk_buff *skb,  struct nlmsghdr *nlh)\n{\n\tstruct rtcanmsg *r;\n\tstruct cgw_job *gwj;\n\tu8 limhops = 0;\n\tint err = 0;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (nlmsg_len(nlh) < sizeof(*r))\n\t\treturn -EINVAL;\n\n\tr = nlmsg_data(nlh);\n\tif (r->can_family != AF_CAN)\n\t\treturn -EPFNOSUPPORT;\n\n\t/* so far we only support CAN -> CAN routings */\n\tif (r->gwtype != CGW_TYPE_CAN_CAN)\n\t\treturn -EINVAL;\n\n\tgwj = kmem_cache_alloc(cgw_cache, GFP_KERNEL);\n\tif (!gwj)\n\t\treturn -ENOMEM;\n\n\tgwj->handled_frames = 0;\n\tgwj->dropped_frames = 0;\n\tgwj->deleted_frames = 0;\n\tgwj->flags = r->flags;\n\tgwj->gwtype = r->gwtype;\n\n\terr = cgw_parse_attr(nlh, &gwj->mod, CGW_TYPE_CAN_CAN, &gwj->ccgw,\n\t\t\t     &limhops);\n\tif (err < 0)\n\t\tgoto out;\n\n\terr = -ENODEV;\n\n\t/* ifindex == 0 is not allowed for job creation */\n\tif (!gwj->ccgw.src_idx || !gwj->ccgw.dst_idx)\n\t\tgoto out;\n\n\tgwj->src.dev = __dev_get_by_index(&init_net, gwj->ccgw.src_idx);\n\n\tif (!gwj->src.dev)\n\t\tgoto out;\n\n\tif (gwj->src.dev->type != ARPHRD_CAN)\n\t\tgoto out;\n\n\tgwj->dst.dev = __dev_get_by_index(&init_net, gwj->ccgw.dst_idx);\n\n\tif (!gwj->dst.dev)\n\t\tgoto out;\n\n\tif (gwj->dst.dev->type != ARPHRD_CAN)\n\t\tgoto out;\n\n\tgwj->limit_hops = limhops;\n\n\tASSERT_RTNL();\n\n\terr = cgw_register_filter(gwj);\n\tif (!err)\n\t\thlist_add_head_rcu(&gwj->list, &cgw_list);\nout:\n\tif (err)\n\t\tkmem_cache_free(cgw_cache, gwj);\n\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
                        "code_after_change": "static int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int cgw_create_job(struct sk_buff *skb,  struct nlmsghdr *nlh)\n{\n\tstruct rtcanmsg *r;\n\tstruct cgw_job *gwj;\n\tu8 limhops = 0;\n\tint err = 0;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (nlmsg_len(nlh) < sizeof(*r))\n\t\treturn -EINVAL;\n\n\tr = nlmsg_data(nlh);\n\tif (r->can_family != AF_CAN)\n\t\treturn -EPFNOSUPPORT;\n\n\t/* so far we only support CAN -> CAN routings */\n\tif (r->gwtype != CGW_TYPE_CAN_CAN)\n\t\treturn -EINVAL;\n\n\tgwj = kmem_cache_alloc(cgw_cache, GFP_KERNEL);\n\tif (!gwj)\n\t\treturn -ENOMEM;\n\n\tgwj->handled_frames = 0;\n\tgwj->dropped_frames = 0;\n\tgwj->deleted_frames = 0;\n\tgwj->flags = r->flags;\n\tgwj->gwtype = r->gwtype;\n\n\terr = cgw_parse_attr(nlh, &gwj->mod, CGW_TYPE_CAN_CAN, &gwj->ccgw,\n\t\t\t     &limhops);\n\tif (err < 0)\n\t\tgoto out;\n\n\terr = -ENODEV;\n\n\t/* ifindex == 0 is not allowed for job creation */\n\tif (!gwj->ccgw.src_idx || !gwj->ccgw.dst_idx)\n\t\tgoto out;\n\n\tgwj->src.dev = __dev_get_by_index(&init_net, gwj->ccgw.src_idx);\n\n\tif (!gwj->src.dev)\n\t\tgoto out;\n\n\tif (gwj->src.dev->type != ARPHRD_CAN)\n\t\tgoto out;\n\n\tgwj->dst.dev = __dev_get_by_index(&init_net, gwj->ccgw.dst_idx);\n\n\tif (!gwj->dst.dev)\n\t\tgoto out;\n\n\tif (gwj->dst.dev->type != ARPHRD_CAN)\n\t\tgoto out;\n\n\tgwj->limit_hops = limhops;\n\n\tASSERT_RTNL();\n\n\terr = cgw_register_filter(gwj);\n\tif (!err)\n\t\thlist_add_head_rcu(&gwj->list, &cgw_list);\nout:\n\tif (err)\n\t\tkmem_cache_free(cgw_cache, gwj);\n\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}",
                        "code_after_change": "static int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int cgw_create_job(struct sk_buff *skb,  struct nlmsghdr *nlh)\n{\n\tstruct rtcanmsg *r;\n\tstruct cgw_job *gwj;\n\tu8 limhops = 0;\n\tint err = 0;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (nlmsg_len(nlh) < sizeof(*r))\n\t\treturn -EINVAL;\n\n\tr = nlmsg_data(nlh);\n\tif (r->can_family != AF_CAN)\n\t\treturn -EPFNOSUPPORT;\n\n\t/* so far we only support CAN -> CAN routings */\n\tif (r->gwtype != CGW_TYPE_CAN_CAN)\n\t\treturn -EINVAL;\n\n\tgwj = kmem_cache_alloc(cgw_cache, GFP_KERNEL);\n\tif (!gwj)\n\t\treturn -ENOMEM;\n\n\tgwj->handled_frames = 0;\n\tgwj->dropped_frames = 0;\n\tgwj->deleted_frames = 0;\n\tgwj->flags = r->flags;\n\tgwj->gwtype = r->gwtype;\n\n\terr = cgw_parse_attr(nlh, &gwj->mod, CGW_TYPE_CAN_CAN, &gwj->ccgw,\n\t\t\t     &limhops);\n\tif (err < 0)\n\t\tgoto out;\n\n\terr = -ENODEV;\n\n\t/* ifindex == 0 is not allowed for job creation */\n\tif (!gwj->ccgw.src_idx || !gwj->ccgw.dst_idx)\n\t\tgoto out;\n\n\tgwj->src.dev = __dev_get_by_index(&init_net, gwj->ccgw.src_idx);\n\n\tif (!gwj->src.dev)\n\t\tgoto out;\n\n\tif (gwj->src.dev->type != ARPHRD_CAN)\n\t\tgoto out;\n\n\tgwj->dst.dev = __dev_get_by_index(&init_net, gwj->ccgw.dst_idx);\n\n\tif (!gwj->dst.dev)\n\t\tgoto out;\n\n\tif (gwj->dst.dev->type != ARPHRD_CAN)\n\t\tgoto out;\n\n\tgwj->limit_hops = limhops;\n\n\tASSERT_RTNL();\n\n\terr = cgw_register_filter(gwj);\n\tif (!err)\n\t\thlist_add_head_rcu(&gwj->list, &cgw_list);\nout:\n\tif (err)\n\t\tkmem_cache_free(cgw_cache, gwj);\n\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int ovl_fill_super(struct super_block *sb, void *data, int silent)\n{\n\tstruct path lowerpath;\n\tstruct path upperpath;\n\tstruct path workpath;\n\tstruct inode *root_inode;\n\tstruct dentry *root_dentry;\n\tstruct ovl_entry *oe;\n\tstruct ovl_fs *ufs;\n\tstruct kstatfs statfs;\n\tint err;\n\n\terr = -ENOMEM;\n\tufs = kzalloc(sizeof(struct ovl_fs), GFP_KERNEL);\n\tif (!ufs)\n\t\tgoto out;\n\n\terr = ovl_parse_opt((char *) data, &ufs->config);\n\tif (err)\n\t\tgoto out_free_config;\n\n\t/* FIXME: workdir is not needed for a R/O mount */\n\terr = -EINVAL;\n\tif (!ufs->config.upperdir || !ufs->config.lowerdir ||\n\t    !ufs->config.workdir) {\n\t\tpr_err(\"overlayfs: missing upperdir or lowerdir or workdir\\n\");\n\t\tgoto out_free_config;\n\t}\n\n\terr = -ENOMEM;\n\toe = ovl_alloc_entry();\n\tif (oe == NULL)\n\t\tgoto out_free_config;\n\n\terr = ovl_mount_dir(ufs->config.upperdir, &upperpath);\n\tif (err)\n\t\tgoto out_free_oe;\n\n\terr = ovl_mount_dir(ufs->config.lowerdir, &lowerpath);\n\tif (err)\n\t\tgoto out_put_upperpath;\n\n\terr = ovl_mount_dir(ufs->config.workdir, &workpath);\n\tif (err)\n\t\tgoto out_put_lowerpath;\n\n\terr = -EINVAL;\n\tif (!S_ISDIR(upperpath.dentry->d_inode->i_mode) ||\n\t    !S_ISDIR(lowerpath.dentry->d_inode->i_mode) ||\n\t    !S_ISDIR(workpath.dentry->d_inode->i_mode)) {\n\t\tpr_err(\"overlayfs: upperdir or lowerdir or workdir not a directory\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (upperpath.mnt != workpath.mnt) {\n\t\tpr_err(\"overlayfs: workdir and upperdir must reside under the same mount\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\tif (!ovl_workdir_ok(workpath.dentry, upperpath.dentry)) {\n\t\tpr_err(\"overlayfs: workdir and upperdir must be separate subtrees\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (!ovl_is_allowed_fs_type(upperpath.dentry)) {\n\t\tpr_err(\"overlayfs: filesystem of upperdir is not supported\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (!ovl_is_allowed_fs_type(lowerpath.dentry)) {\n\t\tpr_err(\"overlayfs: filesystem of lowerdir is not supported\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\terr = vfs_statfs(&lowerpath, &statfs);\n\tif (err) {\n\t\tpr_err(\"overlayfs: statfs failed on lowerpath\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\tufs->lower_namelen = statfs.f_namelen;\n\n\tufs->upper_mnt = clone_private_mount(&upperpath);\n\terr = PTR_ERR(ufs->upper_mnt);\n\tif (IS_ERR(ufs->upper_mnt)) {\n\t\tpr_err(\"overlayfs: failed to clone upperpath\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tufs->lower_mnt = clone_private_mount(&lowerpath);\n\terr = PTR_ERR(ufs->lower_mnt);\n\tif (IS_ERR(ufs->lower_mnt)) {\n\t\tpr_err(\"overlayfs: failed to clone lowerpath\\n\");\n\t\tgoto out_put_upper_mnt;\n\t}\n\n\tufs->workdir = ovl_workdir_create(ufs->upper_mnt, workpath.dentry);\n\terr = PTR_ERR(ufs->workdir);\n\tif (IS_ERR(ufs->workdir)) {\n\t\tpr_err(\"overlayfs: failed to create directory %s/%s\\n\",\n\t\t       ufs->config.workdir, OVL_WORKDIR_NAME);\n\t\tgoto out_put_lower_mnt;\n\t}\n\n\t/*\n\t * Make lower_mnt R/O.  That way fchmod/fchown on lower file\n\t * will fail instead of modifying lower fs.\n\t */\n\tufs->lower_mnt->mnt_flags |= MNT_READONLY;\n\n\t/* If the upper fs is r/o, we mark overlayfs r/o too */\n\tif (ufs->upper_mnt->mnt_sb->s_flags & MS_RDONLY)\n\t\tsb->s_flags |= MS_RDONLY;\n\n\tsb->s_d_op = &ovl_dentry_operations;\n\n\terr = -ENOMEM;\n\troot_inode = ovl_new_inode(sb, S_IFDIR, oe);\n\tif (!root_inode)\n\t\tgoto out_put_workdir;\n\n\troot_dentry = d_make_root(root_inode);\n\tif (!root_dentry)\n\t\tgoto out_put_workdir;\n\n\tmntput(upperpath.mnt);\n\tmntput(lowerpath.mnt);\n\tpath_put(&workpath);\n\n\toe->__upperdentry = upperpath.dentry;\n\toe->lowerdentry = lowerpath.dentry;\n\n\troot_dentry->d_fsdata = oe;\n\n\tsb->s_magic = OVERLAYFS_SUPER_MAGIC;\n\tsb->s_op = &ovl_super_operations;\n\tsb->s_root = root_dentry;\n\tsb->s_fs_info = ufs;\n\n\treturn 0;\n\nout_put_workdir:\n\tdput(ufs->workdir);\nout_put_lower_mnt:\n\tmntput(ufs->lower_mnt);\nout_put_upper_mnt:\n\tmntput(ufs->upper_mnt);\nout_put_workpath:\n\tpath_put(&workpath);\nout_put_lowerpath:\n\tpath_put(&lowerpath);\nout_put_upperpath:\n\tpath_put(&upperpath);\nout_free_oe:\n\tkfree(oe);\nout_free_config:\n\tkfree(ufs->config.lowerdir);\n\tkfree(ufs->config.upperdir);\n\tkfree(ufs->config.workdir);\n\tkfree(ufs);\nout:\n\treturn err;\n}",
                        "code_after_change": "static int ovl_fill_super(struct super_block *sb, void *data, int silent)\n{\n\tstruct path lowerpath;\n\tstruct path upperpath;\n\tstruct path workpath;\n\tstruct inode *root_inode;\n\tstruct dentry *root_dentry;\n\tstruct ovl_entry *oe;\n\tstruct ovl_fs *ufs;\n\tstruct kstatfs statfs;\n\tint err;\n\n\terr = -ENOMEM;\n\tufs = kzalloc(sizeof(struct ovl_fs), GFP_KERNEL);\n\tif (!ufs)\n\t\tgoto out;\n\n\terr = ovl_parse_opt((char *) data, &ufs->config);\n\tif (err)\n\t\tgoto out_free_config;\n\n\t/* FIXME: workdir is not needed for a R/O mount */\n\terr = -EINVAL;\n\tif (!ufs->config.upperdir || !ufs->config.lowerdir ||\n\t    !ufs->config.workdir) {\n\t\tpr_err(\"overlayfs: missing upperdir or lowerdir or workdir\\n\");\n\t\tgoto out_free_config;\n\t}\n\n\terr = -ENOMEM;\n\toe = ovl_alloc_entry();\n\tif (oe == NULL)\n\t\tgoto out_free_config;\n\n\terr = ovl_mount_dir(ufs->config.upperdir, &upperpath);\n\tif (err)\n\t\tgoto out_free_oe;\n\n\terr = ovl_mount_dir(ufs->config.lowerdir, &lowerpath);\n\tif (err)\n\t\tgoto out_put_upperpath;\n\n\terr = ovl_mount_dir(ufs->config.workdir, &workpath);\n\tif (err)\n\t\tgoto out_put_lowerpath;\n\n\terr = -EINVAL;\n\tif (!S_ISDIR(upperpath.dentry->d_inode->i_mode) ||\n\t    !S_ISDIR(lowerpath.dentry->d_inode->i_mode) ||\n\t    !S_ISDIR(workpath.dentry->d_inode->i_mode)) {\n\t\tpr_err(\"overlayfs: upperdir or lowerdir or workdir not a directory\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (upperpath.mnt != workpath.mnt) {\n\t\tpr_err(\"overlayfs: workdir and upperdir must reside under the same mount\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\tif (!ovl_workdir_ok(workpath.dentry, upperpath.dentry)) {\n\t\tpr_err(\"overlayfs: workdir and upperdir must be separate subtrees\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (!ovl_is_allowed_fs_type(upperpath.dentry)) {\n\t\tpr_err(\"overlayfs: filesystem of upperdir is not supported\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (!ovl_is_allowed_fs_type(lowerpath.dentry)) {\n\t\tpr_err(\"overlayfs: filesystem of lowerdir is not supported\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\terr = vfs_statfs(&lowerpath, &statfs);\n\tif (err) {\n\t\tpr_err(\"overlayfs: statfs failed on lowerpath\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\tufs->lower_namelen = statfs.f_namelen;\n\n\tsb->s_stack_depth = max(upperpath.mnt->mnt_sb->s_stack_depth,\n\t\t\t\tlowerpath.mnt->mnt_sb->s_stack_depth) + 1;\n\n\terr = -EINVAL;\n\tif (sb->s_stack_depth > FILESYSTEM_MAX_STACK_DEPTH) {\n\t\tpr_err(\"overlayfs: maximum fs stacking depth exceeded\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tufs->upper_mnt = clone_private_mount(&upperpath);\n\terr = PTR_ERR(ufs->upper_mnt);\n\tif (IS_ERR(ufs->upper_mnt)) {\n\t\tpr_err(\"overlayfs: failed to clone upperpath\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tufs->lower_mnt = clone_private_mount(&lowerpath);\n\terr = PTR_ERR(ufs->lower_mnt);\n\tif (IS_ERR(ufs->lower_mnt)) {\n\t\tpr_err(\"overlayfs: failed to clone lowerpath\\n\");\n\t\tgoto out_put_upper_mnt;\n\t}\n\n\tufs->workdir = ovl_workdir_create(ufs->upper_mnt, workpath.dentry);\n\terr = PTR_ERR(ufs->workdir);\n\tif (IS_ERR(ufs->workdir)) {\n\t\tpr_err(\"overlayfs: failed to create directory %s/%s\\n\",\n\t\t       ufs->config.workdir, OVL_WORKDIR_NAME);\n\t\tgoto out_put_lower_mnt;\n\t}\n\n\t/*\n\t * Make lower_mnt R/O.  That way fchmod/fchown on lower file\n\t * will fail instead of modifying lower fs.\n\t */\n\tufs->lower_mnt->mnt_flags |= MNT_READONLY;\n\n\t/* If the upper fs is r/o, we mark overlayfs r/o too */\n\tif (ufs->upper_mnt->mnt_sb->s_flags & MS_RDONLY)\n\t\tsb->s_flags |= MS_RDONLY;\n\n\tsb->s_d_op = &ovl_dentry_operations;\n\n\terr = -ENOMEM;\n\troot_inode = ovl_new_inode(sb, S_IFDIR, oe);\n\tif (!root_inode)\n\t\tgoto out_put_workdir;\n\n\troot_dentry = d_make_root(root_inode);\n\tif (!root_dentry)\n\t\tgoto out_put_workdir;\n\n\tmntput(upperpath.mnt);\n\tmntput(lowerpath.mnt);\n\tpath_put(&workpath);\n\n\toe->__upperdentry = upperpath.dentry;\n\toe->lowerdentry = lowerpath.dentry;\n\n\troot_dentry->d_fsdata = oe;\n\n\tsb->s_magic = OVERLAYFS_SUPER_MAGIC;\n\tsb->s_op = &ovl_super_operations;\n\tsb->s_root = root_dentry;\n\tsb->s_fs_info = ufs;\n\n\treturn 0;\n\nout_put_workdir:\n\tdput(ufs->workdir);\nout_put_lower_mnt:\n\tmntput(ufs->lower_mnt);\nout_put_upper_mnt:\n\tmntput(ufs->upper_mnt);\nout_put_workpath:\n\tpath_put(&workpath);\nout_put_lowerpath:\n\tpath_put(&lowerpath);\nout_put_upperpath:\n\tpath_put(&upperpath);\nout_free_oe:\n\tkfree(oe);\nout_free_config:\n\tkfree(ufs->config.lowerdir);\n\tkfree(ufs->config.upperdir);\n\tkfree(ufs->config.workdir);\n\tkfree(ufs);\nout:\n\treturn err;\n}",
                        "cve_id": "CVE-2014-9922"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int cgw_create_job(struct sk_buff *skb,  struct nlmsghdr *nlh)\n{\n\tstruct rtcanmsg *r;\n\tstruct cgw_job *gwj;\n\tu8 limhops = 0;\n\tint err = 0;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (nlmsg_len(nlh) < sizeof(*r))\n\t\treturn -EINVAL;\n\n\tr = nlmsg_data(nlh);\n\tif (r->can_family != AF_CAN)\n\t\treturn -EPFNOSUPPORT;\n\n\t/* so far we only support CAN -> CAN routings */\n\tif (r->gwtype != CGW_TYPE_CAN_CAN)\n\t\treturn -EINVAL;\n\n\tgwj = kmem_cache_alloc(cgw_cache, GFP_KERNEL);\n\tif (!gwj)\n\t\treturn -ENOMEM;\n\n\tgwj->handled_frames = 0;\n\tgwj->dropped_frames = 0;\n\tgwj->deleted_frames = 0;\n\tgwj->flags = r->flags;\n\tgwj->gwtype = r->gwtype;\n\n\terr = cgw_parse_attr(nlh, &gwj->mod, CGW_TYPE_CAN_CAN, &gwj->ccgw,\n\t\t\t     &limhops);\n\tif (err < 0)\n\t\tgoto out;\n\n\terr = -ENODEV;\n\n\t/* ifindex == 0 is not allowed for job creation */\n\tif (!gwj->ccgw.src_idx || !gwj->ccgw.dst_idx)\n\t\tgoto out;\n\n\tgwj->src.dev = __dev_get_by_index(&init_net, gwj->ccgw.src_idx);\n\n\tif (!gwj->src.dev)\n\t\tgoto out;\n\n\tif (gwj->src.dev->type != ARPHRD_CAN)\n\t\tgoto out;\n\n\tgwj->dst.dev = __dev_get_by_index(&init_net, gwj->ccgw.dst_idx);\n\n\tif (!gwj->dst.dev)\n\t\tgoto out;\n\n\tif (gwj->dst.dev->type != ARPHRD_CAN)\n\t\tgoto out;\n\n\tgwj->limit_hops = limhops;\n\n\tASSERT_RTNL();\n\n\terr = cgw_register_filter(gwj);\n\tif (!err)\n\t\thlist_add_head_rcu(&gwj->list, &cgw_list);\nout:\n\tif (err)\n\t\tkmem_cache_free(cgw_cache, gwj);\n\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int ovl_fill_super(struct super_block *sb, void *data, int silent)\n{\n\tstruct path lowerpath;\n\tstruct path upperpath;\n\tstruct path workpath;\n\tstruct inode *root_inode;\n\tstruct dentry *root_dentry;\n\tstruct ovl_entry *oe;\n\tstruct ovl_fs *ufs;\n\tstruct kstatfs statfs;\n\tint err;\n\n\terr = -ENOMEM;\n\tufs = kzalloc(sizeof(struct ovl_fs), GFP_KERNEL);\n\tif (!ufs)\n\t\tgoto out;\n\n\terr = ovl_parse_opt((char *) data, &ufs->config);\n\tif (err)\n\t\tgoto out_free_config;\n\n\t/* FIXME: workdir is not needed for a R/O mount */\n\terr = -EINVAL;\n\tif (!ufs->config.upperdir || !ufs->config.lowerdir ||\n\t    !ufs->config.workdir) {\n\t\tpr_err(\"overlayfs: missing upperdir or lowerdir or workdir\\n\");\n\t\tgoto out_free_config;\n\t}\n\n\terr = -ENOMEM;\n\toe = ovl_alloc_entry();\n\tif (oe == NULL)\n\t\tgoto out_free_config;\n\n\terr = ovl_mount_dir(ufs->config.upperdir, &upperpath);\n\tif (err)\n\t\tgoto out_free_oe;\n\n\terr = ovl_mount_dir(ufs->config.lowerdir, &lowerpath);\n\tif (err)\n\t\tgoto out_put_upperpath;\n\n\terr = ovl_mount_dir(ufs->config.workdir, &workpath);\n\tif (err)\n\t\tgoto out_put_lowerpath;\n\n\terr = -EINVAL;\n\tif (!S_ISDIR(upperpath.dentry->d_inode->i_mode) ||\n\t    !S_ISDIR(lowerpath.dentry->d_inode->i_mode) ||\n\t    !S_ISDIR(workpath.dentry->d_inode->i_mode)) {\n\t\tpr_err(\"overlayfs: upperdir or lowerdir or workdir not a directory\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (upperpath.mnt != workpath.mnt) {\n\t\tpr_err(\"overlayfs: workdir and upperdir must reside under the same mount\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\tif (!ovl_workdir_ok(workpath.dentry, upperpath.dentry)) {\n\t\tpr_err(\"overlayfs: workdir and upperdir must be separate subtrees\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (!ovl_is_allowed_fs_type(upperpath.dentry)) {\n\t\tpr_err(\"overlayfs: filesystem of upperdir is not supported\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (!ovl_is_allowed_fs_type(lowerpath.dentry)) {\n\t\tpr_err(\"overlayfs: filesystem of lowerdir is not supported\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\terr = vfs_statfs(&lowerpath, &statfs);\n\tif (err) {\n\t\tpr_err(\"overlayfs: statfs failed on lowerpath\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\tufs->lower_namelen = statfs.f_namelen;\n\n\tufs->upper_mnt = clone_private_mount(&upperpath);\n\terr = PTR_ERR(ufs->upper_mnt);\n\tif (IS_ERR(ufs->upper_mnt)) {\n\t\tpr_err(\"overlayfs: failed to clone upperpath\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tufs->lower_mnt = clone_private_mount(&lowerpath);\n\terr = PTR_ERR(ufs->lower_mnt);\n\tif (IS_ERR(ufs->lower_mnt)) {\n\t\tpr_err(\"overlayfs: failed to clone lowerpath\\n\");\n\t\tgoto out_put_upper_mnt;\n\t}\n\n\tufs->workdir = ovl_workdir_create(ufs->upper_mnt, workpath.dentry);\n\terr = PTR_ERR(ufs->workdir);\n\tif (IS_ERR(ufs->workdir)) {\n\t\tpr_err(\"overlayfs: failed to create directory %s/%s\\n\",\n\t\t       ufs->config.workdir, OVL_WORKDIR_NAME);\n\t\tgoto out_put_lower_mnt;\n\t}\n\n\t/*\n\t * Make lower_mnt R/O.  That way fchmod/fchown on lower file\n\t * will fail instead of modifying lower fs.\n\t */\n\tufs->lower_mnt->mnt_flags |= MNT_READONLY;\n\n\t/* If the upper fs is r/o, we mark overlayfs r/o too */\n\tif (ufs->upper_mnt->mnt_sb->s_flags & MS_RDONLY)\n\t\tsb->s_flags |= MS_RDONLY;\n\n\tsb->s_d_op = &ovl_dentry_operations;\n\n\terr = -ENOMEM;\n\troot_inode = ovl_new_inode(sb, S_IFDIR, oe);\n\tif (!root_inode)\n\t\tgoto out_put_workdir;\n\n\troot_dentry = d_make_root(root_inode);\n\tif (!root_dentry)\n\t\tgoto out_put_workdir;\n\n\tmntput(upperpath.mnt);\n\tmntput(lowerpath.mnt);\n\tpath_put(&workpath);\n\n\toe->__upperdentry = upperpath.dentry;\n\toe->lowerdentry = lowerpath.dentry;\n\n\troot_dentry->d_fsdata = oe;\n\n\tsb->s_magic = OVERLAYFS_SUPER_MAGIC;\n\tsb->s_op = &ovl_super_operations;\n\tsb->s_root = root_dentry;\n\tsb->s_fs_info = ufs;\n\n\treturn 0;\n\nout_put_workdir:\n\tdput(ufs->workdir);\nout_put_lower_mnt:\n\tmntput(ufs->lower_mnt);\nout_put_upper_mnt:\n\tmntput(ufs->upper_mnt);\nout_put_workpath:\n\tpath_put(&workpath);\nout_put_lowerpath:\n\tpath_put(&lowerpath);\nout_put_upperpath:\n\tpath_put(&upperpath);\nout_free_oe:\n\tkfree(oe);\nout_free_config:\n\tkfree(ufs->config.lowerdir);\n\tkfree(ufs->config.upperdir);\n\tkfree(ufs->config.workdir);\n\tkfree(ufs);\nout:\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int ovl_fill_super(struct super_block *sb, void *data, int silent)\n{\n\tstruct path lowerpath;\n\tstruct path upperpath;\n\tstruct path workpath;\n\tstruct inode *root_inode;\n\tstruct dentry *root_dentry;\n\tstruct ovl_entry *oe;\n\tstruct ovl_fs *ufs;\n\tstruct kstatfs statfs;\n\tint err;\n\n\terr = -ENOMEM;\n\tufs = kzalloc(sizeof(struct ovl_fs), GFP_KERNEL);\n\tif (!ufs)\n\t\tgoto out;\n\n\terr = ovl_parse_opt((char *) data, &ufs->config);\n\tif (err)\n\t\tgoto out_free_config;\n\n\t/* FIXME: workdir is not needed for a R/O mount */\n\terr = -EINVAL;\n\tif (!ufs->config.upperdir || !ufs->config.lowerdir ||\n\t    !ufs->config.workdir) {\n\t\tpr_err(\"overlayfs: missing upperdir or lowerdir or workdir\\n\");\n\t\tgoto out_free_config;\n\t}\n\n\terr = -ENOMEM;\n\toe = ovl_alloc_entry();\n\tif (oe == NULL)\n\t\tgoto out_free_config;\n\n\terr = ovl_mount_dir(ufs->config.upperdir, &upperpath);\n\tif (err)\n\t\tgoto out_free_oe;\n\n\terr = ovl_mount_dir(ufs->config.lowerdir, &lowerpath);\n\tif (err)\n\t\tgoto out_put_upperpath;\n\n\terr = ovl_mount_dir(ufs->config.workdir, &workpath);\n\tif (err)\n\t\tgoto out_put_lowerpath;\n\n\terr = -EINVAL;\n\tif (!S_ISDIR(upperpath.dentry->d_inode->i_mode) ||\n\t    !S_ISDIR(lowerpath.dentry->d_inode->i_mode) ||\n\t    !S_ISDIR(workpath.dentry->d_inode->i_mode)) {\n\t\tpr_err(\"overlayfs: upperdir or lowerdir or workdir not a directory\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (upperpath.mnt != workpath.mnt) {\n\t\tpr_err(\"overlayfs: workdir and upperdir must reside under the same mount\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\tif (!ovl_workdir_ok(workpath.dentry, upperpath.dentry)) {\n\t\tpr_err(\"overlayfs: workdir and upperdir must be separate subtrees\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (!ovl_is_allowed_fs_type(upperpath.dentry)) {\n\t\tpr_err(\"overlayfs: filesystem of upperdir is not supported\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tif (!ovl_is_allowed_fs_type(lowerpath.dentry)) {\n\t\tpr_err(\"overlayfs: filesystem of lowerdir is not supported\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\terr = vfs_statfs(&lowerpath, &statfs);\n\tif (err) {\n\t\tpr_err(\"overlayfs: statfs failed on lowerpath\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\tufs->lower_namelen = statfs.f_namelen;\n\n\tsb->s_stack_depth = max(upperpath.mnt->mnt_sb->s_stack_depth,\n\t\t\t\tlowerpath.mnt->mnt_sb->s_stack_depth) + 1;\n\n\terr = -EINVAL;\n\tif (sb->s_stack_depth > FILESYSTEM_MAX_STACK_DEPTH) {\n\t\tpr_err(\"overlayfs: maximum fs stacking depth exceeded\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tufs->upper_mnt = clone_private_mount(&upperpath);\n\terr = PTR_ERR(ufs->upper_mnt);\n\tif (IS_ERR(ufs->upper_mnt)) {\n\t\tpr_err(\"overlayfs: failed to clone upperpath\\n\");\n\t\tgoto out_put_workpath;\n\t}\n\n\tufs->lower_mnt = clone_private_mount(&lowerpath);\n\terr = PTR_ERR(ufs->lower_mnt);\n\tif (IS_ERR(ufs->lower_mnt)) {\n\t\tpr_err(\"overlayfs: failed to clone lowerpath\\n\");\n\t\tgoto out_put_upper_mnt;\n\t}\n\n\tufs->workdir = ovl_workdir_create(ufs->upper_mnt, workpath.dentry);\n\terr = PTR_ERR(ufs->workdir);\n\tif (IS_ERR(ufs->workdir)) {\n\t\tpr_err(\"overlayfs: failed to create directory %s/%s\\n\",\n\t\t       ufs->config.workdir, OVL_WORKDIR_NAME);\n\t\tgoto out_put_lower_mnt;\n\t}\n\n\t/*\n\t * Make lower_mnt R/O.  That way fchmod/fchown on lower file\n\t * will fail instead of modifying lower fs.\n\t */\n\tufs->lower_mnt->mnt_flags |= MNT_READONLY;\n\n\t/* If the upper fs is r/o, we mark overlayfs r/o too */\n\tif (ufs->upper_mnt->mnt_sb->s_flags & MS_RDONLY)\n\t\tsb->s_flags |= MS_RDONLY;\n\n\tsb->s_d_op = &ovl_dentry_operations;\n\n\terr = -ENOMEM;\n\troot_inode = ovl_new_inode(sb, S_IFDIR, oe);\n\tif (!root_inode)\n\t\tgoto out_put_workdir;\n\n\troot_dentry = d_make_root(root_inode);\n\tif (!root_dentry)\n\t\tgoto out_put_workdir;\n\n\tmntput(upperpath.mnt);\n\tmntput(lowerpath.mnt);\n\tpath_put(&workpath);\n\n\toe->__upperdentry = upperpath.dentry;\n\toe->lowerdentry = lowerpath.dentry;\n\n\troot_dentry->d_fsdata = oe;\n\n\tsb->s_magic = OVERLAYFS_SUPER_MAGIC;\n\tsb->s_op = &ovl_super_operations;\n\tsb->s_root = root_dentry;\n\tsb->s_fs_info = ufs;\n\n\treturn 0;\n\nout_put_workdir:\n\tdput(ufs->workdir);\nout_put_lower_mnt:\n\tmntput(ufs->lower_mnt);\nout_put_upper_mnt:\n\tmntput(ufs->upper_mnt);\nout_put_workpath:\n\tpath_put(&workpath);\nout_put_lowerpath:\n\tpath_put(&lowerpath);\nout_put_upperpath:\n\tpath_put(&upperpath);\nout_free_oe:\n\tkfree(oe);\nout_free_config:\n\tkfree(ufs->config.lowerdir);\n\tkfree(ufs->config.upperdir);\n\tkfree(ufs->config.workdir);\n\tkfree(ufs);\nout:\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 456,
            "cve_id": "CVE-2014-0181",
            "code_snippet": "static int tc_modify_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm;\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q, *p;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\t/* Reinit, just in case something touches this. */\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\ttcm = nlmsg_data(n);\n\tclid = tcm->tcm_parent;\n\tq = p = NULL;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (clid != TC_H_INGRESS) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue_create(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\n\t\t/* It may be default qdisc, ignore it */\n\t\tif (q && q->handle == 0)\n\t\t\tq = NULL;\n\n\t\tif (!q || !tcm->tcm_handle || q->handle != tcm->tcm_handle) {\n\t\t\tif (tcm->tcm_handle) {\n\t\t\t\tif (q && !(n->nlmsg_flags & NLM_F_REPLACE))\n\t\t\t\t\treturn -EEXIST;\n\t\t\t\tif (TC_H_MIN(tcm->tcm_handle))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\t\t\tif (!q)\n\t\t\t\t\tgoto create_n_graft;\n\t\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\t\treturn -EEXIST;\n\t\t\t\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tif (q == p ||\n\t\t\t\t    (p && check_loop(q, p, 0)))\n\t\t\t\t\treturn -ELOOP;\n\t\t\t\tatomic_inc(&q->refcnt);\n\t\t\t\tgoto graft;\n\t\t\t} else {\n\t\t\t\tif (!q)\n\t\t\t\t\tgoto create_n_graft;\n\n\t\t\t\t/* This magic test requires explanation.\n\t\t\t\t *\n\t\t\t\t *   We know, that some child q is already\n\t\t\t\t *   attached to this parent and have choice:\n\t\t\t\t *   either to change it or to create/graft new one.\n\t\t\t\t *\n\t\t\t\t *   1. We are allowed to create/graft only\n\t\t\t\t *   if CREATE and REPLACE flags are set.\n\t\t\t\t *\n\t\t\t\t *   2. If EXCL is set, requestor wanted to say,\n\t\t\t\t *   that qdisc tcm_handle is not expected\n\t\t\t\t *   to exist, so that we choose create/graft too.\n\t\t\t\t *\n\t\t\t\t *   3. The last case is when no flags are set.\n\t\t\t\t *   Alas, it is sort of hole in API, we\n\t\t\t\t *   cannot decide what to do unambiguously.\n\t\t\t\t *   For now we select create/graft, if\n\t\t\t\t *   user gave KIND, which does not match existing.\n\t\t\t\t */\n\t\t\t\tif ((n->nlmsg_flags & NLM_F_CREATE) &&\n\t\t\t\t    (n->nlmsg_flags & NLM_F_REPLACE) &&\n\t\t\t\t    ((n->nlmsg_flags & NLM_F_EXCL) ||\n\t\t\t\t     (tca[TCA_KIND] &&\n\t\t\t\t      nla_strcmp(tca[TCA_KIND], q->ops->id))))\n\t\t\t\t\tgoto create_n_graft;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (!tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t}\n\n\t/* Change qdisc parameters */\n\tif (q == NULL)\n\t\treturn -ENOENT;\n\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\treturn -EEXIST;\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\terr = qdisc_change(q, tca);\n\tif (err == 0)\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\treturn err;\n\ncreate_n_graft:\n\tif (!(n->nlmsg_flags & NLM_F_CREATE))\n\t\treturn -ENOENT;\n\tif (clid == TC_H_INGRESS) {\n\t\tif (dev_ingress_queue(dev))\n\t\t\tq = qdisc_create(dev, dev_ingress_queue(dev), p,\n\t\t\t\t\t tcm->tcm_parent, tcm->tcm_parent,\n\t\t\t\t\t tca, &err);\n\t\telse\n\t\t\terr = -ENOENT;\n\t} else {\n\t\tstruct netdev_queue *dev_queue;\n\n\t\tif (p && p->ops->cl_ops && p->ops->cl_ops->select_queue)\n\t\t\tdev_queue = p->ops->cl_ops->select_queue(p, tcm);\n\t\telse if (p)\n\t\t\tdev_queue = p->dev_queue;\n\t\telse\n\t\t\tdev_queue = netdev_get_tx_queue(dev, 0);\n\n\t\tq = qdisc_create(dev, dev_queue, p,\n\t\t\t\t tcm->tcm_parent, tcm->tcm_handle,\n\t\t\t\t tca, &err);\n\t}\n\tif (q == NULL) {\n\t\tif (err == -EAGAIN)\n\t\t\tgoto replay;\n\t\treturn err;\n\t}\n\ngraft:\n\terr = qdisc_graft(dev, p, skb, n, clid, q, NULL);\n\tif (err) {\n\t\tif (q)\n\t\t\tqdisc_destroy(q);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}",
                        "code_after_change": "static int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int tc_modify_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm;\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q, *p;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\t/* Reinit, just in case something touches this. */\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\ttcm = nlmsg_data(n);\n\tclid = tcm->tcm_parent;\n\tq = p = NULL;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (clid != TC_H_INGRESS) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue_create(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\n\t\t/* It may be default qdisc, ignore it */\n\t\tif (q && q->handle == 0)\n\t\t\tq = NULL;\n\n\t\tif (!q || !tcm->tcm_handle || q->handle != tcm->tcm_handle) {\n\t\t\tif (tcm->tcm_handle) {\n\t\t\t\tif (q && !(n->nlmsg_flags & NLM_F_REPLACE))\n\t\t\t\t\treturn -EEXIST;\n\t\t\t\tif (TC_H_MIN(tcm->tcm_handle))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\t\t\tif (!q)\n\t\t\t\t\tgoto create_n_graft;\n\t\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\t\treturn -EEXIST;\n\t\t\t\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tif (q == p ||\n\t\t\t\t    (p && check_loop(q, p, 0)))\n\t\t\t\t\treturn -ELOOP;\n\t\t\t\tatomic_inc(&q->refcnt);\n\t\t\t\tgoto graft;\n\t\t\t} else {\n\t\t\t\tif (!q)\n\t\t\t\t\tgoto create_n_graft;\n\n\t\t\t\t/* This magic test requires explanation.\n\t\t\t\t *\n\t\t\t\t *   We know, that some child q is already\n\t\t\t\t *   attached to this parent and have choice:\n\t\t\t\t *   either to change it or to create/graft new one.\n\t\t\t\t *\n\t\t\t\t *   1. We are allowed to create/graft only\n\t\t\t\t *   if CREATE and REPLACE flags are set.\n\t\t\t\t *\n\t\t\t\t *   2. If EXCL is set, requestor wanted to say,\n\t\t\t\t *   that qdisc tcm_handle is not expected\n\t\t\t\t *   to exist, so that we choose create/graft too.\n\t\t\t\t *\n\t\t\t\t *   3. The last case is when no flags are set.\n\t\t\t\t *   Alas, it is sort of hole in API, we\n\t\t\t\t *   cannot decide what to do unambiguously.\n\t\t\t\t *   For now we select create/graft, if\n\t\t\t\t *   user gave KIND, which does not match existing.\n\t\t\t\t */\n\t\t\t\tif ((n->nlmsg_flags & NLM_F_CREATE) &&\n\t\t\t\t    (n->nlmsg_flags & NLM_F_REPLACE) &&\n\t\t\t\t    ((n->nlmsg_flags & NLM_F_EXCL) ||\n\t\t\t\t     (tca[TCA_KIND] &&\n\t\t\t\t      nla_strcmp(tca[TCA_KIND], q->ops->id))))\n\t\t\t\t\tgoto create_n_graft;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (!tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t}\n\n\t/* Change qdisc parameters */\n\tif (q == NULL)\n\t\treturn -ENOENT;\n\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\treturn -EEXIST;\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\terr = qdisc_change(q, tca);\n\tif (err == 0)\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\treturn err;\n\ncreate_n_graft:\n\tif (!(n->nlmsg_flags & NLM_F_CREATE))\n\t\treturn -ENOENT;\n\tif (clid == TC_H_INGRESS) {\n\t\tif (dev_ingress_queue(dev))\n\t\t\tq = qdisc_create(dev, dev_ingress_queue(dev), p,\n\t\t\t\t\t tcm->tcm_parent, tcm->tcm_parent,\n\t\t\t\t\t tca, &err);\n\t\telse\n\t\t\terr = -ENOENT;\n\t} else {\n\t\tstruct netdev_queue *dev_queue;\n\n\t\tif (p && p->ops->cl_ops && p->ops->cl_ops->select_queue)\n\t\t\tdev_queue = p->ops->cl_ops->select_queue(p, tcm);\n\t\telse if (p)\n\t\t\tdev_queue = p->dev_queue;\n\t\telse\n\t\t\tdev_queue = netdev_get_tx_queue(dev, 0);\n\n\t\tq = qdisc_create(dev, dev_queue, p,\n\t\t\t\t tcm->tcm_parent, tcm->tcm_handle,\n\t\t\t\t tca, &err);\n\t}\n\tif (q == NULL) {\n\t\tif (err == -EAGAIN)\n\t\t\tgoto replay;\n\t\treturn err;\n\t}\n\ngraft:\n\terr = qdisc_graft(dev, p, skb, n, clid, q, NULL);\n\tif (err) {\n\t\tif (q)\n\t\t\tqdisc_destroy(q);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}",
                        "code_after_change": "static int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int tc_modify_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm;\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q, *p;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\t/* Reinit, just in case something touches this. */\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\ttcm = nlmsg_data(n);\n\tclid = tcm->tcm_parent;\n\tq = p = NULL;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (clid != TC_H_INGRESS) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue_create(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\n\t\t/* It may be default qdisc, ignore it */\n\t\tif (q && q->handle == 0)\n\t\t\tq = NULL;\n\n\t\tif (!q || !tcm->tcm_handle || q->handle != tcm->tcm_handle) {\n\t\t\tif (tcm->tcm_handle) {\n\t\t\t\tif (q && !(n->nlmsg_flags & NLM_F_REPLACE))\n\t\t\t\t\treturn -EEXIST;\n\t\t\t\tif (TC_H_MIN(tcm->tcm_handle))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\t\t\tif (!q)\n\t\t\t\t\tgoto create_n_graft;\n\t\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\t\treturn -EEXIST;\n\t\t\t\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tif (q == p ||\n\t\t\t\t    (p && check_loop(q, p, 0)))\n\t\t\t\t\treturn -ELOOP;\n\t\t\t\tatomic_inc(&q->refcnt);\n\t\t\t\tgoto graft;\n\t\t\t} else {\n\t\t\t\tif (!q)\n\t\t\t\t\tgoto create_n_graft;\n\n\t\t\t\t/* This magic test requires explanation.\n\t\t\t\t *\n\t\t\t\t *   We know, that some child q is already\n\t\t\t\t *   attached to this parent and have choice:\n\t\t\t\t *   either to change it or to create/graft new one.\n\t\t\t\t *\n\t\t\t\t *   1. We are allowed to create/graft only\n\t\t\t\t *   if CREATE and REPLACE flags are set.\n\t\t\t\t *\n\t\t\t\t *   2. If EXCL is set, requestor wanted to say,\n\t\t\t\t *   that qdisc tcm_handle is not expected\n\t\t\t\t *   to exist, so that we choose create/graft too.\n\t\t\t\t *\n\t\t\t\t *   3. The last case is when no flags are set.\n\t\t\t\t *   Alas, it is sort of hole in API, we\n\t\t\t\t *   cannot decide what to do unambiguously.\n\t\t\t\t *   For now we select create/graft, if\n\t\t\t\t *   user gave KIND, which does not match existing.\n\t\t\t\t */\n\t\t\t\tif ((n->nlmsg_flags & NLM_F_CREATE) &&\n\t\t\t\t    (n->nlmsg_flags & NLM_F_REPLACE) &&\n\t\t\t\t    ((n->nlmsg_flags & NLM_F_EXCL) ||\n\t\t\t\t     (tca[TCA_KIND] &&\n\t\t\t\t      nla_strcmp(tca[TCA_KIND], q->ops->id))))\n\t\t\t\t\tgoto create_n_graft;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (!tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t}\n\n\t/* Change qdisc parameters */\n\tif (q == NULL)\n\t\treturn -ENOENT;\n\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\treturn -EEXIST;\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\terr = qdisc_change(q, tca);\n\tif (err == 0)\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\treturn err;\n\ncreate_n_graft:\n\tif (!(n->nlmsg_flags & NLM_F_CREATE))\n\t\treturn -ENOENT;\n\tif (clid == TC_H_INGRESS) {\n\t\tif (dev_ingress_queue(dev))\n\t\t\tq = qdisc_create(dev, dev_ingress_queue(dev), p,\n\t\t\t\t\t tcm->tcm_parent, tcm->tcm_parent,\n\t\t\t\t\t tca, &err);\n\t\telse\n\t\t\terr = -ENOENT;\n\t} else {\n\t\tstruct netdev_queue *dev_queue;\n\n\t\tif (p && p->ops->cl_ops && p->ops->cl_ops->select_queue)\n\t\t\tdev_queue = p->ops->cl_ops->select_queue(p, tcm);\n\t\telse if (p)\n\t\t\tdev_queue = p->dev_queue;\n\t\telse\n\t\t\tdev_queue = netdev_get_tx_queue(dev, 0);\n\n\t\tq = qdisc_create(dev, dev_queue, p,\n\t\t\t\t tcm->tcm_parent, tcm->tcm_handle,\n\t\t\t\t tca, &err);\n\t}\n\tif (q == NULL) {\n\t\tif (err == -EAGAIN)\n\t\t\tgoto replay;\n\t\treturn err;\n\t}\n\ngraft:\n\terr = qdisc_graft(dev, p, skb, n, clid, q, NULL);\n\tif (err) {\n\t\tif (q)\n\t\t\tqdisc_destroy(q);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}",
                        "code_after_change": "static int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(skb, net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int tc_modify_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm;\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q, *p;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\t/* Reinit, just in case something touches this. */\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\ttcm = nlmsg_data(n);\n\tclid = tcm->tcm_parent;\n\tq = p = NULL;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (clid != TC_H_INGRESS) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue_create(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\n\t\t/* It may be default qdisc, ignore it */\n\t\tif (q && q->handle == 0)\n\t\t\tq = NULL;\n\n\t\tif (!q || !tcm->tcm_handle || q->handle != tcm->tcm_handle) {\n\t\t\tif (tcm->tcm_handle) {\n\t\t\t\tif (q && !(n->nlmsg_flags & NLM_F_REPLACE))\n\t\t\t\t\treturn -EEXIST;\n\t\t\t\tif (TC_H_MIN(tcm->tcm_handle))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\t\t\tif (!q)\n\t\t\t\t\tgoto create_n_graft;\n\t\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\t\treturn -EEXIST;\n\t\t\t\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tif (q == p ||\n\t\t\t\t    (p && check_loop(q, p, 0)))\n\t\t\t\t\treturn -ELOOP;\n\t\t\t\tatomic_inc(&q->refcnt);\n\t\t\t\tgoto graft;\n\t\t\t} else {\n\t\t\t\tif (!q)\n\t\t\t\t\tgoto create_n_graft;\n\n\t\t\t\t/* This magic test requires explanation.\n\t\t\t\t *\n\t\t\t\t *   We know, that some child q is already\n\t\t\t\t *   attached to this parent and have choice:\n\t\t\t\t *   either to change it or to create/graft new one.\n\t\t\t\t *\n\t\t\t\t *   1. We are allowed to create/graft only\n\t\t\t\t *   if CREATE and REPLACE flags are set.\n\t\t\t\t *\n\t\t\t\t *   2. If EXCL is set, requestor wanted to say,\n\t\t\t\t *   that qdisc tcm_handle is not expected\n\t\t\t\t *   to exist, so that we choose create/graft too.\n\t\t\t\t *\n\t\t\t\t *   3. The last case is when no flags are set.\n\t\t\t\t *   Alas, it is sort of hole in API, we\n\t\t\t\t *   cannot decide what to do unambiguously.\n\t\t\t\t *   For now we select create/graft, if\n\t\t\t\t *   user gave KIND, which does not match existing.\n\t\t\t\t */\n\t\t\t\tif ((n->nlmsg_flags & NLM_F_CREATE) &&\n\t\t\t\t    (n->nlmsg_flags & NLM_F_REPLACE) &&\n\t\t\t\t    ((n->nlmsg_flags & NLM_F_EXCL) ||\n\t\t\t\t     (tca[TCA_KIND] &&\n\t\t\t\t      nla_strcmp(tca[TCA_KIND], q->ops->id))))\n\t\t\t\t\tgoto create_n_graft;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (!tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t}\n\n\t/* Change qdisc parameters */\n\tif (q == NULL)\n\t\treturn -ENOENT;\n\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\treturn -EEXIST;\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\terr = qdisc_change(q, tca);\n\tif (err == 0)\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\treturn err;\n\ncreate_n_graft:\n\tif (!(n->nlmsg_flags & NLM_F_CREATE))\n\t\treturn -ENOENT;\n\tif (clid == TC_H_INGRESS) {\n\t\tif (dev_ingress_queue(dev))\n\t\t\tq = qdisc_create(dev, dev_ingress_queue(dev), p,\n\t\t\t\t\t tcm->tcm_parent, tcm->tcm_parent,\n\t\t\t\t\t tca, &err);\n\t\telse\n\t\t\terr = -ENOENT;\n\t} else {\n\t\tstruct netdev_queue *dev_queue;\n\n\t\tif (p && p->ops->cl_ops && p->ops->cl_ops->select_queue)\n\t\t\tdev_queue = p->ops->cl_ops->select_queue(p, tcm);\n\t\telse if (p)\n\t\t\tdev_queue = p->dev_queue;\n\t\telse\n\t\t\tdev_queue = netdev_get_tx_queue(dev, 0);\n\n\t\tq = qdisc_create(dev, dev_queue, p,\n\t\t\t\t tcm->tcm_parent, tcm->tcm_handle,\n\t\t\t\t tca, &err);\n\t}\n\tif (q == NULL) {\n\t\tif (err == -EAGAIN)\n\t\t\tgoto replay;\n\t\treturn err;\n\t}\n\ngraft:\n\terr = qdisc_graft(dev, p, skb, n, clid, q, NULL);\n\tif (err) {\n\t\tif (q)\n\t\t\tqdisc_destroy(q);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(skb, net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 442,
            "cve_id": "CVE-2014-0181",
            "code_snippet": "static int rtnl_setlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct ifinfomsg *ifm;\n\tstruct net_device *dev;\n\tint err;\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tchar ifname[IFNAMSIZ];\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\terr = -EINVAL;\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse if (tb[IFLA_IFNAME])\n\t\tdev = __dev_get_by_name(net, ifname);\n\telse\n\t\tgoto errout;\n\n\tif (dev == NULL) {\n\t\terr = -ENODEV;\n\t\tgoto errout;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = do_setlink(skb, dev, ifm, tb, ifname, 0);\nerrout:\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}",
                        "code_after_change": "static int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(skb, net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int rtnl_setlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct ifinfomsg *ifm;\n\tstruct net_device *dev;\n\tint err;\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tchar ifname[IFNAMSIZ];\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\terr = -EINVAL;\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse if (tb[IFLA_IFNAME])\n\t\tdev = __dev_get_by_name(net, ifname);\n\telse\n\t\tgoto errout;\n\n\tif (dev == NULL) {\n\t\terr = -ENODEV;\n\t\tgoto errout;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = do_setlink(skb, dev, ifm, tb, ifname, 0);\nerrout:\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int rtnl_newlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tconst struct rtnl_link_ops *ops;\n\tconst struct rtnl_link_ops *m_ops = NULL;\n\tstruct net_device *dev;\n\tstruct net_device *master_dev = NULL;\n\tstruct ifinfomsg *ifm;\n\tchar kind[MODULE_NAME_LEN];\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tstruct nlattr *linkinfo[IFLA_INFO_MAX+1];\n\tint err;\n\n#ifdef CONFIG_MODULES\nreplay:\n#endif\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse {\n\t\tif (ifname[0])\n\t\t\tdev = __dev_get_by_name(net, ifname);\n\t\telse\n\t\t\tdev = NULL;\n\t}\n\n\tif (dev) {\n\t\tmaster_dev = netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\tm_ops = master_dev->rtnl_link_ops;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFLA_LINKINFO]) {\n\t\terr = nla_parse_nested(linkinfo, IFLA_INFO_MAX,\n\t\t\t\t       tb[IFLA_LINKINFO], ifla_info_policy);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t} else\n\t\tmemset(linkinfo, 0, sizeof(linkinfo));\n\n\tif (linkinfo[IFLA_INFO_KIND]) {\n\t\tnla_strlcpy(kind, linkinfo[IFLA_INFO_KIND], sizeof(kind));\n\t\tops = rtnl_link_ops_get(kind);\n\t} else {\n\t\tkind[0] = '\\0';\n\t\tops = NULL;\n\t}\n\n\tif (1) {\n\t\tstruct nlattr *attr[ops ? ops->maxtype + 1 : 0];\n\t\tstruct nlattr *slave_attr[m_ops ? m_ops->slave_maxtype + 1 : 0];\n\t\tstruct nlattr **data = NULL;\n\t\tstruct nlattr **slave_data = NULL;\n\t\tstruct net *dest_net;\n\n\t\tif (ops) {\n\t\t\tif (ops->maxtype && linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\terr = nla_parse_nested(attr, ops->maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_DATA],\n\t\t\t\t\t\t       ops->policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tdata = attr;\n\t\t\t}\n\t\t\tif (ops->validate) {\n\t\t\t\terr = ops->validate(tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (m_ops) {\n\t\t\tif (m_ops->slave_maxtype &&\n\t\t\t    linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\terr = nla_parse_nested(slave_attr,\n\t\t\t\t\t\t       m_ops->slave_maxtype,\n\t\t\t\t\t\t       linkinfo[IFLA_INFO_SLAVE_DATA],\n\t\t\t\t\t\t       m_ops->slave_policy);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tslave_data = slave_attr;\n\t\t\t}\n\t\t\tif (m_ops->slave_validate) {\n\t\t\t\terr = m_ops->slave_validate(tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (dev) {\n\t\t\tint modified = 0;\n\n\t\t\tif (nlh->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\treturn -EEXIST;\n\t\t\tif (nlh->nlmsg_flags & NLM_F_REPLACE)\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\tif (linkinfo[IFLA_INFO_DATA]) {\n\t\t\t\tif (!ops || ops != dev->rtnl_link_ops ||\n\t\t\t\t    !ops->changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = ops->changelink(dev, tb, data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\tif (linkinfo[IFLA_INFO_SLAVE_DATA]) {\n\t\t\t\tif (!m_ops || !m_ops->slave_changelink)\n\t\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t\terr = m_ops->slave_changelink(master_dev, dev,\n\t\t\t\t\t\t\t      tb, slave_data);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\tmodified = 1;\n\t\t\t}\n\n\t\t\treturn do_setlink(skb, dev, ifm, tb, ifname, modified);\n\t\t}\n\n\t\tif (!(nlh->nlmsg_flags & NLM_F_CREATE)) {\n\t\t\tif (ifm->ifi_index == 0 && tb[IFLA_GROUP])\n\t\t\t\treturn rtnl_group_changelink(skb, net,\n\t\t\t\t\t\tnla_get_u32(tb[IFLA_GROUP]),\n\t\t\t\t\t\tifm, tb);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tif (tb[IFLA_MAP] || tb[IFLA_MASTER] || tb[IFLA_PROTINFO])\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!ops) {\n#ifdef CONFIG_MODULES\n\t\t\tif (kind[0]) {\n\t\t\t\t__rtnl_unlock();\n\t\t\t\trequest_module(\"rtnl-link-%s\", kind);\n\t\t\t\trtnl_lock();\n\t\t\t\tops = rtnl_link_ops_get(kind);\n\t\t\t\tif (ops)\n\t\t\t\t\tgoto replay;\n\t\t\t}\n#endif\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (!ifname[0])\n\t\t\tsnprintf(ifname, IFNAMSIZ, \"%s%%d\", ops->kind);\n\n\t\tdest_net = rtnl_link_get_net(net, tb);\n\t\tif (IS_ERR(dest_net))\n\t\t\treturn PTR_ERR(dest_net);\n\n\t\tdev = rtnl_create_link(dest_net, ifname, ops, tb);\n\t\tif (IS_ERR(dev)) {\n\t\t\terr = PTR_ERR(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev->ifindex = ifm->ifi_index;\n\n\t\tif (ops->newlink) {\n\t\t\terr = ops->newlink(net, dev, tb, data);\n\t\t\t/* Drivers should call free_netdev() in ->destructor\n\t\t\t * and unregister it on failure so that device could be\n\t\t\t * finally freed in rtnl_unlock.\n\t\t\t */\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\terr = register_netdevice(dev);\n\t\t\tif (err < 0) {\n\t\t\t\tfree_netdev(dev);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\terr = rtnl_configure_link(dev, ifm);\n\t\tif (err < 0)\n\t\t\tunregister_netdevice(dev);\nout:\n\t\tput_net(dest_net);\n\t\treturn err;\n\t}\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}",
                        "code_after_change": "static int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int rtnl_setlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct ifinfomsg *ifm;\n\tstruct net_device *dev;\n\tint err;\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tchar ifname[IFNAMSIZ];\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\terr = -EINVAL;\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse if (tb[IFLA_IFNAME])\n\t\tdev = __dev_get_by_name(net, ifname);\n\telse\n\t\tgoto errout;\n\n\tif (dev == NULL) {\n\t\terr = -ENODEV;\n\t\tgoto errout;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = do_setlink(skb, dev, ifm, tb, ifname, 0);\nerrout:\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}",
                        "code_after_change": "static int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int rtnl_setlink(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct ifinfomsg *ifm;\n\tstruct net_device *dev;\n\tint err;\n\tstruct nlattr *tb[IFLA_MAX+1];\n\tchar ifname[IFNAMSIZ];\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFLA_MAX, ifla_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strlcpy(ifname, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tifname[0] = '\\0';\n\n\terr = -EINVAL;\n\tifm = nlmsg_data(nlh);\n\tif (ifm->ifi_index > 0)\n\t\tdev = __dev_get_by_index(net, ifm->ifi_index);\n\telse if (tb[IFLA_IFNAME])\n\t\tdev = __dev_get_by_name(net, ifname);\n\telse\n\t\tgoto errout;\n\n\tif (dev == NULL) {\n\t\terr = -ENODEV;\n\t\tgoto errout;\n\t}\n\n\terr = validate_linkmsg(dev, tb);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = do_setlink(skb, dev, ifm, tb, ifname, 0);\nerrout:\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 447,
            "cve_id": "CVE-2014-0181",
            "code_snippet": "static int dn_fib_rtm_delroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 0);\n\tif (!tb)\n\t\treturn -ESRCH;\n\n\treturn tb->delete(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
                        "code_after_change": "static int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int dn_fib_rtm_delroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 0);\n\tif (!tb)\n\t\treturn -ESRCH;\n\n\treturn tb->delete(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}",
                        "code_after_change": "static int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int dn_fib_rtm_delroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 0);\n\tif (!tb)\n\t\treturn -ESRCH;\n\n\treturn tb->delete(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}",
                        "code_after_change": "static int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int dn_fib_rtm_delroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 0);\n\tif (!tb)\n\t\treturn -ESRCH;\n\n\treturn tb->delete(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_nl_newaddr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct net_device *dev;\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[IFA_LOCAL] == NULL)\n\t\treturn -EINVAL;\n\n\tifm = nlmsg_data(nlh);\n\tif ((dev = __dev_get_by_index(&init_net, ifm->ifa_index)) == NULL)\n\t\treturn -ENODEV;\n\n\tif ((dn_db = rtnl_dereference(dev->dn_ptr)) == NULL) {\n\t\tdn_db = dn_dev_create(dev, &err);\n\t\tif (!dn_db)\n\t\t\treturn err;\n\t}\n\n\tif ((ifa = dn_dev_alloc_ifa()) == NULL)\n\t\treturn -ENOBUFS;\n\n\tif (tb[IFA_ADDRESS] == NULL)\n\t\ttb[IFA_ADDRESS] = tb[IFA_LOCAL];\n\n\tifa->ifa_local = nla_get_le16(tb[IFA_LOCAL]);\n\tifa->ifa_address = nla_get_le16(tb[IFA_ADDRESS]);\n\tifa->ifa_flags = tb[IFA_FLAGS] ? nla_get_u32(tb[IFA_FLAGS]) :\n\t\t\t\t\t ifm->ifa_flags;\n\tifa->ifa_scope = ifm->ifa_scope;\n\tifa->ifa_dev = dn_db;\n\n\tif (tb[IFA_LABEL])\n\t\tnla_strlcpy(ifa->ifa_label, tb[IFA_LABEL], IFNAMSIZ);\n\telse\n\t\tmemcpy(ifa->ifa_label, dev->name, IFNAMSIZ);\n\n\terr = dn_dev_insert_ifa(dn_db, ifa);\n\tif (err)\n\t\tdn_dev_free_ifa(ifa);\n\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 455,
            "cve_id": "CVE-2014-0181",
            "code_snippet": "static int tc_ctl_tfilter(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tspinlock_t *root_lock;\n\tstruct tcmsg *t;\n\tu32 protocol;\n\tu32 prio;\n\tu32 nprio;\n\tu32 parent;\n\tstruct net_device *dev;\n\tstruct Qdisc  *q;\n\tstruct tcf_proto **back, **chain;\n\tstruct tcf_proto *tp;\n\tconst struct tcf_proto_ops *tp_ops;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl;\n\tunsigned long fh;\n\tint err;\n\tint tp_created = 0;\n\n\tif ((n->nlmsg_type != RTM_GETTFILTER) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\terr = nlmsg_parse(n, sizeof(*t), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tt = nlmsg_data(n);\n\tprotocol = TC_H_MIN(t->tcm_info);\n\tprio = TC_H_MAJ(t->tcm_info);\n\tnprio = prio;\n\tparent = t->tcm_parent;\n\tcl = 0;\n\n\tif (prio == 0) {\n\t\t/* If no priority is given, user wants we allocated it. */\n\t\tif (n->nlmsg_type != RTM_NEWTFILTER ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\treturn -ENOENT;\n\t\tprio = TC_H_MAKE(0x80000000U, 0U);\n\t}\n\n\t/* Find head of filter chain. */\n\n\t/* Find link */\n\tdev = __dev_get_by_index(net, t->tcm_ifindex);\n\tif (dev == NULL)\n\t\treturn -ENODEV;\n\n\t/* Find qdisc */\n\tif (!parent) {\n\t\tq = dev->qdisc;\n\t\tparent = q->handle;\n\t} else {\n\t\tq = qdisc_lookup(dev, TC_H_MAJ(t->tcm_parent));\n\t\tif (q == NULL)\n\t\t\treturn -EINVAL;\n\t}\n\n\t/* Is it classful? */\n\tcops = q->ops->cl_ops;\n\tif (!cops)\n\t\treturn -EINVAL;\n\n\tif (cops->tcf_chain == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\t/* Do we search for filter, attached to class? */\n\tif (TC_H_MIN(parent)) {\n\t\tcl = cops->get(q, parent);\n\t\tif (cl == 0)\n\t\t\treturn -ENOENT;\n\t}\n\n\t/* And the last stroke */\n\tchain = cops->tcf_chain(q, cl);\n\terr = -EINVAL;\n\tif (chain == NULL)\n\t\tgoto errout;\n\n\t/* Check the chain for existence of proto-tcf with this priority */\n\tfor (back = chain; (tp = *back) != NULL; back = &tp->next) {\n\t\tif (tp->prio >= prio) {\n\t\t\tif (tp->prio == prio) {\n\t\t\t\tif (!nprio ||\n\t\t\t\t    (tp->protocol != protocol && protocol))\n\t\t\t\t\tgoto errout;\n\t\t\t} else\n\t\t\t\ttp = NULL;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\troot_lock = qdisc_root_sleeping_lock(q);\n\n\tif (tp == NULL) {\n\t\t/* Proto-tcf does not exist, create new one */\n\n\t\tif (tca[TCA_KIND] == NULL || !protocol)\n\t\t\tgoto errout;\n\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTFILTER ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto errout;\n\n\n\t\t/* Create new proto tcf */\n\n\t\terr = -ENOBUFS;\n\t\ttp = kzalloc(sizeof(*tp), GFP_KERNEL);\n\t\tif (tp == NULL)\n\t\t\tgoto errout;\n\t\terr = -ENOENT;\n\t\ttp_ops = tcf_proto_lookup_ops(tca[TCA_KIND]);\n\t\tif (tp_ops == NULL) {\n#ifdef CONFIG_MODULES\n\t\t\tstruct nlattr *kind = tca[TCA_KIND];\n\t\t\tchar name[IFNAMSIZ];\n\n\t\t\tif (kind != NULL &&\n\t\t\t    nla_strlcpy(name, kind, IFNAMSIZ) < IFNAMSIZ) {\n\t\t\t\trtnl_unlock();\n\t\t\t\trequest_module(\"cls_%s\", name);\n\t\t\t\trtnl_lock();\n\t\t\t\ttp_ops = tcf_proto_lookup_ops(kind);\n\t\t\t\t/* We dropped the RTNL semaphore in order to\n\t\t\t\t * perform the module load.  So, even if we\n\t\t\t\t * succeeded in loading the module we have to\n\t\t\t\t * replay the request.  We indicate this using\n\t\t\t\t * -EAGAIN.\n\t\t\t\t */\n\t\t\t\tif (tp_ops != NULL) {\n\t\t\t\t\tmodule_put(tp_ops->owner);\n\t\t\t\t\terr = -EAGAIN;\n\t\t\t\t}\n\t\t\t}\n#endif\n\t\t\tkfree(tp);\n\t\t\tgoto errout;\n\t\t}\n\t\ttp->ops = tp_ops;\n\t\ttp->protocol = protocol;\n\t\ttp->prio = nprio ? : TC_H_MAJ(tcf_auto_prio(*back));\n\t\ttp->q = q;\n\t\ttp->classify = tp_ops->classify;\n\t\ttp->classid = parent;\n\n\t\terr = tp_ops->init(tp);\n\t\tif (err != 0) {\n\t\t\tmodule_put(tp_ops->owner);\n\t\t\tkfree(tp);\n\t\t\tgoto errout;\n\t\t}\n\n\t\ttp_created = 1;\n\n\t} else if (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], tp->ops->kind))\n\t\tgoto errout;\n\n\tfh = tp->ops->get(tp, t->tcm_handle);\n\n\tif (fh == 0) {\n\t\tif (n->nlmsg_type == RTM_DELTFILTER && t->tcm_handle == 0) {\n\t\t\tspin_lock_bh(root_lock);\n\t\t\t*back = tp->next;\n\t\t\tspin_unlock_bh(root_lock);\n\n\t\t\ttfilter_notify(net, skb, n, tp, fh, RTM_DELTFILTER);\n\t\t\ttcf_destroy(tp);\n\t\t\terr = 0;\n\t\t\tgoto errout;\n\t\t}\n\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTFILTER ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto errout;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTFILTER:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL) {\n\t\t\t\tif (tp_created)\n\t\t\t\t\ttcf_destroy(tp);\n\t\t\t\tgoto errout;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase RTM_DELTFILTER:\n\t\t\terr = tp->ops->delete(tp, fh);\n\t\t\tif (err == 0)\n\t\t\t\ttfilter_notify(net, skb, n, tp, fh, RTM_DELTFILTER);\n\t\t\tgoto errout;\n\t\tcase RTM_GETTFILTER:\n\t\t\terr = tfilter_notify(net, skb, n, tp, fh, RTM_NEWTFILTER);\n\t\t\tgoto errout;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout;\n\t\t}\n\t}\n\n\terr = tp->ops->change(net, skb, tp, cl, t->tcm_handle, tca, &fh);\n\tif (err == 0) {\n\t\tif (tp_created) {\n\t\t\tspin_lock_bh(root_lock);\n\t\t\ttp->next = *back;\n\t\t\t*back = tp;\n\t\t\tspin_unlock_bh(root_lock);\n\t\t}\n\t\ttfilter_notify(net, skb, n, tp, fh, RTM_NEWTFILTER);\n\t} else {\n\t\tif (tp_created)\n\t\t\ttcf_destroy(tp);\n\t}\n\nerrout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\tif (err == -EAGAIN)\n\t\t/* Replay the request. */\n\t\tgoto replay;\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}",
                        "code_after_change": "static int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int tc_ctl_tfilter(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tspinlock_t *root_lock;\n\tstruct tcmsg *t;\n\tu32 protocol;\n\tu32 prio;\n\tu32 nprio;\n\tu32 parent;\n\tstruct net_device *dev;\n\tstruct Qdisc  *q;\n\tstruct tcf_proto **back, **chain;\n\tstruct tcf_proto *tp;\n\tconst struct tcf_proto_ops *tp_ops;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl;\n\tunsigned long fh;\n\tint err;\n\tint tp_created = 0;\n\n\tif ((n->nlmsg_type != RTM_GETTFILTER) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\terr = nlmsg_parse(n, sizeof(*t), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tt = nlmsg_data(n);\n\tprotocol = TC_H_MIN(t->tcm_info);\n\tprio = TC_H_MAJ(t->tcm_info);\n\tnprio = prio;\n\tparent = t->tcm_parent;\n\tcl = 0;\n\n\tif (prio == 0) {\n\t\t/* If no priority is given, user wants we allocated it. */\n\t\tif (n->nlmsg_type != RTM_NEWTFILTER ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\treturn -ENOENT;\n\t\tprio = TC_H_MAKE(0x80000000U, 0U);\n\t}\n\n\t/* Find head of filter chain. */\n\n\t/* Find link */\n\tdev = __dev_get_by_index(net, t->tcm_ifindex);\n\tif (dev == NULL)\n\t\treturn -ENODEV;\n\n\t/* Find qdisc */\n\tif (!parent) {\n\t\tq = dev->qdisc;\n\t\tparent = q->handle;\n\t} else {\n\t\tq = qdisc_lookup(dev, TC_H_MAJ(t->tcm_parent));\n\t\tif (q == NULL)\n\t\t\treturn -EINVAL;\n\t}\n\n\t/* Is it classful? */\n\tcops = q->ops->cl_ops;\n\tif (!cops)\n\t\treturn -EINVAL;\n\n\tif (cops->tcf_chain == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\t/* Do we search for filter, attached to class? */\n\tif (TC_H_MIN(parent)) {\n\t\tcl = cops->get(q, parent);\n\t\tif (cl == 0)\n\t\t\treturn -ENOENT;\n\t}\n\n\t/* And the last stroke */\n\tchain = cops->tcf_chain(q, cl);\n\terr = -EINVAL;\n\tif (chain == NULL)\n\t\tgoto errout;\n\n\t/* Check the chain for existence of proto-tcf with this priority */\n\tfor (back = chain; (tp = *back) != NULL; back = &tp->next) {\n\t\tif (tp->prio >= prio) {\n\t\t\tif (tp->prio == prio) {\n\t\t\t\tif (!nprio ||\n\t\t\t\t    (tp->protocol != protocol && protocol))\n\t\t\t\t\tgoto errout;\n\t\t\t} else\n\t\t\t\ttp = NULL;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\troot_lock = qdisc_root_sleeping_lock(q);\n\n\tif (tp == NULL) {\n\t\t/* Proto-tcf does not exist, create new one */\n\n\t\tif (tca[TCA_KIND] == NULL || !protocol)\n\t\t\tgoto errout;\n\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTFILTER ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto errout;\n\n\n\t\t/* Create new proto tcf */\n\n\t\terr = -ENOBUFS;\n\t\ttp = kzalloc(sizeof(*tp), GFP_KERNEL);\n\t\tif (tp == NULL)\n\t\t\tgoto errout;\n\t\terr = -ENOENT;\n\t\ttp_ops = tcf_proto_lookup_ops(tca[TCA_KIND]);\n\t\tif (tp_ops == NULL) {\n#ifdef CONFIG_MODULES\n\t\t\tstruct nlattr *kind = tca[TCA_KIND];\n\t\t\tchar name[IFNAMSIZ];\n\n\t\t\tif (kind != NULL &&\n\t\t\t    nla_strlcpy(name, kind, IFNAMSIZ) < IFNAMSIZ) {\n\t\t\t\trtnl_unlock();\n\t\t\t\trequest_module(\"cls_%s\", name);\n\t\t\t\trtnl_lock();\n\t\t\t\ttp_ops = tcf_proto_lookup_ops(kind);\n\t\t\t\t/* We dropped the RTNL semaphore in order to\n\t\t\t\t * perform the module load.  So, even if we\n\t\t\t\t * succeeded in loading the module we have to\n\t\t\t\t * replay the request.  We indicate this using\n\t\t\t\t * -EAGAIN.\n\t\t\t\t */\n\t\t\t\tif (tp_ops != NULL) {\n\t\t\t\t\tmodule_put(tp_ops->owner);\n\t\t\t\t\terr = -EAGAIN;\n\t\t\t\t}\n\t\t\t}\n#endif\n\t\t\tkfree(tp);\n\t\t\tgoto errout;\n\t\t}\n\t\ttp->ops = tp_ops;\n\t\ttp->protocol = protocol;\n\t\ttp->prio = nprio ? : TC_H_MAJ(tcf_auto_prio(*back));\n\t\ttp->q = q;\n\t\ttp->classify = tp_ops->classify;\n\t\ttp->classid = parent;\n\n\t\terr = tp_ops->init(tp);\n\t\tif (err != 0) {\n\t\t\tmodule_put(tp_ops->owner);\n\t\t\tkfree(tp);\n\t\t\tgoto errout;\n\t\t}\n\n\t\ttp_created = 1;\n\n\t} else if (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], tp->ops->kind))\n\t\tgoto errout;\n\n\tfh = tp->ops->get(tp, t->tcm_handle);\n\n\tif (fh == 0) {\n\t\tif (n->nlmsg_type == RTM_DELTFILTER && t->tcm_handle == 0) {\n\t\t\tspin_lock_bh(root_lock);\n\t\t\t*back = tp->next;\n\t\t\tspin_unlock_bh(root_lock);\n\n\t\t\ttfilter_notify(net, skb, n, tp, fh, RTM_DELTFILTER);\n\t\t\ttcf_destroy(tp);\n\t\t\terr = 0;\n\t\t\tgoto errout;\n\t\t}\n\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTFILTER ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto errout;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTFILTER:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL) {\n\t\t\t\tif (tp_created)\n\t\t\t\t\ttcf_destroy(tp);\n\t\t\t\tgoto errout;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase RTM_DELTFILTER:\n\t\t\terr = tp->ops->delete(tp, fh);\n\t\t\tif (err == 0)\n\t\t\t\ttfilter_notify(net, skb, n, tp, fh, RTM_DELTFILTER);\n\t\t\tgoto errout;\n\t\tcase RTM_GETTFILTER:\n\t\t\terr = tfilter_notify(net, skb, n, tp, fh, RTM_NEWTFILTER);\n\t\t\tgoto errout;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout;\n\t\t}\n\t}\n\n\terr = tp->ops->change(net, skb, tp, cl, t->tcm_handle, tca, &fh);\n\tif (err == 0) {\n\t\tif (tp_created) {\n\t\t\tspin_lock_bh(root_lock);\n\t\t\ttp->next = *back;\n\t\t\t*back = tp;\n\t\t\tspin_unlock_bh(root_lock);\n\t\t}\n\t\ttfilter_notify(net, skb, n, tp, fh, RTM_NEWTFILTER);\n\t} else {\n\t\tif (tp_created)\n\t\t\ttcf_destroy(tp);\n\t}\n\nerrout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\tif (err == -EAGAIN)\n\t\t/* Replay the request. */\n\t\tgoto replay;\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int tc_ctl_tclass(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tstruct Qdisc *q = NULL;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl = 0;\n\tunsigned long new_cl;\n\tu32 portid;\n\tu32 clid;\n\tu32 qid;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETTCLASS) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\t/*\n\t   parent == TC_H_UNSPEC - unspecified parent.\n\t   parent == TC_H_ROOT   - class is root, which has no parent.\n\t   parent == X:0\t - parent is root class.\n\t   parent == X:Y\t - parent is a node in hierarchy.\n\t   parent == 0:Y\t - parent is X:Y, where X:0 is qdisc.\n\n\t   handle == 0:0\t - generate handle from kernel pool.\n\t   handle == 0:Y\t - class is X:Y, where X:0 is qdisc.\n\t   handle == X:Y\t - clear.\n\t   handle == X:0\t - root class.\n\t */\n\n\t/* Step 1. Determine qdisc handle X:0 */\n\n\tportid = tcm->tcm_parent;\n\tclid = tcm->tcm_handle;\n\tqid = TC_H_MAJ(clid);\n\n\tif (portid != TC_H_ROOT) {\n\t\tu32 qid1 = TC_H_MAJ(portid);\n\n\t\tif (qid && qid1) {\n\t\t\t/* If both majors are known, they must be identical. */\n\t\t\tif (qid != qid1)\n\t\t\t\treturn -EINVAL;\n\t\t} else if (qid1) {\n\t\t\tqid = qid1;\n\t\t} else if (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\n\t\t/* Now qid is genuine qdisc handle consistent\n\t\t * both with parent and child.\n\t\t *\n\t\t * TC_H_MAJ(portid) still may be unspecified, complete it now.\n\t\t */\n\t\tif (portid)\n\t\t\tportid = TC_H_MAKE(qid, portid);\n\t} else {\n\t\tif (qid == 0)\n\t\t\tqid = dev->qdisc->handle;\n\t}\n\n\t/* OK. Locate qdisc */\n\tq = qdisc_lookup(dev, qid);\n\tif (!q)\n\t\treturn -ENOENT;\n\n\t/* An check that it supports classes */\n\tcops = q->ops->cl_ops;\n\tif (cops == NULL)\n\t\treturn -EINVAL;\n\n\t/* Now try to get class */\n\tif (clid == 0) {\n\t\tif (portid == TC_H_ROOT)\n\t\t\tclid = qid;\n\t} else\n\t\tclid = TC_H_MAKE(qid, clid);\n\n\tif (clid)\n\t\tcl = cops->get(q, clid);\n\n\tif (cl == 0) {\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTCLASS ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto out;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTCLASS:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase RTM_DELTCLASS:\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tif (cops->delete)\n\t\t\t\terr = cops->delete(q, cl);\n\t\t\tif (err == 0)\n\t\t\t\ttclass_notify(net, skb, n, q, cl, RTM_DELTCLASS);\n\t\t\tgoto out;\n\t\tcase RTM_GETTCLASS:\n\t\t\terr = tclass_notify(net, skb, n, q, cl, RTM_NEWTCLASS);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnew_cl = cl;\n\terr = -EOPNOTSUPP;\n\tif (cops->change)\n\t\terr = cops->change(q, clid, portid, tca, &new_cl);\n\tif (err == 0)\n\t\ttclass_notify(net, skb, n, q, new_cl, RTM_NEWTCLASS);\n\nout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}",
                        "code_after_change": "static int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int tc_ctl_tfilter(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tspinlock_t *root_lock;\n\tstruct tcmsg *t;\n\tu32 protocol;\n\tu32 prio;\n\tu32 nprio;\n\tu32 parent;\n\tstruct net_device *dev;\n\tstruct Qdisc  *q;\n\tstruct tcf_proto **back, **chain;\n\tstruct tcf_proto *tp;\n\tconst struct tcf_proto_ops *tp_ops;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl;\n\tunsigned long fh;\n\tint err;\n\tint tp_created = 0;\n\n\tif ((n->nlmsg_type != RTM_GETTFILTER) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\terr = nlmsg_parse(n, sizeof(*t), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tt = nlmsg_data(n);\n\tprotocol = TC_H_MIN(t->tcm_info);\n\tprio = TC_H_MAJ(t->tcm_info);\n\tnprio = prio;\n\tparent = t->tcm_parent;\n\tcl = 0;\n\n\tif (prio == 0) {\n\t\t/* If no priority is given, user wants we allocated it. */\n\t\tif (n->nlmsg_type != RTM_NEWTFILTER ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\treturn -ENOENT;\n\t\tprio = TC_H_MAKE(0x80000000U, 0U);\n\t}\n\n\t/* Find head of filter chain. */\n\n\t/* Find link */\n\tdev = __dev_get_by_index(net, t->tcm_ifindex);\n\tif (dev == NULL)\n\t\treturn -ENODEV;\n\n\t/* Find qdisc */\n\tif (!parent) {\n\t\tq = dev->qdisc;\n\t\tparent = q->handle;\n\t} else {\n\t\tq = qdisc_lookup(dev, TC_H_MAJ(t->tcm_parent));\n\t\tif (q == NULL)\n\t\t\treturn -EINVAL;\n\t}\n\n\t/* Is it classful? */\n\tcops = q->ops->cl_ops;\n\tif (!cops)\n\t\treturn -EINVAL;\n\n\tif (cops->tcf_chain == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\t/* Do we search for filter, attached to class? */\n\tif (TC_H_MIN(parent)) {\n\t\tcl = cops->get(q, parent);\n\t\tif (cl == 0)\n\t\t\treturn -ENOENT;\n\t}\n\n\t/* And the last stroke */\n\tchain = cops->tcf_chain(q, cl);\n\terr = -EINVAL;\n\tif (chain == NULL)\n\t\tgoto errout;\n\n\t/* Check the chain for existence of proto-tcf with this priority */\n\tfor (back = chain; (tp = *back) != NULL; back = &tp->next) {\n\t\tif (tp->prio >= prio) {\n\t\t\tif (tp->prio == prio) {\n\t\t\t\tif (!nprio ||\n\t\t\t\t    (tp->protocol != protocol && protocol))\n\t\t\t\t\tgoto errout;\n\t\t\t} else\n\t\t\t\ttp = NULL;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\troot_lock = qdisc_root_sleeping_lock(q);\n\n\tif (tp == NULL) {\n\t\t/* Proto-tcf does not exist, create new one */\n\n\t\tif (tca[TCA_KIND] == NULL || !protocol)\n\t\t\tgoto errout;\n\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTFILTER ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto errout;\n\n\n\t\t/* Create new proto tcf */\n\n\t\terr = -ENOBUFS;\n\t\ttp = kzalloc(sizeof(*tp), GFP_KERNEL);\n\t\tif (tp == NULL)\n\t\t\tgoto errout;\n\t\terr = -ENOENT;\n\t\ttp_ops = tcf_proto_lookup_ops(tca[TCA_KIND]);\n\t\tif (tp_ops == NULL) {\n#ifdef CONFIG_MODULES\n\t\t\tstruct nlattr *kind = tca[TCA_KIND];\n\t\t\tchar name[IFNAMSIZ];\n\n\t\t\tif (kind != NULL &&\n\t\t\t    nla_strlcpy(name, kind, IFNAMSIZ) < IFNAMSIZ) {\n\t\t\t\trtnl_unlock();\n\t\t\t\trequest_module(\"cls_%s\", name);\n\t\t\t\trtnl_lock();\n\t\t\t\ttp_ops = tcf_proto_lookup_ops(kind);\n\t\t\t\t/* We dropped the RTNL semaphore in order to\n\t\t\t\t * perform the module load.  So, even if we\n\t\t\t\t * succeeded in loading the module we have to\n\t\t\t\t * replay the request.  We indicate this using\n\t\t\t\t * -EAGAIN.\n\t\t\t\t */\n\t\t\t\tif (tp_ops != NULL) {\n\t\t\t\t\tmodule_put(tp_ops->owner);\n\t\t\t\t\terr = -EAGAIN;\n\t\t\t\t}\n\t\t\t}\n#endif\n\t\t\tkfree(tp);\n\t\t\tgoto errout;\n\t\t}\n\t\ttp->ops = tp_ops;\n\t\ttp->protocol = protocol;\n\t\ttp->prio = nprio ? : TC_H_MAJ(tcf_auto_prio(*back));\n\t\ttp->q = q;\n\t\ttp->classify = tp_ops->classify;\n\t\ttp->classid = parent;\n\n\t\terr = tp_ops->init(tp);\n\t\tif (err != 0) {\n\t\t\tmodule_put(tp_ops->owner);\n\t\t\tkfree(tp);\n\t\t\tgoto errout;\n\t\t}\n\n\t\ttp_created = 1;\n\n\t} else if (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], tp->ops->kind))\n\t\tgoto errout;\n\n\tfh = tp->ops->get(tp, t->tcm_handle);\n\n\tif (fh == 0) {\n\t\tif (n->nlmsg_type == RTM_DELTFILTER && t->tcm_handle == 0) {\n\t\t\tspin_lock_bh(root_lock);\n\t\t\t*back = tp->next;\n\t\t\tspin_unlock_bh(root_lock);\n\n\t\t\ttfilter_notify(net, skb, n, tp, fh, RTM_DELTFILTER);\n\t\t\ttcf_destroy(tp);\n\t\t\terr = 0;\n\t\t\tgoto errout;\n\t\t}\n\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTFILTER ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto errout;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTFILTER:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL) {\n\t\t\t\tif (tp_created)\n\t\t\t\t\ttcf_destroy(tp);\n\t\t\t\tgoto errout;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase RTM_DELTFILTER:\n\t\t\terr = tp->ops->delete(tp, fh);\n\t\t\tif (err == 0)\n\t\t\t\ttfilter_notify(net, skb, n, tp, fh, RTM_DELTFILTER);\n\t\t\tgoto errout;\n\t\tcase RTM_GETTFILTER:\n\t\t\terr = tfilter_notify(net, skb, n, tp, fh, RTM_NEWTFILTER);\n\t\t\tgoto errout;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout;\n\t\t}\n\t}\n\n\terr = tp->ops->change(net, skb, tp, cl, t->tcm_handle, tca, &fh);\n\tif (err == 0) {\n\t\tif (tp_created) {\n\t\t\tspin_lock_bh(root_lock);\n\t\t\ttp->next = *back;\n\t\t\t*back = tp;\n\t\t\tspin_unlock_bh(root_lock);\n\t\t}\n\t\ttfilter_notify(net, skb, n, tp, fh, RTM_NEWTFILTER);\n\t} else {\n\t\tif (tp_created)\n\t\t\ttcf_destroy(tp);\n\t}\n\nerrout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\tif (err == -EAGAIN)\n\t\t/* Replay the request. */\n\t\tgoto replay;\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct tcmsg *tcm = nlmsg_data(n);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tstruct net_device *dev;\n\tu32 clid;\n\tstruct Qdisc *q = NULL;\n\tstruct Qdisc *p = NULL;\n\tint err;\n\n\tif ((n->nlmsg_type != RTM_GETQDISC) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\terr = nlmsg_parse(n, sizeof(*tcm), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tdev = __dev_get_by_index(net, tcm->tcm_ifindex);\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tclid = tcm->tcm_parent;\n\tif (clid) {\n\t\tif (clid != TC_H_ROOT) {\n\t\t\tif (TC_H_MAJ(clid) != TC_H_MAJ(TC_H_INGRESS)) {\n\t\t\t\tp = qdisc_lookup(dev, TC_H_MAJ(clid));\n\t\t\t\tif (!p)\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tq = qdisc_leaf(p, clid);\n\t\t\t} else if (dev_ingress_queue(dev)) {\n\t\t\t\tq = dev_ingress_queue(dev)->qdisc_sleeping;\n\t\t\t}\n\t\t} else {\n\t\t\tq = dev->qdisc;\n\t\t}\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\n\t\tif (tcm->tcm_handle && q->handle != tcm->tcm_handle)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tq = qdisc_lookup(dev, tcm->tcm_handle);\n\t\tif (!q)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], q->ops->id))\n\t\treturn -EINVAL;\n\n\tif (n->nlmsg_type == RTM_DELQDISC) {\n\t\tif (!clid)\n\t\t\treturn -EINVAL;\n\t\tif (q->handle == 0)\n\t\t\treturn -ENOENT;\n\t\terr = qdisc_graft(dev, p, skb, n, clid, NULL, q);\n\t\tif (err != 0)\n\t\t\treturn err;\n\t} else {\n\t\tqdisc_notify(net, skb, n, clid, NULL, q);\n\t}\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t  int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\n\t/*\n\t *\tconnect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 0x1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type&IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp &&\n\t    !ipv6_addr_equal(&sk->sk_v6_daddr, &usin->sin6_addr)) {\n\t\ttp->rx_opt.ts_recent = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq = 0;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t *\tTCP over IPv4\n\t */\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &ipv6_mapped;\n\t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\ttp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\terr = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &ipv6_specific;\n\t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (!saddr) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp &&\n\t    ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr))\n\t\ttcp_fetch_timewait_stamp(sk, dst);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (np->opt)\n\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n\t\t\t\t\t  np->opt->opt_nflen);\n\n\ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet6_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tsk_set_txhash(sk);\n\n\tif (!tp->write_seq && likely(!tp->repair))\n\t\ttp->write_seq = secure_tcpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t\t     sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t\t     inet->inet_sport,\n\t\t\t\t\t\t\t     inet->inet_dport);\n\n\terr = tcp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\ttcp_set_state(sk, TCP_CLOSE);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
                        "code_after_change": "static int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t  int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct ipv6_txoptions *opt;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\n\t/*\n\t *\tconnect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 0x1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type&IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp &&\n\t    !ipv6_addr_equal(&sk->sk_v6_daddr, &usin->sin6_addr)) {\n\t\ttp->rx_opt.ts_recent = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq = 0;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t *\tTCP over IPv4\n\t */\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &ipv6_mapped;\n\t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\ttp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\terr = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &ipv6_specific;\n\t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (!saddr) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp &&\n\t    ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr))\n\t\ttcp_fetch_timewait_stamp(sk, dst);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len = opt->opt_flen +\n\t\t\t\t\t opt->opt_nflen;\n\n\ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet6_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tsk_set_txhash(sk);\n\n\tif (!tp->write_seq && likely(!tp->repair))\n\t\ttp->write_seq = secure_tcpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t\t     sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t\t     inet->inet_sport,\n\t\t\t\t\t\t\t     inet->inet_dport);\n\n\terr = tcp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\ttcp_set_state(sk, TCP_CLOSE);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}",
                        "cve_id": "CVE-2016-3841"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int tc_ctl_tfilter(struct sk_buff *skb, struct nlmsghdr *n)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tca[TCA_MAX + 1];\n\tspinlock_t *root_lock;\n\tstruct tcmsg *t;\n\tu32 protocol;\n\tu32 prio;\n\tu32 nprio;\n\tu32 parent;\n\tstruct net_device *dev;\n\tstruct Qdisc  *q;\n\tstruct tcf_proto **back, **chain;\n\tstruct tcf_proto *tp;\n\tconst struct tcf_proto_ops *tp_ops;\n\tconst struct Qdisc_class_ops *cops;\n\tunsigned long cl;\n\tunsigned long fh;\n\tint err;\n\tint tp_created = 0;\n\n\tif ((n->nlmsg_type != RTM_GETTFILTER) && !netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\nreplay:\n\terr = nlmsg_parse(n, sizeof(*t), tca, TCA_MAX, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tt = nlmsg_data(n);\n\tprotocol = TC_H_MIN(t->tcm_info);\n\tprio = TC_H_MAJ(t->tcm_info);\n\tnprio = prio;\n\tparent = t->tcm_parent;\n\tcl = 0;\n\n\tif (prio == 0) {\n\t\t/* If no priority is given, user wants we allocated it. */\n\t\tif (n->nlmsg_type != RTM_NEWTFILTER ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\treturn -ENOENT;\n\t\tprio = TC_H_MAKE(0x80000000U, 0U);\n\t}\n\n\t/* Find head of filter chain. */\n\n\t/* Find link */\n\tdev = __dev_get_by_index(net, t->tcm_ifindex);\n\tif (dev == NULL)\n\t\treturn -ENODEV;\n\n\t/* Find qdisc */\n\tif (!parent) {\n\t\tq = dev->qdisc;\n\t\tparent = q->handle;\n\t} else {\n\t\tq = qdisc_lookup(dev, TC_H_MAJ(t->tcm_parent));\n\t\tif (q == NULL)\n\t\t\treturn -EINVAL;\n\t}\n\n\t/* Is it classful? */\n\tcops = q->ops->cl_ops;\n\tif (!cops)\n\t\treturn -EINVAL;\n\n\tif (cops->tcf_chain == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\t/* Do we search for filter, attached to class? */\n\tif (TC_H_MIN(parent)) {\n\t\tcl = cops->get(q, parent);\n\t\tif (cl == 0)\n\t\t\treturn -ENOENT;\n\t}\n\n\t/* And the last stroke */\n\tchain = cops->tcf_chain(q, cl);\n\terr = -EINVAL;\n\tif (chain == NULL)\n\t\tgoto errout;\n\n\t/* Check the chain for existence of proto-tcf with this priority */\n\tfor (back = chain; (tp = *back) != NULL; back = &tp->next) {\n\t\tif (tp->prio >= prio) {\n\t\t\tif (tp->prio == prio) {\n\t\t\t\tif (!nprio ||\n\t\t\t\t    (tp->protocol != protocol && protocol))\n\t\t\t\t\tgoto errout;\n\t\t\t} else\n\t\t\t\ttp = NULL;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\troot_lock = qdisc_root_sleeping_lock(q);\n\n\tif (tp == NULL) {\n\t\t/* Proto-tcf does not exist, create new one */\n\n\t\tif (tca[TCA_KIND] == NULL || !protocol)\n\t\t\tgoto errout;\n\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTFILTER ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto errout;\n\n\n\t\t/* Create new proto tcf */\n\n\t\terr = -ENOBUFS;\n\t\ttp = kzalloc(sizeof(*tp), GFP_KERNEL);\n\t\tif (tp == NULL)\n\t\t\tgoto errout;\n\t\terr = -ENOENT;\n\t\ttp_ops = tcf_proto_lookup_ops(tca[TCA_KIND]);\n\t\tif (tp_ops == NULL) {\n#ifdef CONFIG_MODULES\n\t\t\tstruct nlattr *kind = tca[TCA_KIND];\n\t\t\tchar name[IFNAMSIZ];\n\n\t\t\tif (kind != NULL &&\n\t\t\t    nla_strlcpy(name, kind, IFNAMSIZ) < IFNAMSIZ) {\n\t\t\t\trtnl_unlock();\n\t\t\t\trequest_module(\"cls_%s\", name);\n\t\t\t\trtnl_lock();\n\t\t\t\ttp_ops = tcf_proto_lookup_ops(kind);\n\t\t\t\t/* We dropped the RTNL semaphore in order to\n\t\t\t\t * perform the module load.  So, even if we\n\t\t\t\t * succeeded in loading the module we have to\n\t\t\t\t * replay the request.  We indicate this using\n\t\t\t\t * -EAGAIN.\n\t\t\t\t */\n\t\t\t\tif (tp_ops != NULL) {\n\t\t\t\t\tmodule_put(tp_ops->owner);\n\t\t\t\t\terr = -EAGAIN;\n\t\t\t\t}\n\t\t\t}\n#endif\n\t\t\tkfree(tp);\n\t\t\tgoto errout;\n\t\t}\n\t\ttp->ops = tp_ops;\n\t\ttp->protocol = protocol;\n\t\ttp->prio = nprio ? : TC_H_MAJ(tcf_auto_prio(*back));\n\t\ttp->q = q;\n\t\ttp->classify = tp_ops->classify;\n\t\ttp->classid = parent;\n\n\t\terr = tp_ops->init(tp);\n\t\tif (err != 0) {\n\t\t\tmodule_put(tp_ops->owner);\n\t\t\tkfree(tp);\n\t\t\tgoto errout;\n\t\t}\n\n\t\ttp_created = 1;\n\n\t} else if (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], tp->ops->kind))\n\t\tgoto errout;\n\n\tfh = tp->ops->get(tp, t->tcm_handle);\n\n\tif (fh == 0) {\n\t\tif (n->nlmsg_type == RTM_DELTFILTER && t->tcm_handle == 0) {\n\t\t\tspin_lock_bh(root_lock);\n\t\t\t*back = tp->next;\n\t\t\tspin_unlock_bh(root_lock);\n\n\t\t\ttfilter_notify(net, skb, n, tp, fh, RTM_DELTFILTER);\n\t\t\ttcf_destroy(tp);\n\t\t\terr = 0;\n\t\t\tgoto errout;\n\t\t}\n\n\t\terr = -ENOENT;\n\t\tif (n->nlmsg_type != RTM_NEWTFILTER ||\n\t\t    !(n->nlmsg_flags & NLM_F_CREATE))\n\t\t\tgoto errout;\n\t} else {\n\t\tswitch (n->nlmsg_type) {\n\t\tcase RTM_NEWTFILTER:\n\t\t\terr = -EEXIST;\n\t\t\tif (n->nlmsg_flags & NLM_F_EXCL) {\n\t\t\t\tif (tp_created)\n\t\t\t\t\ttcf_destroy(tp);\n\t\t\t\tgoto errout;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase RTM_DELTFILTER:\n\t\t\terr = tp->ops->delete(tp, fh);\n\t\t\tif (err == 0)\n\t\t\t\ttfilter_notify(net, skb, n, tp, fh, RTM_DELTFILTER);\n\t\t\tgoto errout;\n\t\tcase RTM_GETTFILTER:\n\t\t\terr = tfilter_notify(net, skb, n, tp, fh, RTM_NEWTFILTER);\n\t\t\tgoto errout;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto errout;\n\t\t}\n\t}\n\n\terr = tp->ops->change(net, skb, tp, cl, t->tcm_handle, tca, &fh);\n\tif (err == 0) {\n\t\tif (tp_created) {\n\t\t\tspin_lock_bh(root_lock);\n\t\t\ttp->next = *back;\n\t\t\t*back = tp;\n\t\t\tspin_unlock_bh(root_lock);\n\t\t}\n\t\ttfilter_notify(net, skb, n, tp, fh, RTM_NEWTFILTER);\n\t} else {\n\t\tif (tp_created)\n\t\t\ttcf_destroy(tp);\n\t}\n\nerrout:\n\tif (cl)\n\t\tcops->put(q, cl);\n\tif (err == -EAGAIN)\n\t\t/* Replay the request. */\n\t\tgoto replay;\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t  int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\n\t/*\n\t *\tconnect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 0x1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type&IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp &&\n\t    !ipv6_addr_equal(&sk->sk_v6_daddr, &usin->sin6_addr)) {\n\t\ttp->rx_opt.ts_recent = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq = 0;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t *\tTCP over IPv4\n\t */\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &ipv6_mapped;\n\t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\ttp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\terr = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &ipv6_specific;\n\t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (!saddr) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp &&\n\t    ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr))\n\t\ttcp_fetch_timewait_stamp(sk, dst);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (np->opt)\n\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n\t\t\t\t\t  np->opt->opt_nflen);\n\n\ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet6_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tsk_set_txhash(sk);\n\n\tif (!tp->write_seq && likely(!tp->repair))\n\t\ttp->write_seq = secure_tcpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t\t     sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t\t     inet->inet_sport,\n\t\t\t\t\t\t\t     inet->inet_dport);\n\n\terr = tcp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\ttcp_set_state(sk, TCP_CLOSE);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t  int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct ipv6_txoptions *opt;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\n\t/*\n\t *\tconnect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 0x1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type&IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp &&\n\t    !ipv6_addr_equal(&sk->sk_v6_daddr, &usin->sin6_addr)) {\n\t\ttp->rx_opt.ts_recent = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq = 0;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t *\tTCP over IPv4\n\t */\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &ipv6_mapped;\n\t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\ttp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\terr = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &ipv6_specific;\n\t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (!saddr) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp &&\n\t    ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr))\n\t\ttcp_fetch_timewait_stamp(sk, dst);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len = opt->opt_flen +\n\t\t\t\t\t opt->opt_nflen;\n\n\ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet6_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tsk_set_txhash(sk);\n\n\tif (!tp->write_seq && likely(!tp->repair))\n\t\ttp->write_seq = secure_tcpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t\t     sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t\t     inet->inet_sport,\n\t\t\t\t\t\t\t     inet->inet_dport);\n\n\terr = tcp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\ttcp_set_state(sk, TCP_CLOSE);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 438,
            "cve_id": "CVE-2014-0181",
            "code_snippet": "static int cgw_remove_job(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct cgw_job *gwj = NULL;\n\tstruct hlist_node *nx;\n\tstruct rtcanmsg *r;\n\tstruct cf_mod mod;\n\tstruct can_can_gw ccgw;\n\tu8 limhops = 0;\n\tint err = 0;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (nlmsg_len(nlh) < sizeof(*r))\n\t\treturn -EINVAL;\n\n\tr = nlmsg_data(nlh);\n\tif (r->can_family != AF_CAN)\n\t\treturn -EPFNOSUPPORT;\n\n\t/* so far we only support CAN -> CAN routings */\n\tif (r->gwtype != CGW_TYPE_CAN_CAN)\n\t\treturn -EINVAL;\n\n\terr = cgw_parse_attr(nlh, &mod, CGW_TYPE_CAN_CAN, &ccgw, &limhops);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* two interface indices both set to 0 => remove all entries */\n\tif (!ccgw.src_idx && !ccgw.dst_idx) {\n\t\tcgw_remove_all_jobs();\n\t\treturn 0;\n\t}\n\n\terr = -EINVAL;\n\n\tASSERT_RTNL();\n\n\t/* remove only the first matching entry */\n\thlist_for_each_entry_safe(gwj, nx, &cgw_list, list) {\n\n\t\tif (gwj->flags != r->flags)\n\t\t\tcontinue;\n\n\t\tif (gwj->limit_hops != limhops)\n\t\t\tcontinue;\n\n\t\tif (memcmp(&gwj->mod, &mod, sizeof(mod)))\n\t\t\tcontinue;\n\n\t\t/* if (r->gwtype == CGW_TYPE_CAN_CAN) - is made sure here */\n\t\tif (memcmp(&gwj->ccgw, &ccgw, sizeof(ccgw)))\n\t\t\tcontinue;\n\n\t\thlist_del(&gwj->list);\n\t\tcgw_unregister_filter(gwj);\n\t\tkmem_cache_free(cgw_cache, gwj);\n\t\terr = 0;\n\t\tbreak;\n\t}\n\n\treturn err;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
                        "code_after_change": "static int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int cgw_remove_job(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct cgw_job *gwj = NULL;\n\tstruct hlist_node *nx;\n\tstruct rtcanmsg *r;\n\tstruct cf_mod mod;\n\tstruct can_can_gw ccgw;\n\tu8 limhops = 0;\n\tint err = 0;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (nlmsg_len(nlh) < sizeof(*r))\n\t\treturn -EINVAL;\n\n\tr = nlmsg_data(nlh);\n\tif (r->can_family != AF_CAN)\n\t\treturn -EPFNOSUPPORT;\n\n\t/* so far we only support CAN -> CAN routings */\n\tif (r->gwtype != CGW_TYPE_CAN_CAN)\n\t\treturn -EINVAL;\n\n\terr = cgw_parse_attr(nlh, &mod, CGW_TYPE_CAN_CAN, &ccgw, &limhops);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* two interface indices both set to 0 => remove all entries */\n\tif (!ccgw.src_idx && !ccgw.dst_idx) {\n\t\tcgw_remove_all_jobs();\n\t\treturn 0;\n\t}\n\n\terr = -EINVAL;\n\n\tASSERT_RTNL();\n\n\t/* remove only the first matching entry */\n\thlist_for_each_entry_safe(gwj, nx, &cgw_list, list) {\n\n\t\tif (gwj->flags != r->flags)\n\t\t\tcontinue;\n\n\t\tif (gwj->limit_hops != limhops)\n\t\t\tcontinue;\n\n\t\tif (memcmp(&gwj->mod, &mod, sizeof(mod)))\n\t\t\tcontinue;\n\n\t\t/* if (r->gwtype == CGW_TYPE_CAN_CAN) - is made sure here */\n\t\tif (memcmp(&gwj->ccgw, &ccgw, sizeof(ccgw)))\n\t\t\tcontinue;\n\n\t\thlist_del(&gwj->list);\n\t\tcgw_unregister_filter(gwj);\n\t\tkmem_cache_free(cgw_cache, gwj);\n\t\terr = 0;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_fib_rtm_newroute(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct dn_fib_table *tb;\n\tstruct rtmsg *r = nlmsg_data(nlh);\n\tstruct nlattr *attrs[RTA_MAX+1];\n\tint err;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\treturn -EINVAL;\n\n\terr = nlmsg_parse(nlh, sizeof(*r), attrs, RTA_MAX, rtm_dn_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\ttb = dn_fib_get_table(rtm_get_table(attrs, r->rtm_table), 1);\n\tif (!tb)\n\t\treturn -ENOBUFS;\n\n\treturn tb->insert(tb, r, attrs, nlh, &NETLINK_CB(skb));\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}",
                        "code_after_change": "static int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int cgw_remove_job(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct cgw_job *gwj = NULL;\n\tstruct hlist_node *nx;\n\tstruct rtcanmsg *r;\n\tstruct cf_mod mod;\n\tstruct can_can_gw ccgw;\n\tu8 limhops = 0;\n\tint err = 0;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (nlmsg_len(nlh) < sizeof(*r))\n\t\treturn -EINVAL;\n\n\tr = nlmsg_data(nlh);\n\tif (r->can_family != AF_CAN)\n\t\treturn -EPFNOSUPPORT;\n\n\t/* so far we only support CAN -> CAN routings */\n\tif (r->gwtype != CGW_TYPE_CAN_CAN)\n\t\treturn -EINVAL;\n\n\terr = cgw_parse_attr(nlh, &mod, CGW_TYPE_CAN_CAN, &ccgw, &limhops);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* two interface indices both set to 0 => remove all entries */\n\tif (!ccgw.src_idx && !ccgw.dst_idx) {\n\t\tcgw_remove_all_jobs();\n\t\treturn 0;\n\t}\n\n\terr = -EINVAL;\n\n\tASSERT_RTNL();\n\n\t/* remove only the first matching entry */\n\thlist_for_each_entry_safe(gwj, nx, &cgw_list, list) {\n\n\t\tif (gwj->flags != r->flags)\n\t\t\tcontinue;\n\n\t\tif (gwj->limit_hops != limhops)\n\t\t\tcontinue;\n\n\t\tif (memcmp(&gwj->mod, &mod, sizeof(mod)))\n\t\t\tcontinue;\n\n\t\t/* if (r->gwtype == CGW_TYPE_CAN_CAN) - is made sure here */\n\t\tif (memcmp(&gwj->ccgw, &ccgw, sizeof(ccgw)))\n\t\t\tcontinue;\n\n\t\thlist_del(&gwj->list);\n\t\tcgw_unregister_filter(gwj);\n\t\tkmem_cache_free(cgw_cache, gwj);\n\t\terr = 0;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int dn_nl_deladdr(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[IFA_MAX+1];\n\tstruct dn_dev *dn_db;\n\tstruct ifaddrmsg *ifm;\n\tstruct dn_ifaddr *ifa;\n\tstruct dn_ifaddr __rcu **ifap;\n\tint err = -EINVAL;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!net_eq(net, &init_net))\n\t\tgoto errout;\n\n\terr = nlmsg_parse(nlh, sizeof(*ifm), tb, IFA_MAX, dn_ifa_policy);\n\tif (err < 0)\n\t\tgoto errout;\n\n\terr = -ENODEV;\n\tifm = nlmsg_data(nlh);\n\tif ((dn_db = dn_dev_by_index(ifm->ifa_index)) == NULL)\n\t\tgoto errout;\n\n\terr = -EADDRNOTAVAIL;\n\tfor (ifap = &dn_db->ifa_list;\n\t     (ifa = rtnl_dereference(*ifap)) != NULL;\n\t     ifap = &ifa->ifa_next) {\n\t\tif (tb[IFA_LOCAL] &&\n\t\t    nla_memcmp(tb[IFA_LOCAL], &ifa->ifa_local, 2))\n\t\t\tcontinue;\n\n\t\tif (tb[IFA_LABEL] && nla_strcmp(tb[IFA_LABEL], ifa->ifa_label))\n\t\t\tcontinue;\n\n\t\tdn_dev_del_ifa(dn_db, ifap, 1);\n\t\treturn 0;\n\t}\n\nerrout:\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}",
                        "code_after_change": "static void\nscsi_nl_rcv_msg(struct sk_buff *skb)\n{\n\tstruct nlmsghdr *nlh;\n\tstruct scsi_nl_hdr *hdr;\n\tu32 rlen;\n\tint err, tport;\n\n\twhile (skb->len >= NLMSG_HDRLEN) {\n\t\terr = 0;\n\n\t\tnlh = nlmsg_hdr(skb);\n\t\tif ((nlh->nlmsg_len < (sizeof(*nlh) + sizeof(*hdr))) ||\n\t\t    (skb->len < nlh->nlmsg_len)) {\n\t\t\tprintk(KERN_WARNING \"%s: discarding partial skb\\n\",\n\t\t\t\t __func__);\n\t\t\treturn;\n\t\t}\n\n\t\trlen = NLMSG_ALIGN(nlh->nlmsg_len);\n\t\tif (rlen > skb->len)\n\t\t\trlen = skb->len;\n\n\t\tif (nlh->nlmsg_type != SCSI_TRANSPORT_MSG) {\n\t\t\terr = -EBADMSG;\n\t\t\tgoto next_msg;\n\t\t}\n\n\t\thdr = nlmsg_data(nlh);\n\t\tif ((hdr->version != SCSI_NL_VERSION) ||\n\t\t    (hdr->magic != SCSI_NL_MAGIC)) {\n\t\t\terr = -EPROTOTYPE;\n\t\t\tgoto next_msg;\n\t\t}\n\n\t\tif (!netlink_capable(skb, CAP_SYS_ADMIN)) {\n\t\t\terr = -EPERM;\n\t\t\tgoto next_msg;\n\t\t}\n\n\t\tif (nlh->nlmsg_len < (sizeof(*nlh) + hdr->msglen)) {\n\t\t\tprintk(KERN_WARNING \"%s: discarding partial message\\n\",\n\t\t\t\t __func__);\n\t\t\tgoto next_msg;\n\t\t}\n\n\t\t/*\n\t\t * Deliver message to the appropriate transport\n\t\t */\n\t\ttport = hdr->transport;\n\t\tif (tport == SCSI_NL_TRANSPORT) {\n\t\t\tswitch (hdr->msgtype) {\n\t\t\tcase SCSI_NL_SHOST_VENDOR:\n\t\t\t\t/* Locate the driver that corresponds to the message */\n\t\t\t\terr = -ESRCH;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\terr = -EBADR;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (err)\n\t\t\t\tprintk(KERN_WARNING \"%s: Msgtype %d failed - err %d\\n\",\n\t\t\t\t       __func__, hdr->msgtype, err);\n\t\t}\n\t\telse\n\t\t\terr = -ENOENT;\n\nnext_msg:\n\t\tif ((err) || (nlh->nlmsg_flags & NLM_F_ACK))\n\t\t\tnetlink_ack(skb, nlh, err);\n\n\t\tskb_pull(skb, rlen);\n\t}\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int cgw_remove_job(struct sk_buff *skb, struct nlmsghdr *nlh)\n{\n\tstruct cgw_job *gwj = NULL;\n\tstruct hlist_node *nx;\n\tstruct rtcanmsg *r;\n\tstruct cf_mod mod;\n\tstruct can_can_gw ccgw;\n\tu8 limhops = 0;\n\tint err = 0;\n\n\tif (!netlink_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (nlmsg_len(nlh) < sizeof(*r))\n\t\treturn -EINVAL;\n\n\tr = nlmsg_data(nlh);\n\tif (r->can_family != AF_CAN)\n\t\treturn -EPFNOSUPPORT;\n\n\t/* so far we only support CAN -> CAN routings */\n\tif (r->gwtype != CGW_TYPE_CAN_CAN)\n\t\treturn -EINVAL;\n\n\terr = cgw_parse_attr(nlh, &mod, CGW_TYPE_CAN_CAN, &ccgw, &limhops);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* two interface indices both set to 0 => remove all entries */\n\tif (!ccgw.src_idx && !ccgw.dst_idx) {\n\t\tcgw_remove_all_jobs();\n\t\treturn 0;\n\t}\n\n\terr = -EINVAL;\n\n\tASSERT_RTNL();\n\n\t/* remove only the first matching entry */\n\thlist_for_each_entry_safe(gwj, nx, &cgw_list, list) {\n\n\t\tif (gwj->flags != r->flags)\n\t\t\tcontinue;\n\n\t\tif (gwj->limit_hops != limhops)\n\t\t\tcontinue;\n\n\t\tif (memcmp(&gwj->mod, &mod, sizeof(mod)))\n\t\t\tcontinue;\n\n\t\t/* if (r->gwtype == CGW_TYPE_CAN_CAN) - is made sure here */\n\t\tif (memcmp(&gwj->ccgw, &ccgw, sizeof(ccgw)))\n\t\t\tcontinue;\n\n\t\thlist_del(&gwj->list);\n\t\tcgw_unregister_filter(gwj);\n\t\tkmem_cache_free(cgw_cache, gwj);\n\t\terr = 0;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int genl_family_rcv_msg(struct genl_family *family,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct nlmsghdr *nlh)\n{\n\tconst struct genl_ops *ops;\n\tstruct net *net = sock_net(skb->sk);\n\tstruct genl_info info;\n\tstruct genlmsghdr *hdr = nlmsg_data(nlh);\n\tstruct nlattr **attrbuf;\n\tint hdrlen, err;\n\n\t/* this family doesn't exist in this netns */\n\tif (!family->netnsok && !net_eq(net, &init_net))\n\t\treturn -ENOENT;\n\n\thdrlen = GENL_HDRLEN + family->hdrsize;\n\tif (nlh->nlmsg_len < nlmsg_msg_size(hdrlen))\n\t\treturn -EINVAL;\n\n\tops = genl_get_cmd(hdr->cmd, family);\n\tif (ops == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((ops->flags & GENL_ADMIN_PERM) &&\n\t    !capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((nlh->nlmsg_flags & NLM_F_DUMP) == NLM_F_DUMP) {\n\t\tint rc;\n\n\t\tif (ops->dumpit == NULL)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (!family->parallel_ops) {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t/* we have const, but the netlink API doesn't */\n\t\t\t\t.data = (void *)ops,\n\t\t\t\t.dump = genl_lock_dumpit,\n\t\t\t\t.done = genl_lock_done,\n\t\t\t};\n\n\t\t\tgenl_unlock();\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t\tgenl_lock();\n\n\t\t} else {\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.module = family->module,\n\t\t\t\t.dump = ops->dumpit,\n\t\t\t\t.done = ops->done,\n\t\t\t};\n\n\t\t\trc = __netlink_dump_start(net->genl_sock, skb, nlh, &c);\n\t\t}\n\n\t\treturn rc;\n\t}\n\n\tif (ops->doit == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\tif (family->maxattr && family->parallel_ops) {\n\t\tattrbuf = kmalloc((family->maxattr+1) *\n\t\t\t\t\tsizeof(struct nlattr *), GFP_KERNEL);\n\t\tif (attrbuf == NULL)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tattrbuf = family->attrbuf;\n\n\tif (attrbuf) {\n\t\terr = nlmsg_parse(nlh, hdrlen, attrbuf, family->maxattr,\n\t\t\t\t  ops->policy);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tinfo.snd_seq = nlh->nlmsg_seq;\n\tinfo.snd_portid = NETLINK_CB(skb).portid;\n\tinfo.nlhdr = nlh;\n\tinfo.genlhdr = nlmsg_data(nlh);\n\tinfo.userhdr = nlmsg_data(nlh) + GENL_HDRLEN;\n\tinfo.attrs = attrbuf;\n\tinfo.dst_sk = skb->sk;\n\tgenl_info_net_set(&info, net);\n\tmemset(&info.user_ptr, 0, sizeof(info.user_ptr));\n\n\tif (family->pre_doit) {\n\t\terr = family->pre_doit(ops, skb, &info);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ops->doit(skb, &info);\n\n\tif (family->post_doit)\n\t\tfamily->post_doit(ops, skb, &info);\n\nout:\n\tif (family->parallel_ops)\n\t\tkfree(attrbuf);\n\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic void\nscsi_nl_rcv_msg(struct sk_buff *skb)\n{\n\tstruct nlmsghdr *nlh;\n\tstruct scsi_nl_hdr *hdr;\n\tu32 rlen;\n\tint err, tport;\n\n\twhile (skb->len >= NLMSG_HDRLEN) {\n\t\terr = 0;\n\n\t\tnlh = nlmsg_hdr(skb);\n\t\tif ((nlh->nlmsg_len < (sizeof(*nlh) + sizeof(*hdr))) ||\n\t\t    (skb->len < nlh->nlmsg_len)) {\n\t\t\tprintk(KERN_WARNING \"%s: discarding partial skb\\n\",\n\t\t\t\t __func__);\n\t\t\treturn;\n\t\t}\n\n\t\trlen = NLMSG_ALIGN(nlh->nlmsg_len);\n\t\tif (rlen > skb->len)\n\t\t\trlen = skb->len;\n\n\t\tif (nlh->nlmsg_type != SCSI_TRANSPORT_MSG) {\n\t\t\terr = -EBADMSG;\n\t\t\tgoto next_msg;\n\t\t}\n\n\t\thdr = nlmsg_data(nlh);\n\t\tif ((hdr->version != SCSI_NL_VERSION) ||\n\t\t    (hdr->magic != SCSI_NL_MAGIC)) {\n\t\t\terr = -EPROTOTYPE;\n\t\t\tgoto next_msg;\n\t\t}\n\n\t\tif (!netlink_capable(skb, CAP_SYS_ADMIN)) {\n\t\t\terr = -EPERM;\n\t\t\tgoto next_msg;\n\t\t}\n\n\t\tif (nlh->nlmsg_len < (sizeof(*nlh) + hdr->msglen)) {\n\t\t\tprintk(KERN_WARNING \"%s: discarding partial message\\n\",\n\t\t\t\t __func__);\n\t\t\tgoto next_msg;\n\t\t}\n\n\t\t/*\n\t\t * Deliver message to the appropriate transport\n\t\t */\n\t\ttport = hdr->transport;\n\t\tif (tport == SCSI_NL_TRANSPORT) {\n\t\t\tswitch (hdr->msgtype) {\n\t\t\tcase SCSI_NL_SHOST_VENDOR:\n\t\t\t\t/* Locate the driver that corresponds to the message */\n\t\t\t\terr = -ESRCH;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\terr = -EBADR;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (err)\n\t\t\t\tprintk(KERN_WARNING \"%s: Msgtype %d failed - err %d\\n\",\n\t\t\t\t       __func__, hdr->msgtype, err);\n\t\t}\n\t\telse\n\t\t\terr = -ENOENT;\n\nnext_msg:\n\t\tif ((err) || (nlh->nlmsg_flags & NLM_F_ACK))\n\t\t\tnetlink_ack(skb, nlh, err);\n\n\t\tskb_pull(skb, rlen);\n\t}\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 1089,
            "cve_id": "CVE-2016-6786",
            "code_snippet": "static long perf_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct perf_event *event = file->private_data;\n\tstruct perf_event_context *ctx;\n\tlong ret;\n\n\tctx = perf_event_ctx_lock(event);\n\tret = _perf_ioctl(event, cmd, arg);\n\tperf_event_ctx_unlock(event, ctx);\n\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tion_free(client, handle);\n\t\tion_handle_put(handle);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}",
                        "code_after_change": "static ssize_t\nperf_read(struct file *file, char __user *buf, size_t count, loff_t *ppos)\n{\n\tstruct perf_event *event = file->private_data;\n\tstruct perf_event_context *ctx;\n\tint ret;\n\n\tctx = perf_event_ctx_lock(event);\n\tret = perf_read_hw(event, buf, count);\n\tperf_event_ctx_unlock(event, ctx);\n\n\treturn ret;\n}",
                        "cve_id": "CVE-2016-9120"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic long perf_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct perf_event *event = file->private_data;\n\tstruct perf_event_context *ctx;\n\tlong ret;\n\n\tctx = perf_event_ctx_lock(event);\n\tret = _perf_ioctl(event, cmd, arg);\n\tperf_event_ctx_unlock(event, ctx);\n\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tion_free(client, handle);\n\t\tion_handle_put(handle);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic ssize_t\nperf_read(struct file *file, char __user *buf, size_t count, loff_t *ppos)\n{\n\tstruct perf_event *event = file->private_data;\n\tstruct perf_event_context *ctx;\n\tint ret;\n\n\tctx = perf_event_ctx_lock(event);\n\tret = perf_read_hw(event, buf, count);\n\tperf_event_ctx_unlock(event, ctx);\n\n\treturn ret;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "asmlinkage long sys_oabi_fcntl64(unsigned int fd, unsigned int cmd,\n\t\t\t\t unsigned long arg)\n{\n\tstruct oabi_flock64 user;\n\tstruct flock64 kernel;\n\tmm_segment_t fs = USER_DS; /* initialized to kill a warning */\n\tunsigned long local_arg = arg;\n\tint ret;\n\n\tswitch (cmd) {\n\tcase F_OFD_GETLK:\n\tcase F_OFD_SETLK:\n\tcase F_OFD_SETLKW:\n\tcase F_GETLK64:\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\tif (copy_from_user(&user, (struct oabi_flock64 __user *)arg,\n\t\t\t\t   sizeof(user)))\n\t\t\treturn -EFAULT;\n\t\tkernel.l_type\t= user.l_type;\n\t\tkernel.l_whence\t= user.l_whence;\n\t\tkernel.l_start\t= user.l_start;\n\t\tkernel.l_len\t= user.l_len;\n\t\tkernel.l_pid\t= user.l_pid;\n\t\tlocal_arg = (unsigned long)&kernel;\n\t\tfs = get_fs();\n\t\tset_fs(KERNEL_DS);\n\t}\n\n\tret = sys_fcntl64(fd, cmd, local_arg);\n\n\tswitch (cmd) {\n\tcase F_GETLK64:\n\t\tif (!ret) {\n\t\t\tuser.l_type\t= kernel.l_type;\n\t\t\tuser.l_whence\t= kernel.l_whence;\n\t\t\tuser.l_start\t= kernel.l_start;\n\t\t\tuser.l_len\t= kernel.l_len;\n\t\t\tuser.l_pid\t= kernel.l_pid;\n\t\t\tif (copy_to_user((struct oabi_flock64 __user *)arg,\n\t\t\t\t\t &user, sizeof(user)))\n\t\t\t\tret = -EFAULT;\n\t\t}\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\tset_fs(fs);\n\t}\n\n\treturn ret;\n}",
                        "code_after_change": "int perf_event_refresh(struct perf_event *event, int refresh)\n{\n\tstruct perf_event_context *ctx;\n\tint ret;\n\n\tctx = perf_event_ctx_lock(event);\n\tret = _perf_event_refresh(event, refresh);\n\tperf_event_ctx_unlock(event, ctx);\n\n\treturn ret;\n}",
                        "cve_id": "CVE-2015-8966"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic long perf_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct perf_event *event = file->private_data;\n\tstruct perf_event_context *ctx;\n\tlong ret;\n\n\tctx = perf_event_ctx_lock(event);\n\tret = _perf_ioctl(event, cmd, arg);\n\tperf_event_ctx_unlock(event, ctx);\n\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nasmlinkage long sys_oabi_fcntl64(unsigned int fd, unsigned int cmd,\n\t\t\t\t unsigned long arg)\n{\n\tstruct oabi_flock64 user;\n\tstruct flock64 kernel;\n\tmm_segment_t fs = USER_DS; /* initialized to kill a warning */\n\tunsigned long local_arg = arg;\n\tint ret;\n\n\tswitch (cmd) {\n\tcase F_OFD_GETLK:\n\tcase F_OFD_SETLK:\n\tcase F_OFD_SETLKW:\n\tcase F_GETLK64:\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\tif (copy_from_user(&user, (struct oabi_flock64 __user *)arg,\n\t\t\t\t   sizeof(user)))\n\t\t\treturn -EFAULT;\n\t\tkernel.l_type\t= user.l_type;\n\t\tkernel.l_whence\t= user.l_whence;\n\t\tkernel.l_start\t= user.l_start;\n\t\tkernel.l_len\t= user.l_len;\n\t\tkernel.l_pid\t= user.l_pid;\n\t\tlocal_arg = (unsigned long)&kernel;\n\t\tfs = get_fs();\n\t\tset_fs(KERNEL_DS);\n\t}\n\n\tret = sys_fcntl64(fd, cmd, local_arg);\n\n\tswitch (cmd) {\n\tcase F_GETLK64:\n\t\tif (!ret) {\n\t\t\tuser.l_type\t= kernel.l_type;\n\t\t\tuser.l_whence\t= kernel.l_whence;\n\t\t\tuser.l_start\t= kernel.l_start;\n\t\t\tuser.l_len\t= kernel.l_len;\n\t\t\tuser.l_pid\t= kernel.l_pid;\n\t\t\tif (copy_to_user((struct oabi_flock64 __user *)arg,\n\t\t\t\t\t &user, sizeof(user)))\n\t\t\t\tret = -EFAULT;\n\t\t}\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\tset_fs(fs);\n\t}\n\n\treturn ret;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nint perf_event_refresh(struct perf_event *event, int refresh)\n{\n\tstruct perf_event_context *ctx;\n\tint ret;\n\n\tctx = perf_event_ctx_lock(event);\n\tret = _perf_event_refresh(event, refresh);\n\tperf_event_ctx_unlock(event, ctx);\n\n\treturn ret;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int perf_event_read_group(struct perf_event *event,\n\t\t\t\t   u64 read_format, char __user *buf)\n{\n\tstruct perf_event *leader = event->group_leader, *sub;\n\tint n = 0, size = 0, ret = -EFAULT;\n\tstruct perf_event_context *ctx = leader->ctx;\n\tu64 values[5];\n\tu64 count, enabled, running;\n\n\tmutex_lock(&ctx->mutex);\n\tcount = perf_event_read_value(leader, &enabled, &running);\n\n\tvalues[n++] = 1 + leader->nr_siblings;\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_ENABLED)\n\t\tvalues[n++] = enabled;\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_RUNNING)\n\t\tvalues[n++] = running;\n\tvalues[n++] = count;\n\tif (read_format & PERF_FORMAT_ID)\n\t\tvalues[n++] = primary_event_id(leader);\n\n\tsize = n * sizeof(u64);\n\n\tif (copy_to_user(buf, values, size))\n\t\tgoto unlock;\n\n\tret = size;\n\n\tlist_for_each_entry(sub, &leader->sibling_list, group_entry) {\n\t\tn = 0;\n\n\t\tvalues[n++] = perf_event_read_value(sub, &enabled, &running);\n\t\tif (read_format & PERF_FORMAT_ID)\n\t\t\tvalues[n++] = primary_event_id(sub);\n\n\t\tsize = n * sizeof(u64);\n\n\t\tif (copy_to_user(buf + ret, values, size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tret += size;\n\t}\nunlock:\n\tmutex_unlock(&ctx->mutex);\n\n\treturn ret;\n}",
                        "code_after_change": "static long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\tmutex_lock(&client->lock);\n\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n\t\tif (IS_ERR(handle)) {\n\t\t\tmutex_unlock(&client->lock);\n\t\t\treturn PTR_ERR(handle);\n\t\t}\n\t\tion_free_nolock(client, handle);\n\t\tion_handle_put_nolock(handle);\n\t\tmutex_unlock(&client->lock);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}",
                        "cve_id": "CVE-2016-6786"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic long perf_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct perf_event *event = file->private_data;\n\tstruct perf_event_context *ctx;\n\tlong ret;\n\n\tctx = perf_event_ctx_lock(event);\n\tret = _perf_ioctl(event, cmd, arg);\n\tperf_event_ctx_unlock(event, ctx);\n\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int perf_event_read_group(struct perf_event *event,\n\t\t\t\t   u64 read_format, char __user *buf)\n{\n\tstruct perf_event *leader = event->group_leader, *sub;\n\tint n = 0, size = 0, ret = -EFAULT;\n\tstruct perf_event_context *ctx = leader->ctx;\n\tu64 values[5];\n\tu64 count, enabled, running;\n\n\tmutex_lock(&ctx->mutex);\n\tcount = perf_event_read_value(leader, &enabled, &running);\n\n\tvalues[n++] = 1 + leader->nr_siblings;\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_ENABLED)\n\t\tvalues[n++] = enabled;\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_RUNNING)\n\t\tvalues[n++] = running;\n\tvalues[n++] = count;\n\tif (read_format & PERF_FORMAT_ID)\n\t\tvalues[n++] = primary_event_id(leader);\n\n\tsize = n * sizeof(u64);\n\n\tif (copy_to_user(buf, values, size))\n\t\tgoto unlock;\n\n\tret = size;\n\n\tlist_for_each_entry(sub, &leader->sibling_list, group_entry) {\n\t\tn = 0;\n\n\t\tvalues[n++] = perf_event_read_value(sub, &enabled, &running);\n\t\tif (read_format & PERF_FORMAT_ID)\n\t\t\tvalues[n++] = primary_event_id(sub);\n\n\t\tsize = n * sizeof(u64);\n\n\t\tif (copy_to_user(buf + ret, values, size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tret += size;\n\t}\nunlock:\n\tmutex_unlock(&ctx->mutex);\n\n\treturn ret;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\tmutex_lock(&client->lock);\n\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n\t\tif (IS_ERR(handle)) {\n\t\t\tmutex_unlock(&client->lock);\n\t\t\treturn PTR_ERR(handle);\n\t\t}\n\t\tion_free_nolock(client, handle);\n\t\tion_handle_put_nolock(handle);\n\t\tmutex_unlock(&client->lock);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 1081,
            "cve_id": "CVE-2016-6786",
            "code_snippet": "void perf_pmu_migrate_context(struct pmu *pmu, int src_cpu, int dst_cpu)\n{\n\tstruct perf_event_context *src_ctx;\n\tstruct perf_event_context *dst_ctx;\n\tstruct perf_event *event, *tmp;\n\tLIST_HEAD(events);\n\n\tsrc_ctx = &per_cpu_ptr(pmu->pmu_cpu_context, src_cpu)->ctx;\n\tdst_ctx = &per_cpu_ptr(pmu->pmu_cpu_context, dst_cpu)->ctx;\n\n\t/*\n\t * See perf_event_ctx_lock() for comments on the details\n\t * of swizzling perf_event::ctx.\n\t */\n\tmutex_lock_double(&src_ctx->mutex, &dst_ctx->mutex);\n\tlist_for_each_entry_safe(event, tmp, &src_ctx->event_list,\n\t\t\t\t event_entry) {\n\t\tperf_remove_from_context(event, false);\n\t\tunaccount_event_cpu(event, src_cpu);\n\t\tput_ctx(src_ctx);\n\t\tlist_add(&event->migrate_entry, &events);\n\t}\n\n\tsynchronize_rcu();\n\n\tlist_for_each_entry_safe(event, tmp, &events, migrate_entry) {\n\t\tlist_del(&event->migrate_entry);\n\t\tif (event->state >= PERF_EVENT_STATE_OFF)\n\t\t\tevent->state = PERF_EVENT_STATE_INACTIVE;\n\t\taccount_event_cpu(event, dst_cpu);\n\t\tperf_install_in_context(dst_ctx, event, dst_cpu);\n\t\tget_ctx(dst_ctx);\n\t}\n\tmutex_unlock(&dst_ctx->mutex);\n\tmutex_unlock(&src_ctx->mutex);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": " */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx;\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tstruct perf_event_context *gctx = group_leader->ctx;\n\n\t\tmutex_lock(&gctx->mutex);\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\n\tif (move_group) {\n\t\tsynchronize_rcu();\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}",
                        "code_after_change": " */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *uninitialized_var(gctx);\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tgctx = group_leader->ctx;\n\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t} else {\n\t\tmutex_lock(&ctx->mutex);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\tif (move_group) {\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\n\tif (move_group) {\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}",
                        "cve_id": "CVE-2016-6786"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nvoid perf_pmu_migrate_context(struct pmu *pmu, int src_cpu, int dst_cpu)\n{\n\tstruct perf_event_context *src_ctx;\n\tstruct perf_event_context *dst_ctx;\n\tstruct perf_event *event, *tmp;\n\tLIST_HEAD(events);\n\n\tsrc_ctx = &per_cpu_ptr(pmu->pmu_cpu_context, src_cpu)->ctx;\n\tdst_ctx = &per_cpu_ptr(pmu->pmu_cpu_context, dst_cpu)->ctx;\n\n\t/*\n\t * See perf_event_ctx_lock() for comments on the details\n\t * of swizzling perf_event::ctx.\n\t */\n\tmutex_lock_double(&src_ctx->mutex, &dst_ctx->mutex);\n\tlist_for_each_entry_safe(event, tmp, &src_ctx->event_list,\n\t\t\t\t event_entry) {\n\t\tperf_remove_from_context(event, false);\n\t\tunaccount_event_cpu(event, src_cpu);\n\t\tput_ctx(src_ctx);\n\t\tlist_add(&event->migrate_entry, &events);\n\t}\n\n\tsynchronize_rcu();\n\n\tlist_for_each_entry_safe(event, tmp, &events, migrate_entry) {\n\t\tlist_del(&event->migrate_entry);\n\t\tif (event->state >= PERF_EVENT_STATE_OFF)\n\t\t\tevent->state = PERF_EVENT_STATE_INACTIVE;\n\t\taccount_event_cpu(event, dst_cpu);\n\t\tperf_install_in_context(dst_ctx, event, dst_cpu);\n\t\tget_ctx(dst_ctx);\n\t}\n\tmutex_unlock(&dst_ctx->mutex);\n\tmutex_unlock(&src_ctx->mutex);\n}\n'''\nRelated vulnerability code snippet:\n'''\n */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx;\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tstruct perf_event_context *gctx = group_leader->ctx;\n\n\t\tmutex_lock(&gctx->mutex);\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\n\tif (move_group) {\n\t\tsynchronize_rcu();\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\n */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *uninitialized_var(gctx);\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tgctx = group_leader->ctx;\n\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t} else {\n\t\tmutex_lock(&ctx->mutex);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\tif (move_group) {\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\n\tif (move_group) {\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void perf_event_for_each(struct perf_event *event,\n\t\t\t\t  void (*func)(struct perf_event *))\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_event *sibling;\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\tevent = event->group_leader;\n\n\tperf_event_for_each_child(event, func);\n\tlist_for_each_entry(sibling, &event->sibling_list, group_entry)\n\t\tperf_event_for_each_child(sibling, func);\n\tmutex_unlock(&ctx->mutex);\n}",
                        "code_after_change": "static void perf_event_for_each(struct perf_event *event,\n\t\t\t\t  void (*func)(struct perf_event *))\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_event *sibling;\n\n\tlockdep_assert_held(&ctx->mutex);\n\n\tevent = event->group_leader;\n\n\tperf_event_for_each_child(event, func);\n\tlist_for_each_entry(sibling, &event->sibling_list, group_entry)\n\t\tperf_event_for_each_child(sibling, func);\n}",
                        "cve_id": "CVE-2016-6786"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nvoid perf_pmu_migrate_context(struct pmu *pmu, int src_cpu, int dst_cpu)\n{\n\tstruct perf_event_context *src_ctx;\n\tstruct perf_event_context *dst_ctx;\n\tstruct perf_event *event, *tmp;\n\tLIST_HEAD(events);\n\n\tsrc_ctx = &per_cpu_ptr(pmu->pmu_cpu_context, src_cpu)->ctx;\n\tdst_ctx = &per_cpu_ptr(pmu->pmu_cpu_context, dst_cpu)->ctx;\n\n\t/*\n\t * See perf_event_ctx_lock() for comments on the details\n\t * of swizzling perf_event::ctx.\n\t */\n\tmutex_lock_double(&src_ctx->mutex, &dst_ctx->mutex);\n\tlist_for_each_entry_safe(event, tmp, &src_ctx->event_list,\n\t\t\t\t event_entry) {\n\t\tperf_remove_from_context(event, false);\n\t\tunaccount_event_cpu(event, src_cpu);\n\t\tput_ctx(src_ctx);\n\t\tlist_add(&event->migrate_entry, &events);\n\t}\n\n\tsynchronize_rcu();\n\n\tlist_for_each_entry_safe(event, tmp, &events, migrate_entry) {\n\t\tlist_del(&event->migrate_entry);\n\t\tif (event->state >= PERF_EVENT_STATE_OFF)\n\t\t\tevent->state = PERF_EVENT_STATE_INACTIVE;\n\t\taccount_event_cpu(event, dst_cpu);\n\t\tperf_install_in_context(dst_ctx, event, dst_cpu);\n\t\tget_ctx(dst_ctx);\n\t}\n\tmutex_unlock(&dst_ctx->mutex);\n\tmutex_unlock(&src_ctx->mutex);\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic void perf_event_for_each(struct perf_event *event,\n\t\t\t\t  void (*func)(struct perf_event *))\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_event *sibling;\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\tevent = event->group_leader;\n\n\tperf_event_for_each_child(event, func);\n\tlist_for_each_entry(sibling, &event->sibling_list, group_entry)\n\t\tperf_event_for_each_child(sibling, func);\n\tmutex_unlock(&ctx->mutex);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic void perf_event_for_each(struct perf_event *event,\n\t\t\t\t  void (*func)(struct perf_event *))\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct perf_event *sibling;\n\n\tlockdep_assert_held(&ctx->mutex);\n\n\tevent = event->group_leader;\n\n\tperf_event_for_each_child(event, func);\n\tlist_for_each_entry(sibling, &event->sibling_list, group_entry)\n\t\tperf_event_for_each_child(sibling, func);\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "void perf_event_enable(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct task_struct *task = ctx->task;\n\n\tif (!task) {\n\t\t/*\n\t\t * Enable the event on the cpu that it's on\n\t\t */\n\t\tcpu_function_call(event->cpu, __perf_event_enable, event);\n\t\treturn;\n\t}\n\n\traw_spin_lock_irq(&ctx->lock);\n\tif (event->state >= PERF_EVENT_STATE_INACTIVE)\n\t\tgoto out;\n\n\t/*\n\t * If the event is in error state, clear that first.\n\t * That way, if we see the event in error state below, we\n\t * know that it has gone back into error state, as distinct\n\t * from the task having been scheduled away before the\n\t * cross-call arrived.\n\t */\n\tif (event->state == PERF_EVENT_STATE_ERROR)\n\t\tevent->state = PERF_EVENT_STATE_OFF;\n\nretry:\n\tif (!ctx->is_active) {\n\t\t__perf_event_mark_enabled(event);\n\t\tgoto out;\n\t}\n\n\traw_spin_unlock_irq(&ctx->lock);\n\n\tif (!task_function_call(task, __perf_event_enable, event))\n\t\treturn;\n\n\traw_spin_lock_irq(&ctx->lock);\n\n\t/*\n\t * If the context is active and the event is still off,\n\t * we need to retry the cross-call.\n\t */\n\tif (ctx->is_active && event->state == PERF_EVENT_STATE_OFF) {\n\t\t/*\n\t\t * task could have been flipped by a concurrent\n\t\t * perf_event_context_sched_out()\n\t\t */\n\t\ttask = ctx->task;\n\t\tgoto retry;\n\t}\n\nout:\n\traw_spin_unlock_irq(&ctx->lock);\n}",
                        "code_after_change": "int perf_event_task_disable(void)\n{\n\tstruct perf_event_context *ctx;\n\tstruct perf_event *event;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_for_each_entry(event, &current->perf_event_list, owner_entry) {\n\t\tctx = perf_event_ctx_lock(event);\n\t\tperf_event_for_each_child(event, _perf_event_disable);\n\t\tperf_event_ctx_unlock(event, ctx);\n\t}\n\tmutex_unlock(&current->perf_event_mutex);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2016-6786"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nvoid perf_pmu_migrate_context(struct pmu *pmu, int src_cpu, int dst_cpu)\n{\n\tstruct perf_event_context *src_ctx;\n\tstruct perf_event_context *dst_ctx;\n\tstruct perf_event *event, *tmp;\n\tLIST_HEAD(events);\n\n\tsrc_ctx = &per_cpu_ptr(pmu->pmu_cpu_context, src_cpu)->ctx;\n\tdst_ctx = &per_cpu_ptr(pmu->pmu_cpu_context, dst_cpu)->ctx;\n\n\t/*\n\t * See perf_event_ctx_lock() for comments on the details\n\t * of swizzling perf_event::ctx.\n\t */\n\tmutex_lock_double(&src_ctx->mutex, &dst_ctx->mutex);\n\tlist_for_each_entry_safe(event, tmp, &src_ctx->event_list,\n\t\t\t\t event_entry) {\n\t\tperf_remove_from_context(event, false);\n\t\tunaccount_event_cpu(event, src_cpu);\n\t\tput_ctx(src_ctx);\n\t\tlist_add(&event->migrate_entry, &events);\n\t}\n\n\tsynchronize_rcu();\n\n\tlist_for_each_entry_safe(event, tmp, &events, migrate_entry) {\n\t\tlist_del(&event->migrate_entry);\n\t\tif (event->state >= PERF_EVENT_STATE_OFF)\n\t\t\tevent->state = PERF_EVENT_STATE_INACTIVE;\n\t\taccount_event_cpu(event, dst_cpu);\n\t\tperf_install_in_context(dst_ctx, event, dst_cpu);\n\t\tget_ctx(dst_ctx);\n\t}\n\tmutex_unlock(&dst_ctx->mutex);\n\tmutex_unlock(&src_ctx->mutex);\n}\n'''\nRelated vulnerability code snippet:\n'''\nvoid perf_event_enable(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct task_struct *task = ctx->task;\n\n\tif (!task) {\n\t\t/*\n\t\t * Enable the event on the cpu that it's on\n\t\t */\n\t\tcpu_function_call(event->cpu, __perf_event_enable, event);\n\t\treturn;\n\t}\n\n\traw_spin_lock_irq(&ctx->lock);\n\tif (event->state >= PERF_EVENT_STATE_INACTIVE)\n\t\tgoto out;\n\n\t/*\n\t * If the event is in error state, clear that first.\n\t * That way, if we see the event in error state below, we\n\t * know that it has gone back into error state, as distinct\n\t * from the task having been scheduled away before the\n\t * cross-call arrived.\n\t */\n\tif (event->state == PERF_EVENT_STATE_ERROR)\n\t\tevent->state = PERF_EVENT_STATE_OFF;\n\nretry:\n\tif (!ctx->is_active) {\n\t\t__perf_event_mark_enabled(event);\n\t\tgoto out;\n\t}\n\n\traw_spin_unlock_irq(&ctx->lock);\n\n\tif (!task_function_call(task, __perf_event_enable, event))\n\t\treturn;\n\n\traw_spin_lock_irq(&ctx->lock);\n\n\t/*\n\t * If the context is active and the event is still off,\n\t * we need to retry the cross-call.\n\t */\n\tif (ctx->is_active && event->state == PERF_EVENT_STATE_OFF) {\n\t\t/*\n\t\t * task could have been flipped by a concurrent\n\t\t * perf_event_context_sched_out()\n\t\t */\n\t\ttask = ctx->task;\n\t\tgoto retry;\n\t}\n\nout:\n\traw_spin_unlock_irq(&ctx->lock);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nint perf_event_task_disable(void)\n{\n\tstruct perf_event_context *ctx;\n\tstruct perf_event *event;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_for_each_entry(event, &current->perf_event_list, owner_entry) {\n\t\tctx = perf_event_ctx_lock(event);\n\t\tperf_event_for_each_child(event, _perf_event_disable);\n\t\tperf_event_ctx_unlock(event, ctx);\n\t}\n\tmutex_unlock(&current->perf_event_mutex);\n\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 1078,
            "cve_id": "CVE-2016-6786",
            "code_snippet": "static void perf_remove_from_owner(struct perf_event *event)\n{\n\tstruct task_struct *owner;\n\n\trcu_read_lock();\n\towner = ACCESS_ONCE(event->owner);\n\t/*\n\t * Matches the smp_wmb() in perf_event_exit_task(). If we observe\n\t * !owner it means the list deletion is complete and we can indeed\n\t * free this event, otherwise we need to serialize on\n\t * owner->perf_event_mutex.\n\t */\n\tsmp_read_barrier_depends();\n\tif (owner) {\n\t\t/*\n\t\t * Since delayed_put_task_struct() also drops the last\n\t\t * task reference we can safely take a new reference\n\t\t * while holding the rcu_read_lock().\n\t\t */\n\t\tget_task_struct(owner);\n\t}\n\trcu_read_unlock();\n\n\tif (owner) {\n\t\t/*\n\t\t * If we're here through perf_event_exit_task() we're already\n\t\t * holding ctx->mutex which would be an inversion wrt. the\n\t\t * normal lock order.\n\t\t *\n\t\t * However we can safely take this lock because its the child\n\t\t * ctx->mutex.\n\t\t */\n\t\tmutex_lock_nested(&owner->perf_event_mutex, SINGLE_DEPTH_NESTING);\n\n\t\t/*\n\t\t * We have to re-check the event->owner field, if it is cleared\n\t\t * we raced with perf_event_exit_task(), acquiring the mutex\n\t\t * ensured they're done, and we can proceed with freeing the\n\t\t * event.\n\t\t */\n\t\tif (event->owner)\n\t\t\tlist_del_init(&event->owner_entry);\n\t\tmutex_unlock(&owner->perf_event_mutex);\n\t\tput_task_struct(owner);\n\t}\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": " */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx;\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tstruct perf_event_context *gctx = group_leader->ctx;\n\n\t\tmutex_lock(&gctx->mutex);\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\n\tif (move_group) {\n\t\tsynchronize_rcu();\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}",
                        "code_after_change": " */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *uninitialized_var(gctx);\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tgctx = group_leader->ctx;\n\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t} else {\n\t\tmutex_lock(&ctx->mutex);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\tif (move_group) {\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\n\tif (move_group) {\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}",
                        "cve_id": "CVE-2016-6786"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic void perf_remove_from_owner(struct perf_event *event)\n{\n\tstruct task_struct *owner;\n\n\trcu_read_lock();\n\towner = ACCESS_ONCE(event->owner);\n\t/*\n\t * Matches the smp_wmb() in perf_event_exit_task(). If we observe\n\t * !owner it means the list deletion is complete and we can indeed\n\t * free this event, otherwise we need to serialize on\n\t * owner->perf_event_mutex.\n\t */\n\tsmp_read_barrier_depends();\n\tif (owner) {\n\t\t/*\n\t\t * Since delayed_put_task_struct() also drops the last\n\t\t * task reference we can safely take a new reference\n\t\t * while holding the rcu_read_lock().\n\t\t */\n\t\tget_task_struct(owner);\n\t}\n\trcu_read_unlock();\n\n\tif (owner) {\n\t\t/*\n\t\t * If we're here through perf_event_exit_task() we're already\n\t\t * holding ctx->mutex which would be an inversion wrt. the\n\t\t * normal lock order.\n\t\t *\n\t\t * However we can safely take this lock because its the child\n\t\t * ctx->mutex.\n\t\t */\n\t\tmutex_lock_nested(&owner->perf_event_mutex, SINGLE_DEPTH_NESTING);\n\n\t\t/*\n\t\t * We have to re-check the event->owner field, if it is cleared\n\t\t * we raced with perf_event_exit_task(), acquiring the mutex\n\t\t * ensured they're done, and we can proceed with freeing the\n\t\t * event.\n\t\t */\n\t\tif (event->owner)\n\t\t\tlist_del_init(&event->owner_entry);\n\t\tmutex_unlock(&owner->perf_event_mutex);\n\t\tput_task_struct(owner);\n\t}\n}\n'''\nRelated vulnerability code snippet:\n'''\n */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx;\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tstruct perf_event_context *gctx = group_leader->ctx;\n\n\t\tmutex_lock(&gctx->mutex);\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\n\tif (move_group) {\n\t\tsynchronize_rcu();\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\n */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *uninitialized_var(gctx);\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tgctx = group_leader->ctx;\n\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t} else {\n\t\tmutex_lock(&ctx->mutex);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\tif (move_group) {\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\n\tif (move_group) {\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "void perf_event_disable(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct task_struct *task = ctx->task;\n\n\tif (!task) {\n\t\t/*\n\t\t * Disable the event on the cpu that it's on\n\t\t */\n\t\tcpu_function_call(event->cpu, __perf_event_disable, event);\n\t\treturn;\n\t}\n\nretry:\n\tif (!task_function_call(task, __perf_event_disable, event))\n\t\treturn;\n\n\traw_spin_lock_irq(&ctx->lock);\n\t/*\n\t * If the event is still active, we need to retry the cross-call.\n\t */\n\tif (event->state == PERF_EVENT_STATE_ACTIVE) {\n\t\traw_spin_unlock_irq(&ctx->lock);\n\t\t/*\n\t\t * Reload the task pointer, it might have been changed by\n\t\t * a concurrent perf_event_context_sched_out().\n\t\t */\n\t\ttask = ctx->task;\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * Since we have the lock this context can't be scheduled\n\t * in, so we can change the state safely.\n\t */\n\tif (event->state == PERF_EVENT_STATE_INACTIVE) {\n\t\tupdate_group_times(event);\n\t\tevent->state = PERF_EVENT_STATE_OFF;\n\t}\n\traw_spin_unlock_irq(&ctx->lock);\n}",
                        "code_after_change": "int create_user_ns(struct cred *new)\n{\n\tstruct user_namespace *ns, *parent_ns = new->user_ns;\n\tkuid_t owner = new->euid;\n\tkgid_t group = new->egid;\n\tint ret;\n\n\t/*\n\t * Verify that we can not violate the policy of which files\n\t * may be accessed that is specified by the root directory,\n\t * by verifing that the root directory is at the root of the\n\t * mount namespace which allows all files to be accessed.\n\t */\n\tif (current_chrooted())\n\t\treturn -EPERM;\n\n\t/* The creator needs a mapping in the parent user namespace\n\t * or else we won't be able to reasonably tell userspace who\n\t * created a user_namespace.\n\t */\n\tif (!kuid_has_mapping(parent_ns, owner) ||\n\t    !kgid_has_mapping(parent_ns, group))\n\t\treturn -EPERM;\n\n\tns = kmem_cache_zalloc(user_ns_cachep, GFP_KERNEL);\n\tif (!ns)\n\t\treturn -ENOMEM;\n\n\tret = proc_alloc_inum(&ns->proc_inum);\n\tif (ret) {\n\t\tkmem_cache_free(user_ns_cachep, ns);\n\t\treturn ret;\n\t}\n\n\tatomic_set(&ns->count, 1);\n\t/* Leave the new->user_ns reference with the new user namespace. */\n\tns->parent = parent_ns;\n\tns->owner = owner;\n\tns->group = group;\n\n\tset_cred_user_ns(new, ns);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2016-6786"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic void perf_remove_from_owner(struct perf_event *event)\n{\n\tstruct task_struct *owner;\n\n\trcu_read_lock();\n\towner = ACCESS_ONCE(event->owner);\n\t/*\n\t * Matches the smp_wmb() in perf_event_exit_task(). If we observe\n\t * !owner it means the list deletion is complete and we can indeed\n\t * free this event, otherwise we need to serialize on\n\t * owner->perf_event_mutex.\n\t */\n\tsmp_read_barrier_depends();\n\tif (owner) {\n\t\t/*\n\t\t * Since delayed_put_task_struct() also drops the last\n\t\t * task reference we can safely take a new reference\n\t\t * while holding the rcu_read_lock().\n\t\t */\n\t\tget_task_struct(owner);\n\t}\n\trcu_read_unlock();\n\n\tif (owner) {\n\t\t/*\n\t\t * If we're here through perf_event_exit_task() we're already\n\t\t * holding ctx->mutex which would be an inversion wrt. the\n\t\t * normal lock order.\n\t\t *\n\t\t * However we can safely take this lock because its the child\n\t\t * ctx->mutex.\n\t\t */\n\t\tmutex_lock_nested(&owner->perf_event_mutex, SINGLE_DEPTH_NESTING);\n\n\t\t/*\n\t\t * We have to re-check the event->owner field, if it is cleared\n\t\t * we raced with perf_event_exit_task(), acquiring the mutex\n\t\t * ensured they're done, and we can proceed with freeing the\n\t\t * event.\n\t\t */\n\t\tif (event->owner)\n\t\t\tlist_del_init(&event->owner_entry);\n\t\tmutex_unlock(&owner->perf_event_mutex);\n\t\tput_task_struct(owner);\n\t}\n}\n'''\nRelated vulnerability code snippet:\n'''\nvoid perf_event_disable(struct perf_event *event)\n{\n\tstruct perf_event_context *ctx = event->ctx;\n\tstruct task_struct *task = ctx->task;\n\n\tif (!task) {\n\t\t/*\n\t\t * Disable the event on the cpu that it's on\n\t\t */\n\t\tcpu_function_call(event->cpu, __perf_event_disable, event);\n\t\treturn;\n\t}\n\nretry:\n\tif (!task_function_call(task, __perf_event_disable, event))\n\t\treturn;\n\n\traw_spin_lock_irq(&ctx->lock);\n\t/*\n\t * If the event is still active, we need to retry the cross-call.\n\t */\n\tif (event->state == PERF_EVENT_STATE_ACTIVE) {\n\t\traw_spin_unlock_irq(&ctx->lock);\n\t\t/*\n\t\t * Reload the task pointer, it might have been changed by\n\t\t * a concurrent perf_event_context_sched_out().\n\t\t */\n\t\ttask = ctx->task;\n\t\tgoto retry;\n\t}\n\n\t/*\n\t * Since we have the lock this context can't be scheduled\n\t * in, so we can change the state safely.\n\t */\n\tif (event->state == PERF_EVENT_STATE_INACTIVE) {\n\t\tupdate_group_times(event);\n\t\tevent->state = PERF_EVENT_STATE_OFF;\n\t}\n\traw_spin_unlock_irq(&ctx->lock);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nint create_user_ns(struct cred *new)\n{\n\tstruct user_namespace *ns, *parent_ns = new->user_ns;\n\tkuid_t owner = new->euid;\n\tkgid_t group = new->egid;\n\tint ret;\n\n\t/*\n\t * Verify that we can not violate the policy of which files\n\t * may be accessed that is specified by the root directory,\n\t * by verifing that the root directory is at the root of the\n\t * mount namespace which allows all files to be accessed.\n\t */\n\tif (current_chrooted())\n\t\treturn -EPERM;\n\n\t/* The creator needs a mapping in the parent user namespace\n\t * or else we won't be able to reasonably tell userspace who\n\t * created a user_namespace.\n\t */\n\tif (!kuid_has_mapping(parent_ns, owner) ||\n\t    !kgid_has_mapping(parent_ns, group))\n\t\treturn -EPERM;\n\n\tns = kmem_cache_zalloc(user_ns_cachep, GFP_KERNEL);\n\tif (!ns)\n\t\treturn -ENOMEM;\n\n\tret = proc_alloc_inum(&ns->proc_inum);\n\tif (ret) {\n\t\tkmem_cache_free(user_ns_cachep, ns);\n\t\treturn ret;\n\t}\n\n\tatomic_set(&ns->count, 1);\n\t/* Leave the new->user_ns reference with the new user namespace. */\n\tns->parent = parent_ns;\n\tns->owner = owner;\n\tns->group = group;\n\n\tset_cred_user_ns(new, ns);\n\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int create_user_ns(struct cred *new)\n{\n\tstruct user_namespace *ns, *parent_ns = new->user_ns;\n\tkuid_t owner = new->euid;\n\tkgid_t group = new->egid;\n\tint ret;\n\n\t/* The creator needs a mapping in the parent user namespace\n\t * or else we won't be able to reasonably tell userspace who\n\t * created a user_namespace.\n\t */\n\tif (!kuid_has_mapping(parent_ns, owner) ||\n\t    !kgid_has_mapping(parent_ns, group))\n\t\treturn -EPERM;\n\n\tns = kmem_cache_zalloc(user_ns_cachep, GFP_KERNEL);\n\tif (!ns)\n\t\treturn -ENOMEM;\n\n\tret = proc_alloc_inum(&ns->proc_inum);\n\tif (ret) {\n\t\tkmem_cache_free(user_ns_cachep, ns);\n\t\treturn ret;\n\t}\n\n\tatomic_set(&ns->count, 1);\n\t/* Leave the new->user_ns reference with the new user namespace. */\n\tns->parent = parent_ns;\n\tns->owner = owner;\n\tns->group = group;\n\n\tset_cred_user_ns(new, ns);\n\n\treturn 0;\n}",
                        "code_after_change": "STATIC int\nxfs_ioctl_setattr(\n\txfs_inode_t\t\t*ip,\n\tstruct fsxattr\t\t*fa,\n\tint\t\t\tmask)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tstruct xfs_trans\t*tp;\n\tunsigned int\t\tlock_flags = 0;\n\tstruct xfs_dquot\t*udqp = NULL;\n\tstruct xfs_dquot\t*pdqp = NULL;\n\tstruct xfs_dquot\t*olddquot = NULL;\n\tint\t\t\tcode;\n\n\ttrace_xfs_ioctl_setattr(ip);\n\n\tif (mp->m_flags & XFS_MOUNT_RDONLY)\n\t\treturn XFS_ERROR(EROFS);\n\tif (XFS_FORCED_SHUTDOWN(mp))\n\t\treturn XFS_ERROR(EIO);\n\n\t/*\n\t * Disallow 32bit project ids when projid32bit feature is not enabled.\n\t */\n\tif ((mask & FSX_PROJID) && (fa->fsx_projid > (__uint16_t)-1) &&\n\t\t\t!xfs_sb_version_hasprojid32bit(&ip->i_mount->m_sb))\n\t\treturn XFS_ERROR(EINVAL);\n\n\t/*\n\t * If disk quotas is on, we make sure that the dquots do exist on disk,\n\t * before we start any other transactions. Trying to do this later\n\t * is messy. We don't care to take a readlock to look at the ids\n\t * in inode here, because we can't hold it across the trans_reserve.\n\t * If the IDs do change before we take the ilock, we're covered\n\t * because the i_*dquot fields will get updated anyway.\n\t */\n\tif (XFS_IS_QUOTA_ON(mp) && (mask & FSX_PROJID)) {\n\t\tcode = xfs_qm_vop_dqalloc(ip, ip->i_d.di_uid,\n\t\t\t\t\t ip->i_d.di_gid, fa->fsx_projid,\n\t\t\t\t\t XFS_QMOPT_PQUOTA, &udqp, NULL, &pdqp);\n\t\tif (code)\n\t\t\treturn code;\n\t}\n\n\t/*\n\t * For the other attributes, we acquire the inode lock and\n\t * first do an error checking pass.\n\t */\n\ttp = xfs_trans_alloc(mp, XFS_TRANS_SETATTR_NOT_SIZE);\n\tcode = xfs_trans_reserve(tp, &M_RES(mp)->tr_ichange, 0, 0);\n\tif (code)\n\t\tgoto error_return;\n\n\tlock_flags = XFS_ILOCK_EXCL;\n\txfs_ilock(ip, lock_flags);\n\n\t/*\n\t * CAP_FOWNER overrides the following restrictions:\n\t *\n\t * The user ID of the calling process must be equal\n\t * to the file owner ID, except in cases where the\n\t * CAP_FSETID capability is applicable.\n\t */\n\tif (!inode_owner_or_capable(VFS_I(ip))) {\n\t\tcode = XFS_ERROR(EPERM);\n\t\tgoto error_return;\n\t}\n\n\t/*\n\t * Do a quota reservation only if projid is actually going to change.\n\t * Only allow changing of projid from init_user_ns since it is a\n\t * non user namespace aware identifier.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\tif (current_user_ns() != &init_user_ns) {\n\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\tgoto error_return;\n\t\t}\n\n\t\tif (XFS_IS_QUOTA_RUNNING(mp) &&\n\t\t    XFS_IS_PQUOTA_ON(mp) &&\n\t\t    xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tASSERT(tp);\n\t\t\tcode = xfs_qm_vop_chown_reserve(tp, ip, udqp, NULL,\n\t\t\t\t\t\tpdqp, capable(CAP_FOWNER) ?\n\t\t\t\t\t\tXFS_QMOPT_FORCE_RES : 0);\n\t\t\tif (code)\t/* out of quota */\n\t\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\tif (mask & FSX_EXTSIZE) {\n\t\t/*\n\t\t * Can't change extent size if any extents are allocated.\n\t\t */\n\t\tif (ip->i_d.di_nextents &&\n\t\t    ((ip->i_d.di_extsize << mp->m_sb.sb_blocklog) !=\n\t\t     fa->fsx_extsize)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * Extent size must be a multiple of the appropriate block\n\t\t * size, if set at all. It must also be smaller than the\n\t\t * maximum extent size supported by the filesystem.\n\t\t *\n\t\t * Also, for non-realtime files, limit the extent size hint to\n\t\t * half the size of the AGs in the filesystem so alignment\n\t\t * doesn't result in extents larger than an AG.\n\t\t */\n\t\tif (fa->fsx_extsize != 0) {\n\t\t\txfs_extlen_t    size;\n\t\t\txfs_fsblock_t   extsize_fsb;\n\n\t\t\textsize_fsb = XFS_B_TO_FSB(mp, fa->fsx_extsize);\n\t\t\tif (extsize_fsb > MAXEXTLEN) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\n\t\t\tif (XFS_IS_REALTIME_INODE(ip) ||\n\t\t\t    ((mask & FSX_XFLAGS) &&\n\t\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME))) {\n\t\t\t\tsize = mp->m_sb.sb_rextsize <<\n\t\t\t\t       mp->m_sb.sb_blocklog;\n\t\t\t} else {\n\t\t\t\tsize = mp->m_sb.sb_blocksize;\n\t\t\t\tif (extsize_fsb > mp->m_sb.sb_agblocks / 2) {\n\t\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\t\tgoto error_return;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (fa->fsx_extsize % size) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\t}\n\n\n\tif (mask & FSX_XFLAGS) {\n\t\t/*\n\t\t * Can't change realtime flag if any extents are allocated.\n\t\t */\n\t\tif ((ip->i_d.di_nextents || ip->i_delayed_blks) &&\n\t\t    (XFS_IS_REALTIME_INODE(ip)) !=\n\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * If realtime flag is set then must have realtime data.\n\t\t */\n\t\tif ((fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tif ((mp->m_sb.sb_rblocks == 0) ||\n\t\t\t    (mp->m_sb.sb_rextsize == 0) ||\n\t\t\t    (ip->i_d.di_extsize % mp->m_sb.sb_rextsize)) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Can't modify an immutable/append-only file unless\n\t\t * we have appropriate permission.\n\t\t */\n\t\tif ((ip->i_d.di_flags &\n\t\t\t\t(XFS_DIFLAG_IMMUTABLE|XFS_DIFLAG_APPEND) ||\n\t\t     (fa->fsx_xflags &\n\t\t\t\t(XFS_XFLAG_IMMUTABLE | XFS_XFLAG_APPEND))) &&\n\t\t    !capable(CAP_LINUX_IMMUTABLE)) {\n\t\t\tcode = XFS_ERROR(EPERM);\n\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\txfs_trans_ijoin(tp, ip, 0);\n\n\t/*\n\t * Change file ownership.  Must be the owner or privileged.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\t/*\n\t\t * CAP_FSETID overrides the following restrictions:\n\t\t *\n\t\t * The set-user-ID and set-group-ID bits of a file will be\n\t\t * cleared upon successful return from chown()\n\t\t */\n\t\tif ((ip->i_d.di_mode & (S_ISUID|S_ISGID)) &&\n\t\t    !capable_wrt_inode_uidgid(VFS_I(ip), CAP_FSETID))\n\t\t\tip->i_d.di_mode &= ~(S_ISUID|S_ISGID);\n\n\t\t/*\n\t\t * Change the ownerships and register quota modifications\n\t\t * in the transaction.\n\t\t */\n\t\tif (xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tif (XFS_IS_QUOTA_RUNNING(mp) && XFS_IS_PQUOTA_ON(mp)) {\n\t\t\t\tolddquot = xfs_qm_vop_chown(tp, ip,\n\t\t\t\t\t\t\t&ip->i_pdquot, pdqp);\n\t\t\t}\n\t\t\txfs_set_projid(ip, fa->fsx_projid);\n\n\t\t\t/*\n\t\t\t * We may have to rev the inode as well as\n\t\t\t * the superblock version number since projids didn't\n\t\t\t * exist before DINODE_VERSION_2 and SB_VERSION_NLINK.\n\t\t\t */\n\t\t\tif (ip->i_d.di_version == 1)\n\t\t\t\txfs_bump_ino_vers2(tp, ip);\n\t\t}\n\n\t}\n\n\tif (mask & FSX_EXTSIZE)\n\t\tip->i_d.di_extsize = fa->fsx_extsize >> mp->m_sb.sb_blocklog;\n\tif (mask & FSX_XFLAGS) {\n\t\txfs_set_diflags(ip, fa->fsx_xflags);\n\t\txfs_diflags_to_linux(ip);\n\t}\n\n\txfs_trans_ichgtime(tp, ip, XFS_ICHGTIME_CHG);\n\txfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);\n\n\tXFS_STATS_INC(xs_ig_attrchg);\n\n\t/*\n\t * If this is a synchronous mount, make sure that the\n\t * transaction goes to disk before returning to the user.\n\t * This is slightly sub-optimal in that truncates require\n\t * two sync transactions instead of one for wsync filesystems.\n\t * One for the truncate and one for the timestamps since we\n\t * don't want to change the timestamps unless we're sure the\n\t * truncate worked.  Truncates are less than 1% of the laddis\n\t * mix so this probably isn't worth the trouble to optimize.\n\t */\n\tif (mp->m_flags & XFS_MOUNT_WSYNC)\n\t\txfs_trans_set_sync(tp);\n\tcode = xfs_trans_commit(tp, 0);\n\txfs_iunlock(ip, lock_flags);\n\n\t/*\n\t * Release any dquot(s) the inode had kept before chown.\n\t */\n\txfs_qm_dqrele(olddquot);\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\n\treturn code;\n\n error_return:\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\txfs_trans_cancel(tp, 0);\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\n\treturn code;\n}",
                        "cve_id": "CVE-2013-1956"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic void perf_remove_from_owner(struct perf_event *event)\n{\n\tstruct task_struct *owner;\n\n\trcu_read_lock();\n\towner = ACCESS_ONCE(event->owner);\n\t/*\n\t * Matches the smp_wmb() in perf_event_exit_task(). If we observe\n\t * !owner it means the list deletion is complete and we can indeed\n\t * free this event, otherwise we need to serialize on\n\t * owner->perf_event_mutex.\n\t */\n\tsmp_read_barrier_depends();\n\tif (owner) {\n\t\t/*\n\t\t * Since delayed_put_task_struct() also drops the last\n\t\t * task reference we can safely take a new reference\n\t\t * while holding the rcu_read_lock().\n\t\t */\n\t\tget_task_struct(owner);\n\t}\n\trcu_read_unlock();\n\n\tif (owner) {\n\t\t/*\n\t\t * If we're here through perf_event_exit_task() we're already\n\t\t * holding ctx->mutex which would be an inversion wrt. the\n\t\t * normal lock order.\n\t\t *\n\t\t * However we can safely take this lock because its the child\n\t\t * ctx->mutex.\n\t\t */\n\t\tmutex_lock_nested(&owner->perf_event_mutex, SINGLE_DEPTH_NESTING);\n\n\t\t/*\n\t\t * We have to re-check the event->owner field, if it is cleared\n\t\t * we raced with perf_event_exit_task(), acquiring the mutex\n\t\t * ensured they're done, and we can proceed with freeing the\n\t\t * event.\n\t\t */\n\t\tif (event->owner)\n\t\t\tlist_del_init(&event->owner_entry);\n\t\tmutex_unlock(&owner->perf_event_mutex);\n\t\tput_task_struct(owner);\n\t}\n}\n'''\nRelated vulnerability code snippet:\n'''\nint create_user_ns(struct cred *new)\n{\n\tstruct user_namespace *ns, *parent_ns = new->user_ns;\n\tkuid_t owner = new->euid;\n\tkgid_t group = new->egid;\n\tint ret;\n\n\t/* The creator needs a mapping in the parent user namespace\n\t * or else we won't be able to reasonably tell userspace who\n\t * created a user_namespace.\n\t */\n\tif (!kuid_has_mapping(parent_ns, owner) ||\n\t    !kgid_has_mapping(parent_ns, group))\n\t\treturn -EPERM;\n\n\tns = kmem_cache_zalloc(user_ns_cachep, GFP_KERNEL);\n\tif (!ns)\n\t\treturn -ENOMEM;\n\n\tret = proc_alloc_inum(&ns->proc_inum);\n\tif (ret) {\n\t\tkmem_cache_free(user_ns_cachep, ns);\n\t\treturn ret;\n\t}\n\n\tatomic_set(&ns->count, 1);\n\t/* Leave the new->user_ns reference with the new user namespace. */\n\tns->parent = parent_ns;\n\tns->owner = owner;\n\tns->group = group;\n\n\tset_cred_user_ns(new, ns);\n\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nSTATIC int\nxfs_ioctl_setattr(\n\txfs_inode_t\t\t*ip,\n\tstruct fsxattr\t\t*fa,\n\tint\t\t\tmask)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tstruct xfs_trans\t*tp;\n\tunsigned int\t\tlock_flags = 0;\n\tstruct xfs_dquot\t*udqp = NULL;\n\tstruct xfs_dquot\t*pdqp = NULL;\n\tstruct xfs_dquot\t*olddquot = NULL;\n\tint\t\t\tcode;\n\n\ttrace_xfs_ioctl_setattr(ip);\n\n\tif (mp->m_flags & XFS_MOUNT_RDONLY)\n\t\treturn XFS_ERROR(EROFS);\n\tif (XFS_FORCED_SHUTDOWN(mp))\n\t\treturn XFS_ERROR(EIO);\n\n\t/*\n\t * Disallow 32bit project ids when projid32bit feature is not enabled.\n\t */\n\tif ((mask & FSX_PROJID) && (fa->fsx_projid > (__uint16_t)-1) &&\n\t\t\t!xfs_sb_version_hasprojid32bit(&ip->i_mount->m_sb))\n\t\treturn XFS_ERROR(EINVAL);\n\n\t/*\n\t * If disk quotas is on, we make sure that the dquots do exist on disk,\n\t * before we start any other transactions. Trying to do this later\n\t * is messy. We don't care to take a readlock to look at the ids\n\t * in inode here, because we can't hold it across the trans_reserve.\n\t * If the IDs do change before we take the ilock, we're covered\n\t * because the i_*dquot fields will get updated anyway.\n\t */\n\tif (XFS_IS_QUOTA_ON(mp) && (mask & FSX_PROJID)) {\n\t\tcode = xfs_qm_vop_dqalloc(ip, ip->i_d.di_uid,\n\t\t\t\t\t ip->i_d.di_gid, fa->fsx_projid,\n\t\t\t\t\t XFS_QMOPT_PQUOTA, &udqp, NULL, &pdqp);\n\t\tif (code)\n\t\t\treturn code;\n\t}\n\n\t/*\n\t * For the other attributes, we acquire the inode lock and\n\t * first do an error checking pass.\n\t */\n\ttp = xfs_trans_alloc(mp, XFS_TRANS_SETATTR_NOT_SIZE);\n\tcode = xfs_trans_reserve(tp, &M_RES(mp)->tr_ichange, 0, 0);\n\tif (code)\n\t\tgoto error_return;\n\n\tlock_flags = XFS_ILOCK_EXCL;\n\txfs_ilock(ip, lock_flags);\n\n\t/*\n\t * CAP_FOWNER overrides the following restrictions:\n\t *\n\t * The user ID of the calling process must be equal\n\t * to the file owner ID, except in cases where the\n\t * CAP_FSETID capability is applicable.\n\t */\n\tif (!inode_owner_or_capable(VFS_I(ip))) {\n\t\tcode = XFS_ERROR(EPERM);\n\t\tgoto error_return;\n\t}\n\n\t/*\n\t * Do a quota reservation only if projid is actually going to change.\n\t * Only allow changing of projid from init_user_ns since it is a\n\t * non user namespace aware identifier.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\tif (current_user_ns() != &init_user_ns) {\n\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\tgoto error_return;\n\t\t}\n\n\t\tif (XFS_IS_QUOTA_RUNNING(mp) &&\n\t\t    XFS_IS_PQUOTA_ON(mp) &&\n\t\t    xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tASSERT(tp);\n\t\t\tcode = xfs_qm_vop_chown_reserve(tp, ip, udqp, NULL,\n\t\t\t\t\t\tpdqp, capable(CAP_FOWNER) ?\n\t\t\t\t\t\tXFS_QMOPT_FORCE_RES : 0);\n\t\t\tif (code)\t/* out of quota */\n\t\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\tif (mask & FSX_EXTSIZE) {\n\t\t/*\n\t\t * Can't change extent size if any extents are allocated.\n\t\t */\n\t\tif (ip->i_d.di_nextents &&\n\t\t    ((ip->i_d.di_extsize << mp->m_sb.sb_blocklog) !=\n\t\t     fa->fsx_extsize)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * Extent size must be a multiple of the appropriate block\n\t\t * size, if set at all. It must also be smaller than the\n\t\t * maximum extent size supported by the filesystem.\n\t\t *\n\t\t * Also, for non-realtime files, limit the extent size hint to\n\t\t * half the size of the AGs in the filesystem so alignment\n\t\t * doesn't result in extents larger than an AG.\n\t\t */\n\t\tif (fa->fsx_extsize != 0) {\n\t\t\txfs_extlen_t    size;\n\t\t\txfs_fsblock_t   extsize_fsb;\n\n\t\t\textsize_fsb = XFS_B_TO_FSB(mp, fa->fsx_extsize);\n\t\t\tif (extsize_fsb > MAXEXTLEN) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\n\t\t\tif (XFS_IS_REALTIME_INODE(ip) ||\n\t\t\t    ((mask & FSX_XFLAGS) &&\n\t\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME))) {\n\t\t\t\tsize = mp->m_sb.sb_rextsize <<\n\t\t\t\t       mp->m_sb.sb_blocklog;\n\t\t\t} else {\n\t\t\t\tsize = mp->m_sb.sb_blocksize;\n\t\t\t\tif (extsize_fsb > mp->m_sb.sb_agblocks / 2) {\n\t\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\t\tgoto error_return;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (fa->fsx_extsize % size) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\t}\n\n\n\tif (mask & FSX_XFLAGS) {\n\t\t/*\n\t\t * Can't change realtime flag if any extents are allocated.\n\t\t */\n\t\tif ((ip->i_d.di_nextents || ip->i_delayed_blks) &&\n\t\t    (XFS_IS_REALTIME_INODE(ip)) !=\n\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * If realtime flag is set then must have realtime data.\n\t\t */\n\t\tif ((fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tif ((mp->m_sb.sb_rblocks == 0) ||\n\t\t\t    (mp->m_sb.sb_rextsize == 0) ||\n\t\t\t    (ip->i_d.di_extsize % mp->m_sb.sb_rextsize)) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Can't modify an immutable/append-only file unless\n\t\t * we have appropriate permission.\n\t\t */\n\t\tif ((ip->i_d.di_flags &\n\t\t\t\t(XFS_DIFLAG_IMMUTABLE|XFS_DIFLAG_APPEND) ||\n\t\t     (fa->fsx_xflags &\n\t\t\t\t(XFS_XFLAG_IMMUTABLE | XFS_XFLAG_APPEND))) &&\n\t\t    !capable(CAP_LINUX_IMMUTABLE)) {\n\t\t\tcode = XFS_ERROR(EPERM);\n\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\txfs_trans_ijoin(tp, ip, 0);\n\n\t/*\n\t * Change file ownership.  Must be the owner or privileged.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\t/*\n\t\t * CAP_FSETID overrides the following restrictions:\n\t\t *\n\t\t * The set-user-ID and set-group-ID bits of a file will be\n\t\t * cleared upon successful return from chown()\n\t\t */\n\t\tif ((ip->i_d.di_mode & (S_ISUID|S_ISGID)) &&\n\t\t    !capable_wrt_inode_uidgid(VFS_I(ip), CAP_FSETID))\n\t\t\tip->i_d.di_mode &= ~(S_ISUID|S_ISGID);\n\n\t\t/*\n\t\t * Change the ownerships and register quota modifications\n\t\t * in the transaction.\n\t\t */\n\t\tif (xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tif (XFS_IS_QUOTA_RUNNING(mp) && XFS_IS_PQUOTA_ON(mp)) {\n\t\t\t\tolddquot = xfs_qm_vop_chown(tp, ip,\n\t\t\t\t\t\t\t&ip->i_pdquot, pdqp);\n\t\t\t}\n\t\t\txfs_set_projid(ip, fa->fsx_projid);\n\n\t\t\t/*\n\t\t\t * We may have to rev the inode as well as\n\t\t\t * the superblock version number since projids didn't\n\t\t\t * exist before DINODE_VERSION_2 and SB_VERSION_NLINK.\n\t\t\t */\n\t\t\tif (ip->i_d.di_version == 1)\n\t\t\t\txfs_bump_ino_vers2(tp, ip);\n\t\t}\n\n\t}\n\n\tif (mask & FSX_EXTSIZE)\n\t\tip->i_d.di_extsize = fa->fsx_extsize >> mp->m_sb.sb_blocklog;\n\tif (mask & FSX_XFLAGS) {\n\t\txfs_set_diflags(ip, fa->fsx_xflags);\n\t\txfs_diflags_to_linux(ip);\n\t}\n\n\txfs_trans_ichgtime(tp, ip, XFS_ICHGTIME_CHG);\n\txfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);\n\n\tXFS_STATS_INC(xs_ig_attrchg);\n\n\t/*\n\t * If this is a synchronous mount, make sure that the\n\t * transaction goes to disk before returning to the user.\n\t * This is slightly sub-optimal in that truncates require\n\t * two sync transactions instead of one for wsync filesystems.\n\t * One for the truncate and one for the timestamps since we\n\t * don't want to change the timestamps unless we're sure the\n\t * truncate worked.  Truncates are less than 1% of the laddis\n\t * mix so this probably isn't worth the trouble to optimize.\n\t */\n\tif (mp->m_flags & XFS_MOUNT_WSYNC)\n\t\txfs_trans_set_sync(tp);\n\tcode = xfs_trans_commit(tp, 0);\n\txfs_iunlock(ip, lock_flags);\n\n\t/*\n\t * Release any dquot(s) the inode had kept before chown.\n\t */\n\txfs_qm_dqrele(olddquot);\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\n\treturn code;\n\n error_return:\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\txfs_trans_cancel(tp, 0);\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\n\treturn code;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 836,
            "cve_id": "CVE-2015-8709",
            "code_snippet": "struct mm_struct *mm_alloc(void)\n{\n\tstruct mm_struct *mm;\n\n\tmm = allocate_mm();\n\tif (!mm)\n\t\treturn NULL;\n\n\tmemset(mm, 0, sizeof(*mm));\n\treturn mm_init(mm, current, current_user_ns());\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static struct mm_struct *dup_mm(struct task_struct *tsk)\n{\n\tstruct mm_struct *mm, *oldmm = current->mm;\n\tint err;\n\n\tmm = allocate_mm();\n\tif (!mm)\n\t\tgoto fail_nomem;\n\n\tmemcpy(mm, oldmm, sizeof(*mm));\n\n\tif (!mm_init(mm, tsk))\n\t\tgoto fail_nomem;\n\n\terr = dup_mmap(mm, oldmm);\n\tif (err)\n\t\tgoto free_pt;\n\n\tmm->hiwater_rss = get_mm_rss(mm);\n\tmm->hiwater_vm = mm->total_vm;\n\n\tif (mm->binfmt && !try_module_get(mm->binfmt->module))\n\t\tgoto free_pt;\n\n\treturn mm;\n\nfree_pt:\n\t/* don't put binfmt in mmput, we haven't got module yet */\n\tmm->binfmt = NULL;\n\tmmput(mm);\n\nfail_nomem:\n\treturn NULL;\n}",
                        "code_after_change": "static struct mm_struct *dup_mm(struct task_struct *tsk)\n{\n\tstruct mm_struct *mm, *oldmm = current->mm;\n\tint err;\n\n\tmm = allocate_mm();\n\tif (!mm)\n\t\tgoto fail_nomem;\n\n\tmemcpy(mm, oldmm, sizeof(*mm));\n\n\tif (!mm_init(mm, tsk, mm->user_ns))\n\t\tgoto fail_nomem;\n\n\terr = dup_mmap(mm, oldmm);\n\tif (err)\n\t\tgoto free_pt;\n\n\tmm->hiwater_rss = get_mm_rss(mm);\n\tmm->hiwater_vm = mm->total_vm;\n\n\tif (mm->binfmt && !try_module_get(mm->binfmt->module))\n\t\tgoto free_pt;\n\n\treturn mm;\n\nfree_pt:\n\t/* don't put binfmt in mmput, we haven't got module yet */\n\tmm->binfmt = NULL;\n\tmmput(mm);\n\nfail_nomem:\n\treturn NULL;\n}",
                        "cve_id": "CVE-2015-8709"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstruct mm_struct *mm_alloc(void)\n{\n\tstruct mm_struct *mm;\n\n\tmm = allocate_mm();\n\tif (!mm)\n\t\treturn NULL;\n\n\tmemset(mm, 0, sizeof(*mm));\n\treturn mm_init(mm, current, current_user_ns());\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic struct mm_struct *dup_mm(struct task_struct *tsk)\n{\n\tstruct mm_struct *mm, *oldmm = current->mm;\n\tint err;\n\n\tmm = allocate_mm();\n\tif (!mm)\n\t\tgoto fail_nomem;\n\n\tmemcpy(mm, oldmm, sizeof(*mm));\n\n\tif (!mm_init(mm, tsk))\n\t\tgoto fail_nomem;\n\n\terr = dup_mmap(mm, oldmm);\n\tif (err)\n\t\tgoto free_pt;\n\n\tmm->hiwater_rss = get_mm_rss(mm);\n\tmm->hiwater_vm = mm->total_vm;\n\n\tif (mm->binfmt && !try_module_get(mm->binfmt->module))\n\t\tgoto free_pt;\n\n\treturn mm;\n\nfree_pt:\n\t/* don't put binfmt in mmput, we haven't got module yet */\n\tmm->binfmt = NULL;\n\tmmput(mm);\n\nfail_nomem:\n\treturn NULL;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic struct mm_struct *dup_mm(struct task_struct *tsk)\n{\n\tstruct mm_struct *mm, *oldmm = current->mm;\n\tint err;\n\n\tmm = allocate_mm();\n\tif (!mm)\n\t\tgoto fail_nomem;\n\n\tmemcpy(mm, oldmm, sizeof(*mm));\n\n\tif (!mm_init(mm, tsk, mm->user_ns))\n\t\tgoto fail_nomem;\n\n\terr = dup_mmap(mm, oldmm);\n\tif (err)\n\t\tgoto free_pt;\n\n\tmm->hiwater_rss = get_mm_rss(mm);\n\tmm->hiwater_vm = mm->total_vm;\n\n\tif (mm->binfmt && !try_module_get(mm->binfmt->module))\n\t\tgoto free_pt;\n\n\treturn mm;\n\nfree_pt:\n\t/* don't put binfmt in mmput, we haven't got module yet */\n\tmm->binfmt = NULL;\n\tmmput(mm);\n\nfail_nomem:\n\treturn NULL;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "void __mmdrop(struct mm_struct *mm)\n{\n\tBUG_ON(mm == &init_mm);\n\tmm_free_pgd(mm);\n\tdestroy_context(mm);\n\tmmu_notifier_mm_destroy(mm);\n\tcheck_mm(mm);\n\tfree_mm(mm);\n}",
                        "code_after_change": "void __mmdrop(struct mm_struct *mm)\n{\n\tBUG_ON(mm == &init_mm);\n\tmm_free_pgd(mm);\n\tdestroy_context(mm);\n\tmmu_notifier_mm_destroy(mm);\n\tcheck_mm(mm);\n\tput_user_ns(mm->user_ns);\n\tfree_mm(mm);\n}",
                        "cve_id": "CVE-2015-8709"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstruct mm_struct *mm_alloc(void)\n{\n\tstruct mm_struct *mm;\n\n\tmm = allocate_mm();\n\tif (!mm)\n\t\treturn NULL;\n\n\tmemset(mm, 0, sizeof(*mm));\n\treturn mm_init(mm, current, current_user_ns());\n}\n'''\nRelated vulnerability code snippet:\n'''\nvoid __mmdrop(struct mm_struct *mm)\n{\n\tBUG_ON(mm == &init_mm);\n\tmm_free_pgd(mm);\n\tdestroy_context(mm);\n\tmmu_notifier_mm_destroy(mm);\n\tcheck_mm(mm);\n\tfree_mm(mm);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nvoid __mmdrop(struct mm_struct *mm)\n{\n\tBUG_ON(mm == &init_mm);\n\tmm_free_pgd(mm);\n\tdestroy_context(mm);\n\tmmu_notifier_mm_destroy(mm);\n\tcheck_mm(mm);\n\tput_user_ns(mm->user_ns);\n\tfree_mm(mm);\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "asmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}",
                        "code_after_change": "static int __ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tconst struct cred *cred = current_cred(), *tcred;\n\tstruct mm_struct *mm;\n\tkuid_t caller_uid;\n\tkgid_t caller_gid;\n\n\tif (!(mode & PTRACE_MODE_FSCREDS) == !(mode & PTRACE_MODE_REALCREDS)) {\n\t\tWARN(1, \"denying ptrace access check without PTRACE_MODE_*CREDS\\n\");\n\t\treturn -EPERM;\n\t}\n\n\t/* May we inspect the given task?\n\t * This check is used both for attaching with ptrace\n\t * and for allowing access to sensitive information in /proc.\n\t *\n\t * ptrace_attach denies several cases that /proc allows\n\t * because setting up the necessary parent/child relationship\n\t * or halting the specified task is impossible.\n\t */\n\n\t/* Don't let security modules deny introspection */\n\tif (same_thread_group(task, current))\n\t\treturn 0;\n\trcu_read_lock();\n\tif (mode & PTRACE_MODE_FSCREDS) {\n\t\tcaller_uid = cred->fsuid;\n\t\tcaller_gid = cred->fsgid;\n\t} else {\n\t\t/*\n\t\t * Using the euid would make more sense here, but something\n\t\t * in userland might rely on the old behavior, and this\n\t\t * shouldn't be a security problem since\n\t\t * PTRACE_MODE_REALCREDS implies that the caller explicitly\n\t\t * used a syscall that requests access to another process\n\t\t * (and not a filesystem syscall to procfs).\n\t\t */\n\t\tcaller_uid = cred->uid;\n\t\tcaller_gid = cred->gid;\n\t}\n\ttcred = __task_cred(task);\n\tif (uid_eq(caller_uid, tcred->euid) &&\n\t    uid_eq(caller_uid, tcred->suid) &&\n\t    uid_eq(caller_uid, tcred->uid)  &&\n\t    gid_eq(caller_gid, tcred->egid) &&\n\t    gid_eq(caller_gid, tcred->sgid) &&\n\t    gid_eq(caller_gid, tcred->gid))\n\t\tgoto ok;\n\tif (ptrace_has_cap(tcred->user_ns, mode))\n\t\tgoto ok;\n\trcu_read_unlock();\n\treturn -EPERM;\nok:\n\trcu_read_unlock();\n\tmm = task->mm;\n\tif (mm &&\n\t    ((get_dumpable(mm) != SUID_DUMP_USER) &&\n\t     !ptrace_has_cap(mm->user_ns, mode)))\n\t    return -EPERM;\n\n\treturn security_ptrace_access_check(task, mode);\n}",
                        "cve_id": "CVE-2014-9870"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstruct mm_struct *mm_alloc(void)\n{\n\tstruct mm_struct *mm;\n\n\tmm = allocate_mm();\n\tif (!mm)\n\t\treturn NULL;\n\n\tmemset(mm, 0, sizeof(*mm));\n\treturn mm_init(mm, current, current_user_ns());\n}\n'''\nRelated vulnerability code snippet:\n'''\nasmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int __ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tconst struct cred *cred = current_cred(), *tcred;\n\tstruct mm_struct *mm;\n\tkuid_t caller_uid;\n\tkgid_t caller_gid;\n\n\tif (!(mode & PTRACE_MODE_FSCREDS) == !(mode & PTRACE_MODE_REALCREDS)) {\n\t\tWARN(1, \"denying ptrace access check without PTRACE_MODE_*CREDS\\n\");\n\t\treturn -EPERM;\n\t}\n\n\t/* May we inspect the given task?\n\t * This check is used both for attaching with ptrace\n\t * and for allowing access to sensitive information in /proc.\n\t *\n\t * ptrace_attach denies several cases that /proc allows\n\t * because setting up the necessary parent/child relationship\n\t * or halting the specified task is impossible.\n\t */\n\n\t/* Don't let security modules deny introspection */\n\tif (same_thread_group(task, current))\n\t\treturn 0;\n\trcu_read_lock();\n\tif (mode & PTRACE_MODE_FSCREDS) {\n\t\tcaller_uid = cred->fsuid;\n\t\tcaller_gid = cred->fsgid;\n\t} else {\n\t\t/*\n\t\t * Using the euid would make more sense here, but something\n\t\t * in userland might rely on the old behavior, and this\n\t\t * shouldn't be a security problem since\n\t\t * PTRACE_MODE_REALCREDS implies that the caller explicitly\n\t\t * used a syscall that requests access to another process\n\t\t * (and not a filesystem syscall to procfs).\n\t\t */\n\t\tcaller_uid = cred->uid;\n\t\tcaller_gid = cred->gid;\n\t}\n\ttcred = __task_cred(task);\n\tif (uid_eq(caller_uid, tcred->euid) &&\n\t    uid_eq(caller_uid, tcred->suid) &&\n\t    uid_eq(caller_uid, tcred->uid)  &&\n\t    gid_eq(caller_gid, tcred->egid) &&\n\t    gid_eq(caller_gid, tcred->sgid) &&\n\t    gid_eq(caller_gid, tcred->gid))\n\t\tgoto ok;\n\tif (ptrace_has_cap(tcred->user_ns, mode))\n\t\tgoto ok;\n\trcu_read_unlock();\n\treturn -EPERM;\nok:\n\trcu_read_unlock();\n\tmm = task->mm;\n\tif (mm &&\n\t    ((get_dumpable(mm) != SUID_DUMP_USER) &&\n\t     !ptrace_has_cap(mm->user_ns, mode)))\n\t    return -EPERM;\n\n\treturn security_ptrace_access_check(task, mode);\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 260,
            "cve_id": "CVE-2013-2930",
            "code_snippet": "static int perf_trace_event_perm(struct ftrace_event_call *tp_event,\n\t\t\t\t struct perf_event *p_event)\n{\n\t/* The ftrace function trace is allowed only for root. */\n\tif (ftrace_event_is_function(tp_event) &&\n\t    perf_paranoid_tracepoint_raw() && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\t/* No tracing, just counting, so no obvious leak */\n\tif (!(p_event->attr.sample_type & PERF_SAMPLE_RAW))\n\t\treturn 0;\n\n\t/* Some events are ok to be traced by non-root users... */\n\tif (p_event->attach_state == PERF_ATTACH_TASK) {\n\t\tif (tp_event->flags & TRACE_EVENT_FL_CAP_ANY)\n\t\t\treturn 0;\n\t}\n\n\t/*\n\t * ...otherwise raw tracepoint data can be a severe data leak,\n\t * only allow root to have these.\n\t */\n\tif (perf_paranoid_tracepoint_raw() && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "asmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}",
                        "code_after_change": "asmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value[0] = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}",
                        "cve_id": "CVE-2014-9870"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int perf_trace_event_perm(struct ftrace_event_call *tp_event,\n\t\t\t\t struct perf_event *p_event)\n{\n\t/* The ftrace function trace is allowed only for root. */\n\tif (ftrace_event_is_function(tp_event) &&\n\t    perf_paranoid_tracepoint_raw() && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\t/* No tracing, just counting, so no obvious leak */\n\tif (!(p_event->attr.sample_type & PERF_SAMPLE_RAW))\n\t\treturn 0;\n\n\t/* Some events are ok to be traced by non-root users... */\n\tif (p_event->attach_state == PERF_ATTACH_TASK) {\n\t\tif (tp_event->flags & TRACE_EVENT_FL_CAP_ANY)\n\t\t\treturn 0;\n\t}\n\n\t/*\n\t * ...otherwise raw tracepoint data can be a severe data leak,\n\t * only allow root to have these.\n\t */\n\tif (perf_paranoid_tracepoint_raw() && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nasmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nasmlinkage int arm_syscall(int no, struct pt_regs *regs)\n{\n\tstruct thread_info *thread = current_thread_info();\n\tsiginfo_t info;\n\n\tif ((no >> 16) != (__ARM_NR_BASE>> 16))\n\t\treturn bad_syscall(no, regs);\n\n\tswitch (no & 0xffff) {\n\tcase 0: /* branch through 0 */\n\t\tinfo.si_signo = SIGSEGV;\n\t\tinfo.si_errno = 0;\n\t\tinfo.si_code  = SEGV_MAPERR;\n\t\tinfo.si_addr  = NULL;\n\n\t\tarm_notify_die(\"branch through zero\", regs, &info, 0, 0);\n\t\treturn 0;\n\n\tcase NR(breakpoint): /* SWI BREAK_POINT */\n\t\tregs->ARM_pc -= thumb_mode(regs) ? 2 : 4;\n\t\tptrace_break(current, regs);\n\t\treturn regs->ARM_r0;\n\n\t/*\n\t * Flush a region from virtual address 'r0' to virtual address 'r1'\n\t * _exclusive_.  There is no alignment requirement on either address;\n\t * user space does not need to know the hardware cache layout.\n\t *\n\t * r2 contains flags.  It should ALWAYS be passed as ZERO until it\n\t * is defined to be something else.  For now we ignore it, but may\n\t * the fires of hell burn in your belly if you break this rule. ;)\n\t *\n\t * (at a later date, we may want to allow this call to not flush\n\t * various aspects of the cache.  Passing '0' will guarantee that\n\t * everything necessary gets flushed to maintain consistency in\n\t * the specified region).\n\t */\n\tcase NR(cacheflush):\n\t\treturn do_cache_op(regs->ARM_r0, regs->ARM_r1, regs->ARM_r2);\n\n\tcase NR(usr26):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr &= ~MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(usr32):\n\t\tif (!(elf_hwcap & HWCAP_26BIT))\n\t\t\tbreak;\n\t\tregs->ARM_cpsr |= MODE32_BIT;\n\t\treturn regs->ARM_r0;\n\n\tcase NR(set_tls):\n\t\tthread->tp_value[0] = regs->ARM_r0;\n\t\tif (tls_emu)\n\t\t\treturn 0;\n\t\tif (has_tls_reg) {\n\t\t\tasm (\"mcr p15, 0, %0, c13, c0, 3\"\n\t\t\t\t: : \"r\" (regs->ARM_r0));\n\t\t} else {\n\t\t\t/*\n\t\t\t * User space must never try to access this directly.\n\t\t\t * Expect your app to break eventually if you do so.\n\t\t\t * The user helper at 0xffff0fe0 must be used instead.\n\t\t\t * (see entry-armv.S for details)\n\t\t\t */\n\t\t\t*((unsigned int *)0xffff0ff0) = regs->ARM_r0;\n\t\t}\n\t\treturn 0;\n\n#ifdef CONFIG_NEEDS_SYSCALL_FOR_CMPXCHG\n\t/*\n\t * Atomically store r1 in *r2 if *r2 is equal to r0 for user space.\n\t * Return zero in r0 if *MEM was changed or non-zero if no exchange\n\t * happened.  Also set the user C flag accordingly.\n\t * If access permissions have to be fixed up then non-zero is\n\t * returned and the operation has to be re-attempted.\n\t *\n\t * *NOTE*: This is a ghost syscall private to the kernel.  Only the\n\t * __kuser_cmpxchg code in entry-armv.S should be aware of its\n\t * existence.  Don't ever use this from user code.\n\t */\n\tcase NR(cmpxchg):\n\tfor (;;) {\n\t\textern void do_DataAbort(unsigned long addr, unsigned int fsr,\n\t\t\t\t\t struct pt_regs *regs);\n\t\tunsigned long val;\n\t\tunsigned long addr = regs->ARM_r2;\n\t\tstruct mm_struct *mm = current->mm;\n\t\tpgd_t *pgd; pmd_t *pmd; pte_t *pte;\n\t\tspinlock_t *ptl;\n\n\t\tregs->ARM_cpsr &= ~PSR_C_BIT;\n\t\tdown_read(&mm->mmap_sem);\n\t\tpgd = pgd_offset(mm, addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tgoto bad_access;\n\t\tpmd = pmd_offset(pgd, addr);\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto bad_access;\n\t\tpte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte_present(*pte) || !pte_write(*pte) || !pte_dirty(*pte)) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tgoto bad_access;\n\t\t}\n\t\tval = *(unsigned long *)addr;\n\t\tval -= regs->ARM_r0;\n\t\tif (val == 0) {\n\t\t\t*(unsigned long *)addr = regs->ARM_r1;\n\t\t\tregs->ARM_cpsr |= PSR_C_BIT;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t\tup_read(&mm->mmap_sem);\n\t\treturn val;\n\n\t\tbad_access:\n\t\tup_read(&mm->mmap_sem);\n\t\t/* simulate a write access fault */\n\t\tdo_DataAbort(addr, 15 + (1 << 11), regs);\n\t}\n#endif\n\n\tdefault:\n\t\t/* Calls 9f00xx..9f07ff are defined to return -ENOSYS\n\t\t   if not implemented, rather than raising SIGILL.  This\n\t\t   way the calling program can gracefully determine whether\n\t\t   a feature is supported.  */\n\t\tif ((no & 0xffff) <= 0x7ff)\n\t\t\treturn -ENOSYS;\n\t\tbreak;\n\t}\n#ifdef CONFIG_DEBUG_USER\n\t/*\n\t * experience shows that these seem to indicate that\n\t * something catastrophic has happened\n\t */\n\tif (user_debug & UDBG_SYSCALL) {\n\t\tprintk(\"[%d] %s: arm syscall %d\\n\",\n\t\t       task_pid_nr(current), current->comm, no);\n\t\tdump_instr(\"\", regs);\n\t\tif (user_mode(regs)) {\n\t\t\t__show_regs(regs);\n\t\t\tc_backtrace(regs->ARM_fp, processor_mode(regs));\n\t\t}\n\t}\n#endif\n\tinfo.si_signo = SIGILL;\n\tinfo.si_errno = 0;\n\tinfo.si_code  = ILL_ILLTRP;\n\tinfo.si_addr  = (void __user *)instruction_pointer(regs) -\n\t\t\t (thumb_mode(regs) ? 2 : 4);\n\n\tarm_notify_die(\"Oops - bad syscall(2)\", regs, &info, no, 0);\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void cn_proc_mcast_ctl(struct cn_msg *msg,\n\t\t\t      struct netlink_skb_parms *nsp)\n{\n\tenum proc_cn_mcast_op *mc_op = NULL;\n\tint err = 0;\n\n\tif (msg->len != sizeof(*mc_op))\n\t\treturn;\n\n\t/* \n\t * Events are reported with respect to the initial pid\n\t * and user namespaces so ignore requestors from\n\t * other namespaces.\n\t */\n\tif ((current_user_ns() != &init_user_ns) ||\n\t    (task_active_pid_ns(current) != &init_pid_ns))\n\t\treturn;\n\n\t/* Can only change if privileged. */\n\tif (!capable(CAP_NET_ADMIN)) {\n\t\terr = EPERM;\n\t\tgoto out;\n\t}\n\n\tmc_op = (enum proc_cn_mcast_op *)msg->data;\n\tswitch (*mc_op) {\n\tcase PROC_CN_MCAST_LISTEN:\n\t\tatomic_inc(&proc_event_num_listeners);\n\t\tbreak;\n\tcase PROC_CN_MCAST_IGNORE:\n\t\tatomic_dec(&proc_event_num_listeners);\n\t\tbreak;\n\tdefault:\n\t\terr = EINVAL;\n\t\tbreak;\n\t}\n\nout:\n\tcn_proc_ack(err, msg->seq, msg->ack);\n}",
                        "code_after_change": "int create_user_ns(struct cred *new)\n{\n\tstruct user_namespace *ns, *parent_ns = new->user_ns;\n\tkuid_t owner = new->euid;\n\tkgid_t group = new->egid;\n\tint ret;\n\n\t/*\n\t * Verify that we can not violate the policy of which files\n\t * may be accessed that is specified by the root directory,\n\t * by verifing that the root directory is at the root of the\n\t * mount namespace which allows all files to be accessed.\n\t */\n\tif (current_chrooted())\n\t\treturn -EPERM;\n\n\t/* The creator needs a mapping in the parent user namespace\n\t * or else we won't be able to reasonably tell userspace who\n\t * created a user_namespace.\n\t */\n\tif (!kuid_has_mapping(parent_ns, owner) ||\n\t    !kgid_has_mapping(parent_ns, group))\n\t\treturn -EPERM;\n\n\tns = kmem_cache_zalloc(user_ns_cachep, GFP_KERNEL);\n\tif (!ns)\n\t\treturn -ENOMEM;\n\n\tret = proc_alloc_inum(&ns->proc_inum);\n\tif (ret) {\n\t\tkmem_cache_free(user_ns_cachep, ns);\n\t\treturn ret;\n\t}\n\n\tatomic_set(&ns->count, 1);\n\t/* Leave the new->user_ns reference with the new user namespace. */\n\tns->parent = parent_ns;\n\tns->owner = owner;\n\tns->group = group;\n\n\tset_cred_user_ns(new, ns);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2014-0181"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int perf_trace_event_perm(struct ftrace_event_call *tp_event,\n\t\t\t\t struct perf_event *p_event)\n{\n\t/* The ftrace function trace is allowed only for root. */\n\tif (ftrace_event_is_function(tp_event) &&\n\t    perf_paranoid_tracepoint_raw() && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\t/* No tracing, just counting, so no obvious leak */\n\tif (!(p_event->attr.sample_type & PERF_SAMPLE_RAW))\n\t\treturn 0;\n\n\t/* Some events are ok to be traced by non-root users... */\n\tif (p_event->attach_state == PERF_ATTACH_TASK) {\n\t\tif (tp_event->flags & TRACE_EVENT_FL_CAP_ANY)\n\t\t\treturn 0;\n\t}\n\n\t/*\n\t * ...otherwise raw tracepoint data can be a severe data leak,\n\t * only allow root to have these.\n\t */\n\tif (perf_paranoid_tracepoint_raw() && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic void cn_proc_mcast_ctl(struct cn_msg *msg,\n\t\t\t      struct netlink_skb_parms *nsp)\n{\n\tenum proc_cn_mcast_op *mc_op = NULL;\n\tint err = 0;\n\n\tif (msg->len != sizeof(*mc_op))\n\t\treturn;\n\n\t/* \n\t * Events are reported with respect to the initial pid\n\t * and user namespaces so ignore requestors from\n\t * other namespaces.\n\t */\n\tif ((current_user_ns() != &init_user_ns) ||\n\t    (task_active_pid_ns(current) != &init_pid_ns))\n\t\treturn;\n\n\t/* Can only change if privileged. */\n\tif (!capable(CAP_NET_ADMIN)) {\n\t\terr = EPERM;\n\t\tgoto out;\n\t}\n\n\tmc_op = (enum proc_cn_mcast_op *)msg->data;\n\tswitch (*mc_op) {\n\tcase PROC_CN_MCAST_LISTEN:\n\t\tatomic_inc(&proc_event_num_listeners);\n\t\tbreak;\n\tcase PROC_CN_MCAST_IGNORE:\n\t\tatomic_dec(&proc_event_num_listeners);\n\t\tbreak;\n\tdefault:\n\t\terr = EINVAL;\n\t\tbreak;\n\t}\n\nout:\n\tcn_proc_ack(err, msg->seq, msg->ack);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nint create_user_ns(struct cred *new)\n{\n\tstruct user_namespace *ns, *parent_ns = new->user_ns;\n\tkuid_t owner = new->euid;\n\tkgid_t group = new->egid;\n\tint ret;\n\n\t/*\n\t * Verify that we can not violate the policy of which files\n\t * may be accessed that is specified by the root directory,\n\t * by verifing that the root directory is at the root of the\n\t * mount namespace which allows all files to be accessed.\n\t */\n\tif (current_chrooted())\n\t\treturn -EPERM;\n\n\t/* The creator needs a mapping in the parent user namespace\n\t * or else we won't be able to reasonably tell userspace who\n\t * created a user_namespace.\n\t */\n\tif (!kuid_has_mapping(parent_ns, owner) ||\n\t    !kgid_has_mapping(parent_ns, group))\n\t\treturn -EPERM;\n\n\tns = kmem_cache_zalloc(user_ns_cachep, GFP_KERNEL);\n\tif (!ns)\n\t\treturn -ENOMEM;\n\n\tret = proc_alloc_inum(&ns->proc_inum);\n\tif (ret) {\n\t\tkmem_cache_free(user_ns_cachep, ns);\n\t\treturn ret;\n\t}\n\n\tatomic_set(&ns->count, 1);\n\t/* Leave the new->user_ns reference with the new user namespace. */\n\tns->parent = parent_ns;\n\tns->owner = owner;\n\tns->group = group;\n\n\tset_cred_user_ns(new, ns);\n\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static struct task_struct *copy_process(unsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace)\n{\n\tint retval;\n\tstruct task_struct *p;\n\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid namespace\n\t * don't allow the creation of threads.\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_NEWPID)) &&\n\t    (task_active_pid_ns(current) != current->nsproxy->pid_ns))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tretval = security_task_create(clone_flags);\n\tif (retval)\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current);\n\tif (!p)\n\t\tgoto fork_out;\n\n\tftrace_graph_init_task(p);\n\tget_seccomp_filter(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&\n\t\t    p->real_cred->user != INIT_USER)\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tif (!try_module_get(task_thread_info(p)->exec_domain->module))\n\t\tgoto bad_fork_cleanup_count;\n\n\tp->did_exec = 0;\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tcopy_flags(clone_flags, p);\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n\tp->utimescaled = p->stimescaled = 0;\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING\n\tp->prev_cputime.utime = p->prev_cputime.stime = 0;\n#endif\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqlock_init(&p->vtime_seqlock);\n\tp->vtime_snap = 0;\n\tp->vtime_snap_whence = VTIME_SLEEPING;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tdo_posix_clock_monotonic_gettime(&p->start_time);\n\tp->real_start_time = p->start_time;\n\tmonotonic_to_bootbased(&p->real_start_time);\n\tp->io_context = NULL;\n\tp->audit_context = NULL;\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_begin(current);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_cgroup;\n\t}\n\tmpol_fix_fork_child_flag(p);\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_init(&p->mems_allowed_seq);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n\tp->hardirqs_enabled = 0;\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_MEMCG\n\tp->memcg_batch.do_batch = 0;\n\tp->memcg_batch.memcg = NULL;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tsched_fork(p);\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\t/* copy all the process information */\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(clone_flags, stack_start, stack_size, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tif (pid != &init_struct_pid) {\n\t\tretval = -ENOMEM;\n\t\tpid = alloc_pid(p->nsproxy->pid_ns);\n\t\tif (!pid)\n\t\t\tgoto bad_fork_cleanup_io;\n\t}\n\n\tp->pid = pid_nr(pid);\n\tp->tgid = p->pid;\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->tgid = current->tgid;\n\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\tuprobe_copy_process(p);\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tp->sas_ss_sp = p->sas_ss_size = 0;\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->exit_signal = -1;\n\telse if (clone_flags & CLONE_PARENT)\n\t\tp->exit_signal = current->group_leader->exit_signal;\n\telse\n\t\tp->exit_signal = (clone_flags & CSIGNAL);\n\n\tp->pdeath_signal = 0;\n\tp->exit_state = 0;\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\t/*\n\t * Ok, make it visible to the rest of the system.\n\t * We dont wake it up yet.\n\t */\n\tp->group_leader = p;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\t/* Need tasklist lock for parent etc handling! */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Process group and session signals need to be delivered to just the\n\t * parent before the fork or both the parent and the child after the\n\t * fork. Restart if a signal comes in before we add the new process to\n\t * it's process group.\n\t * A fatal signal pending means that current will exit, so the new\n\t * thread can't slip out of an OOM kill (or normal SIGKILL).\n\t*/\n\trecalc_sigpending();\n\tif (signal_pending(current)) {\n\t\tspin_unlock(&current->sighand->siglock);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t\tretval = -ERESTARTNOINTR;\n\t\tgoto bad_fork_free_pid;\n\t}\n\n\tif (clone_flags & CLONE_THREAD) {\n\t\tcurrent->signal->nr_threads++;\n\t\tatomic_inc(&current->signal->live);\n\t\tatomic_inc(&current->signal->sigcnt);\n\t\tp->group_leader = current->group_leader;\n\t\tlist_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);\n\t}\n\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tif (thread_group_leader(p)) {\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\n\t\t\tp->signal->leader_pid = pid;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\tattach_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tattach_pid(p, PIDTYPE_SID, task_session(current));\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID, pid);\n\t\tnr_threads++;\n\t}\n\n\ttotal_forks++;\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\n\treturn p;\n\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm)\n\t\tmmput(p->mm);\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_policy:\n\tperf_event_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_cgroup:\n#endif\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tcgroup_exit(p, 0);\n\tdelayacct_tsk_free(p);\n\tmodule_put(task_thread_info(p)->exec_domain->module);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tfree_task(p);\nfork_out:\n\treturn ERR_PTR(retval);\n}",
                        "code_after_change": "static struct task_struct *copy_process(unsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace)\n{\n\tint retval;\n\tstruct task_struct *p;\n\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid namespace\n\t * don't allow the creation of threads.\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_NEWPID)) &&\n\t    (task_active_pid_ns(current) != current->nsproxy->pid_ns))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tretval = security_task_create(clone_flags);\n\tif (retval)\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current);\n\tif (!p)\n\t\tgoto fork_out;\n\n\tftrace_graph_init_task(p);\n\tget_seccomp_filter(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&\n\t\t    p->real_cred->user != INIT_USER)\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tif (!try_module_get(task_thread_info(p)->exec_domain->module))\n\t\tgoto bad_fork_cleanup_count;\n\n\tp->did_exec = 0;\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tcopy_flags(clone_flags, p);\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n\tp->utimescaled = p->stimescaled = 0;\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING\n\tp->prev_cputime.utime = p->prev_cputime.stime = 0;\n#endif\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqlock_init(&p->vtime_seqlock);\n\tp->vtime_snap = 0;\n\tp->vtime_snap_whence = VTIME_SLEEPING;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tdo_posix_clock_monotonic_gettime(&p->start_time);\n\tp->real_start_time = p->start_time;\n\tmonotonic_to_bootbased(&p->real_start_time);\n\tp->io_context = NULL;\n\tp->audit_context = NULL;\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_begin(current);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_cgroup;\n\t}\n\tmpol_fix_fork_child_flag(p);\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_init(&p->mems_allowed_seq);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n\tp->hardirqs_enabled = 0;\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_MEMCG\n\tp->memcg_batch.do_batch = 0;\n\tp->memcg_batch.memcg = NULL;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tsched_fork(p);\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\t/* copy all the process information */\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(clone_flags, stack_start, stack_size, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tif (pid != &init_struct_pid) {\n\t\tretval = -ENOMEM;\n\t\tpid = alloc_pid(p->nsproxy->pid_ns);\n\t\tif (!pid)\n\t\t\tgoto bad_fork_cleanup_io;\n\t}\n\n\tp->pid = pid_nr(pid);\n\tp->tgid = p->pid;\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->tgid = current->tgid;\n\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\tuprobe_copy_process(p);\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tp->sas_ss_sp = p->sas_ss_size = 0;\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->exit_signal = -1;\n\telse if (clone_flags & CLONE_PARENT)\n\t\tp->exit_signal = current->group_leader->exit_signal;\n\telse\n\t\tp->exit_signal = (clone_flags & CSIGNAL);\n\n\tp->pdeath_signal = 0;\n\tp->exit_state = 0;\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\t/*\n\t * Ok, make it visible to the rest of the system.\n\t * We dont wake it up yet.\n\t */\n\tp->group_leader = p;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\t/* Need tasklist lock for parent etc handling! */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Process group and session signals need to be delivered to just the\n\t * parent before the fork or both the parent and the child after the\n\t * fork. Restart if a signal comes in before we add the new process to\n\t * it's process group.\n\t * A fatal signal pending means that current will exit, so the new\n\t * thread can't slip out of an OOM kill (or normal SIGKILL).\n\t*/\n\trecalc_sigpending();\n\tif (signal_pending(current)) {\n\t\tspin_unlock(&current->sighand->siglock);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t\tretval = -ERESTARTNOINTR;\n\t\tgoto bad_fork_free_pid;\n\t}\n\n\tif (clone_flags & CLONE_THREAD) {\n\t\tcurrent->signal->nr_threads++;\n\t\tatomic_inc(&current->signal->live);\n\t\tatomic_inc(&current->signal->sigcnt);\n\t\tp->group_leader = current->group_leader;\n\t\tlist_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);\n\t}\n\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tif (thread_group_leader(p)) {\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\n\t\t\tp->signal->leader_pid = pid;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\tattach_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tattach_pid(p, PIDTYPE_SID, task_session(current));\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID, pid);\n\t\tnr_threads++;\n\t}\n\n\ttotal_forks++;\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\n\treturn p;\n\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm)\n\t\tmmput(p->mm);\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_policy:\n\tperf_event_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_cgroup:\n#endif\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tcgroup_exit(p, 0);\n\tdelayacct_tsk_free(p);\n\tmodule_put(task_thread_info(p)->exec_domain->module);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tfree_task(p);\nfork_out:\n\treturn ERR_PTR(retval);\n}",
                        "cve_id": "CVE-2013-1858"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int perf_trace_event_perm(struct ftrace_event_call *tp_event,\n\t\t\t\t struct perf_event *p_event)\n{\n\t/* The ftrace function trace is allowed only for root. */\n\tif (ftrace_event_is_function(tp_event) &&\n\t    perf_paranoid_tracepoint_raw() && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\t/* No tracing, just counting, so no obvious leak */\n\tif (!(p_event->attr.sample_type & PERF_SAMPLE_RAW))\n\t\treturn 0;\n\n\t/* Some events are ok to be traced by non-root users... */\n\tif (p_event->attach_state == PERF_ATTACH_TASK) {\n\t\tif (tp_event->flags & TRACE_EVENT_FL_CAP_ANY)\n\t\t\treturn 0;\n\t}\n\n\t/*\n\t * ...otherwise raw tracepoint data can be a severe data leak,\n\t * only allow root to have these.\n\t */\n\tif (perf_paranoid_tracepoint_raw() && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic struct task_struct *copy_process(unsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace)\n{\n\tint retval;\n\tstruct task_struct *p;\n\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid namespace\n\t * don't allow the creation of threads.\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_NEWPID)) &&\n\t    (task_active_pid_ns(current) != current->nsproxy->pid_ns))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tretval = security_task_create(clone_flags);\n\tif (retval)\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current);\n\tif (!p)\n\t\tgoto fork_out;\n\n\tftrace_graph_init_task(p);\n\tget_seccomp_filter(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&\n\t\t    p->real_cred->user != INIT_USER)\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tif (!try_module_get(task_thread_info(p)->exec_domain->module))\n\t\tgoto bad_fork_cleanup_count;\n\n\tp->did_exec = 0;\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tcopy_flags(clone_flags, p);\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n\tp->utimescaled = p->stimescaled = 0;\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING\n\tp->prev_cputime.utime = p->prev_cputime.stime = 0;\n#endif\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqlock_init(&p->vtime_seqlock);\n\tp->vtime_snap = 0;\n\tp->vtime_snap_whence = VTIME_SLEEPING;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tdo_posix_clock_monotonic_gettime(&p->start_time);\n\tp->real_start_time = p->start_time;\n\tmonotonic_to_bootbased(&p->real_start_time);\n\tp->io_context = NULL;\n\tp->audit_context = NULL;\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_begin(current);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_cgroup;\n\t}\n\tmpol_fix_fork_child_flag(p);\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_init(&p->mems_allowed_seq);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n\tp->hardirqs_enabled = 0;\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_MEMCG\n\tp->memcg_batch.do_batch = 0;\n\tp->memcg_batch.memcg = NULL;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tsched_fork(p);\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\t/* copy all the process information */\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(clone_flags, stack_start, stack_size, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tif (pid != &init_struct_pid) {\n\t\tretval = -ENOMEM;\n\t\tpid = alloc_pid(p->nsproxy->pid_ns);\n\t\tif (!pid)\n\t\t\tgoto bad_fork_cleanup_io;\n\t}\n\n\tp->pid = pid_nr(pid);\n\tp->tgid = p->pid;\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->tgid = current->tgid;\n\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\tuprobe_copy_process(p);\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tp->sas_ss_sp = p->sas_ss_size = 0;\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->exit_signal = -1;\n\telse if (clone_flags & CLONE_PARENT)\n\t\tp->exit_signal = current->group_leader->exit_signal;\n\telse\n\t\tp->exit_signal = (clone_flags & CSIGNAL);\n\n\tp->pdeath_signal = 0;\n\tp->exit_state = 0;\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\t/*\n\t * Ok, make it visible to the rest of the system.\n\t * We dont wake it up yet.\n\t */\n\tp->group_leader = p;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\t/* Need tasklist lock for parent etc handling! */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Process group and session signals need to be delivered to just the\n\t * parent before the fork or both the parent and the child after the\n\t * fork. Restart if a signal comes in before we add the new process to\n\t * it's process group.\n\t * A fatal signal pending means that current will exit, so the new\n\t * thread can't slip out of an OOM kill (or normal SIGKILL).\n\t*/\n\trecalc_sigpending();\n\tif (signal_pending(current)) {\n\t\tspin_unlock(&current->sighand->siglock);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t\tretval = -ERESTARTNOINTR;\n\t\tgoto bad_fork_free_pid;\n\t}\n\n\tif (clone_flags & CLONE_THREAD) {\n\t\tcurrent->signal->nr_threads++;\n\t\tatomic_inc(&current->signal->live);\n\t\tatomic_inc(&current->signal->sigcnt);\n\t\tp->group_leader = current->group_leader;\n\t\tlist_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);\n\t}\n\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tif (thread_group_leader(p)) {\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\n\t\t\tp->signal->leader_pid = pid;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\tattach_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tattach_pid(p, PIDTYPE_SID, task_session(current));\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID, pid);\n\t\tnr_threads++;\n\t}\n\n\ttotal_forks++;\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\n\treturn p;\n\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm)\n\t\tmmput(p->mm);\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_policy:\n\tperf_event_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_cgroup:\n#endif\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tcgroup_exit(p, 0);\n\tdelayacct_tsk_free(p);\n\tmodule_put(task_thread_info(p)->exec_domain->module);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tfree_task(p);\nfork_out:\n\treturn ERR_PTR(retval);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic struct task_struct *copy_process(unsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace)\n{\n\tint retval;\n\tstruct task_struct *p;\n\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid namespace\n\t * don't allow the creation of threads.\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_NEWPID)) &&\n\t    (task_active_pid_ns(current) != current->nsproxy->pid_ns))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tretval = security_task_create(clone_flags);\n\tif (retval)\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current);\n\tif (!p)\n\t\tgoto fork_out;\n\n\tftrace_graph_init_task(p);\n\tget_seccomp_filter(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&\n\t\t    p->real_cred->user != INIT_USER)\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tif (!try_module_get(task_thread_info(p)->exec_domain->module))\n\t\tgoto bad_fork_cleanup_count;\n\n\tp->did_exec = 0;\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tcopy_flags(clone_flags, p);\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n\tp->utimescaled = p->stimescaled = 0;\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING\n\tp->prev_cputime.utime = p->prev_cputime.stime = 0;\n#endif\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqlock_init(&p->vtime_seqlock);\n\tp->vtime_snap = 0;\n\tp->vtime_snap_whence = VTIME_SLEEPING;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tdo_posix_clock_monotonic_gettime(&p->start_time);\n\tp->real_start_time = p->start_time;\n\tmonotonic_to_bootbased(&p->real_start_time);\n\tp->io_context = NULL;\n\tp->audit_context = NULL;\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_begin(current);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_cgroup;\n\t}\n\tmpol_fix_fork_child_flag(p);\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_init(&p->mems_allowed_seq);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n\tp->hardirqs_enabled = 0;\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_MEMCG\n\tp->memcg_batch.do_batch = 0;\n\tp->memcg_batch.memcg = NULL;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tsched_fork(p);\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\t/* copy all the process information */\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(clone_flags, stack_start, stack_size, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tif (pid != &init_struct_pid) {\n\t\tretval = -ENOMEM;\n\t\tpid = alloc_pid(p->nsproxy->pid_ns);\n\t\tif (!pid)\n\t\t\tgoto bad_fork_cleanup_io;\n\t}\n\n\tp->pid = pid_nr(pid);\n\tp->tgid = p->pid;\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->tgid = current->tgid;\n\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\tuprobe_copy_process(p);\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tp->sas_ss_sp = p->sas_ss_size = 0;\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->exit_signal = -1;\n\telse if (clone_flags & CLONE_PARENT)\n\t\tp->exit_signal = current->group_leader->exit_signal;\n\telse\n\t\tp->exit_signal = (clone_flags & CSIGNAL);\n\n\tp->pdeath_signal = 0;\n\tp->exit_state = 0;\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\t/*\n\t * Ok, make it visible to the rest of the system.\n\t * We dont wake it up yet.\n\t */\n\tp->group_leader = p;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\t/* Need tasklist lock for parent etc handling! */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Process group and session signals need to be delivered to just the\n\t * parent before the fork or both the parent and the child after the\n\t * fork. Restart if a signal comes in before we add the new process to\n\t * it's process group.\n\t * A fatal signal pending means that current will exit, so the new\n\t * thread can't slip out of an OOM kill (or normal SIGKILL).\n\t*/\n\trecalc_sigpending();\n\tif (signal_pending(current)) {\n\t\tspin_unlock(&current->sighand->siglock);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t\tretval = -ERESTARTNOINTR;\n\t\tgoto bad_fork_free_pid;\n\t}\n\n\tif (clone_flags & CLONE_THREAD) {\n\t\tcurrent->signal->nr_threads++;\n\t\tatomic_inc(&current->signal->live);\n\t\tatomic_inc(&current->signal->sigcnt);\n\t\tp->group_leader = current->group_leader;\n\t\tlist_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);\n\t}\n\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tif (thread_group_leader(p)) {\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\n\t\t\tp->signal->leader_pid = pid;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\tattach_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tattach_pid(p, PIDTYPE_SID, task_session(current));\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID, pid);\n\t\tnr_threads++;\n\t}\n\n\ttotal_forks++;\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\n\treturn p;\n\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm)\n\t\tmmput(p->mm);\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_policy:\n\tperf_event_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_cgroup:\n#endif\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tcgroup_exit(p, 0);\n\tdelayacct_tsk_free(p);\n\tmodule_put(task_thread_info(p)->exec_domain->module);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tfree_task(p);\nfork_out:\n\treturn ERR_PTR(retval);\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 1041,
            "cve_id": "CVE-2016-4997",
            "code_snippet": "static int\ncheck_entry_size_and_hooks(struct ipt_entry *e,\n\t\t\t   struct xt_table_info *newinfo,\n\t\t\t   const unsigned char *base,\n\t\t\t   const unsigned char *limit,\n\t\t\t   const unsigned int *hook_entries,\n\t\t\t   const unsigned int *underflows,\n\t\t\t   unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct ipt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,\n\t\t\t\t     e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int\ncheck_entry_size_and_hooks(struct ip6t_entry *e,\n\t\t\t   struct xt_table_info *newinfo,\n\t\t\t   const unsigned char *base,\n\t\t\t   const unsigned char *limit,\n\t\t\t   const unsigned int *hook_entries,\n\t\t\t   const unsigned int *underflows,\n\t\t\t   unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct ip6t_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct ip6t_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct ip6t_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip6_checkentry(&e->ipv6))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}",
                        "code_after_change": "static int\ncheck_entry_size_and_hooks(struct ip6t_entry *e,\n\t\t\t   struct xt_table_info *newinfo,\n\t\t\t   const unsigned char *base,\n\t\t\t   const unsigned char *limit,\n\t\t\t   const unsigned int *hook_entries,\n\t\t\t   const unsigned int *underflows,\n\t\t\t   unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct ip6t_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct ip6t_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct ip6t_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip6_checkentry(&e->ipv6))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,\n\t\t\t\t     e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}",
                        "cve_id": "CVE-2016-4997"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int\ncheck_entry_size_and_hooks(struct ipt_entry *e,\n\t\t\t   struct xt_table_info *newinfo,\n\t\t\t   const unsigned char *base,\n\t\t\t   const unsigned char *limit,\n\t\t\t   const unsigned int *hook_entries,\n\t\t\t   const unsigned int *underflows,\n\t\t\t   unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct ipt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,\n\t\t\t\t     e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int\ncheck_entry_size_and_hooks(struct ip6t_entry *e,\n\t\t\t   struct xt_table_info *newinfo,\n\t\t\t   const unsigned char *base,\n\t\t\t   const unsigned char *limit,\n\t\t\t   const unsigned int *hook_entries,\n\t\t\t   const unsigned int *underflows,\n\t\t\t   unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct ip6t_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct ip6t_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct ip6t_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip6_checkentry(&e->ipv6))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int\ncheck_entry_size_and_hooks(struct ip6t_entry *e,\n\t\t\t   struct xt_table_info *newinfo,\n\t\t\t   const unsigned char *base,\n\t\t\t   const unsigned char *limit,\n\t\t\t   const unsigned int *hook_entries,\n\t\t\t   const unsigned int *underflows,\n\t\t\t   unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct ip6t_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct ip6t_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct ip6t_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip6_checkentry(&e->ipv6))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,\n\t\t\t\t     e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static inline int check_entry_size_and_hooks(struct arpt_entry *e,\n\t\t\t\t\t     struct xt_table_info *newinfo,\n\t\t\t\t\t     const unsigned char *base,\n\t\t\t\t\t     const unsigned char *limit,\n\t\t\t\t\t     const unsigned int *hook_entries,\n\t\t\t\t\t     const unsigned int *underflows,\n\t\t\t\t\t     unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct arpt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}",
                        "code_after_change": "static inline int check_entry_size_and_hooks(struct arpt_entry *e,\n\t\t\t\t\t     struct xt_table_info *newinfo,\n\t\t\t\t\t     const unsigned char *base,\n\t\t\t\t\t     const unsigned char *limit,\n\t\t\t\t\t     const unsigned int *hook_entries,\n\t\t\t\t\t     const unsigned int *underflows,\n\t\t\t\t\t     unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct arpt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,\n\t\t\t\t     e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}",
                        "cve_id": "CVE-2016-4997"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int\ncheck_entry_size_and_hooks(struct ipt_entry *e,\n\t\t\t   struct xt_table_info *newinfo,\n\t\t\t   const unsigned char *base,\n\t\t\t   const unsigned char *limit,\n\t\t\t   const unsigned int *hook_entries,\n\t\t\t   const unsigned int *underflows,\n\t\t\t   unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct ipt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,\n\t\t\t\t     e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic inline int check_entry_size_and_hooks(struct arpt_entry *e,\n\t\t\t\t\t     struct xt_table_info *newinfo,\n\t\t\t\t\t     const unsigned char *base,\n\t\t\t\t\t     const unsigned char *limit,\n\t\t\t\t\t     const unsigned int *hook_entries,\n\t\t\t\t\t     const unsigned int *underflows,\n\t\t\t\t\t     unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct arpt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic inline int check_entry_size_and_hooks(struct arpt_entry *e,\n\t\t\t\t\t     struct xt_table_info *newinfo,\n\t\t\t\t\t     const unsigned char *base,\n\t\t\t\t\t     const unsigned char *limit,\n\t\t\t\t\t     const unsigned int *hook_entries,\n\t\t\t\t\t     const unsigned int *underflows,\n\t\t\t\t\t     unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct arpt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,\n\t\t\t\t     e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int\ncheck_compat_entry_size_and_hooks(struct compat_ipt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ipt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ipt_entry) - sizeof(struct compat_ipt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ip, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ipt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV4, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}",
                        "code_after_change": "static int\ncheck_compat_entry_size_and_hooks(struct compat_ipt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ipt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e, e->elems,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ipt_entry) - sizeof(struct compat_ipt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ip, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ipt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV4, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}",
                        "cve_id": "CVE-2016-4997"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int\ncheck_entry_size_and_hooks(struct ipt_entry *e,\n\t\t\t   struct xt_table_info *newinfo,\n\t\t\t   const unsigned char *base,\n\t\t\t   const unsigned char *limit,\n\t\t\t   const unsigned int *hook_entries,\n\t\t\t   const unsigned int *underflows,\n\t\t\t   unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct ipt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,\n\t\t\t\t     e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int\ncheck_compat_entry_size_and_hooks(struct compat_ipt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ipt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ipt_entry) - sizeof(struct compat_ipt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ip, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ipt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV4, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int\ncheck_compat_entry_size_and_hooks(struct compat_ipt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ipt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e, e->elems,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ipt_entry) - sizeof(struct compat_ipt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ip, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ipt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV4, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 1154,
            "cve_id": "CVE-2016-9644",
            "code_snippet": "static void math_error(struct pt_regs *regs, int error_code, int trapnr)\n{\n\tstruct task_struct *task = current;\n\tstruct fpu *fpu = &task->thread.fpu;\n\tsiginfo_t info;\n\tchar *str = (trapnr == X86_TRAP_MF) ? \"fpu exception\" :\n\t\t\t\t\t\t\"simd exception\";\n\n\tif (notify_die(DIE_TRAP, str, regs, error_code, trapnr, SIGFPE) == NOTIFY_STOP)\n\t\treturn;\n\tconditional_sti(regs);\n\n\tif (!user_mode(regs)) {\n\t\tif (!fixup_exception(regs, trapnr)) {\n\t\t\ttask->thread.error_code = error_code;\n\t\t\ttask->thread.trap_nr = trapnr;\n\t\t\tdie(str, regs, error_code);\n\t\t}\n\t\treturn;\n\t}\n\n\t/*\n\t * Save the info for the exception handler and clear the error.\n\t */\n\tfpu__save(fpu);\n\n\ttask->thread.trap_nr\t= trapnr;\n\ttask->thread.error_code = error_code;\n\tinfo.si_signo\t\t= SIGFPE;\n\tinfo.si_errno\t\t= 0;\n\tinfo.si_addr\t\t= (void __user *)uprobe_get_trap_addr(regs);\n\n\tinfo.si_code = fpu__exception_code(fpu, trapnr);\n\n\t/* Retry when we get spurious exceptions: */\n\tif (!info.si_code)\n\t\treturn;\n\n\tforce_sig_info(SIGFPE, &info, task);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static nokprobe_inline int\ndo_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,\n\t\t  struct pt_regs *regs,\tlong error_code)\n{\n\tif (v8086_mode(regs)) {\n\t\t/*\n\t\t * Traps 0, 1, 3, 4, and 5 should be forwarded to vm86.\n\t\t * On nmi (interrupt 2), do_trap should not be called.\n\t\t */\n\t\tif (trapnr < X86_TRAP_UD) {\n\t\t\tif (!handle_vm86_trap((struct kernel_vm86_regs *) regs,\n\t\t\t\t\t\terror_code, trapnr))\n\t\t\t\treturn 0;\n\t\t}\n\t\treturn -1;\n\t}\n\n\tif (!user_mode(regs)) {\n\t\tif (!fixup_exception(regs)) {\n\t\t\ttsk->thread.error_code = error_code;\n\t\t\ttsk->thread.trap_nr = trapnr;\n\t\t\tdie(str, regs, error_code);\n\t\t}\n\t\treturn 0;\n\t}\n\n\treturn -1;\n}",
                        "code_after_change": "static nokprobe_inline int\ndo_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,\n\t\t  struct pt_regs *regs,\tlong error_code)\n{\n\tif (v8086_mode(regs)) {\n\t\t/*\n\t\t * Traps 0, 1, 3, 4, and 5 should be forwarded to vm86.\n\t\t * On nmi (interrupt 2), do_trap should not be called.\n\t\t */\n\t\tif (trapnr < X86_TRAP_UD) {\n\t\t\tif (!handle_vm86_trap((struct kernel_vm86_regs *) regs,\n\t\t\t\t\t\terror_code, trapnr))\n\t\t\t\treturn 0;\n\t\t}\n\t\treturn -1;\n\t}\n\n\tif (!user_mode(regs)) {\n\t\tif (!fixup_exception(regs, trapnr)) {\n\t\t\ttsk->thread.error_code = error_code;\n\t\t\ttsk->thread.trap_nr = trapnr;\n\t\t\tdie(str, regs, error_code);\n\t\t}\n\t\treturn 0;\n\t}\n\n\treturn -1;\n}",
                        "cve_id": "CVE-2016-9644"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic void math_error(struct pt_regs *regs, int error_code, int trapnr)\n{\n\tstruct task_struct *task = current;\n\tstruct fpu *fpu = &task->thread.fpu;\n\tsiginfo_t info;\n\tchar *str = (trapnr == X86_TRAP_MF) ? \"fpu exception\" :\n\t\t\t\t\t\t\"simd exception\";\n\n\tif (notify_die(DIE_TRAP, str, regs, error_code, trapnr, SIGFPE) == NOTIFY_STOP)\n\t\treturn;\n\tconditional_sti(regs);\n\n\tif (!user_mode(regs)) {\n\t\tif (!fixup_exception(regs, trapnr)) {\n\t\t\ttask->thread.error_code = error_code;\n\t\t\ttask->thread.trap_nr = trapnr;\n\t\t\tdie(str, regs, error_code);\n\t\t}\n\t\treturn;\n\t}\n\n\t/*\n\t * Save the info for the exception handler and clear the error.\n\t */\n\tfpu__save(fpu);\n\n\ttask->thread.trap_nr\t= trapnr;\n\ttask->thread.error_code = error_code;\n\tinfo.si_signo\t\t= SIGFPE;\n\tinfo.si_errno\t\t= 0;\n\tinfo.si_addr\t\t= (void __user *)uprobe_get_trap_addr(regs);\n\n\tinfo.si_code = fpu__exception_code(fpu, trapnr);\n\n\t/* Retry when we get spurious exceptions: */\n\tif (!info.si_code)\n\t\treturn;\n\n\tforce_sig_info(SIGFPE, &info, task);\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic nokprobe_inline int\ndo_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,\n\t\t  struct pt_regs *regs,\tlong error_code)\n{\n\tif (v8086_mode(regs)) {\n\t\t/*\n\t\t * Traps 0, 1, 3, 4, and 5 should be forwarded to vm86.\n\t\t * On nmi (interrupt 2), do_trap should not be called.\n\t\t */\n\t\tif (trapnr < X86_TRAP_UD) {\n\t\t\tif (!handle_vm86_trap((struct kernel_vm86_regs *) regs,\n\t\t\t\t\t\terror_code, trapnr))\n\t\t\t\treturn 0;\n\t\t}\n\t\treturn -1;\n\t}\n\n\tif (!user_mode(regs)) {\n\t\tif (!fixup_exception(regs)) {\n\t\t\ttsk->thread.error_code = error_code;\n\t\t\ttsk->thread.trap_nr = trapnr;\n\t\t\tdie(str, regs, error_code);\n\t\t}\n\t\treturn 0;\n\t}\n\n\treturn -1;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic nokprobe_inline int\ndo_trap_no_signal(struct task_struct *tsk, int trapnr, char *str,\n\t\t  struct pt_regs *regs,\tlong error_code)\n{\n\tif (v8086_mode(regs)) {\n\t\t/*\n\t\t * Traps 0, 1, 3, 4, and 5 should be forwarded to vm86.\n\t\t * On nmi (interrupt 2), do_trap should not be called.\n\t\t */\n\t\tif (trapnr < X86_TRAP_UD) {\n\t\t\tif (!handle_vm86_trap((struct kernel_vm86_regs *) regs,\n\t\t\t\t\t\terror_code, trapnr))\n\t\t\t\treturn 0;\n\t\t}\n\t\treturn -1;\n\t}\n\n\tif (!user_mode(regs)) {\n\t\tif (!fixup_exception(regs, trapnr)) {\n\t\t\ttsk->thread.error_code = error_code;\n\t\t\ttsk->thread.trap_nr = trapnr;\n\t\t\tdie(str, regs, error_code);\n\t\t}\n\t\treturn 0;\n\t}\n\n\treturn -1;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "dotraplinkage void\ndo_general_protection(struct pt_regs *regs, long error_code)\n{\n\tstruct task_struct *tsk;\n\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(), \"entry code didn't wake RCU\");\n\tconditional_sti(regs);\n\n\tif (v8086_mode(regs)) {\n\t\tlocal_irq_enable();\n\t\thandle_vm86_fault((struct kernel_vm86_regs *) regs, error_code);\n\t\treturn;\n\t}\n\n\ttsk = current;\n\tif (!user_mode(regs)) {\n\t\tif (fixup_exception(regs))\n\t\t\treturn;\n\n\t\ttsk->thread.error_code = error_code;\n\t\ttsk->thread.trap_nr = X86_TRAP_GP;\n\t\tif (notify_die(DIE_GPF, \"general protection fault\", regs, error_code,\n\t\t\t       X86_TRAP_GP, SIGSEGV) != NOTIFY_STOP)\n\t\t\tdie(\"general protection fault\", regs, error_code);\n\t\treturn;\n\t}\n\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_nr = X86_TRAP_GP;\n\n\tif (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&\n\t\t\tprintk_ratelimit()) {\n\t\tpr_info(\"%s[%d] general protection ip:%lx sp:%lx error:%lx\",\n\t\t\ttsk->comm, task_pid_nr(tsk),\n\t\t\tregs->ip, regs->sp, error_code);\n\t\tprint_vma_addr(\" in \", regs->ip);\n\t\tpr_cont(\"\\n\");\n\t}\n\n\tforce_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);\n}",
                        "code_after_change": "dotraplinkage void\ndo_general_protection(struct pt_regs *regs, long error_code)\n{\n\tstruct task_struct *tsk;\n\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(), \"entry code didn't wake RCU\");\n\tconditional_sti(regs);\n\n\tif (v8086_mode(regs)) {\n\t\tlocal_irq_enable();\n\t\thandle_vm86_fault((struct kernel_vm86_regs *) regs, error_code);\n\t\treturn;\n\t}\n\n\ttsk = current;\n\tif (!user_mode(regs)) {\n\t\tif (fixup_exception(regs, X86_TRAP_GP))\n\t\t\treturn;\n\n\t\ttsk->thread.error_code = error_code;\n\t\ttsk->thread.trap_nr = X86_TRAP_GP;\n\t\tif (notify_die(DIE_GPF, \"general protection fault\", regs, error_code,\n\t\t\t       X86_TRAP_GP, SIGSEGV) != NOTIFY_STOP)\n\t\t\tdie(\"general protection fault\", regs, error_code);\n\t\treturn;\n\t}\n\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_nr = X86_TRAP_GP;\n\n\tif (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&\n\t\t\tprintk_ratelimit()) {\n\t\tpr_info(\"%s[%d] general protection ip:%lx sp:%lx error:%lx\",\n\t\t\ttsk->comm, task_pid_nr(tsk),\n\t\t\tregs->ip, regs->sp, error_code);\n\t\tprint_vma_addr(\" in \", regs->ip);\n\t\tpr_cont(\"\\n\");\n\t}\n\n\tforce_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);\n}",
                        "cve_id": "CVE-2016-9644"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic void math_error(struct pt_regs *regs, int error_code, int trapnr)\n{\n\tstruct task_struct *task = current;\n\tstruct fpu *fpu = &task->thread.fpu;\n\tsiginfo_t info;\n\tchar *str = (trapnr == X86_TRAP_MF) ? \"fpu exception\" :\n\t\t\t\t\t\t\"simd exception\";\n\n\tif (notify_die(DIE_TRAP, str, regs, error_code, trapnr, SIGFPE) == NOTIFY_STOP)\n\t\treturn;\n\tconditional_sti(regs);\n\n\tif (!user_mode(regs)) {\n\t\tif (!fixup_exception(regs, trapnr)) {\n\t\t\ttask->thread.error_code = error_code;\n\t\t\ttask->thread.trap_nr = trapnr;\n\t\t\tdie(str, regs, error_code);\n\t\t}\n\t\treturn;\n\t}\n\n\t/*\n\t * Save the info for the exception handler and clear the error.\n\t */\n\tfpu__save(fpu);\n\n\ttask->thread.trap_nr\t= trapnr;\n\ttask->thread.error_code = error_code;\n\tinfo.si_signo\t\t= SIGFPE;\n\tinfo.si_errno\t\t= 0;\n\tinfo.si_addr\t\t= (void __user *)uprobe_get_trap_addr(regs);\n\n\tinfo.si_code = fpu__exception_code(fpu, trapnr);\n\n\t/* Retry when we get spurious exceptions: */\n\tif (!info.si_code)\n\t\treturn;\n\n\tforce_sig_info(SIGFPE, &info, task);\n}\n'''\nRelated vulnerability code snippet:\n'''\ndotraplinkage void\ndo_general_protection(struct pt_regs *regs, long error_code)\n{\n\tstruct task_struct *tsk;\n\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(), \"entry code didn't wake RCU\");\n\tconditional_sti(regs);\n\n\tif (v8086_mode(regs)) {\n\t\tlocal_irq_enable();\n\t\thandle_vm86_fault((struct kernel_vm86_regs *) regs, error_code);\n\t\treturn;\n\t}\n\n\ttsk = current;\n\tif (!user_mode(regs)) {\n\t\tif (fixup_exception(regs))\n\t\t\treturn;\n\n\t\ttsk->thread.error_code = error_code;\n\t\ttsk->thread.trap_nr = X86_TRAP_GP;\n\t\tif (notify_die(DIE_GPF, \"general protection fault\", regs, error_code,\n\t\t\t       X86_TRAP_GP, SIGSEGV) != NOTIFY_STOP)\n\t\t\tdie(\"general protection fault\", regs, error_code);\n\t\treturn;\n\t}\n\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_nr = X86_TRAP_GP;\n\n\tif (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&\n\t\t\tprintk_ratelimit()) {\n\t\tpr_info(\"%s[%d] general protection ip:%lx sp:%lx error:%lx\",\n\t\t\ttsk->comm, task_pid_nr(tsk),\n\t\t\tregs->ip, regs->sp, error_code);\n\t\tprint_vma_addr(\" in \", regs->ip);\n\t\tpr_cont(\"\\n\");\n\t}\n\n\tforce_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\ndotraplinkage void\ndo_general_protection(struct pt_regs *regs, long error_code)\n{\n\tstruct task_struct *tsk;\n\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(), \"entry code didn't wake RCU\");\n\tconditional_sti(regs);\n\n\tif (v8086_mode(regs)) {\n\t\tlocal_irq_enable();\n\t\thandle_vm86_fault((struct kernel_vm86_regs *) regs, error_code);\n\t\treturn;\n\t}\n\n\ttsk = current;\n\tif (!user_mode(regs)) {\n\t\tif (fixup_exception(regs, X86_TRAP_GP))\n\t\t\treturn;\n\n\t\ttsk->thread.error_code = error_code;\n\t\ttsk->thread.trap_nr = X86_TRAP_GP;\n\t\tif (notify_die(DIE_GPF, \"general protection fault\", regs, error_code,\n\t\t\t       X86_TRAP_GP, SIGSEGV) != NOTIFY_STOP)\n\t\t\tdie(\"general protection fault\", regs, error_code);\n\t\treturn;\n\t}\n\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_nr = X86_TRAP_GP;\n\n\tif (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&\n\t\t\tprintk_ratelimit()) {\n\t\tpr_info(\"%s[%d] general protection ip:%lx sp:%lx error:%lx\",\n\t\t\ttsk->comm, task_pid_nr(tsk),\n\t\t\tregs->ip, regs->sp, error_code);\n\t\tprint_vma_addr(\" in \", regs->ip);\n\t\tpr_cont(\"\\n\");\n\t}\n\n\tforce_sig_info(SIGSEGV, SEND_SIG_PRIV, tsk);\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static noinline void\nno_context(struct pt_regs *regs, unsigned long error_code,\n\t   unsigned long address, int signal, int si_code)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long flags;\n\tint sig;\n\n\t/* Are we prepared to handle this kernel fault? */\n\tif (fixup_exception(regs)) {\n\t\t/*\n\t\t * Any interrupt that takes a fault gets the fixup. This makes\n\t\t * the below recursive fault logic only apply to a faults from\n\t\t * task context.\n\t\t */\n\t\tif (in_interrupt())\n\t\t\treturn;\n\n\t\t/*\n\t\t * Per the above we're !in_interrupt(), aka. task context.\n\t\t *\n\t\t * In this case we need to make sure we're not recursively\n\t\t * faulting through the emulate_vsyscall() logic.\n\t\t */\n\t\tif (current_thread_info()->sig_on_uaccess_error && signal) {\n\t\t\ttsk->thread.trap_nr = X86_TRAP_PF;\n\t\t\ttsk->thread.error_code = error_code | PF_USER;\n\t\t\ttsk->thread.cr2 = address;\n\n\t\t\t/* XXX: hwpoison faults will set the wrong code. */\n\t\t\tforce_sig_info_fault(signal, si_code, address, tsk, 0);\n\t\t}\n\n\t\t/*\n\t\t * Barring that, we can do the fixup and be happy.\n\t\t */\n\t\treturn;\n\t}\n\n\t/*\n\t * 32-bit:\n\t *\n\t *   Valid to do another page fault here, because if this fault\n\t *   had been triggered by is_prefetch fixup_exception would have\n\t *   handled it.\n\t *\n\t * 64-bit:\n\t *\n\t *   Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n\t/*\n\t * Oops. The kernel tried to access some bad page. We'll have to\n\t * terminate things with extreme prejudice:\n\t */\n\tflags = oops_begin();\n\n\tshow_fault_oops(regs, error_code, address);\n\n\tif (task_stack_end_corrupted(tsk))\n\t\tprintk(KERN_EMERG \"Thread overran stack, or stack corrupted\\n\");\n\n\ttsk->thread.cr2\t\t= address;\n\ttsk->thread.trap_nr\t= X86_TRAP_PF;\n\ttsk->thread.error_code\t= error_code;\n\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_DEFAULT \"CR2: %016lx\\n\", address);\n\n\toops_end(flags, regs, sig);\n}",
                        "code_after_change": "static noinline void\nno_context(struct pt_regs *regs, unsigned long error_code,\n\t   unsigned long address, int signal, int si_code)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long flags;\n\tint sig;\n\n\t/* Are we prepared to handle this kernel fault? */\n\tif (fixup_exception(regs, X86_TRAP_PF)) {\n\t\t/*\n\t\t * Any interrupt that takes a fault gets the fixup. This makes\n\t\t * the below recursive fault logic only apply to a faults from\n\t\t * task context.\n\t\t */\n\t\tif (in_interrupt())\n\t\t\treturn;\n\n\t\t/*\n\t\t * Per the above we're !in_interrupt(), aka. task context.\n\t\t *\n\t\t * In this case we need to make sure we're not recursively\n\t\t * faulting through the emulate_vsyscall() logic.\n\t\t */\n\t\tif (current_thread_info()->sig_on_uaccess_error && signal) {\n\t\t\ttsk->thread.trap_nr = X86_TRAP_PF;\n\t\t\ttsk->thread.error_code = error_code | PF_USER;\n\t\t\ttsk->thread.cr2 = address;\n\n\t\t\t/* XXX: hwpoison faults will set the wrong code. */\n\t\t\tforce_sig_info_fault(signal, si_code, address, tsk, 0);\n\t\t}\n\n\t\t/*\n\t\t * Barring that, we can do the fixup and be happy.\n\t\t */\n\t\treturn;\n\t}\n\n\t/*\n\t * 32-bit:\n\t *\n\t *   Valid to do another page fault here, because if this fault\n\t *   had been triggered by is_prefetch fixup_exception would have\n\t *   handled it.\n\t *\n\t * 64-bit:\n\t *\n\t *   Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n\t/*\n\t * Oops. The kernel tried to access some bad page. We'll have to\n\t * terminate things with extreme prejudice:\n\t */\n\tflags = oops_begin();\n\n\tshow_fault_oops(regs, error_code, address);\n\n\tif (task_stack_end_corrupted(tsk))\n\t\tprintk(KERN_EMERG \"Thread overran stack, or stack corrupted\\n\");\n\n\ttsk->thread.cr2\t\t= address;\n\ttsk->thread.trap_nr\t= X86_TRAP_PF;\n\ttsk->thread.error_code\t= error_code;\n\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_DEFAULT \"CR2: %016lx\\n\", address);\n\n\toops_end(flags, regs, sig);\n}",
                        "cve_id": "CVE-2016-9644"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic void math_error(struct pt_regs *regs, int error_code, int trapnr)\n{\n\tstruct task_struct *task = current;\n\tstruct fpu *fpu = &task->thread.fpu;\n\tsiginfo_t info;\n\tchar *str = (trapnr == X86_TRAP_MF) ? \"fpu exception\" :\n\t\t\t\t\t\t\"simd exception\";\n\n\tif (notify_die(DIE_TRAP, str, regs, error_code, trapnr, SIGFPE) == NOTIFY_STOP)\n\t\treturn;\n\tconditional_sti(regs);\n\n\tif (!user_mode(regs)) {\n\t\tif (!fixup_exception(regs, trapnr)) {\n\t\t\ttask->thread.error_code = error_code;\n\t\t\ttask->thread.trap_nr = trapnr;\n\t\t\tdie(str, regs, error_code);\n\t\t}\n\t\treturn;\n\t}\n\n\t/*\n\t * Save the info for the exception handler and clear the error.\n\t */\n\tfpu__save(fpu);\n\n\ttask->thread.trap_nr\t= trapnr;\n\ttask->thread.error_code = error_code;\n\tinfo.si_signo\t\t= SIGFPE;\n\tinfo.si_errno\t\t= 0;\n\tinfo.si_addr\t\t= (void __user *)uprobe_get_trap_addr(regs);\n\n\tinfo.si_code = fpu__exception_code(fpu, trapnr);\n\n\t/* Retry when we get spurious exceptions: */\n\tif (!info.si_code)\n\t\treturn;\n\n\tforce_sig_info(SIGFPE, &info, task);\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic noinline void\nno_context(struct pt_regs *regs, unsigned long error_code,\n\t   unsigned long address, int signal, int si_code)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long flags;\n\tint sig;\n\n\t/* Are we prepared to handle this kernel fault? */\n\tif (fixup_exception(regs)) {\n\t\t/*\n\t\t * Any interrupt that takes a fault gets the fixup. This makes\n\t\t * the below recursive fault logic only apply to a faults from\n\t\t * task context.\n\t\t */\n\t\tif (in_interrupt())\n\t\t\treturn;\n\n\t\t/*\n\t\t * Per the above we're !in_interrupt(), aka. task context.\n\t\t *\n\t\t * In this case we need to make sure we're not recursively\n\t\t * faulting through the emulate_vsyscall() logic.\n\t\t */\n\t\tif (current_thread_info()->sig_on_uaccess_error && signal) {\n\t\t\ttsk->thread.trap_nr = X86_TRAP_PF;\n\t\t\ttsk->thread.error_code = error_code | PF_USER;\n\t\t\ttsk->thread.cr2 = address;\n\n\t\t\t/* XXX: hwpoison faults will set the wrong code. */\n\t\t\tforce_sig_info_fault(signal, si_code, address, tsk, 0);\n\t\t}\n\n\t\t/*\n\t\t * Barring that, we can do the fixup and be happy.\n\t\t */\n\t\treturn;\n\t}\n\n\t/*\n\t * 32-bit:\n\t *\n\t *   Valid to do another page fault here, because if this fault\n\t *   had been triggered by is_prefetch fixup_exception would have\n\t *   handled it.\n\t *\n\t * 64-bit:\n\t *\n\t *   Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n\t/*\n\t * Oops. The kernel tried to access some bad page. We'll have to\n\t * terminate things with extreme prejudice:\n\t */\n\tflags = oops_begin();\n\n\tshow_fault_oops(regs, error_code, address);\n\n\tif (task_stack_end_corrupted(tsk))\n\t\tprintk(KERN_EMERG \"Thread overran stack, or stack corrupted\\n\");\n\n\ttsk->thread.cr2\t\t= address;\n\ttsk->thread.trap_nr\t= X86_TRAP_PF;\n\ttsk->thread.error_code\t= error_code;\n\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_DEFAULT \"CR2: %016lx\\n\", address);\n\n\toops_end(flags, regs, sig);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic noinline void\nno_context(struct pt_regs *regs, unsigned long error_code,\n\t   unsigned long address, int signal, int si_code)\n{\n\tstruct task_struct *tsk = current;\n\tunsigned long flags;\n\tint sig;\n\n\t/* Are we prepared to handle this kernel fault? */\n\tif (fixup_exception(regs, X86_TRAP_PF)) {\n\t\t/*\n\t\t * Any interrupt that takes a fault gets the fixup. This makes\n\t\t * the below recursive fault logic only apply to a faults from\n\t\t * task context.\n\t\t */\n\t\tif (in_interrupt())\n\t\t\treturn;\n\n\t\t/*\n\t\t * Per the above we're !in_interrupt(), aka. task context.\n\t\t *\n\t\t * In this case we need to make sure we're not recursively\n\t\t * faulting through the emulate_vsyscall() logic.\n\t\t */\n\t\tif (current_thread_info()->sig_on_uaccess_error && signal) {\n\t\t\ttsk->thread.trap_nr = X86_TRAP_PF;\n\t\t\ttsk->thread.error_code = error_code | PF_USER;\n\t\t\ttsk->thread.cr2 = address;\n\n\t\t\t/* XXX: hwpoison faults will set the wrong code. */\n\t\t\tforce_sig_info_fault(signal, si_code, address, tsk, 0);\n\t\t}\n\n\t\t/*\n\t\t * Barring that, we can do the fixup and be happy.\n\t\t */\n\t\treturn;\n\t}\n\n\t/*\n\t * 32-bit:\n\t *\n\t *   Valid to do another page fault here, because if this fault\n\t *   had been triggered by is_prefetch fixup_exception would have\n\t *   handled it.\n\t *\n\t * 64-bit:\n\t *\n\t *   Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n\t/*\n\t * Oops. The kernel tried to access some bad page. We'll have to\n\t * terminate things with extreme prejudice:\n\t */\n\tflags = oops_begin();\n\n\tshow_fault_oops(regs, error_code, address);\n\n\tif (task_stack_end_corrupted(tsk))\n\t\tprintk(KERN_EMERG \"Thread overran stack, or stack corrupted\\n\");\n\n\ttsk->thread.cr2\t\t= address;\n\ttsk->thread.trap_nr\t= X86_TRAP_PF;\n\ttsk->thread.error_code\t= error_code;\n\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_DEFAULT \"CR2: %016lx\\n\", address);\n\n\toops_end(flags, regs, sig);\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 1039,
            "cve_id": "CVE-2016-4997",
            "code_snippet": "static inline int\ncheck_compat_entry_size_and_hooks(struct compat_arpt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_arpt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e, e->elems, e->target_offset,\n\t\t\t\t\t    e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct arpt_entry) - sizeof(struct compat_arpt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\n\tt = compat_arpt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_ARP, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto out;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(NFPROTO_ARP, entry_offset, off);\n\tif (ret)\n\t\tgoto release_target;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nrelease_target:\n\tmodule_put(t->u.kernel.target->me);\nout:\n\treturn ret;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int\ncheck_compat_entry_size_and_hooks(struct compat_ip6t_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ip6t_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ip6t_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ip6t_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip6_checkentry(&e->ipv6))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ip6t_entry) - sizeof(struct compat_ip6t_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ipv6, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ip6t_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV6, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET6, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}",
                        "code_after_change": "static int\ncheck_compat_entry_size_and_hooks(struct compat_ip6t_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ip6t_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ip6t_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ip6t_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip6_checkentry(&e->ipv6))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e, e->elems,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ip6t_entry) - sizeof(struct compat_ip6t_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ipv6, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ip6t_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV6, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET6, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}",
                        "cve_id": "CVE-2016-4997"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic inline int\ncheck_compat_entry_size_and_hooks(struct compat_arpt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_arpt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e, e->elems, e->target_offset,\n\t\t\t\t\t    e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct arpt_entry) - sizeof(struct compat_arpt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\n\tt = compat_arpt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_ARP, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto out;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(NFPROTO_ARP, entry_offset, off);\n\tif (ret)\n\t\tgoto release_target;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nrelease_target:\n\tmodule_put(t->u.kernel.target->me);\nout:\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int\ncheck_compat_entry_size_and_hooks(struct compat_ip6t_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ip6t_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ip6t_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ip6t_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip6_checkentry(&e->ipv6))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ip6t_entry) - sizeof(struct compat_ip6t_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ipv6, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ip6t_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV6, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET6, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int\ncheck_compat_entry_size_and_hooks(struct compat_ip6t_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ip6t_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ip6t_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ip6t_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip6_checkentry(&e->ipv6))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e, e->elems,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ip6t_entry) - sizeof(struct compat_ip6t_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ipv6, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ip6t_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV6, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET6, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int\ncheck_compat_entry_size_and_hooks(struct compat_ipt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ipt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ipt_entry) - sizeof(struct compat_ipt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ip, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ipt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV4, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}",
                        "code_after_change": "static int\ncheck_compat_entry_size_and_hooks(struct compat_ipt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ipt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e, e->elems,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ipt_entry) - sizeof(struct compat_ipt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ip, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ipt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV4, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}",
                        "cve_id": "CVE-2016-4997"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic inline int\ncheck_compat_entry_size_and_hooks(struct compat_arpt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_arpt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e, e->elems, e->target_offset,\n\t\t\t\t\t    e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct arpt_entry) - sizeof(struct compat_arpt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\n\tt = compat_arpt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_ARP, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto out;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(NFPROTO_ARP, entry_offset, off);\n\tif (ret)\n\t\tgoto release_target;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nrelease_target:\n\tmodule_put(t->u.kernel.target->me);\nout:\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int\ncheck_compat_entry_size_and_hooks(struct compat_ipt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ipt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ipt_entry) - sizeof(struct compat_ipt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ip, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ipt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV4, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int\ncheck_compat_entry_size_and_hooks(struct compat_ipt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_match *ematch;\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tunsigned int j;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_ipt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_ipt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_ipt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ip_checkentry(&e->ip))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e, e->elems,\n\t\t\t\t\t    e->target_offset, e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct ipt_entry) - sizeof(struct compat_ipt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\tj = 0;\n\txt_ematch_foreach(ematch, e) {\n\t\tret = compat_find_calc_match(ematch, name, &e->ip, &off);\n\t\tif (ret != 0)\n\t\t\tgoto release_matches;\n\t\t++j;\n\t}\n\n\tt = compat_ipt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_IPV4, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto release_matches;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(AF_INET, entry_offset, off);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_INET_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nout:\n\tmodule_put(t->u.kernel.target->me);\nrelease_matches:\n\txt_ematch_foreach(ematch, e) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tmodule_put(ematch->u.kernel.match->me);\n\t}\n\treturn ret;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static inline int check_entry_size_and_hooks(struct arpt_entry *e,\n\t\t\t\t\t     struct xt_table_info *newinfo,\n\t\t\t\t\t     const unsigned char *base,\n\t\t\t\t\t     const unsigned char *limit,\n\t\t\t\t\t     const unsigned int *hook_entries,\n\t\t\t\t\t     const unsigned int *underflows,\n\t\t\t\t\t     unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct arpt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}",
                        "code_after_change": "static inline int check_entry_size_and_hooks(struct arpt_entry *e,\n\t\t\t\t\t     struct xt_table_info *newinfo,\n\t\t\t\t\t     const unsigned char *base,\n\t\t\t\t\t     const unsigned char *limit,\n\t\t\t\t\t     const unsigned int *hook_entries,\n\t\t\t\t\t     const unsigned int *underflows,\n\t\t\t\t\t     unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct arpt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,\n\t\t\t\t     e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}",
                        "cve_id": "CVE-2016-4997"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic inline int\ncheck_compat_entry_size_and_hooks(struct compat_arpt_entry *e,\n\t\t\t\t  struct xt_table_info *newinfo,\n\t\t\t\t  unsigned int *size,\n\t\t\t\t  const unsigned char *base,\n\t\t\t\t  const unsigned char *limit,\n\t\t\t\t  const unsigned int *hook_entries,\n\t\t\t\t  const unsigned int *underflows,\n\t\t\t\t  const char *name)\n{\n\tstruct xt_entry_target *t;\n\tstruct xt_target *target;\n\tunsigned int entry_offset;\n\tint ret, off, h;\n\n\tduprintf(\"check_compat_entry_size_and_hooks %p\\n\", e);\n\tif ((unsigned long)e % __alignof__(struct compat_arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct compat_arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p, limit = %p\\n\", e, limit);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset < sizeof(struct compat_arpt_entry) +\n\t\t\t     sizeof(struct compat_xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\tret = xt_compat_check_entry_offsets(e, e->elems, e->target_offset,\n\t\t\t\t\t    e->next_offset);\n\tif (ret)\n\t\treturn ret;\n\n\toff = sizeof(struct arpt_entry) - sizeof(struct compat_arpt_entry);\n\tentry_offset = (void *)e - (void *)base;\n\n\tt = compat_arpt_get_target(e);\n\ttarget = xt_request_find_target(NFPROTO_ARP, t->u.user.name,\n\t\t\t\t\tt->u.user.revision);\n\tif (IS_ERR(target)) {\n\t\tduprintf(\"check_compat_entry_size_and_hooks: `%s' not found\\n\",\n\t\t\t t->u.user.name);\n\t\tret = PTR_ERR(target);\n\t\tgoto out;\n\t}\n\tt->u.kernel.target = target;\n\n\toff += xt_compat_target_offset(target);\n\t*size += off;\n\tret = xt_compat_add_offset(NFPROTO_ARP, entry_offset, off);\n\tif (ret)\n\t\tgoto release_target;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h])\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t}\n\n\t/* Clear counters and comefrom */\n\tmemset(&e->counters, 0, sizeof(e->counters));\n\te->comefrom = 0;\n\treturn 0;\n\nrelease_target:\n\tmodule_put(t->u.kernel.target->me);\nout:\n\treturn ret;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic inline int check_entry_size_and_hooks(struct arpt_entry *e,\n\t\t\t\t\t     struct xt_table_info *newinfo,\n\t\t\t\t\t     const unsigned char *base,\n\t\t\t\t\t     const unsigned char *limit,\n\t\t\t\t\t     const unsigned int *hook_entries,\n\t\t\t\t\t     const unsigned int *underflows,\n\t\t\t\t\t     unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct arpt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->target_offset, e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic inline int check_entry_size_and_hooks(struct arpt_entry *e,\n\t\t\t\t\t     struct xt_table_info *newinfo,\n\t\t\t\t\t     const unsigned char *base,\n\t\t\t\t\t     const unsigned char *limit,\n\t\t\t\t\t     const unsigned int *hook_entries,\n\t\t\t\t\t     const unsigned int *underflows,\n\t\t\t\t\t     unsigned int valid_hooks)\n{\n\tunsigned int h;\n\tint err;\n\n\tif ((unsigned long)e % __alignof__(struct arpt_entry) != 0 ||\n\t    (unsigned char *)e + sizeof(struct arpt_entry) >= limit ||\n\t    (unsigned char *)e + e->next_offset > limit) {\n\t\tduprintf(\"Bad offset %p\\n\", e);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e->next_offset\n\t    < sizeof(struct arpt_entry) + sizeof(struct xt_entry_target)) {\n\t\tduprintf(\"checking: element %p size %u\\n\",\n\t\t\t e, e->next_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!arp_checkentry(&e->arp))\n\t\treturn -EINVAL;\n\n\terr = xt_check_entry_offsets(e, e->elems, e->target_offset,\n\t\t\t\t     e->next_offset);\n\tif (err)\n\t\treturn err;\n\n\t/* Check hooks & underflows */\n\tfor (h = 0; h < NF_ARP_NUMHOOKS; h++) {\n\t\tif (!(valid_hooks & (1 << h)))\n\t\t\tcontinue;\n\t\tif ((unsigned char *)e - base == hook_entries[h])\n\t\t\tnewinfo->hook_entry[h] = hook_entries[h];\n\t\tif ((unsigned char *)e - base == underflows[h]) {\n\t\t\tif (!check_underflow(e)) {\n\t\t\t\tpr_debug(\"Underflows must be unconditional and \"\n\t\t\t\t\t \"use the STANDARD target with \"\n\t\t\t\t\t \"ACCEPT/DROP\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tnewinfo->underflow[h] = underflows[h];\n\t\t}\n\t}\n\n\t/* Clear counters and comefrom */\n\te->counters = ((struct xt_counters) { 0, 0 });\n\te->comefrom = 0;\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 351,
            "cve_id": "CVE-2013-6383",
            "code_snippet": "static int aac_compat_ioctl(struct scsi_device *sdev, int cmd, void __user *arg)\n{\n\tstruct aac_dev *dev = (struct aac_dev *)sdev->host->hostdata;\n\tif (!capable(CAP_SYS_RAWIO))\n\t\treturn -EPERM;\n\treturn aac_compat_do_ioctl(dev, cmd, (unsigned long)arg);\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tion_free(client, handle);\n\t\tion_handle_put(handle);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}",
                        "code_after_change": "static long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\tmutex_lock(&client->lock);\n\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n\t\tif (IS_ERR(handle)) {\n\t\t\tmutex_unlock(&client->lock);\n\t\t\treturn PTR_ERR(handle);\n\t\t}\n\t\tion_free_nolock(client, handle);\n\t\tion_handle_put_nolock(handle);\n\t\tmutex_unlock(&client->lock);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}",
                        "cve_id": "CVE-2016-9120"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int aac_compat_ioctl(struct scsi_device *sdev, int cmd, void __user *arg)\n{\n\tstruct aac_dev *dev = (struct aac_dev *)sdev->host->hostdata;\n\tif (!capable(CAP_SYS_RAWIO))\n\t\treturn -EPERM;\n\treturn aac_compat_do_ioctl(dev, cmd, (unsigned long)arg);\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tion_free(client, handle);\n\t\tion_handle_put(handle);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic long ion_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct ion_client *client = filp->private_data;\n\tstruct ion_device *dev = client->dev;\n\tstruct ion_handle *cleanup_handle = NULL;\n\tint ret = 0;\n\tunsigned int dir;\n\n\tunion {\n\t\tstruct ion_fd_data fd;\n\t\tstruct ion_allocation_data allocation;\n\t\tstruct ion_handle_data handle;\n\t\tstruct ion_custom_data custom;\n\t} data;\n\n\tdir = ion_ioctl_dir(cmd);\n\n\tif (_IOC_SIZE(cmd) > sizeof(data))\n\t\treturn -EINVAL;\n\n\tif (dir & _IOC_WRITE)\n\t\tif (copy_from_user(&data, (void __user *)arg, _IOC_SIZE(cmd)))\n\t\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase ION_IOC_ALLOC:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_alloc(client, data.allocation.len,\n\t\t\t\t\t\tdata.allocation.align,\n\t\t\t\t\t\tdata.allocation.heap_id_mask,\n\t\t\t\t\t\tdata.allocation.flags);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\n\t\tdata.allocation.handle = handle->id;\n\n\t\tcleanup_handle = handle;\n\t\tbreak;\n\t}\n\tcase ION_IOC_FREE:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\tmutex_lock(&client->lock);\n\t\thandle = ion_handle_get_by_id_nolock(client, data.handle.handle);\n\t\tif (IS_ERR(handle)) {\n\t\t\tmutex_unlock(&client->lock);\n\t\t\treturn PTR_ERR(handle);\n\t\t}\n\t\tion_free_nolock(client, handle);\n\t\tion_handle_put_nolock(handle);\n\t\tmutex_unlock(&client->lock);\n\t\tbreak;\n\t}\n\tcase ION_IOC_SHARE:\n\tcase ION_IOC_MAP:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_handle_get_by_id(client, data.handle.handle);\n\t\tif (IS_ERR(handle))\n\t\t\treturn PTR_ERR(handle);\n\t\tdata.fd.fd = ion_share_dma_buf_fd(client, handle);\n\t\tion_handle_put(handle);\n\t\tif (data.fd.fd < 0)\n\t\t\tret = data.fd.fd;\n\t\tbreak;\n\t}\n\tcase ION_IOC_IMPORT:\n\t{\n\t\tstruct ion_handle *handle;\n\n\t\thandle = ion_import_dma_buf_fd(client, data.fd.fd);\n\t\tif (IS_ERR(handle))\n\t\t\tret = PTR_ERR(handle);\n\t\telse\n\t\t\tdata.handle.handle = handle->id;\n\t\tbreak;\n\t}\n\tcase ION_IOC_SYNC:\n\t{\n\t\tret = ion_sync_for_device(client, data.fd.fd);\n\t\tbreak;\n\t}\n\tcase ION_IOC_CUSTOM:\n\t{\n\t\tif (!dev->custom_ioctl)\n\t\t\treturn -ENOTTY;\n\t\tret = dev->custom_ioctl(client, data.custom.cmd,\n\t\t\t\t\t\tdata.custom.arg);\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\tif (dir & _IOC_READ) {\n\t\tif (copy_to_user((void __user *)arg, &data, _IOC_SIZE(cmd))) {\n\t\t\tif (cleanup_handle)\n\t\t\t\tion_free(client, cleanup_handle);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn ret;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "asmlinkage long sys_oabi_fcntl64(unsigned int fd, unsigned int cmd,\n\t\t\t\t unsigned long arg)\n{\n\tstruct oabi_flock64 user;\n\tstruct flock64 kernel;\n\tmm_segment_t fs = USER_DS; /* initialized to kill a warning */\n\tunsigned long local_arg = arg;\n\tint ret;\n\n\tswitch (cmd) {\n\tcase F_OFD_GETLK:\n\tcase F_OFD_SETLK:\n\tcase F_OFD_SETLKW:\n\tcase F_GETLK64:\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\tif (copy_from_user(&user, (struct oabi_flock64 __user *)arg,\n\t\t\t\t   sizeof(user)))\n\t\t\treturn -EFAULT;\n\t\tkernel.l_type\t= user.l_type;\n\t\tkernel.l_whence\t= user.l_whence;\n\t\tkernel.l_start\t= user.l_start;\n\t\tkernel.l_len\t= user.l_len;\n\t\tkernel.l_pid\t= user.l_pid;\n\t\tlocal_arg = (unsigned long)&kernel;\n\t\tfs = get_fs();\n\t\tset_fs(KERNEL_DS);\n\t}\n\n\tret = sys_fcntl64(fd, cmd, local_arg);\n\n\tswitch (cmd) {\n\tcase F_GETLK64:\n\t\tif (!ret) {\n\t\t\tuser.l_type\t= kernel.l_type;\n\t\t\tuser.l_whence\t= kernel.l_whence;\n\t\t\tuser.l_start\t= kernel.l_start;\n\t\t\tuser.l_len\t= kernel.l_len;\n\t\t\tuser.l_pid\t= kernel.l_pid;\n\t\t\tif (copy_to_user((struct oabi_flock64 __user *)arg,\n\t\t\t\t\t &user, sizeof(user)))\n\t\t\t\tret = -EFAULT;\n\t\t}\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\tset_fs(fs);\n\t}\n\n\treturn ret;\n}",
                        "code_after_change": "asmlinkage long sys_oabi_fcntl64(unsigned int fd, unsigned int cmd,\n\t\t\t\t unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase F_OFD_GETLK:\n\tcase F_OFD_SETLK:\n\tcase F_OFD_SETLKW:\n\tcase F_GETLK64:\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\treturn do_locks(fd, cmd, arg);\n\n\tdefault:\n\t\treturn sys_fcntl64(fd, cmd, arg);\n\t}\n}",
                        "cve_id": "CVE-2015-8966"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int aac_compat_ioctl(struct scsi_device *sdev, int cmd, void __user *arg)\n{\n\tstruct aac_dev *dev = (struct aac_dev *)sdev->host->hostdata;\n\tif (!capable(CAP_SYS_RAWIO))\n\t\treturn -EPERM;\n\treturn aac_compat_do_ioctl(dev, cmd, (unsigned long)arg);\n}\n'''\nRelated vulnerability code snippet:\n'''\nasmlinkage long sys_oabi_fcntl64(unsigned int fd, unsigned int cmd,\n\t\t\t\t unsigned long arg)\n{\n\tstruct oabi_flock64 user;\n\tstruct flock64 kernel;\n\tmm_segment_t fs = USER_DS; /* initialized to kill a warning */\n\tunsigned long local_arg = arg;\n\tint ret;\n\n\tswitch (cmd) {\n\tcase F_OFD_GETLK:\n\tcase F_OFD_SETLK:\n\tcase F_OFD_SETLKW:\n\tcase F_GETLK64:\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\tif (copy_from_user(&user, (struct oabi_flock64 __user *)arg,\n\t\t\t\t   sizeof(user)))\n\t\t\treturn -EFAULT;\n\t\tkernel.l_type\t= user.l_type;\n\t\tkernel.l_whence\t= user.l_whence;\n\t\tkernel.l_start\t= user.l_start;\n\t\tkernel.l_len\t= user.l_len;\n\t\tkernel.l_pid\t= user.l_pid;\n\t\tlocal_arg = (unsigned long)&kernel;\n\t\tfs = get_fs();\n\t\tset_fs(KERNEL_DS);\n\t}\n\n\tret = sys_fcntl64(fd, cmd, local_arg);\n\n\tswitch (cmd) {\n\tcase F_GETLK64:\n\t\tif (!ret) {\n\t\t\tuser.l_type\t= kernel.l_type;\n\t\t\tuser.l_whence\t= kernel.l_whence;\n\t\t\tuser.l_start\t= kernel.l_start;\n\t\t\tuser.l_len\t= kernel.l_len;\n\t\t\tuser.l_pid\t= kernel.l_pid;\n\t\t\tif (copy_to_user((struct oabi_flock64 __user *)arg,\n\t\t\t\t\t &user, sizeof(user)))\n\t\t\t\tret = -EFAULT;\n\t\t}\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\tset_fs(fs);\n\t}\n\n\treturn ret;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nasmlinkage long sys_oabi_fcntl64(unsigned int fd, unsigned int cmd,\n\t\t\t\t unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase F_OFD_GETLK:\n\tcase F_OFD_SETLK:\n\tcase F_OFD_SETLKW:\n\tcase F_GETLK64:\n\tcase F_SETLK64:\n\tcase F_SETLKW64:\n\t\treturn do_locks(fd, cmd, arg);\n\n\tdefault:\n\t\treturn sys_fcntl64(fd, cmd, arg);\n\t}\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int\nrio_ioctl (struct net_device *dev, struct ifreq *rq, int cmd)\n{\n\tint phy_addr;\n\tstruct netdev_private *np = netdev_priv(dev);\n\tstruct mii_data *miidata = (struct mii_data *) &rq->ifr_ifru;\n\n\tstruct netdev_desc *desc;\n\tint i;\n\n\tphy_addr = np->phy_addr;\n\tswitch (cmd) {\n\tcase SIOCDEVPRIVATE:\n\t\tbreak;\n\n\tcase SIOCDEVPRIVATE + 1:\n\t\tmiidata->out_value = mii_read (dev, phy_addr, miidata->reg_num);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 2:\n\t\tmii_write (dev, phy_addr, miidata->reg_num, miidata->in_value);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 3:\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 4:\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 5:\n\t\tnetif_stop_queue (dev);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 6:\n\t\tnetif_wake_queue (dev);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 7:\n\t\tprintk\n\t\t    (\"tx_full=%x cur_tx=%lx old_tx=%lx cur_rx=%lx old_rx=%lx\\n\",\n\t\t     netif_queue_stopped(dev), np->cur_tx, np->old_tx, np->cur_rx,\n\t\t     np->old_rx);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 8:\n\t\tprintk(\"TX ring:\\n\");\n\t\tfor (i = 0; i < TX_RING_SIZE; i++) {\n\t\t\tdesc = &np->tx_ring[i];\n\t\t\tprintk\n\t\t\t    (\"%02x:cur:%08x next:%08x status:%08x frag1:%08x frag0:%08x\",\n\t\t\t     i,\n\t\t\t     (u32) (np->tx_ring_dma + i * sizeof (*desc)),\n\t\t\t     (u32)le64_to_cpu(desc->next_desc),\n\t\t\t     (u32)le64_to_cpu(desc->status),\n\t\t\t     (u32)(le64_to_cpu(desc->fraginfo) >> 32),\n\t\t\t     (u32)le64_to_cpu(desc->fraginfo));\n\t\t\tprintk (\"\\n\");\n\t\t}\n\t\tprintk (\"\\n\");\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\treturn 0;\n}",
                        "code_after_change": "static int\nrio_ioctl (struct net_device *dev, struct ifreq *rq, int cmd)\n{\n\tint phy_addr;\n\tstruct netdev_private *np = netdev_priv(dev);\n\tstruct mii_ioctl_data *miidata = if_mii(rq);\n\n\tphy_addr = np->phy_addr;\n\tswitch (cmd) {\n\tcase SIOCGMIIPHY:\n\t\tmiidata->phy_id = phy_addr;\n\t\tbreak;\n\tcase SIOCGMIIREG:\n\t\tmiidata->val_out = mii_read (dev, phy_addr, miidata->reg_num);\n\t\tbreak;\n\tcase SIOCSMIIREG:\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\treturn -EPERM;\n\t\tmii_write (dev, phy_addr, miidata->reg_num, miidata->val_in);\n\t\tbreak;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\treturn 0;\n}",
                        "cve_id": "CVE-2012-2313"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int aac_compat_ioctl(struct scsi_device *sdev, int cmd, void __user *arg)\n{\n\tstruct aac_dev *dev = (struct aac_dev *)sdev->host->hostdata;\n\tif (!capable(CAP_SYS_RAWIO))\n\t\treturn -EPERM;\n\treturn aac_compat_do_ioctl(dev, cmd, (unsigned long)arg);\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int\nrio_ioctl (struct net_device *dev, struct ifreq *rq, int cmd)\n{\n\tint phy_addr;\n\tstruct netdev_private *np = netdev_priv(dev);\n\tstruct mii_data *miidata = (struct mii_data *) &rq->ifr_ifru;\n\n\tstruct netdev_desc *desc;\n\tint i;\n\n\tphy_addr = np->phy_addr;\n\tswitch (cmd) {\n\tcase SIOCDEVPRIVATE:\n\t\tbreak;\n\n\tcase SIOCDEVPRIVATE + 1:\n\t\tmiidata->out_value = mii_read (dev, phy_addr, miidata->reg_num);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 2:\n\t\tmii_write (dev, phy_addr, miidata->reg_num, miidata->in_value);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 3:\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 4:\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 5:\n\t\tnetif_stop_queue (dev);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 6:\n\t\tnetif_wake_queue (dev);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 7:\n\t\tprintk\n\t\t    (\"tx_full=%x cur_tx=%lx old_tx=%lx cur_rx=%lx old_rx=%lx\\n\",\n\t\t     netif_queue_stopped(dev), np->cur_tx, np->old_tx, np->cur_rx,\n\t\t     np->old_rx);\n\t\tbreak;\n\tcase SIOCDEVPRIVATE + 8:\n\t\tprintk(\"TX ring:\\n\");\n\t\tfor (i = 0; i < TX_RING_SIZE; i++) {\n\t\t\tdesc = &np->tx_ring[i];\n\t\t\tprintk\n\t\t\t    (\"%02x:cur:%08x next:%08x status:%08x frag1:%08x frag0:%08x\",\n\t\t\t     i,\n\t\t\t     (u32) (np->tx_ring_dma + i * sizeof (*desc)),\n\t\t\t     (u32)le64_to_cpu(desc->next_desc),\n\t\t\t     (u32)le64_to_cpu(desc->status),\n\t\t\t     (u32)(le64_to_cpu(desc->fraginfo) >> 32),\n\t\t\t     (u32)le64_to_cpu(desc->fraginfo));\n\t\t\tprintk (\"\\n\");\n\t\t}\n\t\tprintk (\"\\n\");\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int\nrio_ioctl (struct net_device *dev, struct ifreq *rq, int cmd)\n{\n\tint phy_addr;\n\tstruct netdev_private *np = netdev_priv(dev);\n\tstruct mii_ioctl_data *miidata = if_mii(rq);\n\n\tphy_addr = np->phy_addr;\n\tswitch (cmd) {\n\tcase SIOCGMIIPHY:\n\t\tmiidata->phy_id = phy_addr;\n\t\tbreak;\n\tcase SIOCGMIIREG:\n\t\tmiidata->val_out = mii_read (dev, phy_addr, miidata->reg_num);\n\t\tbreak;\n\tcase SIOCSMIIREG:\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\treturn -EPERM;\n\t\tmii_write (dev, phy_addr, miidata->reg_num, miidata->val_in);\n\t\tbreak;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 839,
            "cve_id": "CVE-2015-8709",
            "code_snippet": "static int ptrace_attach(struct task_struct *task, long request,\n\t\t\t unsigned long addr,\n\t\t\t unsigned long flags)\n{\n\tbool seize = (request == PTRACE_SEIZE);\n\tint retval;\n\n\tretval = -EIO;\n\tif (seize) {\n\t\tif (addr != 0)\n\t\t\tgoto out;\n\t\tif (flags & ~(unsigned long)PTRACE_O_MASK)\n\t\t\tgoto out;\n\t\tflags = PT_PTRACED | PT_SEIZED | (flags << PT_OPT_FLAG_SHIFT);\n\t} else {\n\t\tflags = PT_PTRACED;\n\t}\n\n\taudit_ptrace(task);\n\n\tretval = -EPERM;\n\tif (unlikely(task->flags & PF_KTHREAD))\n\t\tgoto out;\n\tif (same_thread_group(task, current))\n\t\tgoto out;\n\n\t/*\n\t * Protect exec's credential calculations against our interference;\n\t * SUID, SGID and LSM creds get determined differently\n\t * under ptrace.\n\t */\n\tretval = -ERESTARTNOINTR;\n\tif (mutex_lock_interruptible(&task->signal->cred_guard_mutex))\n\t\tgoto out;\n\n\ttask_lock(task);\n\tretval = __ptrace_may_access(task, PTRACE_MODE_ATTACH_REALCREDS);\n\tif (!retval) {\n\t\tstruct mm_struct *mm = task->mm;\n\t\tif (mm && ns_capable(mm->user_ns, CAP_SYS_PTRACE))\n\t\t\tflags |= PT_PTRACE_CAP;\n\t}\n\ttask_unlock(task);\n\tif (retval)\n\t\tgoto unlock_creds;\n\n\twrite_lock_irq(&tasklist_lock);\n\tretval = -EPERM;\n\tif (unlikely(task->exit_state))\n\t\tgoto unlock_tasklist;\n\tif (task->ptrace)\n\t\tgoto unlock_tasklist;\n\n\tif (seize)\n\t\tflags |= PT_SEIZED;\n\ttask->ptrace = flags;\n\n\t__ptrace_link(task, current);\n\n\t/* SEIZE doesn't trap tracee on attach */\n\tif (!seize)\n\t\tsend_sig_info(SIGSTOP, SEND_SIG_FORCED, task);\n\n\tspin_lock(&task->sighand->siglock);\n\n\t/*\n\t * If the task is already STOPPED, set JOBCTL_TRAP_STOP and\n\t * TRAPPING, and kick it so that it transits to TRACED.  TRAPPING\n\t * will be cleared if the child completes the transition or any\n\t * event which clears the group stop states happens.  We'll wait\n\t * for the transition to complete before returning from this\n\t * function.\n\t *\n\t * This hides STOPPED -> RUNNING -> TRACED transition from the\n\t * attaching thread but a different thread in the same group can\n\t * still observe the transient RUNNING state.  IOW, if another\n\t * thread's WNOHANG wait(2) on the stopped tracee races against\n\t * ATTACH, the wait(2) may fail due to the transient RUNNING.\n\t *\n\t * The following task_is_stopped() test is safe as both transitions\n\t * in and out of STOPPED are protected by siglock.\n\t */\n\tif (task_is_stopped(task) &&\n\t    task_set_jobctl_pending(task, JOBCTL_TRAP_STOP | JOBCTL_TRAPPING))\n\t\tsignal_wake_up_state(task, __TASK_STOPPED);\n\n\tspin_unlock(&task->sighand->siglock);\n\n\tretval = 0;\nunlock_tasklist:\n\twrite_unlock_irq(&tasklist_lock);\nunlock_creds:\n\tmutex_unlock(&task->signal->cred_guard_mutex);\nout:\n\tif (!retval) {\n\t\t/*\n\t\t * We do not bother to change retval or clear JOBCTL_TRAPPING\n\t\t * if wait_on_bit() was interrupted by SIGKILL. The tracer will\n\t\t * not return to user-mode, it will exit and clear this bit in\n\t\t * __ptrace_unlink() if it wasn't already cleared by the tracee;\n\t\t * and until then nobody can ptrace this task.\n\t\t */\n\t\twait_on_bit(&task->jobctl, JOBCTL_TRAPPING_BIT, TASK_KILLABLE);\n\t\tproc_ptrace_connector(task, PTRACE_ATTACH);\n\t}\n\n\treturn retval;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static struct task_struct *copy_process(unsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace)\n{\n\tint retval;\n\tstruct task_struct *p;\n\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid namespace\n\t * don't allow the creation of threads.\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_NEWPID)) &&\n\t    (task_active_pid_ns(current) != current->nsproxy->pid_ns))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tretval = security_task_create(clone_flags);\n\tif (retval)\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current);\n\tif (!p)\n\t\tgoto fork_out;\n\n\tftrace_graph_init_task(p);\n\tget_seccomp_filter(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&\n\t\t    p->real_cred->user != INIT_USER)\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tif (!try_module_get(task_thread_info(p)->exec_domain->module))\n\t\tgoto bad_fork_cleanup_count;\n\n\tp->did_exec = 0;\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tcopy_flags(clone_flags, p);\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n\tp->utimescaled = p->stimescaled = 0;\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING\n\tp->prev_cputime.utime = p->prev_cputime.stime = 0;\n#endif\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqlock_init(&p->vtime_seqlock);\n\tp->vtime_snap = 0;\n\tp->vtime_snap_whence = VTIME_SLEEPING;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tdo_posix_clock_monotonic_gettime(&p->start_time);\n\tp->real_start_time = p->start_time;\n\tmonotonic_to_bootbased(&p->real_start_time);\n\tp->io_context = NULL;\n\tp->audit_context = NULL;\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_begin(current);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_cgroup;\n\t}\n\tmpol_fix_fork_child_flag(p);\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_init(&p->mems_allowed_seq);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n\tp->hardirqs_enabled = 0;\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_MEMCG\n\tp->memcg_batch.do_batch = 0;\n\tp->memcg_batch.memcg = NULL;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tsched_fork(p);\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\t/* copy all the process information */\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(clone_flags, stack_start, stack_size, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tif (pid != &init_struct_pid) {\n\t\tretval = -ENOMEM;\n\t\tpid = alloc_pid(p->nsproxy->pid_ns);\n\t\tif (!pid)\n\t\t\tgoto bad_fork_cleanup_io;\n\t}\n\n\tp->pid = pid_nr(pid);\n\tp->tgid = p->pid;\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->tgid = current->tgid;\n\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\tuprobe_copy_process(p);\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tp->sas_ss_sp = p->sas_ss_size = 0;\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->exit_signal = -1;\n\telse if (clone_flags & CLONE_PARENT)\n\t\tp->exit_signal = current->group_leader->exit_signal;\n\telse\n\t\tp->exit_signal = (clone_flags & CSIGNAL);\n\n\tp->pdeath_signal = 0;\n\tp->exit_state = 0;\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\t/*\n\t * Ok, make it visible to the rest of the system.\n\t * We dont wake it up yet.\n\t */\n\tp->group_leader = p;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\t/* Need tasklist lock for parent etc handling! */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Process group and session signals need to be delivered to just the\n\t * parent before the fork or both the parent and the child after the\n\t * fork. Restart if a signal comes in before we add the new process to\n\t * it's process group.\n\t * A fatal signal pending means that current will exit, so the new\n\t * thread can't slip out of an OOM kill (or normal SIGKILL).\n\t*/\n\trecalc_sigpending();\n\tif (signal_pending(current)) {\n\t\tspin_unlock(&current->sighand->siglock);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t\tretval = -ERESTARTNOINTR;\n\t\tgoto bad_fork_free_pid;\n\t}\n\n\tif (clone_flags & CLONE_THREAD) {\n\t\tcurrent->signal->nr_threads++;\n\t\tatomic_inc(&current->signal->live);\n\t\tatomic_inc(&current->signal->sigcnt);\n\t\tp->group_leader = current->group_leader;\n\t\tlist_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);\n\t}\n\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tif (thread_group_leader(p)) {\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\n\t\t\tp->signal->leader_pid = pid;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\tattach_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tattach_pid(p, PIDTYPE_SID, task_session(current));\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID, pid);\n\t\tnr_threads++;\n\t}\n\n\ttotal_forks++;\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\n\treturn p;\n\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm)\n\t\tmmput(p->mm);\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_policy:\n\tperf_event_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_cgroup:\n#endif\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tcgroup_exit(p, 0);\n\tdelayacct_tsk_free(p);\n\tmodule_put(task_thread_info(p)->exec_domain->module);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tfree_task(p);\nfork_out:\n\treturn ERR_PTR(retval);\n}",
                        "code_after_change": "static struct task_struct *copy_process(unsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace)\n{\n\tint retval;\n\tstruct task_struct *p;\n\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid namespace\n\t * don't allow the creation of threads.\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_NEWPID)) &&\n\t    (task_active_pid_ns(current) != current->nsproxy->pid_ns))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tretval = security_task_create(clone_flags);\n\tif (retval)\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current);\n\tif (!p)\n\t\tgoto fork_out;\n\n\tftrace_graph_init_task(p);\n\tget_seccomp_filter(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&\n\t\t    p->real_cred->user != INIT_USER)\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tif (!try_module_get(task_thread_info(p)->exec_domain->module))\n\t\tgoto bad_fork_cleanup_count;\n\n\tp->did_exec = 0;\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tcopy_flags(clone_flags, p);\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n\tp->utimescaled = p->stimescaled = 0;\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING\n\tp->prev_cputime.utime = p->prev_cputime.stime = 0;\n#endif\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqlock_init(&p->vtime_seqlock);\n\tp->vtime_snap = 0;\n\tp->vtime_snap_whence = VTIME_SLEEPING;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tdo_posix_clock_monotonic_gettime(&p->start_time);\n\tp->real_start_time = p->start_time;\n\tmonotonic_to_bootbased(&p->real_start_time);\n\tp->io_context = NULL;\n\tp->audit_context = NULL;\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_begin(current);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_cgroup;\n\t}\n\tmpol_fix_fork_child_flag(p);\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_init(&p->mems_allowed_seq);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n\tp->hardirqs_enabled = 0;\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_MEMCG\n\tp->memcg_batch.do_batch = 0;\n\tp->memcg_batch.memcg = NULL;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tsched_fork(p);\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\t/* copy all the process information */\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(clone_flags, stack_start, stack_size, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tif (pid != &init_struct_pid) {\n\t\tretval = -ENOMEM;\n\t\tpid = alloc_pid(p->nsproxy->pid_ns);\n\t\tif (!pid)\n\t\t\tgoto bad_fork_cleanup_io;\n\t}\n\n\tp->pid = pid_nr(pid);\n\tp->tgid = p->pid;\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->tgid = current->tgid;\n\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\tuprobe_copy_process(p);\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tp->sas_ss_sp = p->sas_ss_size = 0;\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->exit_signal = -1;\n\telse if (clone_flags & CLONE_PARENT)\n\t\tp->exit_signal = current->group_leader->exit_signal;\n\telse\n\t\tp->exit_signal = (clone_flags & CSIGNAL);\n\n\tp->pdeath_signal = 0;\n\tp->exit_state = 0;\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\t/*\n\t * Ok, make it visible to the rest of the system.\n\t * We dont wake it up yet.\n\t */\n\tp->group_leader = p;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\t/* Need tasklist lock for parent etc handling! */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Process group and session signals need to be delivered to just the\n\t * parent before the fork or both the parent and the child after the\n\t * fork. Restart if a signal comes in before we add the new process to\n\t * it's process group.\n\t * A fatal signal pending means that current will exit, so the new\n\t * thread can't slip out of an OOM kill (or normal SIGKILL).\n\t*/\n\trecalc_sigpending();\n\tif (signal_pending(current)) {\n\t\tspin_unlock(&current->sighand->siglock);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t\tretval = -ERESTARTNOINTR;\n\t\tgoto bad_fork_free_pid;\n\t}\n\n\tif (clone_flags & CLONE_THREAD) {\n\t\tcurrent->signal->nr_threads++;\n\t\tatomic_inc(&current->signal->live);\n\t\tatomic_inc(&current->signal->sigcnt);\n\t\tp->group_leader = current->group_leader;\n\t\tlist_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);\n\t}\n\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tif (thread_group_leader(p)) {\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\n\t\t\tp->signal->leader_pid = pid;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\tattach_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tattach_pid(p, PIDTYPE_SID, task_session(current));\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID, pid);\n\t\tnr_threads++;\n\t}\n\n\ttotal_forks++;\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\n\treturn p;\n\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm)\n\t\tmmput(p->mm);\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_policy:\n\tperf_event_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_cgroup:\n#endif\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tcgroup_exit(p, 0);\n\tdelayacct_tsk_free(p);\n\tmodule_put(task_thread_info(p)->exec_domain->module);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tfree_task(p);\nfork_out:\n\treturn ERR_PTR(retval);\n}",
                        "cve_id": "CVE-2013-1858"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int ptrace_attach(struct task_struct *task, long request,\n\t\t\t unsigned long addr,\n\t\t\t unsigned long flags)\n{\n\tbool seize = (request == PTRACE_SEIZE);\n\tint retval;\n\n\tretval = -EIO;\n\tif (seize) {\n\t\tif (addr != 0)\n\t\t\tgoto out;\n\t\tif (flags & ~(unsigned long)PTRACE_O_MASK)\n\t\t\tgoto out;\n\t\tflags = PT_PTRACED | PT_SEIZED | (flags << PT_OPT_FLAG_SHIFT);\n\t} else {\n\t\tflags = PT_PTRACED;\n\t}\n\n\taudit_ptrace(task);\n\n\tretval = -EPERM;\n\tif (unlikely(task->flags & PF_KTHREAD))\n\t\tgoto out;\n\tif (same_thread_group(task, current))\n\t\tgoto out;\n\n\t/*\n\t * Protect exec's credential calculations against our interference;\n\t * SUID, SGID and LSM creds get determined differently\n\t * under ptrace.\n\t */\n\tretval = -ERESTARTNOINTR;\n\tif (mutex_lock_interruptible(&task->signal->cred_guard_mutex))\n\t\tgoto out;\n\n\ttask_lock(task);\n\tretval = __ptrace_may_access(task, PTRACE_MODE_ATTACH_REALCREDS);\n\tif (!retval) {\n\t\tstruct mm_struct *mm = task->mm;\n\t\tif (mm && ns_capable(mm->user_ns, CAP_SYS_PTRACE))\n\t\t\tflags |= PT_PTRACE_CAP;\n\t}\n\ttask_unlock(task);\n\tif (retval)\n\t\tgoto unlock_creds;\n\n\twrite_lock_irq(&tasklist_lock);\n\tretval = -EPERM;\n\tif (unlikely(task->exit_state))\n\t\tgoto unlock_tasklist;\n\tif (task->ptrace)\n\t\tgoto unlock_tasklist;\n\n\tif (seize)\n\t\tflags |= PT_SEIZED;\n\ttask->ptrace = flags;\n\n\t__ptrace_link(task, current);\n\n\t/* SEIZE doesn't trap tracee on attach */\n\tif (!seize)\n\t\tsend_sig_info(SIGSTOP, SEND_SIG_FORCED, task);\n\n\tspin_lock(&task->sighand->siglock);\n\n\t/*\n\t * If the task is already STOPPED, set JOBCTL_TRAP_STOP and\n\t * TRAPPING, and kick it so that it transits to TRACED.  TRAPPING\n\t * will be cleared if the child completes the transition or any\n\t * event which clears the group stop states happens.  We'll wait\n\t * for the transition to complete before returning from this\n\t * function.\n\t *\n\t * This hides STOPPED -> RUNNING -> TRACED transition from the\n\t * attaching thread but a different thread in the same group can\n\t * still observe the transient RUNNING state.  IOW, if another\n\t * thread's WNOHANG wait(2) on the stopped tracee races against\n\t * ATTACH, the wait(2) may fail due to the transient RUNNING.\n\t *\n\t * The following task_is_stopped() test is safe as both transitions\n\t * in and out of STOPPED are protected by siglock.\n\t */\n\tif (task_is_stopped(task) &&\n\t    task_set_jobctl_pending(task, JOBCTL_TRAP_STOP | JOBCTL_TRAPPING))\n\t\tsignal_wake_up_state(task, __TASK_STOPPED);\n\n\tspin_unlock(&task->sighand->siglock);\n\n\tretval = 0;\nunlock_tasklist:\n\twrite_unlock_irq(&tasklist_lock);\nunlock_creds:\n\tmutex_unlock(&task->signal->cred_guard_mutex);\nout:\n\tif (!retval) {\n\t\t/*\n\t\t * We do not bother to change retval or clear JOBCTL_TRAPPING\n\t\t * if wait_on_bit() was interrupted by SIGKILL. The tracer will\n\t\t * not return to user-mode, it will exit and clear this bit in\n\t\t * __ptrace_unlink() if it wasn't already cleared by the tracee;\n\t\t * and until then nobody can ptrace this task.\n\t\t */\n\t\twait_on_bit(&task->jobctl, JOBCTL_TRAPPING_BIT, TASK_KILLABLE);\n\t\tproc_ptrace_connector(task, PTRACE_ATTACH);\n\t}\n\n\treturn retval;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic struct task_struct *copy_process(unsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace)\n{\n\tint retval;\n\tstruct task_struct *p;\n\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid namespace\n\t * don't allow the creation of threads.\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_NEWPID)) &&\n\t    (task_active_pid_ns(current) != current->nsproxy->pid_ns))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tretval = security_task_create(clone_flags);\n\tif (retval)\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current);\n\tif (!p)\n\t\tgoto fork_out;\n\n\tftrace_graph_init_task(p);\n\tget_seccomp_filter(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&\n\t\t    p->real_cred->user != INIT_USER)\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tif (!try_module_get(task_thread_info(p)->exec_domain->module))\n\t\tgoto bad_fork_cleanup_count;\n\n\tp->did_exec = 0;\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tcopy_flags(clone_flags, p);\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n\tp->utimescaled = p->stimescaled = 0;\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING\n\tp->prev_cputime.utime = p->prev_cputime.stime = 0;\n#endif\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqlock_init(&p->vtime_seqlock);\n\tp->vtime_snap = 0;\n\tp->vtime_snap_whence = VTIME_SLEEPING;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tdo_posix_clock_monotonic_gettime(&p->start_time);\n\tp->real_start_time = p->start_time;\n\tmonotonic_to_bootbased(&p->real_start_time);\n\tp->io_context = NULL;\n\tp->audit_context = NULL;\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_begin(current);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_cgroup;\n\t}\n\tmpol_fix_fork_child_flag(p);\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_init(&p->mems_allowed_seq);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n\tp->hardirqs_enabled = 0;\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_MEMCG\n\tp->memcg_batch.do_batch = 0;\n\tp->memcg_batch.memcg = NULL;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tsched_fork(p);\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\t/* copy all the process information */\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(clone_flags, stack_start, stack_size, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tif (pid != &init_struct_pid) {\n\t\tretval = -ENOMEM;\n\t\tpid = alloc_pid(p->nsproxy->pid_ns);\n\t\tif (!pid)\n\t\t\tgoto bad_fork_cleanup_io;\n\t}\n\n\tp->pid = pid_nr(pid);\n\tp->tgid = p->pid;\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->tgid = current->tgid;\n\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\tuprobe_copy_process(p);\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tp->sas_ss_sp = p->sas_ss_size = 0;\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->exit_signal = -1;\n\telse if (clone_flags & CLONE_PARENT)\n\t\tp->exit_signal = current->group_leader->exit_signal;\n\telse\n\t\tp->exit_signal = (clone_flags & CSIGNAL);\n\n\tp->pdeath_signal = 0;\n\tp->exit_state = 0;\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\t/*\n\t * Ok, make it visible to the rest of the system.\n\t * We dont wake it up yet.\n\t */\n\tp->group_leader = p;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\t/* Need tasklist lock for parent etc handling! */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Process group and session signals need to be delivered to just the\n\t * parent before the fork or both the parent and the child after the\n\t * fork. Restart if a signal comes in before we add the new process to\n\t * it's process group.\n\t * A fatal signal pending means that current will exit, so the new\n\t * thread can't slip out of an OOM kill (or normal SIGKILL).\n\t*/\n\trecalc_sigpending();\n\tif (signal_pending(current)) {\n\t\tspin_unlock(&current->sighand->siglock);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t\tretval = -ERESTARTNOINTR;\n\t\tgoto bad_fork_free_pid;\n\t}\n\n\tif (clone_flags & CLONE_THREAD) {\n\t\tcurrent->signal->nr_threads++;\n\t\tatomic_inc(&current->signal->live);\n\t\tatomic_inc(&current->signal->sigcnt);\n\t\tp->group_leader = current->group_leader;\n\t\tlist_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);\n\t}\n\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tif (thread_group_leader(p)) {\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\n\t\t\tp->signal->leader_pid = pid;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\tattach_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tattach_pid(p, PIDTYPE_SID, task_session(current));\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID, pid);\n\t\tnr_threads++;\n\t}\n\n\ttotal_forks++;\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\n\treturn p;\n\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm)\n\t\tmmput(p->mm);\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_policy:\n\tperf_event_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_cgroup:\n#endif\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tcgroup_exit(p, 0);\n\tdelayacct_tsk_free(p);\n\tmodule_put(task_thread_info(p)->exec_domain->module);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tfree_task(p);\nfork_out:\n\treturn ERR_PTR(retval);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic struct task_struct *copy_process(unsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace)\n{\n\tint retval;\n\tstruct task_struct *p;\n\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid namespace\n\t * don't allow the creation of threads.\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_NEWPID)) &&\n\t    (task_active_pid_ns(current) != current->nsproxy->pid_ns))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tretval = security_task_create(clone_flags);\n\tif (retval)\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current);\n\tif (!p)\n\t\tgoto fork_out;\n\n\tftrace_graph_init_task(p);\n\tget_seccomp_filter(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\ttask_rlimit(p, RLIMIT_NPROC)) {\n\t\tif (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&\n\t\t    p->real_cred->user != INIT_USER)\n\t\t\tgoto bad_fork_free;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tif (!try_module_get(task_thread_info(p)->exec_domain->module))\n\t\tgoto bad_fork_cleanup_count;\n\n\tp->did_exec = 0;\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tcopy_flags(clone_flags, p);\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n\tp->utimescaled = p->stimescaled = 0;\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING\n\tp->prev_cputime.utime = p->prev_cputime.stime = 0;\n#endif\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqlock_init(&p->vtime_seqlock);\n\tp->vtime_snap = 0;\n\tp->vtime_snap_whence = VTIME_SLEEPING;\n#endif\n\n#if defined(SPLIT_RSS_COUNTING)\n\tmemset(&p->rss_stat, 0, sizeof(p->rss_stat));\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tdo_posix_clock_monotonic_gettime(&p->start_time);\n\tp->real_start_time = p->start_time;\n\tmonotonic_to_bootbased(&p->real_start_time);\n\tp->io_context = NULL;\n\tp->audit_context = NULL;\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_begin(current);\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_cgroup;\n\t}\n\tmpol_fix_fork_child_flag(p);\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tp->cpuset_slab_spread_rotor = NUMA_NO_NODE;\n\tseqcount_init(&p->mems_allowed_seq);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n\tp->hardirqs_enabled = 0;\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_MEMCG\n\tp->memcg_batch.do_batch = 0;\n\tp->memcg_batch.memcg = NULL;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tsched_fork(p);\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\t/* copy all the process information */\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_files(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(clone_flags, stack_start, stack_size, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tif (pid != &init_struct_pid) {\n\t\tretval = -ENOMEM;\n\t\tpid = alloc_pid(p->nsproxy->pid_ns);\n\t\tif (!pid)\n\t\t\tgoto bad_fork_cleanup_io;\n\t}\n\n\tp->pid = pid_nr(pid);\n\tp->tgid = p->pid;\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->tgid = current->tgid;\n\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\tuprobe_copy_process(p);\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tp->sas_ss_sp = p->sas_ss_size = 0;\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->exit_signal = -1;\n\telse if (clone_flags & CLONE_PARENT)\n\t\tp->exit_signal = current->group_leader->exit_signal;\n\telse\n\t\tp->exit_signal = (clone_flags & CSIGNAL);\n\n\tp->pdeath_signal = 0;\n\tp->exit_state = 0;\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\t/*\n\t * Ok, make it visible to the rest of the system.\n\t * We dont wake it up yet.\n\t */\n\tp->group_leader = p;\n\tINIT_LIST_HEAD(&p->thread_group);\n\tp->task_works = NULL;\n\n\t/* Need tasklist lock for parent etc handling! */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Process group and session signals need to be delivered to just the\n\t * parent before the fork or both the parent and the child after the\n\t * fork. Restart if a signal comes in before we add the new process to\n\t * it's process group.\n\t * A fatal signal pending means that current will exit, so the new\n\t * thread can't slip out of an OOM kill (or normal SIGKILL).\n\t*/\n\trecalc_sigpending();\n\tif (signal_pending(current)) {\n\t\tspin_unlock(&current->sighand->siglock);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t\tretval = -ERESTARTNOINTR;\n\t\tgoto bad_fork_free_pid;\n\t}\n\n\tif (clone_flags & CLONE_THREAD) {\n\t\tcurrent->signal->nr_threads++;\n\t\tatomic_inc(&current->signal->live);\n\t\tatomic_inc(&current->signal->sigcnt);\n\t\tp->group_leader = current->group_leader;\n\t\tlist_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);\n\t}\n\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tif (thread_group_leader(p)) {\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\n\t\t\tp->signal->leader_pid = pid;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\tattach_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tattach_pid(p, PIDTYPE_SID, task_session(current));\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID, pid);\n\t\tnr_threads++;\n\t}\n\n\ttotal_forks++;\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\n\treturn p;\n\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm)\n\t\tmmput(p->mm);\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_policy:\n\tperf_event_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_cgroup:\n#endif\n\tif (clone_flags & CLONE_THREAD)\n\t\tthreadgroup_change_end(current);\n\tcgroup_exit(p, 0);\n\tdelayacct_tsk_free(p);\n\tmodule_put(task_thread_info(p)->exec_domain->module);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tfree_task(p);\nfork_out:\n\treturn ERR_PTR(retval);\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int __ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tconst struct cred *cred = current_cred(), *tcred;\n\tint dumpable = 0;\n\tkuid_t caller_uid;\n\tkgid_t caller_gid;\n\n\tif (!(mode & PTRACE_MODE_FSCREDS) == !(mode & PTRACE_MODE_REALCREDS)) {\n\t\tWARN(1, \"denying ptrace access check without PTRACE_MODE_*CREDS\\n\");\n\t\treturn -EPERM;\n\t}\n\n\t/* May we inspect the given task?\n\t * This check is used both for attaching with ptrace\n\t * and for allowing access to sensitive information in /proc.\n\t *\n\t * ptrace_attach denies several cases that /proc allows\n\t * because setting up the necessary parent/child relationship\n\t * or halting the specified task is impossible.\n\t */\n\n\t/* Don't let security modules deny introspection */\n\tif (same_thread_group(task, current))\n\t\treturn 0;\n\trcu_read_lock();\n\tif (mode & PTRACE_MODE_FSCREDS) {\n\t\tcaller_uid = cred->fsuid;\n\t\tcaller_gid = cred->fsgid;\n\t} else {\n\t\t/*\n\t\t * Using the euid would make more sense here, but something\n\t\t * in userland might rely on the old behavior, and this\n\t\t * shouldn't be a security problem since\n\t\t * PTRACE_MODE_REALCREDS implies that the caller explicitly\n\t\t * used a syscall that requests access to another process\n\t\t * (and not a filesystem syscall to procfs).\n\t\t */\n\t\tcaller_uid = cred->uid;\n\t\tcaller_gid = cred->gid;\n\t}\n\ttcred = __task_cred(task);\n\tif (uid_eq(caller_uid, tcred->euid) &&\n\t    uid_eq(caller_uid, tcred->suid) &&\n\t    uid_eq(caller_uid, tcred->uid)  &&\n\t    gid_eq(caller_gid, tcred->egid) &&\n\t    gid_eq(caller_gid, tcred->sgid) &&\n\t    gid_eq(caller_gid, tcred->gid))\n\t\tgoto ok;\n\tif (ptrace_has_cap(tcred->user_ns, mode))\n\t\tgoto ok;\n\trcu_read_unlock();\n\treturn -EPERM;\nok:\n\trcu_read_unlock();\n\tsmp_rmb();\n\tif (task->mm)\n\t\tdumpable = get_dumpable(task->mm);\n\trcu_read_lock();\n\tif (dumpable != SUID_DUMP_USER &&\n\t    !ptrace_has_cap(__task_cred(task)->user_ns, mode)) {\n\t\trcu_read_unlock();\n\t\treturn -EPERM;\n\t}\n\trcu_read_unlock();\n\n\treturn security_ptrace_access_check(task, mode);\n}",
                        "code_after_change": "static int __ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tconst struct cred *cred = current_cred(), *tcred;\n\tstruct mm_struct *mm;\n\tkuid_t caller_uid;\n\tkgid_t caller_gid;\n\n\tif (!(mode & PTRACE_MODE_FSCREDS) == !(mode & PTRACE_MODE_REALCREDS)) {\n\t\tWARN(1, \"denying ptrace access check without PTRACE_MODE_*CREDS\\n\");\n\t\treturn -EPERM;\n\t}\n\n\t/* May we inspect the given task?\n\t * This check is used both for attaching with ptrace\n\t * and for allowing access to sensitive information in /proc.\n\t *\n\t * ptrace_attach denies several cases that /proc allows\n\t * because setting up the necessary parent/child relationship\n\t * or halting the specified task is impossible.\n\t */\n\n\t/* Don't let security modules deny introspection */\n\tif (same_thread_group(task, current))\n\t\treturn 0;\n\trcu_read_lock();\n\tif (mode & PTRACE_MODE_FSCREDS) {\n\t\tcaller_uid = cred->fsuid;\n\t\tcaller_gid = cred->fsgid;\n\t} else {\n\t\t/*\n\t\t * Using the euid would make more sense here, but something\n\t\t * in userland might rely on the old behavior, and this\n\t\t * shouldn't be a security problem since\n\t\t * PTRACE_MODE_REALCREDS implies that the caller explicitly\n\t\t * used a syscall that requests access to another process\n\t\t * (and not a filesystem syscall to procfs).\n\t\t */\n\t\tcaller_uid = cred->uid;\n\t\tcaller_gid = cred->gid;\n\t}\n\ttcred = __task_cred(task);\n\tif (uid_eq(caller_uid, tcred->euid) &&\n\t    uid_eq(caller_uid, tcred->suid) &&\n\t    uid_eq(caller_uid, tcred->uid)  &&\n\t    gid_eq(caller_gid, tcred->egid) &&\n\t    gid_eq(caller_gid, tcred->sgid) &&\n\t    gid_eq(caller_gid, tcred->gid))\n\t\tgoto ok;\n\tif (ptrace_has_cap(tcred->user_ns, mode))\n\t\tgoto ok;\n\trcu_read_unlock();\n\treturn -EPERM;\nok:\n\trcu_read_unlock();\n\tmm = task->mm;\n\tif (mm &&\n\t    ((get_dumpable(mm) != SUID_DUMP_USER) &&\n\t     !ptrace_has_cap(mm->user_ns, mode)))\n\t    return -EPERM;\n\n\treturn security_ptrace_access_check(task, mode);\n}",
                        "cve_id": "CVE-2015-8709"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int ptrace_attach(struct task_struct *task, long request,\n\t\t\t unsigned long addr,\n\t\t\t unsigned long flags)\n{\n\tbool seize = (request == PTRACE_SEIZE);\n\tint retval;\n\n\tretval = -EIO;\n\tif (seize) {\n\t\tif (addr != 0)\n\t\t\tgoto out;\n\t\tif (flags & ~(unsigned long)PTRACE_O_MASK)\n\t\t\tgoto out;\n\t\tflags = PT_PTRACED | PT_SEIZED | (flags << PT_OPT_FLAG_SHIFT);\n\t} else {\n\t\tflags = PT_PTRACED;\n\t}\n\n\taudit_ptrace(task);\n\n\tretval = -EPERM;\n\tif (unlikely(task->flags & PF_KTHREAD))\n\t\tgoto out;\n\tif (same_thread_group(task, current))\n\t\tgoto out;\n\n\t/*\n\t * Protect exec's credential calculations against our interference;\n\t * SUID, SGID and LSM creds get determined differently\n\t * under ptrace.\n\t */\n\tretval = -ERESTARTNOINTR;\n\tif (mutex_lock_interruptible(&task->signal->cred_guard_mutex))\n\t\tgoto out;\n\n\ttask_lock(task);\n\tretval = __ptrace_may_access(task, PTRACE_MODE_ATTACH_REALCREDS);\n\tif (!retval) {\n\t\tstruct mm_struct *mm = task->mm;\n\t\tif (mm && ns_capable(mm->user_ns, CAP_SYS_PTRACE))\n\t\t\tflags |= PT_PTRACE_CAP;\n\t}\n\ttask_unlock(task);\n\tif (retval)\n\t\tgoto unlock_creds;\n\n\twrite_lock_irq(&tasklist_lock);\n\tretval = -EPERM;\n\tif (unlikely(task->exit_state))\n\t\tgoto unlock_tasklist;\n\tif (task->ptrace)\n\t\tgoto unlock_tasklist;\n\n\tif (seize)\n\t\tflags |= PT_SEIZED;\n\ttask->ptrace = flags;\n\n\t__ptrace_link(task, current);\n\n\t/* SEIZE doesn't trap tracee on attach */\n\tif (!seize)\n\t\tsend_sig_info(SIGSTOP, SEND_SIG_FORCED, task);\n\n\tspin_lock(&task->sighand->siglock);\n\n\t/*\n\t * If the task is already STOPPED, set JOBCTL_TRAP_STOP and\n\t * TRAPPING, and kick it so that it transits to TRACED.  TRAPPING\n\t * will be cleared if the child completes the transition or any\n\t * event which clears the group stop states happens.  We'll wait\n\t * for the transition to complete before returning from this\n\t * function.\n\t *\n\t * This hides STOPPED -> RUNNING -> TRACED transition from the\n\t * attaching thread but a different thread in the same group can\n\t * still observe the transient RUNNING state.  IOW, if another\n\t * thread's WNOHANG wait(2) on the stopped tracee races against\n\t * ATTACH, the wait(2) may fail due to the transient RUNNING.\n\t *\n\t * The following task_is_stopped() test is safe as both transitions\n\t * in and out of STOPPED are protected by siglock.\n\t */\n\tif (task_is_stopped(task) &&\n\t    task_set_jobctl_pending(task, JOBCTL_TRAP_STOP | JOBCTL_TRAPPING))\n\t\tsignal_wake_up_state(task, __TASK_STOPPED);\n\n\tspin_unlock(&task->sighand->siglock);\n\n\tretval = 0;\nunlock_tasklist:\n\twrite_unlock_irq(&tasklist_lock);\nunlock_creds:\n\tmutex_unlock(&task->signal->cred_guard_mutex);\nout:\n\tif (!retval) {\n\t\t/*\n\t\t * We do not bother to change retval or clear JOBCTL_TRAPPING\n\t\t * if wait_on_bit() was interrupted by SIGKILL. The tracer will\n\t\t * not return to user-mode, it will exit and clear this bit in\n\t\t * __ptrace_unlink() if it wasn't already cleared by the tracee;\n\t\t * and until then nobody can ptrace this task.\n\t\t */\n\t\twait_on_bit(&task->jobctl, JOBCTL_TRAPPING_BIT, TASK_KILLABLE);\n\t\tproc_ptrace_connector(task, PTRACE_ATTACH);\n\t}\n\n\treturn retval;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int __ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tconst struct cred *cred = current_cred(), *tcred;\n\tint dumpable = 0;\n\tkuid_t caller_uid;\n\tkgid_t caller_gid;\n\n\tif (!(mode & PTRACE_MODE_FSCREDS) == !(mode & PTRACE_MODE_REALCREDS)) {\n\t\tWARN(1, \"denying ptrace access check without PTRACE_MODE_*CREDS\\n\");\n\t\treturn -EPERM;\n\t}\n\n\t/* May we inspect the given task?\n\t * This check is used both for attaching with ptrace\n\t * and for allowing access to sensitive information in /proc.\n\t *\n\t * ptrace_attach denies several cases that /proc allows\n\t * because setting up the necessary parent/child relationship\n\t * or halting the specified task is impossible.\n\t */\n\n\t/* Don't let security modules deny introspection */\n\tif (same_thread_group(task, current))\n\t\treturn 0;\n\trcu_read_lock();\n\tif (mode & PTRACE_MODE_FSCREDS) {\n\t\tcaller_uid = cred->fsuid;\n\t\tcaller_gid = cred->fsgid;\n\t} else {\n\t\t/*\n\t\t * Using the euid would make more sense here, but something\n\t\t * in userland might rely on the old behavior, and this\n\t\t * shouldn't be a security problem since\n\t\t * PTRACE_MODE_REALCREDS implies that the caller explicitly\n\t\t * used a syscall that requests access to another process\n\t\t * (and not a filesystem syscall to procfs).\n\t\t */\n\t\tcaller_uid = cred->uid;\n\t\tcaller_gid = cred->gid;\n\t}\n\ttcred = __task_cred(task);\n\tif (uid_eq(caller_uid, tcred->euid) &&\n\t    uid_eq(caller_uid, tcred->suid) &&\n\t    uid_eq(caller_uid, tcred->uid)  &&\n\t    gid_eq(caller_gid, tcred->egid) &&\n\t    gid_eq(caller_gid, tcred->sgid) &&\n\t    gid_eq(caller_gid, tcred->gid))\n\t\tgoto ok;\n\tif (ptrace_has_cap(tcred->user_ns, mode))\n\t\tgoto ok;\n\trcu_read_unlock();\n\treturn -EPERM;\nok:\n\trcu_read_unlock();\n\tsmp_rmb();\n\tif (task->mm)\n\t\tdumpable = get_dumpable(task->mm);\n\trcu_read_lock();\n\tif (dumpable != SUID_DUMP_USER &&\n\t    !ptrace_has_cap(__task_cred(task)->user_ns, mode)) {\n\t\trcu_read_unlock();\n\t\treturn -EPERM;\n\t}\n\trcu_read_unlock();\n\n\treturn security_ptrace_access_check(task, mode);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int __ptrace_may_access(struct task_struct *task, unsigned int mode)\n{\n\tconst struct cred *cred = current_cred(), *tcred;\n\tstruct mm_struct *mm;\n\tkuid_t caller_uid;\n\tkgid_t caller_gid;\n\n\tif (!(mode & PTRACE_MODE_FSCREDS) == !(mode & PTRACE_MODE_REALCREDS)) {\n\t\tWARN(1, \"denying ptrace access check without PTRACE_MODE_*CREDS\\n\");\n\t\treturn -EPERM;\n\t}\n\n\t/* May we inspect the given task?\n\t * This check is used both for attaching with ptrace\n\t * and for allowing access to sensitive information in /proc.\n\t *\n\t * ptrace_attach denies several cases that /proc allows\n\t * because setting up the necessary parent/child relationship\n\t * or halting the specified task is impossible.\n\t */\n\n\t/* Don't let security modules deny introspection */\n\tif (same_thread_group(task, current))\n\t\treturn 0;\n\trcu_read_lock();\n\tif (mode & PTRACE_MODE_FSCREDS) {\n\t\tcaller_uid = cred->fsuid;\n\t\tcaller_gid = cred->fsgid;\n\t} else {\n\t\t/*\n\t\t * Using the euid would make more sense here, but something\n\t\t * in userland might rely on the old behavior, and this\n\t\t * shouldn't be a security problem since\n\t\t * PTRACE_MODE_REALCREDS implies that the caller explicitly\n\t\t * used a syscall that requests access to another process\n\t\t * (and not a filesystem syscall to procfs).\n\t\t */\n\t\tcaller_uid = cred->uid;\n\t\tcaller_gid = cred->gid;\n\t}\n\ttcred = __task_cred(task);\n\tif (uid_eq(caller_uid, tcred->euid) &&\n\t    uid_eq(caller_uid, tcred->suid) &&\n\t    uid_eq(caller_uid, tcred->uid)  &&\n\t    gid_eq(caller_gid, tcred->egid) &&\n\t    gid_eq(caller_gid, tcred->sgid) &&\n\t    gid_eq(caller_gid, tcred->gid))\n\t\tgoto ok;\n\tif (ptrace_has_cap(tcred->user_ns, mode))\n\t\tgoto ok;\n\trcu_read_unlock();\n\treturn -EPERM;\nok:\n\trcu_read_unlock();\n\tmm = task->mm;\n\tif (mm &&\n\t    ((get_dumpable(mm) != SUID_DUMP_USER) &&\n\t     !ptrace_has_cap(mm->user_ns, mode)))\n\t    return -EPERM;\n\n\treturn security_ptrace_access_check(task, mode);\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": " */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx;\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tstruct perf_event_context *gctx = group_leader->ctx;\n\n\t\tmutex_lock(&gctx->mutex);\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\n\tif (move_group) {\n\t\tsynchronize_rcu();\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}",
                        "code_after_change": " */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *uninitialized_var(gctx);\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tgctx = group_leader->ctx;\n\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t} else {\n\t\tmutex_lock(&ctx->mutex);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\tif (move_group) {\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\n\tif (move_group) {\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}",
                        "cve_id": "CVE-2016-6786"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int ptrace_attach(struct task_struct *task, long request,\n\t\t\t unsigned long addr,\n\t\t\t unsigned long flags)\n{\n\tbool seize = (request == PTRACE_SEIZE);\n\tint retval;\n\n\tretval = -EIO;\n\tif (seize) {\n\t\tif (addr != 0)\n\t\t\tgoto out;\n\t\tif (flags & ~(unsigned long)PTRACE_O_MASK)\n\t\t\tgoto out;\n\t\tflags = PT_PTRACED | PT_SEIZED | (flags << PT_OPT_FLAG_SHIFT);\n\t} else {\n\t\tflags = PT_PTRACED;\n\t}\n\n\taudit_ptrace(task);\n\n\tretval = -EPERM;\n\tif (unlikely(task->flags & PF_KTHREAD))\n\t\tgoto out;\n\tif (same_thread_group(task, current))\n\t\tgoto out;\n\n\t/*\n\t * Protect exec's credential calculations against our interference;\n\t * SUID, SGID and LSM creds get determined differently\n\t * under ptrace.\n\t */\n\tretval = -ERESTARTNOINTR;\n\tif (mutex_lock_interruptible(&task->signal->cred_guard_mutex))\n\t\tgoto out;\n\n\ttask_lock(task);\n\tretval = __ptrace_may_access(task, PTRACE_MODE_ATTACH_REALCREDS);\n\tif (!retval) {\n\t\tstruct mm_struct *mm = task->mm;\n\t\tif (mm && ns_capable(mm->user_ns, CAP_SYS_PTRACE))\n\t\t\tflags |= PT_PTRACE_CAP;\n\t}\n\ttask_unlock(task);\n\tif (retval)\n\t\tgoto unlock_creds;\n\n\twrite_lock_irq(&tasklist_lock);\n\tretval = -EPERM;\n\tif (unlikely(task->exit_state))\n\t\tgoto unlock_tasklist;\n\tif (task->ptrace)\n\t\tgoto unlock_tasklist;\n\n\tif (seize)\n\t\tflags |= PT_SEIZED;\n\ttask->ptrace = flags;\n\n\t__ptrace_link(task, current);\n\n\t/* SEIZE doesn't trap tracee on attach */\n\tif (!seize)\n\t\tsend_sig_info(SIGSTOP, SEND_SIG_FORCED, task);\n\n\tspin_lock(&task->sighand->siglock);\n\n\t/*\n\t * If the task is already STOPPED, set JOBCTL_TRAP_STOP and\n\t * TRAPPING, and kick it so that it transits to TRACED.  TRAPPING\n\t * will be cleared if the child completes the transition or any\n\t * event which clears the group stop states happens.  We'll wait\n\t * for the transition to complete before returning from this\n\t * function.\n\t *\n\t * This hides STOPPED -> RUNNING -> TRACED transition from the\n\t * attaching thread but a different thread in the same group can\n\t * still observe the transient RUNNING state.  IOW, if another\n\t * thread's WNOHANG wait(2) on the stopped tracee races against\n\t * ATTACH, the wait(2) may fail due to the transient RUNNING.\n\t *\n\t * The following task_is_stopped() test is safe as both transitions\n\t * in and out of STOPPED are protected by siglock.\n\t */\n\tif (task_is_stopped(task) &&\n\t    task_set_jobctl_pending(task, JOBCTL_TRAP_STOP | JOBCTL_TRAPPING))\n\t\tsignal_wake_up_state(task, __TASK_STOPPED);\n\n\tspin_unlock(&task->sighand->siglock);\n\n\tretval = 0;\nunlock_tasklist:\n\twrite_unlock_irq(&tasklist_lock);\nunlock_creds:\n\tmutex_unlock(&task->signal->cred_guard_mutex);\nout:\n\tif (!retval) {\n\t\t/*\n\t\t * We do not bother to change retval or clear JOBCTL_TRAPPING\n\t\t * if wait_on_bit() was interrupted by SIGKILL. The tracer will\n\t\t * not return to user-mode, it will exit and clear this bit in\n\t\t * __ptrace_unlink() if it wasn't already cleared by the tracee;\n\t\t * and until then nobody can ptrace this task.\n\t\t */\n\t\twait_on_bit(&task->jobctl, JOBCTL_TRAPPING_BIT, TASK_KILLABLE);\n\t\tproc_ptrace_connector(task, PTRACE_ATTACH);\n\t}\n\n\treturn retval;\n}\n'''\nRelated vulnerability code snippet:\n'''\n */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx;\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tstruct perf_event_context *gctx = group_leader->ctx;\n\n\t\tmutex_lock(&gctx->mutex);\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\n\tif (move_group) {\n\t\tsynchronize_rcu();\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\n */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *uninitialized_var(gctx);\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tgctx = group_leader->ctx;\n\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t} else {\n\t\tmutex_lock(&ctx->mutex);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\tif (move_group) {\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\n\tif (move_group) {\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 884,
            "cve_id": "CVE-2015-9016",
            "code_snippet": "static void bt_tags_for_each(struct blk_mq_tags *tags,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_tag_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tif (!tags->rqs)\n\t\treturn;\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t\trq = tags->rqs[off + bit];\n\t\t\tfn(rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static void bt_for_each(struct blk_mq_hw_ctx *hctx,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t     \trq = blk_mq_tag_to_rq(hctx->tags, off + bit);\n\t\t\tif (rq->q == hctx->queue)\n\t\t\t\tfn(hctx, rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}",
                        "code_after_change": "static void bt_for_each(struct blk_mq_hw_ctx *hctx,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t\trq = hctx->tags->rqs[off + bit];\n\t\t\tif (rq->q == hctx->queue)\n\t\t\t\tfn(hctx, rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}",
                        "cve_id": "CVE-2015-9016"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic void bt_tags_for_each(struct blk_mq_tags *tags,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_tag_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tif (!tags->rqs)\n\t\treturn;\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t\trq = tags->rqs[off + bit];\n\t\t\tfn(rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic void bt_for_each(struct blk_mq_hw_ctx *hctx,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t     \trq = blk_mq_tag_to_rq(hctx->tags, off + bit);\n\t\t\tif (rq->q == hctx->queue)\n\t\t\t\tfn(hctx, rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic void bt_for_each(struct blk_mq_hw_ctx *hctx,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t\trq = hctx->tags->rqs[off + bit];\n\t\t\tif (rq->q == hctx->queue)\n\t\t\t\tfn(hctx, rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n{\n\tstruct request *rq = tags->rqs[tag];\n\t/* mq_ctx of flush rq is always cloned from the corresponding req */\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(rq->q, rq->mq_ctx);\n\n\tif (!is_flush_request(rq, fq, tag))\n\t\treturn rq;\n\n\treturn fq->flush_rq;\n}",
                        "code_after_change": "struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n{\n\treturn tags->rqs[tag];\n}",
                        "cve_id": "CVE-2015-9016"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic void bt_tags_for_each(struct blk_mq_tags *tags,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_tag_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tif (!tags->rqs)\n\t\treturn;\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t\trq = tags->rqs[off + bit];\n\t\t\tfn(rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}\n'''\nRelated vulnerability code snippet:\n'''\nstruct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n{\n\tstruct request *rq = tags->rqs[tag];\n\t/* mq_ctx of flush rq is always cloned from the corresponding req */\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(rq->q, rq->mq_ctx);\n\n\tif (!is_flush_request(rq, fq, tag))\n\t\treturn rq;\n\n\treturn fq->flush_rq;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstruct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)\n{\n\treturn tags->rqs[tag];\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static __init int hardware_setup(void)\n{\n\tint r = -ENOMEM, i, msr;\n\n\trdmsrl_safe(MSR_EFER, &host_efer);\n\n\tfor (i = 0; i < ARRAY_SIZE(vmx_msr_index); ++i)\n\t\tkvm_define_shared_msr(i, vmx_msr_index[i]);\n\n\tvmx_io_bitmap_a = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_io_bitmap_a)\n\t\treturn r;\n\n\tvmx_io_bitmap_b = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_io_bitmap_b)\n\t\tgoto out;\n\n\tvmx_msr_bitmap_legacy = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_legacy)\n\t\tgoto out1;\n\n\tvmx_msr_bitmap_legacy_x2apic =\n\t\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_legacy_x2apic)\n\t\tgoto out2;\n\n\tvmx_msr_bitmap_longmode = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_longmode)\n\t\tgoto out3;\n\n\tvmx_msr_bitmap_longmode_x2apic =\n\t\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_longmode_x2apic)\n\t\tgoto out4;\n\n\tif (nested) {\n\t\tvmx_msr_bitmap_nested =\n\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\t\tif (!vmx_msr_bitmap_nested)\n\t\t\tgoto out5;\n\t}\n\n\tvmx_vmread_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_vmread_bitmap)\n\t\tgoto out6;\n\n\tvmx_vmwrite_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_vmwrite_bitmap)\n\t\tgoto out7;\n\n\tmemset(vmx_vmread_bitmap, 0xff, PAGE_SIZE);\n\tmemset(vmx_vmwrite_bitmap, 0xff, PAGE_SIZE);\n\n\t/*\n\t * Allow direct access to the PC debug port (it is often used for I/O\n\t * delays, but the vmexits simply slow things down).\n\t */\n\tmemset(vmx_io_bitmap_a, 0xff, PAGE_SIZE);\n\tclear_bit(0x80, vmx_io_bitmap_a);\n\n\tmemset(vmx_io_bitmap_b, 0xff, PAGE_SIZE);\n\n\tmemset(vmx_msr_bitmap_legacy, 0xff, PAGE_SIZE);\n\tmemset(vmx_msr_bitmap_longmode, 0xff, PAGE_SIZE);\n\tif (nested)\n\t\tmemset(vmx_msr_bitmap_nested, 0xff, PAGE_SIZE);\n\n\tif (setup_vmcs_config(&vmcs_config) < 0) {\n\t\tr = -EIO;\n\t\tgoto out8;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_NX))\n\t\tkvm_enable_efer_bits(EFER_NX);\n\n\tif (!cpu_has_vmx_vpid())\n\t\tenable_vpid = 0;\n\tif (!cpu_has_vmx_shadow_vmcs())\n\t\tenable_shadow_vmcs = 0;\n\tif (enable_shadow_vmcs)\n\t\tinit_vmcs_shadow_fields();\n\n\tif (!cpu_has_vmx_ept() ||\n\t    !cpu_has_vmx_ept_4levels()) {\n\t\tenable_ept = 0;\n\t\tenable_unrestricted_guest = 0;\n\t\tenable_ept_ad_bits = 0;\n\t}\n\n\tif (!cpu_has_vmx_ept_ad_bits())\n\t\tenable_ept_ad_bits = 0;\n\n\tif (!cpu_has_vmx_unrestricted_guest())\n\t\tenable_unrestricted_guest = 0;\n\n\tif (!cpu_has_vmx_flexpriority())\n\t\tflexpriority_enabled = 0;\n\n\t/*\n\t * set_apic_access_page_addr() is used to reload apic access\n\t * page upon invalidation.  No need to do anything if not\n\t * using the APIC_ACCESS_ADDR VMCS field.\n\t */\n\tif (!flexpriority_enabled)\n\t\tkvm_x86_ops->set_apic_access_page_addr = NULL;\n\n\tif (!cpu_has_vmx_tpr_shadow())\n\t\tkvm_x86_ops->update_cr8_intercept = NULL;\n\n\tif (enable_ept && !cpu_has_vmx_ept_2m_page())\n\t\tkvm_disable_largepages();\n\n\tif (!cpu_has_vmx_ple())\n\t\tple_gap = 0;\n\n\tif (!cpu_has_vmx_apicv())\n\t\tenable_apicv = 0;\n\n\tif (cpu_has_vmx_tsc_scaling()) {\n\t\tkvm_has_tsc_control = true;\n\t\tkvm_max_tsc_scaling_ratio = KVM_VMX_TSC_MULTIPLIER_MAX;\n\t\tkvm_tsc_scaling_ratio_frac_bits = 48;\n\t}\n\n\tvmx_disable_intercept_for_msr(MSR_FS_BASE, false);\n\tvmx_disable_intercept_for_msr(MSR_GS_BASE, false);\n\tvmx_disable_intercept_for_msr(MSR_KERNEL_GS_BASE, true);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_CS, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_ESP, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_EIP, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_BNDCFGS, true);\n\n\tmemcpy(vmx_msr_bitmap_legacy_x2apic,\n\t\t\tvmx_msr_bitmap_legacy, PAGE_SIZE);\n\tmemcpy(vmx_msr_bitmap_longmode_x2apic,\n\t\t\tvmx_msr_bitmap_longmode, PAGE_SIZE);\n\n\tset_bit(0, vmx_vpid_bitmap); /* 0 is reserved for host */\n\n\tif (enable_apicv) {\n\t\tfor (msr = 0x800; msr <= 0x8ff; msr++)\n\t\t\tvmx_disable_intercept_msr_read_x2apic(msr);\n\n\t\t/* According SDM, in x2apic mode, the whole id reg is used.\n\t\t * But in KVM, it only use the highest eight bits. Need to\n\t\t * intercept it */\n\t\tvmx_enable_intercept_msr_read_x2apic(0x802);\n\t\t/* TMCCT */\n\t\tvmx_enable_intercept_msr_read_x2apic(0x839);\n\t\t/* TPR */\n\t\tvmx_disable_intercept_msr_write_x2apic(0x808);\n\t\t/* EOI */\n\t\tvmx_disable_intercept_msr_write_x2apic(0x80b);\n\t\t/* SELF-IPI */\n\t\tvmx_disable_intercept_msr_write_x2apic(0x83f);\n\t}\n\n\tif (enable_ept) {\n\t\tkvm_mmu_set_mask_ptes(0ull,\n\t\t\t(enable_ept_ad_bits) ? VMX_EPT_ACCESS_BIT : 0ull,\n\t\t\t(enable_ept_ad_bits) ? VMX_EPT_DIRTY_BIT : 0ull,\n\t\t\t0ull, VMX_EPT_EXECUTABLE_MASK);\n\t\tept_set_mmio_spte_mask();\n\t\tkvm_enable_tdp();\n\t} else\n\t\tkvm_disable_tdp();\n\n\tupdate_ple_window_actual_max();\n\n\t/*\n\t * Only enable PML when hardware supports PML feature, and both EPT\n\t * and EPT A/D bit features are enabled -- PML depends on them to work.\n\t */\n\tif (!enable_ept || !enable_ept_ad_bits || !cpu_has_vmx_pml())\n\t\tenable_pml = 0;\n\n\tif (!enable_pml) {\n\t\tkvm_x86_ops->slot_enable_log_dirty = NULL;\n\t\tkvm_x86_ops->slot_disable_log_dirty = NULL;\n\t\tkvm_x86_ops->flush_log_dirty = NULL;\n\t\tkvm_x86_ops->enable_log_dirty_pt_masked = NULL;\n\t}\n\n\tkvm_set_posted_intr_wakeup_handler(wakeup_handler);\n\n\treturn alloc_kvm_area();\n\nout8:\n\tfree_page((unsigned long)vmx_vmwrite_bitmap);\nout7:\n\tfree_page((unsigned long)vmx_vmread_bitmap);\nout6:\n\tif (nested)\n\t\tfree_page((unsigned long)vmx_msr_bitmap_nested);\nout5:\n\tfree_page((unsigned long)vmx_msr_bitmap_longmode_x2apic);\nout4:\n\tfree_page((unsigned long)vmx_msr_bitmap_longmode);\nout3:\n\tfree_page((unsigned long)vmx_msr_bitmap_legacy_x2apic);\nout2:\n\tfree_page((unsigned long)vmx_msr_bitmap_legacy);\nout1:\n\tfree_page((unsigned long)vmx_io_bitmap_b);\nout:\n\tfree_page((unsigned long)vmx_io_bitmap_a);\n\n    return r;\n}",
                        "code_after_change": "static __init int hardware_setup(void)\n{\n\tint r = -ENOMEM, i, msr;\n\n\trdmsrl_safe(MSR_EFER, &host_efer);\n\n\tfor (i = 0; i < ARRAY_SIZE(vmx_msr_index); ++i)\n\t\tkvm_define_shared_msr(i, vmx_msr_index[i]);\n\n\tvmx_io_bitmap_a = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_io_bitmap_a)\n\t\treturn r;\n\n\tvmx_io_bitmap_b = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_io_bitmap_b)\n\t\tgoto out;\n\n\tvmx_msr_bitmap_legacy = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_legacy)\n\t\tgoto out1;\n\n\tvmx_msr_bitmap_legacy_x2apic =\n\t\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_legacy_x2apic)\n\t\tgoto out2;\n\n\tvmx_msr_bitmap_longmode = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_longmode)\n\t\tgoto out3;\n\n\tvmx_msr_bitmap_longmode_x2apic =\n\t\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_longmode_x2apic)\n\t\tgoto out4;\n\n\tif (nested) {\n\t\tvmx_msr_bitmap_nested =\n\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\t\tif (!vmx_msr_bitmap_nested)\n\t\t\tgoto out5;\n\t}\n\n\tvmx_vmread_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_vmread_bitmap)\n\t\tgoto out6;\n\n\tvmx_vmwrite_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_vmwrite_bitmap)\n\t\tgoto out7;\n\n\tmemset(vmx_vmread_bitmap, 0xff, PAGE_SIZE);\n\tmemset(vmx_vmwrite_bitmap, 0xff, PAGE_SIZE);\n\n\t/*\n\t * Allow direct access to the PC debug port (it is often used for I/O\n\t * delays, but the vmexits simply slow things down).\n\t */\n\tmemset(vmx_io_bitmap_a, 0xff, PAGE_SIZE);\n\tclear_bit(0x80, vmx_io_bitmap_a);\n\n\tmemset(vmx_io_bitmap_b, 0xff, PAGE_SIZE);\n\n\tmemset(vmx_msr_bitmap_legacy, 0xff, PAGE_SIZE);\n\tmemset(vmx_msr_bitmap_longmode, 0xff, PAGE_SIZE);\n\tif (nested)\n\t\tmemset(vmx_msr_bitmap_nested, 0xff, PAGE_SIZE);\n\n\tif (setup_vmcs_config(&vmcs_config) < 0) {\n\t\tr = -EIO;\n\t\tgoto out8;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_NX))\n\t\tkvm_enable_efer_bits(EFER_NX);\n\n\tif (!cpu_has_vmx_vpid())\n\t\tenable_vpid = 0;\n\tif (!cpu_has_vmx_shadow_vmcs())\n\t\tenable_shadow_vmcs = 0;\n\tif (enable_shadow_vmcs)\n\t\tinit_vmcs_shadow_fields();\n\n\tif (!cpu_has_vmx_ept() ||\n\t    !cpu_has_vmx_ept_4levels()) {\n\t\tenable_ept = 0;\n\t\tenable_unrestricted_guest = 0;\n\t\tenable_ept_ad_bits = 0;\n\t}\n\n\tif (!cpu_has_vmx_ept_ad_bits())\n\t\tenable_ept_ad_bits = 0;\n\n\tif (!cpu_has_vmx_unrestricted_guest())\n\t\tenable_unrestricted_guest = 0;\n\n\tif (!cpu_has_vmx_flexpriority())\n\t\tflexpriority_enabled = 0;\n\n\t/*\n\t * set_apic_access_page_addr() is used to reload apic access\n\t * page upon invalidation.  No need to do anything if not\n\t * using the APIC_ACCESS_ADDR VMCS field.\n\t */\n\tif (!flexpriority_enabled)\n\t\tkvm_x86_ops->set_apic_access_page_addr = NULL;\n\n\tif (!cpu_has_vmx_tpr_shadow())\n\t\tkvm_x86_ops->update_cr8_intercept = NULL;\n\n\tif (enable_ept && !cpu_has_vmx_ept_2m_page())\n\t\tkvm_disable_largepages();\n\n\tif (!cpu_has_vmx_ple())\n\t\tple_gap = 0;\n\n\tif (!cpu_has_vmx_apicv())\n\t\tenable_apicv = 0;\n\n\tif (cpu_has_vmx_tsc_scaling()) {\n\t\tkvm_has_tsc_control = true;\n\t\tkvm_max_tsc_scaling_ratio = KVM_VMX_TSC_MULTIPLIER_MAX;\n\t\tkvm_tsc_scaling_ratio_frac_bits = 48;\n\t}\n\n\tvmx_disable_intercept_for_msr(MSR_FS_BASE, false);\n\tvmx_disable_intercept_for_msr(MSR_GS_BASE, false);\n\tvmx_disable_intercept_for_msr(MSR_KERNEL_GS_BASE, true);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_CS, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_ESP, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_EIP, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_BNDCFGS, true);\n\n\tmemcpy(vmx_msr_bitmap_legacy_x2apic,\n\t\t\tvmx_msr_bitmap_legacy, PAGE_SIZE);\n\tmemcpy(vmx_msr_bitmap_longmode_x2apic,\n\t\t\tvmx_msr_bitmap_longmode, PAGE_SIZE);\n\n\tset_bit(0, vmx_vpid_bitmap); /* 0 is reserved for host */\n\n\tfor (msr = 0x800; msr <= 0x8ff; msr++)\n\t\tvmx_disable_intercept_msr_read_x2apic(msr);\n\n\t/* According SDM, in x2apic mode, the whole id reg is used.  But in\n\t * KVM, it only use the highest eight bits. Need to intercept it */\n\tvmx_enable_intercept_msr_read_x2apic(0x802);\n\t/* TMCCT */\n\tvmx_enable_intercept_msr_read_x2apic(0x839);\n\t/* TPR */\n\tvmx_disable_intercept_msr_write_x2apic(0x808);\n\t/* EOI */\n\tvmx_disable_intercept_msr_write_x2apic(0x80b);\n\t/* SELF-IPI */\n\tvmx_disable_intercept_msr_write_x2apic(0x83f);\n\n\tif (enable_ept) {\n\t\tkvm_mmu_set_mask_ptes(0ull,\n\t\t\t(enable_ept_ad_bits) ? VMX_EPT_ACCESS_BIT : 0ull,\n\t\t\t(enable_ept_ad_bits) ? VMX_EPT_DIRTY_BIT : 0ull,\n\t\t\t0ull, VMX_EPT_EXECUTABLE_MASK);\n\t\tept_set_mmio_spte_mask();\n\t\tkvm_enable_tdp();\n\t} else\n\t\tkvm_disable_tdp();\n\n\tupdate_ple_window_actual_max();\n\n\t/*\n\t * Only enable PML when hardware supports PML feature, and both EPT\n\t * and EPT A/D bit features are enabled -- PML depends on them to work.\n\t */\n\tif (!enable_ept || !enable_ept_ad_bits || !cpu_has_vmx_pml())\n\t\tenable_pml = 0;\n\n\tif (!enable_pml) {\n\t\tkvm_x86_ops->slot_enable_log_dirty = NULL;\n\t\tkvm_x86_ops->slot_disable_log_dirty = NULL;\n\t\tkvm_x86_ops->flush_log_dirty = NULL;\n\t\tkvm_x86_ops->enable_log_dirty_pt_masked = NULL;\n\t}\n\n\tkvm_set_posted_intr_wakeup_handler(wakeup_handler);\n\n\treturn alloc_kvm_area();\n\nout8:\n\tfree_page((unsigned long)vmx_vmwrite_bitmap);\nout7:\n\tfree_page((unsigned long)vmx_vmread_bitmap);\nout6:\n\tif (nested)\n\t\tfree_page((unsigned long)vmx_msr_bitmap_nested);\nout5:\n\tfree_page((unsigned long)vmx_msr_bitmap_longmode_x2apic);\nout4:\n\tfree_page((unsigned long)vmx_msr_bitmap_longmode);\nout3:\n\tfree_page((unsigned long)vmx_msr_bitmap_legacy_x2apic);\nout2:\n\tfree_page((unsigned long)vmx_msr_bitmap_legacy);\nout1:\n\tfree_page((unsigned long)vmx_io_bitmap_b);\nout:\n\tfree_page((unsigned long)vmx_io_bitmap_a);\n\n    return r;\n}",
                        "cve_id": "CVE-2016-4440"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic void bt_tags_for_each(struct blk_mq_tags *tags,\n\t\tstruct blk_mq_bitmap_tags *bt, unsigned int off,\n\t\tbusy_tag_iter_fn *fn, void *data, bool reserved)\n{\n\tstruct request *rq;\n\tint bit, i;\n\n\tif (!tags->rqs)\n\t\treturn;\n\tfor (i = 0; i < bt->map_nr; i++) {\n\t\tstruct blk_align_bitmap *bm = &bt->map[i];\n\n\t\tfor (bit = find_first_bit(&bm->word, bm->depth);\n\t\t     bit < bm->depth;\n\t\t     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {\n\t\t\trq = tags->rqs[off + bit];\n\t\t\tfn(rq, data, reserved);\n\t\t}\n\n\t\toff += (1 << bt->bits_per_word);\n\t}\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic __init int hardware_setup(void)\n{\n\tint r = -ENOMEM, i, msr;\n\n\trdmsrl_safe(MSR_EFER, &host_efer);\n\n\tfor (i = 0; i < ARRAY_SIZE(vmx_msr_index); ++i)\n\t\tkvm_define_shared_msr(i, vmx_msr_index[i]);\n\n\tvmx_io_bitmap_a = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_io_bitmap_a)\n\t\treturn r;\n\n\tvmx_io_bitmap_b = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_io_bitmap_b)\n\t\tgoto out;\n\n\tvmx_msr_bitmap_legacy = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_legacy)\n\t\tgoto out1;\n\n\tvmx_msr_bitmap_legacy_x2apic =\n\t\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_legacy_x2apic)\n\t\tgoto out2;\n\n\tvmx_msr_bitmap_longmode = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_longmode)\n\t\tgoto out3;\n\n\tvmx_msr_bitmap_longmode_x2apic =\n\t\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_longmode_x2apic)\n\t\tgoto out4;\n\n\tif (nested) {\n\t\tvmx_msr_bitmap_nested =\n\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\t\tif (!vmx_msr_bitmap_nested)\n\t\t\tgoto out5;\n\t}\n\n\tvmx_vmread_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_vmread_bitmap)\n\t\tgoto out6;\n\n\tvmx_vmwrite_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_vmwrite_bitmap)\n\t\tgoto out7;\n\n\tmemset(vmx_vmread_bitmap, 0xff, PAGE_SIZE);\n\tmemset(vmx_vmwrite_bitmap, 0xff, PAGE_SIZE);\n\n\t/*\n\t * Allow direct access to the PC debug port (it is often used for I/O\n\t * delays, but the vmexits simply slow things down).\n\t */\n\tmemset(vmx_io_bitmap_a, 0xff, PAGE_SIZE);\n\tclear_bit(0x80, vmx_io_bitmap_a);\n\n\tmemset(vmx_io_bitmap_b, 0xff, PAGE_SIZE);\n\n\tmemset(vmx_msr_bitmap_legacy, 0xff, PAGE_SIZE);\n\tmemset(vmx_msr_bitmap_longmode, 0xff, PAGE_SIZE);\n\tif (nested)\n\t\tmemset(vmx_msr_bitmap_nested, 0xff, PAGE_SIZE);\n\n\tif (setup_vmcs_config(&vmcs_config) < 0) {\n\t\tr = -EIO;\n\t\tgoto out8;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_NX))\n\t\tkvm_enable_efer_bits(EFER_NX);\n\n\tif (!cpu_has_vmx_vpid())\n\t\tenable_vpid = 0;\n\tif (!cpu_has_vmx_shadow_vmcs())\n\t\tenable_shadow_vmcs = 0;\n\tif (enable_shadow_vmcs)\n\t\tinit_vmcs_shadow_fields();\n\n\tif (!cpu_has_vmx_ept() ||\n\t    !cpu_has_vmx_ept_4levels()) {\n\t\tenable_ept = 0;\n\t\tenable_unrestricted_guest = 0;\n\t\tenable_ept_ad_bits = 0;\n\t}\n\n\tif (!cpu_has_vmx_ept_ad_bits())\n\t\tenable_ept_ad_bits = 0;\n\n\tif (!cpu_has_vmx_unrestricted_guest())\n\t\tenable_unrestricted_guest = 0;\n\n\tif (!cpu_has_vmx_flexpriority())\n\t\tflexpriority_enabled = 0;\n\n\t/*\n\t * set_apic_access_page_addr() is used to reload apic access\n\t * page upon invalidation.  No need to do anything if not\n\t * using the APIC_ACCESS_ADDR VMCS field.\n\t */\n\tif (!flexpriority_enabled)\n\t\tkvm_x86_ops->set_apic_access_page_addr = NULL;\n\n\tif (!cpu_has_vmx_tpr_shadow())\n\t\tkvm_x86_ops->update_cr8_intercept = NULL;\n\n\tif (enable_ept && !cpu_has_vmx_ept_2m_page())\n\t\tkvm_disable_largepages();\n\n\tif (!cpu_has_vmx_ple())\n\t\tple_gap = 0;\n\n\tif (!cpu_has_vmx_apicv())\n\t\tenable_apicv = 0;\n\n\tif (cpu_has_vmx_tsc_scaling()) {\n\t\tkvm_has_tsc_control = true;\n\t\tkvm_max_tsc_scaling_ratio = KVM_VMX_TSC_MULTIPLIER_MAX;\n\t\tkvm_tsc_scaling_ratio_frac_bits = 48;\n\t}\n\n\tvmx_disable_intercept_for_msr(MSR_FS_BASE, false);\n\tvmx_disable_intercept_for_msr(MSR_GS_BASE, false);\n\tvmx_disable_intercept_for_msr(MSR_KERNEL_GS_BASE, true);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_CS, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_ESP, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_EIP, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_BNDCFGS, true);\n\n\tmemcpy(vmx_msr_bitmap_legacy_x2apic,\n\t\t\tvmx_msr_bitmap_legacy, PAGE_SIZE);\n\tmemcpy(vmx_msr_bitmap_longmode_x2apic,\n\t\t\tvmx_msr_bitmap_longmode, PAGE_SIZE);\n\n\tset_bit(0, vmx_vpid_bitmap); /* 0 is reserved for host */\n\n\tif (enable_apicv) {\n\t\tfor (msr = 0x800; msr <= 0x8ff; msr++)\n\t\t\tvmx_disable_intercept_msr_read_x2apic(msr);\n\n\t\t/* According SDM, in x2apic mode, the whole id reg is used.\n\t\t * But in KVM, it only use the highest eight bits. Need to\n\t\t * intercept it */\n\t\tvmx_enable_intercept_msr_read_x2apic(0x802);\n\t\t/* TMCCT */\n\t\tvmx_enable_intercept_msr_read_x2apic(0x839);\n\t\t/* TPR */\n\t\tvmx_disable_intercept_msr_write_x2apic(0x808);\n\t\t/* EOI */\n\t\tvmx_disable_intercept_msr_write_x2apic(0x80b);\n\t\t/* SELF-IPI */\n\t\tvmx_disable_intercept_msr_write_x2apic(0x83f);\n\t}\n\n\tif (enable_ept) {\n\t\tkvm_mmu_set_mask_ptes(0ull,\n\t\t\t(enable_ept_ad_bits) ? VMX_EPT_ACCESS_BIT : 0ull,\n\t\t\t(enable_ept_ad_bits) ? VMX_EPT_DIRTY_BIT : 0ull,\n\t\t\t0ull, VMX_EPT_EXECUTABLE_MASK);\n\t\tept_set_mmio_spte_mask();\n\t\tkvm_enable_tdp();\n\t} else\n\t\tkvm_disable_tdp();\n\n\tupdate_ple_window_actual_max();\n\n\t/*\n\t * Only enable PML when hardware supports PML feature, and both EPT\n\t * and EPT A/D bit features are enabled -- PML depends on them to work.\n\t */\n\tif (!enable_ept || !enable_ept_ad_bits || !cpu_has_vmx_pml())\n\t\tenable_pml = 0;\n\n\tif (!enable_pml) {\n\t\tkvm_x86_ops->slot_enable_log_dirty = NULL;\n\t\tkvm_x86_ops->slot_disable_log_dirty = NULL;\n\t\tkvm_x86_ops->flush_log_dirty = NULL;\n\t\tkvm_x86_ops->enable_log_dirty_pt_masked = NULL;\n\t}\n\n\tkvm_set_posted_intr_wakeup_handler(wakeup_handler);\n\n\treturn alloc_kvm_area();\n\nout8:\n\tfree_page((unsigned long)vmx_vmwrite_bitmap);\nout7:\n\tfree_page((unsigned long)vmx_vmread_bitmap);\nout6:\n\tif (nested)\n\t\tfree_page((unsigned long)vmx_msr_bitmap_nested);\nout5:\n\tfree_page((unsigned long)vmx_msr_bitmap_longmode_x2apic);\nout4:\n\tfree_page((unsigned long)vmx_msr_bitmap_longmode);\nout3:\n\tfree_page((unsigned long)vmx_msr_bitmap_legacy_x2apic);\nout2:\n\tfree_page((unsigned long)vmx_msr_bitmap_legacy);\nout1:\n\tfree_page((unsigned long)vmx_io_bitmap_b);\nout:\n\tfree_page((unsigned long)vmx_io_bitmap_a);\n\n    return r;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic __init int hardware_setup(void)\n{\n\tint r = -ENOMEM, i, msr;\n\n\trdmsrl_safe(MSR_EFER, &host_efer);\n\n\tfor (i = 0; i < ARRAY_SIZE(vmx_msr_index); ++i)\n\t\tkvm_define_shared_msr(i, vmx_msr_index[i]);\n\n\tvmx_io_bitmap_a = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_io_bitmap_a)\n\t\treturn r;\n\n\tvmx_io_bitmap_b = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_io_bitmap_b)\n\t\tgoto out;\n\n\tvmx_msr_bitmap_legacy = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_legacy)\n\t\tgoto out1;\n\n\tvmx_msr_bitmap_legacy_x2apic =\n\t\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_legacy_x2apic)\n\t\tgoto out2;\n\n\tvmx_msr_bitmap_longmode = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_longmode)\n\t\tgoto out3;\n\n\tvmx_msr_bitmap_longmode_x2apic =\n\t\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_msr_bitmap_longmode_x2apic)\n\t\tgoto out4;\n\n\tif (nested) {\n\t\tvmx_msr_bitmap_nested =\n\t\t\t(unsigned long *)__get_free_page(GFP_KERNEL);\n\t\tif (!vmx_msr_bitmap_nested)\n\t\t\tgoto out5;\n\t}\n\n\tvmx_vmread_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_vmread_bitmap)\n\t\tgoto out6;\n\n\tvmx_vmwrite_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\tif (!vmx_vmwrite_bitmap)\n\t\tgoto out7;\n\n\tmemset(vmx_vmread_bitmap, 0xff, PAGE_SIZE);\n\tmemset(vmx_vmwrite_bitmap, 0xff, PAGE_SIZE);\n\n\t/*\n\t * Allow direct access to the PC debug port (it is often used for I/O\n\t * delays, but the vmexits simply slow things down).\n\t */\n\tmemset(vmx_io_bitmap_a, 0xff, PAGE_SIZE);\n\tclear_bit(0x80, vmx_io_bitmap_a);\n\n\tmemset(vmx_io_bitmap_b, 0xff, PAGE_SIZE);\n\n\tmemset(vmx_msr_bitmap_legacy, 0xff, PAGE_SIZE);\n\tmemset(vmx_msr_bitmap_longmode, 0xff, PAGE_SIZE);\n\tif (nested)\n\t\tmemset(vmx_msr_bitmap_nested, 0xff, PAGE_SIZE);\n\n\tif (setup_vmcs_config(&vmcs_config) < 0) {\n\t\tr = -EIO;\n\t\tgoto out8;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_NX))\n\t\tkvm_enable_efer_bits(EFER_NX);\n\n\tif (!cpu_has_vmx_vpid())\n\t\tenable_vpid = 0;\n\tif (!cpu_has_vmx_shadow_vmcs())\n\t\tenable_shadow_vmcs = 0;\n\tif (enable_shadow_vmcs)\n\t\tinit_vmcs_shadow_fields();\n\n\tif (!cpu_has_vmx_ept() ||\n\t    !cpu_has_vmx_ept_4levels()) {\n\t\tenable_ept = 0;\n\t\tenable_unrestricted_guest = 0;\n\t\tenable_ept_ad_bits = 0;\n\t}\n\n\tif (!cpu_has_vmx_ept_ad_bits())\n\t\tenable_ept_ad_bits = 0;\n\n\tif (!cpu_has_vmx_unrestricted_guest())\n\t\tenable_unrestricted_guest = 0;\n\n\tif (!cpu_has_vmx_flexpriority())\n\t\tflexpriority_enabled = 0;\n\n\t/*\n\t * set_apic_access_page_addr() is used to reload apic access\n\t * page upon invalidation.  No need to do anything if not\n\t * using the APIC_ACCESS_ADDR VMCS field.\n\t */\n\tif (!flexpriority_enabled)\n\t\tkvm_x86_ops->set_apic_access_page_addr = NULL;\n\n\tif (!cpu_has_vmx_tpr_shadow())\n\t\tkvm_x86_ops->update_cr8_intercept = NULL;\n\n\tif (enable_ept && !cpu_has_vmx_ept_2m_page())\n\t\tkvm_disable_largepages();\n\n\tif (!cpu_has_vmx_ple())\n\t\tple_gap = 0;\n\n\tif (!cpu_has_vmx_apicv())\n\t\tenable_apicv = 0;\n\n\tif (cpu_has_vmx_tsc_scaling()) {\n\t\tkvm_has_tsc_control = true;\n\t\tkvm_max_tsc_scaling_ratio = KVM_VMX_TSC_MULTIPLIER_MAX;\n\t\tkvm_tsc_scaling_ratio_frac_bits = 48;\n\t}\n\n\tvmx_disable_intercept_for_msr(MSR_FS_BASE, false);\n\tvmx_disable_intercept_for_msr(MSR_GS_BASE, false);\n\tvmx_disable_intercept_for_msr(MSR_KERNEL_GS_BASE, true);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_CS, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_ESP, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_SYSENTER_EIP, false);\n\tvmx_disable_intercept_for_msr(MSR_IA32_BNDCFGS, true);\n\n\tmemcpy(vmx_msr_bitmap_legacy_x2apic,\n\t\t\tvmx_msr_bitmap_legacy, PAGE_SIZE);\n\tmemcpy(vmx_msr_bitmap_longmode_x2apic,\n\t\t\tvmx_msr_bitmap_longmode, PAGE_SIZE);\n\n\tset_bit(0, vmx_vpid_bitmap); /* 0 is reserved for host */\n\n\tfor (msr = 0x800; msr <= 0x8ff; msr++)\n\t\tvmx_disable_intercept_msr_read_x2apic(msr);\n\n\t/* According SDM, in x2apic mode, the whole id reg is used.  But in\n\t * KVM, it only use the highest eight bits. Need to intercept it */\n\tvmx_enable_intercept_msr_read_x2apic(0x802);\n\t/* TMCCT */\n\tvmx_enable_intercept_msr_read_x2apic(0x839);\n\t/* TPR */\n\tvmx_disable_intercept_msr_write_x2apic(0x808);\n\t/* EOI */\n\tvmx_disable_intercept_msr_write_x2apic(0x80b);\n\t/* SELF-IPI */\n\tvmx_disable_intercept_msr_write_x2apic(0x83f);\n\n\tif (enable_ept) {\n\t\tkvm_mmu_set_mask_ptes(0ull,\n\t\t\t(enable_ept_ad_bits) ? VMX_EPT_ACCESS_BIT : 0ull,\n\t\t\t(enable_ept_ad_bits) ? VMX_EPT_DIRTY_BIT : 0ull,\n\t\t\t0ull, VMX_EPT_EXECUTABLE_MASK);\n\t\tept_set_mmio_spte_mask();\n\t\tkvm_enable_tdp();\n\t} else\n\t\tkvm_disable_tdp();\n\n\tupdate_ple_window_actual_max();\n\n\t/*\n\t * Only enable PML when hardware supports PML feature, and both EPT\n\t * and EPT A/D bit features are enabled -- PML depends on them to work.\n\t */\n\tif (!enable_ept || !enable_ept_ad_bits || !cpu_has_vmx_pml())\n\t\tenable_pml = 0;\n\n\tif (!enable_pml) {\n\t\tkvm_x86_ops->slot_enable_log_dirty = NULL;\n\t\tkvm_x86_ops->slot_disable_log_dirty = NULL;\n\t\tkvm_x86_ops->flush_log_dirty = NULL;\n\t\tkvm_x86_ops->enable_log_dirty_pt_masked = NULL;\n\t}\n\n\tkvm_set_posted_intr_wakeup_handler(wakeup_handler);\n\n\treturn alloc_kvm_area();\n\nout8:\n\tfree_page((unsigned long)vmx_vmwrite_bitmap);\nout7:\n\tfree_page((unsigned long)vmx_vmread_bitmap);\nout6:\n\tif (nested)\n\t\tfree_page((unsigned long)vmx_msr_bitmap_nested);\nout5:\n\tfree_page((unsigned long)vmx_msr_bitmap_longmode_x2apic);\nout4:\n\tfree_page((unsigned long)vmx_msr_bitmap_longmode);\nout3:\n\tfree_page((unsigned long)vmx_msr_bitmap_legacy_x2apic);\nout2:\n\tfree_page((unsigned long)vmx_msr_bitmap_legacy);\nout1:\n\tfree_page((unsigned long)vmx_io_bitmap_b);\nout:\n\tfree_page((unsigned long)vmx_io_bitmap_a);\n\n    return r;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 153,
            "cve_id": "CVE-2013-0268",
            "code_snippet": "static int msr_open(struct inode *inode, struct file *file)\n{\n\tunsigned int cpu;\n\tstruct cpuinfo_x86 *c;\n\n\tif (!capable(CAP_SYS_RAWIO))\n\t\treturn -EPERM;\n\n\tcpu = iminor(file->f_path.dentry->d_inode);\n\tif (cpu >= nr_cpu_ids || !cpu_online(cpu))\n\t\treturn -ENXIO;\t/* No such CPU */\n\n\tc = &cpu_data(cpu);\n\tif (!cpu_has(c, X86_FEATURE_MSR))\n\t\treturn -EIO;\t/* MSR not supported */\n\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": " */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx;\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tstruct perf_event_context *gctx = group_leader->ctx;\n\n\t\tmutex_lock(&gctx->mutex);\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\n\tif (move_group) {\n\t\tsynchronize_rcu();\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}",
                        "code_after_change": " */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *uninitialized_var(gctx);\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tgctx = group_leader->ctx;\n\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t} else {\n\t\tmutex_lock(&ctx->mutex);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\tif (move_group) {\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\n\tif (move_group) {\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}",
                        "cve_id": "CVE-2016-6786"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int msr_open(struct inode *inode, struct file *file)\n{\n\tunsigned int cpu;\n\tstruct cpuinfo_x86 *c;\n\n\tif (!capable(CAP_SYS_RAWIO))\n\t\treturn -EPERM;\n\n\tcpu = iminor(file->f_path.dentry->d_inode);\n\tif (cpu >= nr_cpu_ids || !cpu_online(cpu))\n\t\treturn -ENXIO;\t/* No such CPU */\n\n\tc = &cpu_data(cpu);\n\tif (!cpu_has(c, X86_FEATURE_MSR))\n\t\treturn -EIO;\t/* MSR not supported */\n\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\n */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx;\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tstruct perf_event_context *gctx = group_leader->ctx;\n\n\t\tmutex_lock(&gctx->mutex);\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\tmutex_lock(&ctx->mutex);\n\n\tif (move_group) {\n\t\tsynchronize_rcu();\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\n */\nSYSCALL_DEFINE5(perf_event_open,\n\t\tstruct perf_event_attr __user *, attr_uptr,\n\t\tpid_t, pid, int, cpu, int, group_fd, unsigned long, flags)\n{\n\tstruct perf_event *group_leader = NULL, *output_event = NULL;\n\tstruct perf_event *event, *sibling;\n\tstruct perf_event_attr attr;\n\tstruct perf_event_context *ctx, *uninitialized_var(gctx);\n\tstruct file *event_file = NULL;\n\tstruct fd group = {NULL, 0};\n\tstruct task_struct *task = NULL;\n\tstruct pmu *pmu;\n\tint event_fd;\n\tint move_group = 0;\n\tint err;\n\tint f_flags = O_RDWR;\n\n\t/* for future expandability... */\n\tif (flags & ~PERF_FLAG_ALL)\n\t\treturn -EINVAL;\n\n\terr = perf_copy_attr(attr_uptr, &attr);\n\tif (err)\n\t\treturn err;\n\n\tif (!attr.exclude_kernel) {\n\t\tif (perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\tif (attr.freq) {\n\t\tif (attr.sample_freq > sysctl_perf_event_sample_rate)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr.sample_period & (1ULL << 63))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * In cgroup mode, the pid argument is used to pass the fd\n\t * opened to the cgroup directory in cgroupfs. The cpu argument\n\t * designates the cpu on which to monitor threads from that\n\t * cgroup.\n\t */\n\tif ((flags & PERF_FLAG_PID_CGROUP) && (pid == -1 || cpu == -1))\n\t\treturn -EINVAL;\n\n\tif (flags & PERF_FLAG_FD_CLOEXEC)\n\t\tf_flags |= O_CLOEXEC;\n\n\tevent_fd = get_unused_fd_flags(f_flags);\n\tif (event_fd < 0)\n\t\treturn event_fd;\n\n\tif (group_fd != -1) {\n\t\terr = perf_fget_light(group_fd, &group);\n\t\tif (err)\n\t\t\tgoto err_fd;\n\t\tgroup_leader = group.file->private_data;\n\t\tif (flags & PERF_FLAG_FD_OUTPUT)\n\t\t\toutput_event = group_leader;\n\t\tif (flags & PERF_FLAG_FD_NO_GROUP)\n\t\t\tgroup_leader = NULL;\n\t}\n\n\tif (pid != -1 && !(flags & PERF_FLAG_PID_CGROUP)) {\n\t\ttask = find_lively_task_by_vpid(pid);\n\t\tif (IS_ERR(task)) {\n\t\t\terr = PTR_ERR(task);\n\t\t\tgoto err_group_fd;\n\t\t}\n\t}\n\n\tif (task && group_leader &&\n\t    group_leader->attr.inherit != attr.inherit) {\n\t\terr = -EINVAL;\n\t\tgoto err_task;\n\t}\n\n\tget_online_cpus();\n\n\tevent = perf_event_alloc(&attr, cpu, task, group_leader, NULL,\n\t\t\t\t NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\terr = PTR_ERR(event);\n\t\tgoto err_cpus;\n\t}\n\n\tif (flags & PERF_FLAG_PID_CGROUP) {\n\t\terr = perf_cgroup_connect(pid, event, &attr, group_leader);\n\t\tif (err) {\n\t\t\t__free_event(event);\n\t\t\tgoto err_cpus;\n\t\t}\n\t}\n\n\tif (is_sampling_event(event)) {\n\t\tif (event->pmu->capabilities & PERF_PMU_CAP_NO_INTERRUPT) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto err_alloc;\n\t\t}\n\t}\n\n\taccount_event(event);\n\n\t/*\n\t * Special case software events and allow them to be part of\n\t * any hardware group.\n\t */\n\tpmu = event->pmu;\n\n\tif (group_leader &&\n\t    (is_software_event(event) != is_software_event(group_leader))) {\n\t\tif (is_software_event(event)) {\n\t\t\t/*\n\t\t\t * If event and group_leader are not both a software\n\t\t\t * event, and event is, then group leader is not.\n\t\t\t *\n\t\t\t * Allow the addition of software events to !software\n\t\t\t * groups, this is safe because software events never\n\t\t\t * fail to schedule.\n\t\t\t */\n\t\t\tpmu = group_leader->pmu;\n\t\t} else if (is_software_event(group_leader) &&\n\t\t\t   (group_leader->group_flags & PERF_GROUP_SOFTWARE)) {\n\t\t\t/*\n\t\t\t * In case the group is a pure software group, and we\n\t\t\t * try to add a hardware event, move the whole group to\n\t\t\t * the hardware context.\n\t\t\t */\n\t\t\tmove_group = 1;\n\t\t}\n\t}\n\n\t/*\n\t * Get the target context (task or percpu):\n\t */\n\tctx = find_get_context(pmu, task, event->cpu);\n\tif (IS_ERR(ctx)) {\n\t\terr = PTR_ERR(ctx);\n\t\tgoto err_alloc;\n\t}\n\n\tif (task) {\n\t\tput_task_struct(task);\n\t\ttask = NULL;\n\t}\n\n\t/*\n\t * Look up the group leader (we will attach this event to it):\n\t */\n\tif (group_leader) {\n\t\terr = -EINVAL;\n\n\t\t/*\n\t\t * Do not allow a recursive hierarchy (this new sibling\n\t\t * becoming part of another group-sibling):\n\t\t */\n\t\tif (group_leader->group_leader != group_leader)\n\t\t\tgoto err_context;\n\t\t/*\n\t\t * Do not allow to attach to a group in a different\n\t\t * task or CPU context:\n\t\t */\n\t\tif (move_group) {\n\t\t\t/*\n\t\t\t * Make sure we're both on the same task, or both\n\t\t\t * per-cpu events.\n\t\t\t */\n\t\t\tif (group_leader->ctx->task != ctx->task)\n\t\t\t\tgoto err_context;\n\n\t\t\t/*\n\t\t\t * Make sure we're both events for the same CPU;\n\t\t\t * grouping events for different CPUs is broken; since\n\t\t\t * you can never concurrently schedule them anyhow.\n\t\t\t */\n\t\t\tif (group_leader->cpu != event->cpu)\n\t\t\t\tgoto err_context;\n\t\t} else {\n\t\t\tif (group_leader->ctx != ctx)\n\t\t\t\tgoto err_context;\n\t\t}\n\n\t\t/*\n\t\t * Only a group leader can be exclusive or pinned\n\t\t */\n\t\tif (attr.exclusive || attr.pinned)\n\t\t\tgoto err_context;\n\t}\n\n\tif (output_event) {\n\t\terr = perf_event_set_output(event, output_event);\n\t\tif (err)\n\t\t\tgoto err_context;\n\t}\n\n\tevent_file = anon_inode_getfile(\"[perf_event]\", &perf_fops, event,\n\t\t\t\t\tf_flags);\n\tif (IS_ERR(event_file)) {\n\t\terr = PTR_ERR(event_file);\n\t\tgoto err_context;\n\t}\n\n\tif (move_group) {\n\t\tgctx = group_leader->ctx;\n\n\t\t/*\n\t\t * See perf_event_ctx_lock() for comments on the details\n\t\t * of swizzling perf_event::ctx.\n\t\t */\n\t\tmutex_lock_double(&gctx->mutex, &ctx->mutex);\n\n\t\tperf_remove_from_context(group_leader, false);\n\n\t\t/*\n\t\t * Removing from the context ends up with disabled\n\t\t * event. What we want here is event in the initial\n\t\t * startup state, ready to be add into new context.\n\t\t */\n\t\tperf_event__state_init(group_leader);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_remove_from_context(sibling, false);\n\t\t\tperf_event__state_init(sibling);\n\t\t\tput_ctx(gctx);\n\t\t}\n\t} else {\n\t\tmutex_lock(&ctx->mutex);\n\t}\n\n\tWARN_ON_ONCE(ctx->parent_ctx);\n\n\tif (move_group) {\n\t\t/*\n\t\t * Wait for everybody to stop referencing the events through\n\t\t * the old lists, before installing it on new lists.\n\t\t */\n\t\tsynchronize_rcu();\n\n\t\tperf_install_in_context(ctx, group_leader, group_leader->cpu);\n\t\tget_ctx(ctx);\n\t\tlist_for_each_entry(sibling, &group_leader->sibling_list,\n\t\t\t\t    group_entry) {\n\t\t\tperf_install_in_context(ctx, sibling, sibling->cpu);\n\t\t\tget_ctx(ctx);\n\t\t}\n\t}\n\n\tperf_install_in_context(ctx, event, event->cpu);\n\tperf_unpin_context(ctx);\n\n\tif (move_group) {\n\t\tmutex_unlock(&gctx->mutex);\n\t\tput_ctx(gctx);\n\t}\n\tmutex_unlock(&ctx->mutex);\n\n\tput_online_cpus();\n\n\tevent->owner = current;\n\n\tmutex_lock(&current->perf_event_mutex);\n\tlist_add_tail(&event->owner_entry, &current->perf_event_list);\n\tmutex_unlock(&current->perf_event_mutex);\n\n\t/*\n\t * Precalculate sample_data sizes\n\t */\n\tperf_event__header_size(event);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Drop the reference on the group_event after placing the\n\t * new event on the sibling_list. This ensures destruction\n\t * of the group leader will find the pointer to itself in\n\t * perf_group_detach().\n\t */\n\tfdput(group);\n\tfd_install(event_fd, event_file);\n\treturn event_fd;\n\nerr_context:\n\tperf_unpin_context(ctx);\n\tput_ctx(ctx);\nerr_alloc:\n\tfree_event(event);\nerr_cpus:\n\tput_online_cpus();\nerr_task:\n\tif (task)\n\t\tput_task_struct(task);\nerr_group_fd:\n\tfdput(group);\nerr_fd:\n\tput_unused_fd(event_fd);\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static int hfsplus_readdir(struct file *filp, void *dirent, filldir_t filldir)\n{\n\tstruct inode *inode = filp->f_path.dentry->d_inode;\n\tstruct super_block *sb = inode->i_sb;\n\tint len, err;\n\tchar strbuf[HFSPLUS_MAX_STRLEN + 1];\n\thfsplus_cat_entry entry;\n\tstruct hfs_find_data fd;\n\tstruct hfsplus_readdir_data *rd;\n\tu16 type;\n\n\tif (filp->f_pos >= inode->i_size)\n\t\treturn 0;\n\n\terr = hfs_find_init(HFSPLUS_SB(sb)->cat_tree, &fd);\n\tif (err)\n\t\treturn err;\n\thfsplus_cat_build_key(sb, fd.search_key, inode->i_ino, NULL);\n\terr = hfs_brec_find(&fd);\n\tif (err)\n\t\tgoto out;\n\n\tswitch ((u32)filp->f_pos) {\n\tcase 0:\n\t\t/* This is completely artificial... */\n\t\tif (filldir(dirent, \".\", 1, 0, inode->i_ino, DT_DIR))\n\t\t\tgoto out;\n\t\tfilp->f_pos++;\n\t\t/* fall through */\n\tcase 1:\n\t\thfs_bnode_read(fd.bnode, &entry, fd.entryoffset,\n\t\t\tfd.entrylength);\n\t\tif (be16_to_cpu(entry.type) != HFSPLUS_FOLDER_THREAD) {\n\t\t\tprintk(KERN_ERR \"hfs: bad catalog folder thread\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (fd.entrylength < HFSPLUS_MIN_THREAD_SZ) {\n\t\t\tprintk(KERN_ERR \"hfs: truncated catalog thread\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (filldir(dirent, \"..\", 2, 1,\n\t\t\t    be32_to_cpu(entry.thread.parentID), DT_DIR))\n\t\t\tgoto out;\n\t\tfilp->f_pos++;\n\t\t/* fall through */\n\tdefault:\n\t\tif (filp->f_pos >= inode->i_size)\n\t\t\tgoto out;\n\t\terr = hfs_brec_goto(&fd, filp->f_pos - 1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tfor (;;) {\n\t\tif (be32_to_cpu(fd.key->cat.parent) != inode->i_ino) {\n\t\t\tprintk(KERN_ERR \"hfs: walked past end of dir\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\thfs_bnode_read(fd.bnode, &entry, fd.entryoffset,\n\t\t\tfd.entrylength);\n\t\ttype = be16_to_cpu(entry.type);\n\t\tlen = HFSPLUS_MAX_STRLEN;\n\t\terr = hfsplus_uni2asc(sb, &fd.key->cat.name, strbuf, &len);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tif (type == HFSPLUS_FOLDER) {\n\t\t\tif (fd.entrylength <\n\t\t\t\t\tsizeof(struct hfsplus_cat_folder)) {\n\t\t\t\tprintk(KERN_ERR \"hfs: small dir entry\\n\");\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (HFSPLUS_SB(sb)->hidden_dir &&\n\t\t\t    HFSPLUS_SB(sb)->hidden_dir->i_ino ==\n\t\t\t\t\tbe32_to_cpu(entry.folder.id))\n\t\t\t\tgoto next;\n\t\t\tif (filldir(dirent, strbuf, len, filp->f_pos,\n\t\t\t\t    be32_to_cpu(entry.folder.id), DT_DIR))\n\t\t\t\tbreak;\n\t\t} else if (type == HFSPLUS_FILE) {\n\t\t\tif (fd.entrylength < sizeof(struct hfsplus_cat_file)) {\n\t\t\t\tprintk(KERN_ERR \"hfs: small file entry\\n\");\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (filldir(dirent, strbuf, len, filp->f_pos,\n\t\t\t\t    be32_to_cpu(entry.file.id), DT_REG))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tprintk(KERN_ERR \"hfs: bad catalog entry type\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\nnext:\n\t\tfilp->f_pos++;\n\t\tif (filp->f_pos >= inode->i_size)\n\t\t\tgoto out;\n\t\terr = hfs_brec_goto(&fd, 1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\trd = filp->private_data;\n\tif (!rd) {\n\t\trd = kmalloc(sizeof(struct hfsplus_readdir_data), GFP_KERNEL);\n\t\tif (!rd) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tfilp->private_data = rd;\n\t\trd->file = filp;\n\t\tlist_add(&rd->list, &HFSPLUS_I(inode)->open_dir_list);\n\t}\n\tmemcpy(&rd->key, fd.key, sizeof(struct hfsplus_cat_key));\nout:\n\thfs_find_exit(&fd);\n\treturn err;\n}",
                        "code_after_change": "static int hfsplus_readdir(struct file *filp, void *dirent, filldir_t filldir)\n{\n\tstruct inode *inode = filp->f_path.dentry->d_inode;\n\tstruct super_block *sb = inode->i_sb;\n\tint len, err;\n\tchar strbuf[HFSPLUS_MAX_STRLEN + 1];\n\thfsplus_cat_entry entry;\n\tstruct hfs_find_data fd;\n\tstruct hfsplus_readdir_data *rd;\n\tu16 type;\n\n\tif (filp->f_pos >= inode->i_size)\n\t\treturn 0;\n\n\terr = hfs_find_init(HFSPLUS_SB(sb)->cat_tree, &fd);\n\tif (err)\n\t\treturn err;\n\thfsplus_cat_build_key(sb, fd.search_key, inode->i_ino, NULL);\n\terr = hfs_brec_find(&fd);\n\tif (err)\n\t\tgoto out;\n\n\tswitch ((u32)filp->f_pos) {\n\tcase 0:\n\t\t/* This is completely artificial... */\n\t\tif (filldir(dirent, \".\", 1, 0, inode->i_ino, DT_DIR))\n\t\t\tgoto out;\n\t\tfilp->f_pos++;\n\t\t/* fall through */\n\tcase 1:\n\t\tif (fd.entrylength > sizeof(entry) || fd.entrylength < 0) {\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\n\t\thfs_bnode_read(fd.bnode, &entry, fd.entryoffset,\n\t\t\tfd.entrylength);\n\t\tif (be16_to_cpu(entry.type) != HFSPLUS_FOLDER_THREAD) {\n\t\t\tprintk(KERN_ERR \"hfs: bad catalog folder thread\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (fd.entrylength < HFSPLUS_MIN_THREAD_SZ) {\n\t\t\tprintk(KERN_ERR \"hfs: truncated catalog thread\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (filldir(dirent, \"..\", 2, 1,\n\t\t\t    be32_to_cpu(entry.thread.parentID), DT_DIR))\n\t\t\tgoto out;\n\t\tfilp->f_pos++;\n\t\t/* fall through */\n\tdefault:\n\t\tif (filp->f_pos >= inode->i_size)\n\t\t\tgoto out;\n\t\terr = hfs_brec_goto(&fd, filp->f_pos - 1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tfor (;;) {\n\t\tif (be32_to_cpu(fd.key->cat.parent) != inode->i_ino) {\n\t\t\tprintk(KERN_ERR \"hfs: walked past end of dir\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (fd.entrylength > sizeof(entry) || fd.entrylength < 0) {\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\n\t\thfs_bnode_read(fd.bnode, &entry, fd.entryoffset,\n\t\t\tfd.entrylength);\n\t\ttype = be16_to_cpu(entry.type);\n\t\tlen = HFSPLUS_MAX_STRLEN;\n\t\terr = hfsplus_uni2asc(sb, &fd.key->cat.name, strbuf, &len);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tif (type == HFSPLUS_FOLDER) {\n\t\t\tif (fd.entrylength <\n\t\t\t\t\tsizeof(struct hfsplus_cat_folder)) {\n\t\t\t\tprintk(KERN_ERR \"hfs: small dir entry\\n\");\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (HFSPLUS_SB(sb)->hidden_dir &&\n\t\t\t    HFSPLUS_SB(sb)->hidden_dir->i_ino ==\n\t\t\t\t\tbe32_to_cpu(entry.folder.id))\n\t\t\t\tgoto next;\n\t\t\tif (filldir(dirent, strbuf, len, filp->f_pos,\n\t\t\t\t    be32_to_cpu(entry.folder.id), DT_DIR))\n\t\t\t\tbreak;\n\t\t} else if (type == HFSPLUS_FILE) {\n\t\t\tif (fd.entrylength < sizeof(struct hfsplus_cat_file)) {\n\t\t\t\tprintk(KERN_ERR \"hfs: small file entry\\n\");\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (filldir(dirent, strbuf, len, filp->f_pos,\n\t\t\t\t    be32_to_cpu(entry.file.id), DT_REG))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tprintk(KERN_ERR \"hfs: bad catalog entry type\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\nnext:\n\t\tfilp->f_pos++;\n\t\tif (filp->f_pos >= inode->i_size)\n\t\t\tgoto out;\n\t\terr = hfs_brec_goto(&fd, 1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\trd = filp->private_data;\n\tif (!rd) {\n\t\trd = kmalloc(sizeof(struct hfsplus_readdir_data), GFP_KERNEL);\n\t\tif (!rd) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tfilp->private_data = rd;\n\t\trd->file = filp;\n\t\tlist_add(&rd->list, &HFSPLUS_I(inode)->open_dir_list);\n\t}\n\tmemcpy(&rd->key, fd.key, sizeof(struct hfsplus_cat_key));\nout:\n\thfs_find_exit(&fd);\n\treturn err;\n}",
                        "cve_id": "CVE-2012-2319"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int msr_open(struct inode *inode, struct file *file)\n{\n\tunsigned int cpu;\n\tstruct cpuinfo_x86 *c;\n\n\tif (!capable(CAP_SYS_RAWIO))\n\t\treturn -EPERM;\n\n\tcpu = iminor(file->f_path.dentry->d_inode);\n\tif (cpu >= nr_cpu_ids || !cpu_online(cpu))\n\t\treturn -ENXIO;\t/* No such CPU */\n\n\tc = &cpu_data(cpu);\n\tif (!cpu_has(c, X86_FEATURE_MSR))\n\t\treturn -EIO;\t/* MSR not supported */\n\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int hfsplus_readdir(struct file *filp, void *dirent, filldir_t filldir)\n{\n\tstruct inode *inode = filp->f_path.dentry->d_inode;\n\tstruct super_block *sb = inode->i_sb;\n\tint len, err;\n\tchar strbuf[HFSPLUS_MAX_STRLEN + 1];\n\thfsplus_cat_entry entry;\n\tstruct hfs_find_data fd;\n\tstruct hfsplus_readdir_data *rd;\n\tu16 type;\n\n\tif (filp->f_pos >= inode->i_size)\n\t\treturn 0;\n\n\terr = hfs_find_init(HFSPLUS_SB(sb)->cat_tree, &fd);\n\tif (err)\n\t\treturn err;\n\thfsplus_cat_build_key(sb, fd.search_key, inode->i_ino, NULL);\n\terr = hfs_brec_find(&fd);\n\tif (err)\n\t\tgoto out;\n\n\tswitch ((u32)filp->f_pos) {\n\tcase 0:\n\t\t/* This is completely artificial... */\n\t\tif (filldir(dirent, \".\", 1, 0, inode->i_ino, DT_DIR))\n\t\t\tgoto out;\n\t\tfilp->f_pos++;\n\t\t/* fall through */\n\tcase 1:\n\t\thfs_bnode_read(fd.bnode, &entry, fd.entryoffset,\n\t\t\tfd.entrylength);\n\t\tif (be16_to_cpu(entry.type) != HFSPLUS_FOLDER_THREAD) {\n\t\t\tprintk(KERN_ERR \"hfs: bad catalog folder thread\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (fd.entrylength < HFSPLUS_MIN_THREAD_SZ) {\n\t\t\tprintk(KERN_ERR \"hfs: truncated catalog thread\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (filldir(dirent, \"..\", 2, 1,\n\t\t\t    be32_to_cpu(entry.thread.parentID), DT_DIR))\n\t\t\tgoto out;\n\t\tfilp->f_pos++;\n\t\t/* fall through */\n\tdefault:\n\t\tif (filp->f_pos >= inode->i_size)\n\t\t\tgoto out;\n\t\terr = hfs_brec_goto(&fd, filp->f_pos - 1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tfor (;;) {\n\t\tif (be32_to_cpu(fd.key->cat.parent) != inode->i_ino) {\n\t\t\tprintk(KERN_ERR \"hfs: walked past end of dir\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\thfs_bnode_read(fd.bnode, &entry, fd.entryoffset,\n\t\t\tfd.entrylength);\n\t\ttype = be16_to_cpu(entry.type);\n\t\tlen = HFSPLUS_MAX_STRLEN;\n\t\terr = hfsplus_uni2asc(sb, &fd.key->cat.name, strbuf, &len);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tif (type == HFSPLUS_FOLDER) {\n\t\t\tif (fd.entrylength <\n\t\t\t\t\tsizeof(struct hfsplus_cat_folder)) {\n\t\t\t\tprintk(KERN_ERR \"hfs: small dir entry\\n\");\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (HFSPLUS_SB(sb)->hidden_dir &&\n\t\t\t    HFSPLUS_SB(sb)->hidden_dir->i_ino ==\n\t\t\t\t\tbe32_to_cpu(entry.folder.id))\n\t\t\t\tgoto next;\n\t\t\tif (filldir(dirent, strbuf, len, filp->f_pos,\n\t\t\t\t    be32_to_cpu(entry.folder.id), DT_DIR))\n\t\t\t\tbreak;\n\t\t} else if (type == HFSPLUS_FILE) {\n\t\t\tif (fd.entrylength < sizeof(struct hfsplus_cat_file)) {\n\t\t\t\tprintk(KERN_ERR \"hfs: small file entry\\n\");\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (filldir(dirent, strbuf, len, filp->f_pos,\n\t\t\t\t    be32_to_cpu(entry.file.id), DT_REG))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tprintk(KERN_ERR \"hfs: bad catalog entry type\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\nnext:\n\t\tfilp->f_pos++;\n\t\tif (filp->f_pos >= inode->i_size)\n\t\t\tgoto out;\n\t\terr = hfs_brec_goto(&fd, 1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\trd = filp->private_data;\n\tif (!rd) {\n\t\trd = kmalloc(sizeof(struct hfsplus_readdir_data), GFP_KERNEL);\n\t\tif (!rd) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tfilp->private_data = rd;\n\t\trd->file = filp;\n\t\tlist_add(&rd->list, &HFSPLUS_I(inode)->open_dir_list);\n\t}\n\tmemcpy(&rd->key, fd.key, sizeof(struct hfsplus_cat_key));\nout:\n\thfs_find_exit(&fd);\n\treturn err;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int hfsplus_readdir(struct file *filp, void *dirent, filldir_t filldir)\n{\n\tstruct inode *inode = filp->f_path.dentry->d_inode;\n\tstruct super_block *sb = inode->i_sb;\n\tint len, err;\n\tchar strbuf[HFSPLUS_MAX_STRLEN + 1];\n\thfsplus_cat_entry entry;\n\tstruct hfs_find_data fd;\n\tstruct hfsplus_readdir_data *rd;\n\tu16 type;\n\n\tif (filp->f_pos >= inode->i_size)\n\t\treturn 0;\n\n\terr = hfs_find_init(HFSPLUS_SB(sb)->cat_tree, &fd);\n\tif (err)\n\t\treturn err;\n\thfsplus_cat_build_key(sb, fd.search_key, inode->i_ino, NULL);\n\terr = hfs_brec_find(&fd);\n\tif (err)\n\t\tgoto out;\n\n\tswitch ((u32)filp->f_pos) {\n\tcase 0:\n\t\t/* This is completely artificial... */\n\t\tif (filldir(dirent, \".\", 1, 0, inode->i_ino, DT_DIR))\n\t\t\tgoto out;\n\t\tfilp->f_pos++;\n\t\t/* fall through */\n\tcase 1:\n\t\tif (fd.entrylength > sizeof(entry) || fd.entrylength < 0) {\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\n\t\thfs_bnode_read(fd.bnode, &entry, fd.entryoffset,\n\t\t\tfd.entrylength);\n\t\tif (be16_to_cpu(entry.type) != HFSPLUS_FOLDER_THREAD) {\n\t\t\tprintk(KERN_ERR \"hfs: bad catalog folder thread\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (fd.entrylength < HFSPLUS_MIN_THREAD_SZ) {\n\t\t\tprintk(KERN_ERR \"hfs: truncated catalog thread\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (filldir(dirent, \"..\", 2, 1,\n\t\t\t    be32_to_cpu(entry.thread.parentID), DT_DIR))\n\t\t\tgoto out;\n\t\tfilp->f_pos++;\n\t\t/* fall through */\n\tdefault:\n\t\tif (filp->f_pos >= inode->i_size)\n\t\t\tgoto out;\n\t\terr = hfs_brec_goto(&fd, filp->f_pos - 1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tfor (;;) {\n\t\tif (be32_to_cpu(fd.key->cat.parent) != inode->i_ino) {\n\t\t\tprintk(KERN_ERR \"hfs: walked past end of dir\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (fd.entrylength > sizeof(entry) || fd.entrylength < 0) {\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\n\n\t\thfs_bnode_read(fd.bnode, &entry, fd.entryoffset,\n\t\t\tfd.entrylength);\n\t\ttype = be16_to_cpu(entry.type);\n\t\tlen = HFSPLUS_MAX_STRLEN;\n\t\terr = hfsplus_uni2asc(sb, &fd.key->cat.name, strbuf, &len);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tif (type == HFSPLUS_FOLDER) {\n\t\t\tif (fd.entrylength <\n\t\t\t\t\tsizeof(struct hfsplus_cat_folder)) {\n\t\t\t\tprintk(KERN_ERR \"hfs: small dir entry\\n\");\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (HFSPLUS_SB(sb)->hidden_dir &&\n\t\t\t    HFSPLUS_SB(sb)->hidden_dir->i_ino ==\n\t\t\t\t\tbe32_to_cpu(entry.folder.id))\n\t\t\t\tgoto next;\n\t\t\tif (filldir(dirent, strbuf, len, filp->f_pos,\n\t\t\t\t    be32_to_cpu(entry.folder.id), DT_DIR))\n\t\t\t\tbreak;\n\t\t} else if (type == HFSPLUS_FILE) {\n\t\t\tif (fd.entrylength < sizeof(struct hfsplus_cat_file)) {\n\t\t\t\tprintk(KERN_ERR \"hfs: small file entry\\n\");\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (filldir(dirent, strbuf, len, filp->f_pos,\n\t\t\t\t    be32_to_cpu(entry.file.id), DT_REG))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tprintk(KERN_ERR \"hfs: bad catalog entry type\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto out;\n\t\t}\nnext:\n\t\tfilp->f_pos++;\n\t\tif (filp->f_pos >= inode->i_size)\n\t\t\tgoto out;\n\t\terr = hfs_brec_goto(&fd, 1);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\trd = filp->private_data;\n\tif (!rd) {\n\t\trd = kmalloc(sizeof(struct hfsplus_readdir_data), GFP_KERNEL);\n\t\tif (!rd) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tfilp->private_data = rd;\n\t\trd->file = filp;\n\t\tlist_add(&rd->list, &HFSPLUS_I(inode)->open_dir_list);\n\t}\n\tmemcpy(&rd->key, fd.key, sizeof(struct hfsplus_cat_key));\nout:\n\thfs_find_exit(&fd);\n\treturn err;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "int perf_pmu_register(struct pmu *pmu, const char *name, int type)\n{\n\tint cpu, ret;\n\n\tmutex_lock(&pmus_lock);\n\tret = -ENOMEM;\n\tpmu->pmu_disable_count = alloc_percpu(int);\n\tif (!pmu->pmu_disable_count)\n\t\tgoto unlock;\n\n\tpmu->type = -1;\n\tif (!name)\n\t\tgoto skip_type;\n\tpmu->name = name;\n\n\tif (type < 0) {\n\t\ttype = idr_alloc(&pmu_idr, pmu, PERF_TYPE_MAX, 0, GFP_KERNEL);\n\t\tif (type < 0) {\n\t\t\tret = type;\n\t\t\tgoto free_pdc;\n\t\t}\n\t}\n\tpmu->type = type;\n\n\tif (pmu_bus_running) {\n\t\tret = pmu_dev_alloc(pmu);\n\t\tif (ret)\n\t\t\tgoto free_idr;\n\t}\n\nskip_type:\n\tpmu->pmu_cpu_context = find_pmu_context(pmu->task_ctx_nr);\n\tif (pmu->pmu_cpu_context)\n\t\tgoto got_cpu_context;\n\n\tret = -ENOMEM;\n\tpmu->pmu_cpu_context = alloc_percpu(struct perf_cpu_context);\n\tif (!pmu->pmu_cpu_context)\n\t\tgoto free_dev;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct perf_cpu_context *cpuctx;\n\n\t\tcpuctx = per_cpu_ptr(pmu->pmu_cpu_context, cpu);\n\t\t__perf_event_init_context(&cpuctx->ctx);\n\t\tlockdep_set_class(&cpuctx->ctx.mutex, &cpuctx_mutex);\n\t\tlockdep_set_class(&cpuctx->ctx.lock, &cpuctx_lock);\n\t\tcpuctx->ctx.type = cpu_context;\n\t\tcpuctx->ctx.pmu = pmu;\n\n\t\t__perf_cpu_hrtimer_init(cpuctx, cpu);\n\n\t\tINIT_LIST_HEAD(&cpuctx->rotation_list);\n\t\tcpuctx->unique_pmu = pmu;\n\t}\n\ngot_cpu_context:\n\tif (!pmu->start_txn) {\n\t\tif (pmu->pmu_enable) {\n\t\t\t/*\n\t\t\t * If we have pmu_enable/pmu_disable calls, install\n\t\t\t * transaction stubs that use that to try and batch\n\t\t\t * hardware accesses.\n\t\t\t */\n\t\t\tpmu->start_txn  = perf_pmu_start_txn;\n\t\t\tpmu->commit_txn = perf_pmu_commit_txn;\n\t\t\tpmu->cancel_txn = perf_pmu_cancel_txn;\n\t\t} else {\n\t\t\tpmu->start_txn  = perf_pmu_nop_void;\n\t\t\tpmu->commit_txn = perf_pmu_nop_int;\n\t\t\tpmu->cancel_txn = perf_pmu_nop_void;\n\t\t}\n\t}\n\n\tif (!pmu->pmu_enable) {\n\t\tpmu->pmu_enable  = perf_pmu_nop_void;\n\t\tpmu->pmu_disable = perf_pmu_nop_void;\n\t}\n\n\tif (!pmu->event_idx)\n\t\tpmu->event_idx = perf_event_idx_default;\n\n\tlist_add_rcu(&pmu->entry, &pmus);\n\tret = 0;\nunlock:\n\tmutex_unlock(&pmus_lock);\n\n\treturn ret;\n\nfree_dev:\n\tdevice_del(pmu->dev);\n\tput_device(pmu->dev);\n\nfree_idr:\n\tif (pmu->type >= PERF_TYPE_MAX)\n\t\tidr_remove(&pmu_idr, pmu->type);\n\nfree_pdc:\n\tfree_percpu(pmu->pmu_disable_count);\n\tgoto unlock;\n}",
                        "code_after_change": "int perf_pmu_register(struct pmu *pmu, const char *name, int type)\n{\n\tint cpu, ret;\n\n\tmutex_lock(&pmus_lock);\n\tret = -ENOMEM;\n\tpmu->pmu_disable_count = alloc_percpu(int);\n\tif (!pmu->pmu_disable_count)\n\t\tgoto unlock;\n\n\tpmu->type = -1;\n\tif (!name)\n\t\tgoto skip_type;\n\tpmu->name = name;\n\n\tif (type < 0) {\n\t\ttype = idr_alloc(&pmu_idr, pmu, PERF_TYPE_MAX, 0, GFP_KERNEL);\n\t\tif (type < 0) {\n\t\t\tret = type;\n\t\t\tgoto free_pdc;\n\t\t}\n\t}\n\tpmu->type = type;\n\n\tif (pmu_bus_running) {\n\t\tret = pmu_dev_alloc(pmu);\n\t\tif (ret)\n\t\t\tgoto free_idr;\n\t}\n\nskip_type:\n\tpmu->pmu_cpu_context = find_pmu_context(pmu->task_ctx_nr);\n\tif (pmu->pmu_cpu_context)\n\t\tgoto got_cpu_context;\n\n\tret = -ENOMEM;\n\tpmu->pmu_cpu_context = alloc_percpu(struct perf_cpu_context);\n\tif (!pmu->pmu_cpu_context)\n\t\tgoto free_dev;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct perf_cpu_context *cpuctx;\n\n\t\tcpuctx = per_cpu_ptr(pmu->pmu_cpu_context, cpu);\n\t\t__perf_event_init_context(&cpuctx->ctx);\n\t\tlockdep_set_class(&cpuctx->ctx.mutex, &cpuctx_mutex);\n\t\tlockdep_set_class(&cpuctx->ctx.lock, &cpuctx_lock);\n\t\tcpuctx->ctx.pmu = pmu;\n\n\t\t__perf_cpu_hrtimer_init(cpuctx, cpu);\n\n\t\tINIT_LIST_HEAD(&cpuctx->rotation_list);\n\t\tcpuctx->unique_pmu = pmu;\n\t}\n\ngot_cpu_context:\n\tif (!pmu->start_txn) {\n\t\tif (pmu->pmu_enable) {\n\t\t\t/*\n\t\t\t * If we have pmu_enable/pmu_disable calls, install\n\t\t\t * transaction stubs that use that to try and batch\n\t\t\t * hardware accesses.\n\t\t\t */\n\t\t\tpmu->start_txn  = perf_pmu_start_txn;\n\t\t\tpmu->commit_txn = perf_pmu_commit_txn;\n\t\t\tpmu->cancel_txn = perf_pmu_cancel_txn;\n\t\t} else {\n\t\t\tpmu->start_txn  = perf_pmu_nop_void;\n\t\t\tpmu->commit_txn = perf_pmu_nop_int;\n\t\t\tpmu->cancel_txn = perf_pmu_nop_void;\n\t\t}\n\t}\n\n\tif (!pmu->pmu_enable) {\n\t\tpmu->pmu_enable  = perf_pmu_nop_void;\n\t\tpmu->pmu_disable = perf_pmu_nop_void;\n\t}\n\n\tif (!pmu->event_idx)\n\t\tpmu->event_idx = perf_event_idx_default;\n\n\tlist_add_rcu(&pmu->entry, &pmus);\n\tret = 0;\nunlock:\n\tmutex_unlock(&pmus_lock);\n\n\treturn ret;\n\nfree_dev:\n\tdevice_del(pmu->dev);\n\tput_device(pmu->dev);\n\nfree_idr:\n\tif (pmu->type >= PERF_TYPE_MAX)\n\t\tidr_remove(&pmu_idr, pmu->type);\n\nfree_pdc:\n\tfree_percpu(pmu->pmu_disable_count);\n\tgoto unlock;\n}",
                        "cve_id": "CVE-2015-9004"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int msr_open(struct inode *inode, struct file *file)\n{\n\tunsigned int cpu;\n\tstruct cpuinfo_x86 *c;\n\n\tif (!capable(CAP_SYS_RAWIO))\n\t\treturn -EPERM;\n\n\tcpu = iminor(file->f_path.dentry->d_inode);\n\tif (cpu >= nr_cpu_ids || !cpu_online(cpu))\n\t\treturn -ENXIO;\t/* No such CPU */\n\n\tc = &cpu_data(cpu);\n\tif (!cpu_has(c, X86_FEATURE_MSR))\n\t\treturn -EIO;\t/* MSR not supported */\n\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nint perf_pmu_register(struct pmu *pmu, const char *name, int type)\n{\n\tint cpu, ret;\n\n\tmutex_lock(&pmus_lock);\n\tret = -ENOMEM;\n\tpmu->pmu_disable_count = alloc_percpu(int);\n\tif (!pmu->pmu_disable_count)\n\t\tgoto unlock;\n\n\tpmu->type = -1;\n\tif (!name)\n\t\tgoto skip_type;\n\tpmu->name = name;\n\n\tif (type < 0) {\n\t\ttype = idr_alloc(&pmu_idr, pmu, PERF_TYPE_MAX, 0, GFP_KERNEL);\n\t\tif (type < 0) {\n\t\t\tret = type;\n\t\t\tgoto free_pdc;\n\t\t}\n\t}\n\tpmu->type = type;\n\n\tif (pmu_bus_running) {\n\t\tret = pmu_dev_alloc(pmu);\n\t\tif (ret)\n\t\t\tgoto free_idr;\n\t}\n\nskip_type:\n\tpmu->pmu_cpu_context = find_pmu_context(pmu->task_ctx_nr);\n\tif (pmu->pmu_cpu_context)\n\t\tgoto got_cpu_context;\n\n\tret = -ENOMEM;\n\tpmu->pmu_cpu_context = alloc_percpu(struct perf_cpu_context);\n\tif (!pmu->pmu_cpu_context)\n\t\tgoto free_dev;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct perf_cpu_context *cpuctx;\n\n\t\tcpuctx = per_cpu_ptr(pmu->pmu_cpu_context, cpu);\n\t\t__perf_event_init_context(&cpuctx->ctx);\n\t\tlockdep_set_class(&cpuctx->ctx.mutex, &cpuctx_mutex);\n\t\tlockdep_set_class(&cpuctx->ctx.lock, &cpuctx_lock);\n\t\tcpuctx->ctx.type = cpu_context;\n\t\tcpuctx->ctx.pmu = pmu;\n\n\t\t__perf_cpu_hrtimer_init(cpuctx, cpu);\n\n\t\tINIT_LIST_HEAD(&cpuctx->rotation_list);\n\t\tcpuctx->unique_pmu = pmu;\n\t}\n\ngot_cpu_context:\n\tif (!pmu->start_txn) {\n\t\tif (pmu->pmu_enable) {\n\t\t\t/*\n\t\t\t * If we have pmu_enable/pmu_disable calls, install\n\t\t\t * transaction stubs that use that to try and batch\n\t\t\t * hardware accesses.\n\t\t\t */\n\t\t\tpmu->start_txn  = perf_pmu_start_txn;\n\t\t\tpmu->commit_txn = perf_pmu_commit_txn;\n\t\t\tpmu->cancel_txn = perf_pmu_cancel_txn;\n\t\t} else {\n\t\t\tpmu->start_txn  = perf_pmu_nop_void;\n\t\t\tpmu->commit_txn = perf_pmu_nop_int;\n\t\t\tpmu->cancel_txn = perf_pmu_nop_void;\n\t\t}\n\t}\n\n\tif (!pmu->pmu_enable) {\n\t\tpmu->pmu_enable  = perf_pmu_nop_void;\n\t\tpmu->pmu_disable = perf_pmu_nop_void;\n\t}\n\n\tif (!pmu->event_idx)\n\t\tpmu->event_idx = perf_event_idx_default;\n\n\tlist_add_rcu(&pmu->entry, &pmus);\n\tret = 0;\nunlock:\n\tmutex_unlock(&pmus_lock);\n\n\treturn ret;\n\nfree_dev:\n\tdevice_del(pmu->dev);\n\tput_device(pmu->dev);\n\nfree_idr:\n\tif (pmu->type >= PERF_TYPE_MAX)\n\t\tidr_remove(&pmu_idr, pmu->type);\n\nfree_pdc:\n\tfree_percpu(pmu->pmu_disable_count);\n\tgoto unlock;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nint perf_pmu_register(struct pmu *pmu, const char *name, int type)\n{\n\tint cpu, ret;\n\n\tmutex_lock(&pmus_lock);\n\tret = -ENOMEM;\n\tpmu->pmu_disable_count = alloc_percpu(int);\n\tif (!pmu->pmu_disable_count)\n\t\tgoto unlock;\n\n\tpmu->type = -1;\n\tif (!name)\n\t\tgoto skip_type;\n\tpmu->name = name;\n\n\tif (type < 0) {\n\t\ttype = idr_alloc(&pmu_idr, pmu, PERF_TYPE_MAX, 0, GFP_KERNEL);\n\t\tif (type < 0) {\n\t\t\tret = type;\n\t\t\tgoto free_pdc;\n\t\t}\n\t}\n\tpmu->type = type;\n\n\tif (pmu_bus_running) {\n\t\tret = pmu_dev_alloc(pmu);\n\t\tif (ret)\n\t\t\tgoto free_idr;\n\t}\n\nskip_type:\n\tpmu->pmu_cpu_context = find_pmu_context(pmu->task_ctx_nr);\n\tif (pmu->pmu_cpu_context)\n\t\tgoto got_cpu_context;\n\n\tret = -ENOMEM;\n\tpmu->pmu_cpu_context = alloc_percpu(struct perf_cpu_context);\n\tif (!pmu->pmu_cpu_context)\n\t\tgoto free_dev;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct perf_cpu_context *cpuctx;\n\n\t\tcpuctx = per_cpu_ptr(pmu->pmu_cpu_context, cpu);\n\t\t__perf_event_init_context(&cpuctx->ctx);\n\t\tlockdep_set_class(&cpuctx->ctx.mutex, &cpuctx_mutex);\n\t\tlockdep_set_class(&cpuctx->ctx.lock, &cpuctx_lock);\n\t\tcpuctx->ctx.pmu = pmu;\n\n\t\t__perf_cpu_hrtimer_init(cpuctx, cpu);\n\n\t\tINIT_LIST_HEAD(&cpuctx->rotation_list);\n\t\tcpuctx->unique_pmu = pmu;\n\t}\n\ngot_cpu_context:\n\tif (!pmu->start_txn) {\n\t\tif (pmu->pmu_enable) {\n\t\t\t/*\n\t\t\t * If we have pmu_enable/pmu_disable calls, install\n\t\t\t * transaction stubs that use that to try and batch\n\t\t\t * hardware accesses.\n\t\t\t */\n\t\t\tpmu->start_txn  = perf_pmu_start_txn;\n\t\t\tpmu->commit_txn = perf_pmu_commit_txn;\n\t\t\tpmu->cancel_txn = perf_pmu_cancel_txn;\n\t\t} else {\n\t\t\tpmu->start_txn  = perf_pmu_nop_void;\n\t\t\tpmu->commit_txn = perf_pmu_nop_int;\n\t\t\tpmu->cancel_txn = perf_pmu_nop_void;\n\t\t}\n\t}\n\n\tif (!pmu->pmu_enable) {\n\t\tpmu->pmu_enable  = perf_pmu_nop_void;\n\t\tpmu->pmu_disable = perf_pmu_nop_void;\n\t}\n\n\tif (!pmu->event_idx)\n\t\tpmu->event_idx = perf_event_idx_default;\n\n\tlist_add_rcu(&pmu->entry, &pmus);\n\tret = 0;\nunlock:\n\tmutex_unlock(&pmus_lock);\n\n\treturn ret;\n\nfree_dev:\n\tdevice_del(pmu->dev);\n\tput_device(pmu->dev);\n\nfree_idr:\n\tif (pmu->type >= PERF_TYPE_MAX)\n\t\tidr_remove(&pmu_idr, pmu->type);\n\nfree_pdc:\n\tfree_percpu(pmu->pmu_disable_count);\n\tgoto unlock;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        },
        {
            "id": 293,
            "cve_id": "CVE-2013-4299",
            "code_snippet": "static int persistent_prepare_exception(struct dm_exception_store *store,\n\t\t\t\t\tstruct dm_exception *e)\n{\n\tstruct pstore *ps = get_info(store);\n\tsector_t size = get_dev_size(dm_snap_cow(store->snap)->bdev);\n\n\t/* Is there enough room ? */\n\tif (size < ((ps->next_free + 1) * store->chunk_size))\n\t\treturn -ENOSPC;\n\n\te->new_chunk = ps->next_free;\n\n\t/*\n\t * Move onto the next free pending, making sure to take\n\t * into account the location of the metadata chunks.\n\t */\n\tps->next_free++;\n\tskip_metadata(ps);\n\n\tatomic_inc(&ps->pending_count);\n\treturn 0;\n}",
            "detect_result": [
                {
                    "vul_knowledge": {
                        "code_before_change": "static int read_exceptions(struct pstore *ps,\n\t\t\t   int (*callback)(void *callback_context, chunk_t old,\n\t\t\t\t\t   chunk_t new),\n\t\t\t   void *callback_context)\n{\n\tint r, full = 1;\n\n\t/*\n\t * Keeping reading chunks and inserting exceptions until\n\t * we find a partially full area.\n\t */\n\tfor (ps->current_area = 0; full; ps->current_area++) {\n\t\tr = area_io(ps, READ);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = insert_exceptions(ps, callback, callback_context, &full);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tps->current_area--;\n\n\treturn 0;\n}",
                        "code_after_change": "static int read_exceptions(struct pstore *ps,\n\t\t\t   int (*callback)(void *callback_context, chunk_t old,\n\t\t\t\t\t   chunk_t new),\n\t\t\t   void *callback_context)\n{\n\tint r, full = 1;\n\n\t/*\n\t * Keeping reading chunks and inserting exceptions until\n\t * we find a partially full area.\n\t */\n\tfor (ps->current_area = 0; full; ps->current_area++) {\n\t\tr = area_io(ps, READ);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = insert_exceptions(ps, callback, callback_context, &full);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tps->current_area--;\n\n\tskip_metadata(ps);\n\n\treturn 0;\n}",
                        "cve_id": "CVE-2013-4299"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int persistent_prepare_exception(struct dm_exception_store *store,\n\t\t\t\t\tstruct dm_exception *e)\n{\n\tstruct pstore *ps = get_info(store);\n\tsector_t size = get_dev_size(dm_snap_cow(store->snap)->bdev);\n\n\t/* Is there enough room ? */\n\tif (size < ((ps->next_free + 1) * store->chunk_size))\n\t\treturn -ENOSPC;\n\n\te->new_chunk = ps->next_free;\n\n\t/*\n\t * Move onto the next free pending, making sure to take\n\t * into account the location of the metadata chunks.\n\t */\n\tps->next_free++;\n\tskip_metadata(ps);\n\n\tatomic_inc(&ps->pending_count);\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic int read_exceptions(struct pstore *ps,\n\t\t\t   int (*callback)(void *callback_context, chunk_t old,\n\t\t\t\t\t   chunk_t new),\n\t\t\t   void *callback_context)\n{\n\tint r, full = 1;\n\n\t/*\n\t * Keeping reading chunks and inserting exceptions until\n\t * we find a partially full area.\n\t */\n\tfor (ps->current_area = 0; full; ps->current_area++) {\n\t\tr = area_io(ps, READ);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = insert_exceptions(ps, callback, callback_context, &full);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tps->current_area--;\n\n\treturn 0;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic int read_exceptions(struct pstore *ps,\n\t\t\t   int (*callback)(void *callback_context, chunk_t old,\n\t\t\t\t\t   chunk_t new),\n\t\t\t   void *callback_context)\n{\n\tint r, full = 1;\n\n\t/*\n\t * Keeping reading chunks and inserting exceptions until\n\t * we find a partially full area.\n\t */\n\tfor (ps->current_area = 0; full; ps->current_area++) {\n\t\tr = area_io(ps, READ);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = insert_exceptions(ps, callback, callback_context, &full);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tps->current_area--;\n\n\tskip_metadata(ps);\n\n\treturn 0;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "STATIC int\nxfs_ioctl_setattr(\n\txfs_inode_t\t\t*ip,\n\tstruct fsxattr\t\t*fa,\n\tint\t\t\tmask)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tstruct xfs_trans\t*tp;\n\tunsigned int\t\tlock_flags = 0;\n\tstruct xfs_dquot\t*udqp = NULL;\n\tstruct xfs_dquot\t*pdqp = NULL;\n\tstruct xfs_dquot\t*olddquot = NULL;\n\tint\t\t\tcode;\n\n\ttrace_xfs_ioctl_setattr(ip);\n\n\tif (mp->m_flags & XFS_MOUNT_RDONLY)\n\t\treturn XFS_ERROR(EROFS);\n\tif (XFS_FORCED_SHUTDOWN(mp))\n\t\treturn XFS_ERROR(EIO);\n\n\t/*\n\t * Disallow 32bit project ids when projid32bit feature is not enabled.\n\t */\n\tif ((mask & FSX_PROJID) && (fa->fsx_projid > (__uint16_t)-1) &&\n\t\t\t!xfs_sb_version_hasprojid32bit(&ip->i_mount->m_sb))\n\t\treturn XFS_ERROR(EINVAL);\n\n\t/*\n\t * If disk quotas is on, we make sure that the dquots do exist on disk,\n\t * before we start any other transactions. Trying to do this later\n\t * is messy. We don't care to take a readlock to look at the ids\n\t * in inode here, because we can't hold it across the trans_reserve.\n\t * If the IDs do change before we take the ilock, we're covered\n\t * because the i_*dquot fields will get updated anyway.\n\t */\n\tif (XFS_IS_QUOTA_ON(mp) && (mask & FSX_PROJID)) {\n\t\tcode = xfs_qm_vop_dqalloc(ip, ip->i_d.di_uid,\n\t\t\t\t\t ip->i_d.di_gid, fa->fsx_projid,\n\t\t\t\t\t XFS_QMOPT_PQUOTA, &udqp, NULL, &pdqp);\n\t\tif (code)\n\t\t\treturn code;\n\t}\n\n\t/*\n\t * For the other attributes, we acquire the inode lock and\n\t * first do an error checking pass.\n\t */\n\ttp = xfs_trans_alloc(mp, XFS_TRANS_SETATTR_NOT_SIZE);\n\tcode = xfs_trans_reserve(tp, &M_RES(mp)->tr_ichange, 0, 0);\n\tif (code)\n\t\tgoto error_return;\n\n\tlock_flags = XFS_ILOCK_EXCL;\n\txfs_ilock(ip, lock_flags);\n\n\t/*\n\t * CAP_FOWNER overrides the following restrictions:\n\t *\n\t * The user ID of the calling process must be equal\n\t * to the file owner ID, except in cases where the\n\t * CAP_FSETID capability is applicable.\n\t */\n\tif (!inode_owner_or_capable(VFS_I(ip))) {\n\t\tcode = XFS_ERROR(EPERM);\n\t\tgoto error_return;\n\t}\n\n\t/*\n\t * Do a quota reservation only if projid is actually going to change.\n\t * Only allow changing of projid from init_user_ns since it is a\n\t * non user namespace aware identifier.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\tif (current_user_ns() != &init_user_ns) {\n\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\tgoto error_return;\n\t\t}\n\n\t\tif (XFS_IS_QUOTA_RUNNING(mp) &&\n\t\t    XFS_IS_PQUOTA_ON(mp) &&\n\t\t    xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tASSERT(tp);\n\t\t\tcode = xfs_qm_vop_chown_reserve(tp, ip, udqp, NULL,\n\t\t\t\t\t\tpdqp, capable(CAP_FOWNER) ?\n\t\t\t\t\t\tXFS_QMOPT_FORCE_RES : 0);\n\t\t\tif (code)\t/* out of quota */\n\t\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\tif (mask & FSX_EXTSIZE) {\n\t\t/*\n\t\t * Can't change extent size if any extents are allocated.\n\t\t */\n\t\tif (ip->i_d.di_nextents &&\n\t\t    ((ip->i_d.di_extsize << mp->m_sb.sb_blocklog) !=\n\t\t     fa->fsx_extsize)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * Extent size must be a multiple of the appropriate block\n\t\t * size, if set at all. It must also be smaller than the\n\t\t * maximum extent size supported by the filesystem.\n\t\t *\n\t\t * Also, for non-realtime files, limit the extent size hint to\n\t\t * half the size of the AGs in the filesystem so alignment\n\t\t * doesn't result in extents larger than an AG.\n\t\t */\n\t\tif (fa->fsx_extsize != 0) {\n\t\t\txfs_extlen_t    size;\n\t\t\txfs_fsblock_t   extsize_fsb;\n\n\t\t\textsize_fsb = XFS_B_TO_FSB(mp, fa->fsx_extsize);\n\t\t\tif (extsize_fsb > MAXEXTLEN) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\n\t\t\tif (XFS_IS_REALTIME_INODE(ip) ||\n\t\t\t    ((mask & FSX_XFLAGS) &&\n\t\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME))) {\n\t\t\t\tsize = mp->m_sb.sb_rextsize <<\n\t\t\t\t       mp->m_sb.sb_blocklog;\n\t\t\t} else {\n\t\t\t\tsize = mp->m_sb.sb_blocksize;\n\t\t\t\tif (extsize_fsb > mp->m_sb.sb_agblocks / 2) {\n\t\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\t\tgoto error_return;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (fa->fsx_extsize % size) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\t}\n\n\n\tif (mask & FSX_XFLAGS) {\n\t\t/*\n\t\t * Can't change realtime flag if any extents are allocated.\n\t\t */\n\t\tif ((ip->i_d.di_nextents || ip->i_delayed_blks) &&\n\t\t    (XFS_IS_REALTIME_INODE(ip)) !=\n\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * If realtime flag is set then must have realtime data.\n\t\t */\n\t\tif ((fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tif ((mp->m_sb.sb_rblocks == 0) ||\n\t\t\t    (mp->m_sb.sb_rextsize == 0) ||\n\t\t\t    (ip->i_d.di_extsize % mp->m_sb.sb_rextsize)) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Can't modify an immutable/append-only file unless\n\t\t * we have appropriate permission.\n\t\t */\n\t\tif ((ip->i_d.di_flags &\n\t\t\t\t(XFS_DIFLAG_IMMUTABLE|XFS_DIFLAG_APPEND) ||\n\t\t     (fa->fsx_xflags &\n\t\t\t\t(XFS_XFLAG_IMMUTABLE | XFS_XFLAG_APPEND))) &&\n\t\t    !capable(CAP_LINUX_IMMUTABLE)) {\n\t\t\tcode = XFS_ERROR(EPERM);\n\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\txfs_trans_ijoin(tp, ip, 0);\n\n\t/*\n\t * Change file ownership.  Must be the owner or privileged.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\t/*\n\t\t * CAP_FSETID overrides the following restrictions:\n\t\t *\n\t\t * The set-user-ID and set-group-ID bits of a file will be\n\t\t * cleared upon successful return from chown()\n\t\t */\n\t\tif ((ip->i_d.di_mode & (S_ISUID|S_ISGID)) &&\n\t\t    !inode_capable(VFS_I(ip), CAP_FSETID))\n\t\t\tip->i_d.di_mode &= ~(S_ISUID|S_ISGID);\n\n\t\t/*\n\t\t * Change the ownerships and register quota modifications\n\t\t * in the transaction.\n\t\t */\n\t\tif (xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tif (XFS_IS_QUOTA_RUNNING(mp) && XFS_IS_PQUOTA_ON(mp)) {\n\t\t\t\tolddquot = xfs_qm_vop_chown(tp, ip,\n\t\t\t\t\t\t\t&ip->i_pdquot, pdqp);\n\t\t\t}\n\t\t\txfs_set_projid(ip, fa->fsx_projid);\n\n\t\t\t/*\n\t\t\t * We may have to rev the inode as well as\n\t\t\t * the superblock version number since projids didn't\n\t\t\t * exist before DINODE_VERSION_2 and SB_VERSION_NLINK.\n\t\t\t */\n\t\t\tif (ip->i_d.di_version == 1)\n\t\t\t\txfs_bump_ino_vers2(tp, ip);\n\t\t}\n\n\t}\n\n\tif (mask & FSX_EXTSIZE)\n\t\tip->i_d.di_extsize = fa->fsx_extsize >> mp->m_sb.sb_blocklog;\n\tif (mask & FSX_XFLAGS) {\n\t\txfs_set_diflags(ip, fa->fsx_xflags);\n\t\txfs_diflags_to_linux(ip);\n\t}\n\n\txfs_trans_ichgtime(tp, ip, XFS_ICHGTIME_CHG);\n\txfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);\n\n\tXFS_STATS_INC(xs_ig_attrchg);\n\n\t/*\n\t * If this is a synchronous mount, make sure that the\n\t * transaction goes to disk before returning to the user.\n\t * This is slightly sub-optimal in that truncates require\n\t * two sync transactions instead of one for wsync filesystems.\n\t * One for the truncate and one for the timestamps since we\n\t * don't want to change the timestamps unless we're sure the\n\t * truncate worked.  Truncates are less than 1% of the laddis\n\t * mix so this probably isn't worth the trouble to optimize.\n\t */\n\tif (mp->m_flags & XFS_MOUNT_WSYNC)\n\t\txfs_trans_set_sync(tp);\n\tcode = xfs_trans_commit(tp, 0);\n\txfs_iunlock(ip, lock_flags);\n\n\t/*\n\t * Release any dquot(s) the inode had kept before chown.\n\t */\n\txfs_qm_dqrele(olddquot);\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\n\treturn code;\n\n error_return:\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\txfs_trans_cancel(tp, 0);\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\n\treturn code;\n}",
                        "code_after_change": "STATIC int\nxfs_ioctl_setattr(\n\txfs_inode_t\t\t*ip,\n\tstruct fsxattr\t\t*fa,\n\tint\t\t\tmask)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tstruct xfs_trans\t*tp;\n\tunsigned int\t\tlock_flags = 0;\n\tstruct xfs_dquot\t*udqp = NULL;\n\tstruct xfs_dquot\t*pdqp = NULL;\n\tstruct xfs_dquot\t*olddquot = NULL;\n\tint\t\t\tcode;\n\n\ttrace_xfs_ioctl_setattr(ip);\n\n\tif (mp->m_flags & XFS_MOUNT_RDONLY)\n\t\treturn XFS_ERROR(EROFS);\n\tif (XFS_FORCED_SHUTDOWN(mp))\n\t\treturn XFS_ERROR(EIO);\n\n\t/*\n\t * Disallow 32bit project ids when projid32bit feature is not enabled.\n\t */\n\tif ((mask & FSX_PROJID) && (fa->fsx_projid > (__uint16_t)-1) &&\n\t\t\t!xfs_sb_version_hasprojid32bit(&ip->i_mount->m_sb))\n\t\treturn XFS_ERROR(EINVAL);\n\n\t/*\n\t * If disk quotas is on, we make sure that the dquots do exist on disk,\n\t * before we start any other transactions. Trying to do this later\n\t * is messy. We don't care to take a readlock to look at the ids\n\t * in inode here, because we can't hold it across the trans_reserve.\n\t * If the IDs do change before we take the ilock, we're covered\n\t * because the i_*dquot fields will get updated anyway.\n\t */\n\tif (XFS_IS_QUOTA_ON(mp) && (mask & FSX_PROJID)) {\n\t\tcode = xfs_qm_vop_dqalloc(ip, ip->i_d.di_uid,\n\t\t\t\t\t ip->i_d.di_gid, fa->fsx_projid,\n\t\t\t\t\t XFS_QMOPT_PQUOTA, &udqp, NULL, &pdqp);\n\t\tif (code)\n\t\t\treturn code;\n\t}\n\n\t/*\n\t * For the other attributes, we acquire the inode lock and\n\t * first do an error checking pass.\n\t */\n\ttp = xfs_trans_alloc(mp, XFS_TRANS_SETATTR_NOT_SIZE);\n\tcode = xfs_trans_reserve(tp, &M_RES(mp)->tr_ichange, 0, 0);\n\tif (code)\n\t\tgoto error_return;\n\n\tlock_flags = XFS_ILOCK_EXCL;\n\txfs_ilock(ip, lock_flags);\n\n\t/*\n\t * CAP_FOWNER overrides the following restrictions:\n\t *\n\t * The user ID of the calling process must be equal\n\t * to the file owner ID, except in cases where the\n\t * CAP_FSETID capability is applicable.\n\t */\n\tif (!inode_owner_or_capable(VFS_I(ip))) {\n\t\tcode = XFS_ERROR(EPERM);\n\t\tgoto error_return;\n\t}\n\n\t/*\n\t * Do a quota reservation only if projid is actually going to change.\n\t * Only allow changing of projid from init_user_ns since it is a\n\t * non user namespace aware identifier.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\tif (current_user_ns() != &init_user_ns) {\n\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\tgoto error_return;\n\t\t}\n\n\t\tif (XFS_IS_QUOTA_RUNNING(mp) &&\n\t\t    XFS_IS_PQUOTA_ON(mp) &&\n\t\t    xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tASSERT(tp);\n\t\t\tcode = xfs_qm_vop_chown_reserve(tp, ip, udqp, NULL,\n\t\t\t\t\t\tpdqp, capable(CAP_FOWNER) ?\n\t\t\t\t\t\tXFS_QMOPT_FORCE_RES : 0);\n\t\t\tif (code)\t/* out of quota */\n\t\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\tif (mask & FSX_EXTSIZE) {\n\t\t/*\n\t\t * Can't change extent size if any extents are allocated.\n\t\t */\n\t\tif (ip->i_d.di_nextents &&\n\t\t    ((ip->i_d.di_extsize << mp->m_sb.sb_blocklog) !=\n\t\t     fa->fsx_extsize)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * Extent size must be a multiple of the appropriate block\n\t\t * size, if set at all. It must also be smaller than the\n\t\t * maximum extent size supported by the filesystem.\n\t\t *\n\t\t * Also, for non-realtime files, limit the extent size hint to\n\t\t * half the size of the AGs in the filesystem so alignment\n\t\t * doesn't result in extents larger than an AG.\n\t\t */\n\t\tif (fa->fsx_extsize != 0) {\n\t\t\txfs_extlen_t    size;\n\t\t\txfs_fsblock_t   extsize_fsb;\n\n\t\t\textsize_fsb = XFS_B_TO_FSB(mp, fa->fsx_extsize);\n\t\t\tif (extsize_fsb > MAXEXTLEN) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\n\t\t\tif (XFS_IS_REALTIME_INODE(ip) ||\n\t\t\t    ((mask & FSX_XFLAGS) &&\n\t\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME))) {\n\t\t\t\tsize = mp->m_sb.sb_rextsize <<\n\t\t\t\t       mp->m_sb.sb_blocklog;\n\t\t\t} else {\n\t\t\t\tsize = mp->m_sb.sb_blocksize;\n\t\t\t\tif (extsize_fsb > mp->m_sb.sb_agblocks / 2) {\n\t\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\t\tgoto error_return;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (fa->fsx_extsize % size) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\t}\n\n\n\tif (mask & FSX_XFLAGS) {\n\t\t/*\n\t\t * Can't change realtime flag if any extents are allocated.\n\t\t */\n\t\tif ((ip->i_d.di_nextents || ip->i_delayed_blks) &&\n\t\t    (XFS_IS_REALTIME_INODE(ip)) !=\n\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * If realtime flag is set then must have realtime data.\n\t\t */\n\t\tif ((fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tif ((mp->m_sb.sb_rblocks == 0) ||\n\t\t\t    (mp->m_sb.sb_rextsize == 0) ||\n\t\t\t    (ip->i_d.di_extsize % mp->m_sb.sb_rextsize)) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Can't modify an immutable/append-only file unless\n\t\t * we have appropriate permission.\n\t\t */\n\t\tif ((ip->i_d.di_flags &\n\t\t\t\t(XFS_DIFLAG_IMMUTABLE|XFS_DIFLAG_APPEND) ||\n\t\t     (fa->fsx_xflags &\n\t\t\t\t(XFS_XFLAG_IMMUTABLE | XFS_XFLAG_APPEND))) &&\n\t\t    !capable(CAP_LINUX_IMMUTABLE)) {\n\t\t\tcode = XFS_ERROR(EPERM);\n\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\txfs_trans_ijoin(tp, ip, 0);\n\n\t/*\n\t * Change file ownership.  Must be the owner or privileged.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\t/*\n\t\t * CAP_FSETID overrides the following restrictions:\n\t\t *\n\t\t * The set-user-ID and set-group-ID bits of a file will be\n\t\t * cleared upon successful return from chown()\n\t\t */\n\t\tif ((ip->i_d.di_mode & (S_ISUID|S_ISGID)) &&\n\t\t    !capable_wrt_inode_uidgid(VFS_I(ip), CAP_FSETID))\n\t\t\tip->i_d.di_mode &= ~(S_ISUID|S_ISGID);\n\n\t\t/*\n\t\t * Change the ownerships and register quota modifications\n\t\t * in the transaction.\n\t\t */\n\t\tif (xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tif (XFS_IS_QUOTA_RUNNING(mp) && XFS_IS_PQUOTA_ON(mp)) {\n\t\t\t\tolddquot = xfs_qm_vop_chown(tp, ip,\n\t\t\t\t\t\t\t&ip->i_pdquot, pdqp);\n\t\t\t}\n\t\t\txfs_set_projid(ip, fa->fsx_projid);\n\n\t\t\t/*\n\t\t\t * We may have to rev the inode as well as\n\t\t\t * the superblock version number since projids didn't\n\t\t\t * exist before DINODE_VERSION_2 and SB_VERSION_NLINK.\n\t\t\t */\n\t\t\tif (ip->i_d.di_version == 1)\n\t\t\t\txfs_bump_ino_vers2(tp, ip);\n\t\t}\n\n\t}\n\n\tif (mask & FSX_EXTSIZE)\n\t\tip->i_d.di_extsize = fa->fsx_extsize >> mp->m_sb.sb_blocklog;\n\tif (mask & FSX_XFLAGS) {\n\t\txfs_set_diflags(ip, fa->fsx_xflags);\n\t\txfs_diflags_to_linux(ip);\n\t}\n\n\txfs_trans_ichgtime(tp, ip, XFS_ICHGTIME_CHG);\n\txfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);\n\n\tXFS_STATS_INC(xs_ig_attrchg);\n\n\t/*\n\t * If this is a synchronous mount, make sure that the\n\t * transaction goes to disk before returning to the user.\n\t * This is slightly sub-optimal in that truncates require\n\t * two sync transactions instead of one for wsync filesystems.\n\t * One for the truncate and one for the timestamps since we\n\t * don't want to change the timestamps unless we're sure the\n\t * truncate worked.  Truncates are less than 1% of the laddis\n\t * mix so this probably isn't worth the trouble to optimize.\n\t */\n\tif (mp->m_flags & XFS_MOUNT_WSYNC)\n\t\txfs_trans_set_sync(tp);\n\tcode = xfs_trans_commit(tp, 0);\n\txfs_iunlock(ip, lock_flags);\n\n\t/*\n\t * Release any dquot(s) the inode had kept before chown.\n\t */\n\txfs_qm_dqrele(olddquot);\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\n\treturn code;\n\n error_return:\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\txfs_trans_cancel(tp, 0);\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\n\treturn code;\n}",
                        "cve_id": "CVE-2014-4014"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int persistent_prepare_exception(struct dm_exception_store *store,\n\t\t\t\t\tstruct dm_exception *e)\n{\n\tstruct pstore *ps = get_info(store);\n\tsector_t size = get_dev_size(dm_snap_cow(store->snap)->bdev);\n\n\t/* Is there enough room ? */\n\tif (size < ((ps->next_free + 1) * store->chunk_size))\n\t\treturn -ENOSPC;\n\n\te->new_chunk = ps->next_free;\n\n\t/*\n\t * Move onto the next free pending, making sure to take\n\t * into account the location of the metadata chunks.\n\t */\n\tps->next_free++;\n\tskip_metadata(ps);\n\n\tatomic_inc(&ps->pending_count);\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nSTATIC int\nxfs_ioctl_setattr(\n\txfs_inode_t\t\t*ip,\n\tstruct fsxattr\t\t*fa,\n\tint\t\t\tmask)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tstruct xfs_trans\t*tp;\n\tunsigned int\t\tlock_flags = 0;\n\tstruct xfs_dquot\t*udqp = NULL;\n\tstruct xfs_dquot\t*pdqp = NULL;\n\tstruct xfs_dquot\t*olddquot = NULL;\n\tint\t\t\tcode;\n\n\ttrace_xfs_ioctl_setattr(ip);\n\n\tif (mp->m_flags & XFS_MOUNT_RDONLY)\n\t\treturn XFS_ERROR(EROFS);\n\tif (XFS_FORCED_SHUTDOWN(mp))\n\t\treturn XFS_ERROR(EIO);\n\n\t/*\n\t * Disallow 32bit project ids when projid32bit feature is not enabled.\n\t */\n\tif ((mask & FSX_PROJID) && (fa->fsx_projid > (__uint16_t)-1) &&\n\t\t\t!xfs_sb_version_hasprojid32bit(&ip->i_mount->m_sb))\n\t\treturn XFS_ERROR(EINVAL);\n\n\t/*\n\t * If disk quotas is on, we make sure that the dquots do exist on disk,\n\t * before we start any other transactions. Trying to do this later\n\t * is messy. We don't care to take a readlock to look at the ids\n\t * in inode here, because we can't hold it across the trans_reserve.\n\t * If the IDs do change before we take the ilock, we're covered\n\t * because the i_*dquot fields will get updated anyway.\n\t */\n\tif (XFS_IS_QUOTA_ON(mp) && (mask & FSX_PROJID)) {\n\t\tcode = xfs_qm_vop_dqalloc(ip, ip->i_d.di_uid,\n\t\t\t\t\t ip->i_d.di_gid, fa->fsx_projid,\n\t\t\t\t\t XFS_QMOPT_PQUOTA, &udqp, NULL, &pdqp);\n\t\tif (code)\n\t\t\treturn code;\n\t}\n\n\t/*\n\t * For the other attributes, we acquire the inode lock and\n\t * first do an error checking pass.\n\t */\n\ttp = xfs_trans_alloc(mp, XFS_TRANS_SETATTR_NOT_SIZE);\n\tcode = xfs_trans_reserve(tp, &M_RES(mp)->tr_ichange, 0, 0);\n\tif (code)\n\t\tgoto error_return;\n\n\tlock_flags = XFS_ILOCK_EXCL;\n\txfs_ilock(ip, lock_flags);\n\n\t/*\n\t * CAP_FOWNER overrides the following restrictions:\n\t *\n\t * The user ID of the calling process must be equal\n\t * to the file owner ID, except in cases where the\n\t * CAP_FSETID capability is applicable.\n\t */\n\tif (!inode_owner_or_capable(VFS_I(ip))) {\n\t\tcode = XFS_ERROR(EPERM);\n\t\tgoto error_return;\n\t}\n\n\t/*\n\t * Do a quota reservation only if projid is actually going to change.\n\t * Only allow changing of projid from init_user_ns since it is a\n\t * non user namespace aware identifier.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\tif (current_user_ns() != &init_user_ns) {\n\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\tgoto error_return;\n\t\t}\n\n\t\tif (XFS_IS_QUOTA_RUNNING(mp) &&\n\t\t    XFS_IS_PQUOTA_ON(mp) &&\n\t\t    xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tASSERT(tp);\n\t\t\tcode = xfs_qm_vop_chown_reserve(tp, ip, udqp, NULL,\n\t\t\t\t\t\tpdqp, capable(CAP_FOWNER) ?\n\t\t\t\t\t\tXFS_QMOPT_FORCE_RES : 0);\n\t\t\tif (code)\t/* out of quota */\n\t\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\tif (mask & FSX_EXTSIZE) {\n\t\t/*\n\t\t * Can't change extent size if any extents are allocated.\n\t\t */\n\t\tif (ip->i_d.di_nextents &&\n\t\t    ((ip->i_d.di_extsize << mp->m_sb.sb_blocklog) !=\n\t\t     fa->fsx_extsize)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * Extent size must be a multiple of the appropriate block\n\t\t * size, if set at all. It must also be smaller than the\n\t\t * maximum extent size supported by the filesystem.\n\t\t *\n\t\t * Also, for non-realtime files, limit the extent size hint to\n\t\t * half the size of the AGs in the filesystem so alignment\n\t\t * doesn't result in extents larger than an AG.\n\t\t */\n\t\tif (fa->fsx_extsize != 0) {\n\t\t\txfs_extlen_t    size;\n\t\t\txfs_fsblock_t   extsize_fsb;\n\n\t\t\textsize_fsb = XFS_B_TO_FSB(mp, fa->fsx_extsize);\n\t\t\tif (extsize_fsb > MAXEXTLEN) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\n\t\t\tif (XFS_IS_REALTIME_INODE(ip) ||\n\t\t\t    ((mask & FSX_XFLAGS) &&\n\t\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME))) {\n\t\t\t\tsize = mp->m_sb.sb_rextsize <<\n\t\t\t\t       mp->m_sb.sb_blocklog;\n\t\t\t} else {\n\t\t\t\tsize = mp->m_sb.sb_blocksize;\n\t\t\t\tif (extsize_fsb > mp->m_sb.sb_agblocks / 2) {\n\t\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\t\tgoto error_return;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (fa->fsx_extsize % size) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\t}\n\n\n\tif (mask & FSX_XFLAGS) {\n\t\t/*\n\t\t * Can't change realtime flag if any extents are allocated.\n\t\t */\n\t\tif ((ip->i_d.di_nextents || ip->i_delayed_blks) &&\n\t\t    (XFS_IS_REALTIME_INODE(ip)) !=\n\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * If realtime flag is set then must have realtime data.\n\t\t */\n\t\tif ((fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tif ((mp->m_sb.sb_rblocks == 0) ||\n\t\t\t    (mp->m_sb.sb_rextsize == 0) ||\n\t\t\t    (ip->i_d.di_extsize % mp->m_sb.sb_rextsize)) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Can't modify an immutable/append-only file unless\n\t\t * we have appropriate permission.\n\t\t */\n\t\tif ((ip->i_d.di_flags &\n\t\t\t\t(XFS_DIFLAG_IMMUTABLE|XFS_DIFLAG_APPEND) ||\n\t\t     (fa->fsx_xflags &\n\t\t\t\t(XFS_XFLAG_IMMUTABLE | XFS_XFLAG_APPEND))) &&\n\t\t    !capable(CAP_LINUX_IMMUTABLE)) {\n\t\t\tcode = XFS_ERROR(EPERM);\n\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\txfs_trans_ijoin(tp, ip, 0);\n\n\t/*\n\t * Change file ownership.  Must be the owner or privileged.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\t/*\n\t\t * CAP_FSETID overrides the following restrictions:\n\t\t *\n\t\t * The set-user-ID and set-group-ID bits of a file will be\n\t\t * cleared upon successful return from chown()\n\t\t */\n\t\tif ((ip->i_d.di_mode & (S_ISUID|S_ISGID)) &&\n\t\t    !inode_capable(VFS_I(ip), CAP_FSETID))\n\t\t\tip->i_d.di_mode &= ~(S_ISUID|S_ISGID);\n\n\t\t/*\n\t\t * Change the ownerships and register quota modifications\n\t\t * in the transaction.\n\t\t */\n\t\tif (xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tif (XFS_IS_QUOTA_RUNNING(mp) && XFS_IS_PQUOTA_ON(mp)) {\n\t\t\t\tolddquot = xfs_qm_vop_chown(tp, ip,\n\t\t\t\t\t\t\t&ip->i_pdquot, pdqp);\n\t\t\t}\n\t\t\txfs_set_projid(ip, fa->fsx_projid);\n\n\t\t\t/*\n\t\t\t * We may have to rev the inode as well as\n\t\t\t * the superblock version number since projids didn't\n\t\t\t * exist before DINODE_VERSION_2 and SB_VERSION_NLINK.\n\t\t\t */\n\t\t\tif (ip->i_d.di_version == 1)\n\t\t\t\txfs_bump_ino_vers2(tp, ip);\n\t\t}\n\n\t}\n\n\tif (mask & FSX_EXTSIZE)\n\t\tip->i_d.di_extsize = fa->fsx_extsize >> mp->m_sb.sb_blocklog;\n\tif (mask & FSX_XFLAGS) {\n\t\txfs_set_diflags(ip, fa->fsx_xflags);\n\t\txfs_diflags_to_linux(ip);\n\t}\n\n\txfs_trans_ichgtime(tp, ip, XFS_ICHGTIME_CHG);\n\txfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);\n\n\tXFS_STATS_INC(xs_ig_attrchg);\n\n\t/*\n\t * If this is a synchronous mount, make sure that the\n\t * transaction goes to disk before returning to the user.\n\t * This is slightly sub-optimal in that truncates require\n\t * two sync transactions instead of one for wsync filesystems.\n\t * One for the truncate and one for the timestamps since we\n\t * don't want to change the timestamps unless we're sure the\n\t * truncate worked.  Truncates are less than 1% of the laddis\n\t * mix so this probably isn't worth the trouble to optimize.\n\t */\n\tif (mp->m_flags & XFS_MOUNT_WSYNC)\n\t\txfs_trans_set_sync(tp);\n\tcode = xfs_trans_commit(tp, 0);\n\txfs_iunlock(ip, lock_flags);\n\n\t/*\n\t * Release any dquot(s) the inode had kept before chown.\n\t */\n\txfs_qm_dqrele(olddquot);\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\n\treturn code;\n\n error_return:\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\txfs_trans_cancel(tp, 0);\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\n\treturn code;\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nSTATIC int\nxfs_ioctl_setattr(\n\txfs_inode_t\t\t*ip,\n\tstruct fsxattr\t\t*fa,\n\tint\t\t\tmask)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tstruct xfs_trans\t*tp;\n\tunsigned int\t\tlock_flags = 0;\n\tstruct xfs_dquot\t*udqp = NULL;\n\tstruct xfs_dquot\t*pdqp = NULL;\n\tstruct xfs_dquot\t*olddquot = NULL;\n\tint\t\t\tcode;\n\n\ttrace_xfs_ioctl_setattr(ip);\n\n\tif (mp->m_flags & XFS_MOUNT_RDONLY)\n\t\treturn XFS_ERROR(EROFS);\n\tif (XFS_FORCED_SHUTDOWN(mp))\n\t\treturn XFS_ERROR(EIO);\n\n\t/*\n\t * Disallow 32bit project ids when projid32bit feature is not enabled.\n\t */\n\tif ((mask & FSX_PROJID) && (fa->fsx_projid > (__uint16_t)-1) &&\n\t\t\t!xfs_sb_version_hasprojid32bit(&ip->i_mount->m_sb))\n\t\treturn XFS_ERROR(EINVAL);\n\n\t/*\n\t * If disk quotas is on, we make sure that the dquots do exist on disk,\n\t * before we start any other transactions. Trying to do this later\n\t * is messy. We don't care to take a readlock to look at the ids\n\t * in inode here, because we can't hold it across the trans_reserve.\n\t * If the IDs do change before we take the ilock, we're covered\n\t * because the i_*dquot fields will get updated anyway.\n\t */\n\tif (XFS_IS_QUOTA_ON(mp) && (mask & FSX_PROJID)) {\n\t\tcode = xfs_qm_vop_dqalloc(ip, ip->i_d.di_uid,\n\t\t\t\t\t ip->i_d.di_gid, fa->fsx_projid,\n\t\t\t\t\t XFS_QMOPT_PQUOTA, &udqp, NULL, &pdqp);\n\t\tif (code)\n\t\t\treturn code;\n\t}\n\n\t/*\n\t * For the other attributes, we acquire the inode lock and\n\t * first do an error checking pass.\n\t */\n\ttp = xfs_trans_alloc(mp, XFS_TRANS_SETATTR_NOT_SIZE);\n\tcode = xfs_trans_reserve(tp, &M_RES(mp)->tr_ichange, 0, 0);\n\tif (code)\n\t\tgoto error_return;\n\n\tlock_flags = XFS_ILOCK_EXCL;\n\txfs_ilock(ip, lock_flags);\n\n\t/*\n\t * CAP_FOWNER overrides the following restrictions:\n\t *\n\t * The user ID of the calling process must be equal\n\t * to the file owner ID, except in cases where the\n\t * CAP_FSETID capability is applicable.\n\t */\n\tif (!inode_owner_or_capable(VFS_I(ip))) {\n\t\tcode = XFS_ERROR(EPERM);\n\t\tgoto error_return;\n\t}\n\n\t/*\n\t * Do a quota reservation only if projid is actually going to change.\n\t * Only allow changing of projid from init_user_ns since it is a\n\t * non user namespace aware identifier.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\tif (current_user_ns() != &init_user_ns) {\n\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\tgoto error_return;\n\t\t}\n\n\t\tif (XFS_IS_QUOTA_RUNNING(mp) &&\n\t\t    XFS_IS_PQUOTA_ON(mp) &&\n\t\t    xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tASSERT(tp);\n\t\t\tcode = xfs_qm_vop_chown_reserve(tp, ip, udqp, NULL,\n\t\t\t\t\t\tpdqp, capable(CAP_FOWNER) ?\n\t\t\t\t\t\tXFS_QMOPT_FORCE_RES : 0);\n\t\t\tif (code)\t/* out of quota */\n\t\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\tif (mask & FSX_EXTSIZE) {\n\t\t/*\n\t\t * Can't change extent size if any extents are allocated.\n\t\t */\n\t\tif (ip->i_d.di_nextents &&\n\t\t    ((ip->i_d.di_extsize << mp->m_sb.sb_blocklog) !=\n\t\t     fa->fsx_extsize)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * Extent size must be a multiple of the appropriate block\n\t\t * size, if set at all. It must also be smaller than the\n\t\t * maximum extent size supported by the filesystem.\n\t\t *\n\t\t * Also, for non-realtime files, limit the extent size hint to\n\t\t * half the size of the AGs in the filesystem so alignment\n\t\t * doesn't result in extents larger than an AG.\n\t\t */\n\t\tif (fa->fsx_extsize != 0) {\n\t\t\txfs_extlen_t    size;\n\t\t\txfs_fsblock_t   extsize_fsb;\n\n\t\t\textsize_fsb = XFS_B_TO_FSB(mp, fa->fsx_extsize);\n\t\t\tif (extsize_fsb > MAXEXTLEN) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\n\t\t\tif (XFS_IS_REALTIME_INODE(ip) ||\n\t\t\t    ((mask & FSX_XFLAGS) &&\n\t\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME))) {\n\t\t\t\tsize = mp->m_sb.sb_rextsize <<\n\t\t\t\t       mp->m_sb.sb_blocklog;\n\t\t\t} else {\n\t\t\t\tsize = mp->m_sb.sb_blocksize;\n\t\t\t\tif (extsize_fsb > mp->m_sb.sb_agblocks / 2) {\n\t\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\t\tgoto error_return;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (fa->fsx_extsize % size) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\t}\n\n\n\tif (mask & FSX_XFLAGS) {\n\t\t/*\n\t\t * Can't change realtime flag if any extents are allocated.\n\t\t */\n\t\tif ((ip->i_d.di_nextents || ip->i_delayed_blks) &&\n\t\t    (XFS_IS_REALTIME_INODE(ip)) !=\n\t\t    (fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tcode = XFS_ERROR(EINVAL);\t/* EFBIG? */\n\t\t\tgoto error_return;\n\t\t}\n\n\t\t/*\n\t\t * If realtime flag is set then must have realtime data.\n\t\t */\n\t\tif ((fa->fsx_xflags & XFS_XFLAG_REALTIME)) {\n\t\t\tif ((mp->m_sb.sb_rblocks == 0) ||\n\t\t\t    (mp->m_sb.sb_rextsize == 0) ||\n\t\t\t    (ip->i_d.di_extsize % mp->m_sb.sb_rextsize)) {\n\t\t\t\tcode = XFS_ERROR(EINVAL);\n\t\t\t\tgoto error_return;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Can't modify an immutable/append-only file unless\n\t\t * we have appropriate permission.\n\t\t */\n\t\tif ((ip->i_d.di_flags &\n\t\t\t\t(XFS_DIFLAG_IMMUTABLE|XFS_DIFLAG_APPEND) ||\n\t\t     (fa->fsx_xflags &\n\t\t\t\t(XFS_XFLAG_IMMUTABLE | XFS_XFLAG_APPEND))) &&\n\t\t    !capable(CAP_LINUX_IMMUTABLE)) {\n\t\t\tcode = XFS_ERROR(EPERM);\n\t\t\tgoto error_return;\n\t\t}\n\t}\n\n\txfs_trans_ijoin(tp, ip, 0);\n\n\t/*\n\t * Change file ownership.  Must be the owner or privileged.\n\t */\n\tif (mask & FSX_PROJID) {\n\t\t/*\n\t\t * CAP_FSETID overrides the following restrictions:\n\t\t *\n\t\t * The set-user-ID and set-group-ID bits of a file will be\n\t\t * cleared upon successful return from chown()\n\t\t */\n\t\tif ((ip->i_d.di_mode & (S_ISUID|S_ISGID)) &&\n\t\t    !capable_wrt_inode_uidgid(VFS_I(ip), CAP_FSETID))\n\t\t\tip->i_d.di_mode &= ~(S_ISUID|S_ISGID);\n\n\t\t/*\n\t\t * Change the ownerships and register quota modifications\n\t\t * in the transaction.\n\t\t */\n\t\tif (xfs_get_projid(ip) != fa->fsx_projid) {\n\t\t\tif (XFS_IS_QUOTA_RUNNING(mp) && XFS_IS_PQUOTA_ON(mp)) {\n\t\t\t\tolddquot = xfs_qm_vop_chown(tp, ip,\n\t\t\t\t\t\t\t&ip->i_pdquot, pdqp);\n\t\t\t}\n\t\t\txfs_set_projid(ip, fa->fsx_projid);\n\n\t\t\t/*\n\t\t\t * We may have to rev the inode as well as\n\t\t\t * the superblock version number since projids didn't\n\t\t\t * exist before DINODE_VERSION_2 and SB_VERSION_NLINK.\n\t\t\t */\n\t\t\tif (ip->i_d.di_version == 1)\n\t\t\t\txfs_bump_ino_vers2(tp, ip);\n\t\t}\n\n\t}\n\n\tif (mask & FSX_EXTSIZE)\n\t\tip->i_d.di_extsize = fa->fsx_extsize >> mp->m_sb.sb_blocklog;\n\tif (mask & FSX_XFLAGS) {\n\t\txfs_set_diflags(ip, fa->fsx_xflags);\n\t\txfs_diflags_to_linux(ip);\n\t}\n\n\txfs_trans_ichgtime(tp, ip, XFS_ICHGTIME_CHG);\n\txfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);\n\n\tXFS_STATS_INC(xs_ig_attrchg);\n\n\t/*\n\t * If this is a synchronous mount, make sure that the\n\t * transaction goes to disk before returning to the user.\n\t * This is slightly sub-optimal in that truncates require\n\t * two sync transactions instead of one for wsync filesystems.\n\t * One for the truncate and one for the timestamps since we\n\t * don't want to change the timestamps unless we're sure the\n\t * truncate worked.  Truncates are less than 1% of the laddis\n\t * mix so this probably isn't worth the trouble to optimize.\n\t */\n\tif (mp->m_flags & XFS_MOUNT_WSYNC)\n\t\txfs_trans_set_sync(tp);\n\tcode = xfs_trans_commit(tp, 0);\n\txfs_iunlock(ip, lock_flags);\n\n\t/*\n\t * Release any dquot(s) the inode had kept before chown.\n\t */\n\txfs_qm_dqrele(olddquot);\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\n\treturn code;\n\n error_return:\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(pdqp);\n\txfs_trans_cancel(tp, 0);\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\n\treturn code;\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                },
                {
                    "vul_knowledge": {
                        "code_before_change": "static void flush_end_io(struct request *flush_rq, int error)\n{\n\tstruct request_queue *q = flush_rq->q;\n\tstruct list_head *running;\n\tbool queued = false;\n\tstruct request *rq, *n;\n\tunsigned long flags = 0;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, flush_rq->mq_ctx);\n\n\tif (q->mq_ops) {\n\t\tspin_lock_irqsave(&fq->mq_flush_lock, flags);\n\t\tflush_rq->tag = -1;\n\t}\n\n\trunning = &fq->flush_queue[fq->flush_running_idx];\n\tBUG_ON(fq->flush_pending_idx == fq->flush_running_idx);\n\n\t/* account completion of the flush request */\n\tfq->flush_running_idx ^= 1;\n\n\tif (!q->mq_ops)\n\t\telv_completed_request(q, flush_rq);\n\n\t/* and push the waiting requests to the next stage */\n\tlist_for_each_entry_safe(rq, n, running, flush.list) {\n\t\tunsigned int seq = blk_flush_cur_seq(rq);\n\n\t\tBUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);\n\t\tqueued |= blk_flush_complete_seq(rq, fq, seq, error);\n\t}\n\n\t/*\n\t * Kick the queue to avoid stall for two cases:\n\t * 1. Moving a request silently to empty queue_head may stall the\n\t * queue.\n\t * 2. When flush request is running in non-queueable queue, the\n\t * queue is hold. Restart the queue after flush request is finished\n\t * to avoid stall.\n\t * This function is called from request completion path and calling\n\t * directly into request_fn may confuse the driver.  Always use\n\t * kblockd.\n\t */\n\tif (queued || fq->flush_queue_delayed) {\n\t\tWARN_ON(q->mq_ops);\n\t\tblk_run_queue_async(q);\n\t}\n\tfq->flush_queue_delayed = 0;\n\tif (q->mq_ops)\n\t\tspin_unlock_irqrestore(&fq->mq_flush_lock, flags);\n}",
                        "code_after_change": "static void flush_end_io(struct request *flush_rq, int error)\n{\n\tstruct request_queue *q = flush_rq->q;\n\tstruct list_head *running;\n\tbool queued = false;\n\tstruct request *rq, *n;\n\tunsigned long flags = 0;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, flush_rq->mq_ctx);\n\n\tif (q->mq_ops) {\n\t\tstruct blk_mq_hw_ctx *hctx;\n\n\t\t/* release the tag's ownership to the req cloned from */\n\t\tspin_lock_irqsave(&fq->mq_flush_lock, flags);\n\t\thctx = q->mq_ops->map_queue(q, flush_rq->mq_ctx->cpu);\n\t\tblk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);\n\t\tflush_rq->tag = -1;\n\t}\n\n\trunning = &fq->flush_queue[fq->flush_running_idx];\n\tBUG_ON(fq->flush_pending_idx == fq->flush_running_idx);\n\n\t/* account completion of the flush request */\n\tfq->flush_running_idx ^= 1;\n\n\tif (!q->mq_ops)\n\t\telv_completed_request(q, flush_rq);\n\n\t/* and push the waiting requests to the next stage */\n\tlist_for_each_entry_safe(rq, n, running, flush.list) {\n\t\tunsigned int seq = blk_flush_cur_seq(rq);\n\n\t\tBUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);\n\t\tqueued |= blk_flush_complete_seq(rq, fq, seq, error);\n\t}\n\n\t/*\n\t * Kick the queue to avoid stall for two cases:\n\t * 1. Moving a request silently to empty queue_head may stall the\n\t * queue.\n\t * 2. When flush request is running in non-queueable queue, the\n\t * queue is hold. Restart the queue after flush request is finished\n\t * to avoid stall.\n\t * This function is called from request completion path and calling\n\t * directly into request_fn may confuse the driver.  Always use\n\t * kblockd.\n\t */\n\tif (queued || fq->flush_queue_delayed) {\n\t\tWARN_ON(q->mq_ops);\n\t\tblk_run_queue_async(q);\n\t}\n\tfq->flush_queue_delayed = 0;\n\tif (q->mq_ops)\n\t\tspin_unlock_irqrestore(&fq->mq_flush_lock, flags);\n}",
                        "cve_id": "CVE-2015-9016"
                    },
                    "vul_detect_prompt": "I want you to act as a vulnerability detection expert, given the following code snippet and related vulnerability knowledge, please detect whether there is a vulnerability in the code snippet.\nCode Snippet:\n'''\nstatic int persistent_prepare_exception(struct dm_exception_store *store,\n\t\t\t\t\tstruct dm_exception *e)\n{\n\tstruct pstore *ps = get_info(store);\n\tsector_t size = get_dev_size(dm_snap_cow(store->snap)->bdev);\n\n\t/* Is there enough room ? */\n\tif (size < ((ps->next_free + 1) * store->chunk_size))\n\t\treturn -ENOSPC;\n\n\te->new_chunk = ps->next_free;\n\n\t/*\n\t * Move onto the next free pending, making sure to take\n\t * into account the location of the metadata chunks.\n\t */\n\tps->next_free++;\n\tskip_metadata(ps);\n\n\tatomic_inc(&ps->pending_count);\n\treturn 0;\n}\n'''\nRelated vulnerability code snippet:\n'''\nstatic void flush_end_io(struct request *flush_rq, int error)\n{\n\tstruct request_queue *q = flush_rq->q;\n\tstruct list_head *running;\n\tbool queued = false;\n\tstruct request *rq, *n;\n\tunsigned long flags = 0;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, flush_rq->mq_ctx);\n\n\tif (q->mq_ops) {\n\t\tspin_lock_irqsave(&fq->mq_flush_lock, flags);\n\t\tflush_rq->tag = -1;\n\t}\n\n\trunning = &fq->flush_queue[fq->flush_running_idx];\n\tBUG_ON(fq->flush_pending_idx == fq->flush_running_idx);\n\n\t/* account completion of the flush request */\n\tfq->flush_running_idx ^= 1;\n\n\tif (!q->mq_ops)\n\t\telv_completed_request(q, flush_rq);\n\n\t/* and push the waiting requests to the next stage */\n\tlist_for_each_entry_safe(rq, n, running, flush.list) {\n\t\tunsigned int seq = blk_flush_cur_seq(rq);\n\n\t\tBUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);\n\t\tqueued |= blk_flush_complete_seq(rq, fq, seq, error);\n\t}\n\n\t/*\n\t * Kick the queue to avoid stall for two cases:\n\t * 1. Moving a request silently to empty queue_head may stall the\n\t * queue.\n\t * 2. When flush request is running in non-queueable queue, the\n\t * queue is hold. Restart the queue after flush request is finished\n\t * to avoid stall.\n\t * This function is called from request completion path and calling\n\t * directly into request_fn may confuse the driver.  Always use\n\t * kblockd.\n\t */\n\tif (queued || fq->flush_queue_delayed) {\n\t\tWARN_ON(q->mq_ops);\n\t\tblk_run_queue_async(q);\n\t}\n\tfq->flush_queue_delayed = 0;\n\tif (q->mq_ops)\n\t\tspin_unlock_irqrestore(&fq->mq_flush_lock, flags);\n}\n'''\nRelated non-vulnerability code snippet:\n'''\nstatic void flush_end_io(struct request *flush_rq, int error)\n{\n\tstruct request_queue *q = flush_rq->q;\n\tstruct list_head *running;\n\tbool queued = false;\n\tstruct request *rq, *n;\n\tunsigned long flags = 0;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, flush_rq->mq_ctx);\n\n\tif (q->mq_ops) {\n\t\tstruct blk_mq_hw_ctx *hctx;\n\n\t\t/* release the tag's ownership to the req cloned from */\n\t\tspin_lock_irqsave(&fq->mq_flush_lock, flags);\n\t\thctx = q->mq_ops->map_queue(q, flush_rq->mq_ctx->cpu);\n\t\tblk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);\n\t\tflush_rq->tag = -1;\n\t}\n\n\trunning = &fq->flush_queue[fq->flush_running_idx];\n\tBUG_ON(fq->flush_pending_idx == fq->flush_running_idx);\n\n\t/* account completion of the flush request */\n\tfq->flush_running_idx ^= 1;\n\n\tif (!q->mq_ops)\n\t\telv_completed_request(q, flush_rq);\n\n\t/* and push the waiting requests to the next stage */\n\tlist_for_each_entry_safe(rq, n, running, flush.list) {\n\t\tunsigned int seq = blk_flush_cur_seq(rq);\n\n\t\tBUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);\n\t\tqueued |= blk_flush_complete_seq(rq, fq, seq, error);\n\t}\n\n\t/*\n\t * Kick the queue to avoid stall for two cases:\n\t * 1. Moving a request silently to empty queue_head may stall the\n\t * queue.\n\t * 2. When flush request is running in non-queueable queue, the\n\t * queue is hold. Restart the queue after flush request is finished\n\t * to avoid stall.\n\t * This function is called from request completion path and calling\n\t * directly into request_fn may confuse the driver.  Always use\n\t * kblockd.\n\t */\n\tif (queued || fq->flush_queue_delayed) {\n\t\tWARN_ON(q->mq_ops);\n\t\tblk_run_queue_async(q);\n\t}\n\tfq->flush_queue_delayed = 0;\n\tif (q->mq_ops)\n\t\tspin_unlock_irqrestore(&fq->mq_flush_lock, flags);\n}\n'''\nPlease use your own knowledge of vulnerabilities in combination with the above related code snippets to detect whether there is a vulnerability in the code snippet. Please answer YES or NO without explanation.\n",
                    "vul_output": "NO"
                }
            ],
            "detection_model": "qwen-max",
            "model_settings": {
                "temperature": 0.01
            },
            "final_result": 0
        }
    ]
}